{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dipak/FlinkBot/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/dipak/FlinkBot/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'mps', \"trust_remote_code\": True}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_URL = os.environ['MILVUS_URL']\n",
    "MILVUS_KEY = os.environ['MILVUS_URL']\n",
    "DIMS = 1024\n",
    "EMBEDDING_MODEL = \"embed-english-v3.0\"\n",
    "COHERE_KEY=os.environ['COHERE_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_fn = CohereEmbeddings(model=EMBEDDING_MODEL, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = hf.embed_query(\"What is flink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.zilliz import Zilliz\n",
    "\n",
    "zilliz = Zilliz(\n",
    "    embedding_function = hf,\n",
    "    collection_name=\"Flink\",\n",
    "    connection_args={\"uri\": MILVUS_URL, \"token\": MILVUS_KEY},\n",
    "    auto_id=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = zilliz.as_retriever(search_kwargs={\"k\": 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='how to configure the relevant components. The size of those components always has to be between its maximum and minimum value, otherwise Flink startup will fail.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/memory/mem_setup/', 'type': 'document', 'pk': 450143955092274740}),\n",
       " Document(page_content='â\\x96¾ Monitoring Checkpointing Monitoring Back Pressure Upgrading Applications and Flink Versions Production Readiness Checklist Flink Development â\\x96¾ Importing Flink into an IDE Building Flink from Source Internals â\\x96¾ Jobs and Scheduling Task Lifecycle File Systems Project Homepage JavaDocs ScalaDocs PyDocs Pick Docs Version â\\x96¾ 1.16 (â\\x9c\\x93) v1.16 v1.15 All Versions ä¸\\xadæ\\x96\\x87ç\\x89\\x88 Elastic Scaling On This Page Reactive Mode Getting started Usage Limitations Adaptive Scheduler Usage Limitations Adaptive Batch Scheduler Usage Performance tuning Limitations Elastic Scaling # Apache Flink allows you to rescale your jobs. You can do this manually by stopping the job and restarting from the savepoint created during shutdown with a different parallelism. This page describes options where Flink automatically adjusts the parallelism instead. Reactive Mode # Reactive mode is an MVP (“minimum viable product”) feature. The Flink community is actively looking for feedback by users through our mailing', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/', 'type': 'document', 'pk': 450143955092275030}),\n",
       " Document(page_content='scheduling Tuning Checkpoints and Large State # This page gives a guide how to configure and tune applications that use large state. Overview # For Flink applications to run reliably at large scale, two conditions must be fulfilled: The application needs to be able to take checkpoints reliably The resources need to be sufficient catch up with the input data streams after a failure The first sections discuss how to get well performing checkpoints at scale.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273160}),\n",
       " Document(page_content='configuration ) { return this ; } } Capacity Planning # This section discusses how to decide how many resources should be used for a Flink job to run reliably.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273182}),\n",
       " Document(page_content='scheduler are also available with it. The Flink community is working on addressing these limitations. No support for the Elastic Scaling . The elastic scaling only supports slot requests without specified-resource at the moment. No support for task manager redundancy . The slotmanager.redundant-taskmanager-num is used to start redundant TaskManagers to speed up job recovery. This config option will not take effect in fine-grained resource management at the moment. No support for evenly spread out slot strategy . This strategy tries to spread out the slots evenly across all available TaskManagers. The strategy is not supported in the first version of fine-grained resource management and cluster.evenly-spread-out-slots will not take effect in it at the moment. Limited integration with Flinkâ\\x80\\x99s Web UI . Slots in fine-grained resource management can have different resource specs. The web UI only shows the slot number without its details at the moment. Limited integration with batch jobs', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/finegrained_resource/', 'type': 'document', 'pk': 450143955092275108}),\n",
       " Document(page_content='feature. The Flink community is actively looking for feedback by users through our mailing lists. Please check the limitations listed on this page. Reactive Mode configures a job so that it always uses all resources available in the cluster. Adding a TaskManager will scale up your job, removing resources will scale it down. Flink will manage the parallelism of the job, always setting it to the highest possible values. Reactive Mode restarts a job on a rescaling event, restoring it from the latest completed checkpoint. This means that there is no overhead of creating a savepoint (which is needed for manually rescaling a job). Also, the amount of data that is reprocessed after rescaling depends on the checkpointing interval, and the restore time depends on the state size. The Reactive Mode allows Flink users to implement a powerful autoscaling mechanism, by having an external service monitor certain metrics, such as consumer lag, aggregate CPU utilization, throughput or latency. As soon', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/', 'type': 'document', 'pk': 450143955092275032}),\n",
       " Document(page_content='Elastic Scaling | Apache Flink v1.16.2 Try Flink â\\x96¾ First steps Fraud Detection with the DataStream API Real Time Reporting with the Table API Flink Operations Playground Learn Flink â\\x96¾ Overview Intro to the DataStream API Data Pipelines & ETL Streaming Analytics Event-driven Applications Fault Tolerance Concepts â\\x96¾ Overview Stateful Stream Processing Timely Stream Processing Flink Architecture Glossary Application Development â\\x96¾ Project Configuration â\\x96¾ Overview Using Maven Using Gradle Connectors and Formats Test Dependencies Advanced Configuration DataStream API â\\x96¾ Overview Execution Mode (Batch/Streaming) Event Time â\\x96¾ Generating Watermarks Builtin Watermark Generators State & Fault Tolerance â\\x96¾ Working with State The Broadcast State Pattern Checkpointing Queryable State State Backends Data Types & Serialization â\\x96¾ Overview State Schema Evolution Custom State Serialization 3rd Party Serializers User-Defined Functions Operators â\\x96¾ Overview Windows Joining Process Function', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/', 'type': 'document', 'pk': 450143955092275018}),\n",
       " Document(page_content='Hence, you need to build a dedicated Flink Image per application.\\nPlease check here for the details.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274072}),\n",
       " Document(page_content='without putting undue load on your sinks. How much load can your Task Managers sustain: All of Flinks’ built-in state backends support asynchronous checkpointing, meaning the snapshot process will not pause data processing. However, it still does require CPU cycles and network bandwidth from your machines. Incremental checkpointing can be a powerful tool to reduce the cost of any given checkpoint. And most importantly, test and measure your job. Every Flink application is unique, and the best way to find the appropriate checkpoint interval is to see how yours behaves in practice. Configure JobManager High Availability # The JobManager serves as a central coordinator for each Flink deployment, being responsible for both scheduling and resource management of the cluster.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/production_ready/', 'type': 'document', 'pk': 450143955092273914}),\n",
       " Document(page_content='In combination with Kubernetes, the replica count of the TaskManager deployment determines the available resources. Increasing the replica count will scale up the job, reducing it will trigger a scale down. This can also be done automatically by using a Horizontal Pod Autoscaler . To use Reactive Mode on Kubernetes, follow the same steps as for deploying a job using an Application Cluster . But instead of flink-configuration-configmap.yaml use this config map: flink-reactive-mode-configuration-configmap.yaml . It contains the scheduler-mode: reactive setting for Flink. Once you have deployed the Application Cluster , you can scale your job up or down by changing the replica count in the flink-taskmanager deployment. Enabling Local Recovery Across Pod Restarts # In order to speed up recoveries in case of pod failures, you can leverage Flink’s working directory feature together with local recovery.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/', 'type': 'document', 'pk': 450143955092274150}),\n",
       " Document(page_content='Flink on YARN Resource Allocation Behavior High-Availability on YARN Supported Hadoop versions. Running Flink on YARN behind Firewalls User jars & Classpath', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/yarn/', 'type': 'document', 'pk': 450143955092274312}),\n",
       " Document(page_content='While the Flink community has attempted to provide sensible defaults for each configuration, it is important to review this list and ensure the options chosen are sufficient for your needs. Set An Explicit Max Parallelism # The max parallelism, set on a per-job and per-operator granularity, determines the maximum parallelism to which a stateful operator can scale.\\nThere is currently no way to change the maximum parallelism of an operator after a job has started without discarding that operators state.\\nThe reason maximum parallelism exists, versus allowing stateful operators to be infinitely scalable, is that it has some impact on your application’s performance and state size.\\nFlink has to maintain specific metadata for its ability to rescale state which grows linearly with max parallelism.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/production_ready/', 'type': 'document', 'pk': 450143955092273906}),\n",
       " Document(page_content='While the community strives to offer sensible defaults to all configurations, the full breadth of applications\\nthat users deploy on Flink means this isn’t always possible. To provide the most production value to our users,\\nFlink allows both high-level and fine-grained tuning of memory allocation within clusters. To optimize memory requirements, check the network memory tuning guide . The further described memory configuration is applicable starting with the release version 1.10 for TaskManager and 1.11 for JobManager processes. If you upgrade Flink from earlier versions, check the migration guide because many changes were introduced with the 1.10 and 1.11 releases. Configure Total Memory # The total process memory of Flink JVM processes consists of memory consumed by the Flink application ( total Flink memory )', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/memory/mem_setup/', 'type': 'document', 'pk': 450143955092274728}),\n",
       " Document(page_content='if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely. metrics.internal.query-service.port \"0\" String The port range used for Flink\\'s internal metric query service. Accepts a list of ports (â\\x80\\x9c50100,50101â\\x80\\x9d), ranges(â\\x80\\x9c50100-50200â\\x80\\x9d) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. metrics.internal.query-service.thread-priority 1 Integer The thread priority used for Flink\\'s internal metric query service. The thread is created by Akka\\'s thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down. metrics.job.status.enable CURRENT_TIME List<Enum> The selection of job status metrics that should be reported. Possible values: \"STATE\": For a given state, return 1 if the job is currently', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/config/', 'type': 'document', 'pk': 450143955092274468}),\n",
       " Document(page_content='},\\n          \"id\" : {\\n            \"type\" : \"any\"\\n          },\\n          \"maxParallelism\" : {\\n            \"type\" : \"integer\"\\n          },\\n          \"metrics\" : {\\n            \"type\" : \"object\",\\n            \"id\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:job:metrics:IOMetricsInfo\",\\n            \"properties\" : {\\n              \"accumulated-backpressured-time\" : {\\n                \"type\" : \"integer\"\\n              },\\n              \"accumulated-busy-time\" : {\\n                \"type\" : \"number\"\\n              },\\n              \"accumulated-idle-time\" : {\\n                \"type\" : \"integer\"\\n              },\\n              \"read-bytes\" : {\\n                \"type\" : \"integer\"\\n              },\\n              \"read-bytes-complete\" : {\\n                \"type\" : \"boolean\"\\n              },\\n              \"read-records\" : {\\n                \"type\" : \"integer\"\\n              },\\n              \"read-records-complete\" : {\\n                \"type\" : \"boolean\"\\n              },', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273470}),\n",
       " Document(page_content='Other components can be tuned using multiple options. Framework Memory # You should not change the framework heap memory and framework off-heap memory without a good reason.\\nAdjust them only if you are sure that Flink needs more memory for some internal data structures or operations.\\nIt can be related to a particular deployment environment or job structure, like high parallelism.\\nIn addition, Flink dependencies, such as Hadoop may consume more direct or native memory in certain setups. Note Flink neither isolates heap nor off-heap versions of framework and task memory at the moment.\\nThe separation of framework and task memory can be used in future releases for further optimizations. Local Execution # If you start Flink locally on your machine as a single java program without creating a cluster (e.g. from your IDE)', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/memory/mem_setup_tm/', 'type': 'document', 'pk': 450143955092274780}),\n",
       " Document(page_content='since the per-buffer overhead are significantly higher then per-record overheads in the Flink’s runtime. As a rule of thumb, we don’t recommend thinking about increasing the buffer size, or the buffer timeout unless you can observe a network bottleneck in your real life workload\\n(downstream operator idling, upstream backpressured, output buffer queue is full, downstream input queue is empty). If the buffer size is too large, this can lead to: high memory usage huge checkpoint data (for unaligned checkpoints) long checkpoint time (for aligned checkpoints) inefficient use of allocated memory with a small execution.buffer-timeout because flushed buffers would only be sent partially filled Selecting the buffer count # The number of buffers is configured by the taskmanager.network.memory.buffers-per-channel and taskmanager.network.memory.floating-buffers-per-gate settings. For best throughput, we recommend using the default values for the number of exclusive', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/memory/network_mem_tuning/', 'type': 'document', 'pk': 450143955092274940}),\n",
       " Document(page_content='certain metrics, such as consumer lag, aggregate CPU utilization, throughput or latency. As soon as these metrics are above or below a certain threshold, additional TaskManagers can be added or removed from the Flink cluster. This could be implemented through changing the replica factor of a Kubernetes deployment, or an autoscaling group on AWS. This external service only needs to handle the resource allocation and deallocation. Flink will take care of keeping the job running with the resources available. Getting started # If you just want to try out Reactive Mode, follow these instructions. They assume that you are deploying Flink on a single machine. # these instructions assume you are in the root directory of a Flink distribution. # Put Job into lib/ directory cp ./examples/streaming/TopSpeedWindowing.jar lib/ # Submit Job in Reactive Mode ./bin/standalone-job.sh start -Dscheduler-mode = reactive -Dexecution.checkpointing.interval = \"10s\" -j', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/', 'type': 'document', 'pk': 450143955092275034}),\n",
       " Document(page_content='JobManager Options # Key Default Type Description jobmanager.future-pool.size (none) Integer The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores. jobmanager.io-pool.size (none) Integer The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores. Advanced Scheduling Options # These parameters can help with fine-tuning scheduling for specific situations. Key Default Type Description cluster.evenly-spread-out-slots false Boolean Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available TaskExecutors . cluster.fine-grained-resource-management.enabled false', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/config/', 'type': 'document', 'pk': 450143955092274596}),\n",
       " Document(page_content='}\\n          }\\n        },\\n        \"checkpointed_size\" : {\\n          \"type\" : \"object\",\\n          \"id\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto\",\\n          \"properties\" : {\\n            \"avg\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"max\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"min\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"p50\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p90\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p95\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p99\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p999\" : {\\n              \"type\" : \"number\"\\n            }\\n          }\\n        },\\n        \"end_to_end_duration\" : {\\n          \"type\" : \"object\",\\n          \"$ref\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto\"\\n        },', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273518}),\n",
       " Document(page_content='Application Profiling & Debugging Monitoring â\\x96¾ Monitoring Checkpointing Monitoring Back Pressure Upgrading Applications and Flink Versions Production Readiness Checklist Flink Development â\\x96¾ Importing Flink into an IDE Building Flink from Source Internals â\\x96¾ Jobs and Scheduling Task Lifecycle File Systems Project Homepage JavaDocs ScalaDocs PyDocs Pick Docs Version â\\x96¾ 1.16 (â\\x9c\\x93) v1.16 v1.15 All Versions ä¸\\xadæ\\x96\\x87ç\\x89\\x88 Fine-Grained Resource Management On This Page Applicable Scenarios How it works Usage Enable Fine-Grained Resource Management Specify Resource Requirement for Slot Sharing Group Limitations Notice Deep Dive How it improves resource efficiency Resource Allocation Strategy Fine-Grained Resource Management # Apache Flink works hard to auto-derive sensible default resource requirements for all applications out of the box.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/finegrained_resource/', 'type': 'document', 'pk': 450143955092275082}),\n",
       " Document(page_content='Setting the environment variable MALLOC_ARENA_MAX can avoid unlimited memory growth: $ docker run \\\\ --env MALLOC_ARENA_MAX = 1 \\\\ flink:1.16.2-scala_2.12 <jobmanager | standalone-job | taskmanager> Further Customization # There are several ways in which you can further customize the Flink image: install custom software (e.g. python) enable (symlink) optional libraries or plugins from /opt/flink/opt into /opt/flink/lib or /opt/flink/plugins add other libraries to /opt/flink/lib (e.g. Hadoop) add other plugins to /opt/flink/plugins You can customize the Flink image in several ways: override the container entry point with a custom script where you can run any bootstrap actions.\\nAt the end you can call the standard /docker-entrypoint.sh script of the Flink image with the same arguments\\nas described in supported deployment modes . The following example creates a custom entry point script which enables more libraries and plugins.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274098}),\n",
       " Document(page_content='Flink does not use Akka for data transport. Key Default Type Description akka.ask.callstack true Boolean If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint. akka.ask.timeout 10 s Duration Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d). akka.client-socket-worker-pool.pool-size-factor 1.0 Double The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values. akka.client-socket-worker-pool.pool-size-max 2', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/config/', 'type': 'document', 'pk': 450143955092274690}),\n",
       " Document(page_content='parallelism when re-scaling the program (via a savepoint). Flink’s internal bookkeeping tracks parallel state in the granularity of max-parallelism-many key groups .\\nFlink’s design strives to make it efficient to have a very high value for the maximum parallelism, even if\\nexecuting the program with a low parallelism. Compression # Flink offers optional compression (default: off) for all checkpoints and savepoints. Currently, compression always uses\\nthe snappy compression algorithm (version 1.1.10.x) but we are planning to support\\ncustom compression algorithms in the future. Compression works on the granularity of key-groups in keyed state, i.e.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273188}),\n",
       " Document(page_content=\"For example, you can easily deploy Flink applications on Kubernetes without Flink knowing that it runs on Kubernetes (and without specifying any of the Kubernetes config options here.) See this setup guide for an example. The options in this section are necessary for setups where Flink itself actively requests and releases resources from the orchestrators. YARN # Key Default Type Description external-resource.<resource_name>.yarn.config-key (none) String If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.<resource_name>.amount. flink.hadoop.<key> (none) String A general option to probe Hadoop configuration through prefix 'flink.hadoop.'. Flink will remove the prefix to get <key> (from core-default.xml and hdfs-default.xml ) then set the <key> and value to Hadoop configuration. For example, flink.hadoop.dfs.replication=5 in Flink configuration and convert to dfs.replication=5 in Hadoop\", metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/config/', 'type': 'document', 'pk': 450143955092274420})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How does elastic scaling works in FLink. What are the various configuration used for scaling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274102}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/', 'type': 'document', 'pk': 450143955092274178}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/cli/', 'type': 'document', 'pk': 450143955092274966}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/native_kubernetes/', 'type': 'document', 'pk': 450143955092274216}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/', 'type': 'document', 'pk': 450143955092274124}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274088}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273496}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/cli/', 'type': 'document', 'pk': 450143955092274988}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274072}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273518}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/native_kubernetes/', 'type': 'document', 'pk': 450143955092274204}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/', 'type': 'document', 'pk': 450143955092274168}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/overview/', 'type': 'document', 'pk': 450143955092273984}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274092}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273484}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/cli/', 'type': 'document', 'pk': 450143955092274994}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273556}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274104}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/docker/', 'type': 'document', 'pk': 450143955092274068}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/filesystems/gcs/', 'type': 'document', 'pk': 450143955092275260}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/', 'type': 'document', 'pk': 450143955092273124}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273498}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/', 'type': 'document', 'pk': 450143955092274158}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/advanced/logging/', 'type': 'document', 'pk': 450143955092275694}\n",
      "{'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273470}\n"
     ]
    }
   ],
   "source": [
    "for doc in retriever.invoke(\"List down all the commands used in the flink documenatation along with explanation of the command.\"):\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "# from langchain_cohere import CohereRerank\n",
    "# compressor = CohereRerank(top_n=5, cohere_api_key=COHERE_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "# from flashrank import Ranker\n",
    "# ranker = Ranker()\n",
    "compressor = FlashrankRerank(top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='scheduler are also available with it. The Flink community is working on addressing these limitations. No support for the Elastic Scaling . The elastic scaling only supports slot requests without specified-resource at the moment. No support for task manager redundancy . The slotmanager.redundant-taskmanager-num is used to start redundant TaskManagers to speed up job recovery. This config option will not take effect in fine-grained resource management at the moment. No support for evenly spread out slot strategy . This strategy tries to spread out the slots evenly across all available TaskManagers. The strategy is not supported in the first version of fine-grained resource management and cluster.evenly-spread-out-slots will not take effect in it at the moment. Limited integration with Flinkâ\\x80\\x99s Web UI . Slots in fine-grained resource management can have different resource specs. The web UI only shows the slot number without its details at the moment. Limited integration with batch jobs', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/finegrained_resource/', 'type': 'document', 'pk': 450143955092275108, 'relevance_score': 0.9994518}),\n",
       " Document(page_content='parallelism when re-scaling the program (via a savepoint). Flink’s internal bookkeeping tracks parallel state in the granularity of max-parallelism-many key groups .\\nFlink’s design strives to make it efficient to have a very high value for the maximum parallelism, even if\\nexecuting the program with a low parallelism. Compression # Flink offers optional compression (default: off) for all checkpoints and savepoints. Currently, compression always uses\\nthe snappy compression algorithm (version 1.1.10.x) but we are planning to support\\ncustom compression algorithms in the future. Compression works on the granularity of key-groups in keyed state, i.e.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273188, 'relevance_score': 0.9993647}),\n",
       " Document(page_content='Other components can be tuned using multiple options. Framework Memory # You should not change the framework heap memory and framework off-heap memory without a good reason.\\nAdjust them only if you are sure that Flink needs more memory for some internal data structures or operations.\\nIt can be related to a particular deployment environment or job structure, like high parallelism.\\nIn addition, Flink dependencies, such as Hadoop may consume more direct or native memory in certain setups. Note Flink neither isolates heap nor off-heap versions of framework and task memory at the moment.\\nThe separation of framework and task memory can be used in future releases for further optimizations. Local Execution # If you start Flink locally on your machine as a single java program without creating a cluster (e.g. from your IDE)', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/memory/mem_setup_tm/', 'type': 'document', 'pk': 450143955092274780, 'relevance_score': 0.99930876}),\n",
       " Document(page_content='feature. The Flink community is actively looking for feedback by users through our mailing lists. Please check the limitations listed on this page. Reactive Mode configures a job so that it always uses all resources available in the cluster. Adding a TaskManager will scale up your job, removing resources will scale it down. Flink will manage the parallelism of the job, always setting it to the highest possible values. Reactive Mode restarts a job on a rescaling event, restoring it from the latest completed checkpoint. This means that there is no overhead of creating a savepoint (which is needed for manually rescaling a job). Also, the amount of data that is reprocessed after rescaling depends on the checkpointing interval, and the restore time depends on the state size. The Reactive Mode allows Flink users to implement a powerful autoscaling mechanism, by having an external service monitor certain metrics, such as consumer lag, aggregate CPU utilization, throughput or latency. As soon', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/', 'type': 'document', 'pk': 450143955092275032, 'relevance_score': 0.9991038}),\n",
       " Document(page_content='scheduling Tuning Checkpoints and Large State # This page gives a guide how to configure and tune applications that use large state. Overview # For Flink applications to run reliably at large scale, two conditions must be fulfilled: The application needs to be able to take checkpoints reliably The resources need to be sufficient catch up with the input data streams after a failure The first sections discuss how to get well performing checkpoints at scale.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273160, 'relevance_score': 0.9990474}),\n",
       " Document(page_content='if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely. metrics.internal.query-service.port \"0\" String The port range used for Flink\\'s internal metric query service. Accepts a list of ports (â\\x80\\x9c50100,50101â\\x80\\x9d), ranges(â\\x80\\x9c50100-50200â\\x80\\x9d) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port. metrics.internal.query-service.thread-priority 1 Integer The thread priority used for Flink\\'s internal metric query service. The thread is created by Akka\\'s thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down. metrics.job.status.enable CURRENT_TIME List<Enum> The selection of job status metrics that should be reported. Possible values: \"STATE\": For a given state, return 1 if the job is currently', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/config/', 'type': 'document', 'pk': 450143955092274468, 'relevance_score': 0.99904114}),\n",
       " Document(page_content='Elastic Scaling | Apache Flink v1.16.2 Try Flink â\\x96¾ First steps Fraud Detection with the DataStream API Real Time Reporting with the Table API Flink Operations Playground Learn Flink â\\x96¾ Overview Intro to the DataStream API Data Pipelines & ETL Streaming Analytics Event-driven Applications Fault Tolerance Concepts â\\x96¾ Overview Stateful Stream Processing Timely Stream Processing Flink Architecture Glossary Application Development â\\x96¾ Project Configuration â\\x96¾ Overview Using Maven Using Gradle Connectors and Formats Test Dependencies Advanced Configuration DataStream API â\\x96¾ Overview Execution Mode (Batch/Streaming) Event Time â\\x96¾ Generating Watermarks Builtin Watermark Generators State & Fault Tolerance â\\x96¾ Working with State The Broadcast State Pattern Checkpointing Queryable State State Backends Data Types & Serialization â\\x96¾ Overview State Schema Evolution Custom State Serialization 3rd Party Serializers User-Defined Functions Operators â\\x96¾ Overview Windows Joining Process Function', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/', 'type': 'document', 'pk': 450143955092275018, 'relevance_score': 0.9990193}),\n",
       " Document(page_content='without putting undue load on your sinks. How much load can your Task Managers sustain: All of Flinks’ built-in state backends support asynchronous checkpointing, meaning the snapshot process will not pause data processing. However, it still does require CPU cycles and network bandwidth from your machines. Incremental checkpointing can be a powerful tool to reduce the cost of any given checkpoint. And most importantly, test and measure your job. Every Flink application is unique, and the best way to find the appropriate checkpoint interval is to see how yours behaves in practice. Configure JobManager High Availability # The JobManager serves as a central coordinator for each Flink deployment, being responsible for both scheduling and resource management of the cluster.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/production_ready/', 'type': 'document', 'pk': 450143955092273914, 'relevance_score': 0.9989793}),\n",
       " Document(page_content='While the Flink community has attempted to provide sensible defaults for each configuration, it is important to review this list and ensure the options chosen are sufficient for your needs. Set An Explicit Max Parallelism # The max parallelism, set on a per-job and per-operator granularity, determines the maximum parallelism to which a stateful operator can scale.\\nThere is currently no way to change the maximum parallelism of an operator after a job has started without discarding that operators state.\\nThe reason maximum parallelism exists, versus allowing stateful operators to be infinitely scalable, is that it has some impact on your application’s performance and state size.\\nFlink has to maintain specific metadata for its ability to rescale state which grows linearly with max parallelism.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/production_ready/', 'type': 'document', 'pk': 450143955092273906, 'relevance_score': 0.99893206}),\n",
       " Document(page_content='the network throughput. But this is not always feasible or necessary. It is very rare that only a single channel among all the subtasks in the task manager is being used. The purpose of exclusive buffers is to provide a fluent throughput. While one buffer is in transit, the other is being filled up. With high throughput setups, the number of exclusive buffers is the main factor that defines the amount of in-flight data Flink uses. In the case of backpressure in low throughput setups, you should consider reducing the number of exclusive buffers . Summary # Memory configuration tuning for the network in Flink can be simplified by enabling the buffer debloating mechanism. You may have to tune it. If this does not work, you can disable the buffer debloating mechanism and manually configure the memory segment size and the number of buffers. For this second scenario, we recommend: using the default values for max throughput reducing the memory segment size and/or number of exclusive buffers', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/memory/network_mem_tuning/', 'type': 'document', 'pk': 450143955092274946, 'relevance_score': 0.9988187})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_retriever.invoke(\"How does elastic scaling works in FLink. What are the various configuration used for scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# from langchain_cohere import ChatCohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatCohere(model=\"command-r-plus\", temperature=0.0, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]):\n",
    "    \n",
    "    text = \"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        xml_tag_start = f\"<{doc.metadata['url'].lower()}>\"\n",
    "        xml_tag_end = f\"</{doc.metadata['url'].lower()}>\"\n",
    "        content = doc.page_content\n",
    "        text += f\"{xml_tag_start}\\n{content}\\n{xml_tag_end}\\n\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = \"AIzaSyBVI2jAHepUzLwWoK6qwXCOYxD0NFzZIns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GEMINI_KEY, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.Keep the answer concise and to the point. Write down the citation at the end of the answer that you have taken reference from. The citation names are in form of urls, that are provided in the xml tags.\n",
    "Follow below mention format for citation\n",
    "Citation:\n",
    "        (1) Source URL 1\n",
    "        (2) Source URL 2\n",
    "Only provide citation if you have used the information from the document.\n",
    "Question: {question} \\nContext: {context} \\nAnswer\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_rag = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | example_prompt\n",
    "    | google_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +---------------------------------+           \n",
      "              | Parallel<context,question>Input |           \n",
      "              +---------------------------------+           \n",
      "                    ****                ****                \n",
      "                 ***                        ***             \n",
      "               **                              ***          \n",
      "+----------------------+                          **        \n",
      "| VectorStoreRetriever |                           *        \n",
      "+----------------------+                           *        \n",
      "            *                                      *        \n",
      "            *                                      *        \n",
      "            *                                      *        \n",
      "+---------------------+                     +-------------+ \n",
      "| Lambda(format_docs) |                     | Passthrough | \n",
      "+---------------------+                     +-------------+ \n",
      "                    ****                ****                \n",
      "                        ***          ***                    \n",
      "                           **      **                       \n",
      "              +----------------------------------+          \n",
      "              | Parallel<context,question>Output |          \n",
      "              +----------------------------------+          \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                      +----------------+                    \n",
      "                      | PromptTemplate |                    \n",
      "                      +----------------+                    \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                  +------------------------+                \n",
      "                  | ChatGoogleGenerativeAI |                \n",
      "                  +------------------------+                \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                      +-----------------+                   \n",
      "                      | StrOutputParser |                   \n",
      "                      +-----------------+                   \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                   +-----------------------+                \n",
      "                   | StrOutputParserOutput |                \n",
      "                   +-----------------------+                \n"
     ]
    }
   ],
   "source": [
    "google_rag.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_rag.invoke(\"How does elastic scaling works in FLink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Apache Flink offers elastic scaling, allowing you to adjust your job's parallelism dynamically. You can manually rescale by stopping a job, creating a savepoint, and restarting it with a different parallelism. Flink also provides automatic parallelism adjustment options, such as Reactive Mode.  Citation:\n",
       ">         (1) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_rag.invoke(\"How does elastic scaling works for the batch job. WHat configuration are needed for scaling the batch job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> To enable elastic scaling for batch jobs in Apache Flink, you need to configure the Adaptive Batch Scheduler. This involves setting `jobmanager.scheduler: AdaptiveBatch` and ensuring that the `execution.batch-shuffle-mode` is unset or explicitly set to `ALL_EXCHANGES_BLOCKING`. Additionally, you can fine-tune the scaling behavior using parameters like `jobmanager.adaptive-batch-scheduler.min-parallelism`, `jobmanager.adaptive-batch-scheduler.max-parallelism`, and `jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task`. \n",
       "> \n",
       "> Citation: \n",
       "> (1) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/elastic_scaling/ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_rag.invoke(\"How to configure history server. wHat configurations are needed to setup history server on azure?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> This document does not contain the answer for how to setup history server on azure. However, it explains how to integrate history server with log archiving and browsing services. Citation:\n",
       ">         (1) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/advanced/historyserver/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_rag.invoke(\"How to configure flink on azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> To configure Flink on Azure, you can configure the Azure Blob storage key in the `flink-conf.yaml` file.\n",
       "> Citation:\n",
       ">  (1) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/filesystems/azure/ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
