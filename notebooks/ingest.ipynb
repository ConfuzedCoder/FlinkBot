{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from collections import deque\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, max_depth=2, max_pages=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_pages = max_pages\n",
    "        self.seen = set()\n",
    "        self.pages = []\n",
    "        self.queue = deque()\n",
    "\n",
    "    def add_to_queue(self, url, depth):\n",
    "        self.queue.append({'url': url, 'depth': depth})\n",
    "\n",
    "    def should_continue_crawling(self):\n",
    "        return self.queue and len(self.pages) < self.max_pages\n",
    "\n",
    "    def is_too_deep(self, depth):\n",
    "        return depth > self.max_depth\n",
    "\n",
    "    def is_already_seen(self, url):\n",
    "        return url in self.seen\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def parse_html(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for a in soup.find_all('a'):\n",
    "            a.attrs = {}\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "\n",
    "    def extract_urls(self, html, base_url):\n",
    "        urls = list()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        relative_urls = [a.get('href') for a in soup.find_all('a') if a.get('href')]\n",
    "        for relative_url in relative_urls:\n",
    "            if \"zh\" in relative_url.split(\"/\"):\n",
    "                continue\n",
    "            if \"#\" in relative_url:\n",
    "                continue\n",
    "            if  relative_url.startswith(\"//nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/\"):\n",
    "                urls.append(urljoin(\"https:\", relative_url))\n",
    "                # logging.error(f\" -- {relative_url}, {urls[-1]}\")\n",
    "            elif relative_url.startswith(\"https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/\"):\n",
    "                urls.append(relative_url)\n",
    "                # logging.error(f\" -- {relative_url}, {urls[-1]}\")\n",
    "            elif \"https://nightlies.apache.org/flink/flink-docs-release-1.16/\" not in relative_url:\n",
    "                #urls.append(urljoin(\"https://nightlies.apache.org/flink/flink-docs-release-1.16/\", relative_url))\n",
    "                # logging.error(f\"{base_url} -- {relative_url}\")\n",
    "                pass\n",
    "        return urls\n",
    "\n",
    "    def crawl(self, start_url):\n",
    "        self.add_to_queue(start_url, 0)\n",
    "\n",
    "        while self.should_continue_crawling():\n",
    "            current = self.queue.popleft()\n",
    "            url, depth = current['url'], current['depth']\n",
    "\n",
    "            if self.is_too_deep(depth) or self.is_already_seen(url):\n",
    "                continue\n",
    "\n",
    "            self.seen.add(url)\n",
    "            html = self.fetch_page(url)\n",
    "            # print(f\"Fetched {url}\")\n",
    "            if html:\n",
    "                print(f\"Parsing {url}\")\n",
    "                self.pages.append({'url': url, 'content': self.parse_html(html)})\n",
    "                new_urls = self.extract_urls(html, url)\n",
    "                for new_url in new_urls:\n",
    "                    self.add_to_queue(new_url, depth + 1)\n",
    "\n",
    "        return self.pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpointing_under_backpressure/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/savepoints/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/task_failure_recovery/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/metrics/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/batch/batch_shuffle/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/debugging_event_time/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/debugging_classloading/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/flame_graphs/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/application_profiling/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/monitoring/checkpoint_monitoring/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/monitoring/back_pressure/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/upgrading/\n",
      "Parsing https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/production_ready/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpointing_under_backpressure/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/savepoints/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/task_failure_recovery/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/metrics/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/batch/batch_shuffle/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/debugging_event_time/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/debugging_classloading/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/flame_graphs/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/application_profiling/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/monitoring/checkpoint_monitoring/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/monitoring/back_pressure/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/upgrading/\n",
      "https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/production_ready/\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler(max_depth=5, max_pages=2500)\n",
    "new_pages = crawler.crawl('https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/')\n",
    "for page in new_pages:\n",
    "    print(page['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_pages[0]['content'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list()\n",
    "for page in new_pages:\n",
    "    chunks = splitter.split_text(page['content'])\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={'url': page['url'], 'type':\"document\"}\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'document', 'Document'],\n",
       " 'kwargs': {'page_content': '<div style=\"font-weight:450;margin-bottom:0.5em\"><i class=\"fa fa-cogs title maindish\" aria-hidden=\"true\"></i>\\xa0\\xa0Operations</div> | Apache Flink v1.16.2 Try Flink â\\x96¾ First steps Fraud Detection with the DataStream API Real Time Reporting with the Table API Flink Operations Playground Learn Flink â\\x96¾ Overview Intro to the DataStream API Data Pipelines & ETL Streaming Analytics Event-driven Applications Fault Tolerance Concepts â\\x96¾ Overview Stateful Stream Processing Timely Stream Processing Flink Architecture Glossary Application Development â\\x96¾ Project Configuration â\\x96¾ Overview Using Maven Using Gradle Connectors and Formats Test Dependencies Advanced Configuration DataStream API â\\x96¾ Overview Execution Mode (Batch/Streaming) Event Time â\\x96¾ Generating Watermarks Builtin Watermark Generators State & Fault Tolerance â\\x96¾ Working with State The Broadcast State Pattern Checkpointing Queryable State State Backends Data Types & Serialization â\\x96¾ Overview State Schema Evolution Custom State',\n",
       "  'metadata': {'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/',\n",
       "   'type': 'document'},\n",
       "  'type': 'Document'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MILVUS_URL = os.environ['MILVUS_URL']\n",
    "MILVUS_KEY = os.environ['MILVUS_URL']\n",
    "DIMS = 1024\n",
    "EMBEDDING_MODEL = \"embed-english-v3.0\"\n",
    "COHERE_KEY=os.environ['COHERE_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_cohere.embeddings import CohereEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cohere\n",
    "\n",
    "# co = cohere.Client(COHERE_KEY)\n",
    "\n",
    "# response = co.tokenize(text=new_pages[0]['content'], model=EMBEDDING_MODEL)  # optional\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<div style=\"font-weight:450;margin-bottom:0.5em\"><i class=\"fa fa-cogs title maindish\" aria-hidden=\"true\"></i>\\xa0\\xa0Operations</div> | Apache Flink v1.16.2 Try Flink â\\x96¾ First steps Fraud Detection with the DataStream API Real Time Reporting with the Table API Flink Operations Playground Learn Flink â\\x96¾ Overview Intro to the DataStream API Data Pipelines & ETL Streaming Analytics Event-driven Applications Fault Tolerance Concepts â\\x96¾ Overview Stateful Stream Processing Timely Stream Processing Flink Architecture Glossary Application Development â\\x96¾ Project Configuration â\\x96¾ Overview Using Maven Using Gradle Connectors and Formats Test Dependencies Advanced Configuration DataStream API â\\x96¾ Overview Execution Mode (Batch/Streaming) Event Time â\\x96¾ Generating Watermarks Builtin Watermark Generators State & Fault Tolerance â\\x96¾ Working with State The Broadcast State Pattern Checkpointing Queryable State State Backends Data Types & Serialization â\\x96¾ Overview State Schema Evolution Custom State',\n",
       " 'State State Backends Data Types & Serialization â\\x96¾ Overview State Schema Evolution Custom State Serialization 3rd Party Serializers User-Defined Functions Operators â\\x96¾ Overview Windows Joining Process Function Async I/O Data Sources Side Outputs Handling Application Parameters Testing Experimental Features Scala API Extensions Java Lambda Expressions Managing Execution â\\x96¾ Execution Configuration Program Packaging Parallel Execution Table API & SQL â\\x96¾ Overview Concepts & Common API DataStream API Integration Streaming Concepts â\\x96¾ Overview Determinism in Continuous Queries Dynamic Tables Time Attributes Versioned Tables Temporal Table Function Data Types Time Zone Table API SQL â\\x96¾ SQL Getting Started Queries â\\x96¾ Overview Hints WITH clause SELECT & WHERE SELECT DISTINCT Windowing TVF Window Aggregation Group Aggregation Over Aggregation Joins Window JOIN Set Operations ORDER BY clause LIMIT clause Top-N Window Top-N Deduplication Window Deduplication Pattern Recognition CREATE',\n",
       " 'LIMIT clause Top-N Window Top-N Deduplication Window Deduplication Pattern Recognition CREATE Statements DROP Statements ALTER Statements INSERT Statement ANALYZE Statements DESCRIBE Statements EXPLAIN Statements USE Statements SHOW Statements LOAD Statements UNLOAD Statements SET Statements RESET Statements JAR Statements Functions â\\x96¾ Overview System (Built-in) Functions User-defined Functions Modules Catalogs SQL Client SQL Gateway â\\x96¾ Overview REST Endpoint HiveServer2 Endpoint Hive Compatibility â\\x96¾ Hive Dialect â\\x96¾ Overview Queries â\\x96¾ Overview Sort/Cluster/Distributed By Group By Join Set Operations Lateral View Clause Window Functions Sub-Queries CTE Transform Clause Table Sample CREATE Statements DROP Statements ALTER Statements INSERT Statements Load Data Statements SHOW Statements ADD Statements SET Statements HiveServer2 Endpoint Configuration Performance Tuning User-defined Sources & Sinks Python API â\\x96¾ Overview Installation Table API Tutorial DataStream API Tutorial',\n",
       " 'Sources & Sinks Python API â\\x96¾ Overview Installation Table API Tutorial DataStream API Tutorial Table API â\\x96¾ Intro to the Python Table API TableEnvironment Operations â\\x96¾ Overview Row-based Operations Data Types System (Built-in) Functions User Defined Functions â\\x96¾ Overview General User-defined Functions Vectorized User-defined Functions Conversions between PyFlink Table and Pandas DataFrame Conversions between Table and DataStream SQL Catalogs Metrics Connectors DataStream API â\\x96¾ Intro to the Python DataStream API Operators â\\x96¾ Overview Windows Process Function Data Types State Dependency Management Execution Mode Configuration Debugging Environment Variables FAQ DataSet API (Legacy) â\\x96¾ Overview Transformations Iterations Zipping Elements Hadoop MapReduce compatibility with Flink Local Execution Cluster Execution Batch Examples Libraries â\\x96¾ Event Processing (CEP) State Processor API Graphs â\\x96¾ Overview Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph',\n",
       " 'API Graphs â\\x96¾ Overview Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators Bipartite Graph Connectors â\\x96¾ DataSet Connectors â\\x96¾ Formats â\\x96¾ Avro Hadoop Microsoft Azure table DataStream Connectors â\\x96¾ Overview Fault Tolerance Guarantees Formats â\\x96¾ Overview Avro Azure Table storage CSV Hadoop JSON Parquet Text files Kafka Cassandra DynamoDB Elasticsearch Firehose Kinesis MongoDB Opensearch FileSystem RabbitMQ Google Cloud PubSub Hybrid Source Pulsar JDBC Table API Connectors â\\x96¾ Overview Formats â\\x96¾ Formats CSV JSON Avro Confluent Avro Protobuf Debezium Canal Maxwell Ogg Parquet Orc Raw Kafka Upsert Kafka DynamoDB Firehose Kinesis MongoDB JDBC Elasticsearch Opensearch FileSystem HBase DataGen Print BlackHole Hive â\\x96¾ Overview Hive Catalog Hive Read & Write Hive Functions Download Deployment â\\x96¾ Overview Resource Providers â\\x96¾ Standalone â\\x96¾ Overview Working Directory Docker Kubernetes Native Kubernetes YARN Configuration Memory Configuration â\\x96¾ Set',\n",
       " \"Directory Docker Kubernetes Native Kubernetes YARN Configuration Memory Configuration â\\x96¾ Set up Flink's Process Memory Set up TaskManager Memory Set up JobManager Memory Memory Tuning Guide Troubleshooting Migration Guide Network Buffer Tuning Command-Line Interface Elastic Scaling Fine-Grained Resource Management Speculative Execution File Systems â\\x96¾ Overview Common Configurations Amazon S3 Google Cloud Storage Aliyun OSS Azure Blob Storage Plugins High Availability â\\x96¾ Overview ZooKeeper HA Services Kubernetes HA Services Metric Reporters Security â\\x96¾ SSL Setup Kerberos REPLs â\\x96¾ Python REPL Advanced â\\x96¾ External Resources History Server Logging Operations â\\x96¾ State & Fault Tolerance â\\x96¾ Checkpoints Checkpointing under backpressure Savepoints Checkpoints vs. Savepoints State Backends Tuning Checkpoints and Large State Task Failure Recovery Metrics REST API Batch â\\x96¾ Batch Shuffle Debugging â\\x96¾ Debugging Windows & Event Time Debugging Classloading Flame Graphs Application Profiling\",\n",
       " 'â\\x96¾ Debugging Windows & Event Time Debugging Classloading Flame Graphs Application Profiling & Debugging Monitoring â\\x96¾ Monitoring Checkpointing Monitoring Back Pressure Upgrading Applications and Flink Versions Production Readiness Checklist Flink Development â\\x96¾ Importing Flink into an IDE Building Flink from Source Internals â\\x96¾ Jobs and Scheduling Task Lifecycle File Systems Project Homepage JavaDocs ScalaDocs PyDocs Pick Docs Version â\\x96¾ 1.16 (â\\x9c\\x93) v1.16 v1.15 All Versions ä¸\\xadæ\\x96\\x87ç\\x89\\x88 Operations On This Page On This Page']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.split_text(new_pages[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_pages[0]['content'].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_fn = CohereEmbeddings(model=EMBEDDING_MODEL, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dipak/FlinkBot/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'mps', \"trust_remote_code\": True}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.zilliz import Zilliz\n",
    "\n",
    "zilliz = Zilliz(\n",
    "    embedding_function = hf,\n",
    "    collection_name=\"Flink\",\n",
    "    connection_args={\"uri\": MILVUS_URL, \"token\": MILVUS_KEY},\n",
    "    auto_id=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n"
     ]
    }
   ],
   "source": [
    "for index, doc in zip(indexes[start:], documents[start:]):\n",
    "    print(index)\n",
    "    zilliz.add_documents([doc], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = zilliz.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='is that Flink might immediately build an incremental checkpoint on top of the restored one. Therefore,', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/savepoints/', 'type': 'document', 'pk': 450143955092273016}),\n",
       " Document(page_content='memory usage of RocksDB instance(s), Flink leverages a shared cache and write buffer manager among all instances in a single slot.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/', 'type': 'document', 'pk': 450143955092273094}),\n",
       " Document(page_content='This change does not affect the runtime implementation or characteristics of Flink’s state backend or checkpointing process; it is simply to communicate intent better.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/', 'type': 'document', 'pk': 450143955092273130}),\n",
       " Document(page_content='}\\n          }\\n        },\\n        \"checkpointed_size\" : {\\n          \"type\" : \"object\",\\n          \"id\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto\",\\n          \"properties\" : {\\n            \"avg\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"max\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"min\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"p50\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p90\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p95\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p99\" : {\\n              \"type\" : \"number\"\\n            },\\n            \"p999\" : {\\n              \"type\" : \"number\"\\n            }\\n          }\\n        },\\n        \"end_to_end_duration\" : {\\n          \"type\" : \"object\",\\n          \"$ref\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:StatsSummaryDto\"\\n        },', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273518}),\n",
       " Document(page_content='parallelism when re-scaling the program (via a savepoint). Flink’s internal bookkeeping tracks parallel state in the granularity of max-parallelism-many key groups .\\nFlink’s design strives to make it efficient to have a very high value for the maximum parallelism, even if\\nexecuting the program with a low parallelism. Compression # Flink offers optional compression (default: off) for all checkpoints and savepoints. Currently, compression always uses\\nthe snappy compression algorithm (version 1.1.10.x) but we are planning to support\\ncustom compression algorithms in the future. Compression works on the granularity of key-groups in keyed state, i.e.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273188}),\n",
       " Document(page_content='workaround is to store the watermark in the operator state. In that case, watermarks should be\\nstored per key group in a union state to support rescaling. Interplay with long-running record processing # Despite that unaligned checkpoints barriers are able to overtake all other records in the queue.\\nThe handling of this barrier still can be delayed if the current record takes a lot of time to be processed.\\nThis situation can occur when firing many timers all at once, for example in windowed operations.\\nSecond problematic scenario might occur when system is being blocked waiting for more than one\\nnetwork buffer availability when processing a single input record. Flink can not interrupt processing of\\na single input record, and unaligned checkpoints have to wait for the currently processed record to be\\nfully processed. This can cause problems in two scenarios. Either as a result of serialisation of a large', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpointing_under_backpressure/', 'type': 'document', 'pk': 450143955092272962}),\n",
       " Document(page_content='or in-flight record types for the existing operators have changed. Non-arbitrary job upgrade - restoring the snapshot is possible with updated operators if the job graph topology and in-flight record types remain unchanged. Flink minor version upgrade - restoring a snapshot taken with an older minor version of Flink (1.x â\\x86\\x92 1.y). Flink bug/patch version upgrade - restoring a snapshot taken with an older patch version of Flink (1.14.x â\\x86\\x92 1.14.y). Rescaling - restoring the snapshot with a different parallelism than was used during the snapshot creation. Back to top Want to contribute translation? Edit This Page On This Page Overview Capabilities and limitations', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/', 'type': 'document', 'pk': 450143955092273052}),\n",
       " Document(page_content='configuration ) { return this ; } } Capacity Planning # This section discusses how to decide how many resources should be used for a Flink job to run reliably.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273182}),\n",
       " Document(page_content='}\\n          }\\n        },\\n        \"restored\" : {\\n          \"type\" : \"object\",\\n          \"id\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:RestoredCheckpointStatistics\",\\n          \"properties\" : {\\n            \"external_path\" : {\\n              \"type\" : \"string\"\\n            },\\n            \"id\" : {\\n              \"type\" : \"integer\"\\n            },\\n            \"is_savepoint\" : {\\n              \"type\" : \"boolean\"\\n            },\\n            \"restore_timestamp\" : {\\n              \"type\" : \"integer\"\\n            }\\n          }\\n        },\\n        \"savepoint\" : {\\n          \"type\" : \"object\",\\n          \"$ref\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointStatistics:CompletedCheckpointStatistics\"\\n        }\\n      }\\n    },\\n    \"summary\" : {\\n      \"type\" : \"object\",\\n      \"id\" : \"urn:jsonschema:org:apache:flink:runtime:rest:messages:checkpoints:CheckpointingStatistics:Summary\",\\n      \"properties\" : {', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/', 'type': 'document', 'pk': 450143955092273496}),\n",
       " Document(page_content='checkpoints. â\\x9c\\x93 - Flink fully support this type of the snapshot x - Flink doesn’t support this type of the snapshot ! - While these operations currently work, Flink doesn’t officially guarantee support for them, so there is a certain level of risk associated with them Operation Canonical Savepoint Native Savepoint Aligned Checkpoint Unaligned Checkpoint State backend change â\\x9c\\x93 x x x State Processor API (writing) â\\x9c\\x93 x x x State Processor API (reading) â\\x9c\\x93 ! ! x Self-contained and relocatable â\\x9c\\x93 â\\x9c\\x93 x x Schema evolution â\\x9c\\x93 ! ! ! Arbitrary job upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 x Non-arbitrary job upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 Flink minor version upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 x Flink bug/patch version upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 Rescaling â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 State backend change - configuring a different State Backend than was used when taking the snapshot. State Processor API (writing) - the ability to create a new snapshot of this type via the State Processor API. State Processor API (reading) - the ability to read', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/', 'type': 'document', 'pk': 450143955092273046})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"WHat is flink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = CohereRerank(top_n=5, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='memory usage of RocksDB instance(s), Flink leverages a shared cache and write buffer manager among all instances in a single slot.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/', 'type': 'document', 'pk': 450143955092273094, 'relevance_score': 0.94966936}),\n",
       " Document(page_content='parallelism when re-scaling the program (via a savepoint). Flink’s internal bookkeeping tracks parallel state in the granularity of max-parallelism-many key groups .\\nFlink’s design strives to make it efficient to have a very high value for the maximum parallelism, even if\\nexecuting the program with a low parallelism. Compression # Flink offers optional compression (default: off) for all checkpoints and savepoints. Currently, compression always uses\\nthe snappy compression algorithm (version 1.1.10.x) but we are planning to support\\ncustom compression algorithms in the future. Compression works on the granularity of key-groups in keyed state, i.e.', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/large_state_tuning/', 'type': 'document', 'pk': 450143955092273188, 'relevance_score': 0.9165588}),\n",
       " Document(page_content='is that Flink might immediately build an incremental checkpoint on top of the restored one. Therefore,', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/savepoints/', 'type': 'document', 'pk': 450143955092273016, 'relevance_score': 0.8967949}),\n",
       " Document(page_content='or in-flight record types for the existing operators have changed. Non-arbitrary job upgrade - restoring the snapshot is possible with updated operators if the job graph topology and in-flight record types remain unchanged. Flink minor version upgrade - restoring a snapshot taken with an older minor version of Flink (1.x â\\x86\\x92 1.y). Flink bug/patch version upgrade - restoring a snapshot taken with an older patch version of Flink (1.14.x â\\x86\\x92 1.14.y). Rescaling - restoring the snapshot with a different parallelism than was used during the snapshot creation. Back to top Want to contribute translation? Edit This Page On This Page Overview Capabilities and limitations', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/', 'type': 'document', 'pk': 450143955092273052, 'relevance_score': 0.61138195}),\n",
       " Document(page_content='checkpoints. â\\x9c\\x93 - Flink fully support this type of the snapshot x - Flink doesn’t support this type of the snapshot ! - While these operations currently work, Flink doesn’t officially guarantee support for them, so there is a certain level of risk associated with them Operation Canonical Savepoint Native Savepoint Aligned Checkpoint Unaligned Checkpoint State backend change â\\x9c\\x93 x x x State Processor API (writing) â\\x9c\\x93 x x x State Processor API (reading) â\\x9c\\x93 ! ! x Self-contained and relocatable â\\x9c\\x93 â\\x9c\\x93 x x Schema evolution â\\x9c\\x93 ! ! ! Arbitrary job upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 x Non-arbitrary job upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 Flink minor version upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 x Flink bug/patch version upgrade â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 Rescaling â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 â\\x9c\\x93 State backend change - configuring a different State Backend than was used when taking the snapshot. State Processor API (writing) - the ability to create a new snapshot of this type via the State Processor API. State Processor API (reading) - the ability to read', metadata={'url': 'https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/checkpoints_vs_savepoints/', 'type': 'document', 'pk': 450143955092273046, 'relevance_score': 0.54728264})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_retriever.invoke(\"What is flink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_cohere import ChatCohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatCohere(model=\"command-r-plus\", temperature=0.0, cohere_api_key=COHERE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]):\n",
    "    \n",
    "    text = \"\"\n",
    "\n",
    "    for doc in docs:\n",
    "        xml_tag_start = f\"<{doc.metadata['url'].lower()}>\"\n",
    "        xml_tag_end = f\"</{doc.metadata['url'].lower()}>\"\n",
    "        content = doc.page_content\n",
    "        text += f\"{xml_tag_start}\\n{content}\\n{xml_tag_end}\\n\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"List down all the commands used in the flink documenatation along with explanation of the command\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I cannot find the necessary information in the provided context to answer your question.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_KEY = \"AIzaSyBVI2jAHepUzLwWoK6qwXCOYxD0NFzZIns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GEMINI_KEY, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.Keep the answer concise and to the point. Write down the citation at the end of the answer that you have taken reference from. The citation names are in form of urls, that are provided in the xml tags.\n",
    "Follow below mention format for citation\n",
    "Citation:\n",
    "        (1) Source URL 1\n",
    "        (2) Source URL 2\n",
    "Question: {question} \\nContext: {context} \\nAnswer\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_rag = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | example_prompt\n",
    "    | google_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +---------------------------------+           \n",
      "              | Parallel<context,question>Input |           \n",
      "              +---------------------------------+           \n",
      "                    ****                ****                \n",
      "                 ***                        ***             \n",
      "               **                              ***          \n",
      "+----------------------+                          **        \n",
      "| VectorStoreRetriever |                           *        \n",
      "+----------------------+                           *        \n",
      "            *                                      *        \n",
      "            *                                      *        \n",
      "            *                                      *        \n",
      "+---------------------+                     +-------------+ \n",
      "| Lambda(format_docs) |                     | Passthrough | \n",
      "+---------------------+                     +-------------+ \n",
      "                    ****                ****                \n",
      "                        ***          ***                    \n",
      "                           **      **                       \n",
      "              +----------------------------------+          \n",
      "              | Parallel<context,question>Output |          \n",
      "              +----------------------------------+          \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                      +----------------+                    \n",
      "                      | PromptTemplate |                    \n",
      "                      +----------------+                    \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                  +------------------------+                \n",
      "                  | ChatGoogleGenerativeAI |                \n",
      "                  +------------------------+                \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                      +-----------------+                   \n",
      "                      | StrOutputParser |                   \n",
      "                      +-----------------+                   \n",
      "                                *                           \n",
      "                                *                           \n",
      "                                *                           \n",
      "                   +-----------------------+                \n",
      "                   | StrOutputParserOutput |                \n",
      "                   +-----------------------+                \n"
     ]
    }
   ],
   "source": [
    "google_rag.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = google_rag.invoke(\"List down all the commands used in the flink documenatation along with explanation of the command.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> This document does not contain the answer to this question. It provides information about the structure of JSON objects related to Flink's REST API, not commands and their explanations. Citation: (1) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (2) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (3) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (4) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (5) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/state_backends/ (6) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (7) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (8) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ (9) https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/rest_api/ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Here are some of the commands mentioned in the provided Flink documentation:\n",
       "> \n",
       "> * `tar -xzf flink-*.tgz`: This command is used to extract the contents of a tar.gz file named \"flink-*.tgz\". \n",
       "> \n",
       "> * `cd flink-* && ls -l`: This command navigates to the directory named \"flink-*\" and lists the contents of the directory in long format. \n",
       "> \n",
       "> * `./bin/start-cluster.sh`: This command starts a local Flink cluster in the background. \n",
       "> \n",
       "> * `ps aux | grep flink`: This command is used to check the status of the Flink cluster. It lists all the processes running on the system and filters the output to show only the processes related to Flink. \n",
       "> \n",
       "> * `./bin/stop-cluster.sh`: This command stops the local Flink cluster and all its running components. \n",
       "> \n",
       "> * `./bin/flink run examples/streaming/WordCount.jar`: This command submits a Flink job to the running cluster. In this case, it deploys an example word count job located in the \"examples/streaming/\" directory. \n",
       "> \n",
       "> * `tail log/flink-*-taskexecutor-*.out`: This command displays the last few lines of the log file for the Flink task executor. This is useful for verifying the output of the Flink job.\n",
       "> \n",
       "> * `docker-compose build`: This command builds the Docker image for the Flink playground.\n",
       "> \n",
       "> * `mkdir -p /tmp/flink-checkpoints-directory`: This command creates a directory for Flink checkpoints.\n",
       "> \n",
       "> * `mkdir -p /tmp/flink-savepoints-directory`: This command creates a directory for Flink savepoints.\n",
       "> \n",
       "> * `docker-compose up -d`: This command starts the Flink playground in detached mode.\n",
       "> \n",
       "> * `docker-compose ps`: This command lists the running Docker containers for the Flink playground.\n",
       "> \n",
       "> * `docker-compose run --no-deps client flink list`: This command lists the running Flink jobs.\n",
       "> \n",
       "> * `curl localhost:8081/jobs`: This command retrieves information about running jobs from the Flink REST API.\n",
       "> \n",
       "> Citation:\n",
       ">  (1) <https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/try-flink/local_installation/>\n",
       ">  (2) <https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/try-flink/flink-operations-playground/> \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
