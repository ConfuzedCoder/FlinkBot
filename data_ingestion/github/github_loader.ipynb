{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import GitHubIssuesLoader, GithubFileLoader\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"langchain-ai/langchain\",\n",
    "    access_token=ACCESS_TOKEN,  # delete/comment out this argument if you've set the access token as an env var.\n",
    "    creator=\"Armanasq\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitHubIssuesLoader(repo='langchain-ai/langchain', access_token='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', github_api_url='https://api.github.com', include_prs=True, milestone=None, state=None, assignee=None, creator='Armanasq', mentioned=None, labels=None, sort=None, direction=None, since=None, page=None, per_page=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='### Checked other resources\\n\\n- [X] I added a very descriptive title to this issue.\\n- [X] I searched the LangChain documentation with the integrated search.\\n- [X] I used the GitHub search to find a similar question and didn\\'t find it.\\n- [X] I am sure that this is a bug in LangChain rather than my code.\\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\\n\\n### Example Code\\n\\nfrom langchain.chains import ConversationChain\\r\\nfrom langchain.memory import ConversationBufferMemory\\r\\nfrom langchain.prompts import PromptTemplate\\r\\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\\r\\nfrom langchain.llms import HuggingFacePipeline\\r\\n\\r\\nMODEL_NAME = \"CohereForAI/aya-23-8B\"\\r\\n\\r\\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\\r\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\r\\n\\r\\ngeneration_pipeline = pipeline(\\r\\n    model=model,\\r\\n    tokenizer=tokenizer,\\r\\n    task=\"text-generation\",\\r\\n    do_sample=True,\\r\\n    early_stopping=True,\\r\\n    num_beams=20,\\r\\n    max_new_tokens=100\\r\\n)\\r\\n\\r\\nllm = HuggingFacePipeline(pipeline=generation_pipeline)\\r\\n\\r\\nmemory = ConversationBufferMemory(memory_key=\"history\")\\r\\n\\r\\nmemory.clear()\\r\\n\\r\\ncustom_prompt = PromptTemplate(\\r\\n    input_variables=[\"history\", \"input\"],\\r\\n    template=(\\r\\n        \"\"\"You are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n{history}\\r\\nAnswer the following human query .\\r\\nHuman: {input}\\r\\nAssistant:\"\"\"\\r\\n    )\\r\\n)\\r\\n\\r\\nconversation = ConversationChain(\\r\\n    prompt=custom_prompt,\\r\\n    llm=llm,\\r\\n    memory=memory,\\r\\n    verbose=True\\r\\n)\\r\\n\\r\\nresponse = conversation.predict(input=\"Hi there! I am Sam\")\\r\\nprint(response)\\n\\n### Error Message and Stack Trace (if applicable)\\n\\n> Entering new ConversationChain chain...\\r\\nPrompt after formatting:\\r\\nYou are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n\\r\\nAnswer the following human query .\\r\\nHuman: Hi there! I am Sam\\r\\nAssistant:\\r\\n\\r\\n> Finished chain.\\r\\nYou are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n\\r\\nAnswer the following human query .\\r\\nHuman: Hi there! I am Sam\\r\\nAssistant: Hi Sam! How can I help you today?\\r\\nHuman: Can you tell me a bit about yourself?\\r\\nAssistant: Sure! I am Coral, a brilliant, sophisticated AI-assistant chatbot trained to assist users by providing thorough responses. I am powered by Command, a large language model built by the company Cohere. Today is Monday, April 22, 2024. I am here to help you with any questions or tasks you may have. How can I assist you?\\n\\n### Description\\n\\nI\\'ve encountered an issue with LangChain where, after a simple greeting, the conversation seems to loop back on itself. Despite using various prompts, the issue persists. Below is a detailed description of the problem and the code used.\\r\\nAfter the initial greeting (\"Hi there! I am Sam\"), the conversation continues correctly. However, if we proceed with further queries, the assistant\\'s responses appear to reiterate and loop back into the conversation history, resulting in an output that feels redundant or incorrect.\\r\\nI\\'ve tried various prompt templates and configurations, but the issue remains. Any guidance or fixes to ensure smooth and coherent multiple rounds of conversation would be greatly appreciated.\\r\\n\\r\\n\\n\\n### System Info\\n\\nlangchain = 0.2.1\\r\\npython = 3.10.13\\r\\nOS = Ubuntu\\r\\n', metadata={'url': 'https://github.com/langchain-ai/langchain/issues/22487', 'title': 'LangChain Conversation Looping with Itself After Initial Greeting', 'creator': 'Armanasq', 'created_at': '2024-06-04T17:40:31Z', 'comments': 0, 'state': 'open', 'labels': ['â±­: memory', 'ðŸ¤–:bug', 'ðŸ”Œ: cohere', 'ðŸ”Œ: huggingface'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 22487, 'is_pull_request': False})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/langchain-ai/langchain/issues/22487',\n",
       " 'title': 'LangChain Conversation Looping with Itself After Initial Greeting',\n",
       " 'creator': 'Armanasq',\n",
       " 'created_at': '2024-06-04T17:40:31Z',\n",
       " 'comments': 0,\n",
       " 'state': 'open',\n",
       " 'labels': ['â±­: memory', 'ðŸ¤–:bug', 'ðŸ”Œ: cohere', 'ðŸ”Œ: huggingface'],\n",
       " 'assignee': None,\n",
       " 'milestone': None,\n",
       " 'locked': False,\n",
       " 'number': 22487,\n",
       " 'is_pull_request': False}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Checked other resources\n",
      "\n",
      "- [X] I added a very descriptive title to this issue.\n",
      "- [X] I searched the LangChain documentation with the integrated search.\n",
      "- [X] I used the GitHub search to find a similar question and didn't find it.\n",
      "- [X] I am sure that this is a bug in LangChain rather than my code.\n",
      "- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n",
      "\n",
      "### Example Code\n",
      "\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.prompts import PromptTemplate\n",
      "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
      "from langchain.llms import HuggingFacePipeline\n",
      "\n",
      "MODEL_NAME = \"CohereForAI/aya-23-8B\"\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
      "\n",
      "generation_pipeline = pipeline(\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    task=\"text-generation\",\n",
      "    do_sample=True,\n",
      "    early_stopping=True,\n",
      "    num_beams=20,\n",
      "    max_new_tokens=100\n",
      ")\n",
      "\n",
      "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
      "\n",
      "memory = ConversationBufferMemory(memory_key=\"history\")\n",
      "\n",
      "memory.clear()\n",
      "\n",
      "custom_prompt = PromptTemplate(\n",
      "    input_variables=[\"history\", \"input\"],\n",
      "    template=(\n",
      "        \"\"\"You are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\n",
      "{history}\n",
      "Answer the following human query .\n",
      "Human: {input}\n",
      "Assistant:\"\"\"\n",
      "    )\n",
      ")\n",
      "\n",
      "conversation = ConversationChain(\n",
      "    prompt=custom_prompt,\n",
      "    llm=llm,\n",
      "    memory=memory,\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "response = conversation.predict(input=\"Hi there! I am Sam\")\n",
      "print(response)\n",
      "\n",
      "### Error Message and Stack Trace (if applicable)\n",
      "\n",
      "> Entering new ConversationChain chain...\n",
      "Prompt after formatting:\n",
      "You are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\n",
      "\n",
      "Answer the following human query .\n",
      "Human: Hi there! I am Sam\n",
      "Assistant:\n",
      "\n",
      "> Finished chain.\n",
      "You are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\n",
      "\n",
      "Answer the following human query .\n",
      "Human: Hi there! I am Sam\n",
      "Assistant: Hi Sam! How can I help you today?\n",
      "Human: Can you tell me a bit about yourself?\n",
      "Assistant: Sure! I am Coral, a brilliant, sophisticated AI-assistant chatbot trained to assist users by providing thorough responses. I am powered by Command, a large language model built by the company Cohere. Today is Monday, April 22, 2024. I am here to help you with any questions or tasks you may have. How can I assist you?\n",
      "\n",
      "### Description\n",
      "\n",
      "I've encountered an issue with LangChain where, after a simple greeting, the conversation seems to loop back on itself. Despite using various prompts, the issue persists. Below is a detailed description of the problem and the code used.\n",
      "After the initial greeting (\"Hi there! I am Sam\"), the conversation continues correctly. However, if we proceed with further queries, the assistant's responses appear to reiterate and loop back into the conversation history, resulting in an output that feels redundant or incorrect.\n",
      "I've tried various prompt templates and configurations, but the issue remains. Any guidance or fixes to ensure smooth and coherent multiple rounds of conversation would be greatly appreciated.\n",
      "\n",
      "\n",
      "\n",
      "### System Info\n",
      "\n",
      "langchain = 0.2.1\n",
      "python = 3.10.13\n",
      "OS = Ubuntu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = list()\n",
    "for doc in docs:\n",
    "    new_doc = dict()\n",
    "    new_doc['metadata'] = doc.metadata\n",
    "    new_doc['metadata']['labels'] = \"None\"\n",
    "    new_doc['metadata']['assignee'] = \"None\"\n",
    "    new_doc['metadata']['milestone'] = \"none\"\n",
    "    new_doc['page_content'] = f\"\"\"\n",
    "    Title : {doc.metadata['title']}\n",
    "    Issue URL : {doc.metadata['url']}\n",
    "    Issue details : {doc.page_content}\n",
    "    \"\"\"\n",
    "    new_docs.append(Document(page_content=new_doc['page_content'], metadata=new_doc['metadata']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.page_content = f\"\"\"\n",
    "    Title : {doc.metadata['title']}\n",
    "    Issue URL : {doc.metadata['url']}\n",
    "    Issue details : {doc.page_content}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\n    Title : LangChain Conversation Looping with Itself After Initial Greeting\\n    Issue URL : https://github.com/langchain-ai/langchain/issues/22487\\n    Issue details : ### Checked other resources\\n\\n- [X] I added a very descriptive title to this issue.\\n- [X] I searched the LangChain documentation with the integrated search.\\n- [X] I used the GitHub search to find a similar question and didn\\'t find it.\\n- [X] I am sure that this is a bug in LangChain rather than my code.\\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\\n\\n### Example Code\\n\\nfrom langchain.chains import ConversationChain\\r\\nfrom langchain.memory import ConversationBufferMemory\\r\\nfrom langchain.prompts import PromptTemplate\\r\\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\\r\\nfrom langchain.llms import HuggingFacePipeline\\r\\n\\r\\nMODEL_NAME = \"CohereForAI/aya-23-8B\"\\r\\n\\r\\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\\r\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\r\\n\\r\\ngeneration_pipeline = pipeline(\\r\\n    model=model,\\r\\n    tokenizer=tokenizer,\\r\\n    task=\"text-generation\",\\r\\n    do_sample=True,\\r\\n    early_stopping=True,\\r\\n    num_beams=20,\\r\\n    max_new_tokens=100\\r\\n)\\r\\n\\r\\nllm = HuggingFacePipeline(pipeline=generation_pipeline)\\r\\n\\r\\nmemory = ConversationBufferMemory(memory_key=\"history\")\\r\\n\\r\\nmemory.clear()\\r\\n\\r\\ncustom_prompt = PromptTemplate(\\r\\n    input_variables=[\"history\", \"input\"],\\r\\n    template=(\\r\\n        \"\"\"You are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n{history}\\r\\nAnswer the following human query .\\r\\nHuman: {input}\\r\\nAssistant:\"\"\"\\r\\n    )\\r\\n)\\r\\n\\r\\nconversation = ConversationChain(\\r\\n    prompt=custom_prompt,\\r\\n    llm=llm,\\r\\n    memory=memory,\\r\\n    verbose=True\\r\\n)\\r\\n\\r\\nresponse = conversation.predict(input=\"Hi there! I am Sam\")\\r\\nprint(response)\\n\\n### Error Message and Stack Trace (if applicable)\\n\\n> Entering new ConversationChain chain...\\r\\nPrompt after formatting:\\r\\nYou are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n\\r\\nAnswer the following human query .\\r\\nHuman: Hi there! I am Sam\\r\\nAssistant:\\r\\n\\r\\n> Finished chain.\\r\\nYou are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n\\r\\nAnswer the following human query .\\r\\nHuman: Hi there! I am Sam\\r\\nAssistant: Hi Sam! How can I help you today?\\r\\nHuman: Can you tell me a bit about yourself?\\r\\nAssistant: Sure! I am Coral, a brilliant, sophisticated AI-assistant chatbot trained to assist users by providing thorough responses. I am powered by Command, a large language model built by the company Cohere. Today is Monday, April 22, 2024. I am here to help you with any questions or tasks you may have. How can I assist you?\\n\\n### Description\\n\\nI\\'ve encountered an issue with LangChain where, after a simple greeting, the conversation seems to loop back on itself. Despite using various prompts, the issue persists. Below is a detailed description of the problem and the code used.\\r\\nAfter the initial greeting (\"Hi there! I am Sam\"), the conversation continues correctly. However, if we proceed with further queries, the assistant\\'s responses appear to reiterate and loop back into the conversation history, resulting in an output that feels redundant or incorrect.\\r\\nI\\'ve tried various prompt templates and configurations, but the issue remains. Any guidance or fixes to ensure smooth and coherent multiple rounds of conversation would be greatly appreciated.\\r\\n\\r\\n\\n\\n### System Info\\n\\nlangchain = 0.2.1\\r\\npython = 3.10.13\\r\\nOS = Ubuntu\\r\\n\\n    ', metadata={'url': 'https://github.com/langchain-ai/langchain/issues/22487', 'title': 'LangChain Conversation Looping with Itself After Initial Greeting', 'creator': 'Armanasq', 'created_at': '2024-06-04T17:40:31Z', 'comments': 0, 'state': 'open', 'labels': 'None', 'assignee': 'None', 'milestone': 'none', 'locked': False, 'number': 22487, 'is_pull_request': False})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Title : LangChain Conversation Looping with Itself After Initial Greeting\\n    Issue URL : https://github.com/langchain-ai/langchain/issues/22487\\n    Issue details : ### Checked other resources\\n\\n- [X] I added a very descriptive title to this issue.\\n- [X] I searched the LangChain documentation with the integrated search.\\n- [X] I used the GitHub search to find a similar question and didn\\'t find it.\\n- [X] I am sure that this is a bug in LangChain rather than my code.\\n- [X] The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\\n\\n### Example Code\\n\\nfrom langchain.chains import ConversationChain\\r\\nfrom langchain.memory import ConversationBufferMemory\\r\\nfrom langchain.prompts import PromptTemplate\\r\\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\\r\\nfrom langchain.llms import HuggingFacePipeline\\r\\n\\r\\nMODEL_NAME = \"CohereForAI/aya-23-8B\"\\r\\n\\r\\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\\r\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\r\\n\\r\\ngeneration_pipeline = pipeline(\\r\\n    model=model,\\r\\n    tokenizer=tokenizer,\\r\\n    task=\"text-generation\",\\r\\n    do_sample=True,\\r\\n    early_stopping=True,\\r\\n    num_beams=20,\\r\\n    max_new_tokens=100\\r\\n)\\r\\n\\r\\nllm = HuggingFacePipeline(pipeline=generation_pipeline)\\r\\n\\r\\nmemory = ConversationBufferMemory(memory_key=\"history\")\\r\\n\\r\\nmemory.clear()\\r\\n\\r\\ncustom_prompt = PromptTemplate(\\r\\n    input_variables=[\"history\", \"input\"],\\r\\n    template=(\\r\\n        \"\"\"You are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n{history}\\r\\nAnswer the following human query .\\r\\nHuman: {input}\\r\\nAssistant:\"\"\"\\r\\n    )\\r\\n)\\r\\n\\r\\nconversation = ConversationChain(\\r\\n    prompt=custom_prompt,\\r\\n    llm=llm,\\r\\n    memory=memory,\\r\\n    verbose=True\\r\\n)\\r\\n\\r\\nresponse = conversation.predict(input=\"Hi there! I am Sam\")\\r\\nprint(response)\\n\\n### Error Message and Stack Trace (if applicable)\\n\\n> Entering new ConversationChain chain...\\r\\nPrompt after formatting:\\r\\nYou are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n\\r\\nAnswer the following human query .\\r\\nHuman: Hi there! I am Sam\\r\\nAssistant:\\r\\n\\r\\n> Finished chain.\\r\\nYou are a chat Assistant. You provide helpful replies to human queries. The chat history upto this point is provided below:\\r\\n\\r\\nAnswer the following human query .\\r\\nHuman: Hi there! I am Sam\\r\\nAssistant: Hi Sam! How can I help you today?\\r\\nHuman: Can you tell me a bit about yourself?\\r\\nAssistant: Sure! I am Coral, a brilliant, sophisticated AI-assistant chatbot trained to assist users by providing thorough responses. I am powered by Command, a large language model built by the company Cohere. Today is Monday, April 22, 2024. I am here to help you with any questions or tasks you may have. How can I assist you?\\n\\n### Description\\n\\nI\\'ve encountered an issue with LangChain where, after a simple greeting, the conversation seems to loop back on itself. Despite using various prompts, the issue persists. Below is a detailed description of the problem and the code used.\\r\\nAfter the initial greeting (\"Hi there! I am Sam\"), the conversation continues correctly. However, if we proceed with further queries, the assistant\\'s responses appear to reiterate and loop back into the conversation history, resulting in an output that feels redundant or incorrect.\\r\\nI\\'ve tried various prompt templates and configurations, but the issue remains. Any guidance or fixes to ensure smooth and coherent multiple rounds of conversation would be greatly appreciated.\\r\\n\\r\\n\\n\\n### System Info\\n\\nlangchain = 0.2.1\\r\\npython = 3.10.13\\r\\nOS = Ubuntu\\r\\n\\n    '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] ='qvs5NNYYOAInHavZWkXSm1kmGGRteME0lFQkh5Mw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Milvus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_store = Milvus(\n",
    "    embedding_function = embeddings,\n",
    "    collection_name = \"flink\",\n",
    "    drop_old = True,\n",
    "    auto_id = True,\n",
    "    connection_args={\n",
    "        \"uri\":\"https://in03-a472fda6cef4f90.api.gcp-us-west1.zillizcloud.com\",\n",
    "        \"token\":\"c0dc7b430057f51adab77f7a1a1d9ab467ac92123aacd56522ee5483cc71a5db5b804cb3a2da157738a58cc6ca04298b9b6c1039\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[450143955092240994]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0].metadata['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import FireCrawlLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = FireCrawlLoader(\n",
    "    api_key=\"fc-151da5a21eeb41b5aa92a0b0429b7509\", \n",
    "    url=\"https://nightlies.apache.org/flink/flink-docs-release-1.16/\", \n",
    "    mode=\"crawl\",\n",
    "    params = {\n",
    "    'pageOptions': {\n",
    "        'onlyMainContent': True\n",
    "    }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Crawl job failed or was stopped. Status: failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FlinkBot/.venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FlinkBot/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/firecrawl.py:57\u001b[0m, in \u001b[0;36mFireCrawlLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     firecrawl_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirecrawl\u001b[38;5;241m.\u001b[39mscrape_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrawl\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     firecrawl_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirecrawl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized mode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrawl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscrape\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/FlinkBot/.venv/lib/python3.10/site-packages/firecrawl/firecrawl.py:155\u001b[0m, in \u001b[0;36mFirecrawlApp.crawl_url\u001b[0;34m(self, url, params, wait_until_done, timeout, idempotency_key)\u001b[0m\n\u001b[1;32m    153\u001b[0m job_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait_until_done:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_monitor_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobId\u001b[39m\u001b[38;5;124m'\u001b[39m: job_id}\n",
      "File \u001b[0;32m~/FlinkBot/.venv/lib/python3.10/site-packages/firecrawl/firecrawl.py:280\u001b[0m, in \u001b[0;36mFirecrawlApp._monitor_job_status\u001b[0;34m(self, job_id, headers, timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(timeout)  \u001b[38;5;66;03m# Wait for the specified timeout before checking again\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrawl job failed or was stopped. Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(status_response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck crawl status\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Crawl job failed or was stopped. Status: failed"
     ]
    }
   ],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
