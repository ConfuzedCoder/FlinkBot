Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Issue split),Outward issue link (Issue split),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (Testing),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Update Kafka version to 3.2.3,FLINK-29513,13484650,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,05/Oct/22 19:53,10/Oct/22 19:37,04/Jun/24 20:41,10/Oct/22 19:37,,,,,,,,1.16.0,1.17.0,,,Connectors / Kafka,,,,0,pull-request-available,,,,Kafka 3.2.3 contains certain security fixes (see https://downloads.apache.org/kafka/3.2.3/RELEASE_NOTES.html). We should upgrade the dependency in Flink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 19:37:57 UTC 2022,,,,,,,,,,"0|z1936g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 12:17;martijnvisser;Fixed in master: a2230639859342a19e371a596f079efe28cbeedd;;;","06/Oct/22 19:57;martijnvisser;Re-opened to patch also 1.16;;;","10/Oct/22 19:37;martijnvisser;Also fixed in release-1.16 via 90fba507a824eab2075aa42b0a82cc0b69dc44de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align SubtaskCommittableManager checkpointId with CheckpointCommittableManagerImpl checkpointId during recovery,FLINK-29512,13484635,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fpaul,fpaul,fpaul,05/Oct/22 15:49,20/Oct/22 02:36,04/Jun/24 20:41,14/Oct/22 13:40,1.15.1,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,Connectors / Common,,,,0,pull-request-available,,,,"Similar to the issue described in https://issues.apache.org/jira/browse/FLINK-29509 during the recovery of committables, the subtaskCommittables checkpointId is set to always 1 [https://github.com/apache/flink/blob/9152af41c2d401e5eacddee1bb10d1b6bea6c61a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CommittableCollectorSerializer.java#L193] while the holding CheckpointCommittableManager is initialized with the checkpointId that is written into state [https://github.com/apache/flink/blob/9152af41c2d401e5eacddee1bb10d1b6bea6c61a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CommittableCollectorSerializer.java#L155 .|https://github.com/apache/flink/blob/9152af41c2d401e5eacddee1bb10d1b6bea6c61a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CommittableCollectorSerializer.java#L155.]

 

This leads to that during a recovery, the post-commit topology will receive a committable summary with the recovered checkpoint id and multiple `CommittableWithLinage`s with the reset checkpointId causing orphaned `CommittableWithLinages` without a `CommittableSummary` failing the job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 13:39:41 UTC 2022,,,,,,,,,,"0|z19334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 13:39;fpaul;Merged into: 

 

master: d03c334b7ac954e70877a5bdb7b50cf54b50624c

release-1.16: 1bef1aeb6abd4edda0e178a6a5da7f6dc6c3b074

release-1.15: 126ae4df9ef8bab98a53433ef39c698cf8f04c60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort properties/schemas in OpenAPI spec,FLINK-29511,13484634,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Oct/22 15:41,06/Oct/22 16:02,04/Jun/24 20:41,06/Oct/22 16:02,,,,,,,,1.17.0,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,,"The properties/schema order is currently based on whatever order they were looked up, which varies as the spec is being extended.
Sort them by name to prevent this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 06 16:02:15 UTC 2022,,,,,,,,,,"0|z1932w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 16:02;chesnay;master:
c8fb99beeccd81f7b2ebfdc7ce178fcc04d100a1
fa8b1232d8eaee0679b6fe160f66db73cf5a4af2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add NoticeFileChecker tests,FLINK-29510,13484613,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Oct/22 13:14,12/Oct/22 09:51,04/Jun/24 20:41,07/Oct/22 09:21,,,,,,,,1.16.0,,,,Build System,Tests,,,0,pull-request-available,,,,The NoticeFileChecker is too important to not be covered by tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 09:21:46 UTC 2022,,,,,,,,,,"0|z192y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 09:21;chesnay;master:
40f0540a3fa7892e18d897eb35cbdbd091d4fe19..42b00efdc7c78aa3f01a6e0f6f3cf0fdfd093582
1.16: 
a76d8b8b5e6d39f08a0eb4f9e0d809d04f160645..8318717a3eb53f47bbf3222093224618281c083e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set correct subtaskId during recovery of committables,FLINK-29509,13484603,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,KristoffSC,fpaul,fpaul,05/Oct/22 12:29,20/Oct/22 02:36,04/Jun/24 20:41,13/Oct/22 06:58,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,Connectors / Common,,,,0,pull-request-available,,,,"When we recover the `CheckpointCommittableManager` we ignore the subtaskId it is recovered on. [https://github.com/apache/flink/blob/d191bda7e63a2c12416cba56090e5cd75426079b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/sink/committables/CheckpointCommittableManagerImpl.java#L58]

This becomes a problem when a sink uses a post-commit topology because multiple committer operators might forward committable summaries coming from the same subtaskId.

 

It should be possible to use the subtaskId already present in the `CommittableCollector` when creating the `CheckpointCommittableManager`s.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29459,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 06:58:29 UTC 2022,,,,,,,,,,"0|z192w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 13:21;KristoffSC;Hi,
I would like to work on this thicket.

Can someone assign it to me? It seems I can't do that.;;;","07/Oct/22 06:20;KristoffSC;PR ready for review :)
[https://github.com/apache/flink/pull/20979];;;","13/Oct/22 06:58;fpaul;Merge into

master: 7a509c46e45b9a91f2b7d01f13afcdef266b1faf

release-1.16: d51389dc33af21038c982b733b86af8bbb736d19

release-1.15: 6b4882791dd0fd1b0df952ed7712ae7bd68adf36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some NOTICE files are not checked for correctness,FLINK-29508,13484592,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Oct/22 11:58,12/Oct/22 09:50,04/Jun/24 20:41,12/Oct/22 09:50,1.16.0,,,,,,,1.16.0,,,,Build System,,,,0,pull-request-available,,,,"We have 3 modules that are not being deployed (and thus auto-excluded since FLINK-29301) which are still relevant for production though.

We should amend the checker to take into account whether the non-deployed module is bundled by another deployed module.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29301,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 09:50:48 UTC 2022,,,,,,,,,,"0|z192tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 09:50;chesnay;master: 01d7b37f06b3d6da178c214335d9801db76f6383
1.16: d269e22b30c0758e905ebc3262c4463949df54e6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make e2e tests independent of the current directory,FLINK-29507,13484558,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,05/Oct/22 08:12,09/Oct/22 15:18,04/Jun/24 20:41,09/Oct/22 15:18,kubernetes-operator-1.3.0,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 09 15:18:00 UTC 2022,,,,,,,,,,"0|z192m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/22 15:18;gyfora;merged to main 39a275f4f4c306061d37991644f81e4625737e93;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetInputFormatFactory fails to create format on Flink 1.14,FLINK-29506,13484545,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,05/Oct/22 07:19,08/Oct/22 02:38,04/Jun/24 20:41,08/Oct/22 02:38,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"The current way to instantiate format has issues. See
[https://github.com/apache/flink-table-store/blob/master/flink-table-store-format/src/main/java/org/apache/flink/table/store/format/parquet/ParquetInputFormatFactory.java#L36]

ParquetColumnarRowInputFormat#createPartitionedFormat only differs in arguments for Flink 1.14 and Flink 1.15. It'll direct throw IllegalArgumentException when using Flink1.14.

!image-2022-10-05-15-19-25-641.png|width=617,height=375!

 

!image-2022-10-05-15-20-19-422.png|width=617,height=390!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27207,FLINK-29149,,"05/Oct/22 07:19;qingyue;image-2022-10-05-15-19-25-641.png;https://issues.apache.org/jira/secure/attachment/13050089/image-2022-10-05-15-19-25-641.png","05/Oct/22 07:20;qingyue;image-2022-10-05-15-20-19-422.png;https://issues.apache.org/jira/secure/attachment/13050090/image-2022-10-05-15-20-19-422.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 02:38:55 UTC 2022,,,,,,,,,,"0|z192j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 02:38;lzljs3620320;master: 09827774e2f435de3133ced33c61dcf1e6ceae0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[OpenAPI] Need `operationId` on certain operations,FLINK-29505,13484526,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tigerinus,tigerinus,05/Oct/22 01:32,05/Oct/22 08:44,04/Jun/24 20:41,05/Oct/22 08:44,1.15.2,,,,,,,,,,,Runtime / REST,,,,0,openapi,,,,"Install nodejs and run

$ npx --yes autorest --input-file=https://nightlies.apache.org/flink/flink-docs-release-1.15/generated/rest_v1_dispatcher.yml --typescript --output-folder=. 

 

It returns with error

!image-2022-10-04-21-32-28-903.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27819,,,,,"05/Oct/22 01:32;tigerinus;image-2022-10-04-21-32-28-903.png;https://issues.apache.org/jira/secure/attachment/13050079/image-2022-10-04-21-32-28-903.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,yaml,Wed Oct 05 08:44:43 UTC 2022,,,,,,,,,,"0|z192ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 08:44;chesnay;Operation IDs will be available in 1.16.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jar upload spec should define a schema,FLINK-29504,13484525,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,tigerinus,tigerinus,05/Oct/22 01:28,20/Oct/22 02:34,04/Jun/24 20:41,11/Oct/22 11:08,1.15.2,,,,,,,1.15.3,1.16.0,1.17.0,,Runtime / REST,,,,0,openapi,pull-request-available,,,"Install nodejs and run

{{$ npx --yes @openapitools/openapi-generator-cli generate -i [https://nightlies.apache.org/flink/flink-docs-release-1.15/generated/rest_v1_dispatcher.yml] -g typescript-axios -o .}}

 

Then it outputs error 

 

{{Caused by: java.lang.RuntimeException: Request body cannot be null. Possible cause: missing schema in body parameter (OAS v2): class RequestBody {}}
{{    description: null}}
{{    content: class Content {}}
{{        {application/x-java-archive=class MediaType {}}
{{            schema: null}}
{{            examples: null}}
{{            example: null}}
{{            encoding: null}}
{\{        }}}}
{\{    }}}
{{    required: true}}
{{}}}

 

This is because in the YAML:

{{}}{{ requestBody:}}

{{  content:}}

{{{}    application/x-java-archive: {{}}}}

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,yaml,Tue Oct 11 11:08:03 UTC 2022,,,,,,,,,,"0|z192eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 08:54;chesnay;Should be straight forward to fix by adding this in {{OpenApiSpecGenerator#injectFileUploadRequest}}:

{code:java}
schema:
        type: string
        format: binary
{code}



;;;","10/Oct/22 16:24;tigerinus;Verified the issue no longer repros, with the yaml file from https://github.com/apache/flink/pull/20959/files.

 

I could build a JS package out of generated TypeScript code successfully.

 

Thanks.;;;","11/Oct/22 11:08;chesnay;master: e188cb2d66cc68e5eb822c5cca4fe0ee85173157
1.16: 141eeffb6b2c05d7c139c94273c2e5c468b44ec9
1.15: 383d451595f364196010351205f2ee8fd7f073c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add backpressureLevel field without hyphens,FLINK-29503,13484524,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,tigerinus,tigerinus,05/Oct/22 01:23,20/Oct/22 02:34,04/Jun/24 20:41,11/Oct/22 11:08,1.15.2,,,,,,,1.15.3,1.16.0,1.17.0,,Runtime / REST,,,,0,openapi,pull-request-available,,,"Install nodejs and run

{{$ npx --yes --package openapi-typescript-codegen openapi --input [https://nightlies.apache.org/flink/flink-docs-release-1.15/generated/rest_v1_dispatcher.yml] --output .}}

{{$ npx --package typescript tsc }}

The only thing it complains about is:

{{{}src/models/JobVertexBackPressureInfo.ts:21:17 - error TS1003: Identifier expected.{}}}{{{}21     export enum 'backpressure-level' {{}}}

This is because for TypeScript, enum name should not have a hyphen in it.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,yaml,Tue Oct 11 11:08:10 UTC 2022,,,,,,,,,,"0|z192eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 09:11;chesnay;This is less of an issue with a spec but rather the REST API itself. We'll need to duplicate the field.;;;","10/Oct/22 16:21;tigerinus;Verified the issue no longer repros, with yaml file from https://github.com/apache/flink/pull/20962/files.

 

I could build a JS package out of generated TypeScript code successfully.

 

Thanks.;;;","11/Oct/22 11:08;chesnay;master: a743812edd7e19cf5b43d9efd1222471def90d18
1.16: fc708c2fe212826a25fa3151b5b8571e3993f15b
1.15: 5ad5618821f4b004f5de95004280981f1de6d17a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the Hadoop implementation for filesystems to 3.3.4,FLINK-29502,13484495,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,04/Oct/22 19:43,13/Oct/22 11:07,04/Jun/24 20:41,13/Oct/22 11:07,,,,,,,,1.17.0,,,,FileSystems,,,,0,pull-request-available,,,,Flink currently uses Hadoop version 3.3.2 for the Flink filesystem implementations. Upgrading this to version 3.3.4 will resolve some CVEs like https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25168 (which Flink is not affected by),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 11:07:12 UTC 2022,,,,,,,,,,"0|z19288:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 11:07;chesnay;master: 62e48fc40bb510c5767b87054c870515b6680162;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow overriding JobVertex parallelisms during job submission,FLINK-29501,13484451,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,04/Oct/22 14:13,23/Jan/23 16:02,04/Jun/24 20:41,23/Nov/22 15:58,,,,,,,,1.17.0,,,,Runtime / Configuration,Runtime / REST,,,0,pull-request-available,,,,"It is a common scenario that users want to make changes to the parallelisms in the JobGraph. For example, because they discover that the job needs more or less resources. There is the option to do this globally via the job parallelism. However, for fine-tuned jobs jobs with potentially many branches, tuning on the job vertex level is required.

This is to propose a way such that users can apply a mapping \{{jobVertexId => parallelism}} before the job is submitted without having to modify the JobGraph manually.

One way to achieving this would be to add an optional map field to the Rest API jobs endpoint. However, in deployment modes like the application mode, this might not make sense because users do not have control the rest endpoint.

Similarly to how other job parameters are passed in the application mode, we propose to add the overrides as a configuration parameter.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30213,,,,,,FLINK-30773,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 15:12:55 UTC 2022,,,,,,,,,,"0|z191y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/22 14:54;chesnay;Why can't this be handled within the application with a main method argument?;;;","04/Oct/22 16:17;mxm;Good question. How would that work? The user application doesn't have access to the JobGraph during the normal execution flow, at least not with significant workarounds.

This feature is intended to be used for autoscaling Flink jobs, where we observe utilization via task metrics and then redeploy the Flink JobGraph with updated parallelisms. Not something that the user should have to with. It is intended to be used together with the Flink operator.;;;","05/Oct/22 10:21;chesnay;??The user application doesn't have access to the JobGraph during the normal execution flow??

It would set the parallelism like user applications usually do, in the main() method where they define their workflow. You would of course have to parameterize the parallelism of every single operator (and expose that _somehow_, presumably by a standardized <uid>.parallelism argument), but that may not be such a bad idea anyway? (could force certain operations to run with a specific parallelism)
Yes, this approach has problems :)

??redeploy the Flink JobGraph??

I don't really follow. Will you suspend the job, and restart it from another JM with a different configuration?
Or is this something meant to be specific to the YARN per-job mode (which loads the jobgraph from a file)?


On a related note, there were some ideas about adding a REST endpoint for the adaptive scheduler that allows the parallelism to be changed at runtime. Not sure if we ever wrote that down in a JIRA ticket though.;;;","05/Oct/22 11:19;mxm;What you suggest generally works and I've seen this in many user pipelines. However, I think the intention here is slightly different to the normal parameterization of Flink applications. 

The intention of this JIRA is to allow an external component, e.g. autoscaler, to change the parallelism of a Flink deployment without changing the logical topology of the DAG. We can't easily apply parallelism changes based on observed task metrics on the operator level because, from the perspective of an external system, tasks are mostly opaque. The goal is to work on the final JobGraph and prevent any topology changes (e.g. due to chaining, slot sharing, etc) before the actual JobGraph creation. Further, we may not have control over the main method of the user at all.;;;","05/Oct/22 17:16;mxm;{quote}I don't really follow. Will you suspend the job, and restart it from another JM with a different configuration?
Or is this something meant to be specific to the YARN per-job mode (which loads the jobgraph from a file)?
{quote}
Yes, redeploy means deploying a new cluster and re-running the job submissions with a different base configuration which includes the parallelism changes in the JobGraph.;;;","06/Oct/22 13:15;chesnay;I don't really see why we should make this a feature of the job submission.

Ultimately this is just a rescale request. Down the line this will duplicate the logic of externally triggered rescalings via the REST API in the adaptive scheduler (which is supposed to be the default for streaming at some point), and thus in practice adds technical debt.
I'd rather add such a REST endpoint and revisit rescaling for the default scheduler (if it is required at all; maybe you could just use the adaptive scheduler).;;;","06/Oct/22 14:58;mxm;It's definitely an option to use the ""rescaling"" API: [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-rescaling]

I think the rational behind this Jira is to have an immutable way to deploy a rescaled version of a job. With the proposed changes, one simply alters the k8s deployment spec with the updated parallelisms and the job redeploys. With the adaptive scheduler, there is a whole level of indirection and complexity. The upside may be shorter downtime. The downside clearly is the additional rest call which is, as of now, is not reflected in the k8s deployment. I think it is a bit hostile to k8s users to not allow rescale via some form of configuration.

Yes, the k8s operator could be modified to implement the rescale call. We would have to allow additional jobvertex overrides as the endpoint currently only supports setting the default parallelism. Currently this feature is intended for streaming use cases only which work with the adaptive scheduler.;;;","06/Oct/22 15:47;gyfora;I think [~mxm] makes valid points here. 

The problem with the rescale api [~chesnay] is that rescaling with an api call is not a ""persistent"" operation. If a user later wants to upgrade the job using a savepoint, they would have to then tweak the parallelism of the entire pipeline from the main method or rescale again after upgrade.

Having a rescale endpoint is nice but I think there is absolutely no reason to not support doing this through configuration to allow reproducable deployments. Flink already provides a myriad of config options including default parallelism, max paralellism and various execution settings, this is not different from that in my view.

In any case you are right that this should be implemented in a way that works in all deployment modes similar to other execution related config options.;;;","06/Oct/22 19:35;chesnay;??The upside may be shorter downtime.??

The upside is significantly more potential for optimizations down-the-line, like deferring a rescaling to a job failure (== low cost rescaling) or it being properly integrated into batch jobs (==rescale on next stage, similar to the AdaptiveBatchScheduler).

??rescaling with an api call is not a ""persistent"" operation. If a user later wants to upgrade the job using a savepoint, they would have to then tweak the parallelism of the entire pipeline from the main method or rescale again after upgrade.??

If the parallelism is being tuned as part of an auto-scaler, wouldn't it make sense to start from a blank slate if a job is fully resubmitted? I'd assume the initial parallelism to be a reasonable base-line.

??Flink already provides a myriad of config options including default parallelism, max parallelism and various execution settings, this is not different from that in my view.??

This is quite different to me since it introduces the concept of vertex-specific configurations.
It introduces _yet another_ way how the parallelism can be _configured_.
Next you're gonna propose to be able to set the max parallelism, uid, name etc etc when submitting a job.;;;","07/Oct/22 07:40;gyfora;_--- If the parallelism is being tuned as part of an auto-scaler, wouldn't it make sense to start from a blank slate if a job is fully resubmitted? I'd assume the initial parallelism to be a reasonable base-line._

The user may not be aware of the decisions made by the autoscaler, and if the jobgraph did not structurally change (jobvertexes are the same with same id) I think it would be a very bad approach to reset the parallelism to the original one. Of course if we allow this to be configurable we at least do not force one approach, users can decide to set the config or not.



_---Next you're gonna propose to be able to set the max parallelism, uid, name etc etc when submitting a job._

That's a bit of a stretch here :) as far as I understand there was no such intentions there but in any case I don't really see the negative impact. There is clearly a use-case for this and a configuration based approach is required for components like the flink-kubernetes-operator. So far we did not hit any such limitations because almost ""everything"" is configurable in Flink already. This is just filling some gaps.;;;","07/Oct/22 13:17;mxm;Let me try to summarize:

It is a valid use case for external systems to observe the job via task level metrics. Based on these observations, we may want to automatically alter the job vertex parallelisms. There is no programmatic way to do this with Flink now if the k8s application mode is used where we don't have direct control over the JobGraph. Also, the observed job can have **any** topology and we cannot assume the entry point respects parallelism overrides as arguments in the main method.

There are two possible solutions here:
 # Provide the overrides as part of the configuration
 # Issue a call to the rescale API

The problem with (2) is that the Recale API is both disabled and broken at the moment, see https://issues.apache.org/jira/browse/FLINK-12312 It has fundamental issues like not being able to use the HA mode with it. I'm afraid, this makes option (1) the only viable option to me because, frankly, I'm not sure the issues of the rescale mode can easily be fixed.;;;","07/Oct/22 13:25;mxm;[~chesnay] I think you have a better insight into the state of the adaptive scheduler. If you think it is already robust enough to support rescaling requests, we can re-enable the rescale rest API and also add job vertex overrides to it.;;;","07/Oct/22 15:12;chesnay;??the Recale API is both disabled and broken at the moment??

In practice the entire mechanism doesn't exist. I'd ignore that there are still some legacy API fragments around that we haven't removed IIRC exclusively so that users don't hit a 404 when attempting to do a rescale.

??If you think it is already robust enough to support rescaling requests, we can re-enable the rescale rest API and also add job vertex overrides to it??

It's definitely robust enough to be used in production I think.
IIRC Till had a prototype for adding a REST endpoint that adjusts the target parallelism somewhere.
I don't think it worked on a per-vertex basis though; just a global parallelism increase for all vertices (since in reactive mode everything scales uniformly anyway). But that shouldn't be difficult to change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InitializeOnMaster uses wrong parallelism with AdaptiveScheduler,FLINK-29500,13484417,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,04/Oct/22 11:42,20/Oct/22 02:32,04/Jun/24 20:41,06/Oct/22 15:10,1.14.6,1.15.2,1.16.0,,,,,1.15.3,1.16.0,1.17.0,,API / Core,Runtime / Coordination,,,0,pull-request-available,,,,"{{InputOutputFormatVertex}} uses {{JobVertex#getParallelism}} to invoke {{InitializeOnMaster#initializeGlobal}}. However, this parallelism might not be the actual one which will be used to execute the node in combination with Adaptive Scheduler. In case of Adaptive Scheduler the execution parallelism is provided via {{VertexParallelismStore}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 06 15:10:10 UTC 2022,,,,,,,,,,"0|z191qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 15:10;dwysakowicz;Fixed in:
* master
** cadfab59a9a5f6eaaaae7f95e2a9fb3ad7b1b1d9
* 1.16
** 6c6f152e9f3ac10e8d6993e1d9ab8076053e2a44
* 1.15
** 74bc6d2f776baad35b2f7f5be41cdfe87759192f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherOperationCaches should implement AutoCloseableAsync,FLINK-29499,13484415,13482133,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Oct/22 11:31,05/Oct/22 13:46,04/Jun/24 20:41,05/Oct/22 13:46,,,,,,,,1.17.0,,,,Runtime / Coordination,Runtime / REST,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 05 13:46:17 UTC 2022,,,,,,,,,,"0|z191q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 13:46;chesnay;master: 3730e24cc4283f877ac35b189dff355579d1de68;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Async I/O Retry Strategies Do Not Work for Scala AsyncDataStream API,FLINK-29498,13484317,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,eric.xiao,eric.xiao,eric.xiao,03/Oct/22 19:37,17/Nov/22 16:08,04/Jun/24 20:41,17/Nov/22 09:54,1.15.3,,,,,,,1.17.0,,,,API / Scala,,,,0,pull-request-available,,,,"We are using the async I/O to make HTTP calls and one of the features we wanted to leverage was the retries, so we pulled the newest commit: [http://github.com/apache/flink/pull/19983] into our internal Flink fork.

When I try calling the function {{AsyncDataStream.unorderedWaitWithRetry}} from the scala API I with a retry strategy from the java API I get an error as {{unorderedWaitWithRetry}} expects a scala retry strategy. The problem is that retry strategies were only implemented in java and not Scala in this PR: [http://github.com/apache/flink/pull/19983].

 

Here is some of the code to reproduce the error:
{code:java}
import org.apache.flink.streaming.api.scala.AsyncDataStream
import org.apache.flink.streaming.util.retryable.{AsyncRetryStrategies => JAsyncRetryStrategies}

val javaAsyncRetryStrategy = new JAsyncRetryStrategies.FixedDelayRetryStrategyBuilder[Int](3, 100L)
    .build()

val data = AsyncDataStream.unorderedWaitWithRetry(
  source,
  asyncOperator,
  pipelineTimeoutInMs,
  TimeUnit.MILLISECONDS,
  javaAsyncRetryStrategy
){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 17 09:54:24 UTC 2022,,,,,,,,,,"0|z1914g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/22 06:08;lincoln.86xy;[~eric.xiao] There's a separate `org.apache.flink.streaming.api.scala.async.AsyncRetryStrategy` for scala api usage, the java utilities can not be used directly in scala (by design). For your case, you can try to implement your own `AsyncRetryStrategy` to enable retries.;;;","04/Oct/22 14:01;eric.xiao;> For your case, you can try to implement your own `AsyncRetryStrategy` to enable retries.

Thanks [~lincoln.86xy] for replying :), this is what we have done so far, thankfully the code is not a lot but we were wondering if there was a reason why the `AsyncRetryStrategies` are only available in Java API and not the Scala API? Similar for the `RetryPredicates`?

 

I am fairly new to Flink, but I believe I have seem some `.toJava` and `.toScala` helper methods on other Flink components and was wondering if there is room to add the same such functionality to the retry strategies builder and retry predicates?;;;","09/Oct/22 07:26;lincoln.86xy;[~eric.xiao] the retry feature is implemented under the interface-level equivalence between the java and scala api but not included utilities.
Recently a discussion ""Deprecate and remove Scala API support""(https://lists.apache.org/thread/d3borhdzj496nnggohq42fyb6zkwob3h) was raised on the ML,
before a conclusion is reached on it, I think we can move forward with improvements that are valuable to users, [~gaoyunhaii] WDYT?;;;","17/Oct/22 02:25;eric.xiao;As discussed in the [slack thread|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1663957561957419] I am eager to contribute the Scala util classes :). Should we assign the Jira ticket to me (Not sure how to do that)?;;;","19/Oct/22 08:37;gaoyunhaii;Thanks [~eric.xiao] for the efforts! I also think we could first add the improvement since the Scala API would still be exists for several versions after deprecated. I have assigned the issue to you. ;;;","20/Oct/22 19:30;eric.xiao;Hi [~gaoyunhaii], I believe I have a [PR|https://github.com/apache/flink/pull/21077] that is in a reviewable state, what is the process of getting folks from the community to review the PR?;;;","21/Oct/22 03:00;gaoyunhaii;[~eric.xiao] Thanks for the PR! I'll have a look;;;","17/Nov/22 09:54;gaoyunhaii;Merged on master via d426489c9e5c634e2eec8fde6c71356700b7d4b2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide an option to publish the flink-dist jar file artifact,FLINK-29497,13484299,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,thw,thw,thw,03/Oct/22 17:31,15/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Build System,,,,1,pull-request-available,stale-assigned,,,"Currently deployment is skipped for the flink-dist jar file. Instead of hardcoding that in pom.xml, use a property that can control this behavior from the maven command line.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29866,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:35:01 UTC 2023,,,,,,,,,,"0|z1910g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 11:23;chesnay;Can you explain why you'd like this to be configurable, and what parts of flink-dist you're actually interested in?

I'm asking since one my long-time wishes was to split flink-dist into the 2 modules; one module that creates the Flink fat jar (==flink-dist.jar) that would be published (to stream-line the dependency setup of users) and one module that assembles the binary distribution (with all of it's optional dependencies etc) that would still not be published.;;;","09/Oct/22 16:39;thw;Hi [~chesnay] , at the moment we are using this to replace flink-dist.jar in our docker image. Going forward, we may also be interested to use it as one stop shop for local development dependencies, since it would result in a more consistent environment setup.

Your idea of splitting the current dist makes perfectly sense. The dist jar is useful outside the binary distribution and the full binary distribution could be assembled by using the published dist jar.

I'm going to open a PR to add the deploy switch in case someone else is interested using it.

 ;;;","15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Configuration for STS endpoint when using ASSUME_ROLE credential provider,FLINK-29496,13484259,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,03/Oct/22 14:07,19/Jun/23 10:33,04/Jun/24 20:41,31/Oct/22 14:58,1.15.2,1.16.0,,,,,,1.17.0,,,,Connectors / Kinesis,,,,0,pull-request-available,,,,"When using Kinesis connector with credentials provider configured as ASSUME_ROLE in the job running in VPC without internet connection, credentials provider logic tries to access global STS endpoint, {{{}sts.amazonaws.com{}}}. However, only regional endpoints for STS are available in that case.

Connector need support for configuring STS endpoint to allow such use-case.",,,,,,,,,,,,,,,,,,,,FLINK-32375,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 31 14:58:09 UTC 2022,,,,,,,,,,"0|z190rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/22 15:38;dannycranmer;Thanks [~a.pilipenko], please apply this to Amazon Kinesis Data Firehose and Streams connectors. Please cover both AWS V1 (consumer) and V2 (sinks and EFO consumer). ;;;","31/Oct/22 14:58;dannycranmer;Merged commit [{{79712e1}}|https://github.com/apache/flink/commit/79712e16adc13a9ccac016178fd3dcb551afbab3] into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarSinkE2ECase hang,FLINK-29495,13484256,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,hxb,hxb,03/Oct/22 13:55,08/Nov/22 06:56,04/Jun/24 20:41,08/Nov/22 06:56,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,Connectors / Pulsar,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-10-02T05:53:56.0611489Z ""main"" #1 prio=5 os_prio=0 cpu=5171.60ms elapsed=9072.82s tid=0x00007f9508028000 nid=0x54ef1 waiting on condition  [0x00007f950f994000]
2022-10-02T05:53:56.0612041Z    java.lang.Thread.State: TIMED_WAITING (parking)
2022-10-02T05:53:56.0612475Z 	at jdk.internal.misc.Unsafe.park(java.base@11.0.16.1/Native Method)
2022-10-02T05:53:56.0613302Z 	- parking to wait for  <0x0000000087d261f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2022-10-02T05:53:56.0613959Z 	at java.util.concurrent.locks.LockSupport.parkNanos(java.base@11.0.16.1/LockSupport.java:234)
2022-10-02T05:53:56.0614661Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(java.base@11.0.16.1/AbstractQueuedSynchronizer.java:2123)
2022-10-02T05:53:56.0615428Z 	at org.apache.pulsar.common.util.collections.GrowableArrayBlockingQueue.poll(GrowableArrayBlockingQueue.java:203)
2022-10-02T05:53:56.0616165Z 	at org.apache.pulsar.client.impl.MultiTopicsConsumerImpl.internalReceive(MultiTopicsConsumerImpl.java:370)
2022-10-02T05:53:56.0616807Z 	at org.apache.pulsar.client.impl.ConsumerBase.receive(ConsumerBase.java:198)
2022-10-02T05:53:56.0617486Z 	at org.apache.flink.connector.pulsar.testutils.sink.PulsarPartitionDataReader.poll(PulsarPartitionDataReader.java:72) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41526&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24302,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 06:56:20 UTC 2022,,,,,,,,,,"0|z190r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/22 13:56;hxb;cc [~syhily]  Could you help take a look? Thx.;;;","03/Oct/22 13:57;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41525&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=21963;;;","03/Oct/22 14:01;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41527&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1;;;","04/Oct/22 08:44;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41551&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1&l=21573;;;","04/Oct/22 08:46;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41552&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1;;;","07/Oct/22 18:02;syhily;I'll check it latter today.;;;","08/Oct/22 06:13;syhily;[~hxb] PulsarSinkE2ECase should be failed on Java 11. Since this class has been marked with 
{code:java}
@Category(value = {FailsOnJava11.class})
{code}
 I don't know why it would be executed on {{e2e_2_cron_jdk11}}?

The real cause is
{code}
Could not complete the operation. Number of retries has been exhausted. Failed reason: java.lang.OutOfMemoryError: Direct buffer memory
{code}

We are still waiting for Pulsar 2.11.0 for fixing this memory issue.;;;","10/Oct/22 13:40;martijnvisser;The Pulsar test annotation still relies on JUnit4, which is why they still get run on Java 11. I've created https://github.com/apache/flink/pull/21005 to disable them with the correct JUnit5 annotation, pending a successful review;;;","10/Oct/22 19:32;martijnvisser;Test disabled in master via be32eddd3a775cc336412b1cca14ecfa522320ec;;;","10/Oct/22 22:25;syhily;[~martijnvisser] Tks, but how could we know your modification works without running this on nightly build?;;;","11/Oct/22 07:20;martijnvisser;Downgraded from Critical to Major due to disabling the test

[~syhily] You can run {{mvn verify -Prun-end-to-end-tests}} on the {{flink-end-to-end-tests-pulsar}} with JDK11. When doing that, you can see that after the modification the tests are skipped. ;;;","11/Oct/22 08:50;tison;I notice this ticket is tagged to affect 1.15.2 and 1.16.0, shall we backport the fix?;;;","11/Oct/22 09:32;syhily;Yep. We should backport to both 1.15 and 1.16.;;;","11/Oct/22 18:01;martijnvisser;Tests alsof disabled for/via:

release-1.16: 203f36bf2165f3ebfcb1ec3ad36c35af44996015
release-1.15: aa78e3fc7af80d1f465a1747a50372b28c6cb302;;;","02/Nov/22 08:15;martijnvisser;[~syhily] This test is now also hanging for a JDK8 run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42725&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=17472;;;","02/Nov/22 08:53;syhily;[~martijnvisser] I have checked the CI logs. It's wired and shows a NP exception. I think we can create a new one and close this one?

{code}
org.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.ConnectionHandler.handleConnectionError(ConnectionHandler.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:74) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:204) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:140) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ClientBuilderImpl.build(ClientBuilderImpl.java:67) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneaky(PulsarExceptionUtils.java:69) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneakyClient(PulsarExceptionUtils.java:46) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:152) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.transactionCoordinatorClient(PulsarCommitter.java:158) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.commit(PulsarCommitter.java:70) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.commit(CheckpointCommittableManagerImpl.java:126) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmit(CommitterOperator.java:176) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmitCheckpoints(CommitterOperator.java:160) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.notifyCheckpointComplete(CommitterOperator.java:150) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1387) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$14(StreamTask.java:1328) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1367) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d9
3a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 37 more
Caused by: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.connectionOpened(TransactionMetaStoreHandler.java:121) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ConnectionHandler.lambda$grabCnx$0(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 37 more
2022-11-02 03:49:56,321 WARN  org.apache.pulsar.client.impl.ConnectionHandler              [] - [persistent://pulsar/system/transaction_coordinator_assign-partition-0] [Transaction meta store handler [0]] Could not get connection to broker: java.lang.NullPointerException -- Will try again in 0.1 s
2022-11-02 03:49:56,322 ERROR org.apache.pulsar.client.impl.PulsarClientImpl               [] - Start transactionCoordinatorClient error.
org.apache.pulsar.client.api.transaction.TransactionCoordinatorClientException: org.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at org.apache.pulsar.client.api.transaction.TransactionCoordinatorClientException.unwrap(TransactionCoordinatorClientException.java:131) ~[blob_p-4b3d6ed05b6715a4188854f7546af20bd5082468-a8d166b6a98b33c8b856fdb9ed3d9ee1:2.10.0]
   at org.apache.pulsar.client.api.transaction.TransactionCoordinatorClientException.unwrap(TransactionCoordinatorClientException.java:129) ~[blob_p-4b3d6ed05b6715a4188854f7546af20bd5082468-a8d166b6a98b33c8b856fdb9ed3d9ee1:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:70) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:204) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.PulsarClientImpl.<init>(PulsarClientImpl.java:140) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ClientBuilderImpl.build(ClientBuilderImpl.java:67) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneaky(PulsarExceptionUtils.java:69) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.utils.PulsarExceptionUtils.sneakyClient(PulsarExceptionUtils.java:46) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:152) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.transactionCoordinatorClient(PulsarCommitter.java:158) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.connector.pulsar.sink.committer.PulsarCommitter.commit(PulsarCommitter.java:70) ~[blob_p-8b5ac19c79f1f5e8e495fec1e487d5c051157af1-c417788a2d249f7022633051d21c30dd:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.commit(CheckpointCommittableManagerImpl.java:126) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmit(CommitterOperator.java:176) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmitCheckpoints(CommitterOperator.java:160) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.notifyCheckpointComplete(CommitterOperator.java:150) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1387) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$14(StreamTask.java:1328) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1367) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756) ~[flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
   at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
Caused by: org.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.ConnectionHandler.handleConnectionError(ConnectionHandler.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniExceptionallyStage(CompletableFuture.java:898) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.exceptionally(CompletableFuture.java:2209) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:74) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 31 more
Caused by: java.util.concurrent.CompletionException: java.lang.NullPointerException
   at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 31 more
Caused by: java.lang.NullPointerException
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.connectionOpened(TransactionMetaStoreHandler.java:121) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.ConnectionHandler.lambda$grabCnx$0(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.ConnectionHandler.grabCnx(ConnectionHandler.java:73) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.TransactionMetaStoreHandler.<init>(TransactionMetaStoreHandler.java:105) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.lambda$startAsync$0(TransactionCoordinatorClientImpl.java:89) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995) ~[?:1.8.0_342]
   at java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2137) ~[?:1.8.0_342]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.startAsync(TransactionCoordinatorClientImpl.java:78) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   at org.apache.pulsar.client.impl.transaction.TransactionCoordinatorClientImpl.start(TransactionCoordinatorClientImpl.java:68) ~[blob_p-13182ae1972a0afba0da920da789e044f64d93a3-43e2094ec5a07433aa9a86aa77b6ae17:2.10.0]
   ... 31 more
{code}
;;;","04/Nov/22 01:42;syhily;[~martijnvisser] The CI link you have provided is tests on flink release-1.15 branch. And this is an error which has been fixed in https://github.com/apache/pulsar/pull/15840 and released in Pulsar 2.10.1. Do we need to bump the pulsar-client to 2.10.1 in release-1.15 branch to fix this hang issue?;;;","07/Nov/22 15:13;tison;[~syhily] If this is a release-1.15 specific issue, I suggest you open a new issue so that we don't mangle different issues into one entry.;;;","08/Nov/22 06:56;tison;follow-up to fix 1.15 via b37999514cbbd019b31fb2d9c4ae751a956f6c87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangeLogNormalize operator causes unexpected firing of past windows after state restoration,FLINK-29494,13484207,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,rashminpatel405,rashminpatel405,03/Oct/22 06:30,03/Oct/22 08:33,04/Jun/24 20:41,,1.14.2,,,,,,,,,,,Connectors / Kafka,Table SQL / Runtime,,,0,,,,,"*Issue Summary:*

While doing GroupWindowAggregation on stream produced by `upsert-kafka` connector, I am facing an unexpected behaviour, where restoring a job from checkpoint/savepoint is causing past windows(wrt last watermark generated by previous job run) to fire.

*Detailed Description:* 

My program is written in Flink SQL.

Watermark Strategy: max-bounded-out-of-orderness with periodic generation (with default 200ms interval)

Rowtime field: `updated_at_ts` which is monotonically increasing field in changelog stream produced by debezium.

Below is the runtime topology of Flink Job

Kafka Source (upsert mode) >>  ChangeLogNormalize >> GroupWindowAggregate >> PostgresSink

*Job Logic Context:*
I am reading a cdc-stream from kafka and record schema looks something like this:

(pk, loan_acc_no, status, created_at, *updated_at,* __op).

Now I want to count number of distinct loan_acc_no with *hourly* window. So I have created watermark on {{updated_at}} field and hence tumbling also on {{updated_at}}

*Usual scenario which triggers unexpected late windows:*

Now suppose that for the previous job run, the latest running window was{color:#0747a6} {{2022-09-10 08:59:59}}{color} (win_end time) and job had processed events till {{{}08:30{}}}.

Now upon restarting a job, suppose I got a first cdc event like (pk1, loan_1, ""approved"", {color:#00875a}{{2022-09-02 00:00:00}}{color}, {color:#00875a}{{2022-09-10 08:45:00}}{color}, ""u"")  say it {*}E1{*}, which is not a late event wrt the last watermark generated by source operator in previous job run.

Now there is ChangeLogNormalize operator in between kafka source and window operator. So, when kafka source forwards this *E1* to ChangeLogNormalize, it will emit two records which will be of type -U and +U, and will be passed as input to window operator.

 -U (pk1, loan_1, ""pending"", {color:#00875a}{{2022-09-02 00:00:00}}{color}, {color:#00875a}{{2022-09-05 00:00:00}}{color}, ""u"") => previous state of record with key `{_}pk1{_}`
+U (pk1, loan_1, ""approved"", {color:#00875a}{{2022-09-02 00:00:00}}{color}, {color:#00875a}{{2022-09-10 08:45:00}}{color}, ""u"") => same as E1

So this -U type of events are causing the problem since their {{updated_at}} can be of any timestamp in the past and we are tumbling on this field. As per periodic watermarks, during the first watermark interval (i.e 200 ms default), no events will be considered late, so these -U events will create their window state and upon receiving first high watermark, windows created by these events will fire.","Flink version: 1.14.2

API: Flink SQL",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 03 08:33:59 UTC 2022,,,,,,,,,,"0|z190g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/22 08:33;danderson;[~twalthr] [~jingzhang] [~godfrey] This is an interesting problem; it feels to me like we've missed something in the overall design of these operators.

One solution might be to implement watermark checkpointing / recovery, either globally, or just in the ChangeLogNormalize operator. But it also seems problematic that the window operator is consuming -U events. But I probably don't have the big picture. What do you think?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[JDBC] insert sql -> except -> update sql; callback (success, fail)",FLINK-29493,13484100,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Incomplete,,seunggabi,seunggabi,01/Oct/22 09:35,02/Oct/22 04:06,04/Jun/24 20:41,02/Oct/22 04:06,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 02 04:06:18 UTC 2022,,,,,,,,,,"0|z18zso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/22 04:06;martijnvisser;It's not clear from this ticket what is the issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka exactly-once sink causes OutOfMemoryError,FLINK-29492,13484092,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,ruanhang1993,rmetzger,rmetzger,01/Oct/22 06:16,24/Oct/23 10:03,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"My Kafka exactly-once sinks are periodically failing with a {{OutOfMemoryError: Java heap space}}.

This looks very similar to FLINK-28250. But I am running 1.15.2, which contains a fix for FLINK-28250.

Exception:
{code:java}
java.io.IOException: Could not perform checkpoint 2281 for operator http_events[3]: Writer (1/1)#1.
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1210)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:493)
	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
	at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
	at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
	at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 2281 for operator http_events[3]: Writer (1/1)#1. Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:348)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:227)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:212)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:647)
	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:320)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198)
	... 22 more
Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka producer
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:318)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:303)
	at org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:55)
	at org.apache.flink.connector.kafka.sink.KafkaWriter.getOrCreateTransactionalProducer(KafkaWriter.java:327)
	at org.apache.flink.connector.kafka.sink.KafkaWriter.getTransactionalProducer(KafkaWriter.java:315)
	at org.apache.flink.connector.kafka.sink.KafkaWriter.snapshotState(KafkaWriter.java:227)
	at org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler.snapshotState(StatefulSinkWriterStateHandler.java:124)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.snapshotState(SinkWriterOperator.java:152)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222)
	... 33 more
Caused by: org.apache.kafka.common.KafkaException: java.lang.OutOfMemoryError: Java heap space
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:184)
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:192)
	at org.apache.kafka.common.network.ChannelBuilders.clientChannelBuilder(ChannelBuilders.java:81)
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:105)
	at org.apache.kafka.clients.producer.KafkaProducer.newSender(KafkaProducer.java:448)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:429)
	... 43 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.base/java.io.BufferedInputStream.fill(Unknown Source)
	at java.base/java.io.BufferedInputStream.read1(Unknown Source)
	at java.base/java.io.BufferedInputStream.read(Unknown Source)
	at java.base/java.io.DataInputStream.read(Unknown Source)
	at java.base/java.io.InputStream.readNBytes(Unknown Source)
	at java.base/sun.security.util.IOUtils.readExactlyNBytes(Unknown Source)
	at java.base/sun.security.provider.JavaKeyStore.engineLoad(Unknown Source)
	at java.base/sun.security.util.KeyStoreDelegator.engineLoad(Unknown Source)
	at java.base/java.security.KeyStore.load(Unknown Source)
	at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory$FileBasedStore.load(DefaultSslEngineFactory.java:374)
	at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory$FileBasedStore.<init>(DefaultSslEngineFactory.java:349)
	at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.createTruststore(DefaultSslEngineFactory.java:322)
	at org.apache.kafka.common.security.ssl.DefaultSslEngineFactory.configure(DefaultSslEngineFactory.java:168)
	at org.apache.kafka.common.security.ssl.SslFactory.instantiateSslEngineFactory(SslFactory.java:138)
	at org.apache.kafka.common.security.ssl.SslFactory.configure(SslFactory.java:95)
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:180)
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:192)
	at org.apache.kafka.common.network.ChannelBuilders.clientChannelBuilder(ChannelBuilders.java:81)
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:105)
	at org.apache.kafka.clients.producer.KafkaProducer.newSender(KafkaProducer.java:448)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:429)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:318)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:303)
	at org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducer.<init>(FlinkKafkaInternalProducer.java:55)
	at org.apache.flink.connector.kafka.sink.KafkaWriter.getOrCreateTransactionalProducer(KafkaWriter.java:327)
	at org.apache.flink.connector.kafka.sink.KafkaWriter.getTransactionalProducer(KafkaWriter.java:315)
	at org.apache.flink.connector.kafka.sink.KafkaWriter.snapshotState(KafkaWriter.java:227)
	at org.apache.flink.streaming.runtime.operators.sink.StatefulSinkWriterStateHandler.snapshotState(StatefulSinkWriterStateHandler.java:124)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.snapshotState(SinkWriterOperator.java:152)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173)
{code}

What I'm observing is that affected TaskManagers have a high number of {{kafka-producer-network-thread}} (350+ after some time). It seems that the Kafka exactly-once sink is still leaking memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 07 02:29:51 UTC 2022,,,,,,,,,,"0|z18zqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/22 19:12;rmetzger;I have found the problem, but I can not yet explain it. I'm running Flink with operator chaining disabled. 
I believe the KafkaSink implementation assumes that the Writer and Committer are running in the same thread. Theoretically, the KafkaSink is recycling producer instances. But that recycling doesn't work when chaining is disabled.;;;","10/Oct/22 08:23;rmetzger;The problem really is that the Kafka EO Sink assumes chaining to be enabled. Otherwise, the producer instance recycling mechanism doesn't work.;;;","11/Oct/22 10:35;renqs;Thanks for the investigation [~rmetzger] ! I checked the code and confirmed this. I don't think deeply and a naive solution in my mind is to disable the recycling mechanism if the writer and committer is not chained together. I'll investigate more on this issue.  ;;;","12/Oct/22 03:56;ruanhang1993;I am interested in this bug. Maybe I could help to take a look at it.;;;","12/Oct/22 06:01;renqs;Thanks [~ruanhang1993] . I've assign the ticket to you. I think we need to figure out if it is possible to remove the producer from KafkaCommittable. I assume this was intentionally added by FLINK-23854 so we need to confirm this. ;;;","02/Nov/22 13:11;fpaul;[~ruanhang1993] do you think this issue is blocking the 1.15.3 release?;;;","07/Nov/22 02:29;ruanhang1993;[~fpaul] , sorry for my late response.

I think this bug should not be contained in the 1.15.3 release. I changed some methods in the PublicEvolving class.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Primary key without partition field can be supported from full changelog,FLINK-29491,13484080,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,lzljs3620320,lzljs3620320,01/Oct/22 03:34,08/Oct/22 10:09,04/Jun/24 20:41,08/Oct/22 10:09,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,,,,,"When pk does not contain partition fields, an exception will be thrown under any circumstances. We can relax this restriction. When the input is a complete changelog.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-01 03:34:21.0,,,,,,,,,,"0|z18zo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp LTZ is unsupported in table store ,FLINK-29490,13484079,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,01/Oct/22 03:32,29/Mar/23 01:50,04/Jun/24 20:41,29/Mar/23 01:50,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Due to orc format limitation, timestamp ltz is unsupported now. We should fix this, and validate this type cross multiple engines (hive spark trino).
We need to careful about time zone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-10-01 03:32:46.0,,,,,,,,,,"0|z18zo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Display issue when querying complex, deeply nested fields",FLINK-29489,13484026,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jcb1996,jcb1996,30/Sep/22 14:46,02/Oct/22 04:08,04/Jun/24 20:41,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,"Using Flink 1.15, I’m observing some strange behavior when querying fields that have complex, deeply nested fields.

The attached file, FlinkQueries.txt, has two create table statements.

In the first table, I create a Flink table on top of the `tfmsStatusOutput` field. I have no issues querying this field using the SQL client or Java API.

The second table, I create a Flink table on top of the `fltdOutput` field. This field is more complex and has deeply nested fields. Using the SQL client, when I run a simple bounded query such as `SELECT * FROM TBL2 LIMIT 1;` I get a stack trace dump in my display window and when I press any key on my keyboard it returns to the result window as if it's waiting for the results. Those are screenshots  (flink-screenshot1 and flink-screenshot2).

 

Using the Java API, I experience something similar. 

It looks like it is going to return something good:

Starting the SELECT...
Job has been submitted with JobID 0b38a2c51e8357e5fef471b38ac839d0
+----+--------------------------------+--------------------------------+--------------------------------+
| op |                fltdOutput |

but then it starts spitting out some form of diagnostics that doesn’t make much sense:

/* 1 */public final class GeneratedCastExecutor$45 implements org.apache.flink.table.data.utils.CastExecutor {
/* 2 */java.lang.StringBuilder builder$46 = new java.lang.StringBuilder();
/* 3 */java.lang.StringBuilder builder$50 = new java.lang.StringBuilder();
/* 4 */java.lang.StringBuilder builder$55 = new java.lang.StringBuilder();
…
/* 845 */java.lang.StringBuilder builder$7642 = new java.lang.StringBuilder();
/* 846 */java.lang.StringBuilder builder$7654 = new java.lang.StringBuilder();
/* 847 */java.lang.StringBuilder builder$7664 = new java.lang.StringBuilder();
/* 848 */java.lang.StringBuilder builder$7672 = new java.lang.StringBuilder();
/* 849 */public GeneratedCastExecutor$45() {
/* 850 */}
/* 851 */@Override public Object cast(Object _myInputObj) throws org.apache.flink.table.api.TableException {
/* 852 */org.apache.flink.table.data.RowData _myInput = ((org.apache.flink.table.data.RowData)(_myInputObj));
/* 853 */boolean _myInputIsNull = _myInputObj == null;
/* 854 */boolean isNull$0;
/* 855 */org.apache.flink.table.data.binary.BinaryStringData result$1;
/* 856 */boolean isNull$2;
/* 857 */org.apache.flink.table.data.binary.BinaryStringData result$3;
/* 858 */boolean isNull$4;

etc…

 

I couldn't find anything in the task manager logs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/22 14:42;jcb1996;FlinkQueries.txt;https://issues.apache.org/jira/secure/attachment/13050004/FlinkQueries.txt","30/Sep/22 14:44;jcb1996;flink-screenshot1.jpg;https://issues.apache.org/jira/secure/attachment/13050002/flink-screenshot1.jpg","30/Sep/22 14:44;jcb1996;flink-screenshot2.jpg;https://issues.apache.org/jira/secure/attachment/13050003/flink-screenshot2.jpg",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 02 04:08:25 UTC 2022,,,,,,,,,,"0|z18zc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Oct/22 04:08;martijnvisser;[~jark] [~godfreyhe] Any thoughts on this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetricRegistryImpl should implement AutoCloseableAsync,FLINK-29488,13484006,13482133,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Sep/22 11:15,25/Nov/22 02:40,04/Jun/24 20:41,04/Oct/22 09:17,,,,,,,,1.17.0,,,,Runtime / Metrics,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26037,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 04 09:17:09 UTC 2022,,,,,,,,,,"0|z18z80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/22 09:17;chesnay;master: 149785a5725a4727287d2f133b3b3abbb23f99a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RpcService should implement AutoCloseableAsync,FLINK-29487,13484005,13482133,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Sep/22 11:13,05/Oct/22 11:11,04/Jun/24 20:41,05/Oct/22 11:11,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 05 11:11:03 UTC 2022,,,,,,,,,,"0|z18z7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 11:11;chesnay;master: e9f3ec93aad7cec795c765c937ee71807f5478cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Client Parser to get statement type,FLINK-29486,13483984,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,yzl,yzl,30/Sep/22 09:24,29/Jun/23 12:14,04/Jun/24 20:41,12/Oct/22 08:44,1.17.0,,,,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,,"In [FLIP-24|https://cwiki.apache.org/confluence/display/FLINK/FLIP-24%3A+SQL+Client] and  [FLIP-91|https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Gateway#FLIP91:SupportSQLGateway], the SQL client was designed to has two mode: embedded and remote. Currently only embedded mode has been implemented, so we need to implement the remote mode.",,,,,,,,,,,,,,,,,,,,FLINK-24910,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 08:44:39 UTC 2022,,,,,,,,,,"0|z18z34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 08:32;fsk119;Merged into master: 205f70c60f925cd59ca106686da7b4a9df7d3593;;;","12/Oct/22 08:44;fsk119;The Client parser relies on the TokenManager that used by the CalciteParser, which is able to convert the input statement to the token stream. The reason why we don't use `Parser` here is because parser will validate the sql when converting to the Operation, which needs to read the Catalog. However, the client doesn't have the session state in the remote mode.

 

The Client Parser does two things in the future:
 # It's able to remove the comment and submit the statment when semicolon is at the end of the statement.
 # It get the statement type for later processing with the Token type. The type of statement is necessary at the client side because the client goes to the different processing logic for different statement. For example, the client should print the raw content rather than the whole table to the terminal for the SHOW CREATE TABLE statement.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TypeSerializerUpgradeTestBase still uses 1.15 as the current version on master and release-1.16,FLINK-29485,13483982,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,mapohl,mapohl,mapohl,30/Sep/22 09:16,19/Oct/22 11:03,04/Jun/24 20:41,19/Oct/22 11:01,1.15.2,1.16.0,1.17.0,,,,,,,,,Documentation,Tests,,,0,pull-request-available,starter,,,"-TypeSerializerUpgradeTestBase still refers to 1.15 as the current version. We could use FlinkVersions.current() instead to avoid running into this issue again for future major updates.-

-I didn't check other occurrences of FlinkVersions. It should be verified as part of this Jira issue that we don't have the same issue in other locations as well.-

{{TypeSerializerUpgradeTestBase.CURRENT_VERSION}} can be a bit misleading. We might want to add some comment explaining that it's referring to the currently actually released major version and that the corresponding test data should reflect the state of {{1.x.0}} as a baseline for any tests",,,,,,,,,,,,,,,,,,,,,FLINK-27518,,,,,,,,,,,,,,,,FLINK-27727,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 11:01:09 UTC 2022,,,,,,,,,,"0|z18z2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 12:43;mapohl;Using {{FlinkVerions.current()}} doesn't resolve the problem that this test still requires some snapshot data to be generated. thanks for pointing that out [~snuyanzin]. I updated the title accordingly.;;;","30/Sep/22 13:15;mapohl;A similar issue exists for {{StatefulJobSnapshotMigrationITCase}}. But there it's state that we should only progress on the release branch (i.e. to 1.16). Hence, I removed the {{1.17.0}} version tag.;;;","04/Oct/22 07:52;mapohl;I misinterpreted the {{TypeSerializerUpgradeTestBase#CURRENT_VERSION}} member. I updated the Jira issues description: We might want to add some comments in the relevant migration tests to add a bit of context to avoid confusion in the future.;;;","18/Oct/22 10:13;mapohl;FLINK-27518 covers the same topic. I leave this issue open for now until the [dev ML discussion around this topic|https://lists.apache.org/thread/bmgywobc9tb53vcj4zwj4ktv0lkh3n6j] decides on how to merge both approaches or go ahead with one of them. ;;;","19/Oct/22 11:01;mapohl;Closing this issue in favor of FLINK-27518 as discussed in the [related ML thread|https://lists.apache.org/thread/bmgywobc9tb53vcj4zwj4ktv0lkh3n6j];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support orderless check of elements in SourceTestSuiteBase and SinkTestSuiteBase,FLINK-29484,13483975,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,echauchot,echauchot,echauchot,30/Sep/22 09:05,30/Sep/22 13:20,04/Jun/24 20:41,30/Sep/22 13:20,,,,,,,,,,,,Tests,,,,0,,,,,Some backend source and sinks are distributed and to not guarantee order. So the test suites must consider that the tests pass even if the data is out of order. We should add a configuration to the ExternalContext to set orderless per source/sink.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 30 13:20:38 UTC 2022,,,,,,,,,,"0|z18z14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 09:06;martijnvisser;[~chesnay] WDYT?;;;","30/Sep/22 13:08;echauchot;It seems _UnorderedCollectIteratorAssert_ already exists. Need to leverage it in the test suites.;;;","30/Sep/22 13:20;echauchot;Closing because it is already available by overriding SourceTestSuiteBase#checkResultWithSemantic() and using CollectIteratorAssertions#assertUnordered();;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink python udf arrow in thread model bug,FLINK-29483,13483974,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,30/Sep/22 09:03,20/Oct/22 02:33,04/Jun/24 20:41,10/Oct/22 12:20,1.15.2,1.16.0,,,,,,1.15.3,1.16.0,1.17.0,,API / Python,,,,0,pull-request-available,,,,!image-2022-09-30-17-03-05-005.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/22 09:03;jackylau;image-2022-09-30-17-03-05-005.png;https://issues.apache.org/jira/secure/attachment/13049977/image-2022-09-30-17-03-05-005.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 12:20:54 UTC 2022,,,,,,,,,,"0|z18z0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 14:25;hxbks2ks;[~jackylau] You means your job runs in `thread mode` will raise this exception? ;;;","09/Oct/22 02:10;jackylau;yes [~hxbks2ks] ;;;","10/Oct/22 12:20;hxb;Merged into master via 7a6fc2433c746b8d97286d1c758f5a933f40e3e7

Merged into release-1.16 via 6de8d291ef2761c469fb5ba5b3394a838846bea5

Merged into release-1.15 via 5d748f37661cca11f168fae0b54a8c0d0f1ef2a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ingress always forces ClusterIP rest service type,FLINK-29482,13483970,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,morhidi,gyfora,gyfora,30/Sep/22 08:44,24/Nov/22 01:03,04/Jun/24 20:41,02/Nov/22 15:12,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Currently the ingress logic always overrides ClusterIP service type even if the user configured it otherwise.

This might simply be a bug but there could be a good reason for it, we cannot tell at first.

We need to investigate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 02 15:12:27 UTC 2022,,,,,,,,,,"0|z18z00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 08:45;morhidi;Will have a look, thanks!;;;","12/Oct/22 07:59;gyfora;I think based on the slack discussions this is not an issue right [~matyas] ?;;;","02/Nov/22 15:12;morhidi;Yes, we can close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show JobType on WebUI,FLINK-29481,13483948,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,dangshazi,dangshazi,30/Sep/22 06:55,16/May/24 10:11,04/Jun/24 20:41,16/May/24 10:11,,,,,,,,1.20.0,,,,Runtime / Web Frontend,,,,0,features,pull-request-available,,,"This is an umbrella Jira of [FLIP-441|https://cwiki.apache.org/confluence/x/agrPEQ]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/22 06:54;dangshazi;image-2022-09-30-14-54-43-875.png;https://issues.apache.org/jira/secure/attachment/13049969/image-2022-09-30-14-54-43-875.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 03:29:15 UTC 2024,,,,,,,,,,"0|z18yv4:",9223372036854775807,"We display the JobType on the Flink WebUI. The job maintainer or platform administrator can easily see whether the Flink Job is running in Streaming or Batch Mode, which is useful for troubleshooting.",,,,,,,,,,,,,,,,,,,"14/Oct/22 09:36;xtsong;Hi [~dangshazi],

I think this makes a good improvement. Moreover, I think Execution Mode is only used in the deprecated DataSet API. We might want to add an explanation on the web ui, to avoid confusing users with the two similar concepts.

Would you like to take this ticket and open a pull request?;;;","25/Oct/22 09:09;dangshazi;Sure, I'll take this ticket;;;","25/Oct/22 09:32;xtsong;Thanks, [~dangshazi]. You are assigned. Please go ahead.;;;","24/Nov/22 08:49;wanglijie;Hi [~dangshazi] , any updates on this ticket? ;;;","06/Feb/23 02:56;dangshazi;[~wanglijie] I have created a PR on this ticket;;;","15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","27/Mar/24 03:29;fanrui;Discuss with [~dangshazi] offlien, I will take over this JIRA.

Based on the FLIP doc, the rest api and WebUI are public interfaces, I will prepare the FLIP-441 for it recently.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip invalid messages when writing,FLINK-29480,13483943,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Implemented,Alcántara,Alcántara,Alcántara,30/Sep/22 06:07,01/Nov/22 08:53,04/Jun/24 20:41,31/Oct/22 15:58,,,,,,,,1.17.0,,,,Connectors / Kafka,,,,0,pull-request-available,starter,,,"As reported in [1], it seems that it's not possible to skip invalid messages when writing. More specifically, if there is an error serializing messages, there is no option for skipping them and then Flink job enters a crash loop. In particular, the `write` method of the `KafkaWriter` looks like this:
{code:java}
@Override
public void write(IN element, Context context) throws IOException {
  final ProducerRecord<byte[], byte[]> record = recordSerializer.serialize(element, ...);
  currentProducer.send(record, deliveryCallback); // line 200
  numRecordsSendCounter.inc();
} {code}
So, If you make your `serialize` method return `null`, this is what you get at runtime
{code:java}
java.lang.NullPointerException at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:906) at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:885) at org.apache.flink.connector.kafka.sink.KafkaWriter.write(KafkaWriter.java:200) at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:158)  {code}
What I propose is to modify the KafkaWriter [2, 3] like this:
{code:java}
@Override
public void write(IN element, Context context) throws IOException {
  final ProducerRecord<byte[], byte[]> record = recordSerializer.serialize(element, ...);
  if (record != null) { // skip null records (check to be added)
    currentProducer.send(record, deliveryCallback);
    numRecordsSendCounter.inc();
  }
} {code}
In order to at least give a chance of skipping those messages and move on to the next ones.

Obviously, one could prepend the sink with a flatMap operator for filtering out invalid messages, but
 # It looks weird that one has to prepend an operator for ""making sure"" that the serializer will not fail right after. Wouldn't it be simpler to skip the null records directly in order to avoid this pre-check? [4]
 # It's such a simple change (apparently)
 # Brings consistency/symmetry with the reading case [4, 5]

To expand on point 3, by looking at `KafkaDeserializationSchema`:
{code:java}
T deserialize(ConsumerRecord<byte[], byte[]> record) throws Exception;

default void deserialize(ConsumerRecord<byte[], byte[]> message, Collector<T> out) throws Exception {
  T deserialized = deserialize(message);
  if (deserialized != null) { // skip null records (check already exists)
    out.collect(deserialized);
  }
}  {code}
one can simply return `null` in the overriden `deserialize` method in order to skip any message that fails to be deserialized. Similarly, if one uses the `KafkaRecordDeserializationSchema` interface instead:
{code:java}
void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<T> out) throws IOException {code}
then it's also possible not to invoke `out.collect(...)` on null records. To me, it looks strange that the same flexibility is not given in the writing case.

*References*

[1] [https://lists.apache.org/thread/ykmy4llovrrrzlvz0ng3x5yosskjg70h]

[2] [https://nightlies.apache.org/flink/flink-docs-release-1.14/release-notes/flink-1.14/#port-kafkasink-to-new-unified-sink-api-flip-143] 

[3] [https://github.com/apache/flink/blob/f0fe85a50920da2b7d7da815db0a924940522e28/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L197] 

[4] [https://lists.apache.org/thread/pllv5dqq27xkvj6p3lj91vcz409pw38d] 

[5] [https://stackoverflow.com/questions/55538736/how-to-skip-corrupted-messages-in-flink] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/22 11:50;Alcántara;Screenshot 2022-10-28 at 13.48.12.png;https://issues.apache.org/jira/secure/attachment/13051556/Screenshot+2022-10-28+at+13.48.12.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 01 02:09:33 UTC 2022,,,,,,,,,,"0|z18yu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/22 09:51;martijnvisser;[~renqs] [~Leonard] What are your thoughts on this?;;;","25/Oct/22 07:05;leonard;[~Alcántara] I think a null of `ProducerRecord` is meaningless in any case,  I think we can directly skip null for consistency consideration. WDYT? [~renqs]
 ;;;","25/Oct/22 07:17;renqs;Thanks for reporting this [~Alcántara] ! The proposal makes sense to me as a null ProducerRecord is meaningless and can be used to skip records in KafkaWriter. [~Alcántara] Would you like to submit a patch for this? ;;;","26/Oct/22 11:00;Alcántara;Thanks for your reply [~leonard] [~renqs] ! Sure, I'm glad to submit the patch. Will do this week or the next one!;;;","27/Oct/22 15:32;Alcántara;Please [~martijnvisser] [~leonard] [~renqs] can anyone assign the ticket to me?;;;","27/Oct/22 15:43;leonard;[~Alcántara] I've assigned this ticket to you, hope to see you patch ^_^. ;;;","28/Oct/22 09:35;Alcántara;[~leonard] [~renqs] [~martijnvisser] I've submitted the patch here: https://github.com/apache/flink/pull/21186. Note that `mvn -DskipTests clean package` fails for me so there must be something wrong with my local setup, but I think for such a simple change there should be no problems.;;;","29/Oct/22 01:32;mason6345;I also think this is a good idea and can help take a look at the PR. With regards to your local setup, what Java version are you using? If you aren't already, try using Java 11;;;","29/Oct/22 06:45;Alcántara;Thanks [~mason6345]! I've solved my local setup issues so we can move forward with the PR...;;;","30/Oct/22 06:58;Alcántara;[~mason6345] [~leonard] [~renqs] [~martijnvisser] I think the PR is ready for review. Can you please take a look?;;;","31/Oct/22 15:58;leonard;Implemented in master: 70825e2dc57fdab797975b3be6fa870eb565cf1c;;;","31/Oct/22 16:34;Alcántara;Thanks a lot [~leonard] [~martijnvisser]. Will that change be available in v1.17.0?;;;","01/Nov/22 02:09;leonard;[~Alcántara]You're welcome, this improvement will be available in next major release 1.17.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support whether using system PythonPath for PyFlink jobs,FLINK-29479,13483937,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,30/Sep/22 05:33,26/Oct/22 08:26,04/Jun/24 20:41,25/Oct/22 11:33,,,,,,,,1.15.3,1.16.1,1.17.0,,API / Python,,,,0,pull-request-available,,,,"It exists PYTHONPATH env in system,like yarn/k8s images, it will cause conflict with users python depdendency sometimes. so i suggest add a config to do whether using system env of PYTHONPATH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 25 11:33:00 UTC 2022,,,,,,,,,,"0|z18yso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 05:34;jackylau;hi [~dianfu] [~hxbks2ks] , what do you think?;;;","30/Sep/22 05:35;jackylau;{code:java}
// conf like
/** Whether preserves system env of PYTHONPATH. */
@Experimental
public static final ConfigOption<Boolean> PYTHON_PRESERVE_PYTHONPATH_ENABLED =
        ConfigOptions.key(""python.preserve.pythonpath.enabled"")
                .booleanType()
                .defaultValue(true)
                .withDescription(""Whether preserves system env of PYTHONPATH."");
 {code};;;","30/Sep/22 09:38;hxbks2ks;[~jackylau] From my side, the users dependency will have a higher priority, so it shouldn't be affected by the PYTHONPATH of the system env  under normal circumstances. Could you give a more specific example of how the system's PYTHONPATH is having a bad effect in some circumstance.;;;","11/Oct/22 09:25;hxb;I have discussed this problem with [~jackylau]  offline. I think we may really need to introduce a config to decide whether we need to use the PYTHONPATH that comes with the system, but we can reconsider the name of the config. For example, a more general meaning of config means whether to use a completely isolated and independent of all environment variables. cc [~dianfu] ;;;","19/Oct/22 07:29;jackylau;After researching spark, it does not rely on system environment variables. Through careful offline communication with [~hxb] , we reached an agreement to use this parameter to decide whether to retain system environment variables
{code:java}
// code placeholder
public static final ConfigOption<Boolean> PYTHON_SYSTEMENV_ENABLED =
ConfigOptions.key(""python.systemenv.enabled "")
.booleanType()
.defaultValue(true)
.withDescription(""Whether uses system env in python.""); {code};;;","25/Oct/22 11:33;hxb;Merged into master via 8e16cc8e424e352c5b45b46f1520ecf0edec70be

Merged into release-1.16 via 9213effb32a4e80d8113ba7bf36782f33a5e197c

Merged into release-1.15 via 91ccde95c7eae7f020d68592a7fa76201674724a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink sql Connector hive to support 3.1.3 ,FLINK-29478,13483930,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,samrat007,samrat007,samrat007,30/Sep/22 04:46,13/Dec/22 08:56,04/Jun/24 20:41,20/Oct/22 10:55,,,,,,,,1.17.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,"Currently , flink-connector hive support flink-sql-connector-hive-3.1.2 as highest version ! 
h3. hive 3.1.3 released on 08 April 2022



Proposal :- 
We should think of adding support for 3.1.3. ",,,,,,,,,,,,,,,,,,,,FLINK-27423,FLINK-27423,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 24 06:23:16 UTC 2022,,,,,,,,,,"0|z18yr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 07:30;dannycranmer;Hello [~samrat007]. I have not been involved in the hive connector, on the surface bumping 3.1.2 to 3.1.3 sounds like a trivial thing. However I see we have a dedicated \{{flink-sql-connector-hive-3.1.2}} which makes me doubt this. Can you please explain what is required here so we can decide if this needs a FLIP or not? Thanks.;;;","30/Sep/22 09:54;samrat007;Thank you [~dannycranmer] for addressing to this issue . 

currently flink-sql-connector-hive has the following variation 
 * flink-sql-connector-hive-1.2.2
 * flink-sql-connector-hive-2.2.0
 * flink-sql-connector-hive-2.3.6
 * flink-sql-connector-hive-2.3.9
 * flink-sql-connector-hive-3.1.2



My intention of creating this ticket is to add support for hive new version 3.1.3 . Bumping version 3.1.2 would / may lead to breaking change.

I think Providing support for hive 3.1.3 would need seperate `flink-sql-connector-hive-3.1.3` that will be dedicated to flink connector integration with hive for specific version 3.1.3 

Out of curiousity , I have a thought , 
can we generalize this flink-sql-connector-hive , and it uses version from runtime classes ? 
just a thought to improve more and skip the version specific support ;;;","30/Sep/22 10:33;martijnvisser;Instead of keep adding newer versions of Hive to support, we should actually bring that number down imho. Hive 1 is meant for Hadoop 1, which isn't supported by the Hadoop community anymore. So why not drop that version? Why not only support Hive 2.3.9 and 3.1.3, perhaps also Hive 2.2.0. A patch version shouldn't lead to a breaking change, then it should have been a minor or major version upgrade. ;;;","30/Sep/22 11:34;samrat007;Thanks [~martijnvisser] for your input 

there is an associated ticket
https://issues.apache.org/jira/browse/FLINK-27044 
dedicated to remove support for 
 * 1.*
 * 2.1.*
 * 2.2.*

all this support was removed as part of `FLINK-27044`
flink will support only latest version of hive  2.3.x and 3.1.x





As you have suggested to upgrade the hive version to the latest one 3.1.3 will be the way forward. 

I can now allign on the same 

can i work on this ticket to make the upgrade for 3.1.3 ?;;;","30/Sep/22 11:38;martijnvisser;Ah lol that's even my own ticket :D 

I'll assign the ticket to you;;;","30/Sep/22 11:43;samrat007;haha :D Thank you ;;;","02/Oct/22 14:36;samrat007;[~martijnvisser] raised a PR for the changes . 
Please review whenever free time.  
[https://github.com/apache/flink/pull/20937];;;","20/Oct/22 10:55;martijnvisser;Fixed in master: d1ff3dd13b842a86e601e1fdc39339c2fad4db00;;;","24/Oct/22 06:16;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f]
{code:java}
2022-10-22T01:14:14.6319640Z Oct 22 01:14:14 java.lang.AssertionError: Unknown test version 3.1.3
2022-10-22T01:14:14.6321027Z Oct 22 01:14:14 	at org.apache.flink.table.module.hive.HiveModuleTest.verifyNumBuiltInFunctions(HiveModuleTest.java:83)
2022-10-22T01:14:14.6322334Z Oct 22 01:14:14 	at org.apache.flink.table.module.hive.HiveModuleTest.testNumberOfBuiltinFunctions(HiveModuleTest.java:54) {code}
Some hive3 tests failed due to this patch. ;;;","24/Oct/22 06:23;samrat007;Looking into it [~hxb] ! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException when collect primitive array to Python,FLINK-29477,13483924,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,30/Sep/22 02:21,16/Oct/22 04:34,04/Jun/24 20:41,16/Oct/22 04:34,1.15.2,1.16.0,,,,,,1.15.3,1.16.0,,,API / Python,,,,0,pull-request-available,,,,"How to reproduce this bug:
{code:java}
ds = env.from_collection([1, 2], type_info=Types.PRIMITIVE_ARRAY(Types.INT()))
ds.execute_and_collect(){code}
got:
{code:java}
java.lang.ClassCastException: class [I cannot be cast to class [Ljava.lang.Object {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 16 04:34:04 UTC 2022,,,,,,,,,,"0|z18yq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/22 04:34;hxb;Merged into master via c85e6ec45bebb2eb376a911e11294cd118893fb3

Merged into release-1.16 via b0d4dd3a1eaf648f67e0ca7cb075591ecb69e2c4

Merged into release-1.15 via 507b93eef2af79ef2ad5752e1271e5c8915bb15f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis Connector retry mechanism not applied to EOFException,FLINK-29476,13483861,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,29/Sep/22 14:38,20/Oct/22 02:32,04/Jun/24 20:41,06/Oct/22 07:43,1.15.2,,,,,,,1.15.3,1.16.0,1.17.0,,Connectors / Kinesis,,,,0,pull-request-available,,,,"The current retry mechanism in Kinesis connector only considers _SocketTimeoutException_ as recoverable: [KinesisProxy.java#L422|https://github.com/apache/flink/blob/release-1.16.0-rc1/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/proxy/KinesisProxy.java#L422] , however we observed that communication can also fail with EOFException: [^kinesis-exception.log]

This exception should also be considered recoverable and retried.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/22 14:33;afedulov;kinesis-exception.log;https://issues.apache.org/jira/secure/attachment/13049940/kinesis-exception.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 06 07:42:45 UTC 2022,,,,,,,,,,"0|z18yc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/22 07:42;dannycranmer;Merged commit [{{49e0041}}|https://github.com/apache/flink/commit/49e0041042e65e0045701cc5514ce55d74796ba2] into master
Merged commit [{{19aa96a}}|https://github.com/apache/flink/commit/19aa96ac09e1518b3a761f8320681bc5e046f655] into release-1.16
Merged commit [{{9715b5e}}|https://github.com/apache/flink/commit/9715b5e641e76a33c0b869409ce5094f5727105a] into release-1.15



 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add WARNING/ERROR checker for the operator in e2e tests,FLINK-29475,13483856,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,morhidi,,29/Sep/22 14:11,24/Nov/22 01:01,04/Jun/24 20:41,22/Nov/22 21:21,kubernetes-operator-1.3.0,,,,,,,kubernetes-operator-1.3.0,,,,,,,,0,pull-request-available,,,,"We can also try eliminating unwanted warnings like:

{{[WARN ] The client is using resource type 'flinkdeployments' with unstable version 'v1beta1'}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 21:21:26 UTC 2022,,,,,,,,,,"0|z18yb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 21:21;gyfora;merged to main af00c99defbe49c84dbd8a3ac4341136ca3efac9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Name collision: Group already contains a Metric with the name,FLINK-29474,13483853,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,morhidi,,29/Sep/22 13:38,24/Nov/22 01:01,04/Jun/24 20:41,29/Sep/22 19:16,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"k create -f examples/basic-session-deployment-and-job.yaml

results in warnings:
{quote} flink-kubernetes-operator 2022-09-29 13:30:00,001 o.a.f.m.MetricGroup            [WARN ][default/basic-session-job-example] Name collision: Group already contains a Metric with the name  │
│ 'TimeSeconds'. Metric will not be reported.[flink-kubernetes-operator-6f9bbfd557-ljp6w, k8soperator, default, flink-kubernetes-operator, system, Lifecycle, Transition, Resume]            │
│ flink-kubernetes-operator 2022-09-29 13:30:00,001 o.a.f.m.MetricGroup            [WARN ][default/basic-session-job-example] Name collision: Group already contains a Metric with the name  │
│ 'TimeSeconds'. Metric will not be reported.[flink-kubernetes-operator-6f9bbfd557-ljp6w, k8soperator, default, flink-kubernetes-operator, system, Lifecycle, Transition, Upgrade]
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 19:16:14 UTC 2022,,,,,,,,,,"0|z18yag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 19:16;gyfora;merged to main 3ae66dbddd4126a19c1e8f0105027bc370f5e753;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Flink CI utils into separate project,FLINK-29473,13483843,13483838,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,29/Sep/22 12:55,29/Sep/22 12:55,04/Jun/24 20:41,,,,,,,,,,,,,Build System,Build System / CI,,,0,,,,,"We need the ability to improve the CI tools independent of Flink releases. Move flink-ci-tools into it's own repo.

This will require some code changes, for example to allow exclusions to passed via the command-line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-29 12:55:00.0,,,,,,,,,,"0|z18y88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create shared release scripts,FLINK-29472,13483841,13483838,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Sep/22 12:53,10/Nov/22 12:23,04/Jun/24 20:41,10/Nov/22 12:23,,,,,,,,,,,,Release System,,,,0,pull-request-available,,,,"With the versioning & branching model being identical we should be able to share  all release scripts. Put them into a central location that projects can rely on (e.g., via a git submodule).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 10 12:23:13 UTC 2022,,,,,,,,,,"0|z18y7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/22 09:12;chesnay;I've created a [repository|https://github.com/apache/flink-connector-shared-utils/] but there's some permission issue with GitHub.
Once that is resolved I'll publish the scripts I wrote into said repository.;;;","10/Nov/22 12:23;chesnay;606d8b8c56c3aa77e2262b25107c5f911d0ce2ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a flink-connector-parent pom,FLINK-29471,13483839,13483838,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Sep/22 12:50,28/Mar/23 10:26,04/Jun/24 20:41,28/Mar/23 10:26,,,,,,,,,,,,Build System,,,,0,pull-request-available,,,,"Create a shared parent pom for connectors, reducing the overhead of creating new repos and easing plugin maintenance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 16:42:43 UTC 2023,,,,,,,,,,"0|z18y7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 12:52;chesnay;I've published a prototype pom as {{io.github.zentol.flink:flink-connector-parent}} that we can use while we still iron things out to. In the end this should be moved under the Flink umbrella.;;;","30/Sep/22 09:34;chesnay;elasticsearch-main: 6e30d5d63d395b2f731418c34f5838231dcab6b8;;;","12/Jan/23 16:42;chesnay;Parent pom code has been moved to https://github.com/apache/flink-connector-shared-utils/tree/parent_pom. We'll make some small adjustments before making the first release in apache.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup shared utils for connector externalization,FLINK-29470,13483838,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,29/Sep/22 12:49,29/Sep/22 12:49,04/Jun/24 20:41,,,,,,,,,,,,,Build System,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-29 12:49:53.0,,,,,,,,,,"0|z18y74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate noop.jar instead of committing into source,FLINK-29469,13483810,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,29/Sep/22 11:04,29/Sep/22 17:56,04/Jun/24 20:41,29/Sep/22 17:56,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Previously we decided to simply commit an empty jar file into the project to serve a feature. This might be problematic from a release policy perspective, lets replace with a simple maven plugin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 17:56:59 UTC 2022,,,,,,,,,,"0|z18y0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 17:56;gyfora;merged to main ed433cea2b2ea079b9966fa15c31dc5177e46961;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update jackson-bom to 2.13.4,FLINK-29468,13483800,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,29/Sep/22 09:45,03/Nov/22 08:15,04/Jun/24 20:41,10/Oct/22 13:47,1.16.0,,,,,,,1.15.3,1.16.0,elasticsearch-3.0.0,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29638,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 19:31:49 UTC 2022,,,,,,,,,,"0|z18xyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 13:47;mapohl;master: 07bc0185d37c06267a8ab290eabcfe30af55ddf0;;;","10/Oct/22 13:49;mapohl;[~chesnay] is this worth a backport? It doesn't fix any severe vulnerabilities. That's why, i was initially thinking of just doing it on {{master}};;;","19/Oct/22 19:31;mapohl;Backports as part of FLINK-29638:
1.16: 5b206506e6ea2734171e40a5e2d202b6b7bdf549
1.15: 40b3aab5f639b32d9a5e0a069395abe7795dde0d
elasticsearch-main: 13e213768d1e59226aa6c1fa8c07b1daa09eff91
elasticsearch-3.0: 5b4f622b1810f1827ecb0ca6f2e07fdcedf0a659;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync RabbitMQ CI setup with Elasticsearch CI setup,FLINK-29467,13483792,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,29/Sep/22 08:40,29/Sep/22 10:36,04/Jun/24 20:41,29/Sep/22 10:36,,,,,,,,rabbitmq-3.0.0,,,,Connectors/ RabbitMQ,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 10:36:56 UTC 2022,,,,,,,,,,"0|z18xww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 10:36;martijnvisser;Fixed in main: eee3fea6be1c9784974a4cc72d8d2db416b7637d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update org.postgresql:postgresql to 42.5.0,FLINK-29466,13483790,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,29/Sep/22 08:30,14/Dec/22 12:40,04/Jun/24 20:41,13/Oct/22 12:54,,,,,,,,jdbc-3.1.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 12:54:26 UTC 2022,,,,,,,,,,"0|z18xwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 12:54;martijnvisser;Fixed in master: c2039128945f52bea2ff662b622805e4b95bd3ea
1.16: e08b251cf11e6d4a6922e9e5a3653f901a03257c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supports limit factor in the resource definition,FLINK-29465,13483782,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,haoxin,haoxin,29/Sep/22 07:36,29/Sep/22 09:51,04/Jun/24 20:41,29/Sep/22 09:51,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Can we add limit factor supports in the resource definition?
{code:java}
public class Resource {
    /** Amount of CPU allocated to the pod. */
    private Double cpu;

    private double cpuLimitFactor = 1.0;

    /** Amount of memory allocated to the pod. Example: 1024m, 1g */
    private String memory;

    private double memoryLimitFactor = 1.0;

    ...
} {code}
We can set the defaults as `1.0`, and update them to K8s config options
{code:java}
JOB_MANAGER_CPU_LIMIT_FACTOR
JOB_MANAGER_MEMORY_LIMIT_FACTOR
TASK_MANAGER_CPU_LIMIT_FACTOR
TASK_MANAGER_MEMORY_LIMIT_FACTOR {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 09:51:15 UTC 2022,,,,,,,,,,"0|z18xuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 08:23;gyfora;What is the limit factor? What does the setting do?;;;","29/Sep/22 08:40;haoxin;We currently set the pod resources request and limit the same base on the resource definition.

For example, if we add resource definition in the FlinkDeployment CR
{code:java}
taskManager:    
    resource:      
        memory: 2048m      
        cpu: 0.5 {code}
The TM's pod definition will be
{code:java}
resources:
    limits:
        cpu: 500m
        memory: 2Gi
    requests:
        cpu: 500m
        memory: 2Gi {code}
According to the Flink KubernetesConfigOptions docs. (For example TM)
{code:java}
TASK_MANAGER_MEMORY_LIMIT_FACTOR = ConfigOptions.key(""kubernetes.taskmanager.memory.limit-factor"").doubleType().defaultValue(1.0).withDescription(""The limit factor of memory used by task manager. The resources limit memory will be set to memory * limit-factor.""); {code}
With the limit factor, we can do this
{code:java}
taskManager: 
    resource: 
        memory: 512m
        memoryLimitFactor: 4 
        cpu: 0.1
        cpuLimitFactor: 2{code}
so that TM's pod spec will be
{code:java}
resources:
     limits:
         cpu: 100m
         memory: 512Mi
     requests:
         cpu: 200m
         memory: 2Gi{code};;;","29/Sep/22 08:47;haoxin;This is useful for decreasing cloud costs. :);;;","29/Sep/22 09:12;gyfora;You can already set this easily through `flinkConfiguration` right?;;;","29/Sep/22 09:51;haoxin;yes, make sense, will give a try;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobID generation logic could lead to state loss,FLINK-29464,13483779,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,29/Sep/22 07:30,29/Sep/22 17:55,04/Jun/24 20:41,29/Sep/22 17:55,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The recently added job id override logic (https://issues.apache.org/jira/browse/FLINK-29109) can under certain cases lead to state loss.

State loss scenario:
1. Either first deployment / Stateless upgrade mode used -> new jobId will be generated and set in jobStatus
2. Operator/deployment fails during or directly after successful submission -> status is not persisted with the generated jobId
3. User submits a spec update with last-state upgrade
4.  If the job was never observed (due to a failure or early spec update) a last-state upgrade would be performed, deleting the Deployment and simply submitting the job.
5. The current logic would then generate a new jobid (because it's still empty) leading to a failure to recover the state from HA -> data loss


There are multiple ways to solve this issue:
 a ) Record status after generating a jobid
 b ) Only ever set the status during stateless deployment
 c ) Verify no HA data is present before setting the jobid when empty

Probably the most robust solution is a).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 17:55:57 UTC 2022,,,,,,,,,,"0|z18xu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 07:30;gyfora;cc [~thw] ;;;","29/Sep/22 17:55;gyfora;merged to main c33e3ef06296974af47b24f499aef71944c60a4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputFormatCacheLoaderTest.checkCounter failed with AssertionError,FLINK-29463,13483754,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hxbks2ks,hxbks2ks,29/Sep/22 03:37,27/Dec/22 07:44,04/Jun/24 20:41,13/Oct/22 11:50,1.16.0,,,,,,,,,,,Table SQL / Runtime,,,,0,test-stability,,,,"
{code:java}
2022-09-29T02:29:58.9822167Z Sep 29 02:29:58 java.lang.AssertionError: 
2022-09-29T02:29:58.9832438Z Sep 29 02:29:58 
2022-09-29T02:29:58.9833447Z Sep 29 02:29:58 Expecting AtomicInteger(0) to have value:
2022-09-29T02:29:58.9834120Z Sep 29 02:29:58   0
2022-09-29T02:29:58.9834647Z Sep 29 02:29:58 but did not.
2022-09-29T02:29:58.9835734Z Sep 29 02:29:58 	at org.apache.flink.table.runtime.functions.table.fullcache.inputformat.InputFormatCacheLoaderTest.checkCounter(InputFormatCacheLoaderTest.java:74)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f",,,,,,,,,,,,,,,,,,,,FLINK-29405,,,,,,,,,FLINK-29093,FLINK-29405,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 27 07:44:27 UTC 2022,,,,,,,,,,"0|z18xog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 03:37;hxbks2ks;cc [~renqs] [~smiralex];;;","27/Dec/22 07:44;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44249&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11616;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LookupJoinITCase failed on azure due to classloader leaking,FLINK-29462,13483753,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,gaoyunhaii,gaoyunhaii,29/Sep/22 03:37,21/Nov/22 03:39,04/Jun/24 20:41,11/Oct/22 02:19,1.16.0,,,,,,,,,,,Table SQL / Planner,,,,0,test-stability,,,,"{code:java}
09:53:45,656 [ForkJoinPool.commonPool-worker-8] WARN  org.apache.flink.table.runtime.generated.GeneratedClass      [] - Failed to compile split code, falling back to original code
org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.open(GenericRowDataKeySelector.java:50) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.<init>(InputSplitCacheLoadTask.java:60) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:135) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.lambda$reloadCache$0(InputFormatCacheLoader.java:84) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) [?:1.8.0_292]
	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948) [?:1.8.0_292]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) [?:1.8.0_292]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) [?:1.8.0_292]
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) [?:1.8.0_292]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) [?:1.8.0_292]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566) [?:1.8.0_292]
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:85) [flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105) [flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640) [?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	... 21 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	... 21 more
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$157529"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[janino-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[commons-compiler-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[commons-compiler-3.0.11.jar:?]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	... 21 more
Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.lang.Class.forName0(Native Method) ~[?:1.8.0_292]
	at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_292]
	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[janino-3.0.11.jar:?]
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[janino-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[commons-compiler-3.0.11.jar:?]
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[commons-compiler-3.0.11.jar:?]
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-shaded-guava-30.1.1-jre-15.0.jar:30.1.1-jre-15.0]
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	... 21 more
09:53:45,659 [Source: T[11085] -> LookupJoin[11086] -> Calc[11087] -> Sink: Collect table sink (1/1)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: T[11085] -> LookupJoin[11086] -> Calc[11087] -> Sink: Collect table sink (1/1)#0 (ad34f49f737d0018eca963aeff39d79c_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
09:53:45,659 [flink-akka.actor.default-dispatcher-11] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: T[11085] -> LookupJoin[11086] -> Calc[11087] -> Sink: Collect table sink (1/1) (ad34f49f737d0018eca963aeff39d79c_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
09:53:45,662 [Source: T[11085] -> LookupJoin[11086] -> Calc[11087] -> Sink: Collect table sink (1/1)#0] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: T[11085] -> LookupJoin[11086] -> Calc[11087] -> Sink: Collect table sink (1/1)#0 (ad34f49f737d0018eca963aeff39d79c_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:85)
	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:123)
	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
	at LookupFunction$157537.flatMap(Unknown Source)
	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:121)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
	... 5 more
Caused by: java.lang.RuntimeException: Failed to create InputFormatCacheLoadTask
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:137)
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.lambda$reloadCache$0(InputFormatCacheLoader.java:84)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:85)
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
	... 6 more
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$157529'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.open(GenericRowDataKeySelector.java:50)
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.<init>(InputSplitCacheLoadTask.java:60)
	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:135)
	... 16 more
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
	... 19 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
	... 21 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 24 more
Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$157529"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
	... 30 more
Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	... 37 more {code}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41378&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17846",,,,,,,,,,,,,,,,,,,,,FLINK-29427,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 03:39:09 UTC 2022,,,,,,,,,,"0|z18xo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 03:39;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43111&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=18169];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessDataStreamStreamingTests.test_process_function unstable,FLINK-29461,13483752,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,29/Sep/22 03:35,13/Dec/22 03:12,04/Jun/24 20:41,13/Dec/22 03:12,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,API / Python,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-29T02:10:45.3571648Z Sep 29 02:10:45 self = <pyflink.datastream.tests.test_data_stream.ProcessDataStreamStreamingTests testMethod=test_process_function>
2022-09-29T02:10:45.3572279Z Sep 29 02:10:45 
2022-09-29T02:10:45.3572810Z Sep 29 02:10:45     def test_process_function(self):
2022-09-29T02:10:45.3573495Z Sep 29 02:10:45         self.env.set_parallelism(1)
2022-09-29T02:10:45.3574148Z Sep 29 02:10:45         self.env.get_config().set_auto_watermark_interval(2000)
2022-09-29T02:10:45.3580634Z Sep 29 02:10:45         self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
2022-09-29T02:10:45.3583194Z Sep 29 02:10:45         data_stream = self.env.from_collection([(1, '1603708211000'),
2022-09-29T02:10:45.3584515Z Sep 29 02:10:45                                                 (2, '1603708224000'),
2022-09-29T02:10:45.3585957Z Sep 29 02:10:45                                                 (3, '1603708226000'),
2022-09-29T02:10:45.3587132Z Sep 29 02:10:45                                                 (4, '1603708289000')],
2022-09-29T02:10:45.3588094Z Sep 29 02:10:45                                                type_info=Types.ROW([Types.INT(), Types.STRING()]))
2022-09-29T02:10:45.3589090Z Sep 29 02:10:45     
2022-09-29T02:10:45.3589949Z Sep 29 02:10:45         class MyProcessFunction(ProcessFunction):
2022-09-29T02:10:45.3590710Z Sep 29 02:10:45     
2022-09-29T02:10:45.3591856Z Sep 29 02:10:45             def process_element(self, value, ctx):
2022-09-29T02:10:45.3592873Z Sep 29 02:10:45                 current_timestamp = ctx.timestamp()
2022-09-29T02:10:45.3593862Z Sep 29 02:10:45                 current_watermark = ctx.timer_service().current_watermark()
2022-09-29T02:10:45.3594915Z Sep 29 02:10:45                 yield ""current timestamp: {}, current watermark: {}, current_value: {}""\
2022-09-29T02:10:45.3596201Z Sep 29 02:10:45                     .format(str(current_timestamp), str(current_watermark), str(value))
2022-09-29T02:10:45.3597089Z Sep 29 02:10:45     
2022-09-29T02:10:45.3597942Z Sep 29 02:10:45         watermark_strategy = WatermarkStrategy.for_monotonous_timestamps()\
2022-09-29T02:10:45.3599260Z Sep 29 02:10:45             .with_timestamp_assigner(SecondColumnTimestampAssigner())
2022-09-29T02:10:45.3600611Z Sep 29 02:10:45         data_stream.assign_timestamps_and_watermarks(watermark_strategy)\
2022-09-29T02:10:45.3601877Z Sep 29 02:10:45             .process(MyProcessFunction(), output_type=Types.STRING()).add_sink(self.test_sink)
2022-09-29T02:10:45.3603527Z Sep 29 02:10:45         self.env.execute('test process function')
2022-09-29T02:10:45.3604445Z Sep 29 02:10:45         results = self.test_sink.get_results()
2022-09-29T02:10:45.3605684Z Sep 29 02:10:45         expected = [""current timestamp: 1603708211000, current watermark: ""
2022-09-29T02:10:45.3607157Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=1, f1='1603708211000')"",
2022-09-29T02:10:45.3608256Z Sep 29 02:10:45                     ""current timestamp: 1603708224000, current watermark: ""
2022-09-29T02:10:45.3609650Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=2, f1='1603708224000')"",
2022-09-29T02:10:45.3610854Z Sep 29 02:10:45                     ""current timestamp: 1603708226000, current watermark: ""
2022-09-29T02:10:45.3612279Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=3, f1='1603708226000')"",
2022-09-29T02:10:45.3613382Z Sep 29 02:10:45                     ""current timestamp: 1603708289000, current watermark: ""
2022-09-29T02:10:45.3615683Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=4, f1='1603708289000')""]
2022-09-29T02:10:45.3617687Z Sep 29 02:10:45 >       self.assert_equals_sorted(expected, results)
2022-09-29T02:10:45.3618620Z Sep 29 02:10:45 
2022-09-29T02:10:45.3619425Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:986: 
2022-09-29T02:10:45.3620424Z Sep 29 02:10:45 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-09-29T02:10:45.3621886Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:66: in assert_equals_sorted
2022-09-29T02:10:45.3622847Z Sep 29 02:10:45     self.assertEqual(expected, actual)
2022-09-29T02:10:45.3624658Z Sep 29 02:10:45 E   AssertionError: Lists differ: [""cur[414 chars]ark: -9223372036854775808, current_value: Row([22 chars]0')""] != [""cur[414 chars]ark: 1603708225999, current_value: Row(f0=4, f[15 chars]0')""]
2022-09-29T02:10:45.3625881Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3626591Z Sep 29 02:10:45 E   First differing element 3:
2022-09-29T02:10:45.3627726Z Sep 29 02:10:45 E   ""curr[44 chars]ark: -9223372036854775808, current_value: Row([21 chars]00')""
2022-09-29T02:10:45.3628758Z Sep 29 02:10:45 E   ""curr[44 chars]ark: 1603708225999, current_value: Row(f0=4, f[14 chars]00')""
2022-09-29T02:10:45.3629276Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3629842Z Sep 29 02:10:45 E   Diff is 753 characters long. Set self.maxDiff to None to see it.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 03:12:35 UTC 2022,,,,,,,,,,"0|z18xo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/22 08:32;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=25187;;;","02/Nov/22 08:12;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42724&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=25197;;;","04/Nov/22 01:58;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42803&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24482;;;","04/Nov/22 14:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42827&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25085;;;","07/Nov/22 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42858&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28095;;;","07/Nov/22 08:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42867&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27223;;;","08/Nov/22 07:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42908&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27019;;;","11/Nov/22 09:14;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43043&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=25767;;;","21/Nov/22 04:53;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43332&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2;;;","22/Nov/22 07:36;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43367&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","22/Nov/22 07:49;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43369&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf;;;","22/Nov/22 08:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43167&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24422;;;","22/Nov/22 08:41;mapohl;[~hxbks2ks] any updates on this issue?;;;","24/Nov/22 07:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43436&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27416;;;","25/Nov/22 04:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28464;;;","28/Nov/22 09:40;mapohl;Same build, two failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43512&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29899
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43512&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27869;;;","28/Nov/22 13:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43541&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=26458;;;","29/Nov/22 06:03;mapohl;Same build, multiple failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27194
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29488;;;","29/Nov/22 08:41;hxb;[~mapohl] Sorry for the late reply, I will take a look into this issue in these two days.;;;","30/Nov/22 06:59;mapohl;No worries. Thanks for picking it up. :)

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43604&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=29496;;;","01/Dec/22 09:03;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43636&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27587;;;","02/Dec/22 10:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43662&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28078;;;","02/Dec/22 10:56;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43664&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28979;;;","05/Dec/22 09:54;mapohl;Same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=27154
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=26932;;;","05/Dec/22 11:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=28522;;;","05/Dec/22 11:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43711&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=29177;;;","06/Dec/22 08:32;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43742&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28926;;;","07/Dec/22 04:23;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43771&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=28618;;;","09/Dec/22 08:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43817&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=28358;;;","09/Dec/22 08:07;mapohl;[~dianfu] [~hxbks2ks] any updates on that one? Or is there anyone else we can ping to have a look at it?;;;","12/Dec/22 10:14;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43871&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","12/Dec/22 11:40;hxb;[~mapohl] I have prepared a PR to make the test more stable.;;;","13/Dec/22 03:12;hxb;Merged into master via 4df6a398bbe2a9de7c23977176789e54cc0848fa

Merged into release-1.16 via 36c86f1c6cd34482c2eb3cc939d348e08fd08a2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsResultPartitionTest.testRelease failed with AssertionFailedError,FLINK-29460,13483740,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,hxbks2ks,hxbks2ks,29/Sep/22 02:18,30/Sep/22 01:25,04/Jun/24 20:41,30/Sep/22 01:25,1.16.0,,,,,,,1.16.0,,,,Runtime / Network,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-29T01:52:44.4460454Z Sep 29 01:52:44 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testRelease  Time elapsed: 0.271 s  <<< FAILURE!
2022-09-29T01:52:44.4461655Z Sep 29 01:52:44 org.opentest4j.AssertionFailedError: 
2022-09-29T01:52:44.4462489Z Sep 29 01:52:44 
2022-09-29T01:52:44.4463018Z Sep 29 01:52:44 expected: 10
2022-09-29T01:52:44.4463549Z Sep 29 01:52:44  but was: 6
2022-09-29T01:52:44.4464382Z Sep 29 01:52:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-09-29T01:52:44.4465591Z Sep 29 01:52:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-09-29T01:52:44.4466937Z Sep 29 01:52:44 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-09-29T01:52:44.4468568Z Sep 29 01:52:44 	at org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest.testRelease(HsResultPartitionTest.java:245)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8811",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 30 01:25:51 UTC 2022,,,,,,,,,,"0|z18xlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 02:19;hxbks2ks; [~Weijie Guo] Could you help take a look? Thx.;;;","29/Sep/22 03:01;Weijie Guo;[~hxbks2ks] Thanks for reporting this unstable test, I think this is caused by FLINK-29425 and I will fix this problem as soon as possible.;;;","30/Sep/22 01:25;xtsong;- master (1.17): df1681c7ef3f5a946f626c86748a9fe46ee687d7
- release-1.16: 729168f0edde39b5f2313788427a7f8d1a487bd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sink v2 has bugs in supporting legacy v1 implementations with global committer,FLINK-29459,13483703,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gaoyunhaii,gaoyunhaii,gaoyunhaii,28/Sep/22 15:15,11/Mar/24 12:44,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,1.20.0,,,,API / DataStream,,,,0,,,,,"Currently when supporting Sink implementation using version 1 interface, there are issues after restoring from a checkpoint after failover:
 # In global committer operator, when restoring SubtaskCommittableManager, the subtask id is replaced with the one in the current operator. This means that the id originally is the id of the sender task (0 ~ N - 1), but after restoring it has to be 0. This would cause Duplication Key exception during restoring.
 # For Committer operator, the subtaskId of CheckpointCommittableManagerImpl is always restored to 0 after failover for all the subtasks. This makes the summary sent to the Global Committer is attached with wrong subtask id.
 # For Committer operator, the checkpoint id of SubtaskCommittableManager is always restored to 1 after failover, this make the following committable sent to the global committer is attached with wrong checkpoint id. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29509,FLINK-29512,FLINK-29583,FLINK-29627,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 20:54:49 UTC 2023,,,,,,,,,,"0|z18xd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 12:31;fpaul;[~gaoyunhaii] thanks for your analysis. I am currently looking into the issues, and I think it is a good idea to split the different problems into different tickets. 

I already created https://issues.apache.org/jira/browse/FLINK-29509 to fix the subtask id problem during recovery.

Let me know if you have already started with that.;;;","05/Oct/22 12:35;fpaul;It also looks like the first and second point are the same problem, aren't they?;;;","20/Oct/22 15:18;KristoffSC;FYI ticets
29509
29512
29627

are fixing issue with Task manager recovery for Sink architecture with global committer.

The 29583 is about recovering Flink 1.14 unified sinks committer state and migrate it to the extended unified model.;;;","24/Oct/22 12:34;gaoyunhaii;Hi [~fpaul] [~KristoffSC] Very thanks for fixing the issues and very sorry for missed the previous notifications for in the holiday then. Regarding the current sink v2 mechanism I have some more thoughts:

Currently we rely on the CommittableSummary and CommittableWithLineage message to coordinate between Writers and Committers. For each checkpoint, each Writer subtask would first emit a CommittableSummary to the Committers, which contains the number of Committables to send. Then the Writer subtask emit that number of CommittableWithLineage messages to the Committers. The Committers relies on the number in the summary to detect if it has received all the Committables from each write subtask. But the mechanism contains some issues:
 # It could only support the partitioner with one target for each source between Writer and Committer, like forward / rescale. If for the long run we want to support the Committers with arbitrary parallelism, it might cause issues if Writer and Committer have different parallelism. Similarly it also complicate the authors of connectors that using PreCommitterTopolgy. 
 # With unaligned checkpoint and rescale after recovering, if some CommittableSummary messages have been processed and stored in the snapshot, but the corresponding CommittableWithLineage messages have been assigned to other tasks, the number of Committables would be not correct. 

One possible alternative might be instead of relying on numbers, we might first emit the Committables, then followed by a broadcast message that confirms the end of a checkpoint. The Committable would know that it has received all the Committables after received the Confirmed messages from all the previous tasks. The mechanism is a bit like how watermark works. Then for the above two issues:


 # It would support all the partitioners. 
 # For unaligned checkpoint and rescaling case, we could simply commit all the Committables with the startup id and ignore all the confirmation messages of the same checkpoint id on startup. We could then wait for the confirmation message of the next checkpoint id to mark all the previous checkpoints as finished. 

How do you think about this? Sorry if I overlook something. ;;;","21/Jul/23 12:52;martijnvisser;[~gaoyunhaii] Should we restart the discussion for fixing this problem?;;;","24/Jul/23 05:28;gaoyunhaii;Hi [~martijnvisser]  Logically it does not involve the change of API, do you think we should move the discussion to the thread or we could continue the discussion under this issue? Both works from my side. ;;;","30/Aug/23 20:54;martijnvisser;[~gaoyunhaii] Sorry for the late reply, let's continue the discussion here, since we don't have to involve the change of the API. I know that [~tzulitai] is also interested in this topic, so let us know what you think!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When two tables have the same field, do not specify the table name,Exception will be thrown：SqlValidatorException :Column 'currency' is ambiguous",FLINK-29458,13483671,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zmm_flink,zmm_flink,zmm_flink,28/Sep/22 12:58,24/Nov/22 10:35,04/Jun/24 20:41,31/Oct/22 06:34,1.16.0,,,,,,,1.16.1,1.17.0,,,Documentation,Table SQL / API,,,0,pull-request-available,,,,"When two tables are join, the two tables have the same field. When querying select, an exception will be thrown if the table name is not specified

exception content

Column 'currency' is ambiguous。

!image-2022-09-28-21-00-22-733.png!

 

!image-2022-09-28-21-00-01-302.png!

!image-2022-09-28-21-00-09-054.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/22 13:00;zmm_flink;image-2022-09-28-21-00-01-302.png;https://issues.apache.org/jira/secure/attachment/13049871/image-2022-09-28-21-00-01-302.png","28/Sep/22 13:00;zmm_flink;image-2022-09-28-21-00-09-054.png;https://issues.apache.org/jira/secure/attachment/13049872/image-2022-09-28-21-00-09-054.png","28/Sep/22 13:00;zmm_flink;image-2022-09-28-21-00-22-733.png;https://issues.apache.org/jira/secure/attachment/13049873/image-2022-09-28-21-00-22-733.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 10:35:13 UTC 2022,,,,,,,,,,"0|z18x60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 13:01;jark;[~zmm_flink] I assigned this ticket to you. Feel free to open pull request. ;;;","28/Sep/22 13:10;martijnvisser;I believe this the same as https://github.com/apache/flink/pull/20896/;;;","15/Oct/22 14:45;zmm_flink;[~jark]

It has been modified. Please refer to [GitHub Pull Request #20983|https://github.com/apache/flink/pull/20983];;;","31/Oct/22 06:34;jark;Fixed in master: 97fbb701314205fd1d51d7edb1f6ef7a27f880c7;;;","24/Nov/22 10:35;martijnvisser;Fixed in release-1.16: ac17b3a81add99f80fe414a58001404af879ca71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a uid(hash) remapping function,FLINK-29457,13483666,13483651,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Sep/22 12:32,11/Oct/22 11:12,04/Jun/24 20:41,11/Oct/22 11:12,,,,,,,,1.17.0,,,,API / State Processor,,,,0,pull-request-available,,,,Expose functionality for modifying the uid[hash] of a state.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 11:12:51 UTC 2022,,,,,,,,,,"0|z18x4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 11:12;chesnay;master: 8ccca78ca0db23f7965fe77ea9b57d8391b6f583;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add methods that accept OperatorIdentifier,FLINK-29456,13483664,13483651,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Sep/22 12:31,05/Oct/22 14:23,04/Jun/24 20:41,05/Oct/22 14:23,,,,,,,,1.17.0,,,,API / State Processor,,,,0,,,,,Add new variants of all methods in the SavepointReader/-Writer that accept an OperatorIdentifier.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 05 14:23:55 UTC 2022,,,,,,,,,,"0|z18x4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 14:23;chesnay;master: 8d72490377551a35851a3319c0f49b408d31a566;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add OperatorIdentifier,FLINK-29455,13483663,13483651,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Sep/22 12:30,04/Oct/22 09:17,04/Jun/24 20:41,04/Oct/22 09:17,,,,,,,,1.17.0,,,,API / State Processor,,,,0,pull-request-available,,,,"Add a class for identifying operators, that supports both uids and uidhashes, and integrate into the low-level APIs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 04 09:17:52 UTC 2022,,,,,,,,,,"0|z18x48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/22 09:17;chesnay;master: 306c3c4ce3025bb29b84e5f6045cb3c745feb893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deduplicate code in SavepointReader,FLINK-29454,13483652,13483651,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Sep/22 11:44,30/Sep/22 08:43,04/Jun/24 20:41,30/Sep/22 08:43,,,,,,,,1.17.0,,,,API / State Processor,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 30 08:43:38 UTC 2022,,,,,,,,,,"0|z18x1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 08:43;chesnay;master: 178cc4e56fb94fe0aa68aeaee780b6227625768d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add uidHash support to State Processor API ,FLINK-29453,13483651,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Sep/22 11:43,11/Oct/22 11:12,04/Jun/24 20:41,11/Oct/22 11:12,,,,,,,,1.17.0,,,,API / State Processor,,,,1,,,,,"The state process API is currently limited to working with uids.

We should change this since this is a good application for the API.

The API should be extended to support uidHashes wherever a uid is support, and we should add a method to map uid[hashes] to a different uid[hash].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 11:43:39.0,,,,,,,,,,"0|z18x1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RetryOn*ExtensionTest is implemented in a way that doesn't allow the successful execution of individual tests,FLINK-29452,13483644,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,mapohl,mapohl,28/Sep/22 11:06,22/Nov/23 10:15,04/Jun/24 20:41,22/Nov/23 10:15,1.15.3,1.16.0,1.17.0,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,stale-assigned,starter,,"{{RetryOn*ExtensionTest}} does the evaluation of the tests assuming that all tests are executed. Ideally, individual tests should be executable as well without reporting failures.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 22 10:15:46 UTC 2023,,,,,,,,,,"0|z18x00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 13:40;rskraba;Can you assign this to me?;;;","13/Oct/22 13:47;mapohl;Sure, thanks for picking it up. :-);;;","15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","22/Nov/23 10:15;mapohl;master: [2378babf86cd298525ef58c41f019d5c4d900383|https://github.com/apache/flink/commit/2378babf86cd298525ef58c41f019d5c4d900383];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push down group by partition column for Aggregate (Min/Max/Count) for Parquet and Orc,FLINK-29451,13483630,13444611,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,28/Sep/22 09:30,28/Sep/22 09:30,04/Jun/24 20:41,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 09:30:05.0,,,,,,,,,,"0|z18wx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Push down filter by partition column for Aggregate (Min/Max/Count) for Parquet and Orc,FLINK-29450,13483629,13444611,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,28/Sep/22 09:27,28/Sep/22 09:30,04/Jun/24 20:41,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 09:27:07.0,,,,,,,,,,"0|z18www:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregate (Min/Max/Count) push down for Parquet	,FLINK-29449,13483625,13444611,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,28/Sep/22 09:21,28/Sep/22 09:21,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 09:21:14.0,,,,,,,,,,"0|z18ww0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregate (Min/Max/Count) push down for ORC,FLINK-29448,13483624,13444611,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,28/Sep/22 09:20,28/Sep/22 09:20,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 09:20:17.0,,,,,,,,,,"0|z18wvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for federation query  using Hive dialect ,FLINK-29447,13483619,13477344,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,28/Sep/22 09:03,28/Sep/22 09:05,04/Jun/24 20:41,,,,,,,,,,,,,Connectors / Hive,Documentation,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29337,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 09:03:59.0,,,,,,,,,,"0|z18wuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Calcite classes which were fixed in 1.24,FLINK-29446,13483617,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,28/Sep/22 08:47,11/Oct/22 06:47,04/Jun/24 20:41,11/Oct/22 06:47,1.15.2,1.16.0,,,,,,1.17.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"{{SqlDotOperator}}, {{SqlItemOperator}}, {{AliasNamespace}} were introduced as copies from Calcite and with a fix inside at https://github.com/apache/flink/pull/12649
At the same side the fixed was applied in Calcite itself in 1.24. https://issues.apache.org/jira/browse/CALCITE-4085

Since now Flink depends on 1.26 the fix is included and there is no need to have these classes in Flink repo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 06:47:28 UTC 2022,,,,,,,,,,"0|z18wu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 06:47;twalthr;Fixed in master: 90f4239f4af58611ae4922a6b8d032a604cbc650;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When the decimal table column field is printed，program is error,FLINK-29445,13483609,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,maskainv,maskainv,28/Sep/22 08:14,30/Sep/22 01:55,04/Jun/24 20:41,30/Sep/22 01:55,1.12.0,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,0,flink,table-api,,,"runtime is java

error code：

tEnv.executeSql(""select `openScore` from demo"").print();

openScore field is BigDecimal

error log is:

Exception in thread ""main"" java.lang.ClassCastException: org.apache.flink.table.types.logical.LegacyTypeInformationType cannot be cast to org.apache.flink.table.types.logical.DecimalType
    at org.apache.flink.table.utils.PrintUtils.columnWidthsByType(PrintUtils.java:264)
    at org.apache.flink.table.utils.PrintUtils.printAsTableauForm(PrintUtils.java:127)
    at org.apache.flink.table.api.internal.TableResultImpl.print(TableResultImpl.java:149)
    at com.mask.maintest.TestTableEnv.main(TestTableEnv.java:74)

 

source code position:

class : PrintUtils 

line: 177

len = ((DecimalType)type).getPrecision() + 2;

 

!image-2022-09-28-16-18-17-953.png!","<flink.version>1.12.0</flink.version>

flink-table-api-java-bridge_2.11

flink-table-planner_2.11

flink-streaming-scala_2.11",604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/22 08:18;maskainv;image-2022-09-28-16-18-17-953.png;https://issues.apache.org/jira/secure/attachment/13049858/image-2022-09-28-16-18-17-953.png","28/Sep/22 08:04;maskainv;pom.xml;https://issues.apache.org/jira/secure/attachment/13049857/pom.xml",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 30 01:54:49 UTC 2022,,,,,,,,,,"0|z18wsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 09:23;martijnvisser;[~maskainv] Can you please verify this with Flink 1.15, since the Flink community doesn't support Flink 1.12 anymore;;;","30/Sep/22 01:54;maskainv;ok I will try.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup release scripts,FLINK-29444,13483608,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,28/Sep/22 08:10,14/Nov/22 15:28,04/Jun/24 20:41,14/Nov/22 15:28,,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,See https://issues.apache.org/jira/browse/FLINK-29320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 14 15:27:49 UTC 2022,,,,,,,,,,"0|z18ws8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 15:27;dannycranmer;Merged commit [{{dc09993}}|https://github.com/apache/flink-connector-aws/commit/dc099938054398045cfceb87baaa95f853e5f1c8] into main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replicate packaging tests,FLINK-29443,13483607,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,darenwkt,dannycranmer,dannycranmer,28/Sep/22 08:09,30/Jun/23 09:50,04/Jun/24 20:41,30/Jun/23 09:50,,,,,,,,aws-connector-4.2.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,See https://issues.apache.org/jira/browse/FLINK-29316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 09:50:01 UTC 2023,,,,,,,,,,"0|z18ws0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 09:50;dannycranmer; merged commit [{{16b0236}}|https://github.com/apache/flink-connector-aws/commit/16b02363591f335ff0b588ddcb64b2a227be7ebc] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup license checks,FLINK-29442,13483606,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,darenwkt,dannycranmer,dannycranmer,28/Sep/22 08:09,19/Oct/22 06:43,04/Jun/24 20:41,19/Oct/22 06:43,,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,,,,,See https://issues.apache.org/jira/browse/FLINK-29310,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 19 06:42:49 UTC 2022,,,,,,,,,,"0|z18wrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 06:42;dannycranmer;Merged commit [{{6a2b489}}|https://github.com/apache/flink-connector-dynamodb/commit/6a2b489fb82a6681de73dac44b3d81d67c447d67] into main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup dependency convergence check,FLINK-29441,13483599,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,28/Sep/22 07:33,19/Oct/22 06:43,04/Jun/24 20:41,28/Sep/22 08:04,,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"Following the Flink repo:
 * Convergence should be disabled by default
 * Enabled via {{-Pcheck-convergence}}
 * Enabled on CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 08:04:09 UTC 2022,,,,,,,,,,"0|z18wq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 08:04;dannycranmer;Merged commit [{{4357607}}|https://github.com/apache/flink-connector-dynamodb/commit/43576072f540b9086f0c628ee81104d5aec07cc7] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup CI Logging,FLINK-29440,13483595,13483592,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,dannycranmer,dannycranmer,dannycranmer,28/Sep/22 07:15,21/Nov/22 16:21,04/Jun/24 20:41,21/Nov/22 16:21,,,,,,,,,,,,,,,,0,pull-request-available,,,,Configure Log4J for CI builds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 16:21:43 UTC 2022,,,,,,,,,,"0|z18wpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 16:21;dannycranmer;This is not required;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DynamoDB Project Readme,FLINK-29439,13483593,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,28/Sep/22 07:03,19/Oct/22 06:43,04/Jun/24 20:41,28/Sep/22 07:25,,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"Copy and update from the ElasticSearch connector repo:

- https://github.com/apache/flink-connector-elasticsearch/blob/main/README.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 07:24:57 UTC 2022,,,,,,,,,,"0|z18wow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 07:24;dannycranmer;Merged commit [{{9060c82}}|https://github.com/apache/flink-connector-dynamodb/commit/9060c823279f0766c075b807b371dbe4f6b7bf98] into main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup AWS Connectors Project Structure,FLINK-29438,13483592,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,28/Sep/22 07:01,23/Nov/22 11:51,04/Jun/24 20:41,23/Nov/22 11:51,,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,,,,,"Umbrella task for AWS Connector repo setup.

Please put all AWS connector setup related tasks under this task. 

Vote thread for decision to create a single repor: https://lists.apache.org/thread/8n5gnffthygbssshxcrpq5pz8wwkqzvv",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 07:01:14.0,,,,,,,,,,"0|z18woo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The partition of data before and after the Kafka Shuffle are not aligned,FLINK-29437,13483589,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,28/Sep/22 06:52,21/Nov/22 12:55,04/Jun/24 20:41,21/Nov/22 12:55,1.15.3,,,,,,,1.17.0,,,,API / DataStream,Connectors / Kafka,,,0,pull-request-available,,,,"I notice that the key group range in consumer side of Kafka Shuffle is not aligned with the producer side, there are two problems:
 # The data partitioning of the sink(producer) is exactly the same way as a keyed stream that as the same maximum parallelism as the number of kafka partitions does, but in consumer side the number of partitions and key groups are not the same.
 # There is a distribution of assigning kafka partitions to consumer subtasks (See KafkaTopicPartitionAssigner#assign), but the producer of Kafka Shuffle simply assume the partition index equals the subtask index. e.g.

       !image-2022-09-28-14-32-28-116.png|width=1133,height=274!

My proposed change:
 # Set the max parallelism of the key stream in consumer side as the number of kafka partitions. 
 # Use the same method when assigning kafka partitions to consumer subtasks to maintain a map from subtasks to kafka partitions, which is used by the producer to insert into the right partition for data for a subtask. i.e.

       !image-2022-09-28-14-35-47-954.png|width=1030,height=283!",,,,,,,,,,,,FLINK-29430,,,,,,,,,,,,,,,,,,,,,FLINK-21317,,,,,,,"28/Sep/22 06:32;zakelly;image-2022-09-28-14-32-28-116.png;https://issues.apache.org/jira/secure/attachment/13049852/image-2022-09-28-14-32-28-116.png","28/Sep/22 06:35;zakelly;image-2022-09-28-14-35-47-954.png;https://issues.apache.org/jira/secure/attachment/13049851/image-2022-09-28-14-35-47-954.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 12:55:49 UTC 2022,,,,,,,,,,"0|z18wo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 07:57;martijnvisser;[~renqs] WDYT?;;;","28/Sep/22 09:42;renqs;[~martijnvisser] This feature was initially implemented by [~yuanmei], so maybe she could make some comments on this. ;;;","30/Sep/22 10:26;zakelly;After offline sync with [~ym] , we agree to fix this as I proposed.;;;","21/Nov/22 12:55;ym;merged commit [{{0b81668}}|https://github.com/apache/flink/commit/0b816680e51283d261eb7b7bce560dd1641691ce] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Spotless Maven Plugin to 2.27.1,FLINK-29436,13483567,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,tison,tison,28/Sep/22 02:41,20/Feb/24 06:21,04/Jun/24 20:41,29/Sep/22 09:00,,,,,,,,1.17.0,connector-parent-1.1.0,,,Build System,Connectors / Parent,,,0,pull-request-available,,,,This blocker is fixed by: https://github.com/diffplug/spotless/pull/1224 and https://github.com/diffplug/spotless/pull/1228.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 09:00:46 UTC 2022,,,,,,,,,,"0|z18wj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/22 09:00;chesnay;master: e1e9ee81d5a0cca12156aaf9640c79de5e48f118;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
securityConfiguration supports dynamic configuration,FLINK-29435,13483564,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Bo Cui,Bo Cui,Bo Cui,28/Sep/22 02:11,26/Oct/22 01:45,04/Jun/24 20:41,26/Oct/22 01:45,,,,,,,,1.17.0,,,,Command Line Client,,,,0,pull-request-available,,,,"when different tenants submit jobs using the same _flink-conf.yaml_, the same user is displayed on the Yarn page. 
_SecurityConfiguration_ does not support dynamic configuration. Therefore, the user displayed on the Yarn page is the _security.kerberos.login.principal_ in the _flink-conf.yaml_.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 01:45:04 UTC 2022,,,,,,,,,,"0|z18wig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 01:45;xtsong;master (1.17): a8c72ccb98e6eaa59d878d8182f0d1411bf577c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AlgoOperator for Splitter,FLINK-29434,13483561,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,28/Sep/22 01:39,08/Nov/22 09:08,04/Jun/24 20:41,08/Nov/22 09:08,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add AlgoOperator for Splitter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-28 01:39:17.0,,,,,,,,,,"0|z18whs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Auth through the builder pattern in Pulsar connector,FLINK-29433,13483517,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,syhily,syhily,syhily,27/Sep/22 17:53,18/Oct/22 05:29,04/Jun/24 20:41,18/Oct/22 05:29,1.17.0,,,,,,,1.17.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,"Currently in order to use auth with the Flink Connector you needs to do so through the {{setConfig}} method.
It would be nice if similar to the client API we can add methods inside the builder pattern.
Example:

{code:java}
builder.authentication(new AuthenticationToken(""""))
{code}

We can do something similar for the connector instead of having to do:

{code:java}
PulsarSource.builder()
    .setConfig(PulsarOptions.PULSAR_AUTH_PLUGIN_CLASS_NAME, ""org.apache.pulsar.client.impl.auth.oauth2.AuthenticationOAuth2"")
    .setConfig(PulsarOptions.PULSAR_AUTH_PARAMS, ""{""privateKey"":""...""})
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 18 05:29:55 UTC 2022,,,,,,,,,,"0|z18w80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 05:29;tison;master via 25c429734f5beeafda482463e2b61326b7216261;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace GenericUDFNvl with GenericUDFCoalesce,FLINK-29432,13483437,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,prabhujoseph,prabhujoseph,27/Sep/22 11:04,20/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,Connectors / Hive,,,,0,auto-deprioritized-major,pull-request-available,,,"Hive NVL() function has many issues like [HIVE-25193|https://issues.apache.org/jira/browse/HIVE-25193] and it is retired [HIVE-20961|https://issues.apache.org/jira/browse/HIVE-20961]. Our internal hive distribution has the fix for HIVE-20961. With this fix, Flink Build is failing with below as there is no more GenericUDFNvl in Hive. This needs to be replaced with GenericUDFCoalesce.

{code}
[INFO] /codebuild/output/src366217558/src/build/flink/rpm/BUILD/flink-1.15.2/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/copy/HiveParserDefaultGraphWalker.java: Recompile with -Xlint:unchecked for details.
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------

[ERROR] /codebuild/output/src366217558/src/build/flink/rpm/BUILD/flink-1.15.2/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/HiveParserTypeCheckProcFactory.java:[75,45] cannot find symbol
  symbol:   class GenericUDFNvl
  location: package org.apache.hadoop.hive.ql.udf.generic
[ERROR] /codebuild/output/src366217558/src/build/flink/rpm/BUILD/flink-1.15.2/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/HiveParserTypeCheckProcFactory.java:[1216,41] cannot find symbol
  symbol:   class GenericUDFNvl
  location: class org.apache.flink.table.planner.delegation.hive.HiveParserTypeCheckProcFactory.DefaultExprProcessor
[ERROR] /codebuild/output/src366217558/src/build/flink/rpm/BUILD/flink-1.15.2/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/planner/delegation/hive/copy/HiveParserSemanticAnalyzer.java:[231,26] constructor GlobalLimitCtx in class org.apache.hadoop.hive.ql.parse.GlobalLimitCtx cannot be applied to given types;
  required: org.apache.hadoop.hive.conf.HiveConf
  found: no arguments
  reason: actual and formal argument lists differ in length
[INFO] 3 errors
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:18 UTC 2023,,,,,,,,,,"0|z18vqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 01:47;luoyuxia;Thanks for contribution. But to put it briefly, I prefer not to change it immediately.

Actally,  HIVE-20961 is a patch of Hive 4.0. Hive 4.0 is not released and Flink doesn't provide official support for hive 4. Of course it should be fixed if we want to support hive4, but at least, seems we have no such plan in short term. 

Also, we can't just only change replace `GenericUDFNvl` with `GenericUDFCoalesce` to fix it for it may bring other bugs to Hive dialect as reported in [Hive-24902| https://issues.apache.org/jira/browse/HIVE-24902]. Also, I'm doubt it may bring other bugs that have't been found.

 

For your problem, you can change in your flink distribution.

 ;;;","11/Oct/22 05:48;samrat007;Thanks [~luoyuxia] for your view on the issue ! 

I completely agree with the fact that there are known bugs with `GenericUDFCoalesce` and still haven't reached to complete maturity. We wont go ahead with the proposed changes and keep it on hold untill flink start supporting hive-4.;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during job graph serialization lock up client,FLINK-29431,13483417,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,chesnay,chesnay,27/Sep/22 09:44,28/Sep/22 09:24,04/Jun/24 20:41,28/Sep/22 09:24,1.16.0,,,,,,,1.16.0,,,,Client / Job Submission,,,,0,pull-request-available,,,,"The parallel serialization introduced in FLINK-26675 does not properly handle errors.

{{StreamConfig#triggerSerializationAndReturnFuture}} returns a future that is never completed if the serialization fails, because all we do is throw an exception but that isn't propagated anywhere.

As a result {{StreamingJobGraphGenerator#createJobGraph}} blocks indefinitely.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26675,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 09:24:03 UTC 2022,,,,,,,,,,"0|z18vm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 12:32;Weijie Guo;[~chesnay] Thank you for reporting this bug, I'll take a look.;;;","28/Sep/22 09:24;xtsong;- master (1.17): 853ea452bff191fb0416017aeb5acd2046665996
- release-1.16: f66cb67687e7df355006630d50b81307d281c150;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sanity check in InternalKeyContextImpl#setCurrentKeyGroupIndex,FLINK-29430,13483373,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,27/Sep/22 08:12,02/Dec/22 09:13,04/Jun/24 20:41,02/Dec/22 09:13,1.15.3,,,,,,,1.17.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,,"Currently the HeapStateBackend check whether the current key group index is a valid one while the RocksDBStateBackend will not. When using HeapStateBackend, if the user uses a non-deterministic shuffle key, an exception is thrown as follows:

 
{code:java}
java.lang.IllegalArgumentException: Key group 84 is not in KeyGroupRange{startKeyGroup=32, endKeyGroup=63}. Unless you're directly using low level state access APIs, this is most likely caused by non-deterministic shuffle key (hashCode and equals implementation).
    at org.apache.flink.runtime.state.KeyGroupRangeOffsets.newIllegalKeyGroupException(KeyGroupRangeOffsets.java:37)
    at org.apache.flink.runtime.state.heap.StateTable.getMapForKeyGroup(StateTable.java:305)
    at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:261)
    at org.apache.flink.runtime.state.heap.StateTable.get(StateTable.java:143)
    at org.apache.flink.runtime.state.heap.HeapValueState.value(HeapValueState.java:72)
    at com.alibaba.ververica.flink.state.benchmark.wordcount.WordCount$MixedFlatMapper.flatMap(WordCount.java:169)
    at com.alibaba.ververica.flink.state.benchmark.wordcount.WordCount$MixedFlatMapper.flatMap(WordCount.java:160)
    at org.apache.flink.streaming.api.operators.StreamFlatMap.processElement(StreamFlatMap.java:47)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:135)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:106)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:526)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:811)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:760)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:933)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568)
    at java.lang.Thread.run(Thread.java:750)
 {code}
However, the RocksDBStateBackend will run without an exception. The wrong key group index will cause a state correctness problem, so it is better to do a check in {_}InternalKeyContextImpl#{_}{_}setCurrentKeyGroupIndex{_}, and throw an exception immediately.

 ",,,,,,,,,,,FLINK-29437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 02 09:13:14 UTC 2022,,,,,,,,,,"0|z18vcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 08:25;zakelly;Correct me if I'm wrong.

If it is valid, I would like to fix it.;;;","02/Dec/22 09:13;yunta;merged in master: 1af9446248677b9540ed5d53bd2b42f3b724f7b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DataType for Flink ML linear algorithm classes,FLINK-29429,13483369,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfengzhou,yunfengzhou,27/Sep/22 08:05,27/Sep/22 08:05,04/Jun/24 20:41,,ml-2.1.0,,,,,,,,,,,Library / Machine Learning,,,,0,,,,,"DataType instances are used in Table API when creating Tables or Table UDFs. There are helper functions like `DataTypes.of()` that can be used to get the DataType for Flink ML classes like DenseVector in java, but this method is not applicable in pyflink, which seems not to support custom DataTypes yet. Thus we should add DataType subclasses for them in Flink ML.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-27 08:05:15.0,,,,,,,,,,"0|z18vbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
got an ambigious exception in flinksql,FLINK-29428,13483360,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,SpongebobZ,SpongebobZ,27/Sep/22 07:52,28/Sep/22 06:19,04/Jun/24 20:41,,1.14.3,,,,,,,,,,,Table SQL / API,,,,0,,,,,"when I execute my lengthy flinksql that contains 5 joins, I got this exception:

I have no any idea to solve this issue and asking for your help.
{code:java}
Caused by: java.lang.RuntimeException: Hash join exceeded maximum number of recursions, without reducing partitions enough to be memory resident. Probably cause: Too many duplicate keys.
        at org.apache.flink.table.runtime.hashtable.BinaryHashTable.buildTableFromSpilledPartition(BinaryHashTable.java:443)
        at org.apache.flink.table.runtime.hashtable.BinaryHashTable.prepareNextPartition(BinaryHashTable.java:403)
        at org.apache.flink.table.runtime.hashtable.BinaryHashTable.nextMatching(BinaryHashTable.java:265)
        at org.apache.flink.table.runtime.operators.join.HashJoinOperator.endInput(HashJoinOperator.java:176)
        at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.endOperatorInput(TableOperatorWrapper.java:124)
        at org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperator.endInput(BatchMultipleInputStreamOperator.java:56)
        at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:93)
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:100)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:86)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:690)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
        at java.lang.Thread.run(Thread.java:748) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 06:19:00 UTC 2022,,,,,,,,,,"0|z18v9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 06:19;SpongebobZ;when I disable the multiple input operator, it turned to be normal;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LookupJoinITCase failed with classloader problem,FLINK-29427,13483327,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,hxbks2ks,hxbks2ks,27/Sep/22 06:22,09/Feb/23 13:11,04/Jun/24 20:41,06/Feb/23 08:23,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,Table SQL / Planner,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-27T02:49:20.9501313Z Sep 27 02:49:20 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$108341"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-09-27T02:49:20.9502654Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-09-27T02:49:20.9503366Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-09-27T02:49:20.9504044Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-09-27T02:49:20.9504704Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-09-27T02:49:20.9505341Z Sep 27 02:49:20 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-09-27T02:49:20.9505965Z Sep 27 02:49:20 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-09-27T02:49:20.9506584Z Sep 27 02:49:20 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-09-27T02:49:20.9507261Z Sep 27 02:49:20 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-09-27T02:49:20.9507883Z Sep 27 02:49:20 	... 30 more
2022-09-27T02:49:20.9509266Z Sep 27 02:49:20 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-09-27T02:49:20.9510835Z Sep 27 02:49:20 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-09-27T02:49:20.9511760Z Sep 27 02:49:20 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-09-27T02:49:20.9512456Z Sep 27 02:49:20 	at java.lang.Class.forName0(Native Method)
2022-09-27T02:49:20.9513014Z Sep 27 02:49:20 	at java.lang.Class.forName(Class.java:348)
2022-09-27T02:49:20.9513649Z Sep 27 02:49:20 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-09-27T02:49:20.9514339Z Sep 27 02:49:20 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-09-27T02:49:20.9514990Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-09-27T02:49:20.9515659Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
2022-09-27T02:49:20.9516337Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
2022-09-27T02:49:20.9516989Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-09-27T02:49:20.9517632Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-09-27T02:49:20.9518319Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-09-27T02:49:20.9519018Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-09-27T02:49:20.9519680Z Sep 27 02:49:20 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-09-27T02:49:20.9520386Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-09-27T02:49:20.9521042Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-09-27T02:49:20.9521677Z Sep 27 02:49:20 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-09-27T02:49:20.9522299Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-09-27T02:49:20.9522929Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
2022-09-27T02:49:20.9523658Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
2022-09-27T02:49:20.9524365Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
2022-09-27T02:49:20.9525030Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
2022-09-27T02:49:20.9525750Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-09-27T02:49:20.9526383Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-09-27T02:49:20.9527069Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-09-27T02:49:20.9527832Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-09-27T02:49:20.9528560Z Sep 27 02:49:20 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-09-27T02:49:20.9529217Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-09-27T02:49:20.9529862Z Sep 27 02:49:20 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-09-27T02:49:20.9530427Z Sep 27 02:49:20 	... 37 more
2022-09-27T02:49:20.9530852Z Sep 27 02:49:20 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41369&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4",,,,,,,,,,,,,,,,,,,,FLINK-29462,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 13:11:42 UTC 2023,,,,,,,,,,"0|z18v28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 06:22;hxbks2ks;cc [~renqs] [~smiralex];;;","29/Sep/22 02:14;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41434&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21554;;;","30/Sep/22 14:37;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41493&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21545;;;","30/Sep/22 14:38;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41488&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21930;;;","03/Oct/22 13:59;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41527&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","10/Oct/22 03:21;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41753&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","14/Oct/22 06:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41996&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17943;;;","14/Oct/22 09:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41984&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21610;;;","17/Oct/22 03:45;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42055&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17575;;;","18/Oct/22 08:31;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42117&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9]

Hi [~smiralex]  is any update on this issue?;;;","27/Oct/22 04:43;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42466&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/Oct/22 14:15;rmetzger;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42642&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

I'm including the entire stack trace here again for better discoverability of the ticket
{code}
2022-10-31T10:36:52.5727692Z Oct 31 10:36:52 [ERROR] LookupJoinITCase.testJoinTemporalTableWithComputedColumnAndPushDown  Time elapsed: 0.259 s  <<< ERROR!
2022-10-31T10:36:52.5728256Z Oct 31 10:36:52 java.lang.RuntimeException: Failed to fetch next result
2022-10-31T10:36:52.5729008Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-10-31T10:36:52.5729945Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-10-31T10:36:52.5730765Z Oct 31 10:36:52 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
2022-10-31T10:36:52.5731457Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-10-31T10:36:52.5732046Z Oct 31 10:36:52 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
2022-10-31T10:36:52.5732709Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
2022-10-31T10:36:52.5733412Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
2022-10-31T10:36:52.5734111Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
2022-10-31T10:36:52.5734927Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.testJoinTemporalTableWithComputedColumnAndPushDown(LookupJoinITCase.scala:350)
2022-10-31T10:36:52.5735660Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-10-31T10:36:52.5736232Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-10-31T10:36:52.5736871Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5737463Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5738053Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-10-31T10:36:52.5738698Z Oct 31 10:36:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-10-31T10:36:52.5739454Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-10-31T10:36:52.5740160Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-10-31T10:36:52.5740803Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-10-31T10:36:52.5741416Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-10-31T10:36:52.5742038Z Oct 31 10:36:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-10-31T10:36:52.5742704Z Oct 31 10:36:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-10-31T10:36:52.5743255Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5743882Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-10-31T10:36:52.5744496Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-10-31T10:36:52.5745093Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-10-31T10:36:52.5745755Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-10-31T10:36:52.5746353Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5746901Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5747477Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5748061Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5748617Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5749257Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5749783Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-10-31T10:36:52.5750313Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-10-31T10:36:52.5750938Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5751498Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5752060Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5752638Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5753215Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5753783Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5754384Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5754943Z Oct 31 10:36:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-10-31T10:36:52.5755475Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5756039Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5756566Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-10-31T10:36:52.5757065Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-10-31T10:36:52.5757642Z Oct 31 10:36:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-10-31T10:36:52.5758322Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-10-31T10:36:52.5759047Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-10-31T10:36:52.5759749Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
2022-10-31T10:36:52.5760564Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
2022-10-31T10:36:52.5761314Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
2022-10-31T10:36:52.5762093Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
2022-10-31T10:36:52.5762908Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
2022-10-31T10:36:52.5763770Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
2022-10-31T10:36:52.5764459Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-10-31T10:36:52.5765113Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-10-31T10:36:52.5765833Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-10-31T10:36:52.5766596Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-10-31T10:36:52.5767359Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-10-31T10:36:52.5768022Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-10-31T10:36:52.5768649Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-10-31T10:36:52.5769488Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-10-31T10:36:52.5770294Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-10-31T10:36:52.5771012Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-10-31T10:36:52.5771721Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-10-31T10:36:52.5772347Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-10-31T10:36:52.5772962Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-10-31T10:36:52.5773513Z Oct 31 10:36:52 Caused by: java.io.IOException: Failed to fetch job execution result
2022-10-31T10:36:52.5774190Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-10-31T10:36:52.5774987Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-10-31T10:36:52.5775774Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-10-31T10:36:52.5776354Z Oct 31 10:36:52 	... 67 more
2022-10-31T10:36:52.5776877Z Oct 31 10:36:52 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5777540Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-10-31T10:36:52.5778162Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-10-31T10:36:52.5778960Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-10-31T10:36:52.5779537Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5780017Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5780646Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-10-31T10:36:52.5781393Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-10-31T10:36:52.5782101Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-10-31T10:36:52.5782734Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-10-31T10:36:52.5783372Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-10-31T10:36:52.5784133Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-10-31T10:36:52.5784939Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-10-31T10:36:52.5785508Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5785968Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-10-31T10:36:52.5786717Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-10-31T10:36:52.5787595Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-10-31T10:36:52.5788391Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-10-31T10:36:52.5789199Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-10-31T10:36:52.5789962Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-10-31T10:36:52.5790681Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
2022-10-31T10:36:52.5791463Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-10-31T10:36:52.5792172Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
2022-10-31T10:36:52.5792867Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
2022-10-31T10:36:52.5793446Z Oct 31 10:36:52 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2022-10-31T10:36:52.5794040Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5794630Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5795234Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-10-31T10:36:52.5795989Z Oct 31 10:36:52 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-10-31T10:36:52.5796729Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-10-31T10:36:52.5797394Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-10-31T10:36:52.5798089Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-10-31T10:36:52.5798780Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-10-31T10:36:52.5799461Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-10-31T10:36:52.5800100Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-10-31T10:36:52.5800659Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-10-31T10:36:52.5801204Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-10-31T10:36:52.5801782Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-10-31T10:36:52.5802373Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-10-31T10:36:52.5802960Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5803531Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5804146Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-10-31T10:36:52.5804654Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-10-31T10:36:52.5805178Z Oct 31 10:36:52 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-10-31T10:36:52.5805734Z Oct 31 10:36:52 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-10-31T10:36:52.5806261Z Oct 31 10:36:52 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-10-31T10:36:52.5806768Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-10-31T10:36:52.5807283Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-10-31T10:36:52.5807757Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-10-31T10:36:52.5808293Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5808974Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5809591Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5810258Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5811368Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5812174Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:85)
2022-10-31T10:36:52.5813050Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:123)
2022-10-31T10:36:52.5813766Z Oct 31 10:36:52 	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
2022-10-31T10:36:52.5814306Z Oct 31 10:36:52 	at LookupFunction$173622.flatMap(Unknown Source)
2022-10-31T10:36:52.5814897Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
2022-10-31T10:36:52.5815974Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
2022-10-31T10:36:52.5817125Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
2022-10-31T10:36:52.5818114Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
2022-10-31T10:36:52.5818851Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
2022-10-31T10:36:52.5819688Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
2022-10-31T10:36:52.5820440Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
2022-10-31T10:36:52.5821117Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
2022-10-31T10:36:52.5821798Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
2022-10-31T10:36:52.5822579Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
2022-10-31T10:36:52.5823416Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
2022-10-31T10:36:52.5824227Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2022-10-31T10:36:52.5825006Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
2022-10-31T10:36:52.5825688Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-10-31T10:36:52.5826474Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-10-31T10:36:52.5827203Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-10-31T10:36:52.5828314Z Oct 31 10:36:52 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5829081Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-10-31T10:36:52.5829755Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-10-31T10:36:52.5830482Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
2022-10-31T10:36:52.5831117Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2022-10-31T10:36:52.5831732Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5832335Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5832927Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5833542Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5834331Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5835306Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:121)
2022-10-31T10:36:52.5836012Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-10-31T10:36:52.5836485Z Oct 31 10:36:52 	... 5 more
2022-10-31T10:36:52.5836904Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to create InputFormatCacheLoadTask
2022-10-31T10:36:52.5837668Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:137)
2022-10-31T10:36:52.5838837Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.lambda$reloadCache$0(InputFormatCacheLoader.java:84)
2022-10-31T10:36:52.5839754Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-10-31T10:36:52.5840466Z Oct 31 10:36:52 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
2022-10-31T10:36:52.5841085Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-10-31T10:36:52.5841706Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-10-31T10:36:52.5842320Z Oct 31 10:36:52 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-10-31T10:36:52.5842941Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-10-31T10:36:52.5843534Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
2022-10-31T10:36:52.5844279Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:85)
2022-10-31T10:36:52.5845108Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
2022-10-31T10:36:52.5845634Z Oct 31 10:36:52 	... 6 more
2022-10-31T10:36:52.5846343Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$173616'
2022-10-31T10:36:52.5847018Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
2022-10-31T10:36:52.5847875Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.open(GenericRowDataKeySelector.java:50)
2022-10-31T10:36:52.5848704Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.<init>(InputSplitCacheLoadTask.java:60)
2022-10-31T10:36:52.5849736Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:135)
2022-10-31T10:36:52.5850430Z Oct 31 10:36:52 	... 16 more
2022-10-31T10:36:52.5851049Z Oct 31 10:36:52 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.5851820Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
2022-10-31T10:36:52.5852497Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-10-31T10:36:52.5853207Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-10-31T10:36:52.5853713Z Oct 31 10:36:52 	... 19 more
2022-10-31T10:36:52.5854630Z Oct 31 10:36:52 Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.5855559Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
2022-10-31T10:36:52.5856380Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-10-31T10:36:52.5857126Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-10-31T10:36:52.5857833Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-10-31T10:36:52.5858325Z Oct 31 10:36:52 	... 21 more
2022-10-31T10:36:52.5858848Z Oct 31 10:36:52 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.5859653Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
2022-10-31T10:36:52.5860451Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
2022-10-31T10:36:52.5861199Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
2022-10-31T10:36:52.5862000Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
2022-10-31T10:36:52.5862793Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
2022-10-31T10:36:52.5863574Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
2022-10-31T10:36:52.5864587Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
2022-10-31T10:36:52.5865127Z Oct 31 10:36:52 	... 24 more
2022-10-31T10:36:52.5866599Z Oct 31 10:36:52 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$173616"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.5867747Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-10-31T10:36:52.5868442Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-10-31T10:36:52.5869169Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-10-31T10:36:52.5869792Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-10-31T10:36:52.5870422Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-10-31T10:36:52.5870977Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-10-31T10:36:52.5871546Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-10-31T10:36:52.5872163Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-10-31T10:36:52.5872641Z Oct 31 10:36:52 	... 30 more
2022-10-31T10:36:52.5873934Z Oct 31 10:36:52 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.5875109Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-10-31T10:36:52.5875948Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-10-31T10:36:52.5876653Z Oct 31 10:36:52 	at java.lang.Class.forName0(Native Method)
2022-10-31T10:36:52.5877103Z Oct 31 10:36:52 	at java.lang.Class.forName(Class.java:348)
2022-10-31T10:36:52.5877678Z Oct 31 10:36:52 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-10-31T10:36:52.5878302Z Oct 31 10:36:52 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-10-31T10:36:52.5879007Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-10-31T10:36:52.5879625Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
2022-10-31T10:36:52.5880282Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
2022-10-31T10:36:52.5880881Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-10-31T10:36:52.5881471Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-10-31T10:36:52.5882097Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-10-31T10:36:52.5882731Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-10-31T10:36:52.5883330Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-10-31T10:36:52.5883907Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-10-31T10:36:52.5884494Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-10-31T10:36:52.5885078Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-10-31T10:36:52.5885645Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-10-31T10:36:52.5886223Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
2022-10-31T10:36:52.5886846Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
2022-10-31T10:36:52.5887487Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
2022-10-31T10:36:52.5888077Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
2022-10-31T10:36:52.5888650Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-10-31T10:36:52.5889393Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-10-31T10:36:52.5890079Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-10-31T10:36:52.5890785Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-10-31T10:36:52.5891462Z Oct 31 10:36:52 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-10-31T10:36:52.5892050Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-10-31T10:36:52.5892642Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-10-31T10:36:52.5893088Z Oct 31 10:36:52 	... 37 more
2022-10-31T10:36:52.5893363Z Oct 31 10:36:52 
2022-10-31T10:36:52.5893778Z Oct 31 10:36:52 [ERROR] LookupJoinITCase.testJoinTemporalTable  Time elapsed: 0.149 s  <<< ERROR!
2022-10-31T10:36:52.5894285Z Oct 31 10:36:52 java.lang.RuntimeException: Failed to fetch next result
2022-10-31T10:36:52.5894935Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-10-31T10:36:52.5895747Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-10-31T10:36:52.5896561Z Oct 31 10:36:52 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
2022-10-31T10:36:52.5897343Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-10-31T10:36:52.5897914Z Oct 31 10:36:52 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
2022-10-31T10:36:52.5898589Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
2022-10-31T10:36:52.5899401Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
2022-10-31T10:36:52.5900168Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
2022-10-31T10:36:52.5900938Z Oct 31 10:36:52 	at org.apache.flink.table.planner.runtime.batch.sql.join.LookupJoinITCase.testJoinTemporalTable(LookupJoinITCase.scala:219)
2022-10-31T10:36:52.5901602Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-10-31T10:36:52.5902154Z Oct 31 10:36:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-10-31T10:36:52.5902809Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5903400Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5903972Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-10-31T10:36:52.5904634Z Oct 31 10:36:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-10-31T10:36:52.5905295Z Oct 31 10:36:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-10-31T10:36:52.5905933Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-10-31T10:36:52.5906584Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-10-31T10:36:52.5907216Z Oct 31 10:36:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-10-31T10:36:52.5907831Z Oct 31 10:36:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-10-31T10:36:52.5908421Z Oct 31 10:36:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-10-31T10:36:52.5909086Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5909700Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-10-31T10:36:52.5910451Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-10-31T10:36:52.5911065Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-10-31T10:36:52.5911725Z Oct 31 10:36:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-10-31T10:36:52.5912316Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5912881Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5913461Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5914021Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5914598Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5915162Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5915672Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-10-31T10:36:52.5916182Z Oct 31 10:36:52 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-10-31T10:36:52.5916704Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-10-31T10:36:52.5917252Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-10-31T10:36:52.5917826Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-10-31T10:36:52.5918469Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-10-31T10:36:52.5919118Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-10-31T10:36:52.5919711Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5920369Z Oct 31 10:36:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-10-31T10:36:52.5920921Z Oct 31 10:36:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-10-31T10:36:52.5921471Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-10-31T10:36:52.5922027Z Oct 31 10:36:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-10-31T10:36:52.5922541Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-10-31T10:36:52.5923057Z Oct 31 10:36:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-10-31T10:36:52.5923646Z Oct 31 10:36:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-10-31T10:36:52.5924299Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-10-31T10:36:52.5924960Z Oct 31 10:36:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-10-31T10:36:52.5925655Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
2022-10-31T10:36:52.5926403Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
2022-10-31T10:36:52.5927158Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
2022-10-31T10:36:52.5927933Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
2022-10-31T10:36:52.5928737Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
2022-10-31T10:36:52.5929619Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
2022-10-31T10:36:52.5930373Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-10-31T10:36:52.5931100Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-10-31T10:36:52.5931809Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-10-31T10:36:52.5932576Z Oct 31 10:36:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-10-31T10:36:52.5933339Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-10-31T10:36:52.5933991Z Oct 31 10:36:52 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-10-31T10:36:52.5934631Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-10-31T10:36:52.5935386Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-10-31T10:36:52.5936116Z Oct 31 10:36:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-10-31T10:36:52.5936818Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-10-31T10:36:52.5937478Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-10-31T10:36:52.5938086Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-10-31T10:36:52.5938703Z Oct 31 10:36:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-10-31T10:36:52.5939435Z Oct 31 10:36:52 Caused by: java.io.IOException: Failed to fetch job execution result
2022-10-31T10:36:52.5940153Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-10-31T10:36:52.5940940Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-10-31T10:36:52.5941750Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-10-31T10:36:52.5942334Z Oct 31 10:36:52 	... 67 more
2022-10-31T10:36:52.5942840Z Oct 31 10:36:52 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5943518Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-10-31T10:36:52.5944140Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-10-31T10:36:52.5944848Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-10-31T10:36:52.5945415Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5945864Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-10-31T10:36:52.5946478Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-10-31T10:36:52.5947233Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-10-31T10:36:52.5947955Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-10-31T10:36:52.5948577Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-10-31T10:36:52.5949307Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-10-31T10:36:52.5950057Z Oct 31 10:36:52 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-10-31T10:36:52.5950866Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-10-31T10:36:52.5951490Z Oct 31 10:36:52 	... 69 more
2022-10-31T10:36:52.5951967Z Oct 31 10:36:52 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-10-31T10:36:52.5952717Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-10-31T10:36:52.5953584Z Oct 31 10:36:52 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-10-31T10:36:52.5954396Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-10-31T10:36:52.5955108Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-10-31T10:36:52.5955793Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-10-31T10:36:52.5956514Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:739)
2022-10-31T10:36:52.5957242Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-10-31T10:36:52.5957934Z Oct 31 10:36:52 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
2022-10-31T10:36:52.5958631Z Oct 31 10:36:52 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
2022-10-31T10:36:52.5959378Z Oct 31 10:36:52 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2022-10-31T10:36:52.5960033Z Oct 31 10:36:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-10-31T10:36:52.5960609Z Oct 31 10:36:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-10-31T10:36:52.5961233Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-10-31T10:36:52.5961996Z Oct 31 10:36:52 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-10-31T10:36:52.5962722Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-10-31T10:36:52.5963405Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-10-31T10:36:52.5964104Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-10-31T10:36:52.5964785Z Oct 31 10:36:52 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-10-31T10:36:52.5965392Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-10-31T10:36:52.5965956Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-10-31T10:36:52.5966507Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-10-31T10:36:52.5967067Z Oct 31 10:36:52 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-10-31T10:36:52.5967643Z Oct 31 10:36:52 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-10-31T10:36:52.5968220Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-10-31T10:36:52.5968810Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5969483Z Oct 31 10:36:52 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-10-31T10:36:52.5970098Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-10-31T10:36:52.5970603Z Oct 31 10:36:52 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-10-31T10:36:52.5971141Z Oct 31 10:36:52 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-10-31T10:36:52.5971753Z Oct 31 10:36:52 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-10-31T10:36:52.5972286Z Oct 31 10:36:52 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-10-31T10:36:52.5972806Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-10-31T10:36:52.5973303Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-10-31T10:36:52.5973789Z Oct 31 10:36:52 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-10-31T10:36:52.5974320Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5974907Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5975515Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5976117Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5977185Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5978005Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:85)
2022-10-31T10:36:52.5978816Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:123)
2022-10-31T10:36:52.5979625Z Oct 31 10:36:52 	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
2022-10-31T10:36:52.5980299Z Oct 31 10:36:52 	at LookupFunction$173667.flatMap(Unknown Source)
2022-10-31T10:36:52.5980902Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
2022-10-31T10:36:52.5981665Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
2022-10-31T10:36:52.5982425Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
2022-10-31T10:36:52.5983174Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
2022-10-31T10:36:52.5983887Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
2022-10-31T10:36:52.5984564Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
2022-10-31T10:36:52.5985247Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
2022-10-31T10:36:52.5985922Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
2022-10-31T10:36:52.5986585Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
2022-10-31T10:36:52.5987366Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
2022-10-31T10:36:52.5988216Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
2022-10-31T10:36:52.5989111Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2022-10-31T10:36:52.5989941Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
2022-10-31T10:36:52.5990647Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-10-31T10:36:52.5991298Z Oct 31 10:36:52 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-10-31T10:36:52.5992002Z Oct 31 10:36:52 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-10-31T10:36:52.5993055Z Oct 31 10:36:52 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5993733Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-10-31T10:36:52.5994378Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-10-31T10:36:52.5995034Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
2022-10-31T10:36:52.5995687Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2022-10-31T10:36:52.5996297Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-10-31T10:36:52.5996884Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-10-31T10:36:52.5997489Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-10-31T10:36:52.5998102Z Oct 31 10:36:52 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-10-31T10:36:52.5998852Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-10-31T10:36:52.5999594Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:121)
2022-10-31T10:36:52.6000332Z Oct 31 10:36:52 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-10-31T10:36:52.6000882Z Oct 31 10:36:52 	... 5 more
2022-10-31T10:36:52.6001319Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Failed to create InputFormatCacheLoadTask
2022-10-31T10:36:52.6002081Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:137)
2022-10-31T10:36:52.6003036Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.lambda$reloadCache$0(InputFormatCacheLoader.java:84)
2022-10-31T10:36:52.6003830Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-10-31T10:36:52.6004461Z Oct 31 10:36:52 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
2022-10-31T10:36:52.6005063Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-10-31T10:36:52.6005676Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-10-31T10:36:52.6006299Z Oct 31 10:36:52 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-10-31T10:36:52.6006891Z Oct 31 10:36:52 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-10-31T10:36:52.6007488Z Oct 31 10:36:52 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
2022-10-31T10:36:52.6008244Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:85)
2022-10-31T10:36:52.6009168Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
2022-10-31T10:36:52.6009680Z Oct 31 10:36:52 	... 6 more
2022-10-31T10:36:52.6010443Z Oct 31 10:36:52 Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$173661'
2022-10-31T10:36:52.6011113Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
2022-10-31T10:36:52.6011840Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.open(GenericRowDataKeySelector.java:50)
2022-10-31T10:36:52.6012894Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.<init>(InputSplitCacheLoadTask.java:60)
2022-10-31T10:36:52.6013868Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.createCacheLoadTask(InputFormatCacheLoader.java:135)
2022-10-31T10:36:52.6014599Z Oct 31 10:36:52 	... 16 more
2022-10-31T10:36:52.6015253Z Oct 31 10:36:52 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.6016074Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
2022-10-31T10:36:52.6016782Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-10-31T10:36:52.6017490Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-10-31T10:36:52.6018020Z Oct 31 10:36:52 	... 19 more
2022-10-31T10:36:52.6018749Z Oct 31 10:36:52 Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.6019804Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
2022-10-31T10:36:52.6020718Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-10-31T10:36:52.6021576Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-10-31T10:36:52.6022650Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-10-31T10:36:52.6023350Z Oct 31 10:36:52 	... 21 more
2022-10-31T10:36:52.6023920Z Oct 31 10:36:52 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-10-31T10:36:52.6024653Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
2022-10-31T10:36:52.6025358Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
2022-10-31T10:36:52.6026139Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
2022-10-31T10:36:52.6026984Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
2022-10-31T10:36:52.6027822Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
2022-10-31T10:36:52.6028608Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
2022-10-31T10:36:52.6029500Z Oct 31 10:36:52 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
2022-10-31T10:36:52.6030147Z Oct 31 10:36:52 	... 24 more
2022-10-31T10:36:52.6031805Z Oct 31 10:36:52 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$173661"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.6032942Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-10-31T10:36:52.6033536Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-10-31T10:36:52.6034154Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-10-31T10:36:52.6034754Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-10-31T10:36:52.6035453Z Oct 31 10:36:52 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-10-31T10:36:52.6036031Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-10-31T10:36:52.6036599Z Oct 31 10:36:52 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-10-31T10:36:52.6037202Z Oct 31 10:36:52 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-10-31T10:36:52.6037701Z Oct 31 10:36:52 	... 30 more
2022-10-31T10:36:52.6039060Z Oct 31 10:36:52 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-10-31T10:36:52.6040273Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-10-31T10:36:52.6041119Z Oct 31 10:36:52 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-10-31T10:36:52.6041755Z Oct 31 10:36:52 	at java.lang.Class.forName0(Native Method)
2022-10-31T10:36:52.6042202Z Oct 31 10:36:52 	at java.lang.Class.forName(Class.java:348)
2022-10-31T10:36:52.6042760Z Oct 31 10:36:52 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-10-31T10:36:52.6043496Z Oct 31 10:36:52 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-10-31T10:36:52.6044095Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-10-31T10:36:52.6044695Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6749)
2022-10-31T10:36:52.6045315Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
2022-10-31T10:36:52.6045912Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-10-31T10:36:52.6046485Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-10-31T10:36:52.6047113Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-10-31T10:36:52.6047762Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-10-31T10:36:52.6048349Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-10-31T10:36:52.6049024Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-10-31T10:36:52.6049633Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-10-31T10:36:52.6050298Z Oct 31 10:36:52 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-10-31T10:36:52.6050871Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-10-31T10:36:52.6051465Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:1365)
2022-10-31T10:36:52.6052068Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1349)
2022-10-31T10:36:52.6052706Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
2022-10-31T10:36:52.6053313Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
2022-10-31T10:36:52.6053892Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-10-31T10:36:52.6054463Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-10-31T10:36:52.6055106Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-10-31T10:36:52.6055811Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-10-31T10:36:52.6056530Z Oct 31 10:36:52 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-10-31T10:36:52.6057130Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-10-31T10:36:52.6057712Z Oct 31 10:36:52 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-10-31T10:36:52.6058146Z Oct 31 10:36:52 	... 37 more
{code};;;","01/Nov/22 09:00;Weijie Guo;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42693&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21195;;;","02/Nov/22 15:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42734&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21329;;;","04/Nov/22 07:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42802&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17638;;;","07/Nov/22 08:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42863&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21328;;;","07/Nov/22 09:01;chesnay;I'm wondering if this is due to bad locking in the {{CacheLoader}}; since the setting and checking of {{isStopped}} in {{CacheLoader#run/close}} don't run under the lock it's possible for a cache reload to run while the loader was shut down already.;;;","08/Nov/22 07:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42910&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17971;;;","11/Nov/22 09:10;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43035&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21693;;;","14/Nov/22 03:40;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43089&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94;;;","14/Nov/22 03:41;leonard;[~fsk119] Could you take a look this issue?;;;","21/Nov/22 03:22;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43309&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","21/Nov/22 09:20;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43270&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","21/Nov/22 09:24;leonard;I changed this ticket priority to blocker as too many failures happens.  
Could you also help take a look [~lsy] ？;;;","21/Nov/22 09:30;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43254&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","22/Nov/22 07:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43221&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=22265;;;","22/Nov/22 12:41;smiralex;Hi everyone! First of all, I'm very sorry for causing so much troubles with this unstable test to you. Unfortunately, I didn't succeed to reproduce this error locally, so I can only assume what can be the cause of the problem. I tried to fix this problem {color:#000000}[once|https://github.com/apache/flink/pull/20734]{color}, but the core of it still was not solved. Now I suppose, that the core can lie in that {_}{color:#000000}Thread{color}.currentThread{color:#871094}(){color}.getContextClassLoader{_}{color:#871094}_()_ {color}was returning wrong ClassLoader, because it was executed in separate thread, so I changed it in this {color:#000000}[PR|https://github.com/apache/flink/pull/21365] {color} _-_  now Projection class is being compiled just one time in main thread, so ClassLoader leak can't occur anymore. {color:#000000}Please read the description of this PR where I tried to explain my thoughts about this problem. I hope this fix will eventually close this ticket, and I'm waiting for someone's review! {color};;;","23/Nov/22 02:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43380&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=26173;;;","23/Nov/22 08:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43400&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17698;;;","30/Nov/22 15:32;mapohl;aaand we're back:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43615&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21327;;;","01/Dec/22 11:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43642&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21331;;;","02/Dec/22 10:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43649&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17978;;;","05/Dec/22 10:31;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43694&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17846;;;","06/Dec/22 08:34;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43744&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17982;;;","08/Dec/22 11:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17703;;;","08/Dec/22 11:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43794&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17219;;;","09/Dec/22 15:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43824&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17528;;;","10/Dec/22 12:24;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43850&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","10/Dec/22 12:32;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43850&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","13/Dec/22 08:25;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43910&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21913;;;","13/Dec/22 11:19;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43919&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","16/Dec/22 08:27;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43983&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17301;;;","19/Dec/22 09:52;mapohl;2x in same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44056&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=21055
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44056&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21256;;;","20/Dec/22 07:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44084&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=17551;;;","22/Dec/22 08:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44162&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17978;;;","02/Jan/23 09:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44285&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17978;;;","02/Jan/23 11:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44374&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=21863;;;","05/Jan/23 07:40;mapohl;Same build, 2 jobs failed:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=21863
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17978;;;","06/Jan/23 07:49;mapohl;Same build, 2 jobs failed:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44525&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=20486
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44525&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=20741;;;","06/Jan/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44526&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17619;;;","09/Jan/23 08:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44569&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=20249;;;","10/Jan/23 13:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44657&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=25822;;;","11/Jan/23 08:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44683&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18524;;;","11/Jan/23 09:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=18068;;;","11/Jan/23 12:54;rmetzger;[~smiralex] what's the status of this issue?;;;","11/Jan/23 14:35;mapohl;[~renqs] is planning to pick up on that one.;;;","12/Jan/23 12:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44752&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21788;;;","13/Jan/23 11:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44777&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=21750;;;","13/Jan/23 13:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44782&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21788;;;","16/Jan/23 09:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44863&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=18389;;;","17/Jan/23 15:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44941&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=22314;;;","17/Jan/23 15:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44965&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=26411;;;","19/Jan/23 13:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45024&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18557;;;","19/Jan/23 13:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45039&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=16184;;;","20/Jan/23 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45099&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18009;;;","27/Jan/23 07:32;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45220&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=16759;;;","30/Jan/23 07:30;lincoln.86xy;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45368&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","30/Jan/23 14:02;mapohl;[~renqs] this is still an open blocker. Have you had the chance to work on it? Or shall we assign someone else to it? (for now, I changed the assignee from [~smiralex] to Qingsheng because that's the latest state we agreed on);;;","31/Jan/23 00:52;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45438&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","02/Feb/23 09:01;mapohl;same run, multiple times:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=17624]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=17624;;;","03/Feb/23 04:04;renqs;Fixed on master: 0516427d198420fa28d586ed052ef26daf2845ba;;;","03/Feb/23 10:14;mapohl;Reopening this issue: Shouldn't we provide a 1.16 backport PR as well?;;;","03/Feb/23 10:18;leonard;[~mapohl] Of course yes, I add 1.16.2 to fix versions;;;","06/Feb/23 08:23;renqs;Thanks for the reminder [~mapohl] [~leonard] !

Backported to 1.16: 0211d5b3ad3d26d8a70fd2d0234ffbd690483e2f;;;","09/Feb/23 12:56;stayrascal;May I check what use case will meet this problem, does it means LookupJoin will not working well in 1.16.1?;;;","09/Feb/23 13:11;martijnvisser;[~stayrascal] This was a test stability, not a user reported issue. ;;;"
Flink ML 2.1.0 Installation,FLINK-29426,13483317,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tsun,tsun,27/Sep/22 04:47,27/Sep/22 07:04,04/Jun/24 20:41,,ml-2.1.0,,,,,,,,,,,Quickstarts,,,,0,,,,,"Trying to go through Flink-ML quickstart but having an issue with building Flink ML's source code.

Steps to reproduce:

1. sudo wget https://dlcdn.apache.org/flink/flink-ml-2.1.0/flink-ml-2.1.0-src.tgz
2. sudo tar -xzf flink-ml-2.1.0-src.tgz
3. mvn clean package -DskipTests -Dcheckstyle.skip
{quote}Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project flink-ml-parent: Unable to create output stream: /usr/src/flink-ml-2.1.0/target/checkstyle-result.xml: /usr/src/flink-ml-2.1.0/target/checkstyle-result.xml (No such file or directory) -> [Help 1]
{quote}
It doesn't seem like `target` is a folder in `flink-ml-2.1.0`.","Java version: 11.0.16

Flink version: 1.15.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 07:04:09 UTC 2022,,,,,,,,,,"0|z18v00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 07:04;Jiang Xin;Hi Tony, I tried to follow your steps to reproduce the error but built successfully. I suppose it may be a permission issue, could you change your working directory or run `mvn package` with `sudo`?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid full spilling strategy triggering spilling frequently,FLINK-29425,13483315,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Sep/22 04:21,28/Sep/22 08:50,04/Jun/24 20:41,28/Sep/22 08:50,1.16.0,,,,,,,1.16.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"In hybrid shuffle mode, we have an internal config option 'DEFAULT_FULL_STRATEGY_NUM_BUFFERS_TRIGGER_SPILLED' to control spilling frequency in the full spilling strategy. Unfortunately, the default value(10) is too small. As a result, frequent disk spilling calls are observed in the TPC-DS test, which seriously affects performance. When we increase the value, the query performance is improved significantly. We should set a more reasonable default value, or adopt an adaptive strategy to determine the spilling frequency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 08:50:03 UTC 2022,,,,,,,,,,"0|z18uzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 08:50;xtsong;- master (1.17): fe3bdcf0182ded35f1a8ac6eee4d7bf7acec832d
- release-1.16: 790434b9fb7296420c1ea15af0d640273776d0b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Pull Request Template for flink-ml Project,FLINK-29424,13483314,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,27/Sep/22 03:40,23/Nov/22 09:28,04/Jun/24 20:41,30/Sep/22 07:46,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add Pull Request Template for flink-ml Project,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 09:31:54 UTC 2022,,,,,,,,,,"0|z18uzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 09:31;Jiang Xin;Merged to master via 8a792318ebee3c076244bc95dce82fc448e7a172;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobDetails is incorrect OpenAPI spec ,FLINK-29423,13483258,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Sep/22 18:16,22/Nov/22 08:22,04/Jun/24 20:41,22/Nov/22 08:22,1.16.0,,,,,,,1.17.0,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,,"The JobDetails use custom serialization, but the introspection ignores that and analyzes the class as-is, resulting in various fields being documented that shouldn't be.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 08:22:58 UTC 2022,,,,,,,,,,"0|z18un4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 08:22;chesnay;master: ec89c17da5d555ca44f89d3f8739fa9ae00734b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Production tests return/argument types do not take transitivity into account,FLINK-29422,13483247,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,26/Sep/22 16:34,27/Sep/22 10:30,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,API / DataStream,,,,0,,,,,"In FLINK-29403 I'm marking {{SimpleCondition}} as {{PublicEvolving}}, but the production tests reject it:

{code:java}
Architecture Violation [Priority: MEDIUM] - Rule 'Return and argument types of methods annotated with @PublicEvolving must be annotated with @Public(Evolving).' was violated (1 times):
Sep 26 15:20:12 org.apache.flink.cep.pattern.conditions.SimpleCondition.filter(java.lang.Object, org.apache.flink.cep.pattern.conditions.IterativeCondition$Context): Argument leaf type org.apache.flink.cep.pattern.conditions.IterativeCondition$Context does not satisfy: reside outside of package 'org.apache.flink..' or reside in any package ['..shaded..'] or annotated with @Public or annotated with @PublicEvolving or annotated with @Deprecated
{code}

This doesn't make any sense given that {{IterativeCondition}} itself is already {{PublicEvolving}} and contains the exact same method.",,,,,,,,,,,,FLINK-29403,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 10:30:49 UTC 2022,,,,,,,,,,"0|z18ukw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 10:30;chesnay;Alright this is ""solved"" for the IterativeCondition with an exclusion. _great_;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support python 3.10,FLINK-29421,13483221,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,hxbks2ks,sirianni,sirianni,26/Sep/22 14:35,21/Mar/23 08:07,04/Jun/24 20:41,19/Jan/23 12:39,,,,,,,,1.17.0,,,,API / Python,,,,0,pull-request-available,,,,"The {{apache-flink}} package fails to install on Python 3.10 due to inability to compile {{numpy}}

{noformat}
            numpy/core/src/multiarray/scalartypes.c.src:3242:12: error: too few arguments to function ‘_Py_HashDouble’
             3242 |     return _Py_HashDouble(npy_half_to_double(((PyHalfScalarObject *)obj)->obval));
                  |            ^~~~~~~~~~~~~~
            In file included from /home/sirianni/.asdf/installs/python/3.10.6/include/python3.10/Python.h:77,
                             from numpy/core/src/multiarray/scalartypes.c.src:3:
            /home/sirianni/.asdf/installs/python/3.10.6/include/python3.10/pyhash.h:10:23: note: declared here
               10 | PyAPI_FUNC(Py_hash_t) _Py_HashDouble(PyObject *, double);

{noformat}

Numpy issue https://github.com/numpy/numpy/issues/19033

[Mailing list thread|https://lists.apache.org/thread/f4r9hjt1l33xf5ngnswszhnls4cxkk52]",,,,,,,,,,,,,,,,,,,,FLINK-27713,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 08:04:48 UTC 2023,,,,,,,,,,"0|z18uf4:",9223372036854775807,PyFlink 1.17 will support Python 3.10 and remove the support of Python 3.6,,,,,,,,,,,,,,,,,,,"27/Sep/22 02:01;hxbks2ks;Currently, pyflink hasn't supported Python3.10. In release-1.16, pyflink will support 3.9. For 3.10 support, it will probably be in release 1.17.;;;","19/Jan/23 12:39;hxbks2ks;Merged into master via 838b79f5b9cc1a4cf253b2c17009f337bf569ecc;;;","31/Jan/23 09:25;Peter_Howe;Hi [~hxbks2ks]  how about Python 3.11 version support?;;;","21/Mar/23 08:04;hxb;[~Peter_Howe] apache beam hasn't supported python 3.11. After beam support python 3.11, pyflink can upgrade dependency to support 3.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Zookeeper to 3.7,FLINK-29420,13483217,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Sep/22 14:03,21/Nov/23 02:34,04/Jun/24 20:41,06/Dec/22 09:40,,,,,,,,1.17.0,,,,BuildSystem / Shaded,Runtime / Coordination,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 09:40:08 UTC 2022,,,,,,,,,,"0|z18ueg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/22 09:40;chesnay;master: 576c312d3731210ad91eb9411af7061120abae80;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase.testHybridFullExchangesRestart hangs,FLINK-29419,13483201,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,hxbks2ks,hxbks2ks,26/Sep/22 12:55,13/Dec/22 04:39,04/Jun/24 20:41,13/Dec/22 04:39,1.16.0,1.17.0,,,,,,1.16.1,1.17.0,,,Runtime / Network,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-09-26T10:56:44.0766792Z Sep 26 10:56:44 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007f41a4efa000 nid=0x6d76 waiting on condition [0x00007f40ac135000]
2022-09-26T10:56:44.0767432Z Sep 26 10:56:44    java.lang.Thread.State: WAITING (parking)
2022-09-26T10:56:44.0767892Z Sep 26 10:56:44 	at sun.misc.Unsafe.park(Native Method)
2022-09-26T10:56:44.0768644Z Sep 26 10:56:44 	- parking to wait for  <0x00000000a0704e18> (a java.util.concurrent.CompletableFuture$Signaller)
2022-09-26T10:56:44.0769287Z Sep 26 10:56:44 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2022-09-26T10:56:44.0769949Z Sep 26 10:56:44 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2022-09-26T10:56:44.0770623Z Sep 26 10:56:44 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2022-09-26T10:56:44.0771349Z Sep 26 10:56:44 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2022-09-26T10:56:44.0772092Z Sep 26 10:56:44 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-09-26T10:56:44.0772777Z Sep 26 10:56:44 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:57)
2022-09-26T10:56:44.0773534Z Sep 26 10:56:44 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:115)
2022-09-26T10:56:44.0774333Z Sep 26 10:56:44 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchangesRestart(HybridShuffleITCase.java:59)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41343&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7
",,,,,,,,,,,,,,,,,,,,,FLINK-29682,,,,FLINK-30189,FLINK-29298,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 04:39:01 UTC 2022,,,,,,,,,,"0|z18ub4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 12:56;hxbks2ks;cc [~Weijie Guo];;;","26/Sep/22 12:58;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41345&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","26/Sep/22 12:59;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41345&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","26/Sep/22 13:02;Weijie Guo;[~hxbks2ks] Thanks for the feedback. I will check the cause of this problem.;;;","27/Sep/22 06:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41367&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9;;;","30/Sep/22 14:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41494&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=11782;;;","03/Oct/22 13:49;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41535&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","05/Oct/22 12:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41564&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9625;;;","06/Oct/22 06:36;dwysakowicz;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41620&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12048;;;","11/Oct/22 02:07;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41837&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","17/Oct/22 03:52;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42061&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","17/Oct/22 03:53;hxb;[~Weijie Guo]  Is any update on this issue?;;;","17/Oct/22 08:18;Weijie Guo;The specific reason needs to be further investigated. In order not to block flink code merge pipeline, temporarily disable them. I will reactivate these two tests when we find the reason.;;;","17/Oct/22 08:35;hxb;Temporarily disabled on master via fca71d237b2440c0129ef7e6f8266f4091df4884

Temporarily disabled on master via 5d03327d016c6e3250a82566ff5530c65cf5d344;;;","18/Oct/22 08:27;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae]

HybridShuffleITCase.testHybridSelectiveExchanges hangs too
{code:java}
3:03:31.9738556Z Oct 18 03:03:31 ""ForkJoinPool-1-worker-51"" #28 daemon prio=5 os_prio=0 cpu=5134.15ms elapsed=3370.05s tid=0x00007f03f4dad000 nid=0x4f71 waiting on condition  [0x00007f03c8c12000]
2022-10-18T03:03:31.9739287Z Oct 18 03:03:31    java.lang.Thread.State: WAITING (parking)
2022-10-18T03:03:31.9739815Z Oct 18 03:03:31 	at jdk.internal.misc.Unsafe.park(java.base@11.0.11/Native Method)
2022-10-18T03:03:31.9740662Z Oct 18 03:03:31 	- parking to wait for  <0x00000000a2748288> (a java.util.concurrent.CompletableFuture$Signaller)
2022-10-18T03:03:31.9741425Z Oct 18 03:03:31 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.11/LockSupport.java:194)
2022-10-18T03:03:31.9742584Z Oct 18 03:03:31 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.11/CompletableFuture.java:1796)
2022-10-18T03:03:31.9743340Z Oct 18 03:03:31 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.11/ForkJoinPool.java:3118)
2022-10-18T03:03:31.9744059Z Oct 18 03:03:31 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.11/CompletableFuture.java:1823)
2022-10-18T03:03:31.9744783Z Oct 18 03:03:31 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.11/CompletableFuture.java:1998)
2022-10-18T03:03:31.9745501Z Oct 18 03:03:31 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:57)
2022-10-18T03:03:31.9746297Z Oct 18 03:03:31 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:115)
2022-10-18T03:03:31.9747132Z Oct 18 03:03:31 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchanges(HybridShuffleITCase.java:51) {code};;;","19/Oct/22 03:23;hxb;Temporarily disabled `HybridShuffleITCase` on master via 426c39107f434da5805bef0ff84f95912098465b

Temporarily disabled `HybridShuffleITCase` on release-1.16 via 20eb0168aa602d1b7a6b8dd116ddd1abeac8dc5e;;;","24/Nov/22 09:16;Weijie Guo;Through further investigation, we found two possible causes. One is the problem of hybrid result partition may loading data from the file that already consumed from memory, and the other is the bug in the LocalBufferPool (hybrid shuffle scene greatly increases the probability of recurrence).

After these two tickets are resolved, we should be able to enable the relevant tests.;;;","13/Dec/22 04:39;xtsong;Both FLINK-29298 and FLINK-30189 have been fixed.

Re-enabled HybridShuffleITCase in:
- master (1.17): 1a2f638503d5364edbe947b61e29b62b981bb2e8
- release-1.16: ad7ac1decbe97e12e203e294cfb58deeea866aca

Closing the ticket. Feel free to re-open if that happens again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-shaded dependencies,FLINK-29418,13483195,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Sep/22 12:01,27/Sep/22 08:45,04/Jun/24 20:41,27/Sep/22 08:40,,,,,,,,shaded-16.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,"Do a round of updates in flink-shaded.

[asm] Upgrade 9.2 -> 9.3
[jackson] Upgrade 2.12.4 -> 2.13.4
[swagger] Upgrade 2.1.11 -> 2.2.2
[zookeeper] Upgrade 3.5.9 -> 3.5.10
[curator] Upgrade 5.2.0 -> 5.3.0
[netty] Upgrade 4.1.70 -> 4.1.82
[zookeeper] Add 3.7 support
[zookeeper] Add 3.8 support ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27293,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 08:40:21 UTC 2022,,,,,,,,,,"0|z18u9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 08:40;chesnay;master: d630f8b1e8d19f70cd2a967e1d210454ef1c7849..bfb2c5abf75e73ab4e8159de19c379c3324695f0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One or more fetchers have encountered exception,FLINK-29417,13483182,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,ChenHongLiu,ChenHongLiu,26/Sep/22 10:21,26/Sep/22 11:14,04/Jun/24 20:41,26/Sep/22 11:14,,,,,,,,,,,,,,,,0,,,,,"One machine in the cluster goes offline, most tasks failover, and then it automatically restarts successfully. But the task of writing data to kudu failed to restart. The error is as follows:

 
{code:java}
//代码占位符
2022-09-26 16:06:30,634 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - KunlunRecLogOutputWindow (11/90)#6 (9e23189bdc1ba38b19f922342819dab2) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Error at remote task manager '****/***:**'.
    at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:351)
    at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:240)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:112)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flink.runtime.io.network.partition.ProducerFailedException: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:285)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:123)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:234)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
    at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:91)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
    ... 3 more
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:342)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
    ... 1 more
Caused by: java.lang.OutOfMemoryError: Direct buffer memory. The direct out-of-memory error has occurred. This can mean two things: either job(s) require(s) a larger size of JVM direct memory or there is a direct memory leak. The direct memory can be allocated by user code or some of its dependencies. In this case 'taskmanager.memory.task.off-heap.size' configuration option should be increased. Flink framework and its dependencies also consume the direct memory, mostly for network communication. The most of network memory is managed by Flink and should not result in out-of-memory error. In certain special cases, in particular for jobs with high parallelism, the framework may require more direct memory which is not managed by Flink. In this case 'taskmanager.memory.framework.off-heap.size' configuration option should be increased. If the error persists then there is probably a direct memory leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
    at java.nio.Bits.reserveMemory(Bits.java:693)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
    at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:174)
    at sun.nio.ch.IOUtil.read(IOUtil.java:195)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
    at org.apache.kafka.common.network.PlaintextTransportLayer.read(PlaintextTransportLayer.java:103)
    at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:117)
    at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:424)
    at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:385)
    at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:651)
    at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:572)
    at org.apache.kafka.common.network.Selector.poll(Selector.java:483)
    at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
    at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1300)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
    at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    ... 1 more {code}
 

 

I don't know why and how to fix it.",,,,,,,,,,,,,,,,,,,,FLINK-29416,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2022-09-26 10:21:05.0,,,,,,,,,,"0|z18u6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One or more fetchers have encountered exception,FLINK-29416,13483181,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ChenHongLiu,ChenHongLiu,26/Sep/22 10:21,26/Sep/22 13:49,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,,,,,"One machine in the cluster goes offline, most tasks failover, and then it automatically restarts successfully. But the task of writing data to kudu failed to restart. The error is as follows:

 
{code:java}
//代码占位符
2022-09-26 16:06:30,634 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - KunlunRecLogOutputWindow (11/90)#6 (9e23189bdc1ba38b19f922342819dab2) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Error at remote task manager 'bjstream44.jd.163.org/10.196.24.76:39604'.
    at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.decodeMsg(CreditBasedPartitionRequestClientHandler.java:351)
    at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelRead(CreditBasedPartitionRequestClientHandler.java:240)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.runtime.io.network.netty.NettyMessageClientDecoderDelegate.channelRead(NettyMessageClientDecoderDelegate.java:112)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flink.runtime.io.network.partition.ProducerFailedException: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:285)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:123)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:234)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
    at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
    at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:91)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
    ... 3 more
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:342)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
    ... 1 more
Caused by: java.lang.OutOfMemoryError: Direct buffer memory. The direct out-of-memory error has occurred. This can mean two things: either job(s) require(s) a larger size of JVM direct memory or there is a direct memory leak. The direct memory can be allocated by user code or some of its dependencies. In this case 'taskmanager.memory.task.off-heap.size' configuration option should be increased. Flink framework and its dependencies also consume the direct memory, mostly for network communication. The most of network memory is managed by Flink and should not result in out-of-memory error. In certain special cases, in particular for jobs with high parallelism, the framework may require more direct memory which is not managed by Flink. In this case 'taskmanager.memory.framework.off-heap.size' configuration option should be increased. If the error persists then there is probably a direct memory leak in user code or some of its dependencies which has to be investigated and fixed. The task executor has to be shutdown...
    at java.nio.Bits.reserveMemory(Bits.java:693)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)
    at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:174)
    at sun.nio.ch.IOUtil.read(IOUtil.java:195)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
    at org.apache.kafka.common.network.PlaintextTransportLayer.read(PlaintextTransportLayer.java:103)
    at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:117)
    at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:424)
    at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:385)
    at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:651)
    at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:572)
    at org.apache.kafka.common.network.Selector.poll(Selector.java:483)
    at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:547)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
    at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1300)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1240)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1211)
    at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:113)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    ... 1 more {code}
 

 

I don't know why and how to fix it.",,,,,,,,,,,,,,,,,,,,,FLINK-29417,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 13:49:33 UTC 2022,,,,,,,,,,"0|z18u6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 13:49;Weijie Guo;It seems that direct memory is not enough, you can try to increase the size of 
taskmanager.memory.task.off-heap.size
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InitializationFailure when recovering from a checkpoint in Application Mode leads to the cleanup of all HA data,FLINK-29415,13483176,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,mapohl,mapohl,26/Sep/22 09:57,26/Sep/22 13:24,04/Jun/24 20:41,26/Sep/22 13:24,1.14.6,1.15.2,1.16.0,1.17.0,,,,,,,,Runtime / Coordination,,,,0,,,,,"This issue was raised in the user ML thread [JobManager restarts on job failure|https://lists.apache.org/thread/qkmcty3h4gkkx5g09m19gwqrf8z8d383]. Recovering from a external checkpoint is handled differently than recovering from an internal state (see [Dispatcher#handleJobManagerRunner|https://github.com/apache/flink/blob/41ac1ba13679121f1ddf14b26a36f4f4a3cc73e4/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L651]). For the latter case, we explicitly do a local cleanup (i.e. no HA data is cleaned up). For the case, described in the ML thread, a global cleanup is performed. That's not a problem in session mode where a new job ID is used. The new job ID will result in using a new namespace for the HA data. Data from previous runs are not touched during a cleanup. In Application mode, we use the default job ID `0` which would be reused. In case of a failure, all the HA data will be ""namespaced"" using the default job id. As a consequence, the related data is cleaned up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 13:24:05 UTC 2022,,,,,,,,,,"0|z18u5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 13:24;mapohl;There is a solution for this specific case already with {{execution.submit-failed-job-on-application-error=true}} being introduced with FLINK-25715 in Flink 1.15 as [~gyfora] pointed out in the ML thread. I'm gonna close this issue again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup license checks,FLINK-29414,13483174,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Sep/22 09:55,26/Sep/22 13:50,04/Jun/24 20:41,26/Sep/22 13:50,,,,,,,,shaded-16.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 13:50:33 UTC 2022,,,,,,,,,,"0|z18u54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 13:50;chesnay;master: 575912de77153544a5e4211f504cc67e5ee776a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make it possible to associate triggered and completed savepoints,FLINK-29413,13483165,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,26/Sep/22 09:20,24/Nov/22 01:03,04/Jun/24 20:41,03/Oct/22 13:38,,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Currently it is not clear how one would assoicate completed manual savepoints with savepointTriggerNonce-es when using the operator.

This makes it difficult to track when a savepoint was completed vs when it was abandoned

One idea would be to add the savepointTriggerNonce to the completed checkpoint info for Manual savepoints.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 03 13:38:12 UTC 2022,,,,,,,,,,"0|z18u34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 14:43;morhidi;Would it makes sense to add the format type too, for all cases until https://issues.apache.org/jira/browse/FLINK-29322 is fixed?;;;","26/Sep/22 15:15;gyfora;I guess we could add that too if there is no other way to get that info, [~thw] what do you think?;;;","26/Sep/22 15:53;thw;+1 for tracking the triggering nonce

Would there be any other way to retrieve the savepoint type given that Flink does not retain a history beyond current job execution? 

 ;;;","26/Sep/22 16:09;gyfora;We don't really see any other way to retrieve the format info, so it is probably valuable to track that also;;;","03/Oct/22 13:38;gyfora;merged to main 1f6a75056acae90e9fab182fd076ee6755b35bbb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connection leak in orc reader,FLINK-29412,13483164,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Sep/22 09:17,26/Sep/22 13:04,04/Jun/24 20:41,26/Sep/22 13:04,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"1. OrcFileStatsExtractor forget closing reader.
2. HadoopReadOnlyFileSystem forget closing fsDataInputStream.

We need a pocket test to assert all connections are closed. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 13:04:53 UTC 2022,,,,,,,,,,"0|z18u2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 13:04;lzljs3620320;master: 29fc9adf023a04b02126f50d915feb55fdbc2327
release-0.2: 311ab67dd2d7dcb4a7a18bff1de6a2da5861964e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESCRIPTOR in flink_fn_execution_pb2.py always None?,FLINK-29411,13483135,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,grimsby,grimsby,26/Sep/22 07:40,26/Sep/22 09:13,04/Jun/24 20:41,26/Sep/22 09:13,1.16.0,,,,,,,,,,,API / Python,,,,0,,,,,"Browsing through the code trying to debug an error I discovered that PR 20685 might have introduced a bug.
{code:java}
// pyflink/fn_execution/flink_fn_execution_pb2.py
...
DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'...')
... {code}
According to dev-requirements.txt is protobuf installed with version <3.18.

DESCRIPTOR will always be None, since AddSerializedFile in protbuf <3.18 will never return anything:
{code:java}
def AddSerializedFile(self, serialized_file_desc_proto):
    """"""Adds the FileDescriptorProto and its types to this pool.
    Args:
      serialized_file_desc_proto (bytes): A bytes string, serialization of the
        :class:`FileDescriptorProto` to add.
    """"""

    # pylint: disable=g-import-not-at-top
    from google.protobuf import descriptor_pb2
    file_desc_proto = descriptor_pb2.FileDescriptorProto.FromString(
        serialized_file_desc_proto)
    self.Add(file_desc_proto) {code}","Python 3.9

Flink built from release-1.16 branch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 09:12:38 UTC 2022,,,,,,,,,,"0|z18tww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 08:24;hxbks2ks;PyFlink's dependency restriction on protobuf<3.18 is specified in setup.py, and this restriction was introduced in FLINK-24305 since 1.14. 
I ran it locally and the method pointed to by _descriptor_pool.Default().AddSerializedFile is google.protobuf.pyext._message.DescriptorPool.

;;;","26/Sep/22 09:12;grimsby;Turns out pip was playing games with me. Changed to conda packages and it now runs well. And I get the same reference to AddSerializedFile asyou do.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add checks to guarantee the non-deprecated options not conflicting with standard YAML,FLINK-29410,13483118,13393472,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,yunta,yunta,26/Sep/22 06:23,12/Jan/23 13:56,04/Jun/24 20:41,12/Jan/23 13:56,,,,,,,,,,,,Runtime / Configuration,Tests,,,0,,,,,"To support the standard YAML parser, we add suffixes to all necessary options in FLINK-29372. However, this cannot guarantee that newly added options would still obey this rule. Thus, we should add test checks to guarantee this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 22 09:00:23 UTC 2022,,,,,,,,,,"0|z18tt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/22 09:00;wanglijie;I think this work has already been done in FLINK-29372? (The ConfigOptionsYamlSpecTest);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for VarianceThresholdSelector ,FLINK-29409,13483106,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,26/Sep/22 04:23,23/Nov/22 09:28,04/Jun/24 20:41,12/Oct/22 09:30,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add Transformer and Estimator for VarianceThresholdSelector ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 09:30:56 UTC 2022,,,,,,,,,,"0|z18tqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 09:30;Jiang Xin;Merged to master via ebc80ceab35ebee33e33f85704e3f4d6a85a9fd4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogITCase failed with NPE,FLINK-29408,13483098,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,hxbks2ks,hxbks2ks,26/Sep/22 03:02,20/Oct/22 02:34,04/Jun/24 20:41,11/Oct/22 11:27,1.16.0,1.17.0,,,,,,1.16.0,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-09-25T03:41:07.4212129Z Sep 25 03:41:07 [ERROR] org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf  Time elapsed: 0.098 s  <<< ERROR!
2022-09-25T03:41:07.4212662Z Sep 25 03:41:07 java.lang.NullPointerException
2022-09-25T03:41:07.4213189Z Sep 25 03:41:07 	at org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase.testFlinkUdf(HiveCatalogUdfITCase.java:109)
2022-09-25T03:41:07.4213753Z Sep 25 03:41:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-25T03:41:07.4224643Z Sep 25 03:41:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-25T03:41:07.4225311Z Sep 25 03:41:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-25T03:41:07.4225879Z Sep 25 03:41:07 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-25T03:41:07.4226405Z Sep 25 03:41:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-25T03:41:07.4227201Z Sep 25 03:41:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-25T03:41:07.4227807Z Sep 25 03:41:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-25T03:41:07.4228394Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-25T03:41:07.4228966Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-09-25T03:41:07.4229514Z Sep 25 03:41:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T03:41:07.4230066Z Sep 25 03:41:07 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-09-25T03:41:07.4230587Z Sep 25 03:41:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-09-25T03:41:07.4231258Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-25T03:41:07.4231823Z Sep 25 03:41:07 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-25T03:41:07.4232384Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-25T03:41:07.4232930Z Sep 25 03:41:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-25T03:41:07.4233511Z Sep 25 03:41:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-25T03:41:07.4234039Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-25T03:41:07.4234546Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-25T03:41:07.4235057Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-25T03:41:07.4235573Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-25T03:41:07.4236087Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-25T03:41:07.4236635Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-25T03:41:07.4237314Z Sep 25 03:41:07 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-09-25T03:41:07.4238211Z Sep 25 03:41:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T03:41:07.4238775Z Sep 25 03:41:07 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T03:41:07.4239277Z Sep 25 03:41:07 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-09-25T03:41:07.4239769Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-25T03:41:07.4240265Z Sep 25 03:41:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-25T03:41:07.4240731Z Sep 25 03:41:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-25T03:41:07.4241196Z Sep 25 03:41:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-25T03:41:07.4241715Z Sep 25 03:41:07 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-25T03:41:07.4242316Z Sep 25 03:41:07 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-25T03:41:07.4242904Z Sep 25 03:41:07 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-25T03:41:07.4243528Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-25T03:41:07.4244201Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-25T03:41:07.4244883Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-25T03:41:07.4245801Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-25T03:41:07.4246600Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-25T03:41:07.4247226Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-25T03:41:07.4247808Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-25T03:41:07.4248449Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-25T03:41:07.4249567Z Sep 25 03:41:07 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-25T03:41:07.4250222Z Sep 25 03:41:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-25T03:41:07.4250889Z Sep 25 03:41:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-25T03:41:07.4251559Z Sep 25 03:41:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-25T03:41:07.4252193Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-25T03:41:07.4252776Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-25T03:41:07.4253335Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-25T03:41:07.4253884Z Sep 25 03:41:07 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41316&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199",,,,,,,,,,,,,,,,,,,,,FLINK-29407,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 11:27:13 UTC 2022,,,,,,,,,,"0|z18too:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 06:02;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41366&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","27/Sep/22 06:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41367&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","27/Sep/22 06:20;hxbks2ks;cc [~luoyuxia];;;","28/Sep/22 07:09;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41396&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=24932;;;","28/Sep/22 07:10;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41397&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26033;;;","28/Sep/22 09:35;luoyuxia;I try to debug the failure in this pr [https://github.com/apache/flink/pull/20905].

I found when I just change the parameters for CI:

 
{code:java}
test_pool_definition: name: Default{code}
 

to 

 
{code:java}
test_pool_definition: vmImage: 'ubuntu-20.04' 
{code}
 

 

The ci will fail [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41403&view=results].

But when I revert such changes, it pass again [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41415&view=results]

I'm confused about it. [~hxbks2ks] Do you know what's the difference between `name: Default` and `vmImage: 'ubuntu-20.04' `?;;;","28/Sep/22 11:56;chesnay;If you change the {{test_pool_definition}} you run the tests on azure machines, and not our CI machines. Naturally you shouldn't do that.

Note that it may just be a timing thing. I saw plenty of those NPEs in my personal azure which also uses azure machines.;;;","06/Oct/22 08:56;dwysakowicz;It fails reliable most of the time on my private Azure. Any progress on that?

https://dev.azure.com/wysakowiczdawid/Flink/_build/results?buildId=1531&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=25733;;;","11/Oct/22 11:27;chesnay;master: 23e027230838ac5728b2974f02235f512c97010c
1.16: 4403ea5811d9d1edc5896818081005c4d6237efa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogITCase.testCsvTableViaSQL failed with FileNotFoundException,FLINK-29407,13483096,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hxbks2ks,hxbks2ks,26/Sep/22 02:46,28/Sep/22 09:22,04/Jun/24 20:41,28/Sep/22 09:22,1.16.0,,,,,,,,,,,Connectors / Hive,,,,0,test-stability,,,,"
{code:java}
2022-09-25T04:37:47.3992924Z Sep 25 04:37:47 [ERROR] Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 65.785 s <<< FAILURE! - in org.apache.flink.table.catalog.hive.HiveCatalogITCase
2022-09-25T04:37:47.4025795Z Sep 25 04:37:47 [ERROR] org.apache.flink.table.catalog.hive.HiveCatalogITCase.testCsvTableViaSQL  Time elapsed: 5.584 s  <<< ERROR!
2022-09-25T04:37:47.4026393Z Sep 25 04:37:47 java.lang.RuntimeException: Failed to fetch next result
2022-09-25T04:37:47.4027006Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-09-25T04:37:47.4046427Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-09-25T04:37:47.4047674Z Sep 25 04:37:47 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
2022-09-25T04:37:47.4048848Z Sep 25 04:37:47 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-09-25T04:37:47.4051301Z Sep 25 04:37:47 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
2022-09-25T04:37:47.4052042Z Sep 25 04:37:47 	at org.apache.flink.table.catalog.hive.HiveCatalogITCase.testCsvTableViaSQL(HiveCatalogITCase.java:140)
2022-09-25T04:37:47.4052654Z Sep 25 04:37:47 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-25T04:37:47.4053295Z Sep 25 04:37:47 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-25T04:37:47.4053909Z Sep 25 04:37:47 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-25T04:37:47.4054470Z Sep 25 04:37:47 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-25T04:37:47.4055022Z Sep 25 04:37:47 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-25T04:37:47.4055655Z Sep 25 04:37:47 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-25T04:37:47.4056250Z Sep 25 04:37:47 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-25T04:37:47.4056855Z Sep 25 04:37:47 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-25T04:37:47.4057453Z Sep 25 04:37:47 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-25T04:37:47.4058013Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-25T04:37:47.4058592Z Sep 25 04:37:47 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-25T04:37:47.4059176Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-25T04:37:47.4059731Z Sep 25 04:37:47 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-25T04:37:47.4060350Z Sep 25 04:37:47 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-25T04:37:47.4060907Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-25T04:37:47.4061441Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-25T04:37:47.4062200Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-25T04:37:47.4062736Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-25T04:37:47.4063261Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-25T04:37:47.4063821Z Sep 25 04:37:47 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-25T04:37:47.4064407Z Sep 25 04:37:47 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-09-25T04:37:47.4064971Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-25T04:37:47.4065503Z Sep 25 04:37:47 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-25T04:37:47.4066162Z Sep 25 04:37:47 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-25T04:37:47.4066640Z Sep 25 04:37:47 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-25T04:37:47.4067195Z Sep 25 04:37:47 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-25T04:37:47.4067823Z Sep 25 04:37:47 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-25T04:37:47.4068438Z Sep 25 04:37:47 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-25T04:37:47.4069097Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-25T04:37:47.4069798Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-25T04:37:47.4070608Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-25T04:37:47.4071342Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-25T04:37:47.4072064Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-25T04:37:47.4072719Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-25T04:37:47.4073320Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-25T04:37:47.4073980Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-25T04:37:47.4074685Z Sep 25 04:37:47 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-25T04:37:47.4075356Z Sep 25 04:37:47 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-25T04:37:47.4076059Z Sep 25 04:37:47 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-25T04:37:47.4076743Z Sep 25 04:37:47 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-25T04:37:47.4077390Z Sep 25 04:37:47 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-25T04:37:47.4077999Z Sep 25 04:37:47 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-25T04:37:47.4078566Z Sep 25 04:37:47 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-25T04:37:47.4079130Z Sep 25 04:37:47 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-09-25T04:37:47.4079656Z Sep 25 04:37:47 Caused by: java.io.IOException: Failed to fetch job execution result
2022-09-25T04:37:47.4080944Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-09-25T04:37:47.4081794Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-09-25T04:37:47.4082517Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-09-25T04:37:47.4083049Z Sep 25 04:37:47 	... 49 more
2022-09-25T04:37:47.4083523Z Sep 25 04:37:47 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-09-25T04:37:47.4084134Z Sep 25 04:37:47 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-09-25T04:37:47.4084706Z Sep 25 04:37:47 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-09-25T04:37:47.4085339Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-09-25T04:37:47.4085859Z Sep 25 04:37:47 	... 51 more
2022-09-25T04:37:47.4086265Z Sep 25 04:37:47 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-09-25T04:37:47.4086830Z Sep 25 04:37:47 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-09-25T04:37:47.4087503Z Sep 25 04:37:47 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-09-25T04:37:47.4088160Z Sep 25 04:37:47 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-09-25T04:37:47.4088739Z Sep 25 04:37:47 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-09-25T04:37:47.4089395Z Sep 25 04:37:47 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-09-25T04:37:47.4090031Z Sep 25 04:37:47 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-09-25T04:37:47.4090747Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-09-25T04:37:47.4091261Z Sep 25 04:37:47 	... 51 more
2022-09-25T04:37:47.4091698Z Sep 25 04:37:47 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-09-25T04:37:47.4092367Z Sep 25 04:37:47 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-09-25T04:37:47.4093149Z Sep 25 04:37:47 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-09-25T04:37:47.4093882Z Sep 25 04:37:47 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-09-25T04:37:47.4094549Z Sep 25 04:37:47 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-09-25T04:37:47.4164170Z Sep 25 04:37:47 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-09-25T04:37:47.4165013Z Sep 25 04:37:47 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-09-25T04:37:47.4165685Z Sep 25 04:37:47 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-09-25T04:37:47.4166477Z Sep 25 04:37:47 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-09-25T04:37:47.4167126Z Sep 25 04:37:47 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-09-25T04:37:47.4167673Z Sep 25 04:37:47 	at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
2022-09-25T04:37:47.4168224Z Sep 25 04:37:47 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-25T04:37:47.4168761Z Sep 25 04:37:47 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-25T04:37:47.4169579Z Sep 25 04:37:47 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-09-25T04:37:47.4170271Z Sep 25 04:37:47 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-09-25T04:37:47.4171027Z Sep 25 04:37:47 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-09-25T04:37:47.4171649Z Sep 25 04:37:47 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-09-25T04:37:47.4172284Z Sep 25 04:37:47 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-09-25T04:37:47.4172906Z Sep 25 04:37:47 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-09-25T04:37:47.4173462Z Sep 25 04:37:47 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-09-25T04:37:47.4173967Z Sep 25 04:37:47 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-09-25T04:37:47.4174481Z Sep 25 04:37:47 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-09-25T04:37:47.4174996Z Sep 25 04:37:47 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-09-25T04:37:47.4175515Z Sep 25 04:37:47 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-09-25T04:37:47.4176041Z Sep 25 04:37:47 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-09-25T04:37:47.4176571Z Sep 25 04:37:47 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-09-25T04:37:47.4177096Z Sep 25 04:37:47 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-09-25T04:37:47.4177680Z Sep 25 04:37:47 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-09-25T04:37:47.4178142Z Sep 25 04:37:47 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-09-25T04:37:47.4178630Z Sep 25 04:37:47 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-09-25T04:37:47.4179135Z Sep 25 04:37:47 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-09-25T04:37:47.4179611Z Sep 25 04:37:47 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-09-25T04:37:47.4180084Z Sep 25 04:37:47 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-09-25T04:37:47.4180548Z Sep 25 04:37:47 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-09-25T04:37:47.4180989Z Sep 25 04:37:47 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-09-25T04:37:47.4181465Z Sep 25 04:37:47 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-09-25T04:37:47.4181995Z Sep 25 04:37:47 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-09-25T04:37:47.4182573Z Sep 25 04:37:47 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-09-25T04:37:47.4183117Z Sep 25 04:37:47 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-09-25T04:37:47.4184449Z Sep 25 04:37:47 Caused by: java.io.FileNotFoundException: The provided file path file:/__w/1/s/flink-connectors/flink-connector-hive/target/test-classes/csv/test.csv does not exist.
2022-09-25T04:37:47.4185226Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.functions.source.ContinuousFileMonitoringFunction.run(ContinuousFileMonitoringFunction.java:216)
2022-09-25T04:37:47.4185910Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-09-25T04:37:47.4186491Z Sep 25 04:37:47 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-09-25T04:37:47.4187139Z Sep 25 04:37:47 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-09-25T04:37:47.4187626Z Sep 25 04:37:47 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41318&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199
",,,,,,,,,,,,,,,,,,,,FLINK-29408,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-26 02:46:45.0,,,,,,,,,,"0|z18to8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose Finish Method For TableFunction,FLINK-29406,13483089,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,26/Sep/22 01:28,08/Oct/22 02:44,04/Jun/24 20:41,08/Oct/22 02:44,1.14.5,1.15.2,1.16.0,,,,,1.17.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"FLIP-260: Expose Finish Method For TableFunction
https://cwiki.apache.org/confluence/display/FLINK/FLIP-260%3A+Expose+Finish+Method+For+TableFunction",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 02:44:35 UTC 2022,,,,,,,,,,"0|z18tmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 02:44;lzljs3620320;master: 1f2001cfd28dbf6faaaadecdb2645052cdd61c84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputFormatCacheLoaderTest is unstable,FLINK-29405,13483080,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,chesnay,chesnay,25/Sep/22 17:23,06/Feb/23 05:53,04/Jun/24 20:41,06/Feb/23 02:14,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,Table SQL / Runtime,,,,0,pull-request-available,test-stability,,,"#testExceptionDuringReload/#testCloseAndInterruptDuringReload fail reliably when run in a loop.

{code}
java.lang.AssertionError: 
Expecting AtomicInteger(0) to have value:
  0
but did not.

	at org.apache.flink.table.runtime.functions.table.fullcache.inputformat.InputFormatCacheLoaderTest.testCloseAndInterruptDuringReload(InputFormatCacheLoaderTest.java:161)
{code}",,,,,,,,,,,,,,,,,,,,FLINK-30103,FLINK-29463,,,,,,,,FLINK-28948,,,,FLINK-29463,,,,,,FLINK-30354,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 02:26:51 UTC 2023,,,,,,,,,,"0|z18tko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 02:29;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41328&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4 1.16 instance;;;","26/Sep/22 02:31;hxbks2ks;[~smiralex][~renqs] Could you help take a look? Thx.;;;","12/Oct/22 03:01;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41881&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","18/Oct/22 08:23;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94;;;","24/Oct/22 06:17;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","24/Oct/22 06:20;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42340&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11649;;;","04/Nov/22 06:16;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42808&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f;;;","05/Nov/22 05:38;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42837&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10878;;;","09/Nov/22 07:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42955&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=11245;;;","09/Nov/22 14:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42968&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11609;;;","22/Nov/22 11:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43216&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=11334;;;","23/Nov/22 08:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43398&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10945;;;","25/Nov/22 04:41;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10750]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43483&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11317;;;","28/Nov/22 10:35;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43530&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10582;;;","29/Nov/22 06:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43572&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10872;;;","01/Dec/22 08:29;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43636&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10879;;;","05/Dec/22 09:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43692&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10755;;;","05/Dec/22 10:39;mapohl;2x in the same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43700&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=10877
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43700&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10757;;;","05/Dec/22 11:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43711&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11259;;;","06/Dec/22 08:32;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43744&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11616;;;","10/Dec/22 12:35;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43851&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","11/Dec/22 07:03;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43861&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","12/Dec/22 10:23;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43871&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","12/Dec/22 10:25;leonard;I upgrade this priority to blocker as too many failures happens, Could you take a look when you have time [~renqs] [~smiralex] ?;;;","16/Dec/22 08:23;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43960&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10804;;;","16/Dec/22 08:29;martijnvisser;release-1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44002&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11259;;;","19/Dec/22 09:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44025&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11257;;;","19/Dec/22 09:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44034&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11084;;;","02/Jan/23 09:37;mapohl;Copied from FLINK-29463: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44249&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11616;;;","02/Jan/23 09:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44309&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=10718;;;","02/Jan/23 10:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44366&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10289;;;","09/Jan/23 07:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44553&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11690;;;","18/Jan/23 07:44;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11633;;;","23/Jan/23 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45142&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","23/Jan/23 08:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45148&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","24/Jan/23 07:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45157&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9723;;;","24/Jan/23 07:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","26/Jan/23 07:53;mapohl;1 build, 2 failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9719
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=10001;;;","27/Jan/23 07:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45230&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10958;;;","30/Jan/23 08:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9718;;;","30/Jan/23 08:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45353&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10956;;;","31/Jan/23 04:13;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45443&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/Jan/23 07:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9717;;;","01/Feb/23 09:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10032;;;","01/Feb/23 09:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45522&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10959;;;","01/Feb/23 09:32;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45490&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9740;;;","02/Feb/23 08:42;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=9716;;;","02/Feb/23 09:02;mapohl;same run, multiple times:
* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10956]
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11779;;;","02/Feb/23 12:18;renqs;Fixed on master: 64e8c349c4ee04623f2c874314a763f9fc1a3054;;;","03/Feb/23 10:12;mapohl;I'm reopening this one. Shouldn't we also provide a 1.16 backport, [~renqs]?;;;","03/Feb/23 12:24;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45678&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11337;;;","06/Feb/23 02:13;renqs;Thanks for the reminder [~mapohl] 

Backported to 1.16: 793b0f791c4db721d2c81d2252ff785e4731be07;;;","06/Feb/23 02:20;qingyue;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45662&view=logs&j=086353db-23b2-5446-2315-18e660618ef2&t=6cd785f3-2a2e-58a8-8e69-b4a03be28843&l=10314;;;","06/Feb/23 02:26;leonard; [~qingyue] Could you rebase you PR to latest master and re-trigger the CI? I checked you CI branch which does not  contain the fixed commit.;;;",,,,,,,,,,,,,,,,,,,,,,
Fix build failure due to unavailable dependency,FLINK-29404,13483022,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,byyue,byyue,24/Sep/22 06:55,26/Sep/22 07:02,04/Jun/24 20:41,26/Sep/22 07:02,1.16.0,,,,,,,,,,,Connectors / Hive,Table SQL / Client,,,0,pull-request-available,,,,"_hive-exec-2.3.9_ is imported in _flink-connector-hive_ and _flink-sql-client_ whereas {_}pentaho-aggdesigner-algorithm{_}, one of its dependencies, is no longer available in the Maven Central Repository.

 

[*ERROR*] Failed to execute goal on project flink-connector-hive_2.12: *Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.17-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde*: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [repository.jboss.org (http://repository.jboss.org/nexus/content/groups/public/, default, disabled), conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> *[Help 1]*

 

The solution is to add another repository containing this package.",,,,,,,,,,,,,,,,,,,,FLINK-27640,,,,,,,,,,,,,,,,,,,,"24/Sep/22 06:54;byyue;build_failure.png;https://issues.apache.org/jira/secure/attachment/13049712/build_failure.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-24 06:55:39.0,,,,,,,,,,"0|z18t7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streamline SimpleCondition usage,FLINK-29403,13482980,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,23/Sep/22 18:43,04/Oct/22 09:27,04/Jun/24 20:41,04/Oct/22 09:27,,,,,,,,1.17.0,,,,Library / CEP,,,,0,pull-request-available,,,,"CEP SimpleCondition are essentially filter functions, but since it's an abstract class it ends up being incredibly verbose. We can add a simple factory method to streamline this.

Additionally the class should not be annotated with {{@Internal}} given how much it is advertised in the docs.",,,,,,,,,,,FLINK-29422,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 04 09:27:44 UTC 2022,,,,,,,,,,"0|z18syg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/22 09:27;chesnay;master: 990a7dac90af5ea7e8450521a63802c2563f4548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add USE_DIRECT_READ configuration parameter for RocksDB,FLINK-29402,13482946,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Not A Problem,,donaschmi,donaschmi,23/Sep/22 13:56,27/Nov/22 02:44,04/Jun/24 20:41,20/Oct/22 09:05,1.16.0,,,,,,,1.17.0,,,,Runtime / State Backends,,,,0,Enhancement,pull-request-available,rocksdb,,"RocksDB allows the use of DirectIO for read operations to bypass the Linux Page Cache. To understand the impact of Linux Page Cache on performance, one can run a heavy workload on a single-tasked Task Manager with a container memory limit identical to the TM process memory. Running this same workload on a TM with no container memory limit will result in better performances but with the host memory exceeding the TM requirement.

Linux Page Cache are of course useful but can give false results when benchmarking the Managed Memory used by RocksDB. DirectIO is typically enabled for benchmarks on working set estimation [Zwaenepoel et al.|[https://arxiv.org/abs/1702.04323].]

I propose to add a configuration key allowing users to enable the use of DirectIO for reads thanks to the RocksDB API. This configuration would be disabled by default.",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/22 08:48;donaschmi;directIO-performance-comparison.png;https://issues.apache.org/jira/secure/attachment/13049746/directIO-performance-comparison.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Thu Oct 20 09:03:18 UTC 2022,,,,,,,,,,"0|z18sr4:",9223372036854775807,The changes do not break previous releases.,,,,,,,,,,,,,,,,,,,"26/Sep/22 03:04;Yanfei Lei;This is a very interesting proposal, I think this is not hard to implement in Flink. From the [wiki|https://github.com/facebook/rocksdb/wiki/Direct-IO] there are two options to control the DirectIO: {{use_direct_reads}} and {{use_direct_io_for_flush_and_compaction, }}and these two options are supported by current{{{} frocksdb-jni(6.20.3){}}}.  

BTW, do you have quantitative benchmark results about DirectIO *ON* vs DirectIO {*}OFF{*}?;;;","26/Sep/22 05:38;donaschmi;Indeed, with RocksDB API it is easy to add this new option (two new options with {{use_direct_io_for_flush_and_compaction). }}I am not quite familiar with the process of adding a new option, e.g adding it to the doc, localization, ..., but if you give me some resources about guidelines I'll be happy to create a PR.

 

I will edit my ticket later to add some examples with Grafana of the impact of DirectIO on performance.

Edit: text formatting;;;","26/Sep/22 08:58;donaschmi;I added an image showing the behaviour of an identical job under different configuration.

The job is the one used in the e2e test of rocksDB memory control: [https://github.com/apache/flink/blob/master/flink-end-to-end-tests/flink-rocksdb-state-memory-control-test/src/main/java/org/apache/flink/streaming/tests/RocksDBStateMemoryControlTestProgram.java]

The parameters used are as follows:
 * keyspace: 1.000.000
 * payload size: 5.000
 * The workload stops after sending 3Gb worth of records.
 * The mapper used is the ValueStateMapper
 * The payload is not appended to existing key but replacing the previous one.

The first column shows the job running on a 2Gb TM with a container limit of 2Gb. On the first graph we have the backpressure endured by the previous operator (Source/split). The second graph shows that the TM is using Around 1Gb of memory (managed + heap + network + ...) but the pod is effectively using all of the available memory for Linux Page Cache. The third graph shows the ingestion in Mb/s). The last two graphs are RocksDB metrics: cache hit + miss, and cache usage.

The second colunm shows the same workload with the same amount of TM memory but with a container limit of 20Gb. The second graph shows that the container memory raises above the TM specification (around 5Gb, which is the size of the estimated state: 1.000.000 key multiplied by 5.000 bytes). We can see that there is a strongly decreased backpressure as well a better performances on the third graph.

The last column shows the same configuration as the second but with directIO enabled, thus not using Linux Page Cache. The graphs look similar to the first column as expected.;;;","26/Sep/22 10:11;Yanfei Lei;Thanks for your benchmark results. Comparing the second colunm and the third colunm, “num bytes in per second” becomes lower, maybe we should point this out in the documentation.

AFAIK, there isn't a guideline about adding a new option, but I think you can refer to this ticket to start: https://issues.apache.org/jira/browse/FLINK-20496

CC [~yunta], [~yuanmei] ;;;","27/Sep/22 14:02;yunta;Thanks for creating this ticket. From my understanding, this option would not be used in production environments. For benchmarking cases, I believe some streaming systems benchmarks would not enable direct IO, such as https://github.com/nexmark/nexmark , https://www.databricks.com/blog/2017/10/11/benchmarking-structured-streaming-on-databricks-runtime-against-state-of-the-art-streaming-systems.html, and https://github.com/Klarrio/open-stream-processing-benchmark . 
Moreover, we could still let these options enabled via code, I don't think it's so useful to introduce these two options considering we already have so many options.;;;","03/Oct/22 08:03;donaschmi;Thanks for your comment! Considering that Facebook uses DirectIO for reads and writes when performing benchmarks ([https://www.usenix.org/system/files/fast20-cao_zhichao.pdf)] on RocksDB, I would say it is best practice to also enable DirectIO for Flink benchmarks using RocksDB. Disabling DirectIO can lead to unpredictable experiments depending on 1. the container memory limit 2. the amount of free heap memory used by the Page Cache. Again I understand that it is only for research purposes and agree that this could be done programmatically.;;;","20/Oct/22 07:41;ym;Hey [~donaschmi] ! Thanks for the proposal.
 * I understand enabling/disabling DirectIO leads to different performance results. That seems obvious because of page caching.
 * Wondering whether this option is introduced purely for benchmarking or research performance testing?
 * If yes, I would be hesitant to introduce a new option purely for testing purposes. I share the same concern as Yun Tang. 
 * Rocksdb Configuration options have already been complicated, and we should not introduce more to confuse normal users if not having to.;;;","20/Oct/22 08:40;donaschmi;Hey Yuan! This concern of enabling DirectIO for benchmarking came from a Facebook paper ([https://www.usenix.org/system/files/fast20-cao_zhichao.pdf)|https://www.usenix.org/system/files/fast20-cao_zhichao.pdf] where they disabled page caching for their benchmark. It makes sense to avoid any external phenomenon that could interfere with the actual performance of a k-v store. 
In the case of benchmarking Flink stateful stream processing, I would say that this observation also stands. But I 100% agree that it is only for research purposes and should not be introduced to normal users.;;;","20/Oct/22 09:03;ym;Hey [~donaschmi] ,
 # benchmarking for a pure k-v store ruling out other factors (like page cache) totally makes sense.
 # However, from Flink perspective, it is more reasonable to take Flink engine as an entire piece. In this case, most likely we should and need to use Page Cache. Benchmarking with page cache aligns better with a real-world use case. But that's a different topic, I would say.

Since you agree as well that we do not introduce a new config purely for benchmarking purposes, I am going to close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve observer structure,FLINK-29401,13482936,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,23/Sep/22 12:42,14/Oct/22 13:46,04/Jun/24 20:41,14/Oct/22 13:46,,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The AbstractDeploymentObserver and SessionJobObserver at this point share a lot of common logic due to the unification of other parts.

We should factor out the common parts into an abstract base observer class.

Furthermore we should move the logic of the SavepointObserver into the JobStatusObserver where it logically belongs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 13:46:32 UTC 2022,,,,,,,,,,"0|z18sow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 13:46;gyfora;merged to main bca630c3a003149683b74a54ccd376f9a9691028;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default Value of env.log.max in documentation is incorrect,FLINK-29400,13482830,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,dhrapate,dhrapate,22/Sep/22 22:42,07/Oct/22 11:28,04/Jun/24 20:41,07/Oct/22 11:28,,,,,,,,1.17.0,,,,Documentation,,,,0,,,,,"The default value of env.log.max is 10 as per the code in master ([https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/bin/config.sh#L137).]

However the Flink Documentation says the default value is 5 (https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#env-log-max) which is incorrect ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 11:28:50 UTC 2022,,,,,,,,,,"0|z18s2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 02:11;StarBoy1005;Hi ,if no more modify like ""env.log.max"" in conf/flink-conf.yaml  ,i think the default is 10,cause ""DEFAULT_ENV_LOG_MAX"" in config.sh is in use.Then influence  ""appender.rolling.strategy.max""  or ""appender.main.strategy.max"" in log4j* config files.
So,i guess the document should update:D;;;","07/Oct/22 11:28;chesnay;master: 912d00a8816efafb43b5fd4f3e7545df6133ce90;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableITCase is unstable,FLINK-29399,13482828,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,22/Sep/22 21:06,04/Mar/24 11:56,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Table SQL / Planner,Tests,,,0,pull-request-available,,,,"    

{code:java}
    val it = tableResult.collect()
    it.close()
    val jobStatus =
      try {
        Some(tableResult.getJobClient.get().getJobStatus.get())
      } catch {
        // ignore the exception,
        // because the MiniCluster maybe already been shut down when getting job status
        case _: Throwable => None
      }
    if (jobStatus.isDefined) {
      assertNotEquals(jobStatus.get, JobStatus.RUNNING)
    }
{code}

There's no guarantee that the cancellation already went through. The test should periodically poll the job status until another state is reached.
Or even better, use the new collect API, call execute in a separate thread, close the iterator and wait for the thread to terminate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-22 21:06:31.0,,,,,,,,,,"0|z18s2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utilize Rack Awareness in Flink Consumer,FLINK-29398,13482827,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeremy.degroot,jeremy.degroot,jeremy.degroot,22/Sep/22 20:34,26/Jan/24 12:56,04/Jun/24 20:41,29/Sep/23 19:54,,,,,,,,kafka-3.1.0,,,,Connectors / Kafka,,,,0,pull-request-available,stale-assigned,,,"[KIP-36|https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment] was implemented some time ago in Kafka. This allows brokers and consumers to communicate about the rack (or AWS Availability Zone) they're located in. Reading from a local broker can save money in bandwidth and improve latency for your consumers.

Flink Kafka consumers currently cannot easily use rack awareness if they're deployed across multiple racks or availability zones, because they have no control over which rack the Task Manager they'll be assigned to may be in. 

This improvement proposes that a Kafka Consumer could be configured with a callback or Future that could be run when it's being configured on the task manager, that will set the appropriate value at runtime if a value is provided. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 29 19:53:50 UTC 2023,,,,,,,,,,"0|z18s28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 20:37;jeremy.degroot;I'll provide a little further justification and background for this in a comment. At my job we were tasked with reducing our AWS spend, and one place we found that could be improved was Inter-AZ bandwidth. We implemented something similar to what I describe above, and realized significant savings (bringing our billable bandwidth from 60% of the total down to 20%). It seems likely other people would also like to save money in this fashion. If this gets taken up, we'd also be willing to provide our implementation as a basis for development.;;;","23/Sep/22 07:04;martijnvisser;[~renqs] What do you think?;;;","23/Sep/22 11:02;renqs;Thanks for starting the discussion [~jeremy.degroot] ! This is a very interesting also useful feature as you described. 

Under the design of FLIP-27 Source API we do expose host name of source readers when registering readers on the split enumerator, so it's possible to let split enumerator to make assignments according to the mapping of rack and hostname of readers. File source has already implemented this feature (see {{{}LocalityAwareSplitAssigner{}}}). Currently the split assigning strategy of Kafka source is a hard-coded one, so in order to achieve this in KafkaSource we need to design a new API to let users provide pluggable split assigner for split enumerator in Kafka source. 

Moreover a fully optimized solution would be that Flink scheduler could also schedule tasks based on locality, but this is beyond the discussion of this ticket. 

[~jeremy.degroot] I'm not sure if your solution is based on the new KafkaSource instead of the deprecated FlinkKafkaConsumer (we won't add new features to the deprecated one). Would you like to lead the design of this feature? I think a new FLIP is expected for this as we are introducing a new feature to Kafka source. ;;;","27/Sep/22 17:26;jeremy.degroot;[~renqs] Yes, I'd be happy to lead the design of this feature! 

We currently run Flink 1.14 in production, and so we wanted to move from the FlinkKafkaConsumer to the KafkaSource API and that's what our first POC of this feature was based on. However we weren't happy with the performance and behavior of the KafkaSource in the version we were running at the time (the 1.14.2 release, if I remember correctly) so we implemented our final version on FlinkKafkaConsumer. I'd be happy to target the KafkaSource API for this.;;;","28/Sep/22 09:44;renqs;[~jeremy.degroot] Thanks for the contribution! Feel free to start a discussion thread in the dev mailing list or create a FLIP page once you are ready. ;;;","13/Oct/22 06:20;mason6345;+1 for this feature, it was a lot easier to implement in FlinkKafkaConsumer since you could extend the open method :);;;","13/Oct/22 07:40;martijnvisser;Let me know if any of you needs permissions to create a FLIP page;;;","17/Oct/22 15:35;jeremy.degroot;[~martijnvisser] I just signed into the Confluence and I'm unable to create a FLIP page. My username is the same over there.;;;","20/Oct/22 10:22;martijnvisser;[~jeremy.degroot] You should have permission now

One side question: is something similar necessary for Flink to support KIP-392 (Closest Replica)?;;;","21/Oct/22 17:40;jeremy.degroot;[~martijnvisser] If I'm reading KIP-392 correctly, that's been implemented to take advantage of metadata like rack awareness. By implementing this, we'd get the benefit of KIP-392 (at least as it relates to the rack ID).;;;","21/Oct/22 19:35;jeremy.degroot;Here's the FLIP page I made for this https://cwiki.apache.org/confluence/display/FLINK/FLIP-268%3A+Kafka+Rack+Awareness;;;","29/Oct/22 01:33;mason6345;Hi [~jeremy.degroot] I also implemented something similarly internally and can help with the review. Can you start a discussion thread on the dev mailing list on the FLIP so we can move this forward? You can look to other FLIPs for example discussions;;;","31/Oct/22 14:29;jeremy.degroot;[~mason6345] yeah, I'll get that going this week.

 

Also, an update on timing for this. My team are planning to use some self-directed time we have during the holidays due to code freezes to work on this. So we won't be starting until around Thanksgiving, but we should be able to devote some time to it and make rapid progress when we do start.;;;","02/Nov/22 09:15;renqs;[~jeremy.degroot] Thanks for the FLIP! Could you start a discussion thread for this FLIP in the dev mailing list?;;;","11/Nov/22 15:42;jeremy.degroot;Someone on the dev list pointed out I had the wrong KIP referenced here. I fixed it.;;;","13/Apr/23 08:08;chiggi_dev;This is a good thread and a useful feature. Thanks for this.

When can I expect this to be delivered as a Flink release?

Also, any more insights on how you used it in your multi-AZ cluster? Are you deploying a stretched Flink cluster or a Flink cluster in every AZ in some active-active or active-passive mode? 

With a stretched cluster, I assume we will simply replace the broker-consumer cost with the TaskManager-JobManager networking cost. Is this assumption correct?;;;","13/Apr/23 14:46;jeremy.degroot;[~chiggi_dev] There is a PR open for it now (https://github.com/apache/flink-connector-kafka/pull/20), so hopefully in the next release or two. 

Regarding how we use it, we have an MSK cluster stretched across three AZs and a Flink cluster across those same AZs. Since our consumers do a lot of feature extraction, filtering, mapping, and windowing of the data reduces the volume significantly from what is initially read in from kafka. What gets sent to downstream processors and sinks is orders of magnitude smaller and less expensive. If your workflow doesn't reduce intra-cluster traffic as dramatically, you'll probably want to look carefully at your partitioning and chaining choices in your jobs.;;;","14/Apr/23 03:18;chiggi_dev;Yeah, that makes sense. Thanks [~jeremy.degroot] for taking this up. ;;;","15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","29/Sep/23 19:53;tzulitai;Thanks for driving this to the finish line [~jeremy.degroot].

Merged to apache/flink-connector-kafka:main with d89a082180232bb79e3c764228c4e7dbb9eb6b8b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in StreamTask can lead to NPE if changelog is disabled,FLINK-29397,13482790,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,22/Sep/22 14:35,22/Sep/22 23:17,04/Jun/24 20:41,22/Sep/22 23:17,1.15.0,,,,,,,1.15.3,1.16.0,,,Runtime / Task,,,,0,pull-request-available,,,,"{{StreamTask#processInput}} contains a branch where the changelogWriterAvailabilityProvider is accessed without a null check; this field however is nullable in case the changelog is disabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 19:47:58 UTC 2022,,,,,,,,,,"0|z18ru0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 14:38;chesnay;This is trivially reproducible by disabling the changelog int he StreamTaskTest.;;;","22/Sep/22 19:47;chesnay;master: 162db046e1c63e4610393d14cd9843962321915e
1.16: a8979b29e084641a5160768f48400682a5d79bbb
1.15: a5ae2fa25522084c1616a8d11e3c1d1152f08b29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in JobMaster shutdown can leak resource requirements,FLINK-29396,13482788,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,chesnay,chesnay,22/Sep/22 14:33,30/Sep/22 06:21,04/Jun/24 20:41,,1.15.0,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"When a JobMaster is stopped it
a) sends a message to the RM informing it of the final job status
b) removes itself as the leader.

Once the JM loses leadership the RM is also informed about that.

With that we have 2 messages being sent to the RM at about the same time.
If the shutdown notifications arrives first (and job is in a terminal state) we wipe the resource requirements, and the leader loss notification is effectively ignored.
If the leader loss notification arrives first we keep the resource requirements, assuming that another JM will pick the job up later on, and the shutdown notification will be ignored.

This can cause a session cluster to essentially do nothing until the job timeout is triggered due to no leader being present (default 5 minutes).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 13:51:07 UTC 2022,,,,,,,,,,"0|z18rtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 14:38;chesnay;This is a bit of a head scratcher.
We can't (or shouldn't) wipe the requirements if the shutdown notification arrives later, since the message does not come from the current leader.
Maybe we should wait with discarding the leadership until we have actually cleaned up the requirements on the RM.

[~mapohl] Any other ideas?;;;","23/Sep/22 15:40;mapohl;I guess, you have a point here. Initially, I thought that there's a message back from the {{ResourceManager}} to the {{JobMaster}} missing. That would then trigger the shutdown of the {{JobMaster}} and, as a consequence, trigger the stopping of the corresponding {{JobMasterServiceLeadershipRunner}} (in [JobMasterServiceLeadershipRunner:126|https://github.com/apache/flink/blob/e8a91fd8428e417c63b299392a84f7df9d10ddb8/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMasterServiceLeadershipRunner.java#L126]). But this message is actually send in [ResourceManager#closeJobManagerConnection:1072|https://github.com/apache/flink/blob/0263b55288be7b569f56dd42a94c5e48bcc1607b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java#L1072]

Essentially, we would have to instantiate a {{CompletableFuture}} in [JobMaster#stopExecution:1022|https://github.com/apache/flink/blob/b7dd42617a46fcecfffbea3409391e204a40b9b1/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L1022], compose it with the {{terminationFuture}} there and let this future be completed in [JobMaster#disconnectResourceManager|https://github.com/apache/flink/blob/b7dd42617a46fcecfffbea3409391e204a40b9b1/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java#L817]. That will make the JobMaster shutdown process proceed after we got the confirmation from the {{ResourceManager}} that the disconnect succeeded. WDYT?;;;","27/Sep/22 12:53;chesnay;The question is when the RM can send that notification.

The disconnect calls are weirdly cyclic and have 0 notion of who initiated it initially.
The JM calls {{ResourceManagerGateway#disconnectJobManager}} which ends with the RM calling {{JobMasterGateway#disconnectResourceManager}}.
But this sequence can also happen in reverse; so who waits for who?

Maybe we should rework these methods to actually be an {{ask}}.

The shutdown sequence is is actually super annoying in general because even an orderly shutdown by the mini cluster invariably leads to someone getting an exception because the other party has already shut down.;;;","28/Sep/22 05:11;mapohl;hm, good point. And I agree on the shutdown mechanism being annoying. :D Yeah, doing the shutdown in two phases would be the proper way then, I guess.;;;","28/Sep/22 05:17;mapohl;But why did you make this a Blocker. This is not new behavior. And as far as I understand, we just don't release the resources for the job in a timely manner which might limit the execution of other jobs but eventually will be resolved through the timeout. Or am I missing something here?;;;","29/Sep/22 13:51;chesnay;Yes it shouldn't be a blocker.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Kinesis][EFO] Issue using EFO consumer at timestamp with empty shard,FLINK-29395,13482777,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,22/Sep/22 13:21,20/Oct/22 02:33,04/Jun/24 20:41,10/Oct/22 13:42,1.12.7,1.13.6,1.14.5,1.15.2,,,,1.15.3,1.16.0,1.17.0,,Connectors / Kinesis,,,,0,pull-request-available,,,,"*Background*

The consumer fails when an EFO record publisher uses a timestamp sentinel starting position, the first record batch is not empty, but the first deaggregated record batch is empty. This can happen if the user explicitly specifies the hashkey in the KPL, and does not ensure that the explicitHashKey of every record in the aggregated batch is the same.

When resharding occurs, the aggregated record batch can have records that are out of the shard's hash key range. This causes the records to be dropped when deaggregating, and can result in this situation, where record batch is not empty, but the deaggregated record batch is empty.

The symptom seen is similar to the issue seen in https://issues.apache.org/jira/browse/FLINK-20088.

See [here|https://github.com/awslabs/kinesis-aggregation/blob/master/potential_data_loss.md] and [here|https://github.com/awslabs/kinesis-aggregation/issues/11] for a more detailed explanation

*Replicate*

Get shard information
{code:java}
aws kinesis describe-stream --stream-name <stream_name>
{
    ""StreamDescription"": {
        ""Shards"": [
            ...
            {
                ""ShardId"": ""shardId-000000000037"",
                ""ParentShardId"": ""shardId-000000000027"",
                ""HashKeyRange"": {
                    ""StartingHashKey"": ""272225893536750770770699685945414569164"",
                    ""EndingHashKey"": ""340282366920938463463374607431768211455""
                }
            ...
            },
            {
                ""ShardId"": ""shardId-000000000038"",
                ""ParentShardId"": ""shardId-000000000034"",
                ""AdjacentParentShardId"": ""shardId-000000000036"",
                ""HashKeyRange"": {
                    ""StartingHashKey"": ""204169420152563078078024764459060926873"",
                    ""EndingHashKey"": ""272225893536750770770699685945414569163""
                }
            ...
            }
        ]
...
    }
}{code}
Create an aggregate record with two records, each with explicit hash keys belonging to different shards
{code:java}
RecordAggregator aggregator = new RecordAggregator();
String record1 = ""RECORD_1"";
String record2 = ""RECORD_2"";
aggregator.addUserRecord(""pk"", ""272225893536750770770699685945414569162"", record1.getBytes());
aggregator.addUserRecord(""pk"", ""272225893536750770770699685945414569165"", record2.getBytes());

AmazonKinesis kinesisClient = AmazonKinesisClient.builder()
   .build();
kinesisClient.putRecord(aggregator.clearAndGet().toPutRecordRequest(""EFOStreamTest"")); {code}
Consume from given stream whilst specifying a Timestamp where the only record retrieved is the record above.

*Error*
{code:java}
java.lang.IllegalArgumentException: Unexpected sentinel type: AT_TIMESTAMP_SEQUENCE_NUM
	at org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.fromSentinelSequenceNumber(StartingPosition.java:115)
	at org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.fromSequenceNumber(StartingPosition.java:91)
	at org.apache.flink.streaming.connectors.kinesis.model.StartingPosition.continueFromSequenceNumber(StartingPosition.java:72)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.lambda$run$0(FanOutRecordPublisher.java:120)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.consumeAllRecordsFromKinesisShard(FanOutShardSubscriber.java:356)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:188)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:154)

	at 
org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:123)
	at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}
 

*Solution*

This is fixed by reusing the existing timestamp starting position in this condition.",,,,,,,,,,,,,,FLINK-20088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 13:01:32 UTC 2022,,,,,,,,,,"0|z18rr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 13:01;dannycranmer;Merged commit [{{ef93ae4}}|https://github.com/apache/flink/commit/ef93ae4525ea42a87baf77747dd3ffbc007112fd] into master
Merged commit [{{7a1dccd}}|https://github.com/apache/flink/commit/7a1dccd3020ffefe83f1fa80ab70bc3150144640] into release-1.16 
Merged commit [{{ae20e52}}|https://github.com/apache/flink/commit/ae20e524671c26d87f4ffedd171c1ed3418be6d8] into release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink k8s operator observe Flink job restart count,FLINK-29394,13482773,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,22/Sep/22 13:11,13/Oct/22 14:54,04/Jun/24 20:41,13/Oct/22 14:54,,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 13 14:54:47 UTC 2022,,,,,,,,,,"0|z18rq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 14:54;gyfora;merged to main 49f3333796b7c5dae2a441d453b67d7e8bcee3bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Kubernetes operator examples to use the latest Flink base image,FLINK-29393,13482763,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gyfora,gyfora,22/Sep/22 12:34,22/Sep/22 20:40,04/Jun/24 20:41,22/Sep/22 20:40,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,We should update all the examples to refer to the latest Flink base image (1.15.2) before the release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 20:40:17 UTC 2022,,,,,,,,,,"0|z18ro0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 20:40;gyfora;merged to main 66987fb9b4d6f7a315024cff27dac12886b1ee88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionJobs are lost when Session FlinkDeployment is upgraded without HA,FLINK-29392,13482761,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,22/Sep/22 12:32,27/Sep/22 11:57,04/Jun/24 20:41,27/Sep/22 11:57,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Currently SessionJobs are completely lost if the session FlinkDeployment was upgraded and HA wasn't enabled. This is related to FLINK-27979 but its a quite critical manifestation of it.

After that the session job is never restarted and the observer thinks it's ""fine"" and keeps it in the RECONCILING state for some reason.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 11:57:16 UTC 2022,,,,,,,,,,"0|z18rnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:32;gyfora;cc [~aitozi] ;;;","27/Sep/22 11:57;gyfora;merged to main c4445bcc7d00f774489007a3e961dfa0dae3e38d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to set labels and annotations in kubernetes deployment,FLINK-29391,13482749,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,Obradovic,Obradovic,22/Sep/22 11:26,23/Sep/22 14:22,04/Jun/24 20:41,23/Sep/22 14:22,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Using [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/pod-template/,|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/pod-template/] , it is quite easy to to set any configuration, containers, volumes in the pods.

 

However, I have a requirement to be able to set annotations and labels directly on the kubernetes deployments, which manage taskamanger/jobmanager pods.

 

In example
{noformat}
kc describe deployment basic-example
Name:                   basic-example
Namespace:              zonda
CreationTimestamp:      Thu, 22 Sep 2022 10:54:30 +0100
Labels:                 app=basic-example
                        component=jobmanager
                        type=flink-native-kubernetes
Annotations:            deployment.kubernetes.io/revision: 1
                        flinkdeployment.flink.apache.org/generation: 2
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 14:22:17 UTC 2022,,,,,,,,,,"0|z18rkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 14:22;gyfora;you can use the following built in Flink configs:

kubernetes.jobmanager.labels
kubernetes.taskmanager.labels
kubernetes.jobmanager.annotations
kubernetes.taskmanager.annotations;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar SQL Connector: SQLClient E2E testing,FLINK-29390,13482745,13428958,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,tison,affe,affe,22/Sep/22 10:49,26/Feb/24 11:26,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 30 22:35:05 UTC 2023,,,,,,,,,,"0|z18rk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation of JDBC and HBase lookup table for new caching options,FLINK-29389,13482740,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,22/Sep/22 10:19,26/Sep/22 01:25,04/Jun/24 20:41,26/Sep/22 01:25,1.16.0,,,,,,,1.16.0,,,,Connectors / HBase,Connectors / JDBC,Documentation,,0,pull-request-available,,,,Update documentation of JDBC and HBase lookup table for new caching options,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 01:24:48 UTC 2022,,,,,,,,,,"0|z18riw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 01:24;renqs;1.16: 9699ca7ac188d24a3a8d33fc6749b08c10ca85c7

master: 3fa7d03ddad576e99a05ff558e2ee536872a34d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix args in JobSpec not being passed through to Flink in Standalone mode,FLINK-29388,13482733,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,usamj,usamj,22/Sep/22 09:34,13/Apr/23 14:04,04/Jun/24 20:41,23/Sep/22 13:59,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 13:59:35 UTC 2022,,,,,,,,,,"0|z18rhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 13:59;gyfora;merged to main e2b829c7df7501760dec8f9aa47685c680b227cf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalJoinITCase.testIntervalJoinSideOutputRightLateData failed with AssertionError,FLINK-29387,13482731,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,stupid_pig,hxbks2ks,hxbks2ks,22/Sep/22 09:29,15/Nov/22 08:30,04/Jun/24 20:41,15/Nov/22 08:30,1.17.0,,,,,,,1.17.0,,,,API / DataStream,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-09-22T04:40:21.9296331Z Sep 22 04:40:21 [ERROR] org.apache.flink.test.streaming.runtime.IntervalJoinITCase.testIntervalJoinSideOutputRightLateData  Time elapsed: 2.46 s  <<< FAILURE!
2022-09-22T04:40:21.9297487Z Sep 22 04:40:21 java.lang.AssertionError: expected:<[(key,2)]> but was:<[]>
2022-09-22T04:40:21.9298208Z Sep 22 04:40:21 	at org.junit.Assert.fail(Assert.java:89)
2022-09-22T04:40:21.9298927Z Sep 22 04:40:21 	at org.junit.Assert.failNotEquals(Assert.java:835)
2022-09-22T04:40:21.9299655Z Sep 22 04:40:21 	at org.junit.Assert.assertEquals(Assert.java:120)
2022-09-22T04:40:21.9300403Z Sep 22 04:40:21 	at org.junit.Assert.assertEquals(Assert.java:146)
2022-09-22T04:40:21.9301538Z Sep 22 04:40:21 	at org.apache.flink.test.streaming.runtime.IntervalJoinITCase.expectInAnyOrder(IntervalJoinITCase.java:521)
2022-09-22T04:40:21.9302578Z Sep 22 04:40:21 	at org.apache.flink.test.streaming.runtime.IntervalJoinITCase.testIntervalJoinSideOutputRightLateData(IntervalJoinITCase.java:280)
2022-09-22T04:40:21.9303641Z Sep 22 04:40:21 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-22T04:40:21.9304472Z Sep 22 04:40:21 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-22T04:40:21.9305371Z Sep 22 04:40:21 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-22T04:40:21.9306195Z Sep 22 04:40:21 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-22T04:40:21.9307011Z Sep 22 04:40:21 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-22T04:40:21.9308077Z Sep 22 04:40:21 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-22T04:40:21.9308968Z Sep 22 04:40:21 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-22T04:40:21.9309849Z Sep 22 04:40:21 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-22T04:40:21.9310704Z Sep 22 04:40:21 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-22T04:40:21.9311533Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-22T04:40:21.9312386Z Sep 22 04:40:21 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-22T04:40:21.9313231Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-22T04:40:21.9314985Z Sep 22 04:40:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-22T04:40:21.9315857Z Sep 22 04:40:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-22T04:40:21.9316633Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-22T04:40:21.9317450Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-22T04:40:21.9318209Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-22T04:40:21.9318949Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-22T04:40:21.9319680Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-22T04:40:21.9320401Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-22T04:40:21.9321130Z Sep 22 04:40:21 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-22T04:40:21.9321822Z Sep 22 04:40:21 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-22T04:40:21.9322498Z Sep 22 04:40:21 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-22T04:40:21.9323248Z Sep 22 04:40:21 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-22T04:40:21.9324080Z Sep 22 04:40:21 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-22T04:40:21.9324899Z Sep 22 04:40:21 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-22T04:40:21.9325763Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-22T04:40:21.9326690Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-22T04:40:21.9336750Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-22T04:40:21.9337843Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-22T04:40:21.9339018Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-22T04:40:21.9339907Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-22T04:40:21.9340728Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-22T04:40:21.9341624Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-22T04:40:21.9342569Z Sep 22 04:40:21 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-22T04:40:21.9344426Z Sep 22 04:40:21 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-22T04:40:21.9345700Z Sep 22 04:40:21 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-22T04:40:21.9346694Z Sep 22 04:40:21 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-22T04:40:21.9347619Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-22T04:40:21.9348551Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-22T04:40:21.9349389Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-22T04:40:21.9350229Z Sep 22 04:40:21 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-09-22T04:40:21.9351777Z Sep 22 04:40:21 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41236&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24907,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 15 08:30:47 UTC 2022,,,,,,,,,,"0|z18rgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 09:30;hxbks2ks;[~stupid_pig] Could you help take a look? Thx.;;;","22/Sep/22 09:30;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41236&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","23/Sep/22 07:17;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41276&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8100;;;","23/Sep/22 07:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41276&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","26/Sep/22 02:54;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41316&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8267;;;","27/Sep/22 06:03;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41366&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","28/Sep/22 11:20;stupid_pig;I have re-run testIntervalJoinSideOutputLeftLateData() and testIntervalJoinSideOutputRightLateData() localy , and they passed.

I'd like to help fix it  , but I no idea  what wrong with the unit tests.;;;","29/Sep/22 02:00;hxbks2ks;This is an unstable case, not sure if you have run it thousands of times locally to see if you can reproduce it. Another possibility is that the performance of the CI machine is not so good. If this is the case, it will be a little more troublesome to investigate the root cause. 
;;;","30/Sep/22 11:51;chesnay;Running the test multiple times in parallel locally makes this pretty easy to reproduce.;;;","03/Oct/22 13:50;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41535&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","12/Oct/22 08:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41913&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10203;;;","14/Oct/22 10:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42011&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10207;;;","17/Oct/22 03:47;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42059&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","17/Oct/22 03:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42059&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","17/Oct/22 03:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42059&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","17/Oct/22 08:26;stupid_pig;[~chesnay]  Well, how could I  run the unit test in parallel locally .;;;","17/Oct/22 11:26;mapohl;[~stupid_pig] you should create a Run configuration for each of the tests in Intellij. Edit these Run configurations to enable repeated execution (e.g. ""Until failure"") and just start both tests.

Initially, I thought that the issue is due to the test sharing the static member {{IntervalJoinITCase#testResult}} if running the tests in the same JVM. The static member gets reset in {{IntervalJoinITCase#setup}}. That would explain the assert. But that would mean that any other concurrently executed test method of this class would have the same issue which is not the case.;;;","18/Oct/22 08:21;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42115&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","24/Oct/22 06:08;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42328&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8069;;;","01/Nov/22 08:27;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8208;;;","01/Nov/22 08:28;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42680&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10198;;;","03/Nov/22 14:12;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42784&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","07/Nov/22 07:57;mapohl;2 times in the same build:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42856&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7330]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42856&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=9486;;;","08/Nov/22 07:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42908&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10229;;;","12/Nov/22 08:21;stupid_pig;[~mapohl]

[~hxbks2ks] 

I think the problem may be the watermark in the stream instead of parallel execution. For example , let's see the two sources in method _testIntervalJoinSideOutputRightLateData_ : 
{code:java}
DataStream<Tuple2<String, Integer>> streamOne =
        buildSourceStream(
                env,
                (ctx) -> {
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 2), 2L);
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 3), 3L);
                    ctx.emitWatermark(new Watermark(3));
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 4), 4L);
                });

DataStream<Tuple2<String, Integer>> streamTwo =
        buildSourceStream(
                env,
                (ctx) -> {
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 1), 1L);
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 3), 3L);
                    ctx.emitWatermark(new Watermark(3));
                    ctx.collectWithTimestamp(Tuple2.of(""key"", 2), 2L); // late data
                }); {code}
 

If _streamTwo_ emit late data with timestamp=2L *before*  _streamOne emit_ _Watermark(3),  the_ _Watemark_ in IntervalJoin Operator is still the Long.MIN_VALUE. Thus when IntervalJoin Operator handle the late data, it won't sideout. 

 

I try to fix it, but I found it diffcult to control the data order between two streams.  Could you do me a favor?

Finally, I'd like to take this ticket .;;;","14/Nov/22 09:00;pnowojski;I've assigned the ticket to you [~stupid_pig]. 

It sounds to me like this test should not be implemented as an ITCase, but rather a unit test using [the test harnesses|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/testing/#unit-testing-stateful-or-timely-udfs--custom-operators]. This way we would have full control over order of processed elements/watermarks.;;;","15/Nov/22 08:30;pnowojski;merged commit 7e51db9 into apache:master;;;","15/Nov/22 08:30;pnowojski;Thanks for fixing the issue [~stupid_pig];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to compile flink-connector-hive when profile is hive3,FLINK-29386,13482712,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,luoyuxia,luoyuxia,luoyuxia,22/Sep/22 07:31,26/Sep/22 11:10,04/Jun/24 20:41,26/Sep/22 11:10,1.16.0,1.17.0,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,"The compile will fail in hive3. [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41238&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691]

Introduced by FLINK-29152 which introduces org.apache.hadoop.hive.metastore.MetaStoreUtils.

DEFAULT_SERIALIZATION_FORMAT,  TableType.INDEX_TABLE, ErrorMsg.SHOW_CREATETABLE_INDEX.    But they don't exist in Hive3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 07:09:16 UTC 2022,,,,,,,,,,"0|z18rco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 07:09;fsk119;Merged into master: b7b366c0fda1ae8335a458d449b72fec31c409dd
Merged into release-1.16: 21444e5eec7ccf25a54ba0675105ab47a4096ac7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AddColumn in flink table store should check the duplicate field names,FLINK-29385,13482680,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,22/Sep/22 03:03,22/Sep/22 06:52,04/Jun/24 20:41,22/Sep/22 06:52,table-store-0.3.0,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"AddColumn in table store should check the duplicate field names, otherwise the ddl will be successful and create flink store table failed for flink job",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 06:52:10 UTC 2022,,,,,,,,,,"0|z18r5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 06:52;lzljs3620320;master: 98774161860055aef2113a5442ad63dcfe3ea9eb
release-0.2: 9f0acad62c7a21a547cab1312b775baa0b6ab4e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
snakeyaml version 1.30 in flink-kubernetes-operator-1.2-SNAPSHOT-shaded.jar has vulnerabilities,FLINK-29384,13482634,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,jbusche,jbusche,21/Sep/22 17:57,27/Sep/22 17:21,04/Jun/24 20:41,27/Sep/22 07:04,kubernetes-operator-1.2.0,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"I did a twistlock scan of the current operator image from main, and it looks good except for in the flink-kubernetes-operator-1.2-SNAPSHOT-shaded.jar, I'm seeing 5 CVEs on snakeyaml.  Looks like updating from 1.30 to 1.32 should fix it, but I'm not sure how to bump that up, other than the [NOTICES|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/resources/META-INF/NOTICE#L65] entry.

The 5 CVEs are:
[https://nvd.nist.gov/vuln/detail/CVE-2022-25857]

[https://nvd.nist.gov/vuln/detail/CVE-2022-25857]

[https://nvd.nist.gov/vuln/detail/CVE-2022-38751]

[https://nvd.nist.gov/vuln/detail/CVE-2022-38750]

[https://nvd.nist.gov/vuln/detail/CVE-2022-38752]

Resulting in 1 High (CVSS 7.5) and 4 Mediums (CVSS 6.5, 6.5, 5.5, 4)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 17:21:31 UTC 2022,,,,,,,,,,"0|z18qvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 03:10;wangyang0918;The {{org.yaml:snakeyaml:jar:1.30}} is introduces by {{{}io.fabric8:kubernetes-client:jar:5.12.3{}}}. I think we could use the dependencyManagement to pin the version to 1.32, just like what we have done for {{{}com.fasterxml.jackson{}}}.;;;","27/Sep/22 07:04;gyfora;Merged to main 0952b0dd05f04390f549e3ca20f4ba345b067a18;;;","27/Sep/22 16:17;jbusche;Looks clean - thank you [~gyfora]  [~mbalassi] and [~wangyang0918]!;;;","27/Sep/22 17:21;mbalassi;Thanks for confirming [~jbusche] .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additionalPrinterColumns definition (PrinterColumn annotation) for some status fields,FLINK-29383,13482623,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,haoxin,haoxin,21/Sep/22 15:55,23/Sep/22 06:36,04/Jun/24 20:41,23/Sep/22 06:36,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"We should add additionalPrinterColumns definitions in the CRD so that we can use
{code:java}
k get flinksessionjob -o wide
{code}
to see the session jobs statuses.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/378,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 06:36:07 UTC 2022,,,,,,,,,,"0|z18qt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:35;jeesmon;Related to FLINK-27893;;;","23/Sep/22 06:36;gyfora;merged to main fe1356edc29318dbb6c96309a775749ac5a64b09;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink fails to start when created using quick guide for flink operator,FLINK-29382,13482591,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,Obradovic,Obradovic,21/Sep/22 13:35,23/Sep/22 07:07,04/Jun/24 20:41,23/Sep/22 07:07,kubernetes-operator-1.1.0,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"I followed [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/try-flink-kubernetes-operator/quick-start/] to deploy flink operator and then the flink job.

 

 

When following step 
 {{kubectl create -f https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-1.1/examples/basic.yaml}}
the pod starts, but then it keeps crashing with following exception.

 

{noformat}
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: pods is forbidden: User ""system:anonymous"" cannot watch resource ""pods"" in API group """" in the namespace ""zonda""
	at io.fabric8.kubernetes.client.dsl.internal.WatcherWebSocketListener.onFailure(WatcherWebSocketListener.java:74) ~[flink-dist-1.15.2.jar:1.15.2]
	at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:570) ~[flink-dist-1.15.2.jar:1.15.2]
	at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$1.onResponse(RealWebSocket.java:199) ~[flink-dist-1.15.2.jar:1.15.2]
	at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:174) ~[flink-dist-1.15.2.jar:1.15.2]
	at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32) ~[flink-dist-1.15.2.jar:1.15.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
{noformat}

I also noticed following log lines
{noformat}
2022-09-21 13:32:05,715 WARN  io.fabric8.kubernetes.client.Config                          [] - Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.
2022-09-21 13:32:05,719 WARN  io.fabric8.kubernetes.client.Config                          [] - Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.
{noformat}

I think the problem is that container runs as user root, which later uses gosu to became flink user. However, service account is only accessible to the main user in the container, which is root

{noformat}
root@basic-example-658578895d-qwlb2:/opt/flink# ls -hltr /var/run/secrets/kubernetes.io/serviceaccount/token
lrwxrwxrwx. 1 root 1337 12 Sep 21 08:57 /var/run/secrets/kubernetes.io/serviceaccount/token -> ..data/token
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 14:03:45 UTC 2022,,,,,,,,,,"0|z18qm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 13:37;gyfora;What environment are you running in? the quickstart is for minikube;;;","21/Sep/22 14:03;Obradovic;You are right, I'm not running on the minikube, I'm using Kubernetes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Key_Shared subscription isn't works in the latest Pulsar connector,FLINK-29381,13482583,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,21/Sep/22 12:42,30/Sep/22 04:31,04/Jun/24 20:41,30/Sep/22 04:31,1.14.6,1.15.2,1.16.0,,,,,1.15.3,1.16.0,,,Connectors / Pulsar,,,,0,,,,,"Pulsar add [message retry policy|https://github.com/apache/pulsar/pull/14014] for Key_Shared subscription which makes the flink messages consuming not works now in Key_Shared subscription. We can't consume messages by using a subset of sticky key hash range https://github.com/apache/pulsar/issues/17679. So we have to change the Key_Shared subscription to Exclusive subscription which supports consuming messages with partial key hash ranges.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-21 12:42:59.0,,,,,,,,,,"0|z18qk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Two streams union, watermark error, not the minimum value",FLINK-29380,13482554,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Bug,,xiechenling,xiechenling,21/Sep/22 10:06,26/Oct/22 06:02,04/Jun/24 20:41,26/Oct/22 06:02,1.15.2,1.16.0,,,,,,,,,,,,,,0,,,,,"Two streams union, watermark error, not the minimum value, connect operator  watermark is true.
!image-2022-09-21-17-59-01-846.png!

This phenomenon feels related to watermark idle. In flink 1.13.1, watermark is normal whether idle watermark is set or not. In flink 1.15.2, watermark is normal when not set idle or idle set 1000ms, but idle set 1ms watermark wrong.

 !screenshot-1.png! 

 !screenshot-2.png! 

 !screenshot-3.png! 

What I don't understand is, if the maximum watermark is issued by idle, the watermark of the union operator is incorrect, why the watermark of the operator before the union operator is normal?



{code:scala}
import org.apache.flink.api.common.eventtime.{SerializableTimestampAssigner, WatermarkStrategy}
import org.apache.flink.api.connector.source.Source
import org.apache.flink.api.connector.source.lib.NumberSequenceSource
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment
import org.apache.flink.streaming.api.functions.ProcessFunction
import org.apache.flink.util.Collector

import java.time.format.DateTimeFormatter
import java.time.{Duration, Instant, ZoneId}
import java.util

object UnionWaterMarkTest {
  def main(args: Array[String]): Unit = {

    val env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration)
    env.setParallelism(2)

    val numberSequenceSource: Source[Long,
      NumberSequenceSource.NumberSequenceSplit,
      util.Collection[NumberSequenceSource.NumberSequenceSplit]] = new NumberSequenceSource(0L, 100000000L)
      .asInstanceOf[Source[Long,
      NumberSequenceSource.NumberSequenceSplit,
      util.Collection[NumberSequenceSource.NumberSequenceSplit]]]

    val stream1 = env.fromSource(numberSequenceSource,
      WatermarkStrategy
        .forMonotonousTimestamps[Long]()
        .withTimestampAssigner(new SerializableTimestampAssigner[Long] {
          override def extractTimestamp(element: Long, recordTimestamp: Long): Long = {
            Instant.now().toEpochMilli
          }
        }),
      ""source""
    )

    val idleMillis = 1L
    val stream2 = env.fromSource(numberSequenceSource,
      WatermarkStrategy
        .forMonotonousTimestamps[Long]()
        .withTimestampAssigner(new SerializableTimestampAssigner[Long] {
          override def extractTimestamp(element: Long, recordTimestamp: Long): Long = {
            Instant.now().toEpochMilli - (1000L * 60L * 60L)
          }
        })
        .withIdleness(Duration.ofMillis(idleMillis))
      ,
      ""source""
    )

    stream1
      .process(new PrintWatermarkProcess(""stream1""))
      .returns(classOf[Long])
      .startNewChain()
      .union(
        stream2
          .process(new PrintWatermarkProcess(""stream2""))
          .returns(classOf[Long])
          .startNewChain()
          .process(new PrintWatermarkProcess(""stream3""))
          .returns(classOf[Long])
          .startNewChain()
      )
      .process(new PrintWatermarkProcess(""union""))
      .returns(classOf[Long])
      .filter(value => false)
      .print()

    env.execute()

  }

}

class PrintWatermarkProcess(operatorName: String) extends ProcessFunction[Long, Long] {
  override def processElement(value: Long, ctx: ProcessFunction[Long, Long]#Context, out: Collector[Long]): Unit = {
    out.collect(value)
    val watermark = ctx.timerService().currentWatermark()
    if (watermark > 0 && watermark < 2222222222222L) {
      Instant.ofEpochMilli(watermark)
      val datetimeStr = DateTimeFormatter.ISO_LOCAL_DATE_TIME.withZone(ZoneId.systemDefault()).format(Instant.ofEpochMilli(watermark))
//      println(operatorName + ""  "" + datetimeStr)
    }
  }
}

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/22 09:59;xiechenling;image-2022-09-21-17-59-01-846.png;https://issues.apache.org/jira/secure/attachment/13049567/image-2022-09-21-17-59-01-846.png","22/Sep/22 06:44;xiechenling;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13049607/screenshot-1.png","22/Sep/22 06:49;xiechenling;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13049608/screenshot-2.png","22/Sep/22 07:00;xiechenling;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13049610/screenshot-3.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 06:02:04 UTC 2022,,,,,,,,,,"0|z18qds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 10:17;chesnay;I'm gonna assume this also applies to the upcoming 1.16.0.

[~xiechenling] Can you provide us a minimal example that runs into this issue? How reproducible is it? Is the job out still correct?

This could be just be a display error because the watermarks are fetched separately for each vertex, so they aren't necessarily synced w.r.t. time.;;;","22/Sep/22 07:07;xiechenling;[~chesnay] I have added description and code example.;;;","26/Oct/22 06:02;xiechenling;It's not a bug, it's watermark idle and back pressure causing the watermark to not be the upstream minimum.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Back (most of the) ExecutionConfig and CheckpointConfig by Configuration,FLINK-29379,13482538,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,twalthr,twalthr,21/Sep/22 09:10,08/Nov/22 13:16,04/Jun/24 20:41,08/Nov/22 13:15,,,,,,,,1.17.0,,,,API / DataStream,,,,0,pull-request-available,,,,"Not sure if this is a duplicate, but as this issue pops up over and over again, it might be time to discuss it here and fix it.

Currently, configuration is spread across instances of {{Configuration}} and POJOs (e.g. {{ExecutionConfig}} or {{CheckpointConfig}}). This makes it very tricky to handle configuration throughout the stack. The practice has shown that configuration might be passed, layered, merged, restricted, copied, filtered, etc. This is easy with the config option stack but very tricky with the existing POJOs. Esp. it is difficult to keep the two in sync or compare them.

Many locations reveal the current shortcoming. For example, {{org.apache.flink.table.planner.delegation.DefaultExecutor}} has a {{isCheckpointingEnabled()}} method simply because we cannot trust the {{Configuration}} object that is passed around. Same for checking if object reuse is enabled.

A solution is still up for discussion. Ideally, we deprecate {{ExecutionConfig}} and {{CheckpointConfig}} and advocate a pure config option based approach. Alternatively, we could do a hybrid approach similar to `TableConfig` (that is backed by config options but has setters for convenience). The latter approach would cause less deprecations in the API.",,,,,,,,,,,FLINK-29807,,,,,,,,,,,,,,,,,,FLINK-3642,FLINK-29309,,,FLINK-13876,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 13:15:41 UTC 2022,,,,,,,,,,"0|z18qa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 09:25;chesnay;Imagine that the ExecutionConfig did not contain all these LinkedHashMaps or the GlobalJobParameters. Could we keep these classes but merge them internally into a single Configuration in the StreamExEnv that is passed to internal components?

We could probably migrate the CheckpointConfig right away; would ""just"" need a backwards-compatible deserialization path.;;;","21/Sep/22 09:39;twalthr;The question is when does the merging happen?

Option A: ExecutionConfig holds its own Configuration instance and we apply it to StreamExecutionEnvironment#configuration when calling StreamExecutionEnvironment.execute().

Option B: StreamExecutionEnvironment.getConfig() returns an instance of ExecutionConfig that references StreamExecutionEnvironment#configuration.

Only Option B enables that StreamExecutionEnvironment#configuration is always up to date when, for example, querying it in SQL planner.;;;","21/Sep/22 09:45;chesnay;At first glance B) sounds fine to me but we'd need to look through all instantiations.;;;","13/Oct/22 14:29;pnowojski;I would propose to start with an intermediate step. Keep the {{ExecutionConfig}} public methods as they are, but just back the setters and getters via a {{Configuration ExecutionConfig#configuration}} field. All the setters/getters could go through that {{configuration}} field. Thanks to that we don't need to touch how users and Flink itself is interacting with the {{ExecutionConfig}} for the time being. And this would allow us to produce nice error message for {{org.apache.flink.configuration.DeploymentOptions#PROGRAM_CONFIG_ENABLED}} violations.

Later this can easily lead to the Option B that [~twalthr] mentioned. WDYT?;;;","03/Nov/22 10:18;pnowojski;We can not refactor ExecutionConfig as long as we support old style serializers that could have been serialising ExecutionConfig internally. So this depends on FLINK-29807;;;","06/Nov/22 20:21;pnowojski;First part ({{ExecutionConfig}}) merged as 48ae78b1805..12efbb9c85c;;;","08/Nov/22 13:15;pnowojski;Most of the {{CheckpointConfig}} migrated to {{Configuration}} in e98e289991b..bb124f4ada4

Some fields in both {{ExecutionConfig}} or {{CheckpointConfig}} haven't been migrated to {{Configuration}}, because they for example can contain user code or there can be other serialisation issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading logging in Execution for failed state trannsitions,FLINK-29378,13482532,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,21/Sep/22 08:49,22/Sep/22 23:14,04/Jun/24 20:41,22/Sep/22 23:14,,,,,,,,1.16.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"{code}
String.format(
    ""Concurrent unexpected state transition of task %s to %s while deployment was in progress."",
     getVertexWithAttempt(), currentState);
{code}

{{to}} is not the target state.

This whole line needs improvements; log the current, expected and target state. Additionally I'd suggest to log the attempt ID which is much easier to correlate with other messages (like what the TM actually submits).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 12:44:36 UTC 2022,,,,,,,,,,"0|z18q8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:44;chesnay;master: 5766d50dc1401b1269ec83e670c2d21257e20fc5
1.16: 5ba6525db731aa62a770a9a7971b8a1cbf12d9fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make RPC timeout extraction reusable,FLINK-29377,13482531,13481290,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Sep/22 08:41,22/Sep/22 08:34,04/Jun/24 20:41,22/Sep/22 08:34,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 08:34:15 UTC 2022,,,,,,,,,,"0|z18q8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 08:34;chesnay;master: b9e3dfe0ae78aaedf5cfa645c307cc3e52168db5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deployment already exists error if Flink version is not set correctly,FLINK-29376,13482524,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gyfora,gyfora,21/Sep/22 08:07,27/Sep/22 08:35,04/Jun/24 20:41,27/Sep/22 08:35,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"If the user incorrectly sets a Flink version lower than 1.15 when using 1.15 or above there are some strange behaviour currently around cluster shutdown.

Since we always set SHUTDOWN_ON_APPLICATION_FINISH (in the FlinkConfigBuilder) regardless of version but set the JmDeployStatus to MISSING/READY based on version. It can happen that we set the JmStatus to MISSING when the jm deployment is still running.

As the observer skips observing UPGRADING CRs it never updates the MISSING status and the deployment logic fails on duplicate deployment.

An easy fix could be to only set SHUTDOWN_ON_APPLICATION_FINISH for version >= 1.15",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 08:35:34 UTC 2022,,,,,,,,,,"0|z18q74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 08:35;gyfora;merged to main 395956910af19f0ba6f1de1ae4d730aee4b6504f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move getSelfGateway into RpcService,FLINK-29375,13482523,13481290,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Sep/22 07:57,23/Sep/22 09:45,04/Jun/24 20:41,23/Sep/22 09:45,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,,Self gateways are a tricky thing and we should give the RPC implementation control over how they are achieved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 09:45:21 UTC 2022,,,,,,,,,,"0|z18q6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 09:45;chesnay;master: 0154de9edb44b565e9b9cba614f42e7adae3a953;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RpcConnectionTest may pass invalid rpc URL,FLINK-29374,13482519,13481290,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Sep/22 07:42,22/Sep/22 12:41,04/Jun/24 20:41,22/Sep/22 12:41,,,,,,,,1.17.0,,,,Runtime / RPC,Tests,,,0,pull-request-available,,,,"{{testConnectFailure()}} passed a URL under which no endpoint is running to check that this is handle correctly.
However the given URL may be completely invalid; the format of an RPC URL is an implementation detail of the RPC implementation, with the general contract being that you may only pass the return value of RpcSystem#getRpcUrl to connect() and not arbitrary values (and expect these to actually result in an actual connection attempt).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 12:41:31 UTC 2022,,,,,,,,,,"0|z18q60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:41;chesnay;master: c4654f2cbe6b76a6af66d33deb8320e697e51020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataStream to table not support BigDecimalTypeInfo,FLINK-29373,13482515,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hk__lrzy,hk__lrzy,hk__lrzy,21/Sep/22 07:12,11/Nov/22 08:44,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"When we try to transfrom datastream to table, *TypeInfoDataTypeConverter* will try to convert *TypeInformation* to {*}DataType{*}, but if datastream's produce types contains {*}BigDecimalTypeInfo{*}, *TypeInfoDataTypeConverter* will final convert it to {*}RawDataType{*}，then when we want tranform table to datastream again, exception will hapend, and show the data type not match.

Blink planner also will has this exception.

!image-2022-09-22-18-08-44-385.png!

 
{code:java}
Query schema: [f0: RAW('java.math.BigDecimal', '...')]
Sink schema:  [f0: RAW('java.math.BigDecimal', ?)]{code}
how to recurrent
{code:java}
// code placeholder
StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();

EnvironmentSettings.Builder envBuilder = EnvironmentSettings.newInstance()
    .inStreamingMode();
StreamTableEnvironment streamTableEnvironment = StreamTableEnvironment.create(executionEnvironment, envBuilder.build());

FromElementsFunction fromElementsFunction = new FromElementsFunction(new BigDecimal(1.11D));
DataStreamSource dataStreamSource = executionEnvironment.addSource(fromElementsFunction, new BigDecimalTypeInfo(10, 8));
streamTableEnvironment.createTemporaryView(""tmp"", dataStreamSource);
Table table = streamTableEnvironment.sqlQuery(""select * from tmp"");
streamTableEnvironment.toRetractStream(table, table.getSchema().toRowType());{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/22 07:12;hk__lrzy;image-2022-09-21-15-12-11-082.png;https://issues.apache.org/jira/secure/attachment/13049550/image-2022-09-21-15-12-11-082.png","22/Sep/22 10:08;hk__lrzy;image-2022-09-22-18-08-44-385.png;https://issues.apache.org/jira/secure/attachment/13049616/image-2022-09-22-18-08-44-385.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 08:44:47 UTC 2022,,,,,,,,,,"0|z18q54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 08:06;martijnvisser;Can't that be resolved with the annotation as documented at https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/types/#unregistered-structured-types ? ;;;","21/Sep/22 08:32;hk__lrzy;[~martijnvisser] 
yes, we didn't have pojo class to use annotation.;;;","21/Sep/22 14:45;martijnvisser;[~hk__lrzy] Which version of Flink are you using? There's no Blink planner in recent versions. Please make sure that your affected version is correct. ;;;","22/Sep/22 10:11;hk__lrzy;[~martijnvisser] I test in master branch, still have this issue, and i update the description;;;","22/Sep/22 14:51;martijnvisser;[~lzljs3620320] [~jark] Any thoughts?;;;","23/Sep/22 02:55;lzljs3620320;{code:java}
/**
 * Converter from {@link TypeInformation} to {@link DataType}.
 *
 * <p>{@link DataType} is richer than {@link TypeInformation} as it also includes details about the
 * {@link LogicalType}. Therefore, some details will be added implicitly during the conversion. The
 * conversion from {@link DataType} to {@link TypeInformation} is provided by the planner.
 *
 * <p>The behavior of this converter can be summarized as follows:
 *
 * <ul>
 *   <li>All subclasses of {@link TypeInformation} are mapped to {@link LogicalType} including
 *       nullability that is aligned with serializers.
 *   <li>{@link TupleTypeInfoBase} is translated into {@link RowType} or {@link StructuredType}.
 *   <li>{@link BigDecimal} is converted to {@code DECIMAL(38, 18)} by default.
 *   <li>The order of {@link PojoTypeInfo} fields is determined by the converter.
 *   <li>{@link GenericTypeInfo} and other type information that cannot be mapped to a logical type
 *       is converted to {@link RawType} by considering the current configuration.
 *   <li>{@link TypeInformation} that originated from Table API will keep its {@link DataType}
 *       information when implementing {@link DataTypeQueryable}.
 * </ul>
 */
@Internal
public final class TypeInfoDataTypeConverter
{code}

The validation in DynamicSinkUtils should be adjusted. The alignment of these different information in DataType and TypeInformation should be ensured.;;;","23/Sep/22 06:34;hk__lrzy;[~lzljs3620320] can you show more detail about this issue, maybe i can work on this issue can try to fix it.;;;","26/Sep/22 02:17;lzljs3620320;[~hk__lrzy] Thanks, assigned to u. CC [~jark] to review~;;;","11/Nov/22 08:41;hk__lrzy;[~jark]  i fixed it and test it in my local envinonment, plz take a look thanks.;;;","11/Nov/22 08:44;hk__lrzy;Another question, should we move all type info which definetion in `table-runtime` to `table-common`;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a suffix to keys that violate YAML spec,FLINK-29372,13482514,13393472,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Sep/22 06:58,30/Sep/22 09:36,04/Jun/24 20:41,30/Sep/22 09:36,,,,,,,,1.17.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,,"We have a few options where the key is a prefix of other options (e.g., {{high-availability}} and {{high-availability.cluster-id}}.

Add a suffix to these options and keep the old key as deprecated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 09:10:36 UTC 2022,,,,,,,,,,"0|z18q4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 09:10;chesnay;master: 6cce68dcdc1baf4be2a9e1549983d010644b5ee3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature to_timestamp(<CHARACTER>, <CHARACTER>)",FLINK-29371,13482509,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,YMBSKLK,YMBSKLK,21/Sep/22 06:39,21/Sep/22 07:37,04/Jun/24 20:41,,1.13.6,,,,,,,,,,,Table SQL / Legacy Planner,,,,0,,,,,"------------------------------------------------------------
 The program finished with the following exception:

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Execute Flink task error
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
    at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
    at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
    at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
    at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: org.apache.seatunnel.core.base.exception.CommandExecuteException: Execute Flink task error
    at org.apache.seatunnel.core.flink.command.FlinkApiTaskExecuteCommand.execute(FlinkApiTaskExecuteCommand.java:85)
    at org.apache.seatunnel.core.base.Seatunnel.run(Seatunnel.java:40)
    at org.apache.seatunnel.core.flink.SeatunnelFlink.main(SeatunnelFlink.java:34)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
    ... 8 more
Caused by: java.lang.Exception: Flink batch transform sql execute failed, SQL: select to_timestamp('2022-02-02', 'yyyy-MM-dd')
    at org.apache.seatunnel.flink.transform.Sql.processBatch(Sql.java:63)
    at org.apache.seatunnel.flink.batch.FlinkBatchExecution.start(FlinkBatchExecution.java:64)
    at org.apache.seatunnel.core.flink.command.FlinkApiTaskExecuteCommand.execute(FlinkApiTaskExecuteCommand.java:82)
    ... 15 more
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 8 to line 1, column 47: No match found for function signature to_timestamp(<CHARACTER>, <CHARACTER>)
    at org.apache.flink.table.calcite.FlinkPlannerImpl.validateInternal(FlinkPlannerImpl.scala:147)
    at org.apache.flink.table.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:107)
    at org.apache.flink.table.sqlexec.SqlToOperationConverter.convert(SqlToOperationConverter.java:151)
    at org.apache.flink.table.planner.ParserImpl.parse(ParserImpl.java:92)
    at org.apache.flink.table.api.internal.TableEnvImpl.sqlQuery(TableEnvImpl.scala:559)
    at org.apache.seatunnel.flink.transform.Sql.processBatch(Sql.java:61)
    ... 17 more
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 8 to line 1, column 47: No match found for function signature to_timestamp(<CHARACTER>, <CHARACTER>)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883)
    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4860)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1813)
    at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:321)
    at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:226)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5709)
    at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5696)
    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1735)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1726)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:420)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4060)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3346)
    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)
    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:996)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:974)
    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:951)
    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:703)
    at org.apache.flink.table.calcite.FlinkPlannerImpl.validateInternal(FlinkPlannerImpl.scala:142)
    ... 22 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature to_timestamp(<CHARACTER>, <CHARACTER>)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467)
    at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560)
    ... 44 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 07:37:15 UTC 2022,,,,,,,,,,"0|z18q3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 06:55;martijnvisser;[~YMBSKLK] Please verify this with a supported Flink version, preferably Flink 1.15. ;;;","21/Sep/22 07:27;YMBSKLK;TO_TIMESTAMP(string1[, string2]) supported Flink 1.13.6 and it's correct to execute in the sql client;;;","21/Sep/22 07:37;martijnvisser;Yes, but Flink 1.13 is no longer supported by the Flink community. Only the last two versions (currently 1.14 and 1.15) are. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protobuf in flink-sql-protobuf is not shaded,FLINK-29370,13482505,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jark,jark,21/Sep/22 06:14,29/Feb/24 21:52,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,,"The protobuf classes in flink-sql-protobuf is not shaded which may lead to class conflicts. Usually, sql jars should shade common used dependencies, e.g. flink-sql-avro: https://github.com/apache/flink/blob/master/flink-formats/flink-sql-avro/pom.xml#L88 

{code}
➜  Downloads jar tvf flink-sql-protobuf-1.16.0.jar | grep com.google
     0 Tue Sep 13 20:23:44 CST 2022 com/google/
     0 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/
   568 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/ProtobufInternalUtils.class
 19218 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/AbstractMessage$Builder.class
   259 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/AbstractMessage$BuilderParent.class
 10167 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/AbstractMessage.class
  1486 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/AbstractMessageLite$Builder$LimitedInputStream.class
 12399 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/AbstractMessageLite$Builder.class
   279 Tue Sep 13 20:23:44 CST 2022 com/google/protobuf/AbstractMessageLite$InternalOneOfEnu
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 29 21:52:18 UTC 2024,,,,,,,,,,"0|z18q2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 06:17;jark;[~maosuhan] [~libenchao], do you want to take this issue?;;;","21/Sep/22 06:36;chesnay;This was discussed in the original PR and is intentional. Generated code has references to protobuf, hence we can't relocate it. ;;;","21/Sep/22 06:47;libenchao;[~jark] This has been discussed in the [code review|https://github.com/apache/flink/pull/14376#issuecomment-1164395312], what stops us from doing that is we need users to provide their compiled protobuf classes, if we relocate it, that means we requires users to relocate their compiled classes in the same way as well.


CC [~chesnay] [~martijnvisser] since you were also involved in the review.;;;","21/Sep/22 08:53;twalthr;In Avro this is the difference between generic and specific records. Logic for handling generic records can be shaded. Are there plans to support the same for protobuf?;;;","21/Sep/22 11:17;libenchao;We indeed have one version that do not need users provide their compiled classes internally. It's using {{DynamicMessage}} which is mentioned as the first way to implement ProtoBuf Format in FLINK-18202. We used 'wire-schema' to do the schema parsing, and translate it to {{DynamicMessage}}, which is similar to [confluentinc's schema-registry|https://github.com/confluentinc/schema-registry/blob/master/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java]

However, we chose 2nd way (codegen to use classes and builders compiled by protoc) to implement ProtoBuf Format now, and this is hard to avoid user providing compiled classes AFAIK.

If anyone knows there is some way which we can achieve that, I think it will be worth to have a try.;;;","21/Sep/22 16:05;maosuhan;[~libenchao]  What about removing all com.google.protobuf files when packaging flink-sql-protobuf.jar and rely on user to put protobuf classes in the classpath?

If user must provide compiled proto class, a jar with both compile proto class and google protobuf classes should also easy to provide.

The java API of protobuf is relatively stable, so there should be little conflict with the current implementation.;;;","22/Sep/22 07:39;jark;As this is intentional, I downgrade the priority to major. We can discuss in the JIRA for a long-term solution.  ;;;","29/Feb/24 14:57;tanjialiang;[~jark] [~libenchao] [~maosuhan] I found that flink-sql-orc has the protobuf dependency without shading, and it conflicts with the flink-sql-protobuf, my flink version is 1.16.1. For now, the temporary solution is to shade the protobuf dependency in both the flink-sql-protobuf and user-proto classes by myself.;;;","29/Feb/24 21:52;jeyhunkarimov;Hi [~tanjialiang] you might need to consider [this comment|https://github.com/apache/flink/pull/14376#issuecomment-1164395312] before relocating protobuf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit delete file failure due to Checkpoint aborted,FLINK-29369,13482494,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,21/Sep/22 04:28,26/Sep/22 07:39,04/Jun/24 20:41,26/Sep/22 07:39,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"After checkpoint abort, the files in cp5 may fall into cp6, because the compaction commit is deleted first and then added, which may lead to:
-Delete a file
-Add the same file again

This causes the deleted file not to be found.

We need to properly process the merge of the compression files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 07:39:04 UTC 2022,,,,,,,,,,"0|z18q0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 07:39;lzljs3620320;master: 2219fbad07e413a2961a1c806b0f9647ccf84bc8
release-0.2: 6066b1fd7ab06c853ae87d57804ef10683a976dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify DESCRIBE statement docs for new syntax,FLINK-29368,13482492,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,21/Sep/22 04:18,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,"In Flink 1.17.0, DESCRIBE statement syntax will be changed to DESCRIBE/DESC [EXTENDED] [catalog_name.][database_name.]table_name [PARTITION(partition_spec)] [col_name]. So, it need to modify the docs for this statement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-21 04:18:08.0,,,,,,,,,,"0|z18q00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid manifest corruption for incorrect checkpoint recovery,FLINK-29367,13482491,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,21/Sep/22 04:10,22/Sep/22 10:15,04/Jun/24 20:41,22/Sep/22 10:15,table-store-0.2.0,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"When the job runs to checkpoint N, if the user recovers from an old checkpoint (such as checkpoint N-5), the sink of the current FTS will cause a manifest corruption because duplicate files may be committed.

We should avoid such corruption, and the storage should be robust enough.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 10:15:59 UTC 2022,,,,,,,,,,"0|z18pzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 10:15;lzljs3620320;master: 33896da3aeeef1ad3aa523a3cc8e78c7e5347dbe
release-0.2: 9c16283e04cffccfaa42ed854381990448c787d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use flink-shaded-jacson library to parse flink-conf.yaml,FLINK-29366,13482490,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,catyee,catyee,21/Sep/22 04:00,21/Sep/22 06:33,04/Jun/24 20:41,21/Sep/22 06:33,1.13.3,,,,,,,,,,,API / Core,,,,0,,,,,"Now we use a simple implementation(org.apache.flink.configuration.GlobalConfiguration#loadYAMLResource) to parse flink-conf.yaml, which can only parse simple key-value pairs.

Although there have been discussions on this issue historically(see:[https://github.com/stratosphere/stratosphere/issues/113])
but I think that in the actual production environment, we often need to config complex structure into flink-conf.yaml. At this time, the yaml libary is required for parsing, so I suggest to use the yaml library to parse flink-conf.yaml  instead of our own implementation.

In fact, the flink-core module already has a dependency on flink-shaded-jackson which could parse yaml format,  we can use this jar without more dependencies.",,,,,,,,,,,,,,,,,,,,,FLINK-23620,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 04:01:24 UTC 2022,,,,,,,,,,"0|z18pzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 04:01;catyee;[~chesnay] What do you think about this idea?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Millisecond behind latest jumps after Flink 1.15.2 upgrade,FLINK-29365,13482454,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,wilsonwu,wilsonwu,20/Sep/22 19:53,12/Oct/23 18:09,04/Jun/24 20:41,12/Oct/23 18:09,1.15.3,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,,"(First time filling a ticket in Flink community, please let me know if there are any guidelines I need to follow)

I noticed a very strange behavior with a recent version bump from Flink 1.14.4 to 1.15.2. My project consumes around 30K records per second from a sharded kinesis stream, and during the version upgrade, it will follow the best practice to first trigger a savepoint from the running job, start the new job from the savepoint and then remove the old job. So far so good, and the above logic has been tested multiple times without any issue for 1.14.4. Usually, after the version upgrade, our job will have a few minutes delay for millisecond behind latest, but it will catch up with the speed quickly(within 30mins). Our savepoint is around one hundred MBs big, and our job DAG will become 90 - 100% busy with some backpressure when we redeploy but after 10-20 minutes it goes back to normal.

Then the strange thing happened, when I tried to redeploy with 1.15.2 upgrade from a running 1.14.4 job, I can see a savepoint has been created and the new job is running, all the metrics look fine, except suddenly [millisecond behind the latest|https://flink.apache.org/news/2019/02/25/monitoring-best-practices.html] jumps to 10 hours!! and it takes days for my application to catch up with the kinesis stream latest record. I don't understand why it jumps from 0 second to 10+ hours when we restart the new job. The only main change I introduced with version bump is to change [failOnError|https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/connector/kinesis/sink/KinesisStreamsSink.html] from true to false, but I don't think this is the root cause.

I tried to redeploy the new 1.15.2 job by changing our parallelism, redeploying a job from 1.15.2 does not introduce a big delay, so I assume the issue above only happens when we bump version from 1.14.4 to 1.15.2(note the attached screenshot)? I did try to bump it twice and I see the same 10hrs+ jump in delay, we do not have changes related to any timezones.

Please let me know if this can be filled as a bug, as I do not have a running project with all the kinesis setup available that can reproduce the issue.",Redeployment from 1.14.4 to 1.15.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/22 00:43;wilsonwu;2022-09-14T17_00_00.000Z - 2022-09-14T18_03_44.089Z_14.4-15.2.numbers;https://issues.apache.org/jira/secure/attachment/13050077/2022-09-14T17_00_00.000Z+-+2022-09-14T18_03_44.089Z_14.4-15.2.numbers","05/Oct/22 00:43;wilsonwu;2022-09-15T12_49_54.686Z - 2022-09-15T14_57_44.089Z_15.2-15.2.numbers;https://issues.apache.org/jira/secure/attachment/13050078/2022-09-15T12_49_54.686Z+-+2022-09-15T14_57_44.089Z_15.2-15.2.numbers","20/Sep/22 19:49;wilsonwu;Screen Shot 2022-09-19 at 2.50.56 PM.png;https://issues.apache.org/jira/secure/attachment/13049531/Screen+Shot+2022-09-19+at+2.50.56+PM.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 12 18:09:27 UTC 2023,,,,,,,,,,"0|z18prk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 20:52;martijnvisser;CC [~dannycranmer];;;","25/Sep/22 21:43;liangtl;Hi [~wilsonwu],

Thanks for reporting this. Indeed, we don't expect millisBehindLatest to jump after a version upgrade.

I've tried replicating this issue by upgrading my Kinesis consumer application from 1.14.4 to 1.15.2, but was unable to do so. These are the test scenarios I tried:
 * tested both Polling and EFO
 * tested consuming streams without resharding as well as stream with multiple resharding (mix of shards with records, and without records)
 * tested scaling up job from 2 parallelism to 5 parallelism at the same time as upgrade
 * start the consumer from both TRIM_HORIZON and AT_TIMESTAMP (this shouldn't matter when restoring from a snapshot anyways)

In all these cases, there was no spike in client side millisBehindLatest / server-side IteratorAgeMilliseconds.

 

However, in your case, it seems you were able to reliably replicate the spike in MillisBehindLatest. It is likely that there is some difference in our setup / kinesis stream that causes this issue to happen for your case. To help debug further, would you be able to provide the following, to help root cause the issue?
 * Logs from the taskmanager during the 1.15.2 -> 1.15.2 change (no spike) and 1.14.4 -> 1.15.2 (with spike). We can compare and contrast the restored state for each shard.
 * Kinesis streams have [Enhanced metrics|https://docs.aws.amazon.com/streams/latest/dev/monitoring-with-cloudwatch.html#kinesis-metrics-shard]. If you enable it, we can see IteratorAgeMilliseconds for each shard - and we can see which shardId is seeing the spike in MillisBehindLatest.
 * If possible, it would be nice to have the result from the describe stream call (i.e. {*}aws kinesis describe-stream --stream-name <stream_name>{*}), as this will help us determine the shard parent-child relations.

 

 ;;;","05/Oct/22 00:46;wilsonwu;Hi [~liangtl]  Thank you so much for looking into this, unfortunately I only have logs from task manager available and i do not have access to our prod kinesis stream. I'm not sure if enhanced metrics has been enabled for our kinesis stream and I need to ask my team.

I have attached [^2022-09-14T17_00_00.000Z - 2022-09-14T18_03_44.089Z_14.4-15.2.numbers] (14.4 -> 15.2 where 9 hours latency occurred)

and [^2022-09-15T12_49_54.686Z - 2022-09-15T14_57_44.089Z_15.2-15.2.numbers] (15.2 -> 15.2 no latency)

I applied some filters(keyword ""restored"") on the log messages to show taskmanager related logs only, please let me know if there are any specific log or keyword you want to filter for restored state ;;;","05/Oct/22 07:53;liangtl;Thanks for the logs, [~wilsonwu]!

The logs are a little sparse, but I had a question:
 * The 14.4 -> 15.2 logs from 1700 - 1800 seem to include two restore operations, an incremental one, then a full restore, but I only see 1 spike in your graph. Was there a job restart around 17:43 UTC before the new job was started at 17:47 UTC?

 

Would it be possible to just include a full dump of the logs from the hour? The filter of ""restore"" doesn't give much context, especially when we aren't sure if the issue is with state handling in the runtime being incompatible with the consumer or the kinesis consumer itself. I don't mind if there are logs from the JobManager. If there is concern about sensitive data, can we get anything that has ""flink"" or ""runtime"" or ""kinesis""?

This is the type of logs I'll probably start with.

 
{code:java}
Subtask 4 will start consuming seeded shard StreamShardHandle{streamName='ExampleInputStream', shard='{ShardId: shardId-000000000008,ParentShardId: shardId-000000000001,HashKeyRange: {StartingHashKey: 102084710076281539039012382229530463436,EndingHashKey: 136112946768375385385349842972707284581},SequenceNumberRange: {StartingSequenceNumber: 49633595644994698890342922061873759141399670698320855170,}}'} from sequence number EARLIEST_SEQUENCE_NUM with ShardConsumer 0{code}
 

 

Also, just to confirm the Kinesis connector version you're using is 14.4 and 15.2?;;;","12/Oct/23 18:09;dannycranmer;Resolving as cannot reproduce since this is stale;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Root cause of Exceptions thrown in the SourceReader start() method gets ""swallowed"".",FLINK-29364,13482438,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,afedulov,afedulov,20/Sep/22 17:48,01/Dec/22 08:55,04/Jun/24 20:41,01/Dec/22 08:55,1.15.3,,,,,,,,,,,Runtime / Task,,,,0,,,,,"If an exception is thrown in the {_}SourceReader{_}'s _start()_ method, its root cause does not get captured.

The details are still available here: [Task.java#L758|https://github.com/apache/flink/blob/bccecc23067eb7f18e20bade814be73393401be5/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L758]

But the execution falls through to [Task.java#L780|https://github.com/apache/flink/blob/bccecc23067eb7f18e20bade814be73393401be5/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L780]  and discards the root cause of
canceling the source invokable without recording the actual reason.

 

Hot to reproduce: [DataGeneratorSourceITCase.java#L117|https://github.com/afedulov/flink/blob/3df7669fcc6ba08c5147195b80cc97ac1481ec8c/flink-tests/src/test/java/org/apache/flink/api/connector/source/lib/DataGeneratorSourceITCase.java#L117] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 08:55:11 UTC 2022,,,,,,,,,,"0|z18po0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 16:03;wanglijie;-> But the execution falls through to [Task.java#L780|https://github.com/apache/flink/blob/bccecc23067eb7f18e20bade814be73393401be5/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L780]  and discards the root cause of canceling the source invokable without recording the actual reason.

Is this really the root cause? I think the actual cause has been recorded when call {{{}transitionState{}}}（[Task.java#L778|https://github.com/apache/flink/blob/bccecc23067eb7f18e20bade814be73393401be5/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L778] and [Task.java#L1090|https://github.com/apache/flink/blob/bccecc23067eb7f18e20bade814be73393401be5/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L1090]）;;;","03/Nov/22 07:39;gaoyunhaii;Thanks [~wanglijie] for the checking, and [~afedulov] Could you help to have a double check with that?;;;","01/Dec/22 08:55;wanglijie;I will close this ticket because it doesn't seem to exist and the reporter is no longer active. Feel free to reopen it if any further question.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow web ui to fully redirect to other page,FLINK-29363,13482435,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,20/Sep/22 17:28,02/Jan/23 14:09,04/Jun/24 20:41,02/Jan/23 10:03,1.15.3,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"In a streaming platform system, web ui usually integrates with internal authentication and authorization system. Given the validation failed, the request needs to be redirected to a landing page. It does't work for AJAX request. It will be great to have the web ui configurable to allow auto full redirect. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 02 14:09:31 UTC 2023,,,,,,,,,,"0|z18pnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 18:59;rmetzger;So the setup would be that there's an authenticating proxy between the Flink Web UI and Flink's REST API.
The problem is currently that if a REST API call fails, the UI will just break, instead of redirecting to another page.

How would we be able to distinguish between auth errors and generic errors? I guess based on the HTTP error codes?.
One problem I see is that this setting is purely used in the UI, so we need a way of forwarding a ""global setting"" to the UI ... but I guess that's solvable.;;;","20/Sep/22 20:57;martijnvisser;I don't immediately see why Flink should introduce this redirect. You can still enable solutions like https://developer.okta.com/blog/2018/08/28/nginx-auth-request to secure your Flink UI access, right? There's no redirect needed for that. ;;;","21/Sep/22 15:32;ZhenqiuHuang;Yes, our scenarios is exactly the same as what [~rmetzger] explained. 

[~martijnvisser]
Yes, we have auth proxy already globally. But the proxy server our team built is to limit the access of the job owner (who has already been authenticated) to the web ui of jobs running in k8 cluster. The proxy server runs in a k8 cluster as one of the control complane for all of flink jobs. The setting is required by our security team. Basically, AJAX request need to attache cookie for the access. If cookie expires, we need to a way to help users to redirect to our platform's landing page.;;;","21/Sep/22 16:14;gaborgsomogyi;I've had a deeper look at this issue and here are my findings.
{quote}So the setup would be that there's an authenticating proxy between the Flink Web UI and Flink's REST API.
The problem is currently that if a REST API call fails, the UI will just break, instead of redirecting to another page.
{quote}
Yes, this is the main issue. Adding redirect could be used for authentication renewal and/or routing to specific IP address based on user account.
{quote}I guess based on the HTTP error codes?
{quote}
Yes, basically 401 comes back in case of authentication failure but adding that would be not super generic.
I think it would be better to handle redirection which is not bound to auth failure.
{quote}One problem I see is that this setting is purely used in the UI, so we need a way of forwarding a ""global setting"" to the UI ... but I guess that's solvable.
{quote}
Adding unconditional redirect handling wouldn't require any config. Of course we can introduce something if you think needed.;;;","27/Sep/22 12:01;rmetzger;Thanks a lot for your responses [~gaborgsomogyi]! You are right, just handling the redirect on a 30x code seems to be the right thing to do.;;;","02/Jan/23 10:03;rmetzger;Merged to master for 1.17 in https://github.com/apache/flink/commit/ded2df542fd5d585842e77d021fb84a92a5bea76;;;","02/Jan/23 14:09;martijnvisser;[~rmetzger] Given that this feature is currently not documented, should we at least add something to the release notes? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow loading dynamic config for kerberos authentication in CliFrontend,FLINK-29362,13482416,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,bgeng777,bgeng777,20/Sep/22 15:46,20/Oct/22 07:47,04/Jun/24 20:41,20/Oct/22 07:47,,,,,,,,,,,,Command Line Client,,,,1,,,,,"In the [code|https://github.com/apache/flink/blob/97f5a45cd035fbae37a7468c6f771451ddb4a0a4/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java#L1167], Flink's client will try to {{SecurityUtils.install(new SecurityConfiguration(cli.configuration));}} with configs(e.g. {{security.kerberos.login.principal}} and {{security.kerberos.login.keytab}}) from only flink-conf.yaml.
If users specify the above 2 config via -D option, it will not work as {{cli.parseAndRun(args)}} will be executed after installing security configs from flink-conf.yaml.
However, if a user specify principal A in client's flink-conf.yaml and use -D option to specify principal B, the launched YARN container will use principal B though the job is submitted in client end with principal A.

Such behavior can be misleading as Flink provides 2 ways to set a config but does not keep consistency between client and cluster. It also influence users who want use flink with kerberos as they must modify flink-conf.yaml if they want to use another kerberos user.
",,,,,,,,,,,,,,,,,,,,,FLINK-12130,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 07:46:57 UTC 2022,,,,,,,,,,"0|z18pj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 14:18;gvijay452;Possible duplicate of https://issues.apache.org/jira/browse/FLINK-12130 ;;;","20/Oct/22 07:46;wanglijie;I will close this issue because it's duplicated with FLINK-12130. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to set headers with the new Flink KafkaSink,FLINK-29361,13482401,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,haoxin,haoxin,20/Sep/22 13:43,21/Sep/22 07:58,04/Jun/24 20:41,21/Sep/22 07:58,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"I'm using Flink 1.15.2, when I try to migrate to the new KafkaSink, it seems that it's not possible to add Kafka record headers.

I think we should add this feature or document it if we already have it.

 

Below code is what we can do with FlinkKafkaProducer and ProducerRecord
{code:java}
public class SomeKafkaSerializationSchema<T extends SpecificRecordBase>  implements KafkaSerializationSchema<T> {
  ...
  @Override  public ProducerRecord<byte[], byte[]> serialize(T t, Long ts) {
    ...    
    var record = ProducerRecord<byte[], byte[]>(topic, some_bytes_a);
    record.headers().add(""id"", some_bytes_b);    
    return record;  
  }}

...

var producer = new FlinkKafkaProducer<>(
  topic,  
  new SomeKafkaSerializationSchema<>(...),  
  producerProps,  
  FlinkKafkaProducer.Semantic.AT_LEAST_ONCE
);
 {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 07:58:09 UTC 2022,,,,,,,,,,"0|z18pgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 21:00;martijnvisser;Can you elaborate on what you did in FlinkKafkaProducer which doesn't work for you with KafkaSink? I don't think there should be a problem to write Kafka headers. ;;;","21/Sep/22 07:10;haoxin;Added ;;;","21/Sep/22 07:16;chesnay;You can do the same thing using the KafkaRecordSerializationSchema of the KafkaSink (setRecordSerializer on the builder).;;;","21/Sep/22 07:58;haoxin;thank you guys, will give a try~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Table Connector Documentation,FLINK-29360,13482389,13428958,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,tison,affe,affe,20/Sep/22 12:57,31/Aug/23 02:57,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-20 12:57:30.0,,,,,,,,,,"0|z18pds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Table Connector pom config and packaging,FLINK-29359,13482388,13428958,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,affe,affe,20/Sep/22 12:57,31/Aug/23 04:56,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 04:56:20 UTC 2023,,,,,,,,,,"0|z18pdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 02:57;tison;[~affe] [~syhily] [~leonard] I'm unsure if this ticket means to support SQL jar packaging.

I can see that we already have the module flink-sql-connector-pulsar.;;;","31/Aug/23 04:56;affe;[~tison] Yeah I think when the ticket is created it was still an issue. But I didn't update it timely and I'm unaware of the current situation now. I think it should be safe to close this ticket~ ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Table Connector testing,FLINK-29358,13482387,13428958,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tison,affe,affe,20/Sep/22 12:57,31/Aug/23 02:55,04/Jun/24 20:41,31/Aug/23 02:55,1.17.0,,,,,,,pulsar-4.1.0,,,,Connectors / Pulsar,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 02:55:38 UTC 2023,,,,,,,,,,"0|z18pdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 02:55;tison;master via c71fc862e0d4a782c19f361d3bf581da836cca79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Table Sink code: implementation,FLINK-29357,13482386,13428958,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tison,affe,affe,20/Sep/22 12:56,31/Aug/23 02:55,04/Jun/24 20:41,31/Aug/23 02:55,1.17.0,,,,,,,pulsar-4.1.0,,,,Connectors / Pulsar,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 02:55:20 UTC 2023,,,,,,,,,,"0|z18pd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 02:55;tison;master via c71fc862e0d4a782c19f361d3bf581da836cca79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Table Source code :implementation,FLINK-29356,13482385,13428958,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tison,affe,affe,20/Sep/22 12:56,31/Aug/23 02:55,04/Jun/24 20:41,31/Aug/23 02:55,1.17.0,,,,,,,pulsar-4.1.0,,,,Connectors / Pulsar,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 02:55:03 UTC 2023,,,,,,,,,,"0|z18pcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/23 02:55;tison;master via c71fc862e0d4a782c19f361d3bf581da836cca79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql parse failed because of Desc catalog.database.table is incorrectly parsed to desc catalog ,FLINK-29355,13482374,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,20/Sep/22 12:01,11/Mar/24 12:43,04/Jun/24 20:41,,1.17.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,"If user names the CATALOG he uses as ‘catalog’, and he tries to desc table using syntax 'describe catalog.testDatabase.testTable'. This statement will be incorrectly parsed to 'DESC CATALOG' instead of 'DESC TABLE' .

!image-2022-09-20-20-00-19-478.png|width=592,height=187!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 09:47:47 UTC 2022,,,,,,,,,,"0|z18pag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 09:47;martijnvisser;What happens if backticks ` are provided, since `catalog` is a reserved keyword per https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/overview/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support TO_DATE and TO_TIMESTAMP built-in function in the Table API,FLINK-29354,13482365,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,ana4,ana4,20/Sep/22 11:42,08/Oct/22 07:44,04/Jun/24 20:41,08/Oct/22 07:44,1.16.0,,,,,,,1.17.0,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 07:44:13 UTC 2022,,,,,,,,,,"0|z18p8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 08:16;cun8cun8;Hi [~ana4]  I'm interested in this ticket, could you please assign it to me?;;;","02/Oct/22 06:04;dianfu;[~cun8cun8] Have assign it to you~;;;","08/Oct/22 07:44;dianfu;Merged to master via 946949eda11184c02f56272a2fb24200b6badb29;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support UNIX_TIMESTAMP built-in function in Table API,FLINK-29353,13482364,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,ana4,ana4,20/Sep/22 11:40,10/Oct/22 11:12,04/Jun/24 20:41,10/Oct/22 11:12,1.16.0,,,,,,,1.17.0,,,,API / Python,Table SQL / API,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 11:12:53 UTC 2022,,,,,,,,,,"0|z18p88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 08:16;cun8cun8;Hi [~ana4]  I'm interested in this ticket, could you please assign it to me?;;;","02/Oct/22 06:04;dianfu;[~cun8cun8] Have assign it to you~;;;","10/Oct/22 11:12;dianfu;Merged to master via bca164f97d824fac7c4d42555be806e8db393242;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support CONVERT_TZ built-in function in Table API,FLINK-29352,13482362,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,ana4,ana4,20/Sep/22 11:36,08/Oct/22 08:30,04/Jun/24 20:41,08/Oct/22 08:30,1.16.0,,,,,,,1.17.0,,,,API / Python,Table SQL / API,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-28071,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 08:30:23 UTC 2022,,,,,,,,,,"0|z18p7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/22 08:15;cun8cun8;Hi [~ana4]  I'm interested in this ticket, could you please assign it to me?;;;","02/Oct/22 06:03;dianfu;Have assign it to you~;;;","08/Oct/22 08:30;dianfu;Merged to master via 8d5e27740c34dac4685ba8539c2bec7303314ae6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable input buffer floating for blocking shuffle,FLINK-29351,13482357,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,kevin.cyj,kevin.cyj,20/Sep/22 10:58,17/Jan/23 06:30,04/Jun/24 20:41,17/Jan/23 06:30,,,,,,,,,,,,Runtime / Network,,,,0,pull-request-available,,,,"At input gate, Flink needs exclusive buffers for each input channel. For large parallelism jobs, it is easy to cause ""Insufficient number of network buffers"" error. This ticket aims to make all input network buffers floating for blocking shuffle to reduce the possibility of ""Insufficient number of network buffers"" error. This change can also improve the default blocking shuffle performance because buffer floating can increase the buffer utilization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30469,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 06:30:18 UTC 2023,,,,,,,,,,"0|z18p6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 06:30;xtsong;Subsumed by FLIP-266 (FLINK-30469).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a section for moving planner jar in Hive dependencies page,FLINK-29350,13482320,13477344,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,20/Sep/22 08:08,17/Oct/22 09:13,04/Jun/24 20:41,17/Oct/22 09:13,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,Documentation,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 14 01:24:57 UTC 2022,,,,,,,,,,"0|z18oyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 11:26;jark;Fixed in 
 - master: 7c2f278a1745b4b998c071ae1a31c994c42a2ee6
 - release-1.16: 7867dcc2f4f35f23f104b1e1c29351e885cfde83;;;","13/Oct/22 11:26;jark;[~luoyuxia] could you open a pull request for release-1.16 branch?;;;","14/Oct/22 01:24;luoyuxia;[~jark] The pr is in [https://github.com/apache/flink/pull/21058];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use state ttl instead of timer to clean up state in proctime unbounded over aggregate,FLINK-29349,13482314,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,20/Sep/22 07:39,26/Sep/22 09:46,04/Jun/24 20:41,26/Sep/22 09:46,1.15.2,1.16.0,,,,,,1.17.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,,"Currently we rely on the timer based state cleaning  in proctime  over aggregate, this can be optimized to use state ttl for a more efficienct way",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 09:46:33 UTC 2022,,,,,,,,,,"0|z18ox4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 03:25;lsy;This is duplicated with FLINK-22956?;;;","22/Sep/22 06:39;lincoln.86xy;[~lsy] I took a look at the FLINK-22956, there did exist overlaps, jingsong has tried to remove timers for all over agg functions, but it looks like most of the operators have semantic dependencies and the pr was closed. Btw, FLINK-22956 seems not strongly related to the umbrella ticket which aimed for 'Supports change log inputs for event time operators', 
[~lzljs3620320] do you have a plan for continuing FLINK-22956? If not, do we just keep this new issue (only modify the proctime unbounded over agg operator) and close the former one(FLINK-22956)?;;;","23/Sep/22 02:55;lzljs3620320;[~lincoln.86xy] yes, you can;;;","26/Sep/22 07:11;lincoln.86xy;thanks [~lzljs3620320]! I've submitted a pr for this ticket, and it would be appreciated if you have some time to review it.;;;","26/Sep/22 09:46;lzljs3620320;master: 0f8909cfb5179cb9e87de2615404bb6dd12e359f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The DPP(dynamic partition pruning) can not work with adaptive batch scheduler,FLINK-29348,13482312,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,,,20/Sep/22 07:36,29/Sep/22 05:29,04/Jun/24 20:41,29/Sep/22 05:29,1.16.0,,,,,,,1.16.0,,,,Runtime / Coordination,Table SQL / Runtime,,,0,pull-request-available,,,,"When running tpcds with both DPP(dynamic partition pruning) and adaptive batch scheduler enabled, q14a.sql fails due to the following exception:
{code:java}
2022-09-20 10:34:18,244 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job q14a.sql (6d4355bdde514be083b9762e286626d2) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getGlobalFailureHandlingResult(ExecutionFailureHandler.java:102) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleGlobalFailure(DefaultScheduler.java:299) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:125) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SchedulerBase.deliverOperatorEventToCoordinator(SchedulerBase.java:1031) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.sendOperatorEventToCoordinator(JobMaster.java:588) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_2b75f75b-9d98-44d4-b364-927fcb095b21.jar:1.16-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]
Caused by: java.lang.IllegalStateException: Dynamic filtering data listener is missing: b9e97be4-bde0-4718-bfb1-e13d490517f1
	at org.apache.flink.table.runtime.operators.dynamicfiltering.DynamicFilteringDataCollectorOperatorCoordinator.handleEventFromOperator(DynamicFilteringDataCollectorOperatorCoordinator.java:98) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.lambda$handleEventFromOperator$0(RecreateOnResetOperatorCoordinator.java:84) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.applyCall(RecreateOnResetOperatorCoordinator.java:315) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator.handleEventFromOperator(RecreateOnResetOperatorCoordinator.java:82) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.handleEventFromOperator(OperatorCoordinatorHolder.java:218) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.deliverOperatorEventToCoordinator(DefaultOperatorCoordinatorHandler.java:121) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	... 31 more{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 29 05:29:38 UTC 2022,,,,,,,,,,"0|z18owo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 07:37;wanglijie#1;cc [~gaoyunhaii] [~pltbkd] [~zhuzh] ;;;","29/Sep/22 05:29;gaoyunhaii;Merged on master via 7bae0ebb6379c175522bd903838bb3737fc6c65d

Merged on release-1.16 via b30a502c53eaca95630eddd03022871f17fdf299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to restore from list state with empty protobuf object,FLINK-29347,13482293,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,shenjiaqi,shenjiaqi,shenjiaqi,20/Sep/22 06:12,26/Oct/22 09:32,04/Jun/24 20:41,26/Oct/22 09:32,1.14.2,1.15.0,,,,,,1.17.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,bugfix,checkpoint,pull-request-available,states,"I use protobuf generated class in an union list state.
When my flink job restores from checkpoint, I get exception:
{code:java}
Caused by: java.lang.RuntimeException: Could not create class com.MY_PROTOBUF_GENERATED_CLASS
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] 
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left. 
	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] 
Caused by: java.io.EOFException: No more bytes left. 
	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:128) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:314) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40) ~[my-lib-0.1.1-SNAPSHOT.jar:?] 
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:354) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.deserializeOperatorStateValues(OperatorStateRestoreOperation.java:217) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.OperatorStateRestoreOperation.restore(OperatorStateRestoreOperation.java:188) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.state.DefaultOperatorStateBackendBuilder.build(DefaultOperatorStateBackendBuilder.java:80) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createOperatorStateBackend(EmbeddedRocksDBStateBackend.java:482) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$operatorStateBackend$0(StreamTaskStateInitializerImpl.java:277) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.operatorStateBackend(StreamTaskStateInitializerImpl.java:286) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:174) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:109) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:711) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:687) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:654) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575) ~[flink-dist_2.12-1.14.4.jar:1.14.4] 
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] 
{code}
 

I find it is because when protobuf serializer serializes an object, which is built directly with builder without assign any value to field, the serializer will generate a zero length byte[] and then write it into state with content '\0'（indicates zero length data).

When recovered from checkpoint, protobuf seralizer deserialize the data. It get length 0, and call InputStream#read(byte[] bytes, int offset, int count) with count = 0.

The underlying Input implementation is [NoFetchingInput|https://github.com/apache/flink/blob/9d2ae5572897f3e2d9089414261a250cfc2a2ab8/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/NoFetchingInput.java]. It will call Inputsteam#read(byte[] bytes, int offset, int count) with count = 0.

The InputStream implementation is [ByteStateHandleInputStream|https://github.com/apache/flink/blob/53d5e1cf9666517bc2fded60b510f2fd13d93f10/flink-runtime/src/main/java/org/apache/flink/runtime/state/memory/ByteStreamStateHandle.java#L140-L153], It will {*}return -1 as long as no data left in memory，even if count is 0{*}.

A simple fix is add check before return -1. If caller reads 0 bytes, it should always return 0 instead of -1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 09:32:35 UTC 2022,,,,,,,,,,"0|z18osg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/22 09:32;yunta;merged in master: db46d434820e8cd964ea97ab2396d8dc9b43ad9f ... d3c7fc9be6aa13872fa2877236e7b14acd076679;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File naming configuration for filesink output,FLINK-29346,13482280,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Incomplete,,djkooks,djkooks,20/Sep/22 04:21,26/Feb/23 12:18,04/Jun/24 20:41,26/Feb/23 12:18,1.13.6,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"Hello,
I've made some research about file sink, but I couldn't find configuration about file naming (based on elements at stream).

https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/datastream/file_sink/

Are there some reason about this? Because I need naming option of outputs, based on data in stream.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 11:11:01 UTC 2022,,,,,,,,,,"0|z18opk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 09:49;martijnvisser;[~djkooks] There's the possibility to provide prefix and postfix suffixes, per https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/filesystem/#part-file-configuration
Is that sufficient for your use case?;;;","26/Sep/22 10:42;djkooks;[~martijnvisser]hello,
Yes I know about prefix-suffix configuration. But it cannot change full file name, and cannot make flexible by input data information.;;;","26/Sep/22 11:11;martijnvisser;[~djkooks] I'm expecting that the filenames play a role in the exactly-once guarantees of the FileSink, so I don't think it will be possible to change the full file name yourself https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/connector/file/sink/FileSink.html

I would expect that using the OutputFileConfig you should be able to make the prefix and postfix depend on your input data https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/streaming/api/functions/sink/filesystem/OutputFileConfig.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many open files in table store orc writer,FLINK-29345,13482279,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,20/Sep/22 03:58,08/Oct/22 07:45,04/Jun/24 20:41,08/Oct/22 07:45,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,," !image-2022-09-20-11-57-11-373.png! 

We can avoid reading the local file to obtain the config every time we create a new writer by reusing the prepared configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/22 03:57;lzljs3620320;image-2022-09-20-11-57-11-373.png;https://issues.apache.org/jira/secure/attachment/13049484/image-2022-09-20-11-57-11-373.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 07:45:38 UTC 2022,,,,,,,,,,"0|z18opc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 05:38;zjureel;Hi [~lzljs3620320] I'd like to pick this issue to start my contribution to flink-table-store, can you assign it to me? THX;;;","20/Sep/22 06:07;lzljs3620320;[~zjureel] Thanks!;;;","21/Sep/22 03:39;lzljs3620320;master: 835632c6e4758ad7d11ccbdb3a8ebb8dfa6aa709
release-0.2: 1973b737fc97fe1ecc83ee2c80f3e9253557ed7a;;;","28/Sep/22 09:56;lzljs3620320;The orc writer has memory management and needs to be initialized in the writing thread to avoid thread safety conflicts caused by memory management.

We should revert this PR.;;;","28/Sep/22 10:11;lzljs3620320;revert in:
master: 696d65c387b708228c99bef6914a8859c301d6e8
release-0.2:  2cddc8f7df36a2ecce77184a4a6060bfe1f0a916;;;","08/Oct/22 07:45;lzljs3620320;Re-merged in:
master: bee050f0301b00c620d4129c7a74ffbbc3d628ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Adaptive Scheduler supports Fine-Grained Resource Management,FLINK-29344,13482276,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,xtsong,xtsong,20/Sep/22 03:28,25/Jul/23 11:59,04/Jun/24 20:41,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"This ticket is a reflection of the following Slack discussion:
{quote}
Donatien Schmitz
Adaptive Scheduler thread:
Hey all, it seems like the Adaptive Scheduler does not support fine grain resource management. I have fixed it and would like to know if you would be interested in a PR or if it was purposely designed to not support Fine grain resource management.

rmetzger
@Donatien Schmitz: I’m concerned that we don’t have a lot of review capacity right now, and I’m now aware of any users asking for it.

rmetzger
I couldn’t find a ticket for adding this feature, did you find one?
If not, can you add one? This will allow us to at least making this feature show up on google, and people might comment on it, if they need it.

rmetzger
If the change is fairly self-contained, is unlikely to cause instabilities, then we can also consider merging it

rmetzger
@Xintong Song what do you think?

Xintong Song
@rmetzger, thanks for involving me.
@Donatien Schmitz, thanks for bringing this up, and for volunteering on fixing this. Could you explain a bit more about how do you plan to fix this?
Fine-grained resource management is not yet supported by adaptive scheduler, because there’s an issue that we haven’t find a good solution for. Namely, if only part of the resource requirements can be fulfilled, how do we decide which requirements should be fulfilled. E.g., say the job declares it needs 10 slots with resource 1 for map tasks, and another 10 slots with resource 2 for reduce tasks. If there’s not enough resources (say only 10 slots can be allocated for simplicity), how many slots for map / reduce tasks should be allocated? Obviously, <10 map, 0 reduce> & <0 map, 10 reduce> would not work. For this example, a proportional scale-down (<5 map, 5 reduce>) seems reasonable. However, a proportional scale-down is not always easy (e.g., requirements is <100 map, 1 reduce>), and the issue grows more complicated if you take lots of stages and the differences of slot sizes into consideration.
I’d like to see adaptive scheduler also supports fine-grained resource management. If there’s a good solution to the above issue, I’d love to help review the effort.

Donatien Schmitz
Dear Robert and Xintong, thanks for reading and reacting to my message! I'll reply tomorrow (GTM +1 time) if that's quite alright with you. Best, Donatien Schmitz

Donatien Schmitz
@Xintong Song
* We are working on fine-grain scheduling for resource optimisation of long running or periodic jobs. One of the feature we are experiencing is a ""rescheduling plan"", a mapping of operators and Resource Profiles that can be dynamically applied to a running job. This rescheduling would be triggered by policies about some metrics (focus on RocksDB in our case).
* While developing this new feature, we decided to implement it on the Adpative Scheduler instead of the Base Scheduler because the logic brought by the state machine already present made it more logical: transitions from states Executing -> Cancelling -> Rescheduling -> Waiting for Resources -> Creating -> Executing
* In our case we are working on a POC and thus focusing on a real simple job with a // of 1. The issue you brought is indeed something we have faced while raising the // of the job.
* If you create a Jira Ticket we can discuss it over there if you'd like!

Donatien Schmitz
@rmetzger The changes do not break the default resource management but does not fix the issue brought out by Xintong.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 11:59:28 UTC 2023,,,,,,,,,,"0|z18ooo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 03:29;xtsong;cc [~donaschmi] [~rmetzger] ;;;","25/Jul/23 11:59;knaufk;Since the feature freeze has passed, I will mark this as Won't Do for Flink 1.18 in the Wiki and remove the fixVersion. Thanks, Konstantin (one of the release managers for Flink 1.18).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to execute ddl in HiveDialect when use specifc catalog in sql statement,FLINK-29343,13482266,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,20/Sep/22 02:59,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Connectors / Hive,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-20 02:59:43.0,,,,,,,,,,"0|z18omg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not query writer.length() per record in RollingFileWriter,FLINK-29342,13482254,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,20/Sep/22 01:53,26/Sep/22 09:44,04/Jun/24 20:41,26/Sep/22 09:44,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,It is expensive for the local file system to get pos.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 09:44:14 UTC 2022,,,,,,,,,,"0|z18ojs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 09:44;lzljs3620320;master: 557594ed42f075383f0a35c1320ba98f00171e88;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Apache Kafka version to 2.6.3 to resolve CVE-2021-38153,FLINK-29341,13482226,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,snathani406,snathani406,19/Sep/22 21:14,20/Sep/22 07:59,04/Jun/24 20:41,20/Sep/22 07:59,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,The flink-connector-kafka module has Kafka as dependency being intorduced from here [https://github.com/apache/flink/blob/release-1.13.6/flink-connectors/flink-connector-kafka/pom.xml] . The version of kafka is 2.4.1 which is vulnerable having CVE-2021-38153 . In order to remove this CVE kafka version should be upgraded to 2.6.3 as said here  https://lists.apache.org/thread/7vrvjt7tm7m46txds3kt6bywd8vp5px0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 07:59:42 UTC 2022,,,,,,,,,,"0|z18odk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 07:59;martijnvisser;[~snathani406] This has already been addressed in Flink 1.15 via FLINK-24765. You should update your Flink installation to a version that's still supported by the community (Flink 1.14 and Flink 1.15 at this moment). In Flink 1.16 this will even be 3.2.1 via FLINK-28060;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceManagerTest#testProcessResourceRequirementsWhenRecoveryFinished prone to race condition,FLINK-29340,13482218,13481290,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,19/Sep/22 19:24,22/Sep/22 23:23,04/Jun/24 20:41,22/Sep/22 23:22,1.16.0,,,,,,,1.17.0,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,,"The test incorrectly assumes that the {{declareRequiredResources}} has already been run when calling {{runInMainThread}}, while the RPC could still be in flight.
This can result in the test failing because within runInMainThread the test assumes that completing the readyToServeFuture will immediately result in the processing of resources, due to this workflow having been set up within delcareRequiredResources. Without it it will just fail because the completion of the future has in practice no effect.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24713,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 23:22:59 UTC 2022,,,,,,,,,,"0|z18obs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 23:22;chesnay;master: a54b2a8674e4df6345968e538636a7960112bb9e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobMasterPartitionTrackerImpl#requestShuffleDescriptorsFromResourceManager blocks main thread,FLINK-29339,13482201,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,xuannan,chesnay,chesnay,19/Sep/22 16:25,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,stale-assigned,,,"{code:java}
private List<ShuffleDescriptor> requestShuffleDescriptorsFromResourceManager(
        IntermediateDataSetID intermediateDataSetID) {
    Preconditions.checkNotNull(
            resourceManagerGateway, ""JobMaster is not connected to ResourceManager"");
    try {
        return this.resourceManagerGateway
                .getClusterPartitionsShuffleDescriptors(intermediateDataSetID)
                .get(); // <-- there's your problem
    } catch (Throwable e) {
        throw new RuntimeException(
                String.format(
                        ""Failed to get shuffle descriptors of intermediate dataset %s from ResourceManager"",
                        intermediateDataSetID),
                e);
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27523,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 08:25:33 UTC 2023,,,,,,,,,,"0|z18o80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 02:58;gaoyunhaii;[~xuannan]  Could you have a look at this issue~?;;;","20/Sep/22 07:06;xuannan;[~gaoyunhaii] I will take a look. Could you assign the ticket to me?;;;","28/Sep/22 04:20;gaoyunhaii;Hi [~chesnay] , I'm a bit concern in that we mark this issue as blocker: the issue itself should only affects the jobs that using cached result partition. besides, in consideration of that the result is already cached in the `JobMasterPartitionTrackerImpl`, the actual numbers of the rpc request is limited.

But the fix might affect the jobs that do not use cached result partition, thus I'm a bit concern on the risk that we modify the critical path of submitting jobs at a moment closing to publishing. 

I think we may postpone the fix after the publishing, till the start of the next version, then we could have more time to ensure the fix does not bring other issues. How do you think about it?;;;","28/Sep/22 09:01;chesnay;> the actual numbers of the rpc request is limited.

The numbers are irrelevant though, as is the size of the response.
You just need a single request to time out (e.g., due to the RM actor crashing) to potentially crash the entire cluster because of heartbeat timeouts.

You make a good point that the fix is quite involved though, and it is indeed only called if cluster partitions are actually used. So I'm fine with not treating it as a blocker.;;;","29/Sep/22 03:08;gaoyunhaii;> The numbers are irrelevant though, as is the size of the response.
You just need a single request to time out (e.g., due to the RM actor crashing) to potentially crash the entire cluster because of heartbeat timeouts.

Thanks a lot for the clarification. We'll try to avoid the synchronization calls in the future development.

Then let's postpone the fix till the beginning of the next version.;;;","20/Oct/22 07:23;xtsong;Any updates on this? [~xuannan][~gaoyunhaii];;;","27/Oct/22 07:20;gaoyunhaii; The PR is already under review previously, let's continue the review in the pull request page. ;;;","15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","29/Aug/23 08:25;renqs;[~gaoyunhaii] Any updates on this one? Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Try to unify observe/deploy config handling in the operator,FLINK-29338,13482200,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,gyfora,gyfora,gyfora,19/Sep/22 16:19,22/Sep/22 12:36,04/Jun/24 20:41,22/Sep/22 12:36,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"The operator currently juggles different configuration versions in the observer / reconcile loops : observe/deploy config.

The way they are used in certain cases can lead to problems or unexpected behaviour and is complex to understand.

We can probably eliminate the use of observe config or at least try to unify it as much as possible for a more straightforward behaviour.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 12:36:57 UTC 2022,,,,,,,,,,"0|z18o7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:36;gyfora;seems like the current logic is actually fully necessary, let's skip this for now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to query non-hive table in Hive dialect,FLINK-29337,13482171,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,19/Sep/22 12:17,11/Dec/22 10:51,04/Jun/24 20:41,14/Oct/22 06:28,,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,Flink will fail for the query with non-hive table in HiveDialect.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29447,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 02:45:20 UTC 2022,,,,,,,,,,"0|z18o1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/22 12:18;jark;Fixed in 
 - master: 3387fffb05e3973a508726771e6ae48bc5f7c303
 - release-1.16: c8161fde08fc15e2bb102e8678665f4e1d26b7a8;;;","12/Oct/22 12:19;jark;Shall we open a pull request for 1.16 as well? [~luoyuxia];;;","12/Oct/22 12:34;luoyuxia;[~jark] Yes. I have opened the pr [https://github.com/apache/flink/pull/21034];;;","24/Nov/22 02:45;jingzhang;Shall we open a pr for 1.15 as well? [~jark] [~luoyuxia] . ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SubtaskState#discardState swallows exceptions,FLINK-29336,13482170,13433331,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,19/Sep/22 12:00,19/Sep/22 12:00,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,{{SubtaskState#discardState}} logs exceptions instead of forwarding them,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-19 12:00:25.0,,,,,,,,,,"0|z18o1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorSubtaskState swallows exception,FLINK-29335,13482169,13433331,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,19/Sep/22 11:54,19/Sep/22 11:54,04/Jun/24 20:41,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,{{OperatorSubtaskState#discardState}} doesn't forward the exception but prints a log output. This prevents repeatable cleanup.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-19 11:54:55.0,,,,,,,,,,"0|z18o14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateHandleStore#releaseAndTryRemoveAll is not used and can be removed,FLINK-29334,13482166,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Wencong Liu,mapohl,mapohl,19/Sep/22 11:47,13/Apr/23 08:18,04/Jun/24 20:41,13/Apr/23 08:18,1.17.0,,,,,,,1.18.0,,,,Runtime / Coordination,,,,0,pull-request-available,starer,,,{{StateHandleStore#releaseAndTryRemoveAll}} isn't used in production code. There is also not a real reason to do a final cleanup. We should clean up component at the right location that than doing a wipe-out at the end.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 08:18:38 UTC 2023,,,,,,,,,,"0|z18o0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 06:00;Wencong Liu;Hello [~mapohl], I'd like to take this ticket. Could you please assign to me？;;;","27/Mar/23 12:50;mapohl;I assigned the ticket to you.;;;","10/Apr/23 09:37;Wencong Liu;[~mapohl]  CI on GitHub has passed :), please take a look. ;;;","12/Apr/23 10:23;mapohl;It's on my list of things to do (y);;;","13/Apr/23 08:18;mapohl;master: 2dc37f06bd343ef91d2c8c7cd5c00a9bbad9dc01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Planner Module doesn't read config from flink-conf.yaml,FLINK-29333,13482154,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dbadaya,dbadaya,19/Sep/22 10:56,20/Aug/23 22:35,04/Jun/24 20:41,,1.15.0,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,pull-request-available,,,"PlannerModule class doesn't seem to be reading the configs from the {{/etc/flink/conf/flink-conf.yaml}} directory. ([code ref|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-loader/src/main/java/org/apache/flink/table/planner/loader/PlannerModule.java#L95]). It is only reading the default config values defined in the Java code. So, we can't override configs using flink-conf.yaml.

 

 

Use-case: We need to modify the default value of {{io.tmp.dirs}} as the default value (/tmp) is a symlink on our platform , and {{java.nio.file.Files.createDirectory}} (which is used By Flink in this case) doesn't handle symlinks properly [ref|https://bugs.openjdk.org/browse/JDK-8130464].

{code:java}
Caused by: java.nio.file.FileAlreadyExistsException: /tmp at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) at java.nio.file.Files.createDirectory(Files.java:674) at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) at java.nio.file.Files.createDirectories(Files.java:727) at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:96)
{code}
 ",,,,,,,,,,,,,,,,,,,,,FLINK-29332,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 22:35:18 UTC 2023,,,,,,,,,,"0|z18nxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Planner Module doesn't read config from flink-conf.yaml,FLINK-29332,13482153,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,dbadaya,dbadaya,19/Sep/22 10:55,19/Sep/22 13:22,04/Jun/24 20:41,19/Sep/22 13:22,1.15.0,,,,,,,,,,,Table SQL / Planner,,,,0,,,,,"PlannerModule class doesn't seem to be reading the configs from the {{/etc/flink/conf/flink-conf.yaml}} directory. ([code ref|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-loader/src/main/java/org/apache/flink/table/planner/loader/PlannerModule.java#L95]). It is only reading the default config values defined in the Java code. So, we can't override configs using flink-conf.yaml.

 

 

Use-case: We need to modify the default value of {{io.tmp.dirs}} as the default value (/tmp) is a symlink on our platform , and {{java.nio.file.Files.createDirectory}} (which is used By Flink in this case) doesn't handle symlinks properly [ref|https://bugs.openjdk.org/browse/JDK-8130464].

{{}}

{{}}
{code:java}
Caused by: java.nio.file.FileAlreadyExistsException: /tmp at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) at java.nio.file.Files.createDirectory(Files.java:674) at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) at java.nio.file.Files.createDirectories(Files.java:727) at org.apache.flink.table.planner.loader.PlannerModule.<init>(PlannerModule.java:96)
{code}
 ",,,,,,,,,,,,,,,,,,,,FLINK-29333,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-19 10:55:13.0,,,,,,,,,,"0|z18nxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pre-aggregated merge supports changelog inputs,FLINK-29331,13482137,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,19/Sep/22 09:19,19/Mar/23 05:48,04/Jun/24 20:41,19/Mar/23 05:48,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"In FLINK-27626 ,  we have supported pre-agg merge, but no changelog inputs support.
We can support changelog inputs for some function, like sum/count.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27626,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-19 09:19:50.0,,,,,,,,,,"0|z18nu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide better logs of MiniCluster shutdown procedure,FLINK-29330,13482133,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,19/Sep/22 09:13,11/Mar/24 12:43,04/Jun/24 20:41,,,,,,,,,1.20.0,,,,Tests,,,,0,,,,,"I recently ran into an issue where the shutdown of a MiniCluster timed out. The logs weren't helpful at all and I had to go in and check every asynchronously component for whether _that_ component was the cause.

The main issues were that various components don't log anything at all, or that when they did it wasn't clear who owned that component.

I'd like to add a util that makes it easier for us log the start/stop of a shutdown procedure,
{code:java}
public class ShutdownLog {
    /**
     * Logs the beginning and end of the shutdown procedure for the given component.
     *
     * <p>This method accepts a {@link Supplier} instead of a {@link CompletableFuture} because the
     * latter usually required implies the shutdown to already have begun.
     *
     * @param log Logger of owning component
     * @param component component that will be shut down
     * @param shutdownTrigger component shutdown trigger
     * @return termination future of the component
     */
    public static <C> CompletableFuture<C> logShutdown(
            Logger log, String component, Supplier<CompletableFuture<C>> shutdownTrigger) {
        log.debug(""Starting shutdown of {}."", component);
        return FutureUtils.logCompletion(log, ""shutdown of "" + component, shutdownTrigger.get());
    }
}

public class FutureUtils {
    public static <T> CompletableFuture<T> logCompletion(
            Logger log, String action, CompletableFuture<T> future) {
        future.handle(
                (t, throwable) -> {
                    if (throwable == null) {
                        log.debug(""Completed {}."", action);
                    } else {
                        log.debug(""Failed {}."", action, throwable);
                    }
                    return null;
                });
        return future;
    }
...
{code}
and extend the AutoCloseableAsync interface for an easy opt-in and customized logging:
{code:java}
    default CompletableFuture<Void> closeAsync(Logger log) {
        return ShutdownLog.logShutdown(log, getClass().getSimpleName(), this::closeAsync);
    }
{code}
MiniCluster example usages:
{code:java}
-terminationFutures.add(dispatcherResourceManagerComponent.closeAsync())
+terminationFutures.add(dispatcherResourceManagerComponent.closeAsync(LOG))
{code}
{code:java}
-return ExecutorUtils.nonBlockingShutdown(
-        executorShutdownTimeoutMillis, TimeUnit.MILLISECONDS, ioExecutor);
+return ShutdownLog.logShutdown(
+        LOG,
+        ""ioExecutor"",
+        () ->
+                ExecutorUtils.nonBlockingShutdown(
+                        executorShutdownTimeoutMillis,
+                        TimeUnit.MILLISECONDS,
+                        ioExecutor));
{code}
[~mapohl] I'm interested what you think about this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 04 12:32:58 UTC 2022,,,,,,,,,,"0|z18nt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 10:47;mapohl;I guess, it's a good idea. The shutdown logic is hard to understand in general. Having some consistent log output will definitely help identifying the root cause.

When going through the shutdown process ([sequence diagram|https://gist.github.com/XComp/ad221f2ef33e99ac7b9b03113889b918]) I noticed that we do not use {{AutoClosableAsync}} in all the cases where it would be useful (more specifically, {{LeaderRetrievalService}}). This might be a followup aligning the shutdown process in this sense.;;;","19/Sep/22 10:54;chesnay;> I noticed that we do not use {{AutoClosableAsync}} in all the cases where it would be useful

Other examples include RpcService, MetricRegistryimpl, DispatcherOperationCaches.;;;","30/Sep/22 11:20;chesnay;Should LeaderRetrievalService really implement it though? It doesn't seem to be doing asynchronously; or are you proposing to change that or keep that as an implementation detail?;;;","04/Oct/22 12:32;mapohl;I guess, my initial intention was to implement the close functionality consistent throughout the components. In [DispatcherResourceManagerComponent#closeAsyncInternal|https://github.com/apache/flink/blob/cb478fb751dbe28405152707040f9126b5a5269b/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/component/DispatcherResourceManagerComponent.java#L164], {{LeaderRetrievalServer#stop}} is the only synchronous call that is triggered. But you're right - looking into the actual implementation made me realize that there's not much to gain making these calls asynchronous.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint can not be triggered if encountering OOM,FLINK-29329,13482093,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,tanyuxin,tanyuxin,19/Sep/22 03:10,08/Dec/22 08:21,04/Jun/24 20:41,08/Dec/22 08:21,,,,,,,,1.13.7,,,,,,,,0,,,,,"When writing a checkpoint, an OOM error is thrown. But the JM is not failed and is restored because I found a log ""No master state to restore"".

Then JM never makes checkpoints anymore. Currently, the root cause is not that clear, maybe this is a bug and we should deal with the OOM or other exceptions when making checkpoints.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/22 03:15;tanyuxin;job-exceptions-1.txt;https://issues.apache.org/jira/secure/attachment/13049423/job-exceptions-1.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 08 08:20:57 UTC 2022,,,,,,,,,,"0|z18nk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 04:06;yunta;Do you mean once we incurred  OOM, then no checkpoints would be triggered by JM or the next triggered checkpoints could lead to OOM problem each time?;;;","19/Sep/22 08:04;tanyuxin;[~yunta] Thanks for the quick replay. I mean then no checkpoints would be triggered by JM once OOM occurs.;;;","22/Sep/22 02:40;yunta;I think the problem of not triggering the checkpoints anymore should be related to the [schedule timer|https://github.com/apache/flink/blob/b5cd9f34ab73fa69a3db5a09908c1aa954ed0597/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L168]. If you could reproduce this problem, I think you could use jmap dump the job manager to see what happened to CheckpointCoordinator#timer.;;;","08/Dec/22 08:20;tanyuxin;Never reproduce it again, close temporarily. If encountered it again, I will open it and give more details.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
【Flink is having problems using the status expiration setting】,FLINK-29328,13482087,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Bug,,Jason_H,Jason_H,19/Sep/22 02:37,18/Aug/23 02:31,04/Jun/24 20:41,18/Aug/23 02:29,1.14.3,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,"I am flink1.14.3 based version, the following problems when using the finish a Flink homework for the first time, add the TTL Settings, and then start the homework, automatic recovery at a particular time homework problems, the following error, specific see attachment pictures, eventually repair method is, in creating a state descriptor is to change the wording, As follows:
 * Before the error:

{code:java}
public static final MapStateDescriptor<String, Integer> quantityJudgeStateDescriptor = new MapStateDescriptor<>(
        ""quantityJudgeMapState"",
        String.class,
        Integer.class); {code}
 * After the error is reported:

{code:java}
public static final MapStateDescriptor<String, RateUnionVo> rateAlgorithmStateProperties = new MapStateDescriptor<>(
        ""rateAlgorithmMapState"",
        TypeInformation.of(new TypeHint<String>() {
        }),
        TypeInformation.of(new TypeHint<RateUnionVo>() {
        })
); {code}
After changing this way of writing, the test did not appear the above problem, do not know whether it is a bug problem, raise this problem, in order to trace the source.

 ",!报错1.jpg!!报错2.jpg!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/22 01:56;Jason_H;报错1.jpg;https://issues.apache.org/jira/secure/attachment/13049420/%E6%8A%A5%E9%94%991.jpg","19/Sep/22 01:56;Jason_H;报错2.jpg;https://issues.apache.org/jira/secure/attachment/13049419/%E6%8A%A5%E9%94%992.jpg",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 02:29:40 UTC 2023,,,,,,,,,,"0|z18niw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 03:37;Yanfei Lei;From your attachments, this issue is caused by state incompatibility. Did you add some new states when you configurate TTL? 

（从附件中的log来看，是restore前后state不适配引起的。您在配置TTL时在代码中加了新的state吗？;;;","19/Sep/22 03:42;yunta;[~Jason_H] Please use English to update this ticket.;;;","19/Sep/22 09:30;Jason_H;Yanfei Lei Thanks for the quick replay.

No, my job is brand new and has not been changed and started with a brand new job deployment.

我的作业是全新的，没有改变状态，是以新作业启动的。

During this run, only an automatic restart of the job occurred and no state changes were made.

并且，在作业运行过程中，并没有对作业进行过状态的修改。;;;","21/Oct/22 06:53;yunta;First of all, the problem would only occur during state restoring. The state descriptors (quantityJudgeStateDescriptor and rateAlgorithmStateProperties) you offered is totally different, and they must meet the migration problem due to incompatible serializers.;;;","18/Aug/23 02:29;masteryhx;Hi, [~Jason_H] 
I aggree with [~yunta] , this exception is only be thrown when you change your serializer related info which may cause incompatibility.
You could check the logic in [1].

I just closed it because it seems not a bug and no response nearly one year.
Please reopen it if we missed something.

[1] [https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/fault-tolerance/serialization/schema_evolution/];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator configs are showing up among standard Flink configs,FLINK-29327,13481916,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hpeter,morhidi,,16/Sep/22 16:35,24/Nov/22 01:01,04/Jun/24 20:41,23/Sep/22 07:05,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,1,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 07:05:37 UTC 2022,,,,,,,,,,"0|z18mgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 16:43;ZhenqiuHuang;[~matyas] [~bamrabi]
I am glad to take this task.;;;","16/Sep/22 16:59;gyfora;Is this actually a problem that we should fix? I see some value in being able to see the config used when deploying only by looking at the job even if it doesn’t affect the job itself;;;","16/Sep/22 18:44;ZhenqiuHuang;It is true that it doesn't impact the functionality. But from user experience, it is confusing for users to see configures that are not specified by themselves. Specially, when user can see the cluster level config kubernetes.operator.plugins.listeners.kafka-dr.private.key.location, it has also security concern.;;;","16/Sep/22 21:10;gyfora;Makes sense:);;;","23/Sep/22 07:05;gyfora;merged to main 7f7b77cc434e6c2b67071404c5c0a8c0a28e36db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EndOfDataDecoderException observed when uploading a jar,FLINK-29326,13481884,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,mapohl,mapohl,16/Sep/22 13:03,16/Sep/22 14:49,04/Jun/24 20:41,16/Sep/22 14:49,1.15.1,,,,,,,,,,,Runtime / REST,,,,0,,,,,"We've observed a {{EndOfDataDecoderException}} in the JobManager when uploading a jar:
{code}
[2022-09-15 23:55:34,092] WARN File upload failed. (org.apache.flink.runtime.rest.FileUploadHandler:225)
org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostRequestDecoder$EndOfDataDecoderException: null
      at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.hasNext(HttpPostMultipartRequestDecoder.java:381) ~[flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostRequestDecoder.hasNext(HttpPostRequestDecoder.java:228) ~[flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:151) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [flink-dist-1.15.1.jar:1.15.1]
      at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.15.1.jar:1.15.1]
      at java.lang.Thread.run(Thread.java:829) [?:?]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-9500,,,,,,,"16/Sep/22 13:21;mapohl;jobmanager.log;https://issues.apache.org/jira/secure/attachment/13049377/jobmanager.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 14:46:57 UTC 2022,,,,,,,,,,"0|z18ma0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 13:41;mapohl;There's a [PR discussion|https://github.com/apache/flink/pull/6189#pullrequestreview-130727220] related to FLINK-9500.;;;","16/Sep/22 13:48;chesnay;I would have to assume that the client didn't submit everything based on this line in the logs:

{code:java}
java.io.IOException: Connection reset by peer
{code}
;;;","16/Sep/22 14:46;mapohl;hm, good point. I did notice that one because I was looking into the more verbose logs. I should have had a proper look into the filter logs. Anyway, thanks for taking a look. It sounds reasonable.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation bug on how to enable batch mode for streaming examples,FLINK-29325,13481871,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jun He,Jun He,Jun He,16/Sep/22 11:44,21/Sep/22 02:50,04/Jun/24 20:41,21/Sep/22 02:50,1.15.2,,,,,,,1.15.3,1.16.0,1.17.0,,Client / Job Submission,Documentation,,,0,pull-request-available,,,,"In latest flink doc, it says that we should use '-Dexecution.runtime-mode=BATCH' to enable batch mode，but it does not work actually. The wrong way is as below:

bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar

we should use '--execution-mode batch' instead, the correct way is as below

bin/flink run examples/streaming/WordCount.jar --execution-mode batch",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24830,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 02:50:57 UTC 2022,,,,,,,,,,"0|z18m74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 02:50;yunta;merged in master(1.17): 05600f844a904f34ab45f512715a76193974b497

release-1.16: de4aa4b7fee0f112fa3cfe66d0ad620841e18d74

release-1.15: 9ee1589c42565f47fdae8b82d488e6610bdb7fc6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling Kinesis connector close method before subtask starts running results in NPE,FLINK-29324,13481870,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiaohei,foxus,foxus,16/Sep/22 11:30,28/Nov/22 11:04,04/Jun/24 20:41,20/Sep/22 15:52,1.14.5,1.15.2,,,,,,1.15.3,1.16.0,1.17.0,,Connectors / Kinesis,,,,0,pull-request-available,,,,"When a Flink application is stopped before a Kinesis connector subtask has been started, the following exception is thrown:
{noformat}
java.lang.NullPointerException
at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer.close(FlinkKinesisConsumer.java:421)
...{noformat}
This appears to be related to the fact that [fetcher creation|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L307] may not occur by [the time it is referenced when the consumer is closed|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L421].

A suggested fix is to make the {{close()}} method null safe [as it has been in the {{cancel()}} method|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisConsumer.java#L407].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30224,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 15:52:01 UTC 2022,,,,,,,,,,"0|z18m6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 08:12;xiaohei;Hello, could please assign this ticket to me?;;;","19/Sep/22 18:35;dannycranmer;Thanks [~xiaohei];;;","20/Sep/22 15:32;dannycranmer;Merged commit [{{71fea9a}}|https://github.com/apache/flink/commit/71fea9a4522505a6c0f23f1de599b7f87a633ccf] into master.

Merged commit [{{22086c}}|https://github.com/apache/flink/commit/22086c67a6a97148eb74ed32b281eec393721738] into release-1.16

Merged commit [{{eb6565}}|https://github.com/apache/flink/commit/eb65655f8ce39627a6bd28c8bdddd0c92db44d2e] into release-1.15;;;","20/Sep/22 15:52;dannycranmer;Thanks for the contribution [~xiaohei];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add inputSizes parameter for VectorAssembler,FLINK-29323,13481860,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,16/Sep/22 10:03,08/Nov/22 09:07,04/Jun/24 20:41,08/Nov/22 09:07,,,,,,,,ml-2.2.0,,,,,,,,0,pull-request-available,,,,Refine Transformer for VectorAssembler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-16 10:03:48.0,,,,,,,,,,"0|z18m4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose savepoint format on Web UI,FLINK-29322,13481852,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,morhidi,,16/Sep/22 08:56,24/Nov/22 01:01,04/Jun/24 20:41,23/Nov/22 16:46,,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,,"Savepoint format is not exposed on the Web UI, thus users should remember how they triggered it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 23 16:46:28 UTC 2022,,,,,,,,,,"0|z18m2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 13:12;mbalassi;It might make sense to backport this to 1.16, arguagly even 1.15.;;;","03/Nov/22 07:32;xtsong;Any updates for this ticket?;;;","08/Nov/22 07:01;Yu Chen;I have already implemented it, if no one has any objection, I can take this ticket.;;;","08/Nov/22 08:07;mbalassi;Go ahead [~Yu Chen]. If I recall correctly [~matyas] did not have the bandwidth to work on this.;;;","23/Nov/22 16:46;yunta;merged in master: 4ed5daa2f45f2a93a0cfe77f0041e98d7304e7f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release connector,FLINK-29321,13481849,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Sep/22 08:42,11/Nov/22 09:11,04/Jun/24 20:41,11/Nov/22 09:11,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,FLINK-29320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-16 08:42:00.0,,,,,,,,,,"0|z18m28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup release scripts,FLINK-29320,13481848,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Sep/22 08:41,27/Oct/23 07:50,04/Jun/24 20:41,11/Nov/22 09:11,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,,,,,,,,,,,,,,,,,FLINK-29321,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 09:11:49 UTC 2022,,,,,,,,,,"0|z18m20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 08:42;chesnay;Currently blocked on ML discussion regarding the branching/release model.;;;","11/Nov/22 09:11;chesnay;main: 7816c95c55bae831d214522acd7b3933c646f945;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Calcite version to 1.32,FLINK-29319,13481847,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,16/Sep/22 08:39,10/Jul/23 14:38,04/Jun/24 20:41,10/Jul/23 14:31,,,,,,,,1.18.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"{code}
This release fixes CVE-2022-39135, an XML External Entity (XEE) vulnerability that allows a SQL query to read the contents of files via the SQL functions EXISTS_NODE, EXTRACT_XML, XML_TRANSFORM or EXTRACT_VALUE.

Coming 1 month after 1.31.0 with 19 issues fixed by 17 contributors, this release also replaces the ESRI spatial engine with JTS and proj4j, adds 65 spatial SQL functions including ST_Centroid, ST_Covers and ST_GeomFromGeoJSON, adds the CHAR SQL function, and improves the return type of the ARRAY and MULTISET functions.{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20873,FLINK-21239,FLINK-28744,,FLINK-27998,FLINK-31362,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 14:31:43 UTC 2023,,,,,,,,,,"0|z18m1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 09:33;Sergey Nuyanzin;Just want to clarify that these functions mentioned in CVE {{EXISTS_NODE}}, {{EXTRACT_XML}}, {{XML_TRANSFORM}} or {{EXTRACT_VALUE}} are specific for Oracle/MySql dialect and neither them nor {{org.apache.calcite.runtime.XmlFunctions}} where they are defined and where the CVE was detected/fixed[1] not used in Flink. 

So this CVE should not impact Flink

[1] https://github.com/apache/calcite/pull/2892/files;;;","10/Jul/23 14:31;Sergey Nuyanzin;Merged to master [c942d0f61a044a327f5393f3752e04ee489d7d4a|https://github.com/apache/flink/commit/c942d0f61a044a327f5393f3752e04ee489d7d4a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for PolynomialExpansion,FLINK-29318,13481808,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,16/Sep/22 03:47,19/Apr/23 02:16,04/Jun/24 20:41,19/Apr/23 02:16,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,Add Transformer for PolynomialExpansion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 02:16:44 UTC 2023,,,,,,,,,,"0|z18lt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 02:16;lindong;Merged to apache/flink-ml master branch 4a6ecdee6cdc978e16333ac22b0f5b9083514049;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add WebSocket in Dispatcher to support olap query submission and push results in session cluster,FLINK-29317,13481790,13417633,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zjureel,zjureel,16/Sep/22 02:02,25/Jul/23 05:42,04/Jun/24 20:41,,1.14.5,1.15.3,,,,,,,,,,Runtime / Coordination,Runtime / REST,,,0,,,,,"Currently client submit olap query to flink session cluster via http rest api, and pull the results through interval polling. The sink task in TaskManager creates socket server for each query, when the JobManager receives the pull request from client, it requests query results from the socket server. The process is as follows
Job submission path:
client -> http rest -> JobManager -> Sink Socket Server
Result acquisition path:
client <- http rest <- JobManager <- Sink Socket Server

This leads to two problems
1. There will be some performance loss when submitting jobs through http rest, for example, temporary files will be created for each job
2. The client pulls the result data at a certain time interval, which is a fixed cost. The larger interval leads to increase query latency, the smaller interval will increase the pressure of Dispatcher.
3. Each sink task initializes a socket server, it will increase the query latency, on the other hand, it wastes resources.

For the Flink OLAP scenario, we propose to add websocket protocol in session cluster to support submitting jobs and returning results. The client creates and manage a connection with websocket server, submits olap query to session cluster. The TaskManagers create and manage connection to websocket server too, and sink task sends results to the server in stream. When the JobManager receives the results from sink task, it pushes the result data to the client through the connection between them.

We implemented this feature in the internal Flink version of ByteDance. On average, the latency of each query can be reduced by about 100ms, it's a big optimization for OLAP queries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 05:42:06 UTC 2023,,,,,,,,,,"0|z18lpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 08:48;dmvk;Some efforts in this direction make sense, but it is a rather significant change (introduction of a new external interface) that should go through a more careful design process and be agreed upon by the community. Can you please submit a FLIP with the proposal before proceeding?;;;","25/Jul/23 05:42;zjureel;Hi [~dmvk] Sorry for so late rely. Currently we would like to promote improvement about Flink OLAP in the community. Are you still interested in this area? We want to create the FLIP and initiate discussions in the community later, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replicate packaging tests,FLINK-29316,13481740,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Sep/22 16:18,16/Sep/22 08:41,04/Jun/24 20:41,16/Sep/22 08:41,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 08:41:28 UTC 2022,,,,,,,,,,"0|z18leo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 08:41;chesnay;main: 7c719e7239dd06d0b2395661e8c021bfb12c040b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFSTest#testBlobServerRecovery fails on CI,FLINK-29315,13481732,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,chesnay,chesnay,15/Sep/22 15:35,14/Aug/23 11:15,04/Jun/24 20:41,14/Aug/23 11:15,1.15.3,1.16.0,1.17.0,,,,,,,,,Connectors / FileSystem,Test Infrastructure,Tests,,0,pull-request-available,stale-critical,,,"The test started failing 2 days ago on different branches. I suspect something's wrong with the CI infrastructure.


{code:java}
Sep 15 09:11:22 [ERROR] Failures: 
Sep 15 09:11:22 [ERROR]   HDFSTest.testBlobServerRecovery Multiple Failures (2 failures)
Sep 15 09:11:22 	java.lang.AssertionError: Test failed Error while running command to get file permissions : java.io.IOException: Cannot run program ""ls"": error=1, Operation not permitted
Sep 15 09:11:22 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
Sep 15 09:11:22 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:913)
Sep 15 09:11:22 	at org.apache.hadoop.util.Shell.run(Shell.java:869)
Sep 15 09:11:22 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170)
Sep 15 09:11:22 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1264)
Sep 15 09:11:22 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1246)
Sep 15 09:11:22 	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1089)
Sep 15 09:11:22 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:697)
Sep 15 09:11:22 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:672)
Sep 15 09:11:22 	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:233)
Sep 15 09:11:22 	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
Sep 15 09:11:22 	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2580)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2622)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1501)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:851)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:485)
Sep 15 09:11:22 	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:444)
Sep 15 09:11:22 	at org.apache.flink.hdfstests.HDFSTest.createHDFS(HDFSTest.java:93)
Sep 15 09:11:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 15 09:11:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Sep 15 09:11:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Sep 15 09:11:22 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
Sep 15 09:11:22 	... 67 more
Sep 15 09:11:22 
Sep 15 09:11:22 	java.lang.NullPointerException: <no message>
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 11:15:03 UTC 2023,,,,,,,,,,"0|z18lcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 15:37;chesnay;master:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41045&view=results
1.16:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41050&view=results
1.15:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41064&view=results;;;","16/Sep/22 09:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41067&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=48106;;;","16/Sep/22 10:01;mapohl;Did you reach out to [~wangyang0918] or somebody else to check whether something changed with the machines?;;;","16/Sep/22 11:26;chesnay;[~mapohl] I have not done that.;;;","17/Sep/22 05:47;yunta;It seems no pipeline could pass since this Friday.;;;","19/Sep/22 01:57;xuyangzhong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40997&view=logs&j=ab8d7049-0920-5f52-eeed-e548522c2880&t=a7d66b08-71af-51c3-2721-4b90fa4f94e3;;;","19/Sep/22 07:27;dmvk;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41057&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","19/Sep/22 07:38;chesnay;Has anyone seen this fail yet in their own Azure pipeline?;;;","19/Sep/22 08:38;syhily;[~chesnay] I have met the same failure.;;;","19/Sep/22 08:42;chesnay;[~syhily] You got a link to the failed build?...;;;","19/Sep/22 09:44;hxb;[~chesnay] I trigger a test in my private Azure and it succeed. [https://dev.azure.com/hxbks2ks/FLINK-TEST/_build/results?buildId=2082&view=logs&j=10526588-c840-530a-aa3f-b4f0dd9eb9b1&t=56dd9850-e050-58a0-f8f3-7c2b1641df07] ;;;","19/Sep/22 10:51;mapohl;[~hxb] could you access the custom CI workers that are hosted in AliCloud? If not, you could reach out to [~wangyang0918] or [~jingge]. They could help with that, as far as I remember. ;;;","19/Sep/22 10:52;hxb;[~mapohl] Ok. I will reach out them for help.;;;","19/Sep/22 14:05;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41125&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","20/Sep/22 04:22;wangyang0918;[~mapohl] Sorry for the late response. I will help with [~hxb] to figure out the problem of the Alibaba CI machines.;;;","20/Sep/22 04:37;yunta;It seems only apache-flink CI pipeline could fail, and my pipeline could pass: [https://dev.azure.com/myasuka/flink/_build/results?buildId=435&view=results] ;;;","20/Sep/22 07:18;mapohl;FYI: In today's release call we decided to try to fix this before continuing with a rc1 for 1.16.0. But there is the option to disable this test and not blocking the rc1 creation on this issue considering that the test is actual passing on AzureCI machines.;;;","20/Sep/22 07:54;rmetzger;The failing test is executed inside a docker container. I don't understand why the ""ls"" command doesn't work on those machines. 
Those with access to the machines: Can you try running ""ls"" inside a docker container on those hosts?;;;","20/Sep/22 08:48;wangyang0918;[~rmetzger] I have verified that the ""ls"" command run normally in the docker container. Given that the test failed when starting miniDFSCluster, it is also strange that other tests in the same class could pass.;;;","20/Sep/22 11:17;chesnay;It's not just that the other tests are passing, it's _always_ {{testBlobServerRecovery}} that is failing.

Maybe one of the other cases is corrupting the local hdfs directory. While this is supposed to be cleaned up we never actually verify that this happens. As an experiment I'll open a PR that runs each test case in another directory.;;;","20/Sep/22 12:28;chesnay;I found an interesting an error in one of the PR builds:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41165&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=73454cf8-be1c-4c3d-8d8f-41e0823b4231

This error happens _before_ any test even ran.

{code:java}
##[error]One or more errors occurred. (One or more errors occurred. (Access to the path '/home/agent01/myagent/_work/3/s/flink-fs-tests/target/checkstyle-suppressions.xml' is denied.)) (One or more errors occurred. (Access to the path '/home/agent01/myagent/_work/3/s/flink-fs-tests/target/hdfs/hdfsTest/data/data2' is denied.)) (Access to the path '/home/agent01/myagent/_work/3/s/flink-fs-tests/target/checkstyle-suppressions.xml' is denied.) (Access to the path '/home/agent01/myagent/_work/3/s/flink-fs-tests/target/hdfs/hdfsTest/data/data2' is denied.)
{code}

oh boy this is spreading:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41170&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=aae40d7f-6696-4678-bc68-05238737e607;;;","20/Sep/22 12:37;chesnay;I have the feeling we should just restart all agents and wipe the working directories.;;;","22/Sep/22 08:44;wangyang0918;I have tried to update the CI image by creating a new {{ls}} command. Now it seems that the CI could pass.

 
{code:java}
root@9c9b711af88f:/# cat /bin/ls
#!/bin/bash

/bin/ls_bak $*

exitcode=$?

[ -d /__w/_temp ] && echo ""[$(date)][exitcode:$exitcode] $*"" >>/__w/_temp/log.ls

exit $exitcode {code}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41246&view=logs&j=ab8d7049-0920-5f52-eeed-e548522c2880&t=a7d66b08-71af-51c3-2721-4b90fa4f94e3;;;","22/Sep/22 08:50;chesnay;Why wouldn't we just fix the machines?...;;;","22/Sep/22 08:52;wangyang0918;[~chesnay] Your failed pipelines are caused by me because I run the test manually in one docker container with root user on AlibabaCI006. And I have already cleaned up all the generated files with root permission. Sorry about that.;;;","22/Sep/22 08:53;wangyang0918;[~chesnay] I am not sure how to fix the machines.;;;","23/Sep/22 02:24;hxbks2ks;I think it may be very difficult to find the root cause of this problem. +1 for solving this problem by this way temporarily considering we don’t have the passed CI for a week.;;;","23/Sep/22 03:18;renqs;Another two instances: 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41259&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=46735]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41258&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=46342];;;","23/Sep/22 05:57;wangyang0918;[~chesnay] Do you have any other suggestions? I admit that replacing the build-in {{ls}} command is a temporary hack, but I do not find any other solutions until now.;;;","23/Sep/22 07:38;chesnay;I'm -1 on adding some bespoke {{ls}} implementation to the system; god knows whether it hides something else down the line.

Let's disable the test and I'll try a few more things.;;;","23/Sep/22 12:43;chesnay;Experiment results:
- #testBlobServerRecovery fails when using a TempDir
- #testBlobServerRecovery fails with each test case using a different tmp directory
- #testHDFS fails when only running #testHDFS, #testChangingFileNames and #testBlobServerRecovery
- test failed when only running the tests in flink-fs-tests
- test failed when only running full HDFSTest
- test succeeds when only running #testHDFS, #testChangingFileNames and #testBlobServerRecovery and no other test
- test succeeds when only running #testBlobServerRecovery

Will update this as more experiments conclude.;;;","25/Sep/22 12:45;wangyang0918;After more debugging with strace, I found that it might be related with kernel version. Then I upgraded the kernel version for all the CI machines from {{3.10.0-1160.62.1.el7.x86_64}} to {{3.10.0-1160.76.1.el7.x86_64}}. It seems that the CI could pass now.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41323&view=results;;;","26/Sep/22 03:22;hxbks2ks;After upgrading the kernel version, this test has been successfully passed in the latest two days of CI. Thanks to [~wangyang0918], [~chesnay] and [~mapohl] for their help in solving this tricky problem.;;;","26/Sep/22 06:48;mapohl;Great. Thanks for solving it. [~wangyang0918] I'm wondering what triggered that behavior? Did an automatic update of the kernel to {{3.10.0-1160.62.1.el7.x86_64}} happen recently?;;;","26/Sep/22 09:25;wangyang0918;[~mapohl] No. The kernel version {{3.10.0-1160.62.1.el7.x86_64}} is updated in Apr. I do not have any idea why this issue occurred recently too.
{code:java}
[root@iZgw814dze6vb5iq9eabnwZ ~]# rpm -qa --last | grep 'kernel-3.10.0-1160.62.1.el7.x86_64'
kernel-3.10.0-1160.62.1.el7.x86_64            Wed Apr 20 17:09:59 2022
{code};;;","25/Oct/22 06:26;yunta;It looks like the problem happened again https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42357&view=logs&j=ab8d7049-0920-5f52-eeed-e548522c2880&t=a7d66b08-71af-51c3-2721-4b90fa4f94e3 ;;;","26/Oct/22 12:29;mapohl;yes, same with that build: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42453&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=46369;;;","26/Oct/22 12:41;mapohl;I'm reopening the issue because we see multiple builds failing due to this error again:
* {{HDFSTest}}: [build #42453|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42453&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=46415], [build #42425|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42425&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=45793], [build #42422|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42422&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=46190]
* {{HadoopViewFileSystemTruncateTest}}: [build #42428|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42428&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11182], [build #42444|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42444&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=11182]

All failed pipelines ran on {{Alibaba001}};;;","26/Oct/22 12:46;mapohl;[~hxbks2ks] [~wangyang0918] could you investigate the machine?;;;","26/Oct/22 12:46;martijnvisser;Can we do another kernel update? :D ;;;","26/Oct/22 12:56;mapohl;I'm wondering whether we could lower the priority of that one to CRITICAL considering that we're only experiencing it on {{Alibaba001}}. There is, for instance, a succeeded build on {{release-1.16}} that happened in between: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42424&view=results (the {{connect_1}} job was executed on {{Alibaba004}}).;;;","27/Oct/22 02:43;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42455&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=46086;;;","27/Oct/22 05:17;wangyang0918;I will have a look on the CI machine Alibaba001 today.;;;","27/Oct/22 09:00;wangyang0918;I have upgrade the kernel version of Alibaba001 to {{{}5.4.220-1.el7.elrepo.x86_64{}}}. Let's keep an eye on the CI pipelines.;;;","27/Oct/22 12:33;mapohl;thanks [~wangyang0918] . Did you compare the kernel versions of Alibaba001 with the other Alibaba00* machines? I would have thought that all the machines had a matching kernel version from the last update. But then it would be odd that only one would be affected. My concern is that it's actually something else that's causing that issue.;;;","27/Oct/22 13:34;wangyang0918;Yes. All the CI machines have the same kernel version after last upgrade. It is strange that only the Alibaba001 is affected. And I have no idea what is the root cause for this issue.

 

BTW, simply restarting the virtual machine does not help.;;;","01/Nov/22 14:55;mapohl;It appears that the issue disappeared again. I leave the ticket open, though since we still cannot come up with a reasonable explanation. I added 1.17.0 as an affected version.;;;","02/Nov/22 13:13;fpaul;What is the status of this ticket? It is marked as a blocker, but if no one is actively working on it, I will deprioritize it. For example, I do not think it should block the 1.15.3 release.;;;","02/Nov/22 14:36;mapohl;[~fpaul] we didn't see the issue reappearing since the upgrade that [~wangyang0918] performed 5 days ago on {{Alibaba001}}. I went through previous builds and found [that one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42698&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5] where {{connect_1}} was performed on {{Alibaba001}} and succeeded. Therefore, I expect the issue to be resolved temporarily.

I hesitate to lower the priority because we still don't understand the problem and found a proper solution.

;;;","07/Nov/22 05:08;wangyang0918;Maybe I could upgrade the kernel version of all the CI machines to 5.4.220-1.el7.elrepo.x86_64. The current kernel-3.10.0 is too old.;;;","09/Nov/22 15:47;mapohl;hm, I don't know. It might be worth it to wait for the situation to reappear before taking any further action. But upgrading might be good from a security standpoint? 
I did a bit of googling on that topic and found that page summarizing an issue that sounds the same (even though the blog article is about podman): https://www.redhat.com/sysadmin/container-permission-denied-errors

Initially, I thought that we could try that out if we run into the issue again. But it appears that we're already using the {{--priviledged}} flag in our Docker test environment (see [azure-pipelines.yml:44|https://github.com/apache/flink/blob/3b50f19ad27a49c5b804e8e811cbb2062dcff003/azure-pipelines.yml#L44].;;;","12/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 11:15;mapohl;It looks like updating the kernel resolved the issue. We haven't observed anything for 9 month. I'm gonna close this issue as not reproducible.;;;",,,,,,,,,,,,,,,,,,,,,,,
AskTimeoutException for more than two partition with FileSource API on S3,FLINK-29314,13481729,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mlgruby,mlgruby,15/Sep/22 15:25,16/Sep/22 07:31,04/Jun/24 20:41,,1.13.5,,,,,,,,,,,FileSystems,,,,0,,,,,"While using *FileSource.forRecordStreamFormat* api with a build option of *monitorContinuously(Duration.ofMillis(10000))* for avro files on S3 we are getting
{code:java}
akka.pattern.AskTimeoutException: Ask timed out on Actor[akka://flink/user/rpc/dispatcher_2#-1751288098] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply. {code}
error when we have more than 2 partition in a s3 bucket. 
More context. 

For a given S3 uri: *s3://-export-staging/data-export/dataexport.prod-10.S3.integration.dd33/event_type=users.messages.email.Send/*
There are two partition of avro files based on date. 
{code:java}
date=2022-09-12-10/
date=2022-09-12-11/ {code}
Reading of avro file perfectly fine. While the job is running when another partition is added to the uri it works perfectly fine then as well. Now when I stop the job and re-run the job it fails with below exception: 
{code:java}
Exception in thread ""main"" org.apache.flink.util.FlinkException: Failed to execute job 'data-export'.
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1970)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1848)
    at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:69)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1834)
    at org.apache.flink.streaming.api.scala.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.scala:801)
    at com.lightricks.sigma.topologies.EntryPoint$.delayedEndpoint$com$lightricks$sigma$topologies$EntryPoint$1(EntryPoint.scala:75)
    at com.lightricks.sigma.topologies.EntryPoint$delayedInit$body.apply(EntryPoint.scala:31)
    at scala.Function0.apply$mcV$sp(Function0.scala:39)
    at scala.Function0.apply$mcV$sp$(Function0.scala:39)
    at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
    at scala.App.$anonfun$main$1$adapted(App.scala:80)
    at scala.collection.immutable.List.foreach(List.scala:392)
    at scala.App.main(App.scala:80)
    at scala.App.main$(App.scala:78)
    at com.lightricks.sigma.topologies.EntryPoint$.main(EntryPoint.scala:31)
    at com.lightricks.sigma.topologies.EntryPoint.main(EntryPoint.scala)
Caused by: java.lang.RuntimeException: Error while waiting for job to be initialized
    at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:160)
    at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$submitJob$2(PerJobMiniClusterFactory.java:83)
    at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73)
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
    at java.util.concurrent.CompletableFuture$Completion.exec(CompletableFuture.java:457)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.requestJobStatus(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
    at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$null$0(PerJobMiniClusterFactory.java:89)
    at org.apache.flink.client.ClientUtils.waitUntilJobInitializationFinished(ClientUtils.java:144)
    ... 9 more
Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.requestJobStatus(org.apache.flink.api.common.JobID,org.apache.flink.api.common.time.Time) timed out.
    at com.sun.proxy.$Proxy13.requestJobStatus(Unknown Source)
    at org.apache.flink.runtime.minicluster.MiniCluster.lambda$getJobStatus$6(MiniCluster.java:704)
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
    at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
    at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
    at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
    at org.apache.flink.runtime.minicluster.MiniCluster.getJobStatus(MiniCluster.java:703)
    at org.apache.flink.client.program.PerJobMiniClusterFactory.lambda$null$0(PerJobMiniClusterFactory.java:86)
    ... 10 more
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-1751288098]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
    at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:635)
    at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:650)
    at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:874)
    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113)
    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:872)
    at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
    at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:279)
    at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:283)
    at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:235)
    at java.lang.Thread.run(Thread.java:748) {code}
The weird part is that when partition/directory is increased from 2 to 3 it fails with akka ask timeout.","The code I am using is: 


{code:java}
object AvroStreamFormat extends SimpleStreamFormat[Send] {
  override def createReader(config: Configuration, stream: FSDataInputStream): StreamFormat.Reader[Send] = {
    val schema = AvroSchema[Send]
    val reader: AvroInputStream[Send] = AvroInputStream.data[Send].from(stream).build(schema)
    new StreamFormat.Reader[Send] {
      override def read(): Send = {
        if (reader.iterator.hasNext) {
          reader.iterator.next()
        } else {
          null
        }
      }

      override def close(): Unit = reader.close()
    }
  }

  override def getProducedType: TypeInformation[Send] = TypeInformation.of(classOf[Send])
}


val source = FileSource.forRecordStreamFormat(
  AvroStreamFormat,
  new Path(s3Path))
  .monitorContinuously(Duration.ofMillis(10000))
  .build()

val recordStream: DataStream[Send] = env.fromSource(
  source,
  WatermarkStrategy.noWatermarks(),
  ""currents-email-send""
){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-15 15:25:34.0,,,,,,,,,,"0|z18lc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some config overrides are ignored when set under spec.flinkConfiguration,FLINK-29313,13481685,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,morhidi,,15/Sep/22 11:13,24/Nov/22 01:03,04/Jun/24 20:41,21/Sep/22 10:12,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Some [configs|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#resourceuser-configuration] that can be specified under spec.flinkConfiguration won't take affect without an upgrade, e.g.:
 * {{kubernetes.operator.periodic.savepoint.interval}}
 * {{kubernetes.operator.savepoint.format.type}}

These properties are used mainly from the so called 'observeConfig', and won't be available in the operator until the job is restarted. Ideally these should be changed without an upgrade, but at the moment they won't take affect at all.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 10:12:10 UTC 2022,,,,,,,,,,"0|z18l2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 11:15;morhidi;cc [~gyfora] ;;;","16/Sep/22 09:12;xiaohei;[~matyas]  - Just want to understand more about this issue, is there any configs under spec.flinkConfiguration can work as expected? Thanks;;;","16/Sep/22 11:20;morhidi;{{This behaviour affects only the properties prefixed with kubernetes.operator and only o the main branch.}};;;","16/Sep/22 11:27;morhidi;The short term fix could be to do an upgrade when these properties are changed in the spec.flinkConfiguration. (This was the original behaviour in 1.1.0.) I'll try to come up with a good solution before going down this path.;;;","21/Sep/22 10:12;gyfora;merged to main 5e3e9962906c605ea55d4caf2fef2ee5ba400019;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance problems encountered when using StatementSetImpl#addInsertSql in Flink SQL,FLINK-29312,13481670,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,m_jelly,m_jelly,15/Sep/22 09:49,15/Sep/22 09:49,04/Jun/24 20:41,,1.13.5,1.14.2,,,,,,,,,,Table SQL / API,,,,0,,,,,"It is necessary to use insert in batches in a flink sql task. Through testing, it is found that when two insert statements are inserted using StatementSetImpl#addInsertSql, the resource usage is good. When there are more than two, using StatementSetImpl#addInsertSql will consume more resources.


The above resource usage statistics are based on the same environment, using StatementSetImpl#addInsertSql to compare the running situation when it is divided into multiple tasks according to the number of inserts. When there are more than two insert statements, the latter will use less resource.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,2022-09-15 09:49:47.0,,,,,,,,,,"0|z18kz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to use http for Kinesis consumer when testing locally,FLINK-29311,13481657,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pocockn,pocockn,15/Sep/22 09:06,11/Nov/22 08:36,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,Connectors / Kinesis,Stateful Functions,,,0,,,,,"I am attempting to run the StateFun docker image locally in a docker-compose stack. I have Kinesis running using Localstack and want to use it as a consumer. 

The documentation says to use a custom endpoint you can use the following config with the _io.statefun.kinesis.v1/ingress_ module:
{code:java}
awsCredentials:
  type: custom-endpoint
  endpoint: https://localhost:4567
  id: us-west-1{code}
The above errors as the custom endpoint property is on the AWS region instead so it needs to be:
{code:java}
awsRegion:
  id: us-east-1
  type: custom-endpoint
  endpoint: http://localstack:4566{code}
However, I am unable to use HTTP as it complains that:
{noformat}
Caused by: java.lang.IllegalArgumentException: Invalid service endpoint url: http://localstack:4566; Only custom service endpoints using HTTPS are supported{noformat}
I tried using HTTPS but it errors with a certificate issue, I think this is an issue with Localstack.

Some of the docs for the Kinesis connectors show HTTP being used with a custom endpoint

[https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/kinesis/#using-custom-kinesis-endpoints]

It would be nice if we can use HTTP when testing locally

I see from this issue [https://github.com/localstack/localstack/issues/893]  there is a configuration option TRUST_ALL_CERTIFICATES that can be provided to the NettyNioAsyncHttpClient that allows the above to work for local dev.","Here is my Kinesis module code

 

kind: io.statefun.kinesis.v1/ingress
spec:
  id: com.sentiment
  awsRegion:
    id: us-east-1
    type: custom-endpoint
    customEndpoint: [https://localstack:4566|https://localstack:4566/]
  awsCredentials:
    type: basic
    id: us-east-1
    accessKeyId: key
    secretAccessKey: secret
  startupPosition:
    type: latest
  streams:
    - stream: customer-details
      valueType: SentimentEvent
      targets:
        - sentiment
  clientConfigProperties:
    - SocketTimeout: 9999
    - MaxConnections: 15

And I am using  the latest apache/statefun Docker image within docker compose",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-15 09:06:12.0,,,,,,,,,,"0|z18kw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup license checks,FLINK-29310,13481652,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,15/Sep/22 08:53,19/Oct/22 06:42,04/Jun/24 20:41,15/Sep/22 14:53,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 14:53:25 UTC 2022,,,,,,,,,,"0|z18kv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 14:53;chesnay;main: 1fbdeacb1a31f33a6e532562339d3967e7d0cf4b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Relax allow-client-job-configurations for Table API and parameters,FLINK-29309,13481645,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,15/Sep/22 08:06,23/Sep/22 14:55,04/Jun/24 20:41,23/Sep/22 14:55,,,,,,,,1.16.0,,,,API / DataStream,,,,0,pull-request-available,,,,"Currently, the {{execution.allow-client-job-configurations}} is a bit too strict. Due to the complexity of the configuration stack, it makes it impossible to use Table API and also prevents very common parameters like {{pipeline.global-job-parameters}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29379,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 14:55:22 UTC 2022,,,,,,,,,,"0|z18ktk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 14:55;twalthr;Fixed in master: 298b88842025da9ae218b598c1213f7238c0df7c
Fixed in 1.16: c917a6e8a09d42dc87ba27afdfd42a2a640bb984;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoResourceAvailableException fails the batch job,FLINK-29308,13481622,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,aitozi,aitozi,15/Sep/22 05:21,09/Oct/22 06:02,04/Jun/24 20:41,09/Oct/22 06:02,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"When running batch job configured with the following restart strategy
{code:java}
restart-strategy: fixed-delay
restart-strategy.fixed-delay.delay: 15 s
restart-strategy.fixed-delay.attempts: 10 {code}
If the cluster resource is not enough to run the single stage, it can run partial of the stage, but it still will fail after the 10 times {{{}NoResourceAvailableException{}}}. IMO, for batch job the {{NoResourceAvailableException}} do not necessary to trigger the job to fail. Or at least this failure reason is not suitable to share the same restart strategy with other failure reasons",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 30 06:51:12 UTC 2022,,,,,,,,,,"0|z18kog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 05:24;aitozi;cc [~xtsong] , [~wangyang0918] What do you think ?;;;","29/Sep/22 09:26;zhuzh;[~aitozi] would you share more details of the failed job?
 - Flink version
 - Error stack
 - JM logs

By design, batch jobs will not fail with NoResourceAvailableException if there is at least one available slot. NoResourceAvailableException will happen only if the slot requests are not fulfillable(not required to be all fulfilled).;;;","29/Sep/22 11:23;aitozi;Hi [~zhuzh] . I encountered this problem when using the flink-1.15.

The error stack:
!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/202086/1663217971327-2585db3e-a106-457f-9b3f-2f02f1b092ea.png?x-oss-process=image%2Fresize%2Cw_1500%2Climit_0!

 

BTW, we use the fine grained resource model. So it may not match the resource type to spawn the new worker for the next stage although there is already a running slot.;;;","30/Sep/22 03:23;zhuzh;That may be the cause.
If using fine-grained resources, NoResourceAvailableException could happen if Flink cannot find a {{matching}} slot for scheduled vertices (in coarse-grained resources case, a slot can always match any slot request).;;;","30/Sep/22 03:35;aitozi;Yes, but actually the job can keep to run when other slot finished and release the resource to the cluster. So I think the {{NoResourceAvailableException}} do not necessary have to fail the job in this case.;;;","30/Sep/22 03:37;aitozi;In our use case, we just ignore the {{NoResourceAvailableException}} for the batch job to handle this case. I'm not sure whether it is a good way to go.;;;","30/Sep/22 04:09;zhuzh;It's possible that the released resource cannot fulfill the new slot request. Therefore Flink use this mechanism to fail fast, otherwise users may find a job pending for quite sometime with no progress. This may be helpful when the resource cluster is problematic or there is mis-configuration in the job(wrong resource spec, wrong resource queue, etc).
One potential problem to ignore the NoResourceAvailableException is that a job may wait indefinitely until it can obtain a required slot. You can do it if it's acceptable in your case.;;;","30/Sep/22 06:51;aitozi;Yes, your concern is right.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix timeInQueue in CheckpointRequestDecider,FLINK-29307,13481606,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chenzihao,chenzihao,chenzihao,15/Sep/22 02:44,20/Oct/22 08:49,04/Jun/24 20:41,20/Oct/22 08:49,1.17.0,,,,,,,1.17.0,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"In org.apache.flink.runtime.checkpoint.CheckpointRequestDecider#logInQueueTime, request.timestamp is always less than currentTimeMillis(), this time is not the request-time-in-queue. I think it should be currentTimeMillis() - request.timestamp. Hi, [~roman], please help to confirm this, thanks a lot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 08:49:29 UTC 2022,,,,,,,,,,"0|z18kkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 10:32;Yanfei Lei;I think you are right, the current implementation is request.timestamp - currentTimeMillis(), which makes the LOG.info never be touched. ;;;","29/Sep/22 07:17;chenzihao;[~Yanfei Lei] Thanks for your response. I will submit a PR to fix this soon.;;;","20/Oct/22 08:49;ym;merged commit [{{2851078}}|https://github.com/apache/flink/commit/2851078d61ef3c04355d5c41a7becb086405e7c8] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fail to check multiple flink-dist*.jar for config.sh,FLINK-29306,13481542,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,LiuZeshan,LiuZeshan,LiuZeshan,14/Sep/22 16:48,27/Oct/22 07:26,04/Jun/24 20:41,27/Oct/22 07:26,1.16.0,,,,,,,1.17.0,,,,Client / Job Submission,,,,0,pull-request-available,,,,"The following shell command always make FLINK_DIST_COUNT=1 ([config.sh|https://github.com/apache/flink/blob/db98322472cb65ca0358ec1cce7f9ef737198189/flink-dist/src/main/flink-bin/bin/config.sh#L35])
{code:java}
FLINK_DIST_COUNT=""$(echo ""$FLINK_DIST"" | wc -l)"" {code}
and the following condition will always be false, so fail to check multiple flink-dist*.jar。
{code:java}
[[ ""$FLINK_DIST_COUNT"" -gt 1 ]] {code}
examples:
{code:java}
# FLINK_DIST="":/Users/lzs/.data/github/flink/flink-dist/target/flink-1.17-SNAPSHOT-bin/flink-1.17-SNAPSHOT/lib/flink-dist-1.17-SNAPSHOT.jar:/Users/lzs/.data/github/flink/flink-dist/target/flink-1.17-SNAPSHOT-bin/flink-1.17-SNAPSHOT/lib/flink-dist-1.17-xxx.jar""
# echo ""$FLINK_DIST"" | wc -l
1{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:26:11 UTC 2022,,,,,,,,,,"0|z18k6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 17:25;LiuZeshan;[~rudi.kershaw] would you please to have a look ?;;;","15/Sep/22 14:22;rudi.kershaw;[~LiuZeshan], this is a good spot. I've obviously not tested this adequately when I made the change. 

I've checked over the change and added my approval to the pull request.;;;","27/Oct/22 07:26;xtsong;master (1.17): c0e3500722a6b8491748f57daf35ef9d79a4178d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertTestSinkWriter fails if parent of output file does not exist,FLINK-29305,13481539,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Sep/22 16:41,15/Sep/22 07:54,04/Jun/24 20:41,15/Sep/22 07:54,1.16.0,,,,,,,1.17.0,,,,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 07:54:18 UTC 2022,,,,,,,,,,"0|z18k60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 07:54;chesnay;master. 33afc3c8924861025094ae94291805edad7afcd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set timeout on CI,FLINK-29304,13481527,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Sep/22 15:21,15/Sep/22 08:25,04/Jun/24 20:41,15/Sep/22 08:25,,,,,,,,elasticsearch-3.0.0,,,,Build System / CI,Connectors / ElasticSearch,,,0,pull-request-available,,,,With the repo using apache resources we should ensure tests that are stuck arent consuming unnecessary resources.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 08:25:23 UTC 2022,,,,,,,,,,"0|z18k3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 08:25;chesnay;main: 7ccd8c2b372baa23169d2d56dc404d6ccf51e802;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DockerImageVersions,FLINK-29303,13481497,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Sep/22 12:59,14/Sep/22 21:09,04/Jun/24 20:41,14/Sep/22 21:09,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 21:09:16 UTC 2022,,,,,,,,,,"0|z18jwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 21:09;chesnay;main: 861b6fc53079a407714a5815cdc45e083226393e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support DNS policy and configuration in helm chart,FLINK-29302,13481479,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,joaoubaldo,joaoubaldo,joaoubaldo,14/Sep/22 10:39,17/Sep/22 17:15,04/Jun/24 20:41,17/Sep/22 17:15,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,Add support for setting `dnsPolicy` and `dnsConfig` on the Flink Kubernetes operator helm chart.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 17 17:15:28 UTC 2022,,,,,,,,,,"0|z18jso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Sep/22 17:15;mbalassi;[7047b96|https://github.com/apache/flink-kubernetes-operator/commit/7047b9682ef087addc99ee9ebef3ae01382e599d] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish java-ci-tools,FLINK-29301,13481476,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,14/Sep/22 10:30,05/Oct/22 11:58,04/Jun/24 20:41,14/Sep/22 21:08,,,,,,,,1.16.0,,,,Build System / CI,,,,0,pull-request-available,,,,"The java-ci-tools are borderline mandatory for external connector repositories to check the licensing.
Let's rename the module to flink-ci-tools and publish it.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29508,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 21:08:15 UTC 2022,,,,,,,,,,"0|z18js0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 21:08;chesnay;master: 
2d6fa8876a66679c21743c506d7b8f3c5692f7bc..bb23cf7b2f69fef0264e02b6f5df8af15800f16f
1.16:
e9e072c41ad340ec0bffd54d7eb8a8d190263faa..227dad0e449fb6e4fb882ae64d79335f95a72f3d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade python version to 3.10.3,FLINK-29300,13481469,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bilna123,bilna123,14/Sep/22 10:00,28/Sep/22 13:02,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,,,,,"Security scans listed lot of CVEs from Python 3.8. So please upgrade python version to 3.10.3
|Component Name|Component Version Name|Vulnerability Name(s)|Base Score - cvss3|Severity - cvss3|Fix version|
|Python programming language|3.7.3|CVE-2015-20107 (BDSA-2015-0814) |9.8|CRITICAL |3.10.3|
|Python programming language|3.7.3|CVE-2022-26488 (BDSA-2022-0627) |7|HIGH |3.10.3|
|Python programming language|3.7.3|CVE-2021-3737 (BDSA-2021-3183) |7.5|HIGH|3.10.3|
|Python programming language|3.7.3|CVE-2022-0391 (BDSA-2021-4119) |7.5|HIGH|3.10.3|
|Python programming language|3.7.3|CVE-2021-3177 (BDSA-2021-0085) |9.8|CRITICAL|3.10.3|
|Python programming language|3.7.3|CVE-2020-27619 (BDSA-2020-2928) |9.8|CRITICAL |3.10.3|
|Python programming language|3.7.3|CVE-2020-26116 (BDSA-2020-2544) |7.2|HIGH |3.10.3|
|Python programming language|3.7.3|CVE-2019-20907 (BDSA-2019-4493) |7.5|HIGH|3.10.3|
|Python programming language|3.7.3|CVE-2020-15523 (BDSA-2020-1616) |7.8|HIGH|3.10.3|
|Python programming language|3.7.3|CVE-2019-16056 (BDSA-2019-2926) |7.5|HIGH |3.10.3|
|Python programming language|3.7.3|CVE-2019-10160 (BDSA-2019-1748) |9.8|CRITICAL|3.10.3|
|Python programming language|3.7.3|CVE-2019-9948 (BDSA-2019-0819) |9.1|CRITICAL|3.10.3|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 13:02:52 UTC 2022,,,,,,,,,,"0|z18jqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 13:02;zxcoccer;hi，I would like to deal with it. Can you assign this ticket to me ?:)
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the network memory size calculation issue in fine-grained resource mode,FLINK-29299,13481423,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,14/Sep/22 03:21,20/Sep/22 06:31,04/Jun/24 20:41,20/Sep/22 06:31,1.16.0,,,,,,,1.16.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"After FLINK-28663, one intermediate dataset can be consumed by multiple consumers, there is a case where one vertex can consume one intermediate dataset multiple times. However, currently in fine-grained resource mode, when computing the required network buffer size, the intermediate dataset is used as key to record the size of network buffer per input gate, which means it may allocate less network buffers than needed if two input gate of the same vertex consumes the same intermediate dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 06:31:02 UTC 2022,,,,,,,,,,"0|z18jgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 06:31;kevin.cyj;Fix via 

d3513d98953b0922e3dc753ef90806ed4e264926 on master

7367e358ccf34fb1e9ea2cea9d5b1f630b00e10c on 1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalBufferPool request buffer from NetworkBufferPool hanging,FLINK-29298,13481422,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,14/Sep/22 03:15,18/Mar/23 03:27,04/Jun/24 20:41,13/Dec/22 04:30,1.16.0,,,,,,,1.16.1,1.17.0,,,Runtime / Network,,,,0,pull-request-available,,,,"In the scenario where the buffer contention is fierce, sometimes the task hang can be observed. Through the thread dump information, we can found that the task thread is blocked by requestMemorySegmentBlocking forever. After investigating the dumped heap information, I found that the NetworkBufferPool actually has many buffers, but the LocalBufferPool is still unavailable and no buffer has been obtained.

By looking at the code, I am sure that this is a bug in thread race: when the task thread polled out the last buffer in LocalBufferPool and triggered the onGlobalPoolAvailable callback itself, it will skip this notification  (as currently the LocalBufferPool is available), which will cause the BufferPool to eventually become unavailable and will never register a callback to the NetworkBufferPool.

The conditions for triggering the problem are relatively strict, but I have found a stable way to reproduce it, I will try to fix and verify this problem.

!image-2022-09-14-10-52-15-259.png|width=1021,height=219!

!image-2022-09-14-10-58-45-987.png|width=997,height=315!

!image-2022-09-14-11-00-47-309.png|width=453,height=121!",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29923,FLINK-29419,,,,,,,,,,,,"14/Sep/22 02:52;Weijie Guo;image-2022-09-14-10-52-15-259.png;https://issues.apache.org/jira/secure/attachment/13049258/image-2022-09-14-10-52-15-259.png","14/Sep/22 02:58;Weijie Guo;image-2022-09-14-10-58-45-987.png;https://issues.apache.org/jira/secure/attachment/13049257/image-2022-09-14-10-58-45-987.png","14/Sep/22 03:00;Weijie Guo;image-2022-09-14-11-00-47-309.png;https://issues.apache.org/jira/secure/attachment/13049256/image-2022-09-14-11-00-47-309.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 18 03:27:32 UTC 2023,,,,,,,,,,"0|z18jg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/22 07:34;kevin.cyj;[~Weijie Guo] Thanks for reporting this.;;;","15/Nov/22 03:03;AlexXXX;I tested the new github commit, but it still exists the same problem. If I change the first for() to 1024 times, this bug will be trigged everytimes.;;;","15/Nov/22 03:32;Weijie Guo;[~AlexXXX] This pr has not been reviewed, so it may not be the final solution. In addition, I'm a little suspicious that this problem may not be the only one in the LocalBufferPool, I need to confirm after the problem is fixed. As for the change of the first for() statement to 1024, this is expected, because there are only 1024 buffers in the networkBufferPool in the test class, and one buffer will be taken from the @Before method, so the maximum number of buffers can be requested in line 259 is 1023.;;;","13/Dec/22 04:30;xtsong;- master (1.17): 875f27ef38af20d2548d225c0583ebffe0f700fa
- release-1.16: 40fbea33c35bf9d04bff35f01554bb91c874b975;;;","18/Mar/23 01:00;lichen1109;We  alse faced to this problem, how to reproduce this problem with a stable way, Thank you sir;;;","18/Mar/23 03:27;Weijie Guo;Hi [~lichen1109]. Unfortunately, this bug is only possible to reproduce in the case of strong buffer competition. However, I wrote a unit test for this PR, which can reproduce the problem with a high probability. 
In addition, there is another bug (FLINK-31293) that can also cause a similar phenomenon. Whether your job is a batch job or a stream job, there should be no similar problems with batch jobs in the latest release-1.17 and master branches.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Group Table Store file writers into SingleFileWriter and RollingFileWriter,FLINK-29297,13481417,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Sep/22 02:40,10/Oct/22 02:11,04/Jun/24 20:41,21/Sep/22 06:07,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently we have two types of files to write:
* Data files (LSM tree files), where a level 0 data file is a single file and a level >= 1 data file is a set of rolling files. Statistics for these files are needed for pruning when scanning.
* Extra files (changelog files), just a list of records. No statistics are needed.

However, current writers are all based on {{MetricFileWriter}}, which always produces statistics.

We'd like to refactor the writers and group them into {{SingleFileWriter}} and {{RollingFileWriter}}. {{StatsCollectingSingleFileWriter}} should be a subclass of {{SingleFileWriter}} which additionally produces statistics, and data file writers should be a subclass of {{StatsCollectingSingleFileWriter}} or {{RollingFileWriter}} based on their level. For extra file writers, extending from {{SingleFileWriter}} is enough.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-14 02:40:23.0,,,,,,,,,,"0|z18jf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorCoordinatorHolder.create throws NPE,FLINK-29296,13481416,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxb,hxb,14/Sep/22 02:39,20/Oct/22 07:50,04/Jun/24 20:41,20/Oct/22 07:50,1.16.0,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,"{code:java}
2022-09-13T15:22:42.3864318Z Sep 13 15:22:42 [ERROR] Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.633 s <<< FAILURE! - in org.apache.flink.test.streaming.runtime.SourceNAryInputChainingITCase
2022-09-13T15:22:42.3865377Z Sep 13 15:22:42 [ERROR] org.apache.flink.test.streaming.runtime.SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution  Time elapsed: 0.165 s  <<< ERROR!
2022-09-13T15:22:42.3867571Z Sep 13 15:22:42 java.lang.RuntimeException: Failed to fetch next result
2022-09-13T15:22:42.3919112Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-09-13T15:22:42.3920935Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-09-13T15:22:42.3922442Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.datastream.DataStreamUtils.collectBoundedStream(DataStreamUtils.java:106)
2022-09-13T15:22:42.3924085Z Sep 13 15:22:42 	at org.apache.flink.test.streaming.runtime.SourceNAryInputChainingITCase.testDirectSourcesOnlyExecution(SourceNAryInputChainingITCase.java:89)
2022-09-13T15:22:42.3925493Z Sep 13 15:22:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T15:22:42.3926635Z Sep 13 15:22:42 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T15:22:42.3928378Z Sep 13 15:22:42 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T15:22:42.3964273Z Sep 13 15:22:42 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T15:22:42.3965054Z Sep 13 15:22:42 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-13T15:22:42.3965788Z Sep 13 15:22:42 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-13T15:22:42.3966508Z Sep 13 15:22:42 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-13T15:22:42.3967476Z Sep 13 15:22:42 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-13T15:22:42.3968432Z Sep 13 15:22:42 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-09-13T15:22:42.3969233Z Sep 13 15:22:42 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-09-13T15:22:42.3969871Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-13T15:22:42.3970534Z Sep 13 15:22:42 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-13T15:22:42.3971453Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-13T15:22:42.3972453Z Sep 13 15:22:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-13T15:22:42.3973193Z Sep 13 15:22:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-13T15:22:42.3973857Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-13T15:22:42.3974634Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-13T15:22:42.3975420Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-13T15:22:42.3976060Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-13T15:22:42.3976689Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-13T15:22:42.3977555Z Sep 13 15:22:42 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-13T15:22:42.3978248Z Sep 13 15:22:42 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-13T15:22:42.3978856Z Sep 13 15:22:42 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-09-13T15:22:42.3979696Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-13T15:22:42.3980716Z Sep 13 15:22:42 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-13T15:22:42.3981785Z Sep 13 15:22:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-13T15:22:42.3982352Z Sep 13 15:22:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-13T15:22:42.3982989Z Sep 13 15:22:42 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-13T15:22:42.3983913Z Sep 13 15:22:42 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-13T15:22:42.3985205Z Sep 13 15:22:42 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-13T15:22:42.3986116Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-13T15:22:42.3987027Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-13T15:22:42.3988003Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-13T15:22:42.3988886Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-13T15:22:42.3989753Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-13T15:22:42.3990534Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-13T15:22:42.3991262Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-13T15:22:42.3992048Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-13T15:22:42.3992887Z Sep 13 15:22:42 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-13T15:22:42.3993670Z Sep 13 15:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-13T15:22:42.3994498Z Sep 13 15:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-13T15:22:42.3995471Z Sep 13 15:22:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-13T15:22:42.3996252Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-13T15:22:42.3997203Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-13T15:22:42.3998042Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-13T15:22:42.3998717Z Sep 13 15:22:42 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-09-13T15:22:42.3999316Z Sep 13 15:22:42 Caused by: java.io.IOException: Failed to fetch job execution result
2022-09-13T15:22:42.4000057Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-09-13T15:22:42.4000925Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-09-13T15:22:42.4001806Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-09-13T15:22:42.4002441Z Sep 13 15:22:42 	... 49 more
2022-09-13T15:22:42.4003019Z Sep 13 15:22:42 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-09-13T15:22:42.4003764Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-09-13T15:22:42.4004418Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-09-13T15:22:42.4005239Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-09-13T15:22:42.4005865Z Sep 13 15:22:42 	... 51 more
2022-09-13T15:22:42.4006359Z Sep 13 15:22:42 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-09-13T15:22:42.4007514Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-09-13T15:22:42.4008348Z Sep 13 15:22:42 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-09-13T15:22:42.4009146Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-09-13T15:22:42.4009823Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2022-09-13T15:22:42.4010525Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2022-09-13T15:22:42.4011295Z Sep 13 15:22:42 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
2022-09-13T15:22:42.4012173Z Sep 13 15:22:42 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
2022-09-13T15:22:42.4012810Z Sep 13 15:22:42 	... 51 more
2022-09-13T15:22:42.4013448Z Sep 13 15:22:42 Caused by: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
2022-09-13T15:22:42.4014278Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
2022-09-13T15:22:42.4015229Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-09-13T15:22:42.4015959Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-09-13T15:22:42.4016686Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-09-13T15:22:42.4017624Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1609)
2022-09-13T15:22:42.4018352Z Sep 13 15:22:42 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-09-13T15:22:42.4019064Z Sep 13 15:22:42 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-09-13T15:22:42.4019644Z Sep 13 15:22:42 	at java.lang.Thread.run(Thread.java:748)
2022-09-13T15:22:42.4021477Z Sep 13 15:22:42 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]
2022-09-13T15:22:42.4022481Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-09-13T15:22:42.4023211Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-09-13T15:22:42.4023950Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
2022-09-13T15:22:42.4024490Z Sep 13 15:22:42 	... 3 more
2022-09-13T15:22:42.4025502Z Sep 13 15:22:42 Caused by: java.lang.RuntimeException: org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]
2022-09-13T15:22:42.4026394Z Sep 13 15:22:42 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
2022-09-13T15:22:42.4027181Z Sep 13 15:22:42 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:114)
2022-09-13T15:22:42.4028117Z Sep 13 15:22:42 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2022-09-13T15:22:42.4028635Z Sep 13 15:22:42 	... 3 more
2022-09-13T15:22:42.4029580Z Sep 13 15:22:42 Caused by: org.apache.flink.runtime.JobException: Cannot instantiate the coordinator for operator MultipleInputOperator [Source: source-1, Source: source-2, Source: source-3]
2022-09-13T15:22:42.4030458Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.initialize(ExecutionJobVertex.java:229)
2022-09-13T15:22:42.4031405Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertex(DefaultExecutionGraph.java:901)
2022-09-13T15:22:42.4032277Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.initializeJobVertices(DefaultExecutionGraph.java:891)
2022-09-13T15:22:42.4033150Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:848)
2022-09-13T15:22:42.4034250Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.attachJobGraph(DefaultExecutionGraph.java:830)
2022-09-13T15:22:42.4035132Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraphBuilder.buildGraph(DefaultExecutionGraphBuilder.java:198)
2022-09-13T15:22:42.4036060Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:156)
2022-09-13T15:22:42.4037173Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:361)
2022-09-13T15:22:42.4038096Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:206)
2022-09-13T15:22:42.4038824Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:134)
2022-09-13T15:22:42.4039610Z Sep 13 15:22:42 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
2022-09-13T15:22:42.4040587Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
2022-09-13T15:22:42.4041442Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:369)
2022-09-13T15:22:42.4042111Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:346)
2022-09-13T15:22:42.4042959Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
2022-09-13T15:22:42.4043989Z Sep 13 15:22:42 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
2022-09-13T15:22:42.4045007Z Sep 13 15:22:42 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
2022-09-13T15:22:42.4045573Z Sep 13 15:22:42 	... 4 more
2022-09-13T15:22:42.4045981Z Sep 13 15:22:42 Caused by: java.lang.NullPointerException
2022-09-13T15:22:42.4046657Z Sep 13 15:22:42 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.create(OperatorCoordinatorHolder.java:488)
2022-09-13T15:22:42.4047747Z Sep 13 15:22:42 	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.createOperatorCoordinatorHolder(ExecutionJobVertex.java:286)
2022-09-13T15:22:42.4048604Z Sep 13 15:22:42 	at org.apache.f {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40968&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 07:50:35 UTC 2022,,,,,,,,,,"0|z18jew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 02:40;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40968&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","20/Oct/22 07:50;kevin.cyj;I think it is already fixed by https://issues.apache.org/jira/browse/FLINK-29576. I am closing it. Feel free to reopen it if it still reproduces.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear RecordWriter slower to avoid causing frequent compaction conflicts,FLINK-29295,13481414,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,14/Sep/22 02:20,14/Sep/22 06:27,04/Jun/24 20:41,14/Sep/22 06:27,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"In AbstractTableWrite, the Writer is cleaned up as soon as no new files are generated, which may lead to the changes generated after the compaction have not been committed, but the new data from the next checkpoint comes to create a new writer, which conflicts with the changes generated in the next round of checkpoint and the previous round, resulting in an exception:

{code:java}
Caused by: java.lang.IllegalStateException: Trying to delete file {org.apache.flink.table.data.binary.BinaryRowData@5759f99e, 0, 0, data-7bf2498e-d0a1-42a5-97b7-b3860f10b076-0.orc} which is not previously added. Manifest might be corrupted.
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 06:27:15 UTC 2022,,,,,,,,,,"0|z18jeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 06:27;lzljs3620320;master: 6a1b1bca4da131e47ee17cbf5d10472762a66da6
release-0.2: 75a732b40c868bab026f45478ce33530861407d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a CompactRewriter for full compaction,FLINK-29294,13481413,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Sep/22 01:45,27/Oct/22 11:15,04/Jun/24 20:41,27/Oct/22 11:15,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,We need to introduce a special {{CompactRewriter}} for full compaction to write changelog files.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 11:15:12 UTC 2022,,,,,,,,,,"0|z18je8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 11:15;lzljs3620320;master: 08211f0e190ca8dcce288d93b72350a2b1185ff6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a MergeFunction for full compaction,FLINK-29293,13481412,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Sep/22 01:43,10/Oct/22 08:43,04/Jun/24 20:41,10/Oct/22 08:43,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,We need to introduce a special {{MergeFunction}} to produce changelogs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-14 01:43:26.0,,,,,,,,,,"0|z18je0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change MergeFunction to produce not only KeyValues,FLINK-29292,13481411,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Sep/22 01:42,08/Oct/22 09:19,04/Jun/24 20:41,08/Oct/22 09:19,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,{{MergeFunction}} of full compaction need to produce changelogs instead of single {{KeyValue}}. We need to modify {{MergeFunction}} into a generic class.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 09:19:15 UTC 2022,,,,,,,,,,"0|z18jds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 09:19;lzljs3620320;master: e4c19b66a8af319dd9f412aed250683d54969b34;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change DataFileWriter into a factory to create writers,FLINK-29291,13481410,13481409,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Sep/22 01:34,08/Oct/22 04:00,04/Jun/24 20:41,08/Oct/22 04:00,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Currently {{DataFileWriter}} exposes {{write}} method for data files and extra files.

However, as the number of patterns to write files is increasing (for example, we'd like to write some records into a data file, then write some other records into an extra files when producing changelogs from full compaction) we'll have to keep adding methods to {{DataFileWriter}} if we keep the current implementation.

We'd like to refactor {{DataFileWriter}} into a factory to create writers, so that the users of writers can write however they like.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 04:00:58 UTC 2022,,,,,,,,,,"0|z18jdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 04:00;lzljs3620320;master: a881f41368ef48148d5cef795822ac51625811b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Produce changelog during full compaction in Table Store,FLINK-29290,13481409,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,14/Sep/22 01:29,19/Mar/23 05:49,04/Jun/24 20:41,19/Mar/23 05:49,table-store-0.3.0,,,,,,,,,,,Table Store,,,,0,,,,,"Currently Table Store only produces changelog directly from input. Some downstream systems, however, require complete changelogs including both UPDATE_BEFORE and UPDATE_AFTER messages.

We can only get these information during full compaction, so we should add a feature to produce changelog during full compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-14 01:29:23.0,,,,,,,,,,"0|z18jdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SequencefileInputFormat based on the new Source API,FLINK-29289,13481393,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,13/Sep/22 22:18,13/Sep/22 22:18,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 22:18:47.0,,,,,,,,,,"0|z18j9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start a job with a jar in the system classpath,FLINK-29288,13481389,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sap1ens,sap1ens,13/Sep/22 21:53,13/Nov/23 18:04,04/Jun/24 20:41,23/Sep/22 06:31,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"I'm using the latest (unreleased) version of the Kubernetes operator.

It looks like currently, it's impossible to use it with a job jar file in the system classpath (/opt/flink/lib). *jarURI* is required and it's always passed as a *pipeline.jars* parameter to the Flink process. In practice, it means that the same class is loaded twice: once by the system classloader and another time by the user classloader. This leads to exceptions like this:
{quote}java.lang.LinkageError: loader constraint violation: when resolving method 'XXX' the class loader org.apache.flink.util.ChildFirstClassLoader @47a5b70d of the current class, YYY, and the class loader 'app' for the method's defining class, ZZZ, have different Class objects for the type AAA used in the signature
{quote}
In my opinion, jarURI must be made optional even for the application mode. In this case, it's assumed that it's already available in the system classpath.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 18:04:43 UTC 2023,,,,,,,,,,"0|z18j8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 00:05;sap1ens;Apparently removing *pipeline.jars* is not enough, because the operator will always create */opt/flink/usrlib* folder on the jobmanager as a volume mount. This forces Flink's DefaultPackagedProgramRetriever to try using *usrlib* folder for loading classes (which is empty in this case). 

So, in my mind, operator's *UserLibMountDecorator* should only be used when *pipeline.jars / pipeline.classpaths* is specified.;;;","23/Sep/22 06:31;gyfora;merged to main 8f53441a4978eeb38dc5ef229c179cc60598ce87;;;","07/Nov/23 19:18;trystan;With this change, what is the correct procedure to put job jars on the classpath in Application mode with the k8s operator?
 # Is *jarURI* absolutely required?
 # Will classes be loaded once, or multiple times?
 # How does this relate to the flink docs around classloading? [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/debugging_classloading/#avoiding-dynamic-classloading-for-user-code]

 ;;;","07/Nov/23 19:47;sap1ens;> what is the correct procedure to put job jars on the classpath in Application mode with the k8s operator?

Just put your jars in the system classpath (/opt/flink/lib). I do it in a custom Docker image. 

Other answers:
 # No, the attached PR makes it optional.
 # Once.
 # Avoiding dynamic classloading was the primary reason behind this contribution. 

 ;;;","07/Nov/23 21:46;trystan;Thank you! This is exactly what we are trying to do. I thought we had tested this, but clearly it's working for you! No other special incantations - simply putting the job jar in the classpath is enough?

 

The documentation here does suggest it is required: [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/overview/#application-deployments.] I assume it must just be out of date if it's working for you :);;;","07/Nov/23 22:03;sap1ens;Yep, it should work out of the box. That page may not be up to date, but the reference page says it's optional and what to expect when it's not specified: [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/reference/#jobspec]

Do you see any particular error message? ;;;","08/Nov/23 18:50;trystan;Yes, two different kinds of errors. flink-kubernetes-operator v1.6.0, flink version 1.16.1

If I do not include *jarURI* the job immediately goes into {*}Job Status: FINISHED / Lifecycle State: UPGRADING{*}.

If I include *jarURI* and point it at /opt/flink/lib/myjob.jar, I get linkage errors around the kafka source classes (specifically OffsetResetStrategy).

 

Edit: for the case where i'm not including {*}jarURI{*}, I did find an exception in the operator logs. It seems like it may be related to the pipeline options being null? Which I thought should be optional based on the linked PR.
{code:java}
{""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""java.lang.NullPointerException"",""stackTrace"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.NullPointerException\n\tat org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:148)\n\tat org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)\n\tat io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:138)\n\tat io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:96)\n\tat org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)\n\tat io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:95)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)\n\tat io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:414)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NullPointerException\n\tat org.apache.flink.kubernetes.utils.KubernetesUtils.checkJarFileForApplicationMode(KubernetesUtils.java:407)\n\tat org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:207)\n\tat org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)\n\tat org.apache.flink.kubernetes.operator.service.NativeFlinkService.deployApplicationCluster(Native"",""additionalMetadata"":{},""throwableList"":[{""type"":""java.lang.NullPointerException"",""additionalMetadata"":{}}]} {code};;;","11/Nov/23 20:13;trystan;[~sap1ens] are you running native k8s mode or standalone mode? If the mailing list is a better forum for this discussion I can move it there, too.;;;","13/Nov/23 16:48;sap1ens;Standalone mode. I think it may not run in native mode out of the box. ;;;","13/Nov/23 18:04;trystan;That seems to be the difference. It seems like using native kubernetes mode precludes the ability to avoid dynamic classloading altogether;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add PackagingTestUtils,FLINK-29287,13481329,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,13/Sep/22 15:34,15/Sep/22 10:31,04/Jun/24 20:41,15/Sep/22 10:31,,,,,,,,1.16.0,,,,Connectors / Common,Tests,,,0,pull-request-available,,,,"We currently have test for the packaging of various connectors, scattered around in various bash e2e tests.

Add a java utility for writing such tests and make it available to external connector repos.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 10:31:23 UTC 2022,,,,,,,,,,"0|z18ivs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 10:31;chesnay;master:
a98726ee4168fee7cea23944f83c932eadb86985
06b7507106290c873f536de36cd0f07780361a7e
1.16:
c4d254a92a2346a5d29da9f41c4df0022d6c2f36
66f996aecaf501f24a4d883a8d7bca9544d0db3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential compatibility risk between snapshot binary and jars,FLINK-29286,13481316,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,13/Sep/22 14:03,13/Sep/22 14:03,04/Jun/24 20:41,,elasticsearch-3.0.0,,,,,,,,,,,Tests,,,,0,,,,,"E2E tests generally require a binary, where external repos only real choice is downloading the snapshot binaries from S3.

There is however no guarantee that these are actually compatible with the Maven artifacts that were downloaded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 14:03:42.0,,,,,,,,,,"0|z18isw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish TestUtils#getResource,FLINK-29285,13481315,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,13/Sep/22 14:02,14/Sep/22 15:15,04/Jun/24 20:41,14/Sep/22 15:15,,,,,,,,1.16.0,,,,Tests,,,,0,pull-request-available,,,,"The elasticsearch e2e tests need some utils from flink-end-to-end-test-common, that we should move to the flink test utils.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 15:15:27 UTC 2022,,,,,,,,,,"0|z18iso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 15:15;chesnay;master: 469049a4359aea40f083bb2b7e4fbc1f86b65ce9
1.16: b1a2a5c2d2a9d8e2ce4088b970fc806990f6440f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump derby in flink-connector-hive to v10.14.2.0,FLINK-29284,13481310,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,13/Sep/22 13:41,28/Apr/23 14:04,04/Jun/24 20:41,,,,,,,,,,,,,Connectors / Hive,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 13:41:15.0,,,,,,,,,,"0|z18irk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove hardcoded apiVersion from operator unit test,FLINK-29283,13481308,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,tagarr,tagarr,13/Sep/22 13:38,18/Sep/22 10:12,04/Jun/24 20:41,18/Sep/22 10:12,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"The unit test flink-kubernetes-operator/src/test/java/org/apache/flink/kubernetes/operator/utils/ReconciliationUtilsTest.java has a hardcoded apiVersion. To facilitate modifications, it should be using the constants provided in the class CrdConstants i.e. 

assertEquals(API_GROUP + ""/"" + API_VERSION, internalMeta.get(""apiVersion"").asText());

instead of ""flink.apache.org/v1beta1""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 14:48:22 UTC 2022,,,,,,,,,,"0|z18ir4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 15:28;mbalassi;Good catch, [~tagarr]. Would you be interested in creating a PR for this change?;;;","14/Sep/22 14:33;tagarr;Absolutely, assign it to me;;;","14/Sep/22 14:48;mbalassi;Thanks, in the meantime there is a PR open from another contributor:

[https://github.com/apache/flink-kubernetes-operator/pull/367]

Based on [~matyas] 's comment on the PR the setup might be intentional there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decouple Quickstart E2E test from Elasticsearch,FLINK-29282,13481292,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,13/Sep/22 11:45,14/Sep/22 15:13,04/Jun/24 20:41,14/Sep/22 15:13,,,,,,,,1.17.0,,,,Connectors / ElasticSearch,Tests,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 15:13:19 UTC 2022,,,,,,,,,,"0|z18ins:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 15:13;chesnay;master: db98322472cb65ca0358ec1cce7f9ef737198189;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace Akka by gRPC-based RPC implementation,FLINK-29281,13481290,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,13/Sep/22 11:14,27/Jul/23 08:09,04/Jun/24 20:41,,,,,,,,,,,,,Runtime / RPC,,,,2,,,,,"Following the license change I propose to eventually replace Akka.

Based on LEGAL-619 an exemption is not feasible, and while a fork _may_ be created it's long-term future is up in the air and I'd be uncomfortable with relying on it.

I've been experimenting with a new RPC implementation based on gRPC and so far I'm quite optimistic. It's also based on Netty while not requiring as much of a tight coupling as Akka did.
This would also allow us to sidestep migrating our current Akka setup from Netty 3 (which is affected by several CVEs) to Akka Artery, both saving work and not introducing an entirely different network stack to the project.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32468,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 08:09:17 UTC 2023,,,,,,,,,,"0|z18inc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 11:50;echauchot;Good news for the experimentation, thanks ! Alternatively with Lightbend supporting Akka 2.6.x for CVE for another year there is a good chance that a fork comes up in the meantime.;;;","21/Sep/22 08:21;claude;I am working with a group of developers in an attempt to get an Akka fork accepted as an Apache Project.  If anyone here is interested in participating in that project please reach out to me at  claude@apache.org

The repository that we are working from is at https://github.com/mdedetrich/akka-apache We are looking for people who want to participate as we start into the Apache incubator proecss;;;","22/Sep/22 08:31;echauchot;[~claude] Good news !;;;","22/Sep/22 14:49;spangaer;This is the best news I've had this month so far!

We're in no position to carry such a project, but we'll surely switch, participate and where we can, contribute.;;;","26/Jul/23 13:56;ferenc-csaky;[~chesnay] is this annulates the Akka Artery migration (FLINK-28372) at this point? Now that the migration to Pekko is done, which is an Akka 2.6 fork, which does not change the semantics of the Artery migration I guess, but if this is in reach, putting more effort into Artery and update/complete the existing draft may not worth it.;;;","26/Jul/23 14:36;claude;Pekko v1.0.1 was just released.

[dev@pekko.apache.org|mailto:dev@pekko.apache.org] is the dev list.

Announcement email: https://lists.apache.org/thread/c10qoktyq0tv7t1bo14nxv2dt3s3sf2b

 ;;;","27/Jul/23 08:09;chesnay;> is this annulates the Akka Artery migration (FLINK-28372) at this point? 

Yes, if done it would make the artery migration unnecessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join hint are not propagated in subquery,FLINK-29280,13481287,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,13/Sep/22 10:58,19/Sep/22 03:40,04/Jun/24 20:41,19/Sep/22 03:31,1.16.0,,,,,,,1.16.0,1.17.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,"Add the following code in JoinHintTestBase to re-produce this bug.
{code:java}

@Test
public void testJoinHintWithJoinHintInSubQuery() {
    String sql =
            ""select * from T1 WHERE a1 IN (select /*+ %s(T2) */ a2 from T2 join T3 on T2.a2 = T3.a3)"";

    verifyRelPlanByCustom(String.format(sql, buildCaseSensitiveStr(getTestSingleJoinHint())));
} {code}
This is because that calcite will not propagate the hint in subquery and flink also doesn't resolve it in FlinkSubQueryRemoveRule",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 19 03:31:26 UTC 2022,,,,,,,,,,"0|z18imo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 03:31;godfreyhe;Fixed in master: 22cb554008320e6684280b5205f93d7a6f685c6c
in 1.16.0: b37a8153f22b62982ca144604a34056246f6f36c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticsearchWriterITCase fails without logging enabled,FLINK-29279,13481276,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,13/Sep/22 09:55,13/Apr/23 09:00,04/Jun/24 20:41,,elasticsearch-3.0.0,,,,,,,,,,,Connectors / ElasticSearch,Tests,,,0,,,,,"The test relies on certain messages being logged, but by default logging is disabled so the test fails locally as-is.
We need to find a solution for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29269,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 09:55:05.0,,,,,,,,,,"0|z18ik8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BINARY type is not supported in table store,FLINK-29278,13481247,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,13/Sep/22 07:21,21/Sep/22 14:15,04/Jun/24 20:41,21/Sep/22 14:15,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,, !image-2022-09-13-15-21-55-116.png! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/22 07:21;lzljs3620320;image-2022-09-13-15-21-55-116.png;https://issues.apache.org/jira/secure/attachment/13049223/image-2022-09-13-15-21-55-116.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 14:15:47 UTC 2022,,,,,,,,,,"0|z18ids:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 14:15;lzljs3620320;master: 422ea6072b251b04041b6ca8a738316a30069aad
release-0.2: 3eee4bf4eddd6d22a0225e1958601a44f7dd9ba7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink submits tasks to yarn Federation and throws an exception 'org.apache.commons.lang3.NotImplementedException: Code is not implemented',FLINK-29277,13481232,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jkf6160@163.com,jkf6160@163.com,13/Sep/22 06:41,20/Oct/22 09:51,04/Jun/24 20:41,,1.14.3,,,,,,,,,,,Deployment / YARN,,,,0,,,,,"2022-09-13 11:02:35,488 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (102.400mb (107374184 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead
2022-09-13 11:02:35,751 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Could not execute SQL statement.
        at org.apache.flink.table.client.gateway.local.LocalExecutor.executeModifyOperations(LocalExecutor.java:225) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.cli.CliClient.callInserts(CliClient.java:617) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:606) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:466) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.cli.CliClient.lambda$executeStatement$1(CliClient.java:346) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_141]
        at org.apache.flink.table.client.cli.CliClient.executeStatement(CliClient.java:339) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.cli.CliClient.executeFile(CliClient.java:318) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.cli.CliClient.executeInNonInteractiveMode(CliClient.java:234) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:153) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
Caused by: org.apache.flink.table.api.TableException: Failed to execute sql
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:791) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:754) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeModifyOperations$4(LocalExecutor.java:223) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.executeModifyOperations(LocalExecutor.java:223) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        ... 12 more
Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Could not deploy Yarn job cluster.
        at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:489) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:81) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2042) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:773) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:754) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeModifyOperations$4(LocalExecutor.java:223) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.executeModifyOperations(LocalExecutor.java:223) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        ... 12 more        
Caused by: org.apache.hadoop.ipc.RemoteException: Code is not implemented
        at org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor.getClusterNodes(FederationClientInterceptor.java:671)
        at org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.getClusterNodes(RouterClientRMService.java:243)
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getClusterNodes(ApplicationClientProtocolPBServiceImpl.java:321)
        at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:621)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:993)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:921)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1731)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2918)

        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.ipc.Client.call(Client.java:1491) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.ipc.Client.call(Client.java:1388) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.1U19.jar:?]
        at com.sun.proxy.$Proxy187.getClusterNodes(Unknown Source) ~[?:?]
        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:331) ~[hadoop-yarn-common-3.2.1U19.jar:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_141]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_141]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_141]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_141]
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.1U19.jar:?]
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.1U19.jar:?]
        at com.sun.proxy.$Proxy188.getClusterNodes(Unknown Source) ~[?:?]
        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNodeReports(YarnClientImpl.java:645) ~[hadoop-yarn-client-3.2.1U19.jar:?]
        at org.apache.flink.yarn.YarnClientYarnClusterInformationRetriever.getMaxVcores(YarnClientYarnClusterInformationRetriever.java:44) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:329) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:555) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.yarn.YarnClusterDescriptor.deployJobCluster(YarnClusterDescriptor.java:482) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:81) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2042) ~[flink-dist_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:773) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:754) ~[flink-table_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeModifyOperations$4(LocalExecutor.java:223) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.executeModifyOperations(LocalExecutor.java:223) ~[flink-sql-client_2.11-1.14.3-qihoo-f5.jar:1.14.3-qihoo-f5]
        ... 12 more",Flink 1.14.3、JDK8、hadoop-3.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/22 06:41;jkf6160@163.com;error.log;https://issues.apache.org/jira/secure/attachment/13049220/error.log","13/Sep/22 07:56;jkf6160@163.com;image-2022-09-13-15-56-47-631.png;https://issues.apache.org/jira/secure/attachment/13049224/image-2022-09-13-15-56-47-631.png","20/Sep/22 15:34;bgeng777;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13049525/screenshot-1.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 12:15:09 UTC 2022,,,,,,,,,,"0|z18iag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 06:49;jkf6160@163.com;Flink Submits the task to YARN，YarnClientYarnClusterInformationRetriever.getMaxVcores thrown exception，yarn resourcemanager information was not obtained when the task was submitted in federated mode;;;","13/Sep/22 07:56;jkf6160@163.com;Add a configuration that does not get this maxVcores in federated mode ?

 

!image-2022-09-13-15-56-47-631.png|width=384,height=231!;;;","20/Sep/22 15:34;bgeng777;In hadoop3.2.1, org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor#getClusterNodes is not implemented. 
 !screenshot-1.png! ;;;","22/Sep/22 12:15;jkf6160@163.com;Yes, that's a problem. Flink may not have compatible with yarn federations. [~bgeng777] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush all memory in SortBufferMemTable.clear,FLINK-29276,13481215,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,13/Sep/22 04:20,23/Sep/22 15:32,04/Jun/24 20:41,23/Sep/22 15:32,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Now BinaryInMemorySortBuffer.reset will keep one page.
We could free all memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29273,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 15:32:40 UTC 2022,,,,,,,,,,"0|z18i6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 15:32;lzljs3620320;master: 04418793c67d196b4056c5cb6730eb3e3931cbd4
release-0.2: b82f41a8c75f08e683377645a2ae6bb9eee1e755;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temporal Table function: Cannot add expression of different type to set,FLINK-29275,13481214,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,Tally,Tally,13/Sep/22 04:17,13/Sep/22 04:24,04/Jun/24 20:41,13/Sep/22 04:24,1.15.2,,,,,,,,,,,,,,,0,,,,,"I am useing the temporal table funciton to join two stream like this, but got this error. Any ways to solve this?


{code:java}
Exception in thread ""main"" java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, DECIMAL(32, 2) price, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, TIMESTAMP(3) order_time, TIMESTAMP_LTZ(3) *PROCTIME* NOT NULL proctime, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency0, BIGINT conversion_rate, TIMESTAMP(3) update_time, TIMESTAMP_LTZ(3) *PROCTIME* proctime0) NOT NULL
expression type is RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" order_id, DECIMAL(32, 2) price, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency, TIMESTAMP(3) order_time, TIMESTAMP_LTZ(3) *PROCTIME* NOT NULL proctime, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" currency0, BIGINT conversion_rate, TIMESTAMP(3) update_time, TIMESTAMP_LTZ(3) *PROCTIME* NOT NULL proctime0) NOT NULL
set is rel#61:LogicalCorrelate.NONE.any.None: 0.[NONE].[NONE](left=HepRelVertex#59,right=HepRelVertex#60,correlation=$cor0,joinType=inner,requiredColumns={4})
expression is LogicalJoin(condition=[__TEMPORAL_JOIN_CONDITION($4, $7, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY($5))], joinType=[inner])
  LogicalProject(order_id=[$0], price=[$1], currency=[$2], order_time=[$3], proctime=[PROCTIME()])
    LogicalTableScan(table=[[default_catalog, default_database, orders]])
  LogicalProject(currency=[$0], conversion_rate=[$1], update_time=[$2], proctime=[PROCTIME()])
    LogicalTableScan(table=[[default_catalog, default_database, currency_rates]])
 {code}
Fact Table:
{code:java}
CREATE TABLE `orders` (
    order_id    STRING,
    price       DECIMAL(32,2),
    currency    STRING,
    order_time  TIMESTAMP(3),
    proctime as PROCTIME()
 ) WITH (
    'properties.bootstrap.servers' = '127.0.0.1:9092',
    'properties.group.id' = 'test',
    'scan.topic-partition-discovery.interval' = '10000',
    'connector' = 'kafka',
    'format' = 'json',
    'scan.startup.mode' = 'latest-offset', 
    'topic' = 'test1'
  ) {code}
Build Table:
{code:java}
CREATE TABLE `currency_rates` (
    currency    STRING,
    conversion_rate BIGINT,
    update_time  TIMESTAMP(3),
    proctime as PROCTIME()
 ) WITH (
    'properties.bootstrap.servers' = '127.0.0.1:9092',
    'properties.group.id' = 'test',
    'scan.topic-partition-discovery.interval' = '10000',
    'connector' = 'kafka',
    'format' = 'json',
    'scan.startup.mode' = 'latest-offset', 
    'topic' = 'test3'
  ) {code}
The way to generate table function:
{code:java}
TemporalTableFunction table_rate = tEnv.from(""currency_rates"")
.createTemporalTableFunction(""update_time"", ""currency"");

tEnv.registerFunction(""rates"", table_rate); {code}
Join logic:
{code:java}
 SELECT
    order_id,
    price,
    s.currency,
    conversion_rate,
    order_time
 FROM orders AS o,  
 LATERAL TABLE (rates(o.proctime)) AS s
 WHERE o.currency = s.currency {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 04:17:29.0,,,,,,,,,,"0|z18i6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2EndpointITCase.testGetFunctionWithPattern failed with Persistence Manager has been closed,FLINK-29274,13481207,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,hxb,hxb,13/Sep/22 03:07,26/Sep/22 08:58,04/Jun/24 20:41,26/Sep/22 08:58,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,,"{code:java}
4.6807800Z Sep 13 02:07:54 [ERROR] org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetFunctionWithPattern  Time elapsed: 22.127 s  <<< ERROR!
2022-09-13T02:07:54.6813586Z Sep 13 02:07:54 java.sql.SQLException: javax.jdo.JDOFatalUserException: Persistence Manager has been closed
2022-09-13T02:07:54.6815315Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveStatement.waitForOperationToComplete(HiveStatement.java:401)
2022-09-13T02:07:54.6816917Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:266)
2022-09-13T02:07:54.6818338Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testGetFunctionWithPattern$29(HiveServer2EndpointITCase.java:542)
2022-09-13T02:07:54.6819988Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:633)
2022-09-13T02:07:54.6821484Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:621)
2022-09-13T02:07:54.6823318Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetFunctionWithPattern(HiveServer2EndpointITCase.java:539)
2022-09-13T02:07:54.6824711Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.6825817Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.6827003Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.6828259Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.6829478Z Sep 13 02:07:54 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-09-13T02:07:54.6830717Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-09-13T02:07:54.6832444Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-09-13T02:07:54.6834028Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-09-13T02:07:54.6835304Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-09-13T02:07:54.6836734Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-09-13T02:07:54.6838257Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-09-13T02:07:54.6839775Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-09-13T02:07:54.6841400Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-09-13T02:07:54.6843309Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-09-13T02:07:54.6845300Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-09-13T02:07:54.6846879Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-09-13T02:07:54.6848406Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-09-13T02:07:54.6849760Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-09-13T02:07:54.6851297Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-09-13T02:07:54.6853032Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6854384Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-09-13T02:07:54.6856052Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-09-13T02:07:54.6857406Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-09-13T02:07:54.6858824Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-09-13T02:07:54.6860227Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6861752Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.6863318Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.6864623Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.6866007Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6867339Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.6868565Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.6870141Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.6872335Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-09-13T02:07:54.6874262Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-09-13T02:07:54.6875954Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.6877332Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6878774Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.6880095Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.6881341Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.6882928Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6884550Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.6885847Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.6887389Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.6889201Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-09-13T02:07:54.6890940Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.6892639Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6894303Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.6895709Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.6897057Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.6898411Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.6899771Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.6901135Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.6902936Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.6904607Z Sep 13 02:07:54 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-09-13T02:07:54.6905852Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-09-13T02:07:54.6907027Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-09-13T02:07:54.6908157Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-09-13T02:07:54.6933730Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-09-13T02:07:54.6934713Z Sep 13 02:07:54 
2022-09-13T02:07:54.6935600Z Sep 13 02:07:54 [ERROR] org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetPrimaryKeyWithPattern  Time elapsed: 2.187 s  <<< ERROR!
2022-09-13T02:07:54.6936957Z Sep 13 02:07:54 org.apache.hive.service.cli.HiveSQLException: Failed to getOperationResultSchema.
2022-09-13T02:07:54.6938148Z Sep 13 02:07:54 	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:300)
2022-09-13T02:07:54.6939157Z Sep 13 02:07:54 	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:291)
2022-09-13T02:07:54.6940213Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveQueryResultSet.retrieveSchema(HiveQueryResultSet.java:254)
2022-09-13T02:07:54.6941413Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveQueryResultSet.<init>(HiveQueryResultSet.java:198)
2022-09-13T02:07:54.6942920Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveQueryResultSet$Builder.build(HiveQueryResultSet.java:179)
2022-09-13T02:07:54.6944264Z Sep 13 02:07:54 	at org.apache.hive.jdbc.HiveDatabaseMetaData.getPrimaryKeys(HiveDatabaseMetaData.java:583)
2022-09-13T02:07:54.6945876Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testGetPrimaryKeyWithPattern$23(HiveServer2EndpointITCase.java:465)
2022-09-13T02:07:54.6947624Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:633)
2022-09-13T02:07:54.6949577Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runGetObjectTest(HiveServer2EndpointITCase.java:621)
2022-09-13T02:07:54.6951289Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testGetPrimaryKeyWithPattern(HiveServer2EndpointITCase.java:464)
2022-09-13T02:07:54.6952899Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.6954059Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.6955317Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.6956333Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7118261Z Sep 13 02:07:54 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-09-13T02:07:54.7120151Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-09-13T02:07:54.7121551Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-09-13T02:07:54.7123271Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-09-13T02:07:54.7124596Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-09-13T02:07:54.7125915Z Sep 13 02:07:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-09-13T02:07:54.7127292Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-09-13T02:07:54.7128686Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-09-13T02:07:54.7130110Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-09-13T02:07:54.7131626Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-09-13T02:07:54.7133278Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-09-13T02:07:54.7134718Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-09-13T02:07:54.7136060Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-09-13T02:07:54.7137325Z Sep 13 02:07:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-09-13T02:07:54.7138729Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-09-13T02:07:54.7140187Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7141566Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-09-13T02:07:54.7143229Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-09-13T02:07:54.7144564Z Sep 13 02:07:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-09-13T02:07:54.7145876Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-09-13T02:07:54.7147208Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7148884Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.7150187Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.7151460Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.7153057Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7154414Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.7155751Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.7157291Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.7159439Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-09-13T02:07:54.7161313Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-09-13T02:07:54.7163138Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.7164533Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7165931Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.7167229Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.7168552Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.7170224Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7171631Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.7173130Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.7174728Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.7176520Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-09-13T02:07:54.7178170Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-09-13T02:07:54.7179572Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7180993Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-09-13T02:07:54.7182514Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-09-13T02:07:54.7183754Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-09-13T02:07:54.7185162Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-09-13T02:07:54.7186729Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-09-13T02:07:54.7188068Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-09-13T02:07:54.7189602Z Sep 13 02:07:54 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-09-13T02:07:54.7191074Z Sep 13 02:07:54 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-09-13T02:07:54.7192376Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-09-13T02:07:54.7193420Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-09-13T02:07:54.7194454Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-09-13T02:07:54.7195678Z Sep 13 02:07:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-09-13T02:07:54.7196862Z Sep 13 02:07:54 Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to getOperationResultSchema.
2022-09-13T02:07:54.7198223Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.getOperationResultSchema(SqlGatewayServiceImpl.java:158)
2022-09-13T02:07:54.7199739Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.HiveServer2Endpoint.GetResultSetMetadata(HiveServer2Endpoint.java:681)
2022-09-13T02:07:54.7201158Z Sep 13 02:07:54 	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetResultSetMetadata.getResult(TCLIService.java:1817)
2022-09-13T02:07:54.7202730Z Sep 13 02:07:54 	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetResultSetMetadata.getResult(TCLIService.java:1802)
2022-09-13T02:07:54.7204000Z Sep 13 02:07:54 	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
2022-09-13T02:07:54.7205124Z Sep 13 02:07:54 	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
2022-09-13T02:07:54.7206294Z Sep 13 02:07:54 	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
2022-09-13T02:07:54.7207488Z Sep 13 02:07:54 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-09-13T02:07:54.7208672Z Sep 13 02:07:54 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-09-13T02:07:54.7209644Z Sep 13 02:07:54 	at java.lang.Thread.run(Thread.java:748)
2022-09-13T02:07:54.7212142Z Sep 13 02:07:54 Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation d8443ce0-7ee1-4997-a2b6-46970328febd.
2022-09-13T02:07:54.7213657Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:389)
2022-09-13T02:07:54.7215131Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:248)
2022-09-13T02:07:54.7216437Z Sep 13 02:07:54 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-09-13T02:07:54.7217505Z Sep 13 02:07:54 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-09-13T02:07:54.7218567Z Sep 13 02:07:54 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-09-13T02:07:54.7219589Z Sep 13 02:07:54 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-09-13T02:07:54.7220364Z Sep 13 02:07:54 	... 3 more
2022-09-13T02:07:54.7221223Z Sep 13 02:07:54 Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to listDatabases.
2022-09-13T02:07:54.7222702Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.listDatabases(SqlGatewayServiceImpl.java:240)
2022-09-13T02:07:54.7224028Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.executeGetPrimaryKeys(OperationExecutorFactory.java:362)
2022-09-13T02:07:54.7225719Z Sep 13 02:07:54 	at org.apache.flink.table.endpoint.hive.util.OperationExecutorFactory.lambda$createGetPrimaryKeys$6(OperationExecutorFactory.java:154)
2022-09-13T02:07:54.7227198Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$0(OperationManager.java:91)
2022-09-13T02:07:54.7228661Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:239)
2022-09-13T02:07:54.7229693Z Sep 13 02:07:54 	... 7 more
2022-09-13T02:07:54.7230567Z Sep 13 02:07:54 Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to list all databases in hive
2022-09-13T02:07:54.7231721Z Sep 13 02:07:54 	at org.apache.flink.table.catalog.hive.HiveCatalog.listDatabases(HiveCatalog.java:414)
2022-09-13T02:07:54.7233235Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.operation.OperationExecutor.listDatabases(OperationExecutor.java:143)
2022-09-13T02:07:54.7234814Z Sep 13 02:07:54 	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.listDatabases(SqlGatewayServiceImpl.java:237)
2022-09-13T02:07:54.7235737Z Sep 13 02:07:54 	... 11 more
2022-09-13T02:07:54.7236730Z Sep 13 02:07:54 Caused by: MetaException(message:java.lang.RuntimeException: openTransaction called in an interior transaction scope, but currentTransaction is not active.)
2022-09-13T02:07:54.7238137Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6935)
2022-09-13T02:07:54.7239416Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_databases(HiveMetaStore.java:1670)
2022-09-13T02:07:54.7240514Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.7241525Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.7242953Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.7243980Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7245076Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
2022-09-13T02:07:54.7246373Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
2022-09-13T02:07:54.7247443Z Sep 13 02:07:54 	at com.sun.proxy.$Proxy46.get_databases(Unknown Source)
2022-09-13T02:07:54.7248576Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1356)
2022-09-13T02:07:54.7249973Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAllDatabases(HiveMetaStoreClient.java:1351)
2022-09-13T02:07:54.7251113Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.7252388Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.7253613Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.7254665Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7255792Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:208)
2022-09-13T02:07:54.7256921Z Sep 13 02:07:54 	at com.sun.proxy.$Proxy47.getAllDatabases(Unknown Source)
2022-09-13T02:07:54.7258124Z Sep 13 02:07:54 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.getAllDatabases(HiveMetastoreClientWrapper.java:102)
2022-09-13T02:07:54.7259340Z Sep 13 02:07:54 	at org.apache.flink.table.catalog.hive.HiveCatalog.listDatabases(HiveCatalog.java:411)
2022-09-13T02:07:54.7260095Z Sep 13 02:07:54 	... 13 more
2022-09-13T02:07:54.7260935Z Sep 13 02:07:54 Caused by: java.lang.RuntimeException: openTransaction called in an interior transaction scope, but currentTransaction is not active.
2022-09-13T02:07:54.7262611Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:723)
2022-09-13T02:07:54.7263682Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.ObjectStore.getAllDatabases(ObjectStore.java:1150)
2022-09-13T02:07:54.7264592Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-13T02:07:54.7265459Z Sep 13 02:07:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-13T02:07:54.7266370Z Sep 13 02:07:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-13T02:07:54.7267222Z Sep 13 02:07:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-13T02:07:54.7268066Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97)
2022-09-13T02:07:54.7269122Z Sep 13 02:07:54 	at com.sun.proxy.$Proxy44.getAllDatabases(Unknown Source)
2022-09-13T02:07:54.7270453Z Sep 13 02:07:54 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_databases(HiveMetaStore.java:1661)
2022-09-13T02:07:54.7271347Z Sep 13 02:07:54 	... 30 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29118,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 07:37:43 UTC 2022,,,,,,,,,,"0|z18i54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 03:07;hxb;[~yzl] Could you help take a look? Thx.;;;","13/Sep/22 11:09;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40928&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=26736;;;","14/Sep/22 07:38;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40987&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","14/Sep/22 07:39;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40985&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","14/Sep/22 08:28;yzl;[~hxb] I'm trying to find the bug now. Please assign to me.;;;","23/Sep/22 07:37;fsk119;Merged into release-1.16: c653d162f224b8b938e32323a636ae07119f0c11

Merged into master: 6c5f8a8e3cc4e2bc420495ce5b72e43817027978;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Page not enough Exception in SortBufferMemTable,FLINK-29273,13481206,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Sep/22 03:03,13/Sep/22 09:44,04/Jun/24 20:41,13/Sep/22 09:44,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"When there are many partitions, the partition writer will seize memory and may have the following exceptions:
 !screenshot-1.png! 

We need to make sure that the writer has enough memory before it can start.

Actually, there is enough memory, because it can preempt from other writers. The problem is in OwnerMemoryPool.freePages, it should contain preemptable memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29276,,,,,,,"13/Sep/22 03:03;lzljs3620320;image-2022-09-13-11-03-07-855.png;https://issues.apache.org/jira/secure/attachment/13049201/image-2022-09-13-11-03-07-855.png","13/Sep/22 03:04;lzljs3620320;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13049202/screenshot-1.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:44:30 UTC 2022,,,,,,,,,,"0|z18i4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:44;lzljs3620320;master: a353d1535694fa181a7e2032f6314d74925934bd
release-0.2: f33a12be878006a915daab2ac6e0e0d5e484486d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document DataStream API (DataStream to Table) for table store,FLINK-29272,13481205,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,13/Sep/22 03:01,19/Mar/23 05:46,04/Jun/24 20:41,19/Mar/23 05:46,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,We can have documentation to describe how to convert from DataStream to Table to write to TableStore.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-13 03:01:11.0,,,,,,,,,,"0|z18i4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change to byte array from bytebuffer to improve performance when reading parquet file,FLINK-29271,13481202,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiangjiguang0719,jiangjiguang0719,jiangjiguang0719,13/Sep/22 02:53,26/Oct/22 01:58,04/Jun/24 20:41,26/Oct/22 01:58,,,,,,,,1.17.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,,"I have done a test to compare byte array and bytebuffer,  40% performance improvement when using byte array to read parquet file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 01:58:38 UTC 2022,,,,,,,,,,"0|z18i40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 02:53;jiangjiguang0719;please assign me the issue, I will submit a pr. thanks;;;","26/Oct/22 01:58;jark;Fixed in master: be46408c0241570647bfef58f7dbe5336adfa637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Wrong metrics kafka producer (FlinkKafkaProducer, KafkaSink) in EXACTLY_ONCE",FLINK-29270,13481201,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,hilmialf,hilmialf,13/Sep/22 02:44,28/Sep/22 10:14,04/Jun/24 20:41,28/Sep/22 10:14,1.14.4,,,,,,,,,,,Connectors / Kafka,Runtime / Metrics,,,0,,,,,"I did a very simple kafka-to-kafka pipeline with flink 1.14.4 with checkpoint.

I did 4 tests to confirm: for each old API (FlinkKafkaConsumer&FlinkKafkaProducer) and new API (KafkaSource&KafkaSink), I run AT_LEAST_ONCE and EXACLTY_ONCE. The only difference in the code is in the settings of Semantic part.

However, the metrics shown by producer is not correct when doing EXACTLY_ONCE semantics in both APIs.
 * For FlinkKafkaProducer, the metrics seems to be restarted every checkpoint
 * For KafkaSink, the rate shows half of the actual produced rate.

For at_least_once, they are all agree with the consuming rate.

To understand the situation, I also check the incoming rate metrics for both source and sink topics. So here are the summary:
h3. Old API
h4. At Least Once

!image-2022-09-13-11-41-51-759.png|width=616,height=190!
h4. Exactly Once

!image-2022-09-13-11-42-45-297.png|width=616,height=192!
h3. New API (KafkaSource&KafkaSink)
h4. At Least Once

!image-2022-09-13-11-43-38-226.png|width=618,height=190!
h4. Exactly Once

!image-2022-09-13-11-43-50-441.png|width=620,height=189!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/22 02:41;hilmialf;image-2022-09-13-11-41-51-759.png;https://issues.apache.org/jira/secure/attachment/13049197/image-2022-09-13-11-41-51-759.png","13/Sep/22 02:42;hilmialf;image-2022-09-13-11-42-45-297.png;https://issues.apache.org/jira/secure/attachment/13049196/image-2022-09-13-11-42-45-297.png","13/Sep/22 02:43;hilmialf;image-2022-09-13-11-43-38-226.png;https://issues.apache.org/jira/secure/attachment/13049195/image-2022-09-13-11-43-38-226.png","13/Sep/22 02:43;hilmialf;image-2022-09-13-11-43-50-441.png;https://issues.apache.org/jira/secure/attachment/13049194/image-2022-09-13-11-43-50-441.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 10:14:26 UTC 2022,,,,,,,,,,"0|z18i3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/22 10:15;martijnvisser;[~renqs] Any thoughts on this one?;;;","28/Sep/22 10:14;renqs;I think this is because of the implementation of KafkaSink / FlinkKafkaProducer. Both KafkaSink and FlinkKafkaProducer use an individual KafkaProducer for each checkpoint in exactly-once mode, so some KafkaProducer's metrics might not work as expected (like record_send_rate you tested).

In KafkaSink after a checkpoint is finished, the producer will be put into a pool for being reused in later checkpoints. Let's say you have two producers serving in the pool in turn, then the throughput will be divided by two producer and that's why you see the current_send_time is half as expected. 

In FlinkKafkaProducer each checkpoint creates a new producer. If the checkpoint interval is quite short it's possible that the throughput of one producer is quite low. 

Considering above it's better to use Flink-defined numRecordsOut / numRecordsSent for monitoring if the Kafka sink uses exactly-once semantic. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup CI logging,FLINK-29269,13481172,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 19:28,13/Sep/22 09:55,04/Jun/24 20:41,13/Sep/22 08:56,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,,,,,"Logging isn't setup on CI, which breaks some tests that rely on certain message being logged.

Long-term we need a better way for tests to specify their required log level; as is the test would still fail locally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29279,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 08:56:53 UTC 2022,,,,,,,,,,"0|z18hxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 08:56;chesnay;main: ffc0721badd761c88279bdd28b1e56c1360ead0c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync 1.16.0 code to ES repo,FLINK-29268,13481170,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 19:12,13/Sep/22 08:57,04/Jun/24 20:41,13/Sep/22 08:57,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-12 19:12:06.0,,,,,,,,,,"0|z18hww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support external type systems in DDL,FLINK-29267,13481130,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,twalthr,twalthr,twalthr,12/Sep/22 14:58,18/Nov/22 22:02,04/Jun/24 20:41,,,,,,,,,,,,,Connectors / JDBC,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,1,,,,,"Many connectors and formats require supporting external data types. Postgres users request UUID support, Avro users require enum support, etc.

FLINK-19869 implemented support for Postgres UUIDs poorly and even impacts performance with regular strings.

The long-term solution should be user-defined types in Flink. This is however a bigger effort that requires a FLIP and a bigger amount of resources.

As a mid-term solution, we should offer a consistent approach based on DDL options that allows to define a mapping from Flink type system to the external type system. I suggest the following:

{code}
CREATE TABLE MyTable (
...
) WITH(
  'mapping.data-types' = '<Flink field name>: <External field data type>'
)
{code}

The mapping defines a map from Flink data type to external data type. The external data type should be string parsable. This works for most connectors and formats (e.g. Avro schema string).


Examples:

{code}
CREATE TABLE MyTable (
  regular_col STRING,
  uuid_col STRING,
  point_col ARRAY<DOUBLE>,
  box_col ARRAY<ARRAY<DOUBLE>>
) WITH(
  'mapping.data-types' = 'uuid_col: uuid, point_col: point, box_col: box'
)
{code}

We provide a table of supported mapping data types. E.g. the {{point}} type is always maped to {{ARRAY<DOUBLE>}}. In general we choose a data type in Flink that comes closest to the required functionality.


Future work:

In theory, we can also offer mapping of field names. It might be a requirement that Flink's column name is different from the external system's one. 

{code}
CREATE TABLE MyTable (
...
) WITH(
  'mapping.names' = '<Flink field name>: <External field name>'
)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30092,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 14:13:45 UTC 2022,,,,,,,,,,"0|z18ho8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 08:07;martijnvisser;This sounds like a good improvement. A couple of suggestions for the mapping:

Postgres UUID (given we reverted FLINK-19869)
Postgres Enum
Avro Enum

[~twalthr] A lot of these challenges are reported on things like BIGINT UNSIGNED FLINK-18580 and on Debezium data types FLINK-18758
;;;","13/Sep/22 14:08;twalthr;From Henrik Feldt (https://apache-flink.slack.com/archives/C03G7LJTS2G/p1662912163987969)
{code}
Remember to spec for complex types, too

Like geometry

Just so there's an escape hatch

maybe instead of ""external"", name the value ""connector native data type""

and for pg you could also consider what pg extensions need loading before these are available

and instead of ""flink field name"" consider naming it ""flink table catalog column name"" or whichever is more specific

you also have this https://www.postgresql.org/docs/current/sql-createtype.html

finally, it might be useful to make the serialiser/deserialiser work on byte arrays instead of strings

otherwise you need to bring in localisation and string representation for certain types

I ended up creating a pg view as a workaround

Also, for JDBC you don't need to go as far as types.

You can just let me cast the column in pg-specific code:
.column(""app_id"", DataTypes.STRING().notNull(), ""CAST(id AS text)"")
as the third parameter; then you can just push that through the config.

another variant, for JDBC, just open up an escape hatch:
https://stackoverflow.com/questions/56265904/reading-uuid-from-result-set-in-postgres-jdbc
https://crafted-software.blogspot.com/2013/03/uuid-values-from-jdbc-to-postgres.html
So basically, if you let the user type a column as DataTypes.OBJECT<UUID>(), x -> (UUID)x.getObject('column_name'))
{code};;;","13/Sep/22 14:13;twalthr;> ""you could also consider what pg extensions need loading before these are available""
This is what I consider the long-term solution in the description above. At some point, Flink will hopefully also support user-defined types (i.e. CREATE TYPE) and we will be able to load the postgres types into Flink's type system. But this is future work. For now we need to get rid of the biggest obstacles with a rather minimal but effective solution.

> ""just let me cast the column in pg-specific code""
This is not an option as we have to consider many different connectors and formats. E.g. Avro within Kafka or Debezium in Avro within Kafka.

> ""DataTypes.OBJECT<UUID>(), x -> (UUID)x.getObject('column_name'))""
Ideally, a solution must be made available to both Java and pure SQL users.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup ES root pom,FLINK-29266,13481126,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 14:42,13/Sep/22 09:36,04/Jun/24 20:41,13/Sep/22 09:34,,,,,,,,elasticsearch-3.0.0,,,,Build System,Connectors / ElasticSearch,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:34:56 UTC 2022,,,,,,,,,,"0|z18hnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:34;chesnay;main: 965a651ce010f3b0f50e9da3e0b36952231e264d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update ES version to 3.0-SNAPSHOT,FLINK-29265,13481124,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 14:39,13/Sep/22 09:36,04/Jun/24 20:41,13/Sep/22 09:35,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:35:09 UTC 2022,,,,,,,,,,"0|z18hmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:35;chesnay;main: 5e7dae453fdd4687c14677be4632f54927240b17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Give ES parent module a proper artifact ID,FLINK-29264,13481123,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 14:38,13/Sep/22 09:35,04/Jun/24 20:41,13/Sep/22 09:35,,,,,,,,elasticsearch-3.0.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:35:24 UTC 2022,,,,,,,,,,"0|z18hmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:35;chesnay;main: 787e772f5335f7b1c53defd3ad753dffbc9de3d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Elasticsearch connector from apache/flink repo,FLINK-29263,13481100,13437797,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 11:30,16/Sep/22 07:45,04/Jun/24 20:41,16/Sep/22 07:45,,,,,,,,1.17.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 07:45:50 UTC 2022,,,,,,,,,,"0|z18hhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 07:45;chesnay;master: d268ea105cc65407c7c3e4004d620f698e7729a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation,FLINK-29262,13481097,13481074,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 11:09,13/Sep/22 18:53,04/Jun/24 20:41,13/Sep/22 18:53,,,,,,,,1.15.3,1.16.0,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 18:53:19 UTC 2022,,,,,,,,,,"0|z18hgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 18:53;chesnay;master: 030baed8d6e406b83018671d76b817193c258ded
1.16: a325214cd0b47b286dad40b788488533799a6da5
1.15: 1afcba0c61cdc654554b91ca1f47646c540850bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FAIL_ON_UNKNOWN_PROPERTIES=false in ReconciliationUtils,FLINK-29261,13481093,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,morhidi,,12/Sep/22 11:03,21/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,"The operator cannot be downgraded, once the CR specification is written to the `status`
 
Caused by: com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""mode"" (class org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec), not marked as ignorable (12 known properties: ""restartNonce"", ""imagePullPolicy"", ""ingress"", ""flinkConfiguration"", ""serviceAccount"", ""image"", ""job"", ""podTemplate"", ""jobManager"", ""logConfiguration"", ""flinkVersion"", ""taskManager""])
 at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.apache.flink.kubernetes.operator.crd.spec.FlinkDeploymentSpec[""mode""])
at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:61)
at com.fasterxml.jackson.databind.DeserializationContext.handleUnknownProperty(DeserializationContext.java:1127)
at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:1989)
at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1700)
at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1678)
at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:319)
at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:176)
at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)
at com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4650)
at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2831)
at com.fasterxml.jackson.databind.ObjectMapper.treeToValue(ObjectMapper.java:3295)
at org.apache.flink.kubernetes.operator.reconciler.ReconciliationUtils.deserializeSpecWithMeta(ReconciliationUtils.java:288)
... 18 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:25 UTC 2023,,,,,,,,,,"0|z18hg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/22 11:04;morhidi;cc [~gyfora] what do you think? ;;;","18/Sep/22 19:21;gyfora;I think this would be a reasonable improvement as the correct json properties are already verified and guaranteed by Kubernetes itself based on the installed CRD. This would give us a chance to handle multiple versions flexibly;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto-wipe exclusion list after updating reference version,FLINK-29260,13481090,13481074,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 10:56,13/Sep/22 12:51,04/Jun/24 20:41,13/Sep/22 12:51,,,,,,,,1.15.3,1.16.0,,,Release System,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 12:20:04 UTC 2022,,,,,,,,,,"0|z18hfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 12:20;chesnay;master: 19ddba04a4b51a6e4c9accfc5d3d5fcc5f86c01e
1.16: ebd8d8e0341ccd096d5621f4806a01a94d2b52e7
1.15: 771000c511031b4c24917addd4cea494ac4bb48e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update release guide,FLINK-29259,13481081,13481074,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 10:22,12/Sep/22 11:09,04/Jun/24 20:41,12/Sep/22 11:09,,,,,,,,,,,,,,,,0,,,,,"New minor version:
* wipe exclusions on master after release

New patch version X.Y.Z:
* verify no new exclusions were added since last release and/or confirm whether they were discussed anywhere
* wipe exclusions on release-X.Y branch after release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-12 10:22:26.0,,,,,,,,,,"0|z18hdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SAVEPOINT_ONLY upgrade mode,FLINK-29258,13481079,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,mxm,mxm,12/Sep/22 10:18,24/Nov/22 01:03,04/Jun/24 20:41,12/Sep/22 12:11,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"Users may want to explicitly upgrade from a savepoint. the {{SAVEPOINT}} upgrade mode cannot ensure this. When the savepoint creation fails, which might be temporary, the operator falls back to using the latest checkpoint.

To prevent this from happening, we could add a SAVEPOINT_ONLY upgrade mode which does not proceed when the savepoint creation fails.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 16:33:39 UTC 2022,,,,,,,,,,"0|z18hcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/22 12:11;morhidi;The configuration
h5. kubernetes.operator.job.upgrade.last-state-fallback.enabled

does exactly this.
h5.  ;;;","14/Sep/22 16:33;mxm;That's good to know but how does the user know this configuration option is set?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator docs are vague about upgrade mode,FLINK-29257,13481078,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,12/Sep/22 10:14,19/Sep/22 16:09,04/Jun/24 20:41,19/Sep/22 16:09,,,,,,,,kubernetes-operator-1.2.0,,,,Documentation,Kubernetes Operator,,,0,pull-request-available,,,,"Users are confused how the SAVEPOINT ugprade mode works.
 # It's not clear to users that the savepoint is drawn during the upgrade process.
 # It's not clear to users what happens when the process fails.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 19 16:09:09 UTC 2022,,,,,,,,,,"0|z18hco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 16:09;gyfora;merged to main f8d85b0c654334b1892f246427f0b1ac2cdfa018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs for k8s operator webhook,FLINK-29256,13481075,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,mbalassi,mbalassi,12/Sep/22 09:48,19/Sep/22 11:39,04/Jun/24 20:41,19/Sep/22 11:39,kubernetes-operator-1.2.0,,,,,,,kubernetes-operator-1.2.0,,,,Documentation,Kubernetes Operator,,,0,pull-request-available,,,,Please add an admission control section to the architecture page describing the webhook and call out the certificate renewal feature there explicitly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 19 11:39:17 UTC 2022,,,,,,,,,,"0|z18hc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 11:39;gyfora;merged to main 1b2b0f3d5dd519fd6c9ea191e4b353679bc7312e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-258 - Enforce binary compatibility in patch releases,FLINK-29255,13481074,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,12/Sep/22 09:38,13/Sep/22 18:53,04/Jun/24 20:41,13/Sep/22 18:53,,,,,,,,1.15.3,1.16.0,,,Build System,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-12 09:38:19.0,,,,,,,,,,"0|z18hbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RpcGateway should have some form of Close() method,FLINK-29254,13481072,13481290,Sub-task,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,12/Sep/22 09:29,27/Jun/23 17:59,04/Jun/24 20:41,,,,,,,,,,,,,Runtime / RPC,,,,0,,,,,"In a simple client-server model the gateway constitutes the client, which will generally have to allocate some resources to communicate with the server.

There is however currently no way for a user to close a gateway, hence these resources will generally leak (unless the underlying RPC implementation either magically fixes that somehow or doesn't allocate resources for clients in the first place).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 17:59:32 UTC 2023,,,,,,,,,,"0|z18hbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 08:32;chesnay;Getting this right is probably impossible, because we pass gateways around like candy without any clear ownership.

RPC implementations will need to resort to phantom references to clean up resources (like the FS safety net).;;;","23/Sep/22 13:23;chesnay;Turns out this isn't actually that difficult.;;;","27/Jun/23 17:59;dheeren;[~chesnay] : Can you be more specific ? What is the work around ? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultJobmanagerRunnerRegistry#localCleanupAsync calls close instead of closeAsync,FLINK-29253,13481052,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Sep/22 06:26,13/Sep/22 07:12,04/Jun/24 20:41,13/Sep/22 07:12,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,Runtime / Coordination,,,,0,pull-request-available,,,,"{{DefaultJobmanagerRunnerRegistry#localCleanupAsync}} is meant to be called from the main thread. The current implementation calls {{close}} on the {{JobManagerRunner}} instead of {{closeAsync}}. This results in a blocking call on the {{Dispatcher}}'s main thread which we want to avoid.

Thanks for identifying this issue, [~chesnay]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 07:12:31 UTC 2022,,,,,,,,,,"0|z18h6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 07:12;mapohl;master: 4e99d64b4914ca5fbb1fe338455a8f653c164172

1.16: ef5941e36c06070b49b4557e77ddb6750f2af2af

1.15: 776ccfa27d2d29b1ccd674878eeccee1649ee938;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support create table-store table with 'connector'='table-store',FLINK-29252,13480921,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lzljs3620320,MOBIN,MOBIN,09/Sep/22 19:17,26/Oct/22 02:33,04/Jun/24 20:41,26/Oct/22 02:33,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"Support create table-store table with 'connector'='table-store': 

sink to table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'datagen',
    'fields.word.length' = '1'
);
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',  //should rename 'catalog-database'？
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; {code}
source from table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'print'
);
INSERT INTO word_table SELECT word FROM word_count;{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 02:33:08 UTC 2022,,,,,,,,,,"0|z18gdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/22 00:56;liyubin117;According to flip [https://cwiki.apache.org/confluence/display/FLINK/FLIP-188%3A+Introduce+Built-in+Dynamic+Table+Storage] and the defintion of ManagedTableFactory interface, we shouldn't add CONNECTOR option to built-in table.
{code:java}
/**
 * Base interface for configuring a managed dynamic table connector. The managed table
 * factory is used when there is no FactoryUtil.CONNECTOR option.
**/
public interface ManagedTableFactory extends DynamicTableFactory {code}
 ;;;","26/Oct/22 02:33;lzljs3620320;master: bb411ab9d5ffdf11cc071353e2fbc1e184327085;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send CREATED status and Cancel event via FlinkResourceListener,FLINK-29251,13480894,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,09/Sep/22 15:04,24/Nov/22 01:02,04/Jun/24 20:41,14/Sep/22 12:31,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"To complete the lifecycle history of a custom resource the operator should sent:
 * CREATED status notification during initial deployment of a CR
 * Cancel event when deleting a CR",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 15:04:38.0,,,,,,,,,,"0|z18g7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove RpcService#getTerminationFuture,FLINK-29250,13480875,13480871,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Sep/22 13:33,16/Sep/22 15:33,04/Jun/24 20:41,16/Sep/22 15:33,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,,"Unused method; unnecessary because stopService returns a termination future anyway.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 16 15:33:32 UTC 2022,,,,,,,,,,"0|z18g3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/22 15:33;chesnay;master: 4910076bafc7d5d6091e3dae505d0eabbff0b72d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove RpcService#execute/scheduleRunnable,FLINK-29249,13480874,13480871,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Sep/22 13:31,12/Nov/22 11:56,04/Jun/24 20:41,13/Sep/22 18:43,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,,Subsumed be {{getScheduledExecutor()}} .,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30003,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 18:43:11 UTC 2022,,,,,,,,,,"0|z18g34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 18:43;chesnay;master: 0bbee3047ed81d39d25fd736946c527e627a2bad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove RpcService#fenceRpcServer,FLINK-29248,13480873,13480871,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Sep/22 13:30,12/Sep/22 11:02,04/Jun/24 20:41,12/Sep/22 11:02,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,pull-request-available,,,,Unused method.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 12 11:02:04 UTC 2022,,,,,,,,,,"0|z18g2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/22 11:02;chesnay;master: b8fc7f34d2c21a99cabe9b74f9062369e3f26f6c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streamline RpcService interface,FLINK-29247,13480871,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Sep/22 13:26,16/Sep/22 15:33,04/Jun/24 20:41,16/Sep/22 15:33,,,,,,,,1.17.0,,,,Runtime / RPC,,,,0,,,,,This interface contains a number of unused or redundant methods that we could prune.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 13:26:20.0,,,,,,,,,,"0|z18g2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup DynamoDB Maven Modules,FLINK-29246,13480864,13483592,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,09/Sep/22 12:46,08/Nov/22 20:36,04/Jun/24 20:41,22/Sep/22 12:28,,,,,,,,aws-connector-3.0.0,,,,Connectors / DynamoDB,,,,0,pull-request-available,,,,"Setup initial project structure for [flink-connector-dynamodb|https://github.com/apache/flink-connector-dynamodb] including:
- Parent and module pom files
- Basic build configuration
- Quality plugin configuration (checkstyle)

Related to https://cwiki.apache.org/confluence/display/FLINK/FLIP-252%3A+Amazon+DynamoDB+Sink+Connector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 12:28:13 UTC 2022,,,,,,,,,,"0|z18g0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:28;dannycranmer;Merged commit [{{c863a85}}|https://github.com/apache/flink-connector-dynamodb/commit/c863a85d35bdc27b4b32ee46d5faa857e7642b21] into apache:main
Merged commit [{{92090cb}}|https://github.com/apache/flink-connector-dynamodb/commit/92090cb2e2c15d884db8e14ccabb5d8c8e0cd58a] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit 5 Migration] Remove RetryRule,FLINK-29245,13480843,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,09/Sep/22 10:44,07/Oct/22 07:53,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Documentation,Tests,,,0,starter,,,,"With the transition to JUnit5, using [RetryExtension|https://github.com/apache/flink/blob/78b231f60aed59061f0f609e0cfd659d78e6fdd5/flink-test-utils-parent/flink-test-utils-junit/src/main/java/org/apache/flink/testutils/junit/extensions/retry/RetryExtension.java#L43] is favored, anyway. {{RetryExtension}} also utilizes the annotations {{@RetryOnException}} and {{@RetryOnFailure}} which still refer to {{RetryRule}} in its JavaDoc.

This issue is about cleaning things up around {{RetryRule}} and the related JavaDocs.",,,,,,,,,,,FLINK-25538,,,,,,,,,,,,,,FLINK-24627,,,,,,,,,,,,FLINK-29198,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 07 07:53:14 UTC 2022,,,,,,,,,,"0|z18fw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 13:08;mapohl;[~snuyanzin]  pointed outthat I missed a few usages of {{RetryRule}} which have to be cleaned up first (e.g. [KafkaTestBase:100|https://github.com/apache/flink/blob/44f73c496ed1514ea453615b77bee0486b8998db/flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestBase.java#L100]). I updated the ticket description and moved the issue to be a subtask of FLINK-25325;;;","04/Oct/22 05:15;nagarajtantri;Hi [~mapohl], I would like to work on this. Can I raise a PR?;;;","04/Oct/22 07:43;mapohl;Hi [~nagarajtantri], thanks for volunteering to fix this issue. This one specifically is currently blocked by other JUnit5 migrations (e.g. FLINK-25538 as mentioned in [my comment above|https://issues.apache.org/jira/browse/FLINK-29245?focusedCommentId=17602326&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17602326]). I linked FLINK-25538 to make this clearer. You might want to pick up one of the other subtasks under FLINK-25325 if you'd like to help the community migrating to JUnit5;;;","06/Oct/22 18:36;nagarajtantri;Thanks [~mapohl] . Since most of the open tickets had ""Can you assign this to me please?"", I thought they would be picked up. Will keep an eye open for others;;;","07/Oct/22 07:53;mapohl;Good point. I went through the open FLINK-25325 subtasks and updated the assignee to make it more visual where there is work in progress already.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metric lastMaterializationDuration to  ChangelogMaterializationMetricGroup,FLINK-29244,13480825,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,09/Sep/22 09:22,26/Oct/22 09:47,04/Jun/24 20:41,26/Oct/22 09:34,,,,,,,,1.16.1,1.17.0,,,Runtime / State Backends,,,,0,pull-request-available,,,,"Materialization duration can help us evaluate the efficiency of materialization and the impact on the job.

 

How do you think about ? [~roman] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 09:47:29 UTC 2022,,,,,,,,,,"0|z18fsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 00:59;Feifan Wang;Hi [~yuanmei] , how do you think about this ?;;;","27/Sep/22 07:00;masteryhx;I think it makes sense. Please go ahead.;;;","06/Oct/22 02:13;Feifan Wang;Thanks [~masteryhx] , I have submit a [PR|https://github.com/apache/flink/pull/20965] for this ticket, but it seems not linked to this issue automatically. Can you help me review it ?;;;","09/Oct/22 07:40;masteryhx;Sure, Thanks for your pr! 
I have linked your pr and left some comments. PTAL.;;;","26/Oct/22 09:34;yunta;merged in master: de5409afbb003b108518ac5cc2e5ba215063c2b6;;;","26/Oct/22 09:47;yunta;merged in release-1.16: 969967c160bf13a3948df941c4bd0a53005aac5d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a COW mode for table store,FLINK-29243,13480806,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,09/Sep/22 08:41,19/Mar/23 05:46,04/Jun/24 20:41,19/Mar/23 05:46,,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,"Currently, we can combine the COW (Copy on write) mode by three options, but the combination may not be optimal and tedious.
We can introduce a option that specifically turns on the mode of COW.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:46:24 UTC 2023,,,,,,,,,,"0|z18fo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 12:19;liliwei;i want to have a try, could you gave me the ticket?;;;","19/Mar/23 05:46;lzljs3620320;changelog producer = full-compaction is the cow mode.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read time out when close write channel [flink-gs-fs-hadoop ],FLINK-29242,13480784,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,j.zheng,j.zheng,09/Sep/22 07:12,30/Mar/23 12:51,04/Jun/24 20:41,,1.15.0,,,,,,,,,,,FileSystems,,,,0,,,,,"h2. Detail

See in GSBlobStorageImpl
{code:java}
@Override
public int write(byte[] content, int start, int length) throws IOException {
    LOGGER.trace(""Writing {} bytes to blob {}"", length, blobIdentifier);
    Preconditions.checkNotNull(content);
    Preconditions.checkArgument(start >= 0);
    Preconditions.checkArgument(length >= 0);

    ByteBuffer byteBuffer = ByteBuffer.wrap(content, start, length);
    int written = writeChannel.write(byteBuffer);
    LOGGER.trace(""Wrote {} bytes to blob {}"", written, blobIdentifier);
    return written;
}

@Override
public void close() throws IOException {
    LOGGER.trace(""Closing write channel to blob {}"", blobIdentifier);
    writeChannel.close();
} {code}
when I write data into google cloud storage by flink-gs-fs-haddoop.

The service always has read time out exceptions, which can be reproduced in a very short time of task execution. 
I tried to trace the code and found that it always occurs when the writeChannel Close code is executed. I tried retrying by modifying the source code but it didn't solve the problem, the timeout is 20s and the checkpoint will fail if this problem occurs.

I tried to change the chunk size but found no help, with this component, I can't write data to gcs via flink.

 

By the way, I found that 503 service unavailable occurs when create writeChannel. This problem occurs less often than Read time out, but it needs to be checked
{code:java}
@Override
public GSBlobStorage.WriteChannel writeBlob(GSBlobIdentifier blobIdentifier) {
    LOGGER.trace(""Creating writeable blob for identifier {}"", blobIdentifier);
    Preconditions.checkNotNull(blobIdentifier);

    BlobInfo blobInfo = BlobInfo.newBuilder(blobIdentifier.getBlobId()).build();
    com.google.cloud.WriteChannel writeChannel = storage.writer(blobInfo);
    return new WriteChannel(blobIdentifier, writeChannel);
}

@Override
public GSBlobStorage.WriteChannel writeBlob(
        GSBlobIdentifier blobIdentifier, MemorySize chunkSize) {
    LOGGER.trace(
            ""Creating writeable blob for identifier {} with chunk size {}"",
            blobIdentifier,
            chunkSize);
    Preconditions.checkNotNull(blobIdentifier);
    Preconditions.checkArgument(chunkSize.getBytes() > 0);

    BlobInfo blobInfo = BlobInfo.newBuilder(blobIdentifier.getBlobId()).build();
    com.google.cloud.WriteChannel writeChannel = storage.writer(blobInfo);
    writeChannel.setChunkSize((int) chunkSize.getBytes());
    return new WriteChannel(blobIdentifier, writeChannel);
} {code}
 ","flink version： 1.15

jdk: 1.8

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 07:12:21.0,,,,,,,,,,"0|z18fjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not overwrite from empty input,FLINK-29241,13480780,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Sep/22 06:52,09/Sep/22 07:22,04/Jun/24 20:41,09/Sep/22 07:22,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"There is currently no data, which will not trigger an overwrite, which causes the semantics of the sql to not be correct",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 07:22:02 UTC 2022,,,,,,,,,,"0|z18fio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 07:22;lzljs3620320;master: 62d60d53dd52d08bdae2017e2d2a6dd8effc5a8a
release-0.2: 59a1992726ccd437e6cb59a73ec8866b54d7908b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify the ClassLoader in StreamExecutionEnvironment and TableEnvironment,FLINK-29240,13480776,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,09/Sep/22 06:43,11/Mar/24 12:43,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Runtime / Task,Table SQL / API,,,1,,,,,"Since [FLINK-15635| https://issues.apache.org/jira/browse/FLINK-15635], we have introduced a user classloader in table module to manage all user jars, such as the jar added by `ADD JAR` or `CREATE FUNCTION ... USING JAR` syntax. However, in table API  program user can create `StreamExecutionEnvironment` first, then create `TableEnvironment` based on it, the classloader in `StreamExecutionEnvironment` and `TableEnvironment` are not the same.  if the user use `ADD JAR` syntax in SQL query, here maybe occur ClassNotFoundException during compile StreamGraph to JobGraph because of the different classloader, so we need to unify the classloader, make sure the classloader is the same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 18 18:38:31 UTC 2023,,,,,,,,,,"0|z18fhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 06:55;lsy;cc [~zhuzh], [~jark] ;;;","18/Apr/23 18:38;charles-tan;Hi [~lsy], I was wondering if this issue will receive any attention for the next Flink release as this is the root ticket for the breaking UDF classloading functionality since Flink 1.16 (https://issues.apache.org/jira/browse/FLINK-29890). We currently are using a workaround for the issue, but it would be great if this ticket gets picked up so we can clean up the code on our end.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaSerializerWrapper.config should be Map<String, ?> for full configurability.",FLINK-29239,13480773,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Bill G,Bill G,09/Sep/22 06:08,09/Sep/22 09:28,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"{{KafkaSerializerWrapper.config}} is currently type {{Map<String, String>}} which limits the configurability of the passed {{Serializer}} class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-09 06:08:12.0,,,,,,,,,,"0|z18fh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong index information will be obtained after the downstream failover in hybrid full mode,FLINK-29238,13480759,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,09/Sep/22 02:42,13/Sep/22 09:03,04/Jun/24 20:41,13/Sep/22 09:03,1.16.0,,,,,,,1.16.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"Hybrid shuffle relies on the index to read the disk data. Since the spilled data may be consumed from memory, the readable status is introduced. For the readable buffer, FileDataManager does not pre-load it. However, when the downstream fails, the previous readable status will be used incorrectly, resulting in that some buffer cannot be read correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 09:03:43 UTC 2022,,,,,,,,,,"0|z18fe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 09:03;xtsong;- master (1.17): 12dfe6abd66646c7473bbe353d7b5eb0cfd41130
- release-1.16: 438eddefbf13efa604a6bb8c3b0d3e0e10d741ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RexSimplify can not be removed after update to calcite 1.27,FLINK-29237,13480739,13449408,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Sep/22 22:56,10/Feb/23 09:23,04/Jun/24 20:41,10/Feb/23 09:23,,,,,,,,1.18.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"It seems there is some work should be done to make it happen
Currently removal of RexSimplify from Flink repo leads to failure of several tests like
{{IntervalJoinTest#testFallbackToRegularJoin}}
{{CalcITCase#testOrWithIsNullInIf}}
{{CalcITCase#testOrWithIsNullPredicate}}
example of failure
{noformat}
Sep 07 11:25:08 java.lang.AssertionError: 
Sep 07 11:25:08 
Sep 07 11:25:08 Results do not match for query:
Sep 07 11:25:08   
Sep 07 11:25:08 SELECT * FROM NullTable3 AS T
Sep 07 11:25:08 WHERE T.a = 1 OR T.a = 3 OR T.a IS NULL
Sep 07 11:25:08 
Sep 07 11:25:08 
Sep 07 11:25:08 Results
Sep 07 11:25:08  == Correct Result - 4 ==   == Actual Result - 2 ==
Sep 07 11:25:08  +I[1, 1, Hi]               +I[1, 1, Hi]
Sep 07 11:25:08  +I[3, 2, Hello world]      +I[3, 2, Hello world]
Sep 07 11:25:08 !+I[null, 999, NullTuple]   
Sep 07 11:25:08 !+I[null, 999, NullTuple]   
Sep 07 11:25:08         
Sep 07 11:25:08 Plan:
Sep 07 11:25:08   == Abstract Syntax Tree ==
Sep 07 11:25:08 LogicalProject(inputs=[0..2])
Sep 07 11:25:08 +- LogicalFilter(condition=[OR(=($0, 1), =($0, 3), IS NULL($0))])
Sep 07 11:25:08    +- LogicalTableScan(table=[[default_catalog, default_database, NullTable3]])
Sep 07 11:25:08 
Sep 07 11:25:08 == Optimized Logical Plan ==
Sep 07 11:25:08 Calc(select=[a, b, c], where=[SEARCH(a, Sarg[1, 3; NULL AS TRUE])])
Sep 07 11:25:08 +- BoundedStreamScan(table=[[default_catalog, default_database, NullTable3]], fields=[a, b, c])
Sep 07 11:25:08 
Sep 07 11:25:08        
Sep 07 11:25:08 	at org.junit.Assert.fail(Assert.java:89)
Sep 07 11:25:08 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1(BatchTestBase.scala:154)
Sep 07 11:25:08 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1$adapted(BatchTestBase.scala:147)
Sep 07 11:25:08 	at scala.Option.foreach(Option.scala:257)
Sep 07 11:25:08 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:147)
Sep 07 11:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Sep 07 11:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Sep 07 11:25:08 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Sep 07 11:25:08 

{noformat}",,,,,,,,,,,,,,,,,,,,FLINK-28946,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 09:23:46 UTC 2023,,,,,,,,,,"0|z18f9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/23 08:25;Sergey Nuyanzin;It looks like the issue is that {{SearchOperatorGen}} while code generation does not take into account that it should check for {{null}} if {{sarg.containsNull}}
;;;","10/Feb/23 09:23;TsReaper;master: d9102ddd755ec26d14256bcb733d814e063acd2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableFactory wildcard options are not supported,FLINK-29236,13480738,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,krisnaru,krisnaru,08/Sep/22 22:50,09/Sep/22 10:52,04/Jun/24 20:41,09/Sep/22 10:52,1.14.0,1.15.0,1.16.0,,,,,,,,,Table SQL / API,,,,0,,,,,"SQL API:
{code:java}
 CREATE TEMPORARY TABLE `playevents` (upload_time BIGINT, log_id STRING) WITH ( 
  'connector' = 'kafka',
  'topic' = 'topic1', 
  'properties.bootstrap.servers' = xxx', 
  'properties.group.id' = 'kafka-krish-test3',
  'scan.startup.mode' = 'earliest-offset', 
  'format' = 'avro-cloudera',   
  'avro-cloudera.properties.schema.registry.url' = 'yyy',
  'avro-cloudera.schema-name'='zzz'
  ) {code}
{color:#000000}ClouderaRegistryAvroFormatFactory {color}
{code:java}
maven.artifact(
    group = ""org.apache.flink"",
    artifact = ""flink-avro-cloudera-registry"",
    version = ""1.14.0-csadh1.6.0.1"",
), {code}
{color:#000000}returns optionalOptions as [""schema-name"", ""properties.*""]

[https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FactoryUtil.java#L628] does not handle `wildcard patterns`. Hence its throwing error. {color}
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'kafka'.Unsupported options:avro-cloudera.properties.schema.registry.urlSupported options:avro-cloudera.properties.*
avro-cloudera.schema-name
connector
format
key.fields
key.fields-prefix
key.format
properties.bootstrap.servers
properties.group.id
property-version
scan.startup.mode
scan.startup.specific-offsets
scan.startup.timestamp-millis
scan.topic-partition-discovery.interval
sink.delivery-guarantee
sink.parallelism
sink.partitioner
sink.semantic
sink.transactional-id-prefix
topic
topic-pattern
value.fields-include
value.format
        at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624)
        at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914)
        at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978)
        at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validateExcept(FactoryUtil.java:938)
        at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validateExcept(FactoryUtil.java:978)
        at org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactory.createDynamicTableSource(KafkaDynamicTableFactory.java:176)
        at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:156)
 {code}
{color:#000000} {color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 10:52:42 UTC 2022,,,,,,,,,,"0|z18f9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 10:52;martijnvisser;I'm don't think this ticket belongs to Flink. Hortonworks/Cloudera made modifications themselves which are incompatible with ASF Flink, the issue should not be that Flink adapts but that either Hortonworks/Cloudera follow the ASF Flink implementation. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update SnakeYAML to 1.31,FLINK-29235,13480700,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,sergiosp,sergiosp,08/Sep/22 16:35,11/Nov/22 10:06,04/Jun/24 20:41,11/Nov/22 10:06,1.15.3,,,,,,,1.17.0,,,,Build System,BuildSystem / Shaded,,,0,,,,,"Flink uses snakeyaml v1.27. 
flink-shaded uses Jackson 2.12.4, which used snakeyaml v1.29

Those version are vulnerable to CVE-2022-25857. Flink itself is not directly impacted by this CVE, but we should bump this to avoid false flags. 

Ref:

https://nvd.nist.gov/vuln/detail/CVE-2022-25857

https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-jackson/2.12.4-15.0/flink-shaded-jackson-2.12.4-15.0.pom

https://github.com/apache/flink-shaded/blob/master/flink-shaded-jackson-parent/flink-shaded-jackson-2/pom.xml#L73",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 10:06:21 UTC 2022,,,,,,,,,,"0|z18f14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 14:18;martijnvisser;[~chesnay] Should this be fixed before Flink 1.16? ;;;","19/Sep/22 14:44;chesnay;I guess so.;;;","19/Sep/22 14:50;chesnay;Mind you that the CVE likely doesn't really apply because we only use it to parse the configuration.;;;","24/Oct/22 19:45;sergiosp;Hello [~chesnay] 

Noticed the flink-shaded-jackson v2.13.4-16.0 already has the fix (it uses jackson's own snakeyaml version which is 1.31). Could we upgrade flink-shaded version in flink version 1.16.0 to use 2.13.4-16.0?

[https://github.com/apache/flink/blob/release-1.16.0-rc2/pom.xml#L125]

{{<flink.shaded.version>16.0</flink.shaded.version>    <flink.shaded.jackson.version>2.13.4</flink.shaded.jackson.version>}}

 

Ref:

[https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.13.4/jackson-dataformat-yaml-2.13.4.pom]

[https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-jackson/2.13.4-16.0/flink-shaded-jackson-2.13.4-16.0.pom]

 ;;;","01/Nov/22 10:38;martijnvisser;Fixed in master: cffb8d9dfc0aed7038574cb16826cf9e9573248e;;;","07/Nov/22 20:00;sergiosp;Hi , could we evaluate for addition in 1.16.1?;;;","11/Nov/22 10:06;martijnvisser;[~sergiosp] I've changed the description of the ticket to better match the current situation:

1. Flink used snakeyaml v1.27 and has been updated to snakeyaml v1.31. That is planned to be released for Flink 1.17.0
2. There is already flink-shaded release 1.16.0, which uses Jackson 2.13.4 and snakeyaml v1.31. 

Given that Flink only uses snakeyaml to parse the config, the impact for Flink is that low that I don't see an immediate need to backport this to Flink 1.16.0. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead lock in DefaultLeaderElectionService,FLINK-29234,13480686,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,LucentWong,LucentWong,08/Sep/22 14:52,18/Nov/22 14:23,04/Jun/24 20:41,18/Nov/22 14:22,1.13.5,1.14.5,1.15.3,,,,,1.15.4,1.16.1,1.17.0,,Runtime / Coordination,,,,0,pull-request-available,,,,"Jobmanager stop working because the deadlock in DefaultLeaderElectionService.

The log stopped at
{code:java}
org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Stopping DefaultLeaderElectionService. {code}
Which may similar to this ticket https://issues.apache.org/jira/browse/FLINK-20008

Here is the jstack info
{code:java}
Found one Java-level deadlock: 
============================= 
""flink-akka.actor.default-dispatcher-18"": waiting to lock monitor 0x00007f15c7eae3a8 (object 0x0000000678d395e8, a java.lang.Object), which is held by ""main-EventThread"" ""main-EventThread"": waiting to lock monitor 0x00007f15a3811258 (object 0x0000000678cf1be0, a java.lang.Object), which is held by ""flink-akka.actor.default-dispatcher-18"" Java stack information for the threads listed above: 
=================================================== 

""flink-akka.actor.default-dispatcher-18"": 
 at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.stop(DefaultLeaderElectionService.java:104) - waiting to lock <0x0000000678d395e8> (a java.lang.Object)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$closeAsync$0(JobMasterServiceLeadershipRunner.java:147)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner$$Lambda$735/1742012752.run(Unknown Source)
 at org.apache.flink.runtime.concurrent.FutureUtils.lambda$runAfterwardsAsync$18(FutureUtils.java:687)
 at org.apache.flink.runtime.concurrent.FutureUtils$$Lambda$736/6716561.accept(Unknown Source)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
 at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
 at org.apache.flink.runtime.concurrent.DirectExecutorService.execute(DirectExecutorService.java:217)
 at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543)
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:765)
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
 at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:795)
 at java.util.concurrent.CompletableFuture.whenCompleteAsync(CompletableFuture.java:2163)
 at org.apache.flink.runtime.concurrent.FutureUtils.runAfterwardsAsync(FutureUtils.java:684)
 at org.apache.flink.runtime.concurrent.FutureUtils.runAfterwards(FutureUtils.java:651)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.closeAsync(JobMasterServiceLeadershipRunner.java:143) - locked <0x0000000678cf1be0> (a java.lang.Object)
 at org.apache.flink.runtime.dispatcher.Dispatcher.terminateJob(Dispatcher.java:807)
 at org.apache.flink.runtime.dispatcher.Dispatcher.terminateRunningJobs(Dispatcher.java:799)
 at org.apache.flink.runtime.dispatcher.Dispatcher.terminateRunningJobsAndGetTerminationFuture(Dispatcher.java:812)
 at org.apache.flink.runtime.dispatcher.Dispatcher.onStop(Dispatcher.java:268)
 at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186)
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$$Lambda$444/1289054037.apply(Unknown Source)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
 at akka.actor.Actor.aroundReceive(Actor.scala:517)
 at akka.actor.Actor.aroundReceive$(Actor.scala:515)
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
 at akka.actor.ActorCell.invoke(ActorCell.scala:561)
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
 at akka.dispatch.Mailbox.run(Mailbox.scala:225)
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


""main-EventThread"":
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.runIfStateRunning(JobMasterServiceLeadershipRunner.java:468) - waiting to lock <0x0000000678cf1be0> (a java.lang.Object)
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.grantLeadership(JobMasterServiceLeadershipRunner.java:248)
 at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:211) - locked <0x0000000678d395e8> (a java.lang.Object)
 at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:166)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:693)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:689)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100)
 at org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:688) - locked <0x0000000678d39788> (a org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:567)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:618)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:883)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:653)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
 at org.apache.flink.shaded.curator4.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
 at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:601)
 at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 18 14:22:06 UTC 2022,,,,,,,,,,"0|z18ey0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 09:31;martijnvisser;[~LucentWong] Given that Flink 1.13 is not supported by the community anymore, could you verify that this still occurs in Flink 1.15? ;;;","10/Sep/22 09:46;LucentWong;[~martijnvisser] I think this issue still exists in the master branch. The issue is caused by the ordering of Flink to get the lock.

 

In this line, the class *DefaultLeaderElectionService* will try to get the lock of itself, then invoke the method of 

*leaderContender(which is JobMasterServiceLeaderShipRunner in this case).*  

[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L204]

So the order of getting locks is
 # DefaultLeaderElectionService
 # JobMasterServiceLeaderShipRunner

 

And in this line *JobMasterServiceLeaderShipRunner* will try to get the lock of itself, then invoke the method of *leaderElectionService(which is* {*}DefaultLeaderElectionService in this case{*}{*}).{*}

[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMasterServiceLeadershipRunner.java#L148]

So the order of getting the locks is
 # JobMasterServiceLeaderShipRunner
 # DefaultLeaderElectionService

 

So if these two functions are invoked nearly at the same time, it will cause the dead lock issue.

 ;;;","20/Oct/22 07:58;xtsong;Looks like a valid bug to me. [~Weijie Guo], could you please take a look?;;;","20/Oct/22 09:31;Weijie Guo;Yes, I will take a look and fix this~;;;","02/Nov/22 13:14;fpaul;[~Weijie Guo] can you give an estimate of when this will be fixed? I want to start the release process of 1.15.3 and include this change.;;;","02/Nov/22 13:29;Weijie Guo;[~fpaul] , The PR changes the production code very little, and it may take 1～2 days to complete the review.;;;","18/Nov/22 14:22;xtsong;- master (1.17): d745f5b3f7a64445854c735668afa9b72edb3fee
- release-1.16: a5b9efec917c696a5836fd1779ed787d6e7a7108
- release-1.15: 74097409e7a874584d28b84d10bf6bfe98a11af3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kubernetes Operator example/kustomize not running,FLINK-29233,13480652,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,Research,Research,08/Sep/22 12:49,12/Oct/22 07:59,04/Jun/24 20:41,12/Oct/22 07:59,kubernetes-operator-0.1.0,kubernetes-operator-1.0.0,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,,,,Kubernetes Operator,,,,0,,,,,"h1. question
 * I failed to pull the flink-Kubernetes -operator:latest image when installing the Flink Kubernetes operator using the Kustomize Sidecar case provided by the operator.
 * The following is the Helm install operator code and the corresponding error:

 
{code:java}
# code
helm install flink-kubernetes-operator helm/flink-kubernetes-operator -f flink-kubernetes-operator/examples/kustomize/sidecar/values.yaml --post-renderer flink-kubernetes-operator/examples/kustomize/sidecar/render 
# error
image:flink-kubernetes-operator:latest not pull{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 19:13:19 UTC 2022,,,,,,,,,,"0|z18eqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 19:13;mbalassi;[~Research] could you try with adding the following to the Helm command?
{noformat}
--set image.repository=apache/flink-kubernetes-operator{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File store continuous reading support from_timestamp scan mode,FLINK-29232,13480645,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Sep/22 12:04,22/Nov/22 01:52,04/Jun/24 20:41,22/Nov/22 01:52,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,The file store can find a suitable snapshot according to start timestamp and read incremental data from it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 22 01:52:41 UTC 2022,,,,,,,,,,"0|z18eow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/22 01:52;lzljs3620320;master: 0d2440984f6ca9c4e26caca7034280e9c3726337;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink UDAF produces different results in the same sliding window,FLINK-29231,13480612,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,08/Sep/22 09:21,16/Jan/23 01:36,04/Jun/24 20:41,16/Jan/23 01:36,1.15.3,,,,,,,1.15.4,1.16.1,1.17.0,,API / Python,,,,0,pull-request-available,,,,"It seems that PyFlink udtaf produces different results in the same sliding window. It can be reproduced with the given code and input. It is not always happening but the possibility is relatively high.

The incorrect output is the following:

!image-2022-09-08-17-20-06-296.png!

 

We can see that the output contains different `val_sum` at `window_time` 2022-01-01 00:01:59.999.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/22 09:20;xuannan;image-2022-09-08-17-20-06-296.png;https://issues.apache.org/jira/secure/attachment/13049079/image-2022-09-08-17-20-06-296.png","08/Sep/22 09:20;xuannan;input;https://issues.apache.org/jira/secure/attachment/13049078/input","08/Sep/22 09:20;xuannan;test_agg.py;https://issues.apache.org/jira/secure/attachment/13049077/test_agg.py",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 01:36:56 UTC 2023,,,,,,,,,,"0|z18ehk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 01:36;dianfu;Fixed in:
- master via addca4e18aa1806984070b8c450d3d0df5e473d3
- release-1.16 via 6d3d66ca71a87c50f9d160b5c05aa6cbe49ae1b0
- release-1.15 via d9115d8c2aae75c0b17471c1ab8a0769e606def1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish blogpost about the Akka license change,FLINK-29230,13480605,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Sep/22 08:40,11/Jul/23 06:23,04/Jun/24 20:41,23/Sep/22 13:25,,,,,,,,,,,,Project Website,,,,1,pull-request-available,,,,"People reached out to the project via every conceivable channel to inquire about the impact of the recent Akka change; let's write a blogpost to centralize answers to that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 06:23:56 UTC 2023,,,,,,,,,,"0|z18eg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/23 06:23;gvauvert;The corresponding blogpost is

https://flink.apache.org/2022/09/08/regarding-akkas-licensing-change/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveServer2 Endpoint doesn't support execute statements in sync mode,FLINK-29229,13480589,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,08/Sep/22 07:03,23/Sep/22 07:15,04/Jun/24 20:41,22/Sep/22 10:32,1.16.0,,,,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 10:32:35 UTC 2022,,,,,,,,,,"0|z18ecg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 10:32;fsk119;Merged into master: f2c72772345e25f51faa70185e6ebd764f63b05a

Merged into release-1.16: 6493e7c1c6a2eb050e5415c1748ae5c267625ea2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the schema of the  HiveServer2 getMetadata with JDBC,FLINK-29228,13480588,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,fsk119,fsk119,08/Sep/22 07:02,22/Sep/22 10:35,04/Jun/24 20:41,22/Sep/22 10:35,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 22 10:35:33 UTC 2022,,,,,,,,,,"0|z18ec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 10:35;fsk119;Merged into release-1.16: fd76fb5bba579ed85da84a361a406db7af54ae9e

Merged into master: 1650734eeb3e4400e97ee0dd2089539f35afe467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shoud package disruptor(com.lmax) to flink lib for aync logger when xxconnector using it.,FLINK-29227,13480577,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,jackylau,jackylau,08/Sep/22 05:12,09/Sep/22 01:54,04/Jun/24 20:41,09/Sep/22 01:54,1.17.0,,,,,,,1.17.0,,,,Build System,,,,0,pull-request-available,,,,"when i develop xxConnector which dependecncy like this

      xxconnector -> log4j2 -> AsyncLoggerConfig （jar: disruptor(com.lmax)）

xconnector loaded by user(childFirst) classloader

log4j2 which loaded by app classloader, which also make AsyncLoggerConfig loaded by app classloader, according to the principle of classloader. and the flink lib don't have disruptor, which cause ClassNotFound. Although, the disruptor jar exists in my xxconnector

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/22 12:03;jackylau;image-2022-09-08-20-03-27-219.png;https://issues.apache.org/jira/secure/attachment/13049089/image-2022-09-08-20-03-27-219.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 01:52:51 UTC 2022,,,,,,,,,,"0|z18e9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 05:13;jackylau;hi [~chesnay]  [~tiwalter]  so i I propose to  add  disruptor to lib, whay do you think?

 

<groupId>com.lmax</groupId>
<artifactId>disruptor</artifactId>;;;","08/Sep/22 11:17;chesnay;I don't really get the problem. If you want to use disruptor, add it to lib/. I don't understand why we should bundle additional things into Flink.;;;","08/Sep/22 11:22;jackylau;if xxconnector  using async logger it will exists same problem. [~chesnay]  https://logging.apache.org/log4j/2.x/manual/async.html;;;","08/Sep/22 11:26;chesnay;What problem? If something you use for logging requires disruptor, then you must add disruptor to lib/. If adding it to lib/ doesn't solve the problem, then bundling it in Flink _also_ doesn't solve the problem.;;;","08/Sep/22 11:45;jackylau;[~chesnay] yes,  just need add disruptor to lib/. so i suggest add it into flink/lib. Otherwise users need to be added to their projects. 

And the current Flink project has declared this dependency in the parent POM. And the disruptor is dependent on Log4J2 for async logger and personally i believe it should be entered as well as log4j;;;","08/Sep/22 12:03;jackylau;[~chesnay]  and Async Logger is configurable like this

!image-2022-09-08-20-03-27-219.png!;;;","08/Sep/22 13:04;jark;I agree with [~chesnay]. Flink should keep the default dependencies small and shouldn't bundle unnecessary dependencies. AFAIK, disruptor is a widely used dependency which means the high probability of conflict, but few users use async logger. So I think it's fine for the users who want to use async logger to manually add disruptor to flink/lib. It's just like how Hadoop users setup their Flink environment. 
;;;","09/Sep/22 01:51;jackylau;[~jark]  thanks for your careful explanation;;;","09/Sep/22 01:52;jackylau;i close this issue [~jark] [~chesnay] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception for streaming insert overwrite,FLINK-29226,13480563,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,08/Sep/22 03:16,09/Sep/22 07:21,04/Jun/24 20:41,09/Sep/22 07:21,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Currently, table store dose not support streaming insert overwrite, we should throw exception for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 07:21:18 UTC 2022,,,,,,,,,,"0|z18e6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 01:37;nicholasjiang;[~lzljs3620320], could you please assign this ticket to me?;;;","09/Sep/22 07:21;lzljs3620320;master: f35ae349f115da82dcadb369e607b56e8978f220
release-0.2: 0cb007a15a890ee16a1ac8db90314f0a859bd04e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete message incorrectly ignored in SinkUpsertMaterializer,FLINK-29225,13480552,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lincoln.86xy,lincoln.86xy,08/Sep/22 01:56,11/Nov/22 08:37,04/Jun/24 20:41,,1.14.5,1.15.3,,,,,,,,,,,,,,0,,,,,"Currently if the interval between the arrival of the delete message and the insert/update message exceeds state ttl, the delete message was ignored incorrectly in `SinkUpsertMaterializer`. This will cause wrong result in corresponding sink table(dirty data left).

 

1. if state ttl is set to '10 hour', then the following delete message will be ignored (the '+I (1, a1)' will be left in sink table forever)
{code:java}
00:00:01 +I (1, a1) 
10:00:02 -D (1, a1)
{code}
 

2. but another contrast case which will wrongly delete data in sink table if we send delete message when state staled
{code:java}
00:00:01 +I (1, a1)
00:00:02 +I (1, a2) 
10:00:03 -D (1, a1)
{code}
 

compare the two choice of current implementation and eager deletion, the former will cause dirty data left, but the later will cause some data lost(seems the former is less harmful..)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-08 01:56:44.0,,,,,,,,,,"0|z18e4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-mirror does not pick up the new release branch 1.16,FLINK-29224,13480542,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,07/Sep/22 22:20,12/Sep/22 18:47,04/Jun/24 20:41,12/Sep/22 18:47,,,,,,,,1.17.0,,,,Build System / CI,,,,0,,,,,"The {{release-1.16}} branch has been cut by the community, but it's not getting picked up by flink-ci/flink-mirror. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 12 18:46:56 UTC 2022,,,,,,,,,,"0|z18e28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 22:24;jingge;Fixed. It seems there was some issues with git fetch.;;;","12/Sep/22 12:22;jingge;permission is missing while pushing the workflow change;;;","12/Sep/22 18:46;jingge;fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing info output for when filtering JobGraphs based on their persisted JobResult,FLINK-29223,13480504,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,snuyanzin,mapohl,mapohl,07/Sep/22 15:08,12/Sep/22 12:19,04/Jun/24 20:41,12/Sep/22 12:18,1.15.2,1.16.0,1.17.0,,,,,1.15.3,1.16.0,1.17.0,,Runtime / Coordination,,,,0,pull-request-available,starter,,,"We have the case where we don't see (in the logs) a job being registered in the \{[JobResultStore}} after it reached a globally-terminal state (HA-mode enabled).

We would have expected the job to be picked up again for recovery after the JM failover which didn't happen as well. We're missing a debug statement here that would help us identify the case that the job was actually registered in the {{JobResultStore}} but the [log message afterwards|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1145] isn't printed.

We could fix that by adding some info logs for the filtering mechanism when recovering the jobs as a {{else}} branch in [SessionDispatcherLeaderProcess:149|https://github.com/apache/flink/blob/63817b5ffdf7ba24a168aeec95464d13e4d78e13/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/runner/SessionDispatcherLeaderProcess.java#L149] (and in [JobDispatcherLeaderProcessFactoryFactory|https://github.com/apache/flink/blob/63817b5ffdf7ba24a168aeec95464d13e4d78e13/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/runner/JobDispatcherLeaderProcessFactoryFactory.java] accordingly)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 12 12:18:43 UTC 2022,,,,,,,,,,"0|z18dug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/22 12:18;mapohl;* master (I missed that we didn't squash the commits here): 
** c5248b62abf55970aca8780fd816310f90f7cbd8 
** 6393f00db8f6ed23245a259d3eb775d5dcc03223
* 1.16: 
** 3035a8e9747c89b1920746d2b87e34e89ef89066
* 1.15: 
** b552505c513795e14d6318e2b47167b75449c0af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong behavior for Hive's load data inpath,FLINK-29222,13480472,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,07/Sep/22 12:06,21/Sep/22 01:48,04/Jun/24 20:41,21/Sep/22 01:48,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,"In hive, `load data inpath` will remove src file, and `load data local inpath` won't remove the src file.

But When using the following sql with Hive dialect:
{code:java}
load data local inpath 'test.txt' INTO TABLE tab2 {code}
The file `test.txt` will be removed, although the expected is not to remove the `test.txt`.

The reason is the parameter order is not right when try to call `HiveCatalog#loadTable(...,  isOverWrite, isSourceLocal)`,

It'll call it with 
{code:java}
hiveCatalog.loadTable(
       ..., 
        hiveLoadDataOperation.isSrcLocal(), // should be isOverwrite
        hiveLoadDataOperation.isOverwrite()); // should be isSrcLocal{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 14:30:37 UTC 2022,,,,,,,,,,"0|z18dnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 14:30;jark;Fixed in  
 - master: 4448d9fd5e344bd0c2e197c2676c403bc2b665b9
 - release-1.16: bff0985aef4ed43681e6ad3bd81fc460bef3c6a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding join hint in sql may cause imcompatible state,FLINK-29221,13480429,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,07/Sep/22 09:17,07/Sep/22 09:18,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,,,,,"The cause of the possible imcompatible state is that the sql before adding join hint and after is changed.

Adding the following code in DagOptimizationTest.scala can re-produce this change.
{code:java}

@Test
def testMultiSinks6(): Unit = {
  val stmtSet = util.tableEnv.createStatementSet()
  util.tableEnv.getConfig.set(
    RelNodeBlockPlanBuilder.TABLE_OPTIMIZER_REUSE_OPTIMIZE_BLOCK_WITH_DIGEST_ENABLED,
    Boolean.box(true))
  // test with non-deterministic udf
  util.tableEnv.registerFunction(""random_udf"", new NonDeterministicUdf())
  val table1 = util.tableEnv.sqlQuery(
    ""SELECT random_udf(a) AS a, cast(b as int) as b, c FROM MyTable join MyTable1 on MyTable.c = MyTable1.f"")
  util.tableEnv.registerTable(""table1"", table1)
  val table2 = util.tableEnv.sqlQuery(""SELECT SUM(a) AS total_sum FROM table1"")
  val table3 = util.tableEnv.sqlQuery(""SELECT MIN(b) AS total_min FROM table1"")

  val sink1 = util.createCollectTableSink(Array(""total_sum""), Array(INT))
  util.tableEnv.asInstanceOf[TableEnvironmentInternal].registerTableSinkInternal(""sink1"", sink1)
  stmtSet.addInsert(""sink1"", table2)

  val sink2 = util.createCollectTableSink(Array(""total_min""), Array(INT))
  util.tableEnv.asInstanceOf[TableEnvironmentInternal].registerTableSinkInternal(""sink2"", sink2)
  stmtSet.addInsert(""sink2"", table3)

  util.verifyExecPlan(stmtSet)
} {code}
The plan is :
{code:java}
// ast
LogicalLegacySink(name=[`default_catalog`.`default_database`.`sink1`], fields=[total_sum])
+- LogicalAggregate(group=[{}], total_sum=[SUM($0)])
   +- LogicalProject(a=[$0])
      +- LogicalProject(a=[random_udf($0)], b=[CAST($1):INTEGER], c=[$2])
         +- LogicalJoin(condition=[=($2, $5)], joinType=[inner])
            :- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
            +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(d, e, f)]]])

LogicalLegacySink(name=[`default_catalog`.`default_database`.`sink2`], fields=[total_min])
+- LogicalAggregate(group=[{}], total_min=[MIN($0)])
   +- LogicalProject(b=[$1])
      +- LogicalProject(a=[random_udf($0)], b=[CAST($1):INTEGER], c=[$2])
         +- LogicalJoin(condition=[=($2, $5)], joinType=[inner])
            :- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
            +- LogicalTableScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(d, e, f)]]])

// optimized exec
HashJoin(joinType=[InnerJoin], where=[(c = f)], select=[a, b, c, d, e, f], build=[right])(reuse_id=[1])
:- Exchange(distribution=[hash[c]])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+- Exchange(distribution=[hash[f]])
   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])

LegacySink(name=[`default_catalog`.`default_database`.`sink1`], fields=[total_sum])
+- HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS total_sum])
   +- Exchange(distribution=[single])
      +- LocalHashAggregate(select=[Partial_SUM(a) AS sum$0])
         +- Calc(select=[random_udf(a) AS a])
            +- Reused(reference_id=[1])

LegacySink(name=[`default_catalog`.`default_database`.`sink2`], fields=[total_min])
+- HashAggregate(isMerge=[true], select=[Final_MIN(min$0) AS total_min])
   +- Exchange(distribution=[single])
      +- LocalHashAggregate(select=[Partial_MIN(b) AS min$0])
         +- Calc(select=[CAST(b AS INTEGER) AS b])
            +- Reused(reference_id=[1]){code}
If the join hint is added, the `sqlToRelConverterConfig` will add a config 'withBloat(-1)' and disable merging project when convert sql node to rel node(see more in FlinkPlannerImpl), and the optimized exec plan will be changed because of SubGraphBasedOptimizer:
{code:java}
// optimized exec
Calc(select=[random_udf(a) AS a, CAST(b AS INTEGER) AS b, c])(reuse_id=[1])
+- HashJoin(joinType=[InnerJoin], where=[(c = f)], select=[a, b, c, f], build=[right])
   :- Exchange(distribution=[hash[c]])
   :  +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
   +- Exchange(distribution=[hash[f]])
      +- Calc(select=[f])
         +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(d, e, f)]]], fields=[d, e, f])LegacySink(name=[`default_catalog`.`default_database`.`sink1`], fields=[total_sum])
+- HashAggregate(isMerge=[true], select=[Final_SUM(sum$0) AS total_sum])
   +- Exchange(distribution=[single])
      +- LocalHashAggregate(select=[Partial_SUM(a) AS sum$0])
         +- Calc(select=[a])
            +- Reused(reference_id=[1])LegacySink(name=[`default_catalog`.`default_database`.`sink2`], fields=[total_min])
+- HashAggregate(isMerge=[true], select=[Final_MIN(min$0) AS total_min])
   +- Exchange(distribution=[single])
      +- LocalHashAggregate(select=[Partial_MIN(b) AS min$0])
         +- Calc(select=[b])
            +- Reused(reference_id=[1]) {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29120,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-07 09:17:40.0,,,,,,,,,,"0|z18de0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip ci tests on docs-only-PRs,FLINK-29220,13480423,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxb,hxb,hxb,07/Sep/22 09:06,07/Sep/22 09:10,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Build System / Azure Pipelines,,,,0,,,,,"For the docs-only-PRs, we can skip the ci tests. But there is an exception here, if the content of `static/generated` or `layouts/shortcodes/generated` is modified, we also need to trigger the ci tests, which can prevent that the doc of config or rest api has been changed, but the corresponding code is not together changed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-07 09:06:43.0,,,,,,,,,,"0|z18dco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TABLE AS statement blocks SQL client's execution,FLINK-29219,13480410,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,renqs,renqs,07/Sep/22 07:56,01/Nov/22 08:47,04/Jun/24 20:41,27/Sep/22 01:53,1.16.0,,,,,,,1.16.0,1.17.0,,,Table SQL / API,Table SQL / Client,,,0,pull-request-available,,,,"When executing CREATE TABLE AS statement to create a sink table in SQL client, the client could create the table in catalog and submit the job to cluster successfully, but stops emitting new prompts and accepts new inputs, and user has to use SIGTERM (Control + C) to forcefully stop the SQL client. 

As contrast the behavior of INSERT INTO statement in SQL client is printing ""Job is submitted with JobID xxxx"" and being ready to accept user's input. 

From the log it looks like the client was waiting for the job to finish.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 27 01:53:39 UTC 2022,,,,,,,,,,"0|z18d9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 04:22;lsy;cc [~tartarus] ;;;","08/Sep/22 06:21;tartarus;[~renqs]  Thank you very much for your feedback on this issue!

I will try to reproduce this soon and can you give more informations such as logs or sql, thanks;;;","15/Sep/22 07:06;tartarus;[~renqs]  This problem has been located because sql-client does not handle `CreateTableASOperation`, I have reproduced offline.

I will fix it soon;;;","27/Sep/22 01:53;jark;Fixed in 
 - master: 44009efc5400c6706563fa53335eccf445cf5d0c
 - release-1.16: 4c126223cbc2dec3f08b50c1b398bdf522747ba9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADD JAR syntax could not work with Hive catalog in SQL client,FLINK-29218,13480406,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,lsy,renqs,renqs,07/Sep/22 07:49,09/Sep/22 11:29,04/Jun/24 20:41,09/Sep/22 11:29,1.16.0,,,,,,,,,,,Connectors / Hive,Table SQL / API,,,0,,,,,"ADD JAR syntax is not working for adding Hive / Hadoop dependencies into SQL client. 

To reproduce the problem:
 # Place Hive connector and Hadoop JAR outside {{{}lib{}}}, and add them into the session using {{ADD JAR}} syntax.
 # Create a Hive catalog using {{CREATE CATALOG}}

Exception thrown by SQL client: 
{code:java}
2022-09-07 15:23:15,737 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Could not execute SQL statement.
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:208) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:634) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:468) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:371) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:328) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:279) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:227) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: Unable to create catalog 'myhive'.Catalog options are:
'hive-conf-dir'='file:///Users/renqs/Workspaces/flink/flink-master/build-target/opt/hive-conf'
'type'='hive'
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:438) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1423) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to load hive-site.xml from specified path:file:/Users/renqs/Workspaces/flink/flink-master/build-target/opt/hive-conf/hive-site.xml
    at org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf(HiveCatalog.java:273) ~[?:?]
    at org.apache.flink.table.catalog.hive.HiveCatalog.<init>(HiveCatalog.java:184) ~[?:?]
    at org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.createCatalog(HiveCatalogFactory.java:76) ~[?:?]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:435) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1423) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more
Caused by: java.io.IOException: No FileSystem for scheme: file
    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2799) ~[?:?]
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2810) ~[?:?]
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100) ~[?:?]
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849) ~[?:?]
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831) ~[?:?]
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389) ~[?:?]
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) ~[?:?]
    at org.apache.flink.table.catalog.hive.HiveCatalog.createHiveConf(HiveCatalog.java:268) ~[?:?]
    at org.apache.flink.table.catalog.hive.HiveCatalog.<init>(HiveCatalog.java:184) ~[?:?]
    at org.apache.flink.table.catalog.hive.factories.HiveCatalogFactory.createCatalog(HiveCatalogFactory.java:76) ~[?:?]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:435) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1423) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more
 {code}
 

Another test:
 # Put Hadoop JAR inside {{{}lib{}}}, and Hive connector outside, then add Hive connector JAR into the session with {{ADD JAR}} syntax
 # Create a Hive catalog using {{CREATE CATALOG}}

Exception thrown by SQL client:
{code:java}
2022-09-07 15:29:57,362 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Could not execute SQL statement.
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:208) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:634) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:468) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:371) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:328) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:279) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:227) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: Could not execute CREATE CATALOG: (catalogName: [myhive], properties: [{hive-conf-dir=file:///Users/renqs/Workspaces/flink/flink-master/build-target/opt/hive-conf, type=hive}])
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1432) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more
Caused by: org.apache.flink.table.catalog.exceptions.CatalogException: Failed to create Hive Metastore client
    at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:74) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:283) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:84) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:74) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32) ~[?:?]
    at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:300) ~[?:?]
    at org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:211) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1428) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:72) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:283) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:84) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:74) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32) ~[?:?]
    at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:300) ~[?:?]
    at org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:211) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1428) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: org.apache.hadoop.hive.metastore.HiveMetaStoreClient class not found
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.getClass(MetaStoreUtils.java:1710) ~[?:?]
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:131) ~[?:?]
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:89) ~[?:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:72) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:283) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:84) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:74) ~[?:?]
    at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32) ~[?:?]
    at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:300) ~[?:?]
    at org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:211) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1428) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1165) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 10 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 11:27:21 UTC 2022,,,,,,,,,,"0|z18d8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 06:55;lsy;Thanks for report it, I will take a look.;;;","09/Sep/22 11:27;lsy;After some tests, I found it is not easy to resolve the class loading issue of hadoop and hive, so I think we should not use hive catalog by add jar syntax, we should place it in the lib folder. The add jar has some restrictions to hive connector, we should remind the user how to use hive connector correctly in the release note.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.testConcurrentCheckpoint failed with AssertionFailedError,FLINK-29217,13480390,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,hxb,hxb,07/Sep/22 05:59,03/Jan/23 09:15,04/Jun/24 20:41,03/Jan/23 09:15,1.16.0,,,,,,,1.16.1,1.17.0,,,Runtime / Checkpointing,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-09-07T02:00:50.2507464Z Sep 07 02:00:50 [ERROR] org.apache.flink.streaming.runtime.tasks.CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.testConcurrentCheckpoint  Time elapsed: 2.137 s  <<< FAILURE!
2022-09-07T02:00:50.2508673Z Sep 07 02:00:50 org.opentest4j.AssertionFailedError: 
2022-09-07T02:00:50.2509309Z Sep 07 02:00:50 
2022-09-07T02:00:50.2509945Z Sep 07 02:00:50 Expecting value to be false but was true
2022-09-07T02:00:50.2511950Z Sep 07 02:00:50 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-09-07T02:00:50.2513254Z Sep 07 02:00:50 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-09-07T02:00:50.2514621Z Sep 07 02:00:50 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-09-07T02:00:50.2516342Z Sep 07 02:00:50 	at org.apache.flink.streaming.runtime.tasks.CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.testConcurrentCheckpoint(CoordinatorEventsToStreamOperatorRecipientExactlyOnceITCase.java:173)
2022-09-07T02:00:50.2517852Z Sep 07 02:00:50 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-09-07T02:00:50.2518888Z Sep 07 02:00:50 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-09-07T02:00:50.2520065Z Sep 07 02:00:50 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-09-07T02:00:50.2521153Z Sep 07 02:00:50 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-09-07T02:00:50.2522747Z Sep 07 02:00:50 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-09-07T02:00:50.2523973Z Sep 07 02:00:50 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-09-07T02:00:50.2525158Z Sep 07 02:00:50 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-09-07T02:00:50.2526347Z Sep 07 02:00:50 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-09-07T02:00:50.2527525Z Sep 07 02:00:50 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-09-07T02:00:50.2528646Z Sep 07 02:00:50 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-09-07T02:00:50.2529708Z Sep 07 02:00:50 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-09-07T02:00:50.2530744Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-07T02:00:50.2532008Z Sep 07 02:00:50 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-09-07T02:00:50.2533137Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-09-07T02:00:50.2544265Z Sep 07 02:00:50 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-09-07T02:00:50.2545595Z Sep 07 02:00:50 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-09-07T02:00:50.2546782Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-09-07T02:00:50.2547810Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-09-07T02:00:50.2548890Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-09-07T02:00:50.2549932Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-09-07T02:00:50.2550933Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-09-07T02:00:50.2552325Z Sep 07 02:00:50 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-09-07T02:00:50.2553660Z Sep 07 02:00:50 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-09-07T02:00:50.2554661Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-09-07T02:00:50.2555590Z Sep 07 02:00:50 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-09-07T02:00:50.2556454Z Sep 07 02:00:50 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-09-07T02:00:50.2557291Z Sep 07 02:00:50 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-09-07T02:00:50.2558317Z Sep 07 02:00:50 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-09-07T02:00:50.2559462Z Sep 07 02:00:50 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-09-07T02:00:50.2560581Z Sep 07 02:00:50 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-09-07T02:00:50.2562110Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-09-07T02:00:50.2563590Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-09-07T02:00:50.2564992Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-09-07T02:00:50.2566400Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-09-07T02:00:50.2567801Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-09-07T02:00:50.2569115Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-09-07T02:00:50.2570303Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-09-07T02:00:50.2572140Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-09-07T02:00:50.2573462Z Sep 07 02:00:50 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-09-07T02:00:50.2574744Z Sep 07 02:00:50 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-09-07T02:00:50.2576081Z Sep 07 02:00:50 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-09-07T02:00:50.2577397Z Sep 07 02:00:50 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-09-07T02:00:50.2578627Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-09-07T02:00:50.2579773Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-09-07T02:00:50.2580911Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-09-07T02:00:50.2582658Z Sep 07 02:00:50 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40763&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28941,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 26 08:11:52 UTC 2022,,,,,,,,,,"0|z18d5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 05:59;hxb;[~yunfengzhou] Could you help take a look? Thx.;;;","08/Sep/22 06:11;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef]

 ;;;","20/Oct/22 11:42;chesnay;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42281&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","26/Dec/22 08:11;yunfengzhou;According to offline discussions with Dong Lin, a solution to this problem could be to make OperatorCoordinators generate checkpoint barriers and send them to their subtasks. The subtasks would need to align these barriers with the ones they receive from upstream operators or sources, and actually trigger the checkpoint when checkpoint barrier alignment is reached.

The solution mentioned above requires further discussion about the behavior and performance of Flink runtime during checkpoints. Given that currently no subclass of OperatorCoordinator would be affected by this function, it is thus of lower priority and we would temporarily remove the guarantee that this function works correctly under concurrent checkpoints.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rows belong to update rowkind are transfered into delete and insert,FLINK-29216,13480374,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,SpongebobZ,SpongebobZ,07/Sep/22 02:31,16/Sep/22 06:54,04/Jun/24 20:41,16/Sep/22 06:54,1.14.3,,,,,,,,,,,Table SQL / API,,,,0,,,,,"When I declared primary key on the sink table, all UPDATE rows were transfered into DELETE and INSERT rowkind, despite the primay key values were not changed at all. If this is the expected logic then how could I change this behavior ?

 

following is the test code:
{code:java}
CREATE TABLE SOURCE(STUNAME STRING, SUBJECT STRING, SCORE INT) WITH ('connector'='kafka','properties.bootstrap.servers'='node01:9092','topic'='jolinTest','properties.group.id'='test','scan.startup.mode'='latest-offset','format'='debezium-json')

CREATE TABLE SINK(STUNAME STRING, SUBJECT STRING, SCORE INT, PRIMARY KEY(STUNAME) NOT ENFORCED) WITH ('connector'='print')

insert into SINK select * from SOURCE 



when I produced these message to kafka:
{""op"":""c"",""after"":{""STUNAME"":""BOB"",""SUBJECT"":""MATH"",""SCORE"":10}}
{""op"":""u"",""before"":{""STUNAME"":""BOB"",""SUBJECT"":""MATH"",""SCORE"":10},""after"":{""STUNAME"":""BOB"",""SUBJECT"":""MATH"",""SCORE"":16}}


flink output these rows:
+I[BOB, MATH, 10]
-D[BOB, MATH, 10]
+I[BOB, MATH, 16]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 08:15:52 UTC 2022,,,,,,,,,,"0|z18d1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 06:18;lincoln.86xy;[~SpongebobZ] for above query, the sink table with a primary key (STUNAME) which is not the same as the update stream produced from the source (SOURCE table has no primary key definition and is a change log source that has changes, not insert-only messages),  in such case, planner will add a SinkMaterializer(pk='STUNAME') operator to generate the key required by sink, since there's no source key, it will never produce update messages, so only Insert and Delete appears.

If your SOURCE table actually has a same primary key (STUNAME), then the SinkMaterializer operator will not be added, and changes from source table will keep its original rowkind to the sink.

wish this can helps.;;;","07/Sep/22 06:21;lincoln.86xy;more details of the SinkMaterializer can see the option: ['table.exec.sink.upsert-materialize'| https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/config/#table-exec-sink-upsert-materialize];;;","07/Sep/22 07:08;SpongebobZ;Thanks [~lincoln.86xy] , it apparents normal when I set this option to NONE, since this option is used to solve the disorder in distribute system. But if I run my application in single parallelism would it be ok ?;;;","09/Sep/22 08:15;lincoln.86xy;[~SpongebobZ]  yes, single parallelism has no distributed disorder issue, you can set the option to NONE or default value AUTO (which will not enable SinkMaterializer in case of single parallelism or append-only streams);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use config based constructors for flink calcite Converter rules,FLINK-29215,13480356,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Sep/22 22:11,02/Feb/23 09:06,04/Jun/24 20:41,02/Feb/23 09:05,,,,,,,,1.17.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"Since Calcite 1.25 [1] there was introduced {{Config}} based constructors for rules and all other constructors are marked as deprecated
The task is to use {{Config}} based constructors for {{ConverterRule}}.
The process how to do it is described at {{RelRule}}'s javadoc [2].
With non {{ConverterRule}} it will be trickier because in 1.28 there were introduced code generation with immutables and somehow it should be supported for scala rules as well.
[1] https://issues.apache.org/jira/browse/CALCITE-3923
[2] https://github.com/apache/calcite/blob/main/core/src/main/java/org/apache/calcite/plan/RelRule.java#L45-L114",,,,,,,,,,,,,,,,,,,,FLINK-29133,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 09:05:08 UTC 2023,,,,,,,,,,"0|z18cxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:05;Sergey Nuyanzin;Merged as https://github.com/apache/flink/commit/0ce8cb1f2bd13c7cfca8b777972db0cbaa99af46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove usages of deprecated agg.indicator,FLINK-29214,13480351,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,06/Sep/22 21:33,19/Apr/23 07:19,04/Jun/24 20:41,19/Apr/23 07:19,,,,,,,,1.18.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,"In https://issues.apache.org/jira/browse/CALCITE-2944 {{Aggregate#indicator}} was made deprecated and always {{false}}.
So it could be removed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 07:19:31 UTC 2023,,,,,,,,,,"0|z18cwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 07:19;Sergey Nuyanzin;Merged to master [0412329dea41d8f1325b6eefc2ab0914142be007|https://github.com/apache/flink/commit/0412329dea41d8f1325b6eefc2ab0914142be007];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pretty schema/snapshot json file,FLINK-29213,13480319,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,MOBIN,MOBIN,MOBIN,06/Sep/22 16:22,07/Sep/22 02:40,04/Jun/24 20:41,07/Sep/22 02:40,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"pretty schema/snapshot json file,easy to read

before improvement：

  !before.jpeg|width=1669,height=115!

after improvement：
!after.jpeg|width=559,height=435! 
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/22 16:28;MOBIN;after.jpeg;https://issues.apache.org/jira/secure/attachment/13049023/after.jpeg","06/Sep/22 16:28;MOBIN;before.jpeg;https://issues.apache.org/jira/secure/attachment/13049022/before.jpeg",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 02:40:23 UTC 2022,,,,,,,,,,"0|z18cpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 02:40;lzljs3620320;master: 84850d24de119531bd419313ec541cb2a3e92616;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Properly load Hadoop native libraries in Flink docker images,FLINK-29212,13480281,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,rmetzger,rmetzger,06/Sep/22 12:23,09/Sep/22 10:11,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Documentation,flink-docker,,,0,,,,,"On startup, Flink logs:

{code:java}
2022-09-04 12:36:03.559 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
{code}

Hadoop native libraries are used for:
- Compression Codecs (bzip2, lz4, zlib)
- Native IO utilities for HDFS Short-Circuit Local Reads and Centralized Cache Management in HDFS
- CRC32 checksum implementation
(Source: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/NativeLibraries.html)

Resolving this for the docker images we are providing should be easy, remove one unnecessary WARNing and provide performance benefits for some users.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 10:11:36 UTC 2022,,,,,,,,,,"0|z18ch4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 12:52;chesnay;We don't ship Hadoop in our images; how do you propose we solve that?
Which version of the native libraries should we be using?;;;","06/Sep/22 13:17;rmetzger;That's a good point -- I overlooked this. Then this ticket is much more about proper documentation (e.g. how to build a custom image based on the Flink image that points to the right native library.);;;","06/Sep/22 15:20;chesnay;Do you know if there is any risk of conflicts when installing the native hadoop libraries between whatever Hadoop version the user provides and our filesystem plugins?;;;","09/Sep/22 10:11;wangyang0918;Given that the native lib is loaded by JNI, I think it is better to have the same version with Hadoop jars. However, it seems that the native codes in the Hadoop repository changes very rarely.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive 2.3.9 NOTICE is incorrect,FLINK-29211,13480263,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,06/Sep/22 10:32,09/Sep/22 09:41,04/Jun/24 20:41,09/Sep/22 09:40,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27063,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 09:40:31 UTC 2022,,,,,,,,,,"0|z18cd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 09:40;chesnay;master: ec2f3d99ac99059579a5c3e7a4976d9b37d37ceb
1.16: d09f3eb1f427f6f30bedf1919682362c47ca2915;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documented SQL Client setup when using Docker Compose doesn't work,FLINK-29210,13480259,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,06/Sep/22 10:10,07/Sep/22 14:47,04/Jun/24 20:41,07/Sep/22 14:47,,,,,,,,1.17.0,,,,Deployment / Scripts,Table SQL / Client,,,0,pull-request-available,,,,The SQL Client Docker Compose setup at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/ is missing required parameter {{rest.address}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 14:47:29 UTC 2022,,,,,,,,,,"0|z18cc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 14:47;martijnvisser;Fixed in master: 442ab0ce6cbba4e276659db765846f93ab07abdf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to connect HiveServer endpoint with TProtocolException,FLINK-29209,13480256,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Problem,,luoyuxia,luoyuxia,06/Sep/22 10:06,19/Sep/22 12:14,04/Jun/24 20:41,19/Sep/22 12:14,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,,,,0,,,,,"When I try to connect HiveServer endpoint with some BI tools like Apache SuperSet / FineBI / MetaBase, it fails to connect with the following exception:
{code:java}
2022-09-05 20:12:36,179 ERROR org.apache.thrift.server.TThreadPoolServer                   [] - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Missing version in readMessageBegin, old client?
    at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:228) ~[flink-sql-connector-hive-2.3.9_2.12-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27) ~[flink-sql-connector-hive-2.3.9_2.12-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [flink-sql-connector-hive-2.3.9_2.12-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_332]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_332]
    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332] {code}
The jdbc url is ""jdbc:hive2://host:port/default;auth=noSasl"".

But then I try to connect Hive's own HiveServer, the jdbc url is ""jdbc:hive2://host:port/default"",  it works well.

Seems we need extra configuration or adaption to connect to Flink's HiveServer endpoint ",,,,,,,,,,,,,FLINK-28954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 10:06:41 UTC 2022,,,,,,,,,,"0|z18cbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 10:06;luoyuxia;cc [~fsk119] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logs and stdout endpoints not mentioned in Docs or OpenAPI spec,FLINK-29208,13480248,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nkruber,nkruber,06/Sep/22 09:29,11/Nov/22 08:37,04/Jun/24 20:41,,1.15.3,1.16.0,,,,,,,,,,Runtime / REST,,,,0,,,,,"Using Flink's web UI and clicking on ""Stdout"" or ""Logs"" in a JM or TM accesses endpoints {{/jobmanager/logs}} and {{/jobmanager/stdout}} (and similar for TMs) but these don't seem to exist in the [REST API docs|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/] or the [REST API OpenAPI spec|https://nightlies.apache.org/flink/flink-docs-master/generated/rest_v1_dispatcher.yml].

Either these should become some webui-internal APIs (for which no concept exists at the moment), or these endpoints should be added to the docs and spec.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 03 07:56:57 UTC 2022,,,,,,,,,,"0|z18ca0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 07:56;wanglijie;I found that ""{*}{{logs""}}{*} already exists in REST API Docs ([link|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobmanager-logs]) and OpenAPI spec ([link|https://nightlies.apache.org/flink/flink-docs-master/generated/rest_v1_dispatcher.yml#:~:text=components/schemas/EnvironmentInfo%27-,/jobmanager/logs%3A,-get%3A%0A%20%20%20%20%20%20description%3A%20Returns]). But the ""{*}{{stdout}}{*}"" does not, I can help to see why and add it :).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar message eventTime may be incorrectly set to a negative number,FLINK-29207,13480247,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wenbing.shen,wenbing.shen,wenbing.shen,06/Sep/22 09:26,09/Sep/22 10:53,04/Jun/24 20:41,09/Sep/22 10:52,1.15.2,,,,,,,1.15.3,1.16.0,,,Connectors / Pulsar,,,,0,pull-request-available,,,,"We'd better judge that the timestamp is greater than 0, we should skip setting eventTime when timestamp less than or equal to 0, otherwise the pulsar client will throw an exception.

!image-2022-09-06-17-21-46-923.png!

!image-2022-09-06-17-20-27-220.png!",,,,,,,,,,,,,,,,,,,,FLINK-28506,FLINK-27736,,,,,,,,,,,,,,,,,,,"06/Sep/22 09:20;wenbing.shen;image-2022-09-06-17-20-27-220.png;https://issues.apache.org/jira/secure/attachment/13049001/image-2022-09-06-17-20-27-220.png","06/Sep/22 09:21;wenbing.shen;image-2022-09-06-17-21-46-923.png;https://issues.apache.org/jira/secure/attachment/13049000/image-2022-09-06-17-21-46-923.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 10:53:05 UTC 2022,,,,,,,,,,"0|z18c9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 10:52;tison;master via 1e6897c24e3b05e5eecebfc6c5b75a9492011529
release-1.16 via 77500580899c94ccf1adba8f0aeb0d28aeec7a56
release-1.15 via a3adb1343681f3d3b01e7cc373027be63755f0b2;;;","09/Sep/22 10:53;tison;Thanks for your report and fixing [~wenbing.shen]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default Flink configuration contains irrelevant settings,FLINK-29206,13480245,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mbalassi,tashoyan,tashoyan,06/Sep/22 09:21,24/Nov/22 01:03,04/Jun/24 20:41,14/Sep/22 16:42,1.15.2,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"The default configuration of a FlinkDeployment contains the following settings:
{code:yaml}
kubernetes.operator.metrics.reporter.slf4j.interval: 5 MINUTE
kubernetes.operator.metrics.reporter.slf4j.factory.class: org.apache.flink.metrics.slf4j.Slf4jReporterFactory
kubernetes.operator.reconcile.interval: 15 s
kubernetes.operator.observer.progress-check.interval: 5 s
{code}
 
These settings are specific to Flink Kubernetes Operator itself, but not to Flink applications that the Operator runs. Although these settings most probably do not make any influence on a Flink application (they are just ignored), but the configuration is misleading. It would be better to remove these settings from the default configuration of a Flink application.

Actually these settings come from the _defaultConfiguration_ section of the file {_}helm/flink-kubernetes-operator/values.yaml{_}:
{code:yaml}
defaultConfiguration:   ...
  flink-conf.yaml: |+
    # TODO Remove these Operator-specific settings - they are irrelevant to Flink apps
    kubernetes.operator.metrics.reporter.slf4j.factory.class: org.apache.flink.metrics.slf4j.Slf4jReporterFactory
    kubernetes.operator.metrics.reporter.slf4j.interval: 5 MINUTE
    kubernetes.operator.reconcile.interval: 15 s
    kubernetes.operator.observer.progress-check.interval: 5 s
{code}
This piece of configuration is misleading - is it the configuration of the Operator itself or the configuration of applications started by the Operator?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 16:42:10 UTC 2022,,,,,,,,,,"0|z18c9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 19:19;mbalassi;Hi [~tashoyan],

Thanks for bringing this up, these are in fact the configuration values of the operator itself. The helm property value being named defaultConfiguration certainly makes a wrong impression. Let me think about this and come back to you.;;;","14/Sep/22 07:12;morhidi;We used to have two separate config files for these, one default config for Flink and another one for the Operator. It was unified for the sake of simplicity and users can also override certain operator behaviour, that affect how a specific job is being handled. [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#specifying-operator-configuration.] 

cc [~gyfora] ;;;","14/Sep/22 16:42;gyfora;This is an intentional design , and configs never overlap. The user shouldn’t need to know what config is resolved on what component.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumer not respecting Credential Provider configuration for EFO,FLINK-29205,13480240,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,06/Sep/22 08:37,06/Sep/22 16:00,04/Jun/24 20:41,06/Sep/22 16:00,1.15.2,,,,,,,1.15.3,1.16.0,,,Connectors / Kinesis,,,,0,pull-request-available,,,,"Reported in [https://lists.apache.org/thread/xgpk0n59z5wq7kg6j8m8pgy5mcjzvvw5]

 

FlinkKinesisConsumer is not respecting the credential provider configuration. It appears as though the legacy property transform is discarding valid config, this results in the AUTO credential provider being used

- https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/proxy/KinesisProxyV2Factory.java#L65",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 16:00:13 UTC 2022,,,,,,,,,,"0|z18c88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 16:00;dannycranmer;1.15: Merged commit [{{e1cbf9e}}|https://github.com/apache/flink/commit/e1cbf9ed54c04118f3d6f6a89fc23599be0a0bb7] into apache:release-1.15
1.16: Merged commit [{{b0830ce}}|https://github.com/apache/flink/commit/b0830cedb3f0d30a16fc2a312bc7beb6900b2cd9] into apache:release-1.16 
1.17: Merged commit [{{cb32dad}}|https://github.com/apache/flink/commit/cb32dad300652cceb3c909c2e3d495538463a55e] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinITCase#testUncorrelatedScalar fails with Cannot generate a valid execution plan for the given query after calcite update 1.27,FLINK-29204,13480215,13350780,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,06/Sep/22 06:53,07/Oct/22 20:41,04/Jun/24 20:41,07/Oct/22 20:41,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,"{noformat}

org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

FlinkLogicalSink(table=[*anonymous_collect$69*], fields=[b])
+- FlinkLogicalCalc(select=[EXPR$0 AS b])
   +- FlinkLogicalJoin(condition=[true], joinType=[left])
      :- FlinkLogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[\{ 0 }]])
      +- FlinkLogicalValues(type=[RecordType(INTEGER EXPR$0)], tuples=[[\{ 1 }]])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.

    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:93)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:45)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:45)
    at scala.collection.immutable.List.foreach(List.scala:388)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1723)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:860)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1368)
    at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:475)
    at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
    at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:144)
    at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:108)
    at org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.testUncorrelatedScalar(JoinITCase.scala:1061)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=BATCH_PHYSICAL, FlinkRelDistributionTraitDef=any, sort=[].
Missing conversion is FlinkLogicalJoin[convention: LOGICAL -> BATCH_PHYSICAL]
There is 1 empty subset: rel#62551:RelSubset#7.BATCH_PHYSICAL.any.[], the relevant part of the original plan is as follows
62538:FlinkLogicalJoin(condition=[true], joinType=[left])
  62508:FlinkLogicalValues(subset=[rel#62536:RelSubset#5.LOGICAL.any.[0]], tuples=[[\{ 0 }]])
  62510:FlinkLogicalValues(subset=[rel#62537:RelSubset#6.LOGICAL.any.[0]], tuples=[[\{ 1 }]])

Root: rel#62545:RelSubset#9.BATCH_PHYSICAL.any.[]
Original rel:
FlinkLogicalSink(subset=[rel#62506:RelSubset#4.LOGICAL.any.[]], table=[*anonymous_collect$69*], fields=[b]): rowcount = 1.0, cumulative cost = \{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 62517
  FlinkLogicalCalc(subset=[rel#62516:RelSubset#3.LOGICAL.any.[]], select=[EXPR$0 AS b]): rowcount = 1.0, cumulative cost = \{1.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 62520
    FlinkLogicalJoin(subset=[rel#62514:RelSubset#2.LOGICAL.any.[]], condition=[true], joinType=[left]): rowcount = 1.0, cumulative cost = \{1.0 rows, 2.0 cpu, 5.0 io, 0.0 network, 0.0 memory}, id = 62513
      FlinkLogicalValues(subset=[rel#62509:RelSubset#0.LOGICAL.any.[0]], tuples=[[\{ 0 }]]): rowcount = 1.0, cumulative cost = \{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 62508
      FlinkLogicalValues(subset=[rel#62511:RelSubset#1.LOGICAL.any.[0]], tuples=[[\{ 1 }]]): rowcount = 1.0, cumulative cost = \{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 62510

Sets:
Set#5, type: RecordType(INTEGER ZERO)
    rel#62536:RelSubset#5.LOGICAL.any.[0], best=rel#62508
        rel#62508:FlinkLogicalValues.LOGICAL.any.[0](type=RecordType(INTEGER ZERO),tuples=[\{ 0 }]), rowcount=1.0, cumulative cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}
    rel#62548:RelSubset#5.BATCH_PHYSICAL.any.[0], best=rel#62547
        rel#62547:BatchPhysicalValues.BATCH_PHYSICAL.any.[0](type=RecordType(INTEGER ZERO),tuples=[\{ 0 }],values=ZERO), rowcount=1.0, cumulative cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}
Set#6, type: RecordType(INTEGER EXPR$0)
    rel#62537:RelSubset#6.LOGICAL.any.[0], best=rel#62510
        rel#62510:FlinkLogicalValues.LOGICAL.any.[0](type=RecordType(INTEGER EXPR$0),tuples=[\{ 1 }]), rowcount=1.0, cumulative cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}
    rel#62550:RelSubset#6.BATCH_PHYSICAL.any.[0], best=rel#62549
        rel#62549:BatchPhysicalValues.BATCH_PHYSICAL.any.[0](type=RecordType(INTEGER EXPR$0),tuples=[\{ 1 }],values=EXPR$0), rowcount=1.0, cumulative cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}
Set#7, type: RecordType(INTEGER ZERO, INTEGER EXPR$0)
    rel#62539:RelSubset#7.LOGICAL.any.[], best=rel#62538
        rel#62538:FlinkLogicalJoin.LOGICAL.any.[](left=RelSubset#62536,right=RelSubset#62537,condition=true,joinType=left), rowcount=1.0, cumulative cost=\{3.0 rows, 4.0 cpu, 5.0 io, 0.0 network, 0.0 memory}
    rel#62551:RelSubset#7.BATCH_PHYSICAL.any.[], best=null
Set#8, type: RecordType(INTEGER b)
    rel#62541:RelSubset#8.LOGICAL.any.[], best=rel#62540
        rel#62540:FlinkLogicalCalc.LOGICAL.any.[](input=RelSubset#62539,select=EXPR$0 AS b), rowcount=1.0, cumulative cost=\{4.0 rows, 4.0 cpu, 5.0 io, 0.0 network, 0.0 memory}
    rel#62553:RelSubset#8.BATCH_PHYSICAL.any.[], best=null
        rel#62552:BatchPhysicalCalc.BATCH_PHYSICAL.any.[](input=RelSubset#62551,select=EXPR$0 AS b), rowcount=1.0, cumulative cost=\{inf}
Set#9, type: RecordType(INTEGER b)
    rel#62544:RelSubset#9.LOGICAL.any.[], best=rel#62543
        rel#62543:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#62541,table=*anonymous_collect$69*,fields=b), rowcount=1.0, cumulative cost=\{5.0 rows, 5.0 cpu, 5.0 io, 0.0 network, 0.0 memory}
    rel#62545:RelSubset#9.BATCH_PHYSICAL.any.[], best=null
        rel#62546:AbstractConverter.BATCH_PHYSICAL.any.[](input=RelSubset#62544,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]), rowcount=1.0, cumulative cost=\{inf}
        rel#62554:BatchPhysicalSink.BATCH_PHYSICAL.any.[](input=RelSubset#62553,table=*anonymous_collect$69*,fields=b), rowcount=1.0, cumulative cost=\{inf}

Graphviz:
digraph G {
    root [style=filled,label=""Root""];
    subgraph cluster5{
        label=""Set 5 RecordType(INTEGER ZERO)"";
        rel62508 [label=""rel#62508:FlinkLogicalValues\ntype=RecordType(INTEGER ZERO),tuples=[\{ 0 }]\nrows=1.0, cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        rel62547 [label=""rel#62547:BatchPhysicalValues\ntype=RecordType(INTEGER ZERO),tuples=[\{ 0 }],values=ZERO\nrows=1.0, cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        subset62536 [label=""rel#62536:RelSubset#5.LOGICAL.any.[0]""]
        subset62548 [label=""rel#62548:RelSubset#5.BATCH_PHYSICAL.any.[0]""]
    }
    subgraph cluster6{
        label=""Set 6 RecordType(INTEGER EXPR$0)"";
        rel62510 [label=""rel#62510:FlinkLogicalValues\ntype=RecordType(INTEGER EXPR$0),tuples=[\{ 1 }]\nrows=1.0, cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        rel62549 [label=""rel#62549:BatchPhysicalValues\ntype=RecordType(INTEGER EXPR$0),tuples=[\{ 1 }],values=EXPR$0\nrows=1.0, cost=\{1.0 rows, 1.0 cpu, 0.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        subset62537 [label=""rel#62537:RelSubset#6.LOGICAL.any.[0]""]
        subset62550 [label=""rel#62550:RelSubset#6.BATCH_PHYSICAL.any.[0]""]
    }
    subgraph cluster7{
        label=""Set 7 RecordType(INTEGER ZERO, INTEGER EXPR$0)"";
        rel62538 [label=""rel#62538:FlinkLogicalJoin\nleft=RelSubset#62536,right=RelSubset#62537,condition=true,joinType=left\nrows=1.0, cost=\{3.0 rows, 4.0 cpu, 5.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        subset62539 [label=""rel#62539:RelSubset#7.LOGICAL.any.[]""]
        subset62551 [label=""rel#62551:RelSubset#7.BATCH_PHYSICAL.any.[]"",color=red]
    }
    subgraph cluster8{
        label=""Set 8 RecordType(INTEGER b)"";
        rel62540 [label=""rel#62540:FlinkLogicalCalc\ninput=RelSubset#62539,select=EXPR$0 AS b\nrows=1.0, cost=\{4.0 rows, 4.0 cpu, 5.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        rel62552 [label=""rel#62552:BatchPhysicalCalc\ninput=RelSubset#62551,select=EXPR$0 AS b\nrows=1.0, cost=\{inf}"",shape=box]
        subset62541 [label=""rel#62541:RelSubset#8.LOGICAL.any.[]""]
        subset62553 [label=""rel#62553:RelSubset#8.BATCH_PHYSICAL.any.[]""]
    }
    subgraph cluster9{
        label=""Set 9 RecordType(INTEGER b)"";
        rel62543 [label=""rel#62543:FlinkLogicalSink\ninput=RelSubset#62541,table=*anonymous_collect$69*,fields=b\nrows=1.0, cost=\{5.0 rows, 5.0 cpu, 5.0 io, 0.0 network, 0.0 memory}"",color=blue,shape=box]
        rel62546 [label=""rel#62546:AbstractConverter\ninput=RelSubset#62544,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]\nrows=1.0, cost=\{inf}"",shape=box]
        rel62554 [label=""rel#62554:BatchPhysicalSink\ninput=RelSubset#62553,table=*anonymous_collect$69*,fields=b\nrows=1.0, cost=\{inf}"",shape=box]
        subset62544 [label=""rel#62544:RelSubset#9.LOGICAL.any.[]""]
        subset62545 [label=""rel#62545:RelSubset#9.BATCH_PHYSICAL.any.[]""]
    }
    root -> subset62545;
    subset62536 -> rel62508[color=blue];
    subset62548 -> rel62547[color=blue];
    subset62537 -> rel62510[color=blue];
    subset62550 -> rel62549[color=blue];
    subset62539 -> rel62538[color=blue]; rel62538 -> subset62536[color=blue,label=""0""]; rel62538 -> subset62537[color=blue,label=""1""];
    subset62541 -> rel62540[color=blue]; rel62540 -> subset62539[color=blue];
    subset62553 -> rel62552; rel62552 -> subset62551;
    subset62544 -> rel62543[color=blue]; rel62543 -> subset62541[color=blue];
    subset62545 -> rel62546; rel62546 -> subset62544;
    subset62545 -> rel62554; rel62554 -> subset62553;
}
    at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:709)
    at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:390)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:539)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:316)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
    ... 72 more

 

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-06 06:53:13.0,,,,,,,,,,"0|z18c2o:",9223372036854775807,Seems a duplicate of https://issues.apache.org/jira/browse/FLINK-29203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support optimization of Union(all, Values, Values) to Values ",FLINK-29203,13480214,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,06/Sep/22 06:48,21/Oct/22 07:54,04/Jun/24 20:41,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,"This optimization was introduced at https://issues.apache.org/jira/browse/CALCITE-4383 
There are several issues with that
1. now union all tries to do casting to least restrictive type [1] as a result SetOperatorsITCase#testUnionAllWithCommonType fails like below
2. JoinITCase#testUncorrelatedScalar fails like mentioned at https://issues.apache.org/jira/browse/FLINK-29204
3. org.apache.calcite.plan.hep.HepPlanner#findBestExp could be empty for LogicalValues  after such optimization
{noformat}
org.apache.flink.table.planner.codegen.CodeGenException: Incompatible types of expression and result type. Expression[GeneratedExpression(((int) 12),false,,INT NOT NULL,Some(12))] type is [INT NOT NULL], result type is [DECIMAL(13, 3) NOT NULL]

    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.$anonfun$generateResultExpression$2(ExprCodeGenerator.scala:309)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.$anonfun$generateResultExpression$2$adapted(ExprCodeGenerator.scala:293)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateResultExpression(ExprCodeGenerator.scala:293)
    at org.apache.flink.table.planner.codegen.ExprCodeGenerator.generateResultExpression(ExprCodeGenerator.scala:247)
    at org.apache.flink.table.planner.codegen.ValuesCodeGenerator$.$anonfun$generatorInputFormat$1(ValuesCodeGenerator.scala:45)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.codegen.ValuesCodeGenerator$.generatorInputFormat(ValuesCodeGenerator.scala:43)
    at org.apache.flink.table.planner.codegen.ValuesCodeGenerator.generatorInputFormat(ValuesCodeGenerator.scala)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecValues.translateToPlanInternal(CommonExecValues.java:66)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecValues.translateToPlanInternal(BatchExecValues.java:57)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:158)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
    at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:158)
    at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:93)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:92)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1723)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:860)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1368)
    at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:475)
    at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:308)
    at org.apache.flink.table.planner.runtime.batch.table.SetOperatorsITCase.testUnionAllWithCommonType(SetOperatorsITCase.scala:74)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)

{noformat}
[1] https://github.com/apache/calcite/commit/ff4c16d1ea2192435e543fc9572ae3a44decbf79#diff-3be99bddc7edf13dc8198da7425d7e97c97237e3561c263fd74903b4a42d8cd9R2072-R2084",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-06 06:48:08.0,,,,,,,,,,"0|z18c2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClient fails with NPE during start (after calcite update to 1.27),FLINK-29202,13480212,13350780,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,06/Sep/22 06:45,07/Oct/22 20:05,04/Jun/24 20:41,07/Oct/22 20:05,,,,,,,,,,,,Table SQL / Client,Table SQL / Planner,,,0,,,,,"After update to calcite 1.27 sqlclient fails with
{noformat}

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Unexpected exception. This is a bug. Please consider filing an issue.
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:201)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161)
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.calcite.tools.Frameworks$ConfigBuilder.<init>(Frameworks.java:241)
	at org.apache.calcite.tools.Frameworks$ConfigBuilder.<init>(Frameworks.java:217)
	at org.apache.calcite.tools.Frameworks.newConfigBuilder(Frameworks.java:201)
	at org.apache.flink.table.planner.delegation.PlannerContext.createFrameworkConfig(PlannerContext.java:140)
	at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:124)
	at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:121)
	at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:65)
	at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)
	at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.createStreamTableEnvironment(ExecutionContext.java:130)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.createTableEnvironment(ExecutionContext.java:104)
	at org.apache.flink.table.client.gateway.context.ExecutionContext.<init>(ExecutionContext.java:66)
	at org.apache.flink.table.client.gateway.context.SessionContext.create(SessionContext.java:229)
	at org.apache.flink.table.client.gateway.local.LocalContextUtils.buildSessionContext(LocalContextUtils.java:87)
	at org.apache.flink.table.client.gateway.local.LocalExecutor.openSession(LocalExecutor.java:87)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:88)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187)
	... 1 more
Caused by: java.lang.NullPointerException
	at sun.reflect.annotation.TypeAnnotationParser.mapTypeAnnotations(TypeAnnotationParser.java:356)
	at sun.reflect.annotation.AnnotatedTypeFactory$AnnotatedTypeBaseImpl.<init>(AnnotatedTypeFactory.java:139)
	at sun.reflect.annotation.AnnotatedTypeFactory.buildAnnotatedType(AnnotatedTypeFactory.java:65)
	at sun.reflect.annotation.TypeAnnotationParser.buildAnnotatedType(TypeAnnotationParser.java:79)
	at java.lang.reflect.Executable.getAnnotatedReturnType0(Executable.java:640)
	at java.lang.reflect.Method.getAnnotatedReturnType(Method.java:648)
	at org.apache.calcite.util.ImmutableBeans.makeDef(ImmutableBeans.java:146)
	at org.apache.calcite.util.ImmutableBeans.access$000(ImmutableBeans.java:55)
	at org.apache.calcite.util.ImmutableBeans$1.load(ImmutableBeans.java:68)
	at org.apache.calcite.util.ImmutableBeans$1.load(ImmutableBeans.java:65)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.get(LocalCache.java:3951)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974)
	at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958)
	at org.apache.calcite.util.ImmutableBeans.create_(ImmutableBeans.java:95)
	at org.apache.calcite.util.ImmutableBeans.create(ImmutableBeans.java:76)
	at org.apache.calcite.sql.validate.SqlValidator$Config.<clinit>(SqlValidator.java:792)
	... 18 more

Process finished with exit code 1

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 08:00:30 UTC 2022,,,,,,,,,,"0|z18c20:",9223372036854775807,Solved by inclusion org.checkerframework:checker-qual to shade plugin artifact set,,,,,,,,,,,,,,,,,,,"06/Sep/22 08:00;Sergey Nuyanzin;Looks like it is reproducible only with jdk8
it seems very similar to [https://bugs.openjdk.org/browse/JDK-8152174]
after putting checker-qual.jar to classpath cliclient starts normally;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Some tests from flink-sql-client, flink-connector-hive are not compiling in IntellijIdea with new maven",FLINK-29201,13480176,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,05/Sep/22 22:05,07/Sep/22 11:17,04/Jun/24 20:41,07/Sep/22 09:48,,,,,,,,,,,,Tests,,,,0,pull-request-available,,,,"The issue is similar to https://issues.apache.org/jira/browse/FLINK-27640
and is a follow up task for https://github.com/apache/flink/pull/20753#issuecomment-1237062757

the solution is to use maven wrapper in ide settings",,,,,,,,,,,,,,,,,,,,,FLINK-27640,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 09:48:08 UTC 2022,,,,,,,,,,"0|z18bug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 08:02;chesnay;That's not a real solution though, in particular with FLINK-28016 being in the works.
I'd recommend to just bite the bullet and modify .m2/settings.xml.;;;","07/Sep/22 09:48;Sergey Nuyanzin;
Thanks for the feedback
modified .m2/settings.xml also works, thanks.
After also offline discussion with [~mapohl] it was decided to close it as a duplicate of https://issues.apache.org/jira/browse/FLINK-27640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide the way to delay the pod deletion for debugging purpose,FLINK-29200,13480151,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,aitozi,aitozi,05/Sep/22 15:10,15/Sep/22 05:55,04/Jun/24 20:41,15/Sep/22 05:55,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"Currently, if the TaskManager heartbeat timeout the pod will be deleted immediately. It's not very convenient for debugging the internal reason, eg: we can not easily get the core dump files if it's crashed for JVM bugs and so on.

So, I propose to introduce an option to control the delay of the pod deletion, it can be enabled to keep the pod alive for some debugging reason.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 15 05:55:49 UTC 2022,,,,,,,,,,"0|z18bow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 15:11;aitozi;cc [~wangyang0918] do you think it's worth to support it ? ;;;","08/Sep/22 03:59;wangyang0918;I hesitate to do this in the production code. By using the pod template, you could mount a hostpath or PV to persist the logs and even core dump file.

Moreover, if the TaskManager process already crashed, I do not think we could tunnel in the pod and do some debugging.;;;","15/Sep/22 05:55;aitozi;Make sense, Closing it now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support blue-green deployment type,FLINK-29199,13480142,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Do,,olegv,olegv,05/Sep/22 14:01,31/Aug/23 04:01,04/Jun/24 20:41,28/Nov/22 09:35,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Are there any plans to support blue-green deployment/rollout mode similar to *BlueGreen* in the [flinkk8soperator|https://github.com/lyft/flinkk8soperator] to avoid downtime while updating?
The idea is to run a new version in parallel with an old one and remove the old one only after the stability condition of the new one is satisfied (like in [rollbacks|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.1/docs/custom-resource/job-management/#application-upgrade-rollbacks-experimental]).
For stateful apps with {*}upgradeMode: savepoint{*}, this means: not cancelling an old job after creating a savepoint -> starting new job from that savepoint -> waiting for it to become running/one successful checkpoint/timeout or something else -> cancelling and removing old job.",Kubernetes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 04:01:38 UTC 2023,,,,,,,,,,"0|z18bmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 19:22;mbalassi;This would be great, [~olegv]. The challenge that I see with this in the general case is that after you start the new job for a short while you have both applications running, in theory producing to the same sinks and consequently effectively temporarily duplicating the output. How would you suggest to handle this?;;;","18/Sep/22 19:18;gyfora;Also this will be quite tricky to implement with the current architecture and while using Flink Kubernetes HA. 

It would mean we have to dynamically change and juggle clusterids which are currently fixed and greately simplify the logic.;;;","18/Sep/22 19:19;gyfora;This is something that could be built on top of the current logic in a custom way using manual savepoint triggering, forking a new CR and tracking the status of the new deployment before deleting the old one.;;;","26/Sep/22 10:25;olegv;[~bamrabi] Yep, running both blue and green versions at the same time, will produce duplicated output and this is expected behavior and should be handled by the consumer. There will be a race between blue and green, so a consumer, for example, could pick first response and ignore others.;;;","26/Sep/22 11:06;olegv;[~gyfora], thanks for your suggestion about custom implementation, not sure if I understand the problem with *cluster-id* that you described (I haven't tried this operator yet).
Are you saying that currently *cluster-id* is picked from the configuration and used as is without, for example, adding a hash of meaningful fields from CR or something like that?
Does it mean that creating 2 similar *FlinkDeployment* objects in the same namespace with different object names and *image* fields, configured to use Kubernetes HA will not work properly and will interfere with each other (will use the same configmaps?).;;;","26/Sep/22 11:11;gyfora;What I was saying is that cluster-id which must be unique within a namespace is currently tied to the name of the FlinkDeployment.

If you create 2 flinkdeployments, they will have different names so different cluster-ids. But since we cannot deploy 2 jobs using the same cluster-id for a single FlinkDeployment we would have to completely rework this logic whihc would have many implications to last-state recovery , HA etc.;;;","27/Sep/22 09:32;olegv;I see, thanks!;;;","28/Nov/22 09:35;gyfora;We should revisit this if we see a need in the future;;;","29/Aug/23 07:41;nfraison.datadog;We are looking in supporting this deployment for our Flink users:
 * Is there any plan to support Blue Green deployment within the flink operator?
 * Are some of you already performing such deployment relying on an external custom solution? If yes I would be happy to discuss it if possible;;;","30/Aug/23 12:17;wangm92;[~nfraison.datadog] I'm curious, is BlueGreen deployment acceptable in your scenario?
At a certain moment, two jobs (new job and old job) are run at the same time. If the data source is mq, it will cause consumer offset confusion, affect lag monitoring, and also cause double data output.;;;","30/Aug/23 12:30;nfraison.datadog;This is indeed a concern we have as we rely on kafka data sources.

Currently we are thinking on relying on 2 different kafka consumer group (one blue and one green) for those 2 jobs. I need to validate that resuming the green deployment from blue checkpoint with a different consumer group can be done.

For the double data output we can tolerate some duplicate for those specific jobs;;;","31/Aug/23 04:01;wangm92;[~nfraison.datadog] I think [double data output] requires corresponding processing by the job and is not a more general solution. In our internal practice, we will strictly ensure that at any time, only one job is running in a Region, because we believe that job double-running will have a relatively great impact on the business, and the downstream of the job all need to deal with duplicate data. However, we have also encountered some scenarios that require the ability to release in grayscale. We are currently exploring the ability to roll upgrading new versions of jobs at Region granularity.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RetryExtension doesn't make the test fail if the retries are exhausted,FLINK-29198,13480136,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,mapohl,mapohl,05/Sep/22 13:30,04/Nov/22 15:40,04/Jun/24 20:41,04/Nov/22 13:24,1.15.0,1.16.0,1.17.0,,,,,1.15.3,1.16.1,1.17.0,,Tests,,,,0,pull-request-available,starter,,,"FLINK-24627 introduced retry functionality for JUnit5-based tests. It appears that the retry mechanism doesn't have the desired behavior: If the retries are exhausted without the test ever succeeding will result in the test being ignored. I would expect the test to fail in that case. Otherwise, a CI run would succeed without anyone noticing the malfunctioning of the test.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24627,,FLINK-29891,,,,,,,,,,,,FLINK-29245,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 04 13:24:08 UTC 2022,,,,,,,,,,"0|z18blk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 08:34;rskraba;{{@RetryOnFailure}} works as as expected, but I can reproduce this for {{@RetryOnException}} :
{code:java}
@TestTemplate
@RetryOnException(times = 3 exception = IllegalArgumentException.class)
void testFailForever() {
    throw new IllegalArgumentException();
} {code}
This test never passes, but it never fails either.  The test case is aborted and is counted as ""skipped"".  According to the [Javadoc|https://github.com/apache/flink/blob/8a4538ae4551713591bbe92fd3ec46b1f212e36e/flink-test-utils-parent/flink-test-utils-junit/src/main/java/org/apache/flink/testutils/junit/RetryOnException.java#L47], it should fail after 4 tries.

It looks like the new RetryExtension explicitly does not fail on the given exception class ever. I think it might be surprising to some developers!

Can you assign this to me?

 ;;;","09/Sep/22 10:46;mapohl;Thanks for picking it up and clarifying the actual problem, [~rskraba]. I assigned the ticket to you. Thanks to your finding, I also realized that we could do some cleanup around the JavaDoc and {{{}RetryRule{}}}. I created FLINK-29245 to cover that.;;;","04/Nov/22 13:24;mapohl;master: 733969001036b1263bd798433e229816acff1658
1.16: f4fe35fdf179683f565ae0ee6bc374ed6e85c605
1.15: b68f37cad6dc5207692badfe970ae803143054f4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate the IDE setup documentation into Chinese,FLINK-29197,13480133,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,05/Sep/22 13:16,07/Sep/22 09:42,04/Jun/24 20:41,07/Sep/22 09:42,,,,,,,,,,,,chinese-translation,Documentation,,,0,starter,,,,"This is a follow up issue for the changes done within https://github.com/apache/flink/pull/20753.
Currently for Chinese page the was copied an English version. It would be nice to have a proper translation there",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-05 13:16:12.0,,,,,,,,,,"0|z18bkw:",9223372036854775807,closed in favor of https://issues.apache.org/jira/browse/FLINK-27640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-python NOTICE is incorrect,FLINK-29196,13480123,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxb,hxb,hxb,05/Sep/22 12:01,06/Sep/22 09:22,04/Jun/24 20:41,06/Sep/22 09:22,1.16.0,,,,,,,1.16.0,,,,Release System,,,,0,pull-request-available,,,,"We should check the licensing of the 1.16 release.

[https://cwiki.apache.org/confluence/display/FLINK/Licensing]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 09:22:47 UTC 2022,,,,,,,,,,"0|z18bio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 13:53;chesnay;There are numerous issues in the flink-python NOTICE; apart from that we're good. Reusing this ticket for resolving the python issues.

The list below may not be complete.

Not listed as bundled:
- io.netty:netty-buffer:4.1.70.Final
- io.netty:netty-common:4.1.70.Final

Listed but not bundled:
- org.apache.arrow:arrow-memory:5.0.0

No record of what beam-vendor-bytebuddy bundles
(net.bytebuddy:byte-buddy:1.11.0)

vendor-grpc issues:
- wrong protobuf version (3.19.2)
- missing guava (30.1.1-jre)
- wrong opencensus version (0.28.0)
- wrong google-auth version (0.22.2)
- wrong gson version (2.8.9)
- wrong netty version (4.1.63.Final)
- wrong netty tcnative version (2.0.38.Final)
- jzlib is not bundled
- bouncycastle is not bundled
- lz4 is not bundled
- javanano is not bundled
- compress-lzf is not bundled
- lzma-java is not bundled;;;","06/Sep/22 01:49;hxb;Thanks [~chesnay] for the check, I will fix the NOTICE problem in flink-python asap;;;","06/Sep/22 09:22;hxb;Merged into master via ed29940ace7f1752ab5f4312b87f999ee74c1131

Merged into release-1.16 via e22bfd51a898e9c98a0d656e6250a38aee700591;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose the metrics of LastCompletedCheckpointId ,FLINK-29195,13480117,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,stupid_pig,stupid_pig,stupid_pig,05/Sep/22 11:18,20/Oct/22 07:47,04/Jun/24 20:41,20/Oct/22 07:47,,,,,,,,1.17.0,,,,Runtime / Checkpointing,Runtime / Metrics,,,0,pull-request-available,,,," May be we could expose the LastCompletedCheckpointId metric.

With this metric, we could monitor for flink application and get notice when the job restoring from an unexpect checkpoint, instead of extract checkpointId from the metric lastCheckpointExternalPath",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 07:47:38 UTC 2022,,,,,,,,,,"0|z18bhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 11:25;stupid_pig;I'd like to take this ticket;;;","06/Sep/22 02:40;masteryhx;Actually, you could get the latest checkpoint statistics from the REST API[1] (including latest checkpoint id).
Does it meet your needs ?
[1]https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/ops/rest_api/#jobs-jobid-checkpoints;;;","06/Sep/22 14:18;stupid_pig;Thanks for your tips!  We could do get the  latest checkpoint id from the REST API. 

However, in a general scenior, we may collect metrics of  flink application  by various metric reporters(like jmx/prometheus) and build some alert basing on them. In this case  maybe we can't report checkpoint id using REST API directly.

[~masteryhx] ;;;","07/Sep/22 04:52;masteryhx;[~yunta] WDYT?;;;","07/Sep/22 11:18;yunta;I think this idea is okay, [~stupid_pig] please go ahead.;;;","13/Sep/22 11:49;ym;I think it is fine/useful to report last completed checkpoint id

Please see my comments in the PR.;;;","20/Oct/22 07:47;ym;merged commit [{{667732e}}|https://github.com/apache/flink/commit/667732e0af117db11fcd0813407a4075012df9b5] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Logging for ResourceListener,FLINK-29194,13480108,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,05/Sep/22 10:01,24/Nov/22 01:03,04/Jun/24 20:41,07/Sep/22 19:02,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"For auditing/debugging purposes the operator needs a way to report the emitted events / status updates in the logs:

{{Event  | Info    | SpecChanged     | UPGRADE change(s) detected (FlinkDeploymentSpec[image=flink:1.15,restartNonce=<null>] differs from FlinkDeploymentSpec[image=flink:1.15asdf,restartNonce=123]), starting reconciliation.}}
{{Event  | Info    | Suspended       | Suspending existing deployment.}}
{{Status | Info    | UPGRADING       | The resource is being upgraded }}
{{Status | Info    | UPGRADING       | The resource is being upgraded }}
{{Event  | Info    | Submit          | Starting deployment}}
{{Status | Info    | DEPLOYED        | The resource is deployed/submitted to Kubernetes, but it’s not yet considered to be stable and might be rolled back in the future }}
{{Status | Info    | DEPLOYED        | The resource is deployed/submitted to Kubernetes, but it’s not yet considered to be stable and might be rolled back in the future }}
{{Event  | Info    | StatusChanged   | Job status changed from RECONCILING to CREATED}}
{{Status | Info    | DEPLOYED        | The resource is deployed/submitted to Kubernetes, but it’s not yet considered to be stable and might be rolled back in the future }}
{{Event  | Info    | StatusChanged   | Job status changed from CREATED to RUNNING}}
{{Status | Info    | STABLE          | The resource deployment is considered to be stable and won’t be rolled back }}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 19:02:33 UTC 2022,,,,,,,,,,"0|z18bfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 10:19;gyfora;I think we should not tie debug logging to any listener/plugin mechanism. Simply add the missing debug logs to the recorder components.

Otherwise if the user uses a different listener the logging behavior changes which is a little weird interaction 

 ;;;","05/Sep/22 12:26;morhidi;I've been thinking about this too [~gyfora] and started adding the logs in EventRecorder/StatusRecorded. But realised you cannot simply enable/disable/separate log entries for this specific component, unless it is in a single class. So eventually ended up using an implementation for the ResourceListener that is always registered, regardless of other listeners. where the logs can be controlled by:

```

logger.event.name = org.apache.flink.kubernetes.operator.listener.LoggingResourceListener
logger.event.level = WARN

```;;;","05/Sep/22 12:28;morhidi;This is how the Log4jReporter works, but in this case it cannot be configured and always runs. I can turn the class into a utility class if you think it works better in this case.;;;","05/Sep/22 23:40;gyfora;I don’t really see why it’s important to be able to configure this in a single setting. I think people are used to configured logging on a per class level and having a somewhat arbitrary utility class to capture logging seems to be a little confusing. 
For me it looks natural that the statusrecorder logs independently of the event recorder and you control them independently.;;;","06/Sep/22 11:32;morhidi;It's more about finding these messages related to a specific resource easily by tailing/greping in prod logs.;;;","07/Sep/22 19:02;mbalassi;[{{b7fc9b7}}|https://github.com/apache/flink-kubernetes-operator/commit/b7fc9b71458ca72364431635007945816a4e107f] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Add Transformer for Normalizer,FLINK-29193,13480102,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,05/Sep/22 09:31,19/Apr/23 02:14,04/Jun/24 20:41,19/Apr/23 02:14,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,, Add Transformer for Normalizer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:54:00 UTC 2023,,,,,,,,,,"0|z18beg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:54;lindong;Merged to apache/flink-ml master branch 6cda7c374fac2e09f2a0e897bd2038ac532624a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add completeness test for built-in functions between Java and Python Table API,FLINK-29192,13480096,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pemide,dianfu,dianfu,05/Sep/22 08:55,11/Mar/24 12:43,04/Jun/24 20:41,,,,,,,,,1.20.0,,,,API / Python,,,,0,,,,,"I have found many times that users are adding built-in functions in the Java Table API, however, not aligning them in Python Table API. We should add completeness tests between the Java and Python Table API to force aligning them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 07:39:45 UTC 2022,,,,,,,,,,"0|z18bd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 07:05;pemide;[~dianfu] Could I take this ticket?;;;","06/Sep/22 07:39;dianfu;[~pemide] Thanks. Have assigned it to you~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect can't get value for the variables set by  set command,FLINK-29191,13480092,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,05/Sep/22 08:11,21/Sep/22 01:47,04/Jun/24 20:41,21/Sep/22 01:47,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,"When using Hive dialect, we can use 
{code:java}
set k1=v1;
{code}
to set variable to Flink's table config.

But if we want the get the value for `k1` by using 
{code:java}
set k1;
{code}
we will get nothing.

The reason is Hive dialect won't lookup the vairable in Flink's table config.

To fix it, we also need to lookup in Flink's table config.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 01:47:39 UTC 2022,,,,,,,,,,"0|z18bc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 01:47;jark;Fixed in 
 - master: 64c550c67c2d580f369dfaa6ff48e2e6816c6fcd
 - release-1.16: d4d855a3c08733afac935d87df6544f0811aef84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink hive left join mysql error,FLINK-29190,13480084,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kcz,kcz,05/Sep/22 07:22,05/Sep/22 08:53,04/Jun/24 20:41,05/Sep/22 08:53,1.14.0,,,,,,,1.15.2,,,,Table SQL / Runtime,,,,0,,,,,"If I remove the custom function, it is working fine.

select * from hive_table left join mysql_table on hive_table.id=mysql_table .id where

mysql_table .name='kcz' and hive_table.dt=date_sub(CURRENT_DATE, 1);

 

date_sub is a user function.

public class DateSubUDF extends ScalarFunction {


public String eval(LocalDate date, int day) {
return DateUtils.getStringByLocalDate(date.minusDays(day));
}
}

 

CREATE TABLE if not exists car_info
(
vin STRING,
battery_factory STRING
) WITH (
'connector' = 'jdbc',
'url' = 'url',
'table-name' = 'car_info',
'username' = 'name',
'password' = 'password'
);

hive table does not use flink's table building syntax, using the hive native table.

 

this is a error log.

 

 

org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Could not instantiate generated class 'PartitionPruner$9'
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
    at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)
    at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)
    at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)
    at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1761)
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
    at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'PartitionPruner$9'
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:75)
    at org.apache.flink.table.planner.plan.utils.PartitionPruner$.prunePartitions(PartitionPruner.scala:112)
    at org.apache.flink.table.planner.plan.utils.PartitionPruner.prunePartitions(PartitionPruner.scala)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.lambda$onMatch$3(PushPartitionIntoTableSourceScanRule.java:163)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogWithoutFilterAndPrune(PushPartitionIntoTableSourceScanRule.java:373)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionFromCatalogAndPrune(PushPartitionIntoTableSourceScanRule.java:351)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.readPartitionsAndPrune(PushPartitionIntoTableSourceScanRule.java:303)
    at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.onMatch(PushPartitionIntoTableSourceScanRule.java:171)
    at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:333)
    at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:542)
    at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
    at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
    at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
    at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
    at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:63)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1$$anonfun$apply$1.apply(FlinkGroupProgram.scala:60)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:60)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram$$anonfun$optimize$1.apply(FlinkGroupProgram.scala:55)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.immutable.Range.foreach(Range.scala:160)
    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:163)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:77)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:300)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:183)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:805)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1274)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
    at com.hycan.bigdata.utils.SqlUtils.callCommand(SqlUtils.java:49)
    at com.hycan.bigdata.job.SchemaJob.main(SchemaJob.java:94)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
    ... 11 more
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:69)
    ... 72 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
    ... 74 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
    ... 77 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 11, Column 30: Cannot determine simple type name ""com""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873)
    at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6499)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6494)
    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855)
    at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497)
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494)
    at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494)
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
    ... 83 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 08:52:11 UTC 2022,,,,,,,,,,"0|z18bag:",9223372036854775807,https://issues.apache.org/jira/browse/FLINK-24761,,,,,,,,,,,,,,,,,,,"05/Sep/22 08:03;jark;Is it caused by FLINK-24761? Could you try a newer version? ;;;","05/Sep/22 08:52;kcz;tks jark,use 1.15.2 ok.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass KeyValue to MergeFunction,FLINK-29189,13480074,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Sep/22 06:04,06/Sep/22 02:24,04/Jun/24 20:41,06/Sep/22 02:24,,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"We should pass KeyValue to MergeFunction, give MergeFunction more capabilities, such as throwing an exception when it sees an unreasonable RowKind.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27626,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 02:24:15 UTC 2022,,,,,,,,,,"0|z18b88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 02:24;lzljs3620320;master: 3b149792a9b106a748fe60d787c14daf55635d43;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The option `sql-gateway.endpint.hiveserver2.catalog.hive-conf-dir` should be requried,FLINK-29188,13480045,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,fsk119,fsk119,05/Sep/22 02:22,24/Oct/22 02:05,04/Jun/24 20:41,24/Oct/22 02:05,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,0,,,,,,,,,,,,,,,,,,FLINK-28954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-05 02:22:52.0,,,,,,,,,,"0|z18b1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn spotless:apply to format code in Windows,FLINK-29187,13479981,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hjw,hjw,03/Sep/22 14:23,11/Nov/22 08:37,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,Build System,Build System / CI,,,0,,,,,"When use mvn spotless:apply command to format flink code in Windows, the EOL of file will be replced to CRLF.

I think we can add a `.gitattributes` file to force the code eol to be LF.

The realtion issue is [EOL CRLF vs LF|https://github.com/diffplug/spotless/issues/39]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 03 14:26:32 UTC 2022,,,,,,,,,,"0|z18ao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Sep/22 14:26;hjw;I'd like to get a ticket to fix it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store summarized results in internal nodes of BKD for time series points,FLINK-29186,13479896,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Abandoned,,rishabhmaurya,rishabhmaurya,02/Sep/22 13:58,02/Sep/22 14:13,04/Jun/24 20:41,02/Sep/22 14:13,,,,,,,,,,,,API / Core,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-02 13:58:01.0,,,,,,,,,,"0|z18a54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to execute USING JAR in Hive Dialect,FLINK-29185,13479826,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,fsk119,fsk119,02/Sep/22 09:11,21/Sep/22 01:48,04/Jun/24 20:41,21/Sep/22 01:48,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 09:12:30 UTC 2022,,,,,,,,,,"0|z189pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 06:06;luoyuxia;Only happens for `create temporary function xxx using jar`. The reason is haven't registered such resource when creating  temporary function.

To fix it, we need to register such resource.;;;","20/Sep/22 09:12;jark;Fixed in
 - master: 3994788892fc761cf0c2fd09f362d4dab8f14c61
 - release-1.16: 82ab2918e992f747043dbe49d900b36fe28df282;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Gateway should remove the downloaded resources,FLINK-29184,13479820,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,02/Sep/22 09:05,08/Sep/22 11:42,04/Jun/24 20:41,08/Sep/22 06:28,1.16.0,,,,,,,1.16.0,,,,Table SQL / Gateway,,,,0,,,,,SessionContext doesn't close the `ResourceManager`.,,,,,,,,,,,,,FLINK-28953,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 06:28:54 UTC 2022,,,,,,,,,,"0|z189o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 06:28;fsk119;Merged into master: bd1d97391be408885dc8df35c8a83770fe7e0c38

Merged into release-1.16: 219b78d840cedbf71cc1345cf1a182a931456a17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink job ended before window time up while using bounded stream source,FLINK-29183,13479799,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,landlord,landlord,02/Sep/22 07:45,20/Oct/22 10:57,04/Jun/24 20:41,20/Oct/22 10:57,1.15.2,,,,,,,,,,,API / DataStream,,,,0,,,,,"!image-2022-09-02-15-35-18-590.png|width=574,height=306!

 

when i use a long time window while using bounded stream in STREAMING mode，the job shutdown before the window has been triggered and then i have nothing to print.

if i use BATCH mode with the same code,  it will work. But the batch shuffle is so expensive so i still prefer STREAMING. 

Is there any method could fix this problem? Or should i have mistake in my code?

Looking forward to your reply.  Thanks in advance.",,,,,,,,,,FLINK-18647,,,,,,,,,,,,,,,FLINK-18647,,,,,,,,,,,,,,,"02/Sep/22 07:33;landlord;image-2022-09-02-15-33-08-459.png;https://issues.apache.org/jira/secure/attachment/13048893/image-2022-09-02-15-33-08-459.png","02/Sep/22 07:35;landlord;image-2022-09-02-15-35-18-590.png;https://issues.apache.org/jira/secure/attachment/13048892/image-2022-09-02-15-35-18-590.png","06/Sep/22 10:09;landlord;image-2022-09-06-18-09-40-132.png;https://issues.apache.org/jira/secure/attachment/13049002/image-2022-09-06-18-09-40-132.png","06/Sep/22 10:09;landlord;image-2022-09-06-18-09-43-745.png;https://issues.apache.org/jira/secure/attachment/13049003/image-2022-09-06-18-09-43-745.png","06/Sep/22 10:10;landlord;image-2022-09-06-18-10-11-848.png;https://issues.apache.org/jira/secure/attachment/13049004/image-2022-09-06-18-10-11-848.png","06/Sep/22 10:11;landlord;image-2022-09-06-18-11-02-915.png;https://issues.apache.org/jira/secure/attachment/13049005/image-2022-09-06-18-11-02-915.png",,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 20 10:57:53 UTC 2022,,,,,,,,,,"0|z189jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 06:54;Weijie Guo;[~landlord] I think this is the semantic different between streaming and batch, I'm not sure if there are configuration options that can control this behavior, but if you just want to use pipelined shuffle in batch mode, you can configure execution.batch-shuffle-mode: ALL_ EXCHANGES_ PIPELINED.;;;","06/Sep/22 10:14;landlord;!image-2022-09-06-18-09-43-745.png|width=464,height=262!

when i use batch mode and ALL_ EXCHANGES_ PIPELINED, it seems like the ALL_ EXCHANGES_ PIPELINED dosen't work.

this last print also need to wait all the first map finished. like this:

!image-2022-09-06-18-10-11-848.png|width=303,height=267!

but when i use STREAM mode ,  it behaves like this:

!image-2022-09-06-18-11-02-915.png|width=355,height=217!

 

[~Weijie Guo] ;;;","07/Sep/22 13:48;Weijie Guo;[~landlord] First of all, the phenomenon you observed is in line with expectations, because this is the difference between batch and streaming semantics. In batch mode, print needs to wait and sort all inputs, and then output them together. In streaming mode, it is not required. Second, BatchShuffleMode only affects the selection of your shuffle mode, which has nothing to do with operator logic and semantics.;;;","08/Sep/22 01:33;landlord;[~Weijie Guo] I know what do u mean.  But let's look at the original purpose of mine.    {*}I need the flink job ended after all window time up while using bounded stream source{*}.    The bounded stream may be very large, so the ""wait and sort"" in the batch mode will occupy large memory which the environment can't afford. Or it will flush memory to disk which will take a long time and high IO.   Neither is satisfactory.     Hope u can understand my troubles and thanks a lot for your answer.;;;","20/Oct/22 07:37;gaoyunhaii;Hi [~landlord] , this is indeed a known issue that currently Flink will ignores all the register times at the end of bounded stream. Recently we have some discussion on this issue under https://issues.apache.org/jira/browse/FLINK-18647.   

We are planning to allow users to specify how to deal with the timers at the end of streams. I'll open a FLIP and discussion for that soon. 

If you feel ok I'll first close this issue and let's unify the discussion in FLINK-18647 and the mail list. ;;;","20/Oct/22 10:57;landlord;[~gaoyunhaii]  Thanks for tell me this!   I have just looked at the discussion in FLINK-18647 . It is exactly the problem i had.  I will close this issue myself and unify the discussion in FLINK-18647 and the mail list.  Thx!;;;","20/Oct/22 10:57;landlord;unify the discussion in FLINK-18647 and the mail list. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SumAggFunction redundant computations,FLINK-29182,13479784,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,02/Sep/22 06:42,05/Sep/22 04:16,04/Jun/24 20:41,05/Sep/22 04:16,1.16.0,,,,,,,1.17.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"{code:java}
// code placeholder
public Expression[] accumulateExpressions() {
    return new Expression[] {
        /* sum = */ ifThenElse(
                isNull(operand(0)),
                sum,
                ifThenElse(
                        isNull(operand(0)),
                        sum,
                        ifThenElse(isNull(sum), operand(0), adjustedPlus(sum, operand(0)))))
    };
} {code}
it exists redundant computations",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 04:16:43 UTC 2022,,,,,,,,,,"0|z189g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 04:16;jark;Fixed in master: b98534cafd03cde47ecd6d54248a397540dffa71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.system can be congiured by dynamic options,FLINK-29181,13479781,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Sep/22 06:38,27/Sep/22 02:50,04/Jun/24 20:41,02/Sep/22 07:46,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"Now log.system default value is null, it can not be configured by dynamic option.
We can let default value is 'none'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 02 07:46:10 UTC 2022,,,,,,,,,,"0|z189fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 07:46;lzljs3620320;master: 39a6f9b4c04ba0354cb78c5702b1c0745616c807
release-0.2: 7d6be436f74a70ba7446986c1163b9b3f98c204c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show subtask metrics as default in vertex detail,FLINK-29180,13479767,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,,,02/Sep/22 03:01,02/Sep/22 09:13,04/Jun/24 20:41,02/Sep/22 09:13,,,,,,,,1.16.0,,,,Runtime / Web Frontend,,,,0,pull-request-available,ui,user_experience,,"Switch the order of the following tabs and show subtask metrics as default.

!image-2022-09-02-11-00-25-312.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/22 03:00;image-2022-09-02-11-00-25-312.png;https://issues.apache.org/jira/secure/attachment/13048885/image-2022-09-02-11-00-25-312.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 02 09:12:51 UTC 2022,,,,,,,,,,"0|z189cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 09:12;junhan#1;master (1.16): 121e63e8702e449559dc548fc29b4e9c4ce535e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Temporal Table Function"" page of ""Streaming Concepts"" into Chinese",FLINK-29179,13479764,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,ChunJi,ChunJi,02/Sep/22 02:31,17/Jan/24 04:08,04/Jun/24 20:41,16/Jan/24 11:01,1.15.3,,,,,,,1.19.0,,,,chinese-translation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/concepts/temporal_table_function/]

The markdown file is located in docs/content.zh/docs/dev/table/concepts/temporal_table_function.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink/pull/20741,,,,,,,,,,9223372036854775807,,,,Thu May 18 09:55:33 UTC 2023,,,,,,,,,,"0|z189bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 09:16;ChunJi;hi,[~jark] ,i have done this translation,please assign to me,thanks;;;","05/Oct/22 03:18;lyy9001111@163.com;{{{}hi,[~jark] ,if i want to translate template table function page of streaming concepts{}}},need you to add a new sub-task and assign to me or i ceate a new issue.it's my first time try to contribe,thank you.;;;","18/May/23 09:55;niliushall;Hi, I want to translate this doc. May you assign it to me? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink - on -yarn out-off-memory,FLINK-29178,13479759,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zhangyang93,zhangyang93,02/Sep/22 02:06,07/Sep/22 06:50,04/Jun/24 20:41,07/Sep/22 06:50,1.14.2,,,,,,,,,,,API / DataStream,Deployment / YARN,,,0,,,,,"{color:#202124}hello,{color}
My task has an ""Out Of Memory"" exception error after running for 3 hours. The version of my cluster is 1.14.2, the memory allocation of TaskManager is 2g, and the attachment is the log file of jvm's heap memory analysis, which involves the flink framework. So asking for help, thanks!
[^flink.gc_05.log.0 (2).current]","|thread num：24*2|

|Intel(R) Xeon(R) CPU           X5650  @ 2.67GHz|

|mem: 64GB|

|1333 MHz|

|disk size：1497G|

|RW：6.0 Gb/s(600M/s)|

|BCM5709*4；|

|JAVA build 1.8.0_281-b09|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/22 02:03;zhangyang93;flink.gc_05.log.0 (2).current;https://issues.apache.org/jira/secure/attachment/13048882/flink.gc_05.log.0+%282%29.current","02/Sep/22 02:03;zhangyang93;job_manager.txt;https://issues.apache.org/jira/secure/attachment/13048881/job_manager.txt",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,Wed Sep 07 06:49:21 UTC 2022,,,,,,,,,,"0|z189ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 06:50;martijnvisser;[~zhangyang93] This question is better suited for the Flink mailing list or Slack channel, since Jira is reserved for (confirmed) bugs. Can you ask your question there?;;;","07/Sep/22 06:49;zhangyang93;Thanks for the guidance, I have sent an email to discuss. I will close the question;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shade the org.apache.commons in flink-sql-connector-hive to avoid the class conflict,FLINK-29177,13479755,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,02/Sep/22 01:38,15/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,stale-assigned,,,"Reported by user https://lists.apache.org/thread/zbyz28b8dfqvb9ppb9bbtw8zp1ql72cp

We should shade the class to avoid class conflict.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 22:35:03 UTC 2023,,,,,,,,,,"0|z1899s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add python support for ChiSqTest,FLINK-29176,13479754,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhangzp,zhangzp,02/Sep/22 01:25,12/Oct/22 10:11,04/Jun/24 20:41,12/Oct/22 09:05,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,,,,,"We already implemented ChiSqTest in java. In this pr, we aim to add python source/test/example for ChiSqTest",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 12 09:03:48 UTC 2022,,,,,,,,,,"0|z1899k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 13:06;zxcoccer;hi，I would like to deal with it. Can you assign this ticket to me ?
 ;;;","12/Oct/22 09:03;zhangzp;Hi [~zxcoccer] , sorry for the late reply.

This Jira is duplicate with https://issues.apache.org/jira/browse/FLINK-28571.

For other possisble Flink ML JIRAs, please refer to

https://issues.apache.org/jira/browse/FLINK-29604?filter=-1&jql=project%20%3D%20FLINK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20resolution%20%3D%20Unresolved%20AND%20component%20%3D%20%22Library%20%2F%20Machine%20Learning%22%20order%20by%20updated%20DESC ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add documents for KBinsDiscretizer, VectorIndexer, Tokenizer, RegexTokenizer",FLINK-29175,13479752,13479751,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,02/Sep/22 00:59,07/Sep/22 01:41,04/Jun/24 20:41,07/Sep/22 01:41,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-02 00:59:57.0,,,,,,,,,,"0|z18994:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for ML algorithms,FLINK-29174,13479751,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,02/Sep/22 00:57,10/Jan/23 04:24,04/Jun/24 20:41,10/Jan/23 04:24,,,,,,,,,,,,Library / Machine Learning,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-02 00:57:26.0,,,,,,,,,,"0|z1898w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade curator,FLINK-29173,13479675,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,01/Sep/22 12:45,30/Jan/23 11:48,04/Jun/24 20:41,30/Jan/23 11:48,1.15.3,1.16.0,1.17.0,,,,,,,,,Runtime / Coordination,,,,0,,,,,"FLINK-28078 revealed a bug in curator's LeaderLatch implementation that is covered by CURATOR-645. FLINK-28078 came up with a workaround in {{ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers}} (see [655184c|https://github.com/apache/flink/commit/655184cdb086ac2adec3e743701868f1a55b6129] {{master}} and [ed8700d|https://github.com/apache/flink/commit/ed8700d03cccc47bfa39da5e1e6611eb9be7d5a1] for {{release-1.15}}) that should get reverted after the curator version was bumbed to a version that includes CURATOR-645.

There's a similar issue CURATOR-644 which we didn't run into but we might want to observe and include as well, probably.",,,,,,,,,,,,,,,FLINK-30772,FLINK-28078,FLINK-30484,,,,,,,,,,,,CURATOR-645,CURATOR-644,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 11:47:25 UTC 2023,,,,,,,,,,"0|z188s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 11:47;mapohl;I'm inclined to close this issue:
* FLINK-30772 will update curator for 1.17 and, therefore, is a (partial) duplicate of this issue.
* 1.15 is not gonna be supported after 1.17 is out.
* We fixed FLINK-28078 through a workaround and FLINK-30484 doesn't come up that often (and if so, we could use the very same workaround from FLINK-28078 to fix it).

That way, we avoid doing another flink-shaded 16.2 release. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add built-in string_split function.,FLINK-29172,13479668,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,damumu,damumu,01/Sep/22 12:27,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Table SQL / API,,,,0,,,,,"Syntax:
{code:java}
STRING_SPLIT ( string , separator [ , enable_ordinal ] )  {code}
h2. Arguments
h4. _string_

Is an [expression|https://docs.microsoft.com/en-us/sql/t-sql/language-elements/expressions-transact-sql?view=sql-server-ver16] of any character type (for example, {*}nvarchar{*}, {*}varchar{*}, {*}nchar{*}, or {*}char{*}).
h4. _separator_

Is a single character [expression|https://docs.microsoft.com/en-us/sql/t-sql/language-elements/expressions-transact-sql?view=sql-server-ver16] of any character type (for example, {*}nvarchar(1){*}, {*}varchar(1){*}, {*}nchar(1){*}, or {*}char(1){*}) that is used as separator for concatenated substrings.
h4. _enable_ordinal_

An *int* or *bit* [expression|https://docs.microsoft.com/en-us/sql/t-sql/language-elements/expressions-transact-sql?view=sql-server-ver16] that serves as a flag to enable or disable the {{ordinal}} output column. A value of 1 enables the {{ordinal}} column. If _enable_ordinal_ is omitted, {{{}NULL{}}}, or has a value of 0, the {{ordinal}} column is disabled.

See more:
 * sql server: https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-ver16",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 12:27:05.0,,,,,,,,,,"0|z188qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add documents for MaxAbs, FeatureHasher, Interaction, VectorSlicer, ElementwiseProduct and Binarizer",FLINK-29171,13479667,13479751,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,01/Sep/22 12:22,07/Sep/22 01:41,04/Jun/24 20:41,07/Sep/22 01:41,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Add docs of MaxAbs, FeatureHasher, Interaction, VectorSlicer, ElementwiseProduct and Binarizer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 12:22:00.0,,,,,,,,,,"0|z188q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for VarianceThresholdSelector,FLINK-29170,13479653,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Jiang Xin,zhangzp,zhangzp,01/Sep/22 11:00,10/Jan/23 04:23,04/Jun/24 20:41,10/Jan/23 04:23,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 11:00:16.0,,,,,,,,,,"0|z188n4:",9223372036854775807,This is duplicate of https://issues.apache.org/jira/browse/FLINK-29409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for UnivariateFeatureSelector,FLINK-29169,13479652,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Jiang Xin,zhangzp,zhangzp,01/Sep/22 10:59,10/Jan/23 04:22,04/Jun/24 20:41,10/Jan/23 04:22,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 10:59:37.0,,,,,,,,,,"0|z188mw:",9223372036854775807,This is duplicate of FLINK-29601,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for NGram,FLINK-29168,13479651,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,01/Sep/22 10:58,06/Sep/22 14:09,04/Jun/24 20:41,06/Sep/22 14:09,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 10:58:44.0,,,,,,,,,,"0|z188mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Time out-of-order optimization for merging multiple data streams into one data stream,FLINK-29167,13479648,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,zhangyang93,zhangyang93,01/Sep/22 10:24,01/Dec/22 09:00,04/Jun/24 20:41,01/Dec/22 09:00,1.14.2,,,,,,,,,,,API / DataStream,,,,0,,,,,"Problem Description: 

     I have many demand scenarios and need to combine more than 2 data streams (DataStreams) into one data stream. The business behind the data stream processing requires the time sequence of events to complete the scene requirements, so I use the union operator of flink to The confluence is completed, but the data after the confluence does not guarantee its original event time sequence.
{code:java}
dataStream0 = dataStream0.union(dataStreamArray);  {code}
Design suggestion: 

    When designing the source code, you can merge into the stream in the order of the array in the dataStreamArray instead of random order.

 

Solution suggestion: 

   At present, I use windowAll to sort the data after the confluence in chronological order, and complete the overall scene realization, but the parallelism of windowAll can only be 1, which affects the performance of the entire directed acyclic graph. In addition, there are two confluence scene sorting scenes. I haven't thought of a good remedy, so I can only think that the union of the union is the sequence, which can save a lot of unnecessary trouble for the event-time stream merging.

 

 ",,43200,43200,,0%,43200,43200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,Thu Dec 01 09:00:44 UTC 2022,,,,,,,,,,"0|z188m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 07:43;wanglijie;Is your job bounded stream? Or unbounded stream？;;;","01/Dec/22 09:00;zhuzh;The record orders are retained if they are from the same source and are sent to the same downstream task.
Even without a {{union}},  the time of records are never guaranteed to be sorted after a shuffle from different subtasks of a source.;;;","01/Dec/22 09:00;zhuzh;Closing the ticket because it does not seem to be a problem and is not active for months.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
多流合并成一个的方案优化,FLINK-29166,13479641,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zhangyang93,zhangyang93,01/Sep/22 10:07,01/Sep/22 10:12,04/Jun/24 20:41,01/Sep/22 10:10,1.14.2,,,,,,,,,,,API / DataStream,,,,0,pull-request-available,,,,"问题描述：

   我有很多需求场景，需要进行3个以上的数据流（DataStream）进行合并成一条处理后面的业务，后面的业务对数据是有事件时间顺序要求，用flink的union的合完流之后就没有保证其原始的事件时间顺序
{code:java}
//dataStreamArray是有多个的
ataStream0 = dataStream0.union(dataStreamArray); {code}
设计建议：可以按照可以源码中按照dataStreamArray中数组的顺序进行顺序合入流中，而不是随机顺序。

解决方案：目前使用windowAll对合流之后的数据进行了时间顺序排序，完成了整体场景实现，但是windowAll并行度只能是1，影响了整个有向无环图的性能。另外还有个排序场景还没有想到好的弥补方案，只能想union的合流是顺序就能给事件时间的流合并省去很多不必要的麻烦",,43200,43200,,0%,43200,43200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,Thu Sep 01 10:12:14 UTC 2022,,,,,,,,,,"0|z188kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 10:10;martijnvisser;Please use English for Jira tickets;;;","01/Sep/22 10:12;zhangyang93;ok;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator quickstart examples using code,FLINK-29165,13479598,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaurav726,gaurav726,gaurav726,01/Sep/22 06:28,21/Sep/22 08:10,04/Jun/24 20:41,21/Sep/22 08:10,kubernetes-operator-0.1.0,kubernetes-operator-1.0.0,kubernetes-operator-1.0.1,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,doc,pull-request-available,,,I will update doc(quick-start.md) and write code examples to create yaml and submit it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Wed Sep 21 08:10:16 UTC 2022,,,,,,,,,,"0|z188b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 07:14;gaurav726;can anyone assigned it to me, raising PR, [~gyfora] ;;;","02/Sep/22 14:35;gaurav726;[~matyas]  can you please have a look into it;;;","21/Sep/22 08:10;gyfora;merged to main f5130eb7439dc487cb0224e1dc21872056485bbd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is a problem when compiling Flink 1.15.rc-2 in Windows operations,FLINK-29164,13479596,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mindest,mindest,01/Sep/22 06:11,01/Sep/22 11:06,04/Jun/24 20:41,01/Sep/22 10:11,1.15.2,,,,,,,,,,,flink-contrib,,,,0,,,,,"Compile Flink 1.15.2 on Windows platform and it appears

 

Appears when compiling to Flink clients

[ERROR] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-avro-confluent-registry:compilation failure

[/ D: / xxx / XX / cachedschemacoderprovidertest. Java] unable to access org.apache.kafka.common Configurable

The class file of org.apache.kafka.common.configurable cannot be found

 

The compiled JDK version is 11

 

Maven version is 3.2.5",,,,,,,,,,,,,,,,,,,,FLINK-29163,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 11:06:05 UTC 2022,,,,,,,,,,"0|z188ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 11:06;mindest;This is a new issue, not the same as Hu's previous one，

[ERROR] failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-avro-confluent-registry:compilation failure

[/ D: / xxx / XX / cachedschemacoderprovidertest. Java] unable to access org.apache.kafka.common Configurable

The class file of org.apache.kafka.common.configurable cannot be found

 

Please take a closer look

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is a problem when compiling Flink 1.15.rc-2 and flink-1.14.3-rc1 in Windows operations,FLINK-29163,13479590,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,mindest,mindest,01/Sep/22 03:50,01/Sep/22 10:53,04/Jun/24 20:41,01/Sep/22 10:53,1.15.2,,,,,,,,,,,flink-contrib,,,,0,,,,,"Compile Flink 1.15.2 on Windows platform and it appears

 

[ERROR]Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single

(create-test-dependency-user-jar-depend) on project flink-clients_2.11:failed to create assembly:Error creating assembly archive test-user-classloader-job-lib-jar:You must set as least one file.->[help 1]

Error creating assembly archive Pack: you must set at least one file. Occurs

 

when compiling to Flink clients

 

The compiled JDK version is 11

 

Maven version is 3.2.5",,,,,,,,,,,,,,,,,,,,,FLINK-29160,FLINK-29164,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 10:51:50 UTC 2022,,,,,,,,,,"0|z1889k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 10:13;martijnvisser;Are you sure that you're compiling Flink in a Unix-like environment, like Cygwin or WSL? This is required, as mentioned in the README https://github.com/apache/flink/blob/master/README.md?plain=1#L70;;;","01/Sep/22 10:51;chesnay;It _should_ (and _does_) work in plain Windows as well.

flink-clients_2.11 doesn't exist in 1.15; cleanup your git repo, then it should work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missCounter and loadCount metrics also increase even if cache hint,FLINK-29162,13479581,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,01/Sep/22 02:56,01/Sep/22 10:13,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,!image-2022-09-01-10-56-34-264.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/22 02:56;lsy;image-2022-09-01-10-56-34-264.png;https://issues.apache.org/jira/secure/attachment/13048837/image-2022-09-01-10-56-34-264.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-09-01 02:56:44.0,,,,,,,,,,"0|z1887s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Run kubernetes application HA test failed with unable to prepare context: path ""dev/test_kubernetes_application_ha-debian"" not found",FLINK-29161,13479576,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxb,hxb,hxb,01/Sep/22 02:16,01/Sep/22 08:39,04/Jun/24 20:41,01/Sep/22 08:39,1.14.5,1.15.2,1.16.0,,,,,1.14.6,1.15.3,1.16.0,,Build System / Azure Pipelines,flink-docker,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-31T09:13:50.8050601Z Aug 31 09:13:50 Preparing Dockeriles
2022-08-31T09:13:50.8051483Z Aug 31 09:13:50 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-master --single-branch
2022-08-31T09:13:50.8069617Z Cloning into 'flink-docker'...
2022-08-31T09:13:51.4728169Z Generating Dockerfiles... done.
2022-08-31T09:13:51.4732889Z Aug 31 09:13:51 Building images
2022-08-31T09:13:51.5013231Z unable to prepare context: path ""dev/test_kubernetes_application_ha-debian"" not found
2022-08-31T09:13:51.5041357Z Aug 31 09:13:51 ~/work/1/s/cri-dockerd
2022-08-31T09:13:51.5042427Z Aug 31 09:13:51 Command: build_image test_kubernetes_application_ha 10.1.0.160 failed. Retrying...
2022-08-31T09:13:53.5058625Z Aug 31 09:13:53 Starting fileserver for Flink distribution
2022-08-31T09:13:53.5060253Z Aug 31 09:13:53 ~/work/1/s/flink-dist/target/flink-1.16-SNAPSHOT-bin ~/work/1/s/cri-dockerd
2022-08-31T09:14:13.8161820Z Aug 31 09:14:13 ~/work/1/s/cri-dockerd
2022-08-31T09:14:13.8164441Z Aug 31 09:14:13 ~/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-06753986230 ~/work/1/s/cri-dockerd
2022-08-31T09:14:13.8165256Z Aug 31 09:14:13 Preparing Dockeriles
2022-08-31T09:14:13.8166118Z Aug 31 09:14:13 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-master --single-branch
2022-08-31T09:14:13.8182037Z fatal: destination path 'flink-docker' already exists and is not an empty directory.
2022-08-31T09:14:13.8188942Z Aug 31 09:14:13 Retry 1/5 exited 128, retrying in 1 seconds...
2022-08-31T09:14:14.1481612Z Traceback (most recent call last):
2022-08-31T09:14:14.1483171Z   File ""/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
2022-08-31T09:14:14.1484572Z     httpd = socketserver.TCPServer(("""", 9999), handler)
2022-08-31T09:14:14.1485179Z   File ""/usr/lib/python3.8/socketserver.py"", line 452, in __init__
2022-08-31T09:14:14.1485724Z     self.server_bind()
2022-08-31T09:14:14.1486247Z   File ""/usr/lib/python3.8/socketserver.py"", line 466, in server_bind
2022-08-31T09:14:14.1486795Z     self.socket.bind(self.server_address)
2022-08-31T09:14:14.1487783Z OSError: [Errno 98] Address already in use {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40556&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14]

 ",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29137,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 08:39:49 UTC 2022,,,,,,,,,,"0|z1886o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 08:39;chesnay;master: a9c94e06d6cddc90a037a82c18b50dd062b06cbd
1.15: 3dad3a6d92525e3d034eb95274a7ec6f340e059f 
1.14: d9ac4c7b1062470bdec68818bfc2bc839da94ef3 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 在windows 编译flink 1.15.rc-2和flink-1.14.3-rc1 时出现问题 There is a problem when compiling Flink 1.15.rc-2 and flink-1.14.3-rc1 in windows operations,FLINK-29160,13479575,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mindest,mindest,01/Sep/22 01:55,01/Sep/22 10:11,04/Jun/24 20:41,01/Sep/22 10:11,1.15.2,,,,,,,,,,,flink-contrib,,,,0,,,,,"Compile Flink 1.15.2 on Windows platform and it appears

 

Error creating assembly archive Pack: you must set at least one file. Occurs when compiling to Flink clients

 

The compiled JDK version is 11

 

Maven version is 3.2.5","操作系统:windows 

jdk:11.

maven:3.2.5",,,,,,,,,,,,,,,,,,,FLINK-29163,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 03:37:22 UTC 2022,,,,,,,,,,"0|z1886g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 01:58;hxb;Please use English to create JIRA;;;","01/Sep/22 03:37;mindest;Compile Flink 1.14.3-rc1 on Windows platform and it appears same bug.

that is 

[ERROR] failed to Execute goal org.apache.maven.plugins:maven-assembly-plugin:2.4:single (create-test-dependency-user-jar-depend) on project flink-clients_2.11:Failed to create assembly:Error creating assembly archive test-user-classloader-job-lib-jar:You must set at least one file.->[Help 1]

 

The compiled JDK version is 11

Maven version is 3.2.5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revisit/harden initial deployment logic,FLINK-29159,13479503,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,thw,thw,31/Aug/22 14:27,26/Sep/22 08:33,04/Jun/24 20:41,26/Sep/22 08:33,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"Found isFirstDeployment logic not working as expected for a deployment that had never successfully deployed (image pull error).  We are probably also lacking test coverage for the initialSavepointPath field.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29100,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 26 08:33:24 UTC 2022,,,,,,,,,,"0|z187r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 08:33;gyfora;merged to main d2161ac99e02af322b38b961b6df2660de812c59;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix logging in DefaultCompletedCheckpointStore,FLINK-29158,13479477,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,roman,roman,roman,31/Aug/22 11:23,28/Sep/22 03:19,04/Jun/24 20:41,28/Sep/22 03:19,1.15.2,,,,,,,1.15.3,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,See [https://github.com/apache/flink/pull/16582#discussion_r949214456],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 03:19:11 UTC 2022,,,,,,,,,,"0|z187lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 03:19;hxbks2ks;Merged into release-1.15 via 4760601f03267389b95de7dcff2f97ed45dc4e62;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify the contract between CompletedCheckpointStore and SharedStateRegistry,FLINK-29157,13479474,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Yanfei Lei,roman,roman,31/Aug/22 11:05,27/Oct/22 05:22,04/Jun/24 20:41,27/Oct/22 02:31,1.15.2,1.16.0,,,,,,1.16.1,1.17.0,,,Runtime / Checkpointing,,,,0,pull-request-available,,,,"After FLINK-24611, CompletedCheckpointStore is required to call SharedStateRegistry.unregisterUnusedState() on checkpoint subsumption and shutdown.

Although it's not clear whether CompletedCheckpointStore is internal there are in fact external implementations (which weren't updated accordingly).

 

After FLINK-25872, CompletedCheckpointStore also must call checkpointsCleaner.cleanSubsumedCheckpoints.

 

Another issue with a custom implementation was using different java objects for state for CheckpointStore and SharedStateRegistry (after FLINK-24086). 

 

So it makes sense to:
 * clarify the contract (different in 1.15 and 1.16)
 * require using the same checkpoint objects by SharedStateRegistryFactory and CompletedCheckpointStore
 * mark the interface(s) as PublicEvolving",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 02:31:50 UTC 2022,,,,,,,,,,"0|z187ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/22 06:07;Yanfei Lei;> mark the interface(s) as PublicEvolving

Since mark `CompletedCheckpointStore` would cause `CompletedCheckpoint` , `SharedStateRegistry`, `CheckpointsCleaner`, `StreamStateHandle`, `OperatorState`, `CompletedCheckpointStats`  ...  to change with it， and some of them should be internal. So don't mark `CompletedCheckpointStore` as PulicEvolving for now.;;;","27/Oct/22 02:31;klion26;merged into master 63767c5ed91642c67f97d9f16ff2b8955f9ae421

1.16 be2bd93838548f7858baecf5e8beb469836081d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support LISTAGG in the Table API,FLINK-29156,13479450,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,cun8cun8,cun8cun8,cun8cun8,31/Aug/22 08:41,05/Oct/22 20:00,04/Jun/24 20:41,03/Oct/22 07:01,,,,,,,,1.17.0,,,,Table SQL / API,,,,0,pull-request-available,,,,"Currently, LISTAGG  are not supported in Table API.
table.group_by(col(""a""))
     .select(
         col(""a""),
        call(""LISTAGG"", col(""b""), ','))",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 03 07:01:47 UTC 2022,,,,,,,,,,"0|z187fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/22 07:01;dianfu;Merged to master via 397141ed7c00586e5b5449177840243babe2a776;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve default config of grpcServer in Process Mode,FLINK-29155,13479439,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,hxb,hxb,31/Aug/22 07:35,01/Dec/22 02:23,04/Jun/24 20:41,01/Dec/22 02:23,1.14.5,1.15.3,1.16.0,,,,,1.15.4,1.16.1,1.17.0,,API / Python,,,,0,pull-request-available,,,,The existing grpcServer configuration may cause channel disconnection when there is a large amount of data. Some PyFlink users are very troubled by this problem.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 01 02:23:17 UTC 2022,,,,,,,,,,"0|z187cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 02:23;hxb;Merged into master via 37bfd8cedf003c2bd714c8b4e87d1594924c748e

Merged into release-1.16 via 6d66200d5722aa5f33a32de801259eb16f095d15

Merged into release-1.15 via 6659d0049d9270173458b44fa87dd5317e9c6638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support LookupTableSource for table store,FLINK-29154,13479438,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,31/Aug/22 07:24,27/Sep/22 02:49,04/Jun/24 20:41,06/Sep/22 08:21,,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,"At present, the bottom layer of Table Store is LSM structure, and it has full and incremental reading, which can have certain lookup capability. We can unlock Table Store to implement LookupTableSource.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 08:21:17 UTC 2022,,,,,,,,,,"0|z187co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 08:21;lzljs3620320;master: e55dde87296b51132d37fb4265bd3c863f05db40
release-0.2: c24016d512b5c01c04b6976b19ebdbf9a2ba78d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumerThread should catch WakeupException when committing offsets,FLINK-29153,13479430,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,renqs,renqs,31/Aug/22 07:01,08/Nov/23 06:40,04/Jun/24 20:41,13/Sep/22 02:20,1.16.0,,,,,,,1.16.0,,,,Connectors / Kafka,,,,0,pull-request-available,,,,"{{KafkaConsumerThread}} in the legacy {{FlinkKafkaConsumer}} makes a wakeup on the {{KafkaConsumer}} on offset commit to wakeup the potential blocking {{KafkaConsumer.poll()}}. However the wakeup might happen when the consumer is not polling. The wakeup will be remembered by the consumer and re-examined while committing the offset asynchronously, which leads to an unnecessary {{WakeupException}}. 

As the JavaDoc and method signature of KafkaConsumer does not show that the WakeupException could be thrown in KafkaConsumer#commitAsync, this could be considered as a bug on Kafka side (breaking the contract of public API), but we can still fix it by catching the WakeupException and retrying. ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29018,FLINK-24119,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 03:18:12 UTC 2022,,,,,,,,,,"0|z187aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 03:01;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40563&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","02/Sep/22 02:27;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40612&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","02/Sep/22 08:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40616&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","02/Sep/22 08:48;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40616&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;","05/Sep/22 11:06;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40674&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","05/Sep/22 11:07;hxb;[~renqs] Any updates on the progress?;;;","06/Sep/22 06:33;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40714&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=ffa8837a-b445-534e-cdf4-db364cf8235d;;;","07/Sep/22 01:46;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40742&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36925;;;","07/Sep/22 01:52;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40759&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","07/Sep/22 06:07;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40763&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;","08/Sep/22 06:09;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36934;;;","08/Sep/22 06:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","08/Sep/22 06:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40789&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a;;;","09/Sep/22 02:21;renqs;Fixed on master: 3e883e687d2bdb80ad2f160b08184fca11f4f554

release-1.16: dbcf949f89b5bead6c2bec4de77ca68bc8614fe6;;;","09/Sep/22 03:16;hxb;[~renqs] Should we need to cp this fix to release-1.16;;;","09/Sep/22 03:18;renqs;[~hxb] Yes I'm waiting for the CI on release-1.16. Will close the ticket then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe statement resutls is different from the Hive ,FLINK-29152,13479428,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,fsk119,fsk119,31/Aug/22 06:50,21/Sep/22 15:10,04/Jun/24 20:41,21/Sep/22 15:10,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,"In hive, the results schema is 


{code:java}
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| a         | int        |          |
| b         | string     |          |
+-----------+------------+----------+
{code}

but our implementation is 

{code:java}
0: jdbc:hive2://localhost:10000/default> describe sink;
+-------+-------+-------+-------+---------+------------+
| name  | type  | null  |  key  | extras  | watermark  |
+-------+-------+-------+-------+---------+------------+
| a     | INT   | true  | NULL  | NULL    | NULL       |
+-------+-------+-------+-------+---------+------------+
{code}

BTW, it's better we can support {{DESCRIBE FORMATTED}} like hive does.

{code:java}
+-------------------------------+----------------------------------------------------+-----------------------+
|           col_name            |                     data_type                      |        comment        |
+-------------------------------+----------------------------------------------------+-----------------------+
| # col_name                    | data_type                                          | comment               |
|                               | NULL                                               | NULL                  |
| a                             | int                                                |                       |
| b                             | string                                             |                       |
|                               | NULL                                               | NULL                  |
| # Detailed Table Information  | NULL                                               | NULL                  |
| Database:                     | default                                            | NULL                  |
| Owner:                        | null                                               | NULL                  |
| CreateTime:                   | Tue Aug 30 06:54:00 UTC 2022                       | NULL                  |
| LastAccessTime:               | UNKNOWN                                            | NULL                  |
| Retention:                    | 0                                                  | NULL                  |
| Location:                     | hdfs://namenode:8020/user/hive/warehouse/sink      | NULL                  |
| Table Type:                   | MANAGED_TABLE                                      | NULL                  |
| Table Parameters:             | NULL                                               | NULL                  |
|                               | comment                                            |                       |
|                               | numFiles                                           | 0                     |
|                               | totalSize                                          | 0                     |
|                               | transient_lastDdlTime                              | 1661842440            |
|                               | NULL                                               | NULL                  |
| # Storage Information         | NULL                                               | NULL                  |
| SerDe Library:                | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL                  |
| InputFormat:                  | org.apache.hadoop.mapred.TextInputFormat           | NULL                  |
| OutputFormat:                 | org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat | NULL                  |
| Compressed:                   | No                                                 | NULL                  |
| Num Buckets:                  | -1                                                 | NULL                  |
| Bucket Columns:               | []                                                 | NULL                  |
| Sort Columns:                 | []                                                 | NULL                  |
| Storage Desc Params:          | NULL                                               | NULL                  |
|                               | serialization.format                               | 1                     |
+-------------------------------+----------------------------------------------------+-----------------------+
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 15:10:16 UTC 2022,,,,,,,,,,"0|z187ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 08:10;luoyuxia;I will first to fix insistent behavior for `describe table` statement.;;;","21/Sep/22 15:10;jark;Fixed in
 - master: b5cd9f34ab73fa69a3db5a09908c1aa954ed0597
 - release-1.16: a6e954ca3bff9c62713d475627b49dd18a4f02fd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHOW CREATE TABLE doesn't work for Hive dialect,FLINK-29151,13479426,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,fsk119,fsk119,31/Aug/22 06:45,21/Sep/22 15:09,04/Jun/24 20:41,21/Sep/22 15:09,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,,,,,"{code:java}
0: jdbc:hive2://localhost:10000/default> show create table sink;
Error: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation 9b060771-34b8-453d-abf5-674c86b62921.
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:389)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:248)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.table.api.ValidationException
    at org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.handleUnsupportedOperation(HiveParserDDLSemanticAnalyzer.java:2188)
    at org.apache.flink.table.planner.delegation.hive.parse.HiveParserDDLSemanticAnalyzer.convertToOperation(HiveParserDDLSemanticAnalyzer.java:414)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.processCmd(HiveParser.java:334)
    at org.apache.flink.table.planner.delegation.hive.HiveParser.parse(HiveParser.java:213)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:90)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$0(SqlGatewayServiceImpl.java:182)
    at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:111)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:239)
    ... 7 more
Caused by: java.lang.UnsupportedOperationException: Unsupported operation: TOK_SHOW_CREATETABLE
    ... 15 more (state=,code=0) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28952,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 21 07:31:27 UTC 2022,,,,,,,,,,"0|z187a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 07:31;jark;Fixed in 
 - master: 7ddf059d3b7b6888f550bfca9eb09c0cdeb7d682
 - release-1.16: 8a5eec9945a3cdbd35b934508586803f470a3f2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIMESTAMPDIFF  microsecond unit for table api,FLINK-29150,13479420,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,31/Aug/22 06:08,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,like mysql https://www.geeksforgeeks.org/timestampdiff-function-in-mysql/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 02 07:23:12 UTC 2022,,,,,,,,,,"0|z1878o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 07:23;jackylau;https://issues.apache.org/jira/browse/CALCITE-3529

https://issues.apache.org/jira/browse/CALCITE-3530

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate E2E tests to catalog-based and enable E2E tests for Flink1.14,FLINK-29149,13479407,13478162,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,31/Aug/22 04:02,08/Oct/22 03:43,04/Jun/24 20:41,08/Oct/22 03:43,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"For LogStoreE2eTest, should add a step to manually create Kafka topic",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29506,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 08 03:43:09 UTC 2022,,,,,,,,,,"0|z1875s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/22 03:43;lzljs3620320;master: 7e2f5850c4ae2315a729b3b1ba007162414ccd89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs for SQL Gateway,FLINK-29148,13479406,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,fsk119,fsk119,31/Aug/22 03:51,20/Sep/22 08:05,04/Jun/24 20:41,19/Sep/22 14:11,1.16.0,,,,,,,1.16.0,1.17.0,,,Documentation,,,,0,pull-request-available,,,,Add basic doc for SQL Gateway.,,,,,,,,,,,,,,,,FLINK-29026,,,,FLINK-28937,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 19 09:48:14 UTC 2022,,,,,,,,,,"0|z1875k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 09:48;jark;Fixed in 
 - master: 530019a59e076c9bb65dc1a046a708edf01c0f98 to be3c35c560d797c5a07be605a36c3c1d1127a3d0
 - release-1.16: 102dd0225d56b18e839d4f3e24b34975446ff61f to 7f2261022f2f77775d1fdf04480972f74ebf6e87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogLocalRecoveryITCase.testRestartTM failed with CheckpointException,FLINK-29147,13479405,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,hxb,hxb,31/Aug/22 03:44,13/Sep/22 03:17,04/Jun/24 20:41,13/Sep/22 03:17,1.16.0,,,,,,,1.16.0,,,,Runtime / State Backends,,,,0,test-stability,,,,"{code:java}
2022-08-30T13:30:24.9214696Z Aug 30 13:30:24 [ERROR] ChangelogLocalRecoveryITCase.testRestartTM  Time elapsed: 9.056 s  <<< ERROR!
2022-08-30T13:30:24.9235235Z Aug 30 13:30:24 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Source: Custom Source (1/1) of job 2070563b800eceb80fb9f6cab1da5c00 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2022-08-30T13:30:24.9254389Z Aug 30 13:30:24 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-08-30T13:30:24.9274122Z Aug 30 13:30:24 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-08-30T13:30:24.9293512Z Aug 30 13:30:24 	at org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase.testRestartTM(ChangelogLocalRecoveryITCase.java:145)
2022-08-30T13:30:24.9295187Z Aug 30 13:30:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-30T13:30:24.9304089Z Aug 30 13:30:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-30T13:30:24.9305377Z Aug 30 13:30:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-30T13:30:24.9324825Z Aug 30 13:30:24 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-30T13:30:24.9326046Z Aug 30 13:30:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-30T13:30:24.9327330Z Aug 30 13:30:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-30T13:30:24.9328669Z Aug 30 13:30:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-30T13:30:24.9329927Z Aug 30 13:30:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-30T13:30:24.9331480Z Aug 30 13:30:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-30T13:30:24.9332662Z Aug 30 13:30:24 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-30T13:30:24.9426069Z Aug 30 13:30:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-30T13:30:24.9456433Z Aug 30 13:30:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-30T13:30:24.9475038Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-30T13:30:24.9494771Z Aug 30 13:30:24 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-30T13:30:24.9505400Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-30T13:30:24.9506695Z Aug 30 13:30:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-30T13:30:24.9507972Z Aug 30 13:30:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-30T13:30:24.9509158Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-30T13:30:24.9510253Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-30T13:30:24.9511548Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-30T13:30:24.9512714Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-30T13:30:24.9513916Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-30T13:30:24.9515034Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-30T13:30:24.9516384Z Aug 30 13:30:24 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-08-30T13:30:24.9517273Z Aug 30 13:30:24 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-08-30T13:30:24.9518307Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-30T13:30:24.9519469Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-30T13:30:24.9520610Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-30T13:30:24.9521760Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-30T13:30:24.9523124Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-30T13:30:24.9524401Z Aug 30 13:30:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-30T13:30:24.9525449Z Aug 30 13:30:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-30T13:30:24.9526527Z Aug 30 13:30:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-08-30T13:30:24.9560457Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-30T13:30:24.9561855Z Aug 30 13:30:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-30T13:30:24.9563025Z Aug 30 13:30:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-30T13:30:24.9564004Z Aug 30 13:30:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-30T13:30:24.9565071Z Aug 30 13:30:24 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-30T13:30:24.9566334Z Aug 30 13:30:24 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-30T13:30:24.9567586Z Aug 30 13:30:24 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-30T13:30:24.9568894Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-30T13:30:24.9570335Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-30T13:30:24.9571887Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-30T13:30:24.9573559Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-30T13:30:24.9575078Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-30T13:30:24.9576292Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-30T13:30:24.9577496Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-30T13:30:24.9578835Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-30T13:30:24.9580247Z Aug 30 13:30:24 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-30T13:30:24.9581655Z Aug 30 13:30:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-30T13:30:24.9583120Z Aug 30 13:30:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-30T13:30:24.9584223Z Aug 30 13:30:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-30T13:30:24.9585323Z Aug 30 13:30:24 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-30T13:30:24.9586535Z Aug 30 13:30:24 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-30T13:30:24.9587644Z Aug 30 13:30:24 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-30T13:30:24.9588972Z Aug 30 13:30:24 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-30T13:30:24.9590451Z Aug 30 13:30:24 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task Source: Custom Source (1/1) of job 2070563b800eceb80fb9f6cab1da5c00 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2022-08-30T13:30:24.9592265Z Aug 30 13:30:24 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:143)
2022-08-30T13:30:24.9594179Z Aug 30 13:30:24 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:105)
2022-08-30T13:30:24.9595523Z Aug 30 13:30:24 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2022-08-30T13:30:24.9596706Z Aug 30 13:30:24 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453)
2022-08-30T13:30:24.9597928Z Aug 30 13:30:24 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-30T13:30:24.9599129Z Aug 30 13:30:24 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453)
2022-08-30T13:30:24.9600265Z Aug 30 13:30:24 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218)
2022-08-30T13:30:24.9601601Z Aug 30 13:30:24 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-30T13:30:24.9602826Z Aug 30 13:30:24 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-30T13:30:24.9706005Z Aug 30 13:30:24 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-30T13:30:24.9707060Z Aug 30 13:30:24 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-30T13:30:24.9708120Z Aug 30 13:30:24 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-30T13:30:24.9709020Z Aug 30 13:30:24 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-30T13:30:24.9709994Z Aug 30 13:30:24 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-30T13:30:24.9710936Z Aug 30 13:30:24 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-30T13:30:24.9712028Z Aug 30 13:30:24 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-30T13:30:24.9713123Z Aug 30 13:30:24 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-30T13:30:24.9714024Z Aug 30 13:30:24 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-30T13:30:24.9714799Z Aug 30 13:30:24 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-30T13:30:24.9715658Z Aug 30 13:30:24 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-30T13:30:24.9716626Z Aug 30 13:30:24 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-30T13:30:24.9717480Z Aug 30 13:30:24 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-30T13:30:24.9718390Z Aug 30 13:30:24 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-30T13:30:24.9719299Z Aug 30 13:30:24 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-30T13:30:24.9720175Z Aug 30 13:30:24 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-30T13:30:24.9721250Z Aug 30 13:30:24 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-30T13:30:24.9722347Z Aug 30 13:30:24 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-30T13:30:24.9723617Z Aug 30 13:30:24 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-30T13:30:24.9724863Z Aug 30 13:30:24 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40530&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10110",,,,,,,,,,,,,,,,,,,,,FLINK-29102,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 03:44:46 UTC 2022,,,,,,,,,,"0|z1875c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 03:44;hxb;[~Yanfei Lei] Could you help take a look? Thx.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User set job configuration can not be retirieved from JobGraph and ExecutionGraph,FLINK-29146,13479402,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,csq,csq,31/Aug/22 03:14,05/Sep/22 09:09,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,0,,,,,"Currently, when building an ExecutionGraph, it requires to set the job specific information (like job id, job name, job configuration, etc) and most of them are from JobGraph.But I find that the configuration in JobGraph is a new Configuration instance when the JobGraph is built, and it does not contain any user set configuration. As a result, we are not able retrieve the use specified job configuration in ExecutionGraph built from JobGraph during execution runtime.

BTW, in StreamExecutionEnvironment, it seems that job configurations that not contained in built-in options will be ignored when calling StreamExecutionEnvironment.configure(ReadableConfig[, ClassLoader]). However, it will be included when constructing a StreamExecutionEnvironment, which seems a bit inconsistent. Is it by design?

{code:java}
Configuration configuration = new Configuration();
// These configured string will take effect.
configuration.setString(""k1"", ""v1"");
configuration.setString(""k2"", ""v2"");
configuration.setString(""k3"", ""v3"");
configuration.set(HeartbeatManagerOptions.HEARTBEAT_TIMEOUT, 300000L);
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(configuration);

// These configured string will be ignored.
configuration.setString(""k4"", ""v4"");
configuration.setString(""k5"", ""v5"");
configuration.setString(""k6"", ""v6"");
env.configure(configuration);
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 09:09:40 UTC 2022,,,,,,,,,,"0|z1874o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 09:09;klion26;A similar problem was encountered in a lower version, I think it needs to be fixed in master as well, otherwise, others may encounter problems when using ExecutionGraph#getJobConfiguration in the future;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker-entrypoint.sh - Read-only file system,FLINK-29145,13479320,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,tashoyan,tashoyan,30/Aug/22 13:59,05/Jan/23 19:36,04/Jun/24 20:41,22/Sep/22 12:39,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"When running a FlinkDeployment, we can see error messages:
{code:none}
sed: couldn't open temporary file /opt/flink/conf/sedHIUGh6: Read-only file system
sed: couldn't open temporary file /opt/flink/conf/sedNALzes: Read-only file system
/docker-entrypoint.sh: line 73: /opt/flink/conf/flink-conf.yaml: Read-only file system
/docker-entrypoint.sh: line 89: /opt/flink/conf/flink-conf.yaml.tmp: Read-only file system
{code}
The script _docker-entrypoint.sh_ tries to modify some settings in {_}flink-conf.yaml{_}, despite this config file is mounted read-only from a ConfigMap.

Such on-the-fly config modifications make no sense for deployments in Kubernetes. Instead we specify Flink settings via the _flinkConfiguration_ setting in the FlinkDeployment descriptor.

In addition, _docker-entrypoint.sh_ tries to modify a deprecated setting {_}query.server.port{_}:
{code:bash}
set_config_option query.server.port 6125
{code}
The setting _query.server.port_ is deprecated in favor of {_}queryable-state.server.ports{_}:
[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/queryable_state/#state-server]",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21383,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 20:09:08 UTC 2022,,,,,,,,,,"0|z186n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 15:43;guoweiwu1994;Did you write the script _docker-entrypoint.sh_ by yourself ? I can't find _docker-entrypoint.sh_ in flink source code.;;;","30/Aug/22 19:38;tashoyan;This script is in another repository: https://github.com/apache/flink-docker/blob/master/1.15/scala_2.12-java8-debian/docker-entrypoint.sh;;;","30/Aug/22 20:09;gyfora;I think we can ignore those errors safely when using the opertor/native kubernetes integration;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable multiple jar entries for jarURI,FLINK-29144,13479313,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,tashoyan,tashoyan,30/Aug/22 13:04,23/Sep/22 06:33,04/Jun/24 20:41,23/Sep/22 06:33,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,,,,,"The setting _job.jarURI_ accepts a string with the path to the jar-file:

{code:yaml}
job:
  jarURI: local:///opt/flink/examples/streaming/StateMachineExample.jar
{code}

This could be improved to accept a list of jars:

{code:yaml}
job:
  jarURIs:
  - local:///opt/flink/examples/streaming/StateMachineExample.jar
  - local:///opt/common/scala-logging.jar
{code}

This could also be improved to accept one or more directories with jars:

{code:yaml}
job:
  jarDirs:
  - local:///opt/app/lib
  - local:///opt/common/lib
{code}

The order of entries in the list defines the order of jars in the classpath.

Internally, Flink Kubernetes Operator uses the property _pipeline.jars_ - see [FlinkConfigBuilder.java |https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/FlinkConfigBuilder.java#L259]:

{code:java}
effectiveConfig.set(PipelineOptions.JARS, Collections.singletonList(uri.toString()));
{code}

The property _pipeline.jars_ allows to pass more than one jar entry.

This improvement allows to avoid building a fat-jar. Instead we could provide directories with normal jars.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 06:33:20 UTC 2022,,,,,,,,,,"0|z186lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 13:14;gyfora;This would be a breaking change in the API so I would suggest to make the change to simply accept a comma/semicolon delimited list in the jarURI directly.;;;","30/Aug/22 13:31;tashoyan;For API compatibility, we can preserve the old parameter _jarURI_. New parameters _jarURIs_ and _jarDirs_ do not break backward compatibility.;;;","30/Aug/22 13:44;gyfora;But it would be pretty ugly to have duplicated parameters. I would still prefer to keep the current name. The most common case is to have a single jar anyways ;;;","30/Aug/22 13:48;gyfora;Or we could simply let users define the pipeline jars parameter for multiple jars on the config.[~thw] what do you think?;;;","30/Aug/22 14:26;thw;[~gyfora] since this isn't a common requirement, I would also prefer we allow using the configuration parameter PipelineOptions.JARS for this. That would mean the operator needs a slight change to not just overwrite that parameter. ;;;","30/Aug/22 16:25;gyfora;agreed +1 for simply enabling the users to put this in the `flinkConfiguration`;;;","30/Aug/22 19:48;tashoyan;Then, how do you suggest merging values from _job.jarURI_ and {_}flinkConfiguration.pipeline.jars{_}? Should the latter overwrite the former? Or should both parameters contribute to the resulting classpath?
{code:none}
classpath := ${job.jarURI}:${jars from flinkConfiguration.pipeline.jars}
{code};;;","30/Aug/22 19:52;tashoyan;Although _flinkConfiguration.pipeline.jars_ technically solves the problem, the possibility to have _job.jarDirs_ would be much more convenient. In practice we may have dosens or even hundreds of jars - it is quite inconvenient to specify such huge list in {_}flinkConfiguration.pipeline.jars{_}. It would be much more convenient to specify one (or a couple of) directory.;;;","30/Aug/22 20:02;thw;Please note that the operator is not in the business of interpreting that config, including not listing directories to assemble a jar file list. Jar files don't have to be ""local"" either. ;;;","30/Aug/22 20:03;gyfora;I think if the user defines job.jarURI we use that, otherwise we use what is in the configuration, no need to add merging logic.

I understand that it is a slight inconvenience but I would say it's extremely uncommon that someone would have so many jars. People tend to repackage them into one or few jars in those cases. It's much more easier to work with a single fatjar then having to juggle the classpath with dozens of jars;;;","31/Aug/22 10:11;gyfora;[~tashoyan] would you like to work on this improvement? I think it should be easy enough to change the FlinkConfigBuilder to not override pipelinejars config unless the jarURI is defined;;;","06/Sep/22 09:22;tashoyan;[~gyfora] now I'm a bit overloaded, but maybe a couple of months later I will be able to make it.;;;","23/Sep/22 06:33;gyfora;Resolved by 8f53441a4978eeb38dc5ef229c179cc60598ce87;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE after calcite upgrade at  org.apache.calcite.tools.RelBuilder.groupKey(RelBuilder.java:1287),FLINK-29143,13479285,13449408,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/22 10:20,08/Nov/22 11:10,04/Jun/24 20:41,08/Nov/22 11:10,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"en example of failure after upgrade to 1.30
It seems a root cause of about 40 failing tests
at AggregateReduceGroupingTest, LookupJoinTest, NestedLoopSemiAntiJoinTest, SemiAntiJoinTest, ShuffledHashSemiAntiJoinTest, MultipleInputCreationTest, RemoveCollationTest and others
 {noformat}
java.lang.RuntimeException: Error while applying rule FlinkAggregateJoinTransposeRule, args [rel#23788:LogicalAggregate.NONE.any.[](input=RelSubset#23754,group={0, 1, 3, 4},EXPR$4=COUNT($2),agg#1=SUM($5),agg#2=COUNT($5)), rel#23753:LogicalJoin.NONE.any.[](left=RelSubset#23749,right=RelSubset#23752,condition==($1, $3),joinType=inner)]
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:250)
	at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:59)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:523)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:317)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:93)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.assertPlanEquals(TableTestBase.scala:982)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyPlan(TableTestBase.scala:847)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExecPlan(TableTestBase.scala:634)
	at org.apache.flink.table.planner.plan.common.AggregateReduceGroupingTestBase.verifyPlan(AggregateReduceGroupingTestBase.scala:347)
	at org.apache.flink.table.planner.plan.common.AggregateReduceGroupingTestBase.testAggOnInnerJoin2(AggregateReduceGroupingTestBase.scala:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:878)
	at com.google.common.collect.ImmutableList.copyOf(ImmutableList.java:229)
	at org.apache.calcite.tools.RelBuilder.groupKey(RelBuilder.java:1287)
	at org.apache.flink.table.planner.plan.rules.logical.FlinkAggregateJoinTransposeRule.onMatch(FlinkAggregateJoinTransposeRule.java:373)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:223)
	... 77 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 11:10:41 UTC 2022,,,,,,,,,,"0|z186fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 11:10;Sergey Nuyanzin;The reason was absence of checkerframework in classpath which is required since 1.27.0
The issue was fixed within https://issues.apache.org/jira/browse/FLINK-20873 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"round(float, int) fails to compile in Flink 1.14",FLINK-29142,13479283,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,TsReaper,TsReaper,30/Aug/22 09:58,31/Aug/22 02:14,04/Jun/24 20:41,,1.14.5,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"This bug affects only Flink 1.13 & 1.14, Flink >= 1.15 does not have this issue.

Add the following test to {{TableEnvironmentITCase}} to reproduce this bug.

{code:scala}
@Test
def myTest(): Unit = {
  tEnv.executeSql(
    """"""
      |create table T (
      |  a float
      |) with (
      |  'connector' = 'datagen',
      |  'number-of-rows' = '3'
      |)
      |"""""".stripMargin)
  tEnv.executeSql(""select round(a, 2) from T"").print()
}
{code}

The exception is
{code}
/* 1 */
/* 2 */      public class StreamExecCalc$3 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 7 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 8 */
/* 9 */        public StreamExecCalc$3(
/* 10 */            Object[] references,
/* 11 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 12 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 13 */            org.apache.flink.streaming.api.operators.Output output,
/* 14 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 15 */          this.references = references;
/* 16 */          
/* 17 */          this.setup(task, config, output);
/* 18 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 19 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 20 */              .setProcessingTimeService(processingTimeService);
/* 21 */          }
/* 22 */        }
/* 23 */
/* 24 */        @Override
/* 25 */        public void open() throws Exception {
/* 26 */          super.open();
/* 27 */          
/* 28 */        }
/* 29 */
/* 30 */        @Override
/* 31 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 32 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 33 */          
/* 34 */          float field$0;
/* 35 */          boolean isNull$0;
/* 36 */          boolean isNull$1;
/* 37 */          float result$2;
/* 38 */          
/* 39 */          
/* 40 */          isNull$0 = in1.isNullAt(0);
/* 41 */          field$0 = -1.0f;
/* 42 */          if (!isNull$0) {
/* 43 */            field$0 = in1.getFloat(0);
/* 44 */          }
/* 45 */          
/* 46 */          out.setRowKind(in1.getRowKind());
/* 47 */          
/* 48 */          
/* 49 */          
/* 50 */          
/* 51 */          
/* 52 */          
/* 53 */          isNull$1 = isNull$0 || false;
/* 54 */          result$2 = -1.0f;
/* 55 */          if (!isNull$1) {
/* 56 */            
/* 57 */            result$2 = 
/* 58 */          org.apache.calcite.runtime.SqlFunctions.sround(field$0, ((int) 2))
/* 59 */                     ;
/* 60 */            
/* 61 */          }
/* 62 */          
/* 63 */          if (isNull$1) {
/* 64 */            out.setNullAt(0);
/* 65 */          } else {
/* 66 */            out.setFloat(0, result$2);
/* 67 */          }
/* 68 */                    
/* 69 */                  
/* 70 */          output.collect(outElement.replace(out));
/* 71 */          
/* 72 */          
/* 73 */        }
/* 74 */
/* 75 */        
/* 76 */
/* 77 */        @Override
/* 78 */        public void close() throws Exception {
/* 79 */           super.close();
/* 80 */          
/* 81 */        }
/* 82 */
/* 83 */        
/* 84 */      }
/* 85 */    


java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
	at org.apache.flink.table.utils.PrintUtils.printAsTableauForm(PrintUtils.java:152)
	at org.apache.flink.table.api.internal.TableResultImpl.print(TableResultImpl.java:160)
	at org.apache.flink.table.api.TableEnvironmentITCase.myTest(TableEnvironmentITCase.scala:98)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:177)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:120)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 43 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:175)
	... 45 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:614)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1983)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:174)
	... 45 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$3'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:85)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:712)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:686)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:626)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:187)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:63)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:663)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:651)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:76)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:102)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:83)
	... 14 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:74)
	... 16 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:89)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:74)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 19 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 57, Column 23: Assignment conversion not possible from type ""double"" to type ""float""
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
	at org.codehaus.janino.UnitCompiler.assignmentConversion(UnitCompiler.java:11062)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3790)
	at org.codehaus.janino.UnitCompiler.access$6100(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3754)
	at org.codehaus.janino.UnitCompiler$13.visitAssignment(UnitCompiler.java:3734)
	at org.codehaus.janino.Java$Assignment.accept(Java.java:4477)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
	at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1553)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1493)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$Block.accept(Java.java:2779)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2476)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1495)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1487)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2950)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:86)
	... 25 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-30 09:58:40.0,,,,,,,,,,"0|z186ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Scala Suffixes from download instructions,FLINK-29141,13479280,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Aug/22 09:50,01/Sep/22 12:14,04/Jun/24 20:41,01/Sep/22 12:14,,,,,,,,,,,,Project Website,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 12:14:53 UTC 2022,,,,,,,,,,"0|z186e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 12:14;chesnay;asf-site: ee10ce51dc83eb69164bba28c2ccd4c87797f211;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add basic podtemplate validation,FLINK-29140,13479277,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gyfora,gyfora,30/Aug/22 09:31,24/Nov/22 01:03,04/Jun/24 20:41,30/Aug/22 20:19,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,We should add basic podTemplate validation such as main container exists etc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 20:19:46 UTC 2022,,,,,,,,,,"0|z186dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:31;morhidi;https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/native_kubernetes/#pod-template;;;","30/Aug/22 12:57;gaborgsomogyi;I've taken a look at the issue and seems like it has been resolved in FLINK-27856 in Flink version 1.15.2+.
Please see the [following|https://github.com/apache/flink/pull/20516/files#diff-de992ba74f3034810a44550bd1690fa08c04cc22f6bc6f54551a5b1050343698R423] code snippet for further details.
Missing spec and missing main container are handled properly so using Flink version 1.15.2+ is the proper solution.;;;","30/Aug/22 20:19;gyfora;merged to main 1b87903b8a48faf1531c99c6cdb73878224a27e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Almost every test inside FlinkSqlParserImplTest is failing after uprgade to calcite of higher versions,FLINK-29139,13479272,13449408,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/22 09:15,07/Mar/23 12:08,04/Jun/24 20:41,07/Mar/23 12:08,,,,,,,,,,,,Table SQL / API,,,,0,,,,,"It first there is dependency to linq4j which is currently excluded in pom.

After stopping excluding there are lots of failures because of different reasons (on the first look).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 12:08:31 UTC 2023,,,,,,,,,,"0|z186cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:08;Sergey Nuyanzin;Finally it could be fixed in step by step manner as it is already done for 1.27.0, 1.28.0, 1.29.0.

And in 1.30.0 PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project pushdown not work for lookup source,FLINK-29138,13479270,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,30/Aug/22 09:02,04/Jul/23 07:41,04/Jun/24 20:41,05/Sep/22 02:37,,,,,,,,1.14.6,1.15.3,1.16.0,,Table SQL / Planner,,,,0,pull-request-available,,,,"Current tests: LookupJoinTest#testJoinTemporalTableWithProjectionPushDown
{code:java}
@Test
def testJoinTemporalTableWithProjectionPushDown(): Unit = {
val sql =
""""""
|SELECT T.*, D.id
|FROM MyTable AS T
|JOIN LookupTable FOR SYSTEM_TIME AS OF T.proctime AS D
|ON T.a = D.id
"""""".stripMargin

util.verifyExecPlan(sql)
}

{code}
the optimized plan doesn't print the selected columns from lookup source, but actually it didn't push the project into lookup source (still select all columns from source), this is not as expected
{code:java}
<Resource name=""optimized exec plan"">
<![CDATA[
Calc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])
+- LookupJoin(table=[default_catalog.default_database.LookupTable], joinType=[InnerJoin], lookup=[id=a], select=[a, b, c, proctime, rowtime, id])
+- DataStreamScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
]]>
</Resource>

{code}
 

incorrect intermediate optimization result
{code:java}
=========  logical_rewrite ========
 optimize result: 
FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
:- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
+- FlinkLogicalSnapshot(period=[$cor0.proctime])
   +- FlinkLogicalCalc(select=[id])
      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])


=========  time_indicator ========
 optimize result: 
FlinkLogicalCalc(select=[a, b, c, PROCTIME_MATERIALIZE(proctime) AS proctime, rowtime, id])
+- FlinkLogicalJoin(condition=[=($0, $5)], joinType=[inner])
   :- FlinkLogicalDataStreamTableScan(table=[[default_catalog, default_database, MyTable]], fields=[a, b, c, proctime, rowtime])
   +- FlinkLogicalSnapshot(period=[$cor0.proctime])
      +- FlinkLogicalCalc(select=[id])
         +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, LookupTable]], fields=[id, name, age])

{code}
 

plan comparison after fix

!image-2022-08-30-20-33-24-105.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20840,,,,,,,"30/Aug/22 12:33;lincoln.86xy;image-2022-08-30-20-33-24-105.png;https://issues.apache.org/jira/secure/attachment/13048768/image-2022-08-30-20-33-24-105.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 07:41:54 UTC 2023,,,,,,,,,,"0|z186c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:11;lzljs3620320;[~lincoln.86xy] Thanks for the reporting. Assigned to u~;;;","31/Aug/22 02:24;lzljs3620320;I think we should fix this in 1.14 and 1.15 too, what do you think? [~lincoln.86xy] [~godfreyhe];;;","31/Aug/22 03:32;godfreyhe;+1 for 1.14 and 1.15, because this change does effect plan and state. The project will be pushed into lookup operator but not into the lookup source, so this is a inner change in lookup operator. WDYT ? [~lincoln.86xy][~lzljs3620320];;;","31/Aug/22 12:33;lincoln.86xy;[~godfreyhe] [~lzljs3620320]  Since it will not affect compatibility, I agree to fix it also in 1.14 and 1.15, this is a baisc and important optimization.;;;","05/Sep/22 02:37;lzljs3620320;master: 2e2fb24bdbc69c9d85883081b0c29f9db254088e
release-1.15: e7c7df4c9b07667344e33c23bb92cb8a07e3ac0b
release-1.14: 54b6b69941533b756dbd4f68b43d8d4118a8c4e5;;;","04/Jul/23 07:41;waywtdcc;hello, Can this pr be merged into 1.14.5? Which PR do you still rely on?   [~lzljs3620320] 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch docker images to eclipse temurin base,FLINK-29137,13479258,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,30/Aug/22 08:11,05/Sep/22 08:04,04/Jun/24 20:41,31/Aug/22 09:15,,,,,,,,,,,,flink-docker,,,,0,pull-request-available,,,,"The openjdk images have been deprecated, causing our images to be rejected from, official-images; switch to Temurin.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29161,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 09:15:06 UTC 2022,,,,,,,,,,"0|z1869c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 09:15;chesnay;docker-master: 3c259f46231b97202925a111a8205193c15bbf78
dev-master: 47747fa25eeccbbea31c11b6a8bfe957c490a91c
dev-1.15: ff76f1e2d2facca7d0ae946e1547db16762e9150
dev-1.14: 19f876183bb888576ac309c10a9ae5fa880282e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cast of integer array to string stopped working after upgrade to calcite 1.30,FLINK-29136,13479251,13449408,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/22 07:32,06/Mar/23 17:02,04/Jun/24 20:41,06/Mar/23 17:02,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,,"e.g. 
{noformat}

org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 878 to line 1, column 896: Cast function cannot convert value of type INTEGER ARRAY to type VARCHAR(2147483647)

	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:185)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:112)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:703)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$SqlResultTestItem.query(BuiltInFunctionTestBase.java:488)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:350)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:320)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:113)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:93)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 878 to line 1, column 896: Cast function cannot convert value of type INTEGER ARRAY to type VARCHAR(2147483647)
	at sun.reflect.GeneratedConstructorAccessor83.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:932)
	at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:917)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5159)
	at org.apache.calcite.sql.SqlCallBinding.newError(SqlCallBinding.java:374)
	at org.apache.calcite.sql.fun.SqlCastFunction.checkOperandTypes(SqlCastFunction.java:139)
	at org.apache.calcite.sql.SqlOperator.validateOperands(SqlOperator.java:499)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:335)
	at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:231)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6212)
	at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:6197)
	at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:161)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1859)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveType(SqlValidatorImpl.java:1850)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:450)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4322)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3568)
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:64)
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:89)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1043)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1018)
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:247)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:993)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:742)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:181)
	... 45 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Cast function cannot convert value of type INTEGER ARRAY to type VARCHAR(2147483647)
	at sun.reflect.GeneratedConstructorAccessor82.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:505)
	at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:599)
	... 69 more


{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-30 07:32:01.0,,,,,,,,,,"0|z1867s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
After updating to calcite 1.30.0 and janino 3.1.6 some tests started to fail with compilation issues in generated code,FLINK-29135,13479249,13449408,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/22 07:28,02/Feb/23 09:10,04/Jun/24 20:41,02/Feb/23 09:10,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,0,,,,," e.g. {{org.apache.flink.table.planner.functions.CastFunctionITCase}} for {{cast(f0, ARRAY<BIGINT>) }}
{noformat}
/* 1 */
/* 2 */      public class StreamExecCalc$4813 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4805 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4806 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4805, 3, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4813(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4806.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4806.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4806.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (true) {
/* 33 */            writer$4806.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4806.writeInt(1, ((int) -1));
/* 36 */          }
/* 37 */                    
/* 38 */          
/* 39 */          
/* 40 */          if (false) {
/* 41 */            writer$4806.setNullInt(2);
/* 42 */          } else {
/* 43 */            writer$4806.writeInt(2, ((int) 2));
/* 44 */          }
/* 45 */                    
/* 46 */          writer$4806.complete();
/* 47 */                   
/* 48 */          this.setup(task, config, output);
/* 49 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 50 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 51 */              .setProcessingTimeService(processingTimeService);
/* 52 */          }
/* 53 */        }
/* 54 */
/* 55 */        @Override
/* 56 */        public void open() throws Exception {
/* 57 */          super.open();
/* 58 */          
/* 59 */        }
/* 60 */
/* 61 */        @Override
/* 62 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 63 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 64 */          
/* 65 */          boolean isNull$4808;
/* 66 */          org.apache.flink.table.data.ArrayData result$4809;
/* 67 */          long result$4812;
/* 68 */          
/* 69 */          
/* 70 */          
/* 71 */          
/* 72 */          out.setRowKind(in1.getRowKind());
/* 73 */          
/* 74 */          
/* 75 */          
/* 76 */          
/* 77 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 78 */           isNull$4808 = false;
/* 79 */          if (!isNull$4808) {
/* 80 */          java.lang.Long[] objArray$4810 = new java.lang.Long[array$4805.size()];
/* 81 */          for (int i$4811 = 0; i$4811 < array$4805.size(); i$4811++) {
/* 82 */          if (!array$4805.isNullAt(i$4811)) {
/* 83 */          result$4812 = ((long)(array$4805.getInt(i$4811)));
/* 84 */          objArray$4810[i$4811] = result$4812;
/* 85 */          }
/* 86 */          }
/* 87 */          result$4809 = new org.apache.flink.table.data.GenericArrayData(objArray$4810);
/* 88 */          isNull$4808 = result$4809 == null;
/* 89 */          } else {
/* 90 */          result$4809 = null;
/* 91 */          }
/* 92 */          
/* 93 */           // --- End cast section
/* 94 */                         
/* 95 */          if (isNull$4808) {
/* 96 */            out.setNullAt(0);
/* 97 */          } else {
/* 98 */            out.setNonPrimitiveValue(0, result$4809);
/* 99 */          }
/* 100 */                    
/* 101 */                  
/* 102 */          output.collect(outElement.replace(out));
/* 103 */          
/* 104 */          
/* 105 */        }
/* 106 */
/* 107 */        
/* 108 */
/* 109 */        @Override
/* 110 */        public void close() throws Exception {
/* 111 */           super.close();
/* 112 */          
/* 113 */        }
/* 114 */
/* 115 */        
/* 116 */      }
/* 117 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4813 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4805 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4806 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4805, 3, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4813(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4806.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4806.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4806.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (true) {
/* 33 */            writer$4806.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4806.writeInt(1, ((int) -1));
/* 36 */          }
/* 37 */                    
/* 38 */          
/* 39 */          
/* 40 */          if (false) {
/* 41 */            writer$4806.setNullInt(2);
/* 42 */          } else {
/* 43 */            writer$4806.writeInt(2, ((int) 2));
/* 44 */          }
/* 45 */                    
/* 46 */          writer$4806.complete();
/* 47 */                   
/* 48 */          this.setup(task, config, output);
/* 49 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 50 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 51 */              .setProcessingTimeService(processingTimeService);
/* 52 */          }
/* 53 */        }
/* 54 */
/* 55 */        @Override
/* 56 */        public void open() throws Exception {
/* 57 */          super.open();
/* 58 */          
/* 59 */        }
/* 60 */
/* 61 */        @Override
/* 62 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 63 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 64 */          
/* 65 */          boolean isNull$4808;
/* 66 */          org.apache.flink.table.data.ArrayData result$4809;
/* 67 */          long result$4812;
/* 68 */          
/* 69 */          
/* 70 */          
/* 71 */          
/* 72 */          out.setRowKind(in1.getRowKind());
/* 73 */          
/* 74 */          
/* 75 */          
/* 76 */          
/* 77 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 78 */           isNull$4808 = false;
/* 79 */          if (!isNull$4808) {
/* 80 */          java.lang.Long[] objArray$4810 = new java.lang.Long[array$4805.size()];
/* 81 */          for (int i$4811 = 0; i$4811 < array$4805.size(); i$4811++) {
/* 82 */          if (!array$4805.isNullAt(i$4811)) {
/* 83 */          result$4812 = ((long)(array$4805.getInt(i$4811)));
/* 84 */          objArray$4810[i$4811] = result$4812;
/* 85 */          }
/* 86 */          }
/* 87 */          result$4809 = new org.apache.flink.table.data.GenericArrayData(objArray$4810);
/* 88 */          isNull$4808 = result$4809 == null;
/* 89 */          } else {
/* 90 */          result$4809 = null;
/* 91 */          }
/* 92 */          
/* 93 */           // --- End cast section
/* 94 */                         
/* 95 */          if (isNull$4808) {
/* 96 */            out.setNullAt(0);
/* 97 */          } else {
/* 98 */            out.setNonPrimitiveValue(0, result$4809);
/* 99 */          }
/* 100 */                    
/* 101 */                  
/* 102 */          output.collect(outElement.replace(out));
/* 103 */          
/* 104 */          
/* 105 */        }
/* 106 */
/* 107 */        
/* 108 */
/* 109 */        @Override
/* 110 */        public void close() throws Exception {
/* 111 */           super.close();
/* 112 */          
/* 113 */        }
/* 114 */
/* 115 */        
/* 116 */      }
/* 117 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4875 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4859 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4860 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4859, 2, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4875(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4860.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4860.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4860.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (false) {
/* 33 */            writer$4860.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4860.writeInt(1, ((int) 2));
/* 36 */          }
/* 37 */                    
/* 38 */          writer$4860.complete();
/* 39 */                   
/* 40 */          this.setup(task, config, output);
/* 41 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 42 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 43 */              .setProcessingTimeService(processingTimeService);
/* 44 */          }
/* 45 */        }
/* 46 */
/* 47 */        @Override
/* 48 */        public void open() throws Exception {
/* 49 */          super.open();
/* 50 */          
/* 51 */        }
/* 52 */
/* 53 */        @Override
/* 54 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 55 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 56 */          
/* 57 */          boolean isNull$4867;
/* 58 */          org.apache.flink.table.data.ArrayData result$4870;
/* 59 */          long result$4874;
/* 60 */          
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          out.setRowKind(in1.getRowKind());
/* 65 */          
/* 66 */          
/* 67 */          
/* 68 */          
/* 69 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 70 */           isNull$4867 = false;
/* 71 */          if (!isNull$4867) {
/* 72 */          long[] objArray$4871 = new long[array$4859.size()];
/* 73 */          for (int i$4872 = 0; i$4872 < array$4859.size(); i$4872++) {
/* 74 */          result$4874 = ((long)(array$4859.getInt(i$4872)));
/* 75 */          objArray$4871[i$4872] = result$4874;
/* 76 */          }
/* 77 */          result$4870 = new org.apache.flink.table.data.GenericArrayData(objArray$4871);
/* 78 */          isNull$4867 = result$4870 == null;
/* 79 */          } else {
/* 80 */          result$4870 = null;
/* 81 */          }
/* 82 */          
/* 83 */           // --- End cast section
/* 84 */                         
/* 85 */          if (isNull$4867) {
/* 86 */            out.setNullAt(0);
/* 87 */          } else {
/* 88 */            out.setNonPrimitiveValue(0, result$4870);
/* 89 */          }
/* 90 */                    
/* 91 */                  
/* 92 */          output.collect(outElement.replace(out));
/* 93 */          
/* 94 */          
/* 95 */        }
/* 96 */
/* 97 */        
/* 98 */
/* 99 */        @Override
/* 100 */        public void close() throws Exception {
/* 101 */           super.close();
/* 102 */          
/* 103 */        }
/* 104 */
/* 105 */        
/* 106 */      }
/* 107 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4875 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4859 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4860 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4859, 2, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4875(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4860.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4860.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4860.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (false) {
/* 33 */            writer$4860.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4860.writeInt(1, ((int) 2));
/* 36 */          }
/* 37 */                    
/* 38 */          writer$4860.complete();
/* 39 */                   
/* 40 */          this.setup(task, config, output);
/* 41 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 42 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 43 */              .setProcessingTimeService(processingTimeService);
/* 44 */          }
/* 45 */        }
/* 46 */
/* 47 */        @Override
/* 48 */        public void open() throws Exception {
/* 49 */          super.open();
/* 50 */          
/* 51 */        }
/* 52 */
/* 53 */        @Override
/* 54 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 55 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 56 */          
/* 57 */          boolean isNull$4867;
/* 58 */          org.apache.flink.table.data.ArrayData result$4870;
/* 59 */          long result$4874;
/* 60 */          
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          out.setRowKind(in1.getRowKind());
/* 65 */          
/* 66 */          
/* 67 */          
/* 68 */          
/* 69 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 70 */           isNull$4867 = false;
/* 71 */          if (!isNull$4867) {
/* 72 */          long[] objArray$4871 = new long[array$4859.size()];
/* 73 */          for (int i$4872 = 0; i$4872 < array$4859.size(); i$4872++) {
/* 74 */          result$4874 = ((long)(array$4859.getInt(i$4872)));
/* 75 */          objArray$4871[i$4872] = result$4874;
/* 76 */          }
/* 77 */          result$4870 = new org.apache.flink.table.data.GenericArrayData(objArray$4871);
/* 78 */          isNull$4867 = result$4870 == null;
/* 79 */          } else {
/* 80 */          result$4870 = null;
/* 81 */          }
/* 82 */          
/* 83 */           // --- End cast section
/* 84 */                         
/* 85 */          if (isNull$4867) {
/* 86 */            out.setNullAt(0);
/* 87 */          } else {
/* 88 */            out.setNonPrimitiveValue(0, result$4870);
/* 89 */          }
/* 90 */                    
/* 91 */                  
/* 92 */          output.collect(outElement.replace(out));
/* 93 */          
/* 94 */          
/* 95 */        }
/* 96 */
/* 97 */        
/* 98 */
/* 99 */        @Override
/* 100 */        public void close() throws Exception {
/* 101 */           super.close();
/* 102 */          
/* 103 */        }
/* 104 */
/* 105 */        
/* 106 */      }
/* 107 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4838 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4823 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4824 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4823, 3, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4838(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4824.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4824.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4824.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (true) {
/* 33 */            writer$4824.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4824.writeInt(1, ((int) -1));
/* 36 */          }
/* 37 */                    
/* 38 */          
/* 39 */          
/* 40 */          if (false) {
/* 41 */            writer$4824.setNullInt(2);
/* 42 */          } else {
/* 43 */            writer$4824.writeInt(2, ((int) 2));
/* 44 */          }
/* 45 */                    
/* 46 */          writer$4824.complete();
/* 47 */                   
/* 48 */          this.setup(task, config, output);
/* 49 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 50 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 51 */              .setProcessingTimeService(processingTimeService);
/* 52 */          }
/* 53 */        }
/* 54 */
/* 55 */        @Override
/* 56 */        public void open() throws Exception {
/* 57 */          super.open();
/* 58 */          
/* 59 */        }
/* 60 */
/* 61 */        @Override
/* 62 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 63 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 64 */          
/* 65 */          boolean isNull$4827;
/* 66 */          org.apache.flink.table.data.ArrayData result$4828;
/* 67 */          long result$4831;
/* 68 */          
/* 69 */          
/* 70 */          
/* 71 */          
/* 72 */          out.setRowKind(in1.getRowKind());
/* 73 */          
/* 74 */          
/* 75 */          
/* 76 */          
/* 77 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 78 */           isNull$4827 = false;
/* 79 */          if (!isNull$4827) {
/* 80 */          java.lang.Long[] objArray$4829 = new java.lang.Long[array$4823.size()];
/* 81 */          for (int i$4830 = 0; i$4830 < array$4823.size(); i$4830++) {
/* 82 */          if (!array$4823.isNullAt(i$4830)) {
/* 83 */          result$4831 = ((long)(array$4823.getInt(i$4830)));
/* 84 */          objArray$4829[i$4830] = result$4831;
/* 85 */          }
/* 86 */          }
/* 87 */          result$4828 = new org.apache.flink.table.data.GenericArrayData(objArray$4829);
/* 88 */          isNull$4827 = result$4828 == null;
/* 89 */          } else {
/* 90 */          result$4828 = null;
/* 91 */          }
/* 92 */          
/* 93 */           // --- End cast section
/* 94 */                         
/* 95 */          if (isNull$4827) {
/* 96 */            out.setNullAt(0);
/* 97 */          } else {
/* 98 */            out.setNonPrimitiveValue(0, result$4828);
/* 99 */          }
/* 100 */                    
/* 101 */                  
/* 102 */          output.collect(outElement.replace(out));
/* 103 */          
/* 104 */          
/* 105 */        }
/* 106 */
/* 107 */        
/* 108 */
/* 109 */        @Override
/* 110 */        public void close() throws Exception {
/* 111 */           super.close();
/* 112 */          
/* 113 */        }
/* 114 */
/* 115 */        
/* 116 */      }
/* 117 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4838 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4823 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4824 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4823, 3, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4838(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4824.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4824.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4824.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (true) {
/* 33 */            writer$4824.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4824.writeInt(1, ((int) -1));
/* 36 */          }
/* 37 */                    
/* 38 */          
/* 39 */          
/* 40 */          if (false) {
/* 41 */            writer$4824.setNullInt(2);
/* 42 */          } else {
/* 43 */            writer$4824.writeInt(2, ((int) 2));
/* 44 */          }
/* 45 */                    
/* 46 */          writer$4824.complete();
/* 47 */                   
/* 48 */          this.setup(task, config, output);
/* 49 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 50 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 51 */              .setProcessingTimeService(processingTimeService);
/* 52 */          }
/* 53 */        }
/* 54 */
/* 55 */        @Override
/* 56 */        public void open() throws Exception {
/* 57 */          super.open();
/* 58 */          
/* 59 */        }
/* 60 */
/* 61 */        @Override
/* 62 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 63 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 64 */          
/* 65 */          boolean isNull$4827;
/* 66 */          org.apache.flink.table.data.ArrayData result$4828;
/* 67 */          long result$4831;
/* 68 */          
/* 69 */          
/* 70 */          
/* 71 */          
/* 72 */          out.setRowKind(in1.getRowKind());
/* 73 */          
/* 74 */          
/* 75 */          
/* 76 */          
/* 77 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 78 */           isNull$4827 = false;
/* 79 */          if (!isNull$4827) {
/* 80 */          java.lang.Long[] objArray$4829 = new java.lang.Long[array$4823.size()];
/* 81 */          for (int i$4830 = 0; i$4830 < array$4823.size(); i$4830++) {
/* 82 */          if (!array$4823.isNullAt(i$4830)) {
/* 83 */          result$4831 = ((long)(array$4823.getInt(i$4830)));
/* 84 */          objArray$4829[i$4830] = result$4831;
/* 85 */          }
/* 86 */          }
/* 87 */          result$4828 = new org.apache.flink.table.data.GenericArrayData(objArray$4829);
/* 88 */          isNull$4827 = result$4828 == null;
/* 89 */          } else {
/* 90 */          result$4828 = null;
/* 91 */          }
/* 92 */          
/* 93 */           // --- End cast section
/* 94 */                         
/* 95 */          if (isNull$4827) {
/* 96 */            out.setNullAt(0);
/* 97 */          } else {
/* 98 */            out.setNonPrimitiveValue(0, result$4828);
/* 99 */          }
/* 100 */                    
/* 101 */                  
/* 102 */          output.collect(outElement.replace(out));
/* 103 */          
/* 104 */          
/* 105 */        }
/* 106 */
/* 107 */        
/* 108 */
/* 109 */        @Override
/* 110 */        public void close() throws Exception {
/* 111 */           super.close();
/* 112 */          
/* 113 */        }
/* 114 */
/* 115 */        
/* 116 */      }
/* 117 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4821 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4804 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4807 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4804, 2, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4821(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4807.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4807.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4807.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (false) {
/* 33 */            writer$4807.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4807.writeInt(1, ((int) 2));
/* 36 */          }
/* 37 */                    
/* 38 */          writer$4807.complete();
/* 39 */                   
/* 40 */          this.setup(task, config, output);
/* 41 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 42 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 43 */              .setProcessingTimeService(processingTimeService);
/* 44 */          }
/* 45 */        }
/* 46 */
/* 47 */        @Override
/* 48 */        public void open() throws Exception {
/* 49 */          super.open();
/* 50 */          
/* 51 */        }
/* 52 */
/* 53 */        @Override
/* 54 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 55 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 56 */          
/* 57 */          boolean isNull$4814;
/* 58 */          org.apache.flink.table.data.ArrayData result$4815;
/* 59 */          long result$4818;
/* 60 */          
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          out.setRowKind(in1.getRowKind());
/* 65 */          
/* 66 */          
/* 67 */          
/* 68 */          
/* 69 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 70 */           isNull$4814 = false;
/* 71 */          if (!isNull$4814) {
/* 72 */          long[] objArray$4816 = new long[array$4804.size()];
/* 73 */          for (int i$4817 = 0; i$4817 < array$4804.size(); i$4817++) {
/* 74 */          result$4818 = ((long)(array$4804.getInt(i$4817)));
/* 75 */          objArray$4816[i$4817] = result$4818;
/* 76 */          }
/* 77 */          result$4815 = new org.apache.flink.table.data.GenericArrayData(objArray$4816);
/* 78 */          isNull$4814 = result$4815 == null;
/* 79 */          } else {
/* 80 */          result$4815 = null;
/* 81 */          }
/* 82 */          
/* 83 */           // --- End cast section
/* 84 */                         
/* 85 */          if (isNull$4814) {
/* 86 */            out.setNullAt(0);
/* 87 */          } else {
/* 88 */            out.setNonPrimitiveValue(0, result$4815);
/* 89 */          }
/* 90 */                    
/* 91 */                  
/* 92 */          output.collect(outElement.replace(out));
/* 93 */          
/* 94 */          
/* 95 */        }
/* 96 */
/* 97 */        
/* 98 */
/* 99 */        @Override
/* 100 */        public void close() throws Exception {
/* 101 */           super.close();
/* 102 */          
/* 103 */        }
/* 104 */
/* 105 */        
/* 106 */      }
/* 107 */    

/* 1 */
/* 2 */      public class StreamExecCalc$4821 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        
/* 7 */        org.apache.flink.table.data.binary.BinaryArrayData array$4804 = new org.apache.flink.table.data.binary.BinaryArrayData();
/* 8 */        org.apache.flink.table.data.writer.BinaryArrayWriter writer$4807 = new org.apache.flink.table.data.writer.BinaryArrayWriter(array$4804, 2, 4);
/* 9 */               
/* 10 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(1);
/* 11 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 12 */
/* 13 */        public StreamExecCalc$4821(
/* 14 */            Object[] references,
/* 15 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 16 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 17 */            org.apache.flink.streaming.api.operators.Output output,
/* 18 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 19 */          this.references = references;
/* 20 */          
/* 21 */          writer$4807.reset();
/* 22 */          
/* 23 */          
/* 24 */          if (false) {
/* 25 */            writer$4807.setNullInt(0);
/* 26 */          } else {
/* 27 */            writer$4807.writeInt(0, ((int) 1));
/* 28 */          }
/* 29 */                    
/* 30 */          
/* 31 */          
/* 32 */          if (false) {
/* 33 */            writer$4807.setNullInt(1);
/* 34 */          } else {
/* 35 */            writer$4807.writeInt(1, ((int) 2));
/* 36 */          }
/* 37 */                    
/* 38 */          writer$4807.complete();
/* 39 */                   
/* 40 */          this.setup(task, config, output);
/* 41 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 42 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 43 */              .setProcessingTimeService(processingTimeService);
/* 44 */          }
/* 45 */        }
/* 46 */
/* 47 */        @Override
/* 48 */        public void open() throws Exception {
/* 49 */          super.open();
/* 50 */          
/* 51 */        }
/* 52 */
/* 53 */        @Override
/* 54 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 55 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 56 */          
/* 57 */          boolean isNull$4814;
/* 58 */          org.apache.flink.table.data.ArrayData result$4815;
/* 59 */          long result$4818;
/* 60 */          
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          out.setRowKind(in1.getRowKind());
/* 65 */          
/* 66 */          
/* 67 */          
/* 68 */          
/* 69 */           // --- Cast section generated by org.apache.flink.table.planner.functions.casting.ArrayToArrayCastRule
/* 70 */           isNull$4814 = false;
/* 71 */          if (!isNull$4814) {
/* 72 */          long[] objArray$4816 = new long[array$4804.size()];
/* 73 */          for (int i$4817 = 0; i$4817 < array$4804.size(); i$4817++) {
/* 74 */          result$4818 = ((long)(array$4804.getInt(i$4817)));
/* 75 */          objArray$4816[i$4817] = result$4818;
/* 76 */          }
/* 77 */          result$4815 = new org.apache.flink.table.data.GenericArrayData(objArray$4816);
/* 78 */          isNull$4814 = result$4815 == null;
/* 79 */          } else {
/* 80 */          result$4815 = null;
/* 81 */          }
/* 82 */          
/* 83 */           // --- End cast section
/* 84 */                         
/* 85 */          if (isNull$4814) {
/* 86 */            out.setNullAt(0);
/* 87 */          } else {
/* 88 */            out.setNonPrimitiveValue(0, result$4815);
/* 89 */          }
/* 90 */                    
/* 91 */                  
/* 92 */          output.collect(outElement.replace(out));
/* 93 */          
/* 94 */          
/* 95 */        }
/* 96 */
/* 97 */        
/* 98 */
/* 99 */        @Override
/* 100 */        public void close() throws Exception {
/* 101 */           super.close();
/* 102 */          
/* 103 */        }
/* 104 */
/* 105 */        
/* 106 */      }
/* 107 */    


java.lang.RuntimeException: Failed to fetch next result

	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
	at org.assertj.core.internal.Iterators.assertHasNext(Iterators.java:49)
	at org.assertj.core.api.AbstractIteratorAssert.hasNext(AbstractIteratorAssert.java:60)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:356)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:320)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:113)
	at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:93)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.io.IOException: Failed to fetch job execution result
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	... 44 more
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
	... 46 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:138)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:181)
	... 46 more
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
	at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.RuntimeException: Could not instantiate generated class 'StreamExecCalc$4838'
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:84)
	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperator(OperatorChain.java:762)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOperatorChain(OperatorChain.java:736)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:676)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:195)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:60)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:678)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:666)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:82)
	... 14 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
	... 16 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
	... 19 more
Caused by: org.codehaus.commons.compiler.InternalCompilerException: Compiling ""StreamExecCalc$4838"" in Line 2, Column 14: Line 62, Column 21: Compiling ""processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element)"": Line 81, Column 32: Compiling ""i$4830"": Invalid local variable index 8
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:369)
	at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)
	at org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:330)
	at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:367)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:330)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:245)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:473)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:223)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:209)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:82)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
	... 25 more
Caused by: org.codehaus.commons.compiler.InternalCompilerException: Line 62, Column 21: Compiling ""processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element)"": Line 81, Column 32: Compiling ""i$4830"": Invalid local variable index 8
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3222)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1379)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1352)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:800)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:412)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:391)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:386)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1692)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:386)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:359)
	... 37 more
Caused by: org.codehaus.commons.compiler.InternalCompilerException: Line 81, Column 32: Compiling ""i$4830"": Invalid local variable index 8
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5731)
	at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4369)
	at org.codehaus.janino.UnitCompiler.access$6500(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$13.visitBinaryOperation(UnitCompiler.java:4166)
	at org.codehaus.janino.UnitCompiler$13.visitBinaryOperation(UnitCompiler.java:4144)
	at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:5222)
	at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:4144)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1714)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$6.visitForStatement(UnitCompiler.java:1532)
	at org.codehaus.janino.UnitCompiler$6.visitForStatement(UnitCompiler.java:1523)
	at org.codehaus.janino.Java$ForStatement.accept(Java.java:3321)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1523)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1607)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1592)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1529)
	at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1523)
	at org.codehaus.janino.Java$Block.accept(Java.java:3103)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1523)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2509)
	at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1531)
	at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1523)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:3274)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1523)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1607)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3531)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3218)
	... 47 more
Caused by: org.codehaus.commons.compiler.InternalCompilerException: Invalid local variable index 8
	at org.codehaus.janino.UnitCompiler.getLocalVariableTypeInfo(UnitCompiler.java:13214)
	at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:12232)
	at org.codehaus.janino.UnitCompiler.load(UnitCompiler.java:12209)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4679)
	at org.codehaus.janino.UnitCompiler.access$7900(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$15$1.visitLocalVariableAccess(UnitCompiler.java:4616)
	at org.codehaus.janino.UnitCompiler$15$1.visitLocalVariableAccess(UnitCompiler.java:4608)
	at org.codehaus.janino.Java$LocalVariableAccess.accept(Java.java:4632)
	at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4608)
	at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4604)
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4498)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4604)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4675)
	at org.codehaus.janino.UnitCompiler.access$7400(UnitCompiler.java:231)
	at org.codehaus.janino.UnitCompiler$15$1.visitAmbiguousName(UnitCompiler.java:4611)
	at org.codehaus.janino.UnitCompiler$15$1.visitAmbiguousName(UnitCompiler.java:4608)
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4574)
	at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4608)
	at org.codehaus.janino.UnitCompiler$15.visitLvalue(UnitCompiler.java:4604)
	at org.codehaus.janino.Java$Lvalue.accept(Java.java:4498)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4604)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5729)
	... 75 more


{noformat}",,,,,,,,,,,,,,,,,,,,,FLINK-27995,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 09:10:29 UTC 2023,,,,,,,,,,"0|z1867c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:10;Sergey Nuyanzin;Close it in favor of https://issues.apache.org/jira/browse/FLINK-27995;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fetch metrics may cause oom(ThreadPool task pile up),FLINK-29134,13479236,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,nagist,nagist,30/Aug/22 06:43,26/Oct/22 02:49,04/Jun/24 20:41,26/Oct/22 02:49,1.11.0,1.15.2,,,,,,1.15.3,1.16.1,1.17.0,,Runtime / Metrics,,,,0,pull-request-available,,,,"When we queryMetrics we use thread pool to process the data which are returned by TMs. 
{code:java}
private void queryMetrics(final MetricQueryServiceGateway queryServiceGateway) {
    LOG.debug(""Query metrics for {}."", queryServiceGateway.getAddress());

    queryServiceGateway
            .queryMetrics(timeout)
            .whenCompleteAsync(
                    (MetricDumpSerialization.MetricSerializationResult result, Throwable t) -> {
                        if (t != null) {
                            LOG.debug(""Fetching metrics failed."", t);
                        } else {
                            metrics.addAll(deserializer.deserialize(result));
                        }
                    },
                    executor);
} {code}
The only condition we will fetch metrics is update time is larger than updateInterval
{code:java}
public void update() {
    synchronized (this) {
        long currentTime = System.currentTimeMillis();
        if (currentTime - lastUpdateTime > updateInterval) {
            lastUpdateTime = currentTime;
            fetchMetrics();
        }
    }
} {code}
Therefore, if we could not process the data in update-interval-time, metrics data will accumulate.

Besides, webMonitorEndpoint, restHandlers and metrics share the thread pool. 

When we open ui, it maybe even worse.
{code:java}
final ScheduledExecutorService executor =
        WebMonitorEndpoint.createExecutorService(
                configuration.getInteger(RestOptions.SERVER_NUM_THREADS),
                configuration.getInteger(RestOptions.SERVER_THREAD_PRIORITY),
                ""DispatcherRestEndpoint"");

final long updateInterval =
        configuration.getLong(MetricOptions.METRIC_FETCHER_UPDATE_INTERVAL);
final MetricFetcher metricFetcher =
        updateInterval == 0
                ? VoidMetricFetcher.INSTANCE
                : MetricFetcherImpl.fromConfiguration(
                        configuration,
                        metricQueryServiceRetriever,
                        dispatcherGatewayRetriever,
                        executor);

webMonitorEndpoint =
        restEndpointFactory.createRestEndpoint(
                configuration,
                dispatcherGatewayRetriever,
                resourceManagerGatewayRetriever,
                blobServer,
                executor,
                metricFetcher,
                highAvailabilityServices.getClusterRestEndpointLeaderElectionService(),
                fatalErrorHandler); {code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/22 06:43;nagist;dump-queueTask.png;https://issues.apache.org/jira/secure/attachment/13048751/dump-queueTask.png","30/Aug/22 06:43;nagist;dump-threadPool.png;https://issues.apache.org/jira/secure/attachment/13048752/dump-threadPool.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 26 02:49:29 UTC 2022,,,,,,,,,,"0|z1864g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 06:57;martijnvisser;[~nagist] The Flink community doesn't support Flink 1.11 anymore, can you please verify this with either Flink 1.14 or Flink 1.15? ;;;","30/Aug/22 07:03;nagist;Hi, [~martijnvisser]. I check the code of master, fetch-metrics still has this problem.;;;","30/Aug/22 10:52;martijnvisser;[~chesnay] Any thoughts on this?;;;","13/Sep/22 07:51;nagist;Hi, [~chesnay]. Any thoughts on this issue？;;;","20/Oct/22 08:01;xtsong;I think this can be fixed by checking whether the processing of previous fetched data has completed in {{update()}}, and skipping the fetch if not.

[~tanyuxin], could you help fix this?;;;","20/Oct/22 09:20;tanyuxin;[~xtsong] Thanks for your idea. OK, I will take a look at this issue.;;;","26/Oct/22 02:49;xtsong;- master (1.17): 93af1e45a1e868ad3752923453562861d831104f
- release-1.16: c52e1519b509b6a918df037fe3665872ed8146fb
- release-1.15: eeda260d71ec11476edf532c5ff655828b0c4c7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adapt to new calcite rule configuration api,FLINK-29133,13479228,13449408,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,30/Aug/22 06:17,02/Feb/23 09:06,04/Jun/24 20:41,02/Feb/23 09:06,,,,,,,,1.17.0,,,,Table SQL / API,Table SQL / Runtime,,,0,,,,,"The new API for rule configuration started being added in 1.25.0 [1] and finished at 1.29.0 [2] with removal of some entities e.g. {{RelRule.Config.EMPTY}} which are used across the code of rules.
So current code uses deprecated. It would make sense to move a new one. Probably some work could be done before calcite upgrade

[1] https://issues.apache.org/jira/browse/CALCITE-3923 
[2] https://issues.apache.org/jira/browse/CALCITE-4839",,,,,,,,,,,,,,,,,,,,,FLINK-29215,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 09:06:51 UTC 2023,,,,,,,,,,"0|z1862o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:06;Sergey Nuyanzin;closed as duplicate of https://issues.apache.org/jira/browse/FLINK-29215;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SubtaskMetricStore causes memory leak.,FLINK-29132,13479208,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pltbkd,nagist,nagist,30/Aug/22 03:08,08/Sep/22 12:59,04/Jun/24 20:41,08/Sep/22 12:59,1.16.0,,,,,,,1.16.0,,,,Runtime / Metrics,,,,0,pull-request-available,,,,"In [FLINK-28588], MetricStore supports multiple attempts of a subtask. However, `SubtaskMetricStore` doesn't have a clean mechanism.  In failover scenario, `attempts` pile up and cause OOM.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28588,,,,,,,,,,,,,,,"30/Aug/22 06:09;nagist;dump-attempts.png;https://issues.apache.org/jira/secure/attachment/13048745/dump-attempts.png","30/Aug/22 06:09;nagist;dump-metricStore.png;https://issues.apache.org/jira/secure/attachment/13048744/dump-metricStore.png","30/Aug/22 03:56;mayuehappy;image-2022-08-30-11-56-34-608.png;https://issues.apache.org/jira/secure/attachment/13048739/image-2022-08-30-11-56-34-608.png","30/Aug/22 03:58;mayuehappy;image-2022-08-30-11-57-59-325.png;https://issues.apache.org/jira/secure/attachment/13048740/image-2022-08-30-11-57-59-325.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 12:59:51 UTC 2022,,,,,,,,,,"0|z185y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 03:58;mayuehappy; i think u may add some log or heap dump analysis to better describe the problem;;;","30/Aug/22 08:02;nagist;updated. thanks for your comment;;;","31/Aug/22 06:55;nagist;Anyone has any thoughts on this?;;;","31/Aug/22 08:25;chesnay;True, the attempts will continue to pile up until the job/tm terminates.

[~pltbkd] please fix by leveraging the execution history size parameter;;;","31/Aug/22 10:33;pltbkd;[~nagist] 
Thanks a lot for reporting the issue! 
And thanks [~chesnay] and [~mayuehappy] for taking care of it! 
I'll fix the issue as soon as possible. ;;;","08/Sep/22 12:59;gaoyunhaii;Merged on master via 26eeabfdd1f5f976ed1b5d761a3469bbcb7d3223

Merged on 1.16 via 4dad2e29a090c731a0474a80e320264040c348ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator webhook can use hostPort,FLINK-29131,13479111,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,dylanmei,dylanmei,dylanmei,29/Aug/22 13:12,16/Jan/23 20:04,04/Jun/24 20:41,16/Jan/23 20:04,kubernetes-operator-1.1.0,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"When running Flink operator on EKS cluster with Calico networking the control-plane (managed by AWS) cannot reach the webhook. Requests to create Flink resources fail with {_}Address is not allowed{_}.

When the webhook listens on hostPort the requests to create Flink resources are successful. However, a pod security policy is generally required to allow webhook to listen on such ports.

To support this scenario with the Helm chart make changes so that we can
 * Specify a hostPort value for the webhook
 * Name the port that the webhook listens on
 * Use the named port in the webhook service
 * Add a ""use"" pod security policy verb to cluster role",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 04 19:58:28 UTC 2022,,,,,,,,,,"0|z185cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 15:06;dylanmei;[~gyfora] as part of FLINK-28218 we declared a port :8085 for the health check. This allows, for example, user to port-forward to the health check point.

However,  if I set hostNetwork=true I am forced to include the health port to the host networking, which is certainly undesired.

As a part of this MR I currently intend to leave the probes but remove the port declaration unless there is another idea.;;;","11/Sep/22 15:16;dylanmei;I have come to conclusion that setting 'hostNetwork: true' on pod for the webhook causes unwelcome change for the operator:
 * liveness probe must also become available on host
 * metrics port must also become available on host

So I think it's worth exploring a single-use proxy to run on host networking and stream requests from control-plane to flink-operator-webhook-service.;;;","15/Sep/22 14:36;dylanmei;I've abandoned my original approach due to the invasiveness of host networking on the operator pod. Referencing [the Helm chart for cert-manager|[https://github.com/cert-manager/cert-manager/blob/release-1.9/deploy/charts/cert-manager/values.yaml#L342-L351]] it seems a good approach is to separate the webhook container into its own pod. I did successfully after specifying `webhook: \{create: false}` and separately creating the webhook as its own deployment.

Applying this strategy to Helm chart is a large undertaking may warrant a FLIP.;;;","19/Sep/22 06:34;gyfora;Thank you for exploring the options [~dylanmei] . Do we need a seperate deployment or a separate pod is enough within the same deployment?;;;","20/Sep/22 11:34;dylanmei;This can be in the same Helm chart, but as a separate deployment alongside the Operator. It also has a distinct ServiceAccount and ClusterRole from the Operator, as it only requires privileges to use the PodSecurityPolicy.;;;","24/Sep/22 09:11;gyfora;[~dylanmei] are you planning to work on this?;;;","25/Sep/22 20:54;dylanmei;The Helm chart changes dramatically to do this work. I'm not even clear if it could cleanly upgrade. Testing many upgrade scenarios is intimidating.

When I first brought up this idea in Slack we agreed

_if you have time opening a JIRA ticket and a minimal PR to address it, we would be happy to review and merge this_

Do we still envision a small change? ;;;","26/Sep/22 11:15;gyfora;I agree changing the helm chart to have a single Deployment to 2 deployments (operator + webhook separately) is a quite big change and will affect every user who currently uses the webhook.

Before we go any further I think it might make sense to write a proposal on the dev mailing list and discuss this openly + have a FLIP afterwards if there is a general agreement.

 ;;;","04/Oct/22 13:45;dylanmei;In the course of all these investigations, my team and I have drifted away from using Helm to deploy operator and its webhook. It is a wonderful, low-friction tool normally. But we had at least these problems with it:
 * Webhook not reachable in EKS clusters using Calico networking
 * Helm does not update CRDs after initially installing them

We used what we learned working with Helm to author our own Terraform modules to apply on our many AWS EKS clusters.

Although not using Helm, I am in a good position to describe the work required and offer usability suggestions to benefit future Helmers. I am happy to write a proposal on the dev mailing list.;;;","04/Oct/22 19:58;gyfora;If you feel that it would be enough to extend/rework our helm documentation section ([https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/helm/)] that would be an extremely valuable contribution :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the doc description of state.backend.local-recovery,FLINK-29130,13479104,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,29/Aug/22 12:41,31/Aug/22 09:47,04/Jun/24 20:41,31/Aug/22 09:47,,,,,,,,1.14.6,1.15.3,1.16.0,,Documentation,Runtime / State Backends,,,0,pull-request-available,,,,"Currently, the docs of configuration {{state.backend.local-recovery}} said wrongly on the active scope of this configuration. MemoryStateBackend and HashMapStateBackend actually support this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 09:47:55 UTC 2022,,,,,,,,,,"0|z185bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 09:47;yunta;merged 

master(1.16.0): 581e1fe3682e95e0935e0cd1cf4565529af49f84

release-1.15: ad33fe8ef104801a6470b11c792d6663120c172e

release-1.14: 2fa574910d52804f7bbdeb9597d736644aca7301;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Try best to push down value filter,FLINK-29129,13479065,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,29/Aug/22 10:05,05/Sep/22 13:33,04/Jun/24 20:41,05/Sep/22 13:33,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"During reading, if generated section (by interval partition) does not have overlapped key ranges, value filters can be pushed down alongside with key filters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 05 13:33:48 UTC 2022,,,,,,,,,,"0|z1852w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 13:33;lzljs3620320;master: 769f19d8566f0a30d3a7c09b297b9dce3280ac1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncatch IllegalStateException found when log split changes handling result in KafkaPartitionSplitReader,FLINK-29128,13479039,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,Leo zhang,Leo zhang,Leo zhang,29/Aug/22 08:20,11/Oct/23 18:51,04/Jun/24 20:41,,1.14.5,1.15.3,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,stale-assigned,,,"When logger is set to debug mode, KafkaPartitionSplitReader#maybeLogSplitChangesHandingResult log the handing result of all  SplitsChange<KafkaPartitionSplit>, and the handling result include the kafka partition's starting offset, which is get from kafka api(consumer.position).

When a SplitsChange<KafkaPartitionSplit> is a empty split,it will be removed(unassign partition), IllegalStateException will be thrown by consumer.position, since we can only check the position for partitions assigned to the consumer.And this exception has not been catch, and is rethrown as RuntimeExption, which lead to a failure of the application's execution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 10:35:09 UTC 2023,,,,,,,,,,"0|z184x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 17:11;mason6345;I wonder if we did not solve ""empty partitions"" correctly. 

 

In the bounded case, yes, we don't care about these empty splits. But for the unbounded case, the empty splits may eventually contain data, so we should assign them.;;;","31/Aug/22 03:50;Leo zhang;Actually, for unbounded case, we do assign the empty splits since it may eventually contain data. The ""empty split"" is only for bounded case, for unbounded case, this ""empty split"" is never considered as empty .

Following I would show some evidence:

1、By default, if no stopping offset is set in the constructor, NO_STOPPING_OFFSET which will be  assigned as  Long.MIN_VALUE, a negative number.

public KafkaPartitionSplit(TopicPartition tp, long startingOffset)

{ this(tp, startingOffset, NO_STOPPING_OFFSET); }

2、In KafkaPartitionSplit#getStoppingOffset, any negative number except LATEST_OFFSET (-1),COMMITTED_OFFSET (-3), will be considered as no stopping offset, so we return Optional.empty().

public Optional<Long> getStoppingOffset()

{ return stoppingOffset >= 0 || stoppingOffset == LATEST_OFFSET // -1 || stoppingOffset == COMMITTED_OFFSET //-3 ? Optional.of(stoppingOffset) : Optional.empty(); }

3、In KafkaPartitionSplitReader#parseStoppingOffsets, only when split.getStoppingOffset().ifPresent, we do parse the stopping offset.This means if the stopping offset is empty,  no stopping offset will be parse.

private void parseStoppingOffsets(
KafkaPartitionSplit split,
List<TopicPartition> partitionsStoppingAtLatest,
Set<TopicPartition> partitionsStoppingAtCommitted) {
TopicPartition tp = split.getTopicPartition();
split.getStoppingOffset()
.ifPresent(
stoppingOffset -> {
if (stoppingOffset >= 0)

{ stoppingOffsets.put(tp, stoppingOffset); }

else if (stoppingOffset == KafkaPartitionSplit.LATEST_OFFSET)

{ partitionsStoppingAtLatest.add(tp); }

else if (stoppingOffset == KafkaPartitionSplit.COMMITTED_OFFSET)

{ partitionsStoppingAtCommitted.add(tp); }

else

{ // This should not happen. throw new FlinkRuntimeException( String.format( ""Invalid stopping offset %d for partition %s"", stoppingOffset, tp)); }

});
}

4、If stopping offset is LATEST_OFFSET, COMMITTED_OFFSET, it will be set to actual offset by KafkaPartitionSplitReader#acquireAndSetStoppingOffsets.

5、In KafkaPartitionSplitReader#getStoppingOffset, the default stopping offset is set to Long.MAX_VALUE. This means if no stopping offset is set or parsed, the stream's stopping offset will be considered as  Long.MAX_VALUE, just like the stream is unbounded, and will be run until the offset is up to  Long.MAX_VALUE.

private long getStoppingOffset(TopicPartition tp)

{ return stoppingOffsets.getOrDefault(tp, Long.MAX_VALUE); }

I make a summary here:

1、Only when stoppingOffset >=0， or equal to LATEST_OFFSET ,COMMITTED_OFFSET , a stopping offset will be parse and set. In this case, it's bounded.If the starting offset is equal to or less than the stopping offset, an empty split is found, and this partition will be unassign.

2、When the stopping offset is not set, or the stopping offset is set to a wrong negative number, this stream is considered as unbounded, the stopping offset will be considered as Long.MAX_VALUE.In this case, no empty split will be found in unbounded mode.

Since a change split maybe found as a empty split, and relative topic partition will by unsigned, this situation should be considered in KafkaPartitionSplitReader#maybeLogSplitChangesHandlingResult. That how I solve this bug in the pull request.;;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade to chill java to 0.11.0 and kryo-shaded to 4.0.2,FLINK-29127,13478995,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Bo Cui,Bo Cui,29/Aug/22 01:48,29/Aug/22 13:56,04/Jun/24 20:41,29/Aug/22 13:56,,,,,,,,,,,,,,,,0,,,,,"upgrade to chill java to 0.11.0 and kryo-shaded to 4.0.2

My job depends on flink1.15 and hudi 0.11, but the job encountered a `NoSuchMethodError`  of Kryo#DefaultInstantiatorStrategy. 
Because the kryo version of flink is 2.24.0 and the kryo version of hudi is 4.0.2, they are incompatible.",,,,,,,,,,,,,,,,,,,,FLINK-3154,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 09:24:56 UTC 2022,,,,,,,,,,"0|z184nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 01:53;Bo Cui;I will create a pr later;;;","29/Aug/22 09:24;martijnvisser;[~Bo Cui] This isn't an easy problem to fix, because savepoints won't be compatible. It can only be resolved in a Flink 2.* release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix spliting file optimization doesn't work for orc format,FLINK-29126,13478992,13444738,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,29/Aug/22 01:29,17/Oct/22 06:50,04/Jun/24 20:41,17/Oct/22 06:50,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,FLINK-27338 try to improve file spliting for orc format. But it doesn't work for a making  mistake in judge whether the table is stored as orc format or not. We should fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 06:16:15 UTC 2022,,,,,,,,,,"0|z184mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 06:16;godfreyhe;Fixed in master: cf70844a56a0994dfcd7fb1859408683f2b621a3
Fixed in 1.16: cbc9d462b295243a61ebc544d9cf9ff6fa2a8aa6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Placeholder in Apache Flink Web Frontend to display some ""tags"" to distinguish between frontends of different clusters",FLINK-29125,13478970,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dkrovi,dkrovi,28/Aug/22 12:33,21/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,Runtime / Web Frontend,,,,0,auto-deprioritized-major,pull-request-available,,,"When there are several Apache Flink clusters running and the corresponding Web Frontend is opened in browser tabs, it would be great if these UIs can be distinguished in a visible way. Port number in the browser location bar might be useful.

In our use case, we switch among multiple clusters, connect to only one cluster at a time and use the same port for forwarding. In such a case, there is no visible cue to identify the cluster of the UI being accessed on the browser.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:26 UTC 2023,,,,,,,,,,"0|z184hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 07:32;xtsong;[~junhan], would you have time to take a look at this?;;;","23/Dec/22 04:30;junhan;[~dkrovi] You want to have a cluster identification tag in the web UI, but displaying port number in the browser location bar (I believe you are referring to the routers) is not a good way. Besides, I am not sure if the Front-end have any information about the current cluster through any REST apis.;;;","13/Jan/23 06:35;Wencong Liu;I think there are several options to represent the cluster identification tag:

*Option1: Use IP & Port of rest server to identify the cluster.*

We can show the tag like this:
{code:java}
Apache Flink Web Dashboard(47.100.234.179:8081){code}
*Option2: Use a ID to identify the cluster:*

Currently, the cluster can be identified by the DispatcherID owned by Dispatcher. But I think it's not suitable to use DispatcherID directly, there are two reasons:
 # The DispatcherID is generated by leaderSessionID, and leaderSessionID is randomly generated during leader selection. If the cluster is standalone, DispatcherID will use a default value and cause misunderstanding. In this case all clusters will have a same DispatcherID.
 # The length of DispatcherID.toString() is too long, like this ""00000000-0000-0000-0000-000000000000"". It's not pretty to show it on browser location bar.

Using a randomly generated fixed-length string to identify the cluster maybe a better choice. We can show the tag like this:
{code:java}
Apache Flink Web Dashboard(ID: yy787u){code}
*Option3: Enable the user to set a custom name to cluster in flink-conf.yaml*

Maybe user can add a configuration item like this:
{code:java}
flink.cluster-id: ""cluster 1""{code}
Then we can show the tag like this:
{code:java}
Apache Flink Web Dashboard(ID: cluster 1){code}
If the user doesn't specify this item,  the cluster id will not be shown, or we can fallback to either *Option1* and {*}Option2{*}.

WDYT？ [~dkrovi] [~junhan] [~xtsong] [~ziyang] ;;;","17/Jan/23 14:16;dkrovi;Option3 is ideal and agree that Option2 modified (to generate a fixed-length string) is also a good option.

 

Thank you.;;;","17/Jan/23 14:33;martijnvisser;As part of the overall project I think it would be better that Flink becomes cluster/rack aware, including displaying this in the UI, but this would require a proposal and a FLIP and would be part of a larger effort. ;;;","18/Jan/23 05:41;Wencong Liu;Thanks for the reply [~dkrovi] [~martijnvisser] . I also think it needs a discuss and I'll put these options on the proposal. WDYT? cc [~xtsong] [~junhan];;;","30/Jan/23 02:36;xtsong;Sorry for joining the discussion late.

I think option1 is probably the easiest. However, I'm not entirely sure it's always ok to expose the ip & port on UI. E.g., if the user access the web ui via a proxy, and the jobmanager ip & port are meant to be internal.

I'm overall -1 for option2. We should not expose the internal dispatcher id. And introducing another random id could be confusing because we can hardly explain to the users what is the random string being displayed.

For option3, please be aware we already have a `kubernetes.cluster-id` for native k8s deployments, and a `yarn.application.id` for yarn deployments. If we are introducing another `flink.cluster-id`, we would need to clarify the relationship between these concepts. My gut feeling this might be an overkill for the simple purpose of distinguishing browser tabs.

In addition to which identifier to choose, I think the way we display the identifier also matter. `Apache Flink Web Dashboard (<id>)` may not be a good choice, because when there are many tabs in the browser, most likely the user would only see `Apache Flink Web ...`.;;;","30/Jan/23 02:48;xtsong;Actually, I'm not entirely sure whether this feature is necessary.

I wonder in how many use cases do we need to access many flink clusters at the same time. And in such cases, would there be other easier ways to distinguish the tabs? E.g., if you google ""chrome rename tabs"", there're many chrome extensions that allow you to customize the name of tabs.;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redundant checkNotNull in cli Package,FLINK-29124,13478910,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,floudk,floudk,27/Aug/22 05:19,27/Oct/22 07:39,04/Jun/24 20:41,27/Oct/22 07:39,,,,,,,,,,,,Command Line Client,,,,0,,,,,"Redundant NotNull checks in function cli/CliFrontend.
{*}getEffectiveConfiguration{*}():
{code:java}
final ExecutionConfigAccessor executionParameters =                
ExecutionConfigAccessor.fromProgramOptions(
checkNotNull(programOptions), checkNotNull(jobJars));
 {code}
while *ExecutionConfigAccessor.fromProgramOptions* indeed does the notNull check
{code:java}
public static <T> ExecutionConfigAccessor fromProgramOptions(
            final ProgramOptions options, 
                  final List<T> jobJars) {        
checkNotNull(options);       
checkNotNull(jobJars);
...
} 


{code}
I have searched other *ExecutionConfigAccessor.fromProgramOptions()* usages, and all of them do not use checkNotNull in invokion",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:39:13 UTC 2022,,,,,,,,,,"0|z1844g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 13:08;zxcoccer;hi，I would like to deal with it. Can you assign this ticket to me ?
 ;;;","27/Oct/22 07:39;xtsong;I don't think this is a valid issue. Not relying on the callers correctness is a good coding style IMO.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic paramters are not pushed to working with kubernetes,FLINK-29123,13478856,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,26/Aug/22 16:58,30/Aug/22 12:15,04/Jun/24 20:41,30/Aug/22 12:15,1.15.2,,,,,,,1.16.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,,It is not possible to push dynamic parameters for the kubernetes deployments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 12:15:32 UTC 2022,,,,,,,,,,"0|z183sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Aug/22 09:38;gyfora;I agree this would be an important fix that would allow us to finally leverage this feature in the Flink Kubernetes Operator;;;","30/Aug/22 12:15;gyfora;merged to main c37643031dca2e6d4c299c0d704081a8bffece1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve robustness of FileUtils.expandDirectory() ,FLINK-29122,13478824,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,anupamaggarwal,rmetzger,rmetzger,26/Aug/22 13:14,13/Mar/24 15:02,04/Jun/24 20:41,13/Mar/24 15:02,1.16.0,1.17.0,,,,,,1.20.0,,,,API / Core,,,,0,pull-request-available,,,,`FileUtils.expandDirectory()` can potentially write to invalid locations if the zip file is invalid (contains entry names with ../).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 13 15:02:24 UTC 2024,,,,,,,,,,"0|z183lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 13:58;rmetzger;Draft: https://github.com/rmetzger/flink/pull/new/expand_dir
CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9224&view=results;;;","13/Mar/24 15:02;rmetzger;Merged to master in https://github.com/apache/flink/commit/0da60ca1a4754f858cf7c52dd4f0c97ae0e1b0cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability is failed,FLINK-29121,13478811,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,jark,jark,26/Aug/22 11:46,29/Aug/22 06:11,04/Jun/24 20:41,29/Aug/22 06:11,,,,,,,,1.16.0,,,,Table SQL / Gateway,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40416&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4


{code}
2022-08-26T11:03:07.6108823Z Aug 26 11:03:07 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.379 s <<< FAILURE! - in org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest
2022-08-26T11:03:07.6110033Z Aug 26 11:03:07 [ERROR] org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability(SqlGatewayRestAPIVersion)[1]  Time elapsed: 0.347 s  <<< FAILURE!
2022-08-26T11:03:07.6110730Z Aug 26 11:03:07 org.opentest4j.AssertionFailedError: 
2022-08-26T11:03:07.6112493Z Aug 26 11:03:07 No compatible call could be found for {""url"":""/sessions/:session_handle/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6115158Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6116664Z Aug 26 11:03:07 	Compatibility grade: 7/8
2022-08-26T11:03:07.6122943Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6123711Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6124489Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6125445Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/cancel""
2022-08-26T11:03:07.6128775Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/close"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6130953Z Aug 26 11:03:07 	Compatibility grade: 6/8
2022-08-26T11:03:07.6131525Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6132055Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6132735Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6133551Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/close""
2022-08-26T11:03:07.6134338Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6134777Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6135124Z Aug 26 11:03:07  but was: ""DELETE""
2022-08-26T11:03:07.6137281Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/status"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6138587Z Aug 26 11:03:07 	Compatibility grade: 6/8
2022-08-26T11:03:07.6138940Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6139281Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6139687Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6140200Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/status""
2022-08-26T11:03:07.6140643Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6141136Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6141622Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6144287Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:CloseSessionResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6146275Z Aug 26 11:03:07 	Compatibility grade: 5/8
2022-08-26T11:03:07.6147068Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6147651Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6148335Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6149084Z Aug 26 11:03:07  but was: ""/sessions/:session_handle""
2022-08-26T11:03:07.6149721Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6150272Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6150775Z Aug 26 11:03:07  but was: ""DELETE""
2022-08-26T11:03:07.6151913Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6154692Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/api_versions"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetApiVersionResponseBody"",""properties"":{""versions"":{""type"":""array"",""items"":{""type"":""string""}}}}}.
2022-08-26T11:03:07.6157059Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6157436Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6157777Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6158191Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6158618Z Aug 26 11:03:07  but was: ""/api_versions""
2022-08-26T11:03:07.6159120Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6159445Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6159759Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6160504Z Aug 26 11:03:07 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:07.6161015Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6161424Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6162919Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/info"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetInfoResponseBody"",""properties"":{""productName"":{""type"":""string""},""version"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6164087Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6164470Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6164791Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6165205Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6165633Z Aug 26 11:03:07  but was: ""/info""
2022-08-26T11:03:07.6165944Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6166278Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6166607Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6167384Z Aug 26 11:03:07 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:07.6167902Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6168321Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6170825Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionRequestBody"",""properties"":{""sessionName"":{""type"":""string""},""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionResponseBody"",""properties"":{""sessionHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6173021Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6173390Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6173735Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6174215Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6174652Z Aug 26 11:03:07  but was: ""/sessions""
2022-08-26T11:03:07.6174989Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6175300Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6175639Z Aug 26 11:03:07  but was: ""POST""
2022-08-26T11:03:07.6176375Z Aug 26 11:03:07 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:07.6177054Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6177499Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6179177Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:GetSessionConfigResponseBody"",""properties"":{""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}}}.
2022-08-26T11:03:07.6180529Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6180898Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6181219Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6181630Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6182075Z Aug 26 11:03:07  but was: ""/sessions/:session_handle""
2022-08-26T11:03:07.6182536Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6182860Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6183174Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6184091Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6184595Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6185022Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6186196Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/heartbeat"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""any""}}.
2022-08-26T11:03:07.6187190Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6187563Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6187891Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6188286Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6188773Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/heartbeat""
2022-08-26T11:03:07.6189152Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6189459Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6189791Z Aug 26 11:03:07  but was: ""POST""
2022-08-26T11:03:07.6190437Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6191111Z Aug 26 11:03:07 		response: [Type of field was changed from 'object' to 'any'.] 
2022-08-26T11:03:07.6191536Z Aug 26 11:03:07 expected: ""object""
2022-08-26T11:03:07.6191867Z Aug 26 11:03:07  but was: ""any""
2022-08-26T11:03:07.6193723Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/result/:token"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""},{""key"":""token""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:FetchResultsResponseBody"",""properties"":{""results"":{""type"":""any""},""resultType"":{""type"":""string""},""nextResultUri"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6195169Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6195538Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6195877Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6196268Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6196954Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/operations/:operation_handle/result/:token""
2022-08-26T11:03:07.6197421Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6197730Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6198114Z Aug 26 11:03:07  but was: ""GET""
2022-08-26T11:03:07.6198975Z Aug 26 11:03:07 		path-parameters: New path parameter token was added.
2022-08-26T11:03:07.6199588Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6200250Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6202961Z Aug 26 11:03:07 	Rejected by candidate: {""url"":""/sessions/:session_handle/statements"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementRequestBody"",""properties"":{""statement"":{""type"":""string""},""executionTimeout"":{""type"":""integer""},""executionConfig"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementResponseBody"",""properties"":{""operationHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:07.6205404Z Aug 26 11:03:07 	Compatibility grade: 4/8
2022-08-26T11:03:07.6206095Z Aug 26 11:03:07 	Incompatibilities: 
2022-08-26T11:03:07.6206523Z Aug 26 11:03:07 		url: 
2022-08-26T11:03:07.6207210Z Aug 26 11:03:07 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:07.6207873Z Aug 26 11:03:07  but was: ""/sessions/:session_handle/statements""
2022-08-26T11:03:07.6208387Z Aug 26 11:03:07 		method: 
2022-08-26T11:03:07.6208943Z Aug 26 11:03:07 expected: ""PUT""
2022-08-26T11:03:07.6209508Z Aug 26 11:03:07  but was: ""POST""
2022-08-26T11:03:07.6210608Z Aug 26 11:03:07 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:07.6211479Z Aug 26 11:03:07 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:07.6212064Z Aug 26 11:03:07 Expecting actual not to be null
2022-08-26T11:03:07.6212552Z Aug 26 11:03:07 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
2022-08-26T11:03:07.6213409Z Aug 26 11:03:07 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:134)
2022-08-26T11:03:07.6214712Z Aug 26 11:03:07 	at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.fail(RestAPIStabilityTestUtils.java:209)
2022-08-26T11:03:07.6216196Z Aug 26 11:03:07 	at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.assertCompatible(RestAPIStabilityTestUtils.java:138)
2022-08-26T11:03:07.6217787Z Aug 26 11:03:07 	at org.apache.flink.runtime.rest.compatibility.RestAPIStabilityTestUtils.testStability(RestAPIStabilityTestUtils.java:78)
2022-08-26T11:03:07.6218756Z Aug 26 11:03:07 	at org.apache.flink.table.gateway.rest.compatibility.SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability(SqlGatewayRestAPIStabilityTest.java:57)
2022-08-26T11:03:07.6219518Z Aug 26 11:03:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-26T11:03:07.6220092Z Aug 26 11:03:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-26T11:03:07.6220833Z Aug 26 11:03:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-26T11:03:07.6221472Z Aug 26 11:03:07 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-26T11:03:07.6222067Z Aug 26 11:03:07 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-26T11:03:07.6222767Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-26T11:03:07.6223567Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-26T11:03:07.6224456Z Aug 26 11:03:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-26T11:03:07.6225187Z Aug 26 11:03:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-26T11:03:07.6225958Z Aug 26 11:03:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-08-26T11:03:07.6226971Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-26T11:03:07.6227801Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-26T11:03:07.6228811Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-26T11:03:07.6229837Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-26T11:03:07.6230701Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-26T11:03:07.6231478Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-26T11:03:07.6232220Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-26T11:03:07.6233022Z Aug 26 11:03:07 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-26T11:03:07.6233781Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-26T11:03:07.6234656Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6235446Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-26T11:03:07.6236487Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-26T11:03:07.6237597Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-26T11:03:07.6238794Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-26T11:03:07.6239601Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6240373Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6241101Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6241881Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6242923Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6243996Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6244758Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6245627Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6246673Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-08-26T11:03:07.6248243Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-08-26T11:03:07.6249734Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-08-26T11:03:07.6251055Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-08-26T11:03:07.6251888Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-08-26T11:03:07.6252628Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-26T11:03:07.6253463Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6254491Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-26T11:03:07.6255342Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6256361Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-26T11:03:07.6257198Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6257831Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6258620Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6259230Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-26T11:03:07.6259857Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6260794Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-26T11:03:07.6261877Z Aug 26 11:03:07 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
2022-08-26T11:03:07.6262842Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-26T11:03:07.6264074Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-26T11:03:07.6265240Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-26T11:03:07.6266437Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-26T11:03:07.6267746Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-26T11:03:07.6268842Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-26T11:03:07.6270170Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-26T11:03:07.6271320Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6272436Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6273549Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-26T11:03:07.6274812Z Aug 26 11:03:07 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-26T11:03:07.6275964Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-26T11:03:07.6277234Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-26T11:03:07.6278413Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-26T11:03:07.6279630Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-26T11:03:07.6280773Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-26T11:03:07.6281903Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-26T11:03:07.6283011Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-26T11:03:07.6284251Z Aug 26 11:03:07 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-26T11:03:07.6285420Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-26T11:03:07.6286356Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-26T11:03:07.6287606Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-26T11:03:07.6288756Z Aug 26 11:03:07 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-26T11:03:07.6289721Z Aug 26 11:03:07 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-26T11:03:07.6290598Z Aug 26 11:03:07 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-26T11:03:07.6291318Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-08-26T11:03:07.6292107Z Aug 26 11:03:07 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-08-26T11:03:07.6293092Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-26T11:03:07.6293968Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6294757Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6295473Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6296174Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6297316Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6298067Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6298789Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6299667Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6300711Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-26T11:03:07.6301615Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-26T11:03:07.6302559Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6303356Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6304135Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6304850Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6305612Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6306371Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6307249Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6308365Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6309412Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-26T11:03:07.6310319Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-26T11:03:07.6311225Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6311985Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-26T11:03:07.6312884Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-26T11:03:07.6314150Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-26T11:03:07.6314950Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-26T11:03:07.6315874Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-26T11:03:07.6316609Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-26T11:03:07.6317690Z Aug 26 11:03:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-26T11:03:07.6318978Z Aug 26 11:03:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-26T11:03:07.6319845Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-26T11:03:07.6320479Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-26T11:03:07.6321103Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-26T11:03:07.6321734Z Aug 26 11:03:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-26T11:03:07.6322201Z Aug 26 11:03:07 
2022-08-26T11:03:07.6498722Z Aug 26 11:03:07 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.022 s - in org.apache.flink.table.gateway.rest.SessionCaseITTest
2022-08-26T11:03:08.4973396Z Aug 26 11:03:08 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.396 s - in org.apache.flink.table.gateway.service.session.SessionManagerTest
2022-08-26T11:03:08.8545807Z Aug 26 11:03:08 [INFO] 
2022-08-26T11:03:08.8546577Z Aug 26 11:03:08 [INFO] Results:
2022-08-26T11:03:08.8547189Z Aug 26 11:03:08 [INFO] 
2022-08-26T11:03:08.8547603Z Aug 26 11:03:08 [ERROR] Failures: 
2022-08-26T11:03:08.8550509Z Aug 26 11:03:08 [ERROR]   SqlGatewayRestAPIStabilityTest.testSqlGatewayRestAPIStability:57 No compatible call could be found for {""url"":""/sessions/:session_handle/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8554432Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/cancel"",""method"":""PUT"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8555944Z Aug 26 11:03:08 	Compatibility grade: 7/8
2022-08-26T11:03:08.8556404Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8556878Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8557341Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8558016Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/cancel""
2022-08-26T11:03:08.8560084Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/close"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8561585Z Aug 26 11:03:08 	Compatibility grade: 6/8
2022-08-26T11:03:08.8561952Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8562272Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8562682Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8563323Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/close""
2022-08-26T11:03:08.8563736Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8564157Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8564499Z Aug 26 11:03:08  but was: ""DELETE""
2022-08-26T11:03:08.8566446Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/status"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:operation:OperationStatusResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8569300Z Aug 26 11:03:08 	Compatibility grade: 6/8
2022-08-26T11:03:08.8569937Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8570496Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8571215Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8572246Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/status""
2022-08-26T11:03:08.8572825Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8573133Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8573460Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8575227Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""DELETE"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:CloseSessionResponseBody"",""properties"":{""status"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8576635Z Aug 26 11:03:08 	Compatibility grade: 5/8
2022-08-26T11:03:08.8577360Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8578025Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8578743Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8579537Z Aug 26 11:03:08  but was: ""/sessions/:session_handle""
2022-08-26T11:03:08.8579915Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8580234Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8580549Z Aug 26 11:03:08  but was: ""DELETE""
2022-08-26T11:03:08.8581230Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8582810Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/api_versions"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetApiVersionResponseBody"",""properties"":{""versions"":{""type"":""array"",""items"":{""type"":""string""}}}}}.
2022-08-26T11:03:08.8583981Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8584332Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8584663Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8585070Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8585736Z Aug 26 11:03:08  but was: ""/api_versions""
2022-08-26T11:03:08.8586081Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8586407Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8586722Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8587606Z Aug 26 11:03:08 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:08.8588091Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8588516Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8589990Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/info"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:util:GetInfoResponseBody"",""properties"":{""productName"":{""type"":""string""},""version"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8591280Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8591646Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8591959Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8592369Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8592795Z Aug 26 11:03:08  but was: ""/info""
2022-08-26T11:03:08.8593101Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8593427Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8593756Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8594450Z Aug 26 11:03:08 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:08.8594951Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8595371Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8619910Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionRequestBody"",""properties"":{""sessionName"":{""type"":""string""},""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:OpenSessionResponseBody"",""properties"":{""sessionHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8622586Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8623158Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8623683Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8624400Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8625066Z Aug 26 11:03:08  but was: ""/sessions""
2022-08-26T11:03:08.8625574Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8626077Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8626554Z Aug 26 11:03:08  but was: ""POST""
2022-08-26T11:03:08.8627935Z Aug 26 11:03:08 		path-parameters: Existing Path parameter session_handle was removed.
2022-08-26T11:03:08.8628783Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8629445Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8632160Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:session:GetSessionConfigResponseBody"",""properties"":{""properties"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}}}.
2022-08-26T11:03:08.8634154Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8634728Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8635242Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8635900Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8637125Z Aug 26 11:03:08  but was: ""/sessions/:session_handle""
2022-08-26T11:03:08.8637679Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8638215Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8638757Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8639897Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8640744Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8641438Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8643308Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/heartbeat"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""any""}}.
2022-08-26T11:03:08.8645016Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8645627Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8646163Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8647000Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8647784Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/heartbeat""
2022-08-26T11:03:08.8648331Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8648821Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8649321Z Aug 26 11:03:08  but was: ""POST""
2022-08-26T11:03:08.8650391Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8651470Z Aug 26 11:03:08 		response: [Type of field was changed from 'object' to 'any'.] 
2022-08-26T11:03:08.8652215Z Aug 26 11:03:08 expected: ""object""
2022-08-26T11:03:08.8652757Z Aug 26 11:03:08  but was: ""any""
2022-08-26T11:03:08.8655846Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/operations/:operation_handle/result/:token"",""method"":""GET"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""},{""key"":""operation_handle""},{""key"":""token""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""any""},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:FetchResultsResponseBody"",""properties"":{""results"":{""type"":""any""},""resultType"":{""type"":""string""},""nextResultUri"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8658253Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8658849Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8659376Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8660027Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8660918Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/operations/:operation_handle/result/:token""
2022-08-26T11:03:08.8661621Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8662145Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8662684Z Aug 26 11:03:08  but was: ""GET""
2022-08-26T11:03:08.8663723Z Aug 26 11:03:08 		path-parameters: New path parameter token was added.
2022-08-26T11:03:08.8664619Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8665305Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8669057Z Aug 26 11:03:08 	Rejected by candidate: {""url"":""/sessions/:session_handle/statements"",""method"":""POST"",""status-code"":""200 OK"",""file-upload"":false,""path-parameters"":{""pathParameters"":[{""key"":""session_handle""}]},""query-parameters"":{""queryParameters"":[]},""request"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementRequestBody"",""properties"":{""statement"":{""type"":""string""},""executionTimeout"":{""type"":""integer""},""executionConfig"":{""type"":""object"",""additionalProperties"":{""type"":""string""}}}},""response"":{""type"":""object"",""id"":""urn:jsonschema:org:apache:flink:table:gateway:rest:message:statement:ExecuteStatementResponseBody"",""properties"":{""operationHandle"":{""type"":""string""}}}}.
2022-08-26T11:03:08.8671949Z Aug 26 11:03:08 	Compatibility grade: 4/8
2022-08-26T11:03:08.8672450Z Aug 26 11:03:08 	Incompatibilities: 
2022-08-26T11:03:08.8672936Z Aug 26 11:03:08 		url: 
2022-08-26T11:03:08.8673517Z Aug 26 11:03:08 expected: ""/sessions/:session_handle/:operation_handle/cancel""
2022-08-26T11:03:08.8674326Z Aug 26 11:03:08  but was: ""/sessions/:session_handle/statements""
2022-08-26T11:03:08.8674933Z Aug 26 11:03:08 		method: 
2022-08-26T11:03:08.8675409Z Aug 26 11:03:08 expected: ""PUT""
2022-08-26T11:03:08.8675870Z Aug 26 11:03:08  but was: ""POST""
2022-08-26T11:03:08.8676947Z Aug 26 11:03:08 		path-parameters: Existing Path parameter operation_handle was removed.
2022-08-26T11:03:08.8677838Z Aug 26 11:03:08 		response: [Field {""type"":""string""} was removed.] 
2022-08-26T11:03:08.8678425Z Aug 26 11:03:08 Expecting actual not to be null
2022-08-26T11:03:08.8678939Z Aug 26 11:03:08 [INFO] 
2022-08-26T11:03:08.8679504Z Aug 26 11:03:08 [ERROR] Tests run: 32, Failures: 1, Errors: 0, Skipped: 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 06:10:48 UTC 2022,,,,,,,,,,"0|z183ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 11:46;jark;cc [~fsk119];;;","27/Aug/22 08:30;wangyang0918;Another instance, https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40413&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","28/Aug/22 00:54;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40428&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Aug/22 02:31;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40398&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Aug/22 02:37;fsk119;Sorry for everyone. It seems I merge a commit with probelms. The root cause is we updte the rest api but not updating the api document. The test here compares the difference between the actual api and the api doc and find they are different. So I update the api document in the PR.;;;","29/Aug/22 06:10;fsk119;Merged into master: e246a05cd898e16758a52335250571e165a5148a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected join hint propagation into view,FLINK-29120,13478786,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,26/Aug/22 09:48,10/Sep/22 02:47,04/Jun/24 20:41,10/Sep/22 02:47,1.16.0,,,,,,,1.16.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"As expected, Join Hint should only affects the current query block, and does not affect the Join strategy in subquery and view.

But current implementation behaviors inconsistently:

use source tables of flink-tpch-test, the following join hint takes effect unexpectedly
{code:java}
Flink SQL> create temporary view v1 as SELECT
>    p_name,
>    p_mfgr,
>    p_brand,
>    p_type,
>    s_name,
>    s_address
>  FROM
>    part,
>    supplier
>  WHERE p_partkey = s_suppkey;
[INFO] Execute statement succeed.

 


Flink SQL> explain SELECT /*+ SHUFFLE_MERGE(part)  */ * from v1;
== Abstract Syntax Tree ==
LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])
+- LogicalFilter(condition=[=($0, $9)])
   +- LogicalJoin(condition=[true], joinType=[inner], joinHints=[[[SHUFFLE_MERGE inheritPath:[0, 0] options:[part]]]])
      :- LogicalTableScan(table=[[default_catalog, default_database, part]])
      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]])

== Optimized Physical Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- SortMergeJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])
   :- Exchange(distribution=[hash[p_partkey]])
   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[hash[s_suppkey]])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

== Optimized Execution Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- SortMergeJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])
   :- Exchange(distribution=[hash[p_partkey]])
   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[hash[s_suppkey]])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

{code}
 

without hint

{code}

Flink SQL> explain SELECT * from v1;
== Abstract Syntax Tree ==
LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])
+- LogicalFilter(condition=[=($0, $9)])
   +- LogicalJoin(condition=[true], joinType=[inner])
      :- LogicalTableScan(table=[[default_catalog, default_database, part]])
      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]])

== Optimized Physical Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- HashJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address], isBroadcast=[true], build=[right])
   :- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[broadcast])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

== Optimized Execution Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- MultipleInput(readOrder=[1,0], members=[\nHashJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address], isBroadcast=[true], build=[right])\n:- [#1] TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])\n+- [#2] Exchange(distribution=[broadcast])\n])
   :- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[broadcast])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

{code}

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28969,FLINK-29221,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 10 02:47:42 UTC 2022,,,,,,,,,,"0|z183cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Sep/22 02:47;godfreyhe;Fixed in master: 6722c89d0df35643dde38c1b8f096aa785579884
in 1.16: 357221b9b3543447b2b439a413639c0ed201ab35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should clarify how join hints work with CTE,FLINK-29119,13478784,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,lincoln.86xy,lincoln.86xy,26/Aug/22 09:39,26/Aug/22 09:39,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,,,,,"use source tables of flink-tpch-test

join hint on a single expression name of CTE works fine:

{code}

Flink SQL> explain with q1 as (SELECT
>   p_name,
>   p_mfgr,
>   p_brand,
>   p_type,
>   s_name,
>   s_address
> FROM
>   part,
>   supplier
> WHERE p_partkey = s_suppkey)
>
> SELECT /*+ SHUFFLE_MERGE(part,supplier)  */ * from q1;
== Abstract Syntax Tree ==
LogicalProject(p_name=[$1], p_mfgr=[$2], p_brand=[$3], p_type=[$4], s_name=[$10], s_address=[$11])
+- LogicalFilter(condition=[=($0, $9)])
   +- LogicalJoin(condition=[true], joinType=[inner], joinHints=[[[SHUFFLE_MERGE inheritPath:[0, 0] options:[part, supplier]]]])
      :- LogicalTableScan(table=[[default_catalog, default_database, part]], hints=[[[ALIAS inheritPath:[] options:[part]]]])
      +- LogicalTableScan(table=[[default_catalog, default_database, supplier]], hints=[[[ALIAS inheritPath:[] options:[supplier]]]])

== Optimized Physical Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- SortMergeJoin(joinType=[InnerJoin], where=[=(p_partkey, s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])
   :- Exchange(distribution=[hash[p_partkey]])
   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[hash[s_suppkey]])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

== Optimized Execution Plan ==
Calc(select=[p_name, p_mfgr, p_brand, p_type, s_name, s_address])
+- SortMergeJoin(joinType=[InnerJoin], where=[(p_partkey = s_suppkey)], select=[p_partkey, p_name, p_mfgr, p_brand, p_type, s_suppkey, s_name, s_address])
   :- Exchange(distribution=[hash[p_partkey]])
   :  +- TableSourceScan(table=[[default_catalog, default_database, part, project=[p_partkey, p_name, p_mfgr, p_brand, p_type], metadata=[]]], fields=[p_partkey, p_name, p_mfgr, p_brand, p_type])
   +- Exchange(distribution=[hash[s_suppkey]])
      +- TableSourceScan(table=[[default_catalog, default_database, supplier, project=[s_suppkey, s_name, s_address], metadata=[]]], fields=[s_suppkey, s_name, s_address])

{code}

but raise an error when there co-exists an alias of the expression name

{code}

Flink SQL> explain with q1 as (SELECT
>   p_name,
>   p_mfgr,
>   p_brand,
>   p_type,
>   s_name,
>   s_address
> FROM
>   part,
>   supplier
> WHERE p_partkey = s_suppkey)
>
> SELECT /*+ SHUFFLE_MERGE(part,supplier)  */ * from q1, q1 q2 where q1.p_name = q2.p_name;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: The options of following hints cannot match the name of input tables or views:
`SHUFFLE_MERGE(part, supplier)`

{code}

The expected behavior with CTE should be clarified in the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28969,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-26 09:39:10.0,,,,,,,,,,"0|z183cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove default_catalog in the HiveServer2 Endpoint,FLINK-29118,13478783,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,fsk119,fsk119,26/Aug/22 09:38,13/Sep/22 03:07,04/Jun/24 20:41,08/Sep/22 06:29,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,0,pull-request-available,,,,"Hive only has one Catalog. We don't require the default_catalog. Hive JDBC Driver also doesn't support multiple catalogs.

 

 

!image-2022-08-26-17-40-49-989.png!",,,,,,,,,,,,,FLINK-28954,,,,,,,,,,,,,,,,,,,,FLINK-29274,,,,,,,"26/Aug/22 09:40;fsk119;image-2022-08-26-17-40-49-989.png;https://issues.apache.org/jira/secure/attachment/13048632/image-2022-08-26-17-40-49-989.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 06:29:56 UTC 2022,,,,,,,,,,"0|z183c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 06:29;fsk119;Merged into master: 833e7ffbb5f075d2014dfaec547b6987d59bc89f

Merged into release-1.16: c8e75d201df6225e21facb7433667ac186e07f00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tried to associate with unreachable remote resourcemanager address,FLINK-29117,13478776,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,geonyeong,geonyeong,26/Aug/22 08:41,24/Sep/22 09:13,04/Jun/24 20:41,24/Sep/22 09:13,kubernetes-operator-1.1.0,,,,,,,,,,,Deployment / Kubernetes,Kubernetes Operator,,,0,,,,,"Hello.

I am planning to distribute and use FlinkDeployment through the flink kubernetes operator.

CRD, operator, webbook, etc. are all set up, and we actually distributed FlinkDeployment to confirm normal operation.

*However, strangely, connecting to resource manager fails if you make more than one task manager pod replica.*

I thought it might be a problem with akka, timeout, etc. so I increased the values as below
The connection continues to fail.
 - akka.retry-gate-closed-for: 10000
 - akka.server-socket-worker-pool.pool-size-min: 6
 - akka.server-socket-worker-pool.pool-size-max: 10
 - akka.client-socket-worker-pool.pool-size-max: 10
 - akka.client-socket-worker-pool.pool-size-min: 6
 - blob.client.connect.timeout: 30000

The log of the taskmanager is as follows.

 
{code:java}
Association with remote system [akka.tcp://flink@10.238.80.92:6123] has failed, address is now gated for [10000] ms. Reason: [Disassociated] 
Could not resolve ResourceManager address akka.tcp://flink@10.238.80.92:6123/user/rpc/resourcemanager_1, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@10.238.80.92:6123/user/rpc/resourcemanager_1. 
Tried to associate with unreachable remote address [akka.tcp://flink@10.238.80.92:6123]. Address is now gated for 10000 ms, all messages to this address will be delivered to dead letters. Reason: [The remote system has quarantined this system. No further associations to the remote system are possible until this system is restarted.]  {code}
*If you go into the task manager pod and tcp check, the connection is open.*

*Below are the flink versions I used.*
 * flink image: 1.15.1

 - flink kubernetes operator: 1.1.0

 

*I would appreciate it if you could check the problem quickly.*
*If it's a bug, please tell me how to detour in the current situation.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/22 08:44;geonyeong;taskmanager_log.png;https://issues.apache.org/jira/secure/attachment/13048628/taskmanager_log.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Sat Sep 24 09:13:36 UTC 2022,,,,,,,,,,"0|z183ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 12:40;gyfora;I have never hit or seen this issue with the operator, are you still experiencing it?;;;","24/Sep/22 09:13;gyfora;I am closing this for now, if you feel that this is still happening for you and can share more details we can revisit this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tried to associate with unreachable remote address,FLINK-29116,13478775,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,geonyeong,geonyeong,26/Aug/22 08:38,26/Aug/22 08:46,04/Jun/24 20:41,26/Aug/22 08:46,1.15.1,kubernetes-operator-1.1.0,,,,,,,,,,,,,,0,bug,features,,,"Hello.

I am planning to distribute and use FlinkDeployment through the flink kubernetes operator.

CRD, operator, webbook, etc. are all set up, and we actually distributed FlinkDeployment to confirm normal operation.

*However, strangely, connecting to resource manager fails if you make more than one task manager pod replica.*

I thought it might be a problem with akka, timeout, etc. so I increased the values as below
The connection continues to fail.

- akka.retry-gate-closed-for: 10000
- akka.server-socket-worker-pool.pool-size-min: 6
- akka.server-socket-worker-pool.pool-size-max: 10
- akka.client-socket-worker-pool.pool-size-max: 10
- akka.client-socket-worker-pool.pool-size-min: 6
- blob.client.connect.

 

The log of the taskmanager is as follows.


{code:java}
Association with remote system [akka.tcp://flink@10.238.80.92:6123] has failed, address is now gated for [10000] ms. Reason: [Disassociated]
Could not resolve ResourceManager address akka.tcp://flink@10.238.80.92:6123/user/rpc/resourcemanager_1, retrying in 10000 ms: Could not connect to rpc endpoint under address akka.tcp://flink@10.238.80.92:6123/user/rpc/resourcemanager_1.
Tried to associate with unreachable remote address [akka.tcp://flink@10.238.80.92:6123]. Address is now gated for 10000 ms, all messages to this address will be delivered to dead letters. Reason: [The remote system has quarantined this system. No further associations to the remote system are possible until this system is restarted.]
{code}
 

*If you go into the task manager pod and tcp check, the connection is open.*

*Below are the flink versions I used.*

*- flink image: 1.15.1*
*- flink kubernetes operator: 1.1.0*

*I would appreciate it if you could check the problem quickly.*
*If it's a bug, please tell me how to detour in the current situation.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/22 08:30;geonyeong;Screen Shot 2022-08-26 at 5.04.37 PM.png;https://issues.apache.org/jira/secure/attachment/13048626/Screen+Shot+2022-08-26+at+5.04.37+PM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-26 08:38:28.0,,,,,,,,,,"0|z183ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the quickstart of Flink ML python API,FLINK-29115,13478774,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yunfengzhou,yunfengzhou,26/Aug/22 08:34,19/Apr/23 01:49,04/Jun/24 20:41,19/Apr/23 01:49,ml-2.1.0,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Currently, in Flink ML's document, python users are required to build Flink ML Java project before they can build and use Flink ML's python API. Thus an improvement should be made to the setup process and quick start so as to simplify the usage of Flink ML.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:49:36 UTC 2023,,,,,,,,,,"0|z183a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:49;lindong;Merged to apache/flink-ml master branch:
- 0c93277a31a95b806c72b898416a5bc5b12ab357
- c8177c73c1b294baefbf63cdd9a247c4f1659d7b
- 0166a4c28579fafb1e33e622758c74fcd90943a1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableSourceITCase#testTableHintWithLogicalTableScanReuse sometimes fails with result mismatch ,FLINK-29114,13478768,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,Sergey Nuyanzin,Sergey Nuyanzin,26/Aug/22 08:07,19/Mar/24 07:54,04/Jun/24 20:41,19/Mar/24 07:53,1.15.0,1.17.0,1.18.0,1.19.0,1.20.0,,,1.17.3,1.18.2,1.19.1,1.20.0,Table SQL / Planner,Tests,,,0,auto-deprioritized-major,pull-request-available,test-stability,,"It could be reproduced locally by repeating tests. Usually about 100 iterations are enough to have several failed tests
{noformat}
[ERROR] Tests run: 13, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.664 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase
[ERROR] org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.testTableHintWithLogicalTableScanReuse  Time elapsed: 0.108 s  <<< FAILURE!
java.lang.AssertionError: expected:<List(1,1,Hi, 2,2,Hello, 2,2,Hello, 3,2,Hello world, 3,2,Hello world, 3,2,Hello world)> but was:<List(1,1,Hi, 2,2,Hello, 2,2,Hello, 3,2,Hello world, 3,2,Hello world)>
    at org.junit.Assert.fail(Assert.java:89)
    at org.junit.Assert.failNotEquals(Assert.java:835)
    at org.junit.Assert.assertEquals(Assert.java:120)
    at org.junit.Assert.assertEquals(Assert.java:146)
    at org.apache.flink.table.planner.runtime.batch.sql.TableSourceITCase.testTableHintWithLogicalTableScanReuse(TableSourceITCase.scala:428)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
    at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
    at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
    at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
    at java.util.Iterator.forEachRemaining(Iterator.java:116)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
    at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{noformat}",,,,,,,,,,,,,,,,,,,,FLINK-34564,,,,,,,,,FLINK-34432,,,,,,,,,,,"16/Feb/24 09:06;mapohl;FLINK-29114.log;https://issues.apache.org/jira/secure/attachment/13066762/FLINK-29114.log","27/Feb/24 07:23;qingyue;image-2024-02-27-15-23-49-494.png;https://issues.apache.org/jira/secure/attachment/13067044/image-2024-02-27-15-23-49-494.png","27/Feb/24 07:26;qingyue;image-2024-02-27-15-26-07-657.png;https://issues.apache.org/jira/secure/attachment/13067045/image-2024-02-27-15-26-07-657.png","27/Feb/24 07:32;qingyue;image-2024-02-27-15-32-48-317.png;https://issues.apache.org/jira/secure/attachment/13067046/image-2024-02-27-15-32-48-317.png",,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 07:53:22 UTC 2024,,,,,,,,,,"0|z1838w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","20/Dec/23 11:26;mapohl;https://github.com/XComp/flink/actions/runs/7253843919/job/19761640929#step:12:11644;;;","15/Feb/24 07:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57533&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11541;;;","16/Feb/24 07:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57550&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11539;;;","16/Feb/24 09:07;mapohl;I bumped the priority to Major for now. We might want to increase it to critical if that starts to appear regularly now. That might be an indication that something else changed.;;;","16/Feb/24 09:43;Sergey Nuyanzin;That might be the reason of more often occurrences FLINK-18356, especially
https://issues.apache.org/jira/browse/FLINK-18356?focusedCommentId=17816293&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17816293;;;","16/Feb/24 10:30;mapohl;Good point, you mean switching back to forked integration test execution for the table-planner module (FLINK-34432).;;;","16/Feb/24 10:31;mapohl;It failed for the GHA nightly quite regularely now:
* https://github.com/apache/flink/actions/runs/7925038538/job/21637812208#step:10:11690
* https://github.com/apache/flink/actions/runs/7925038538/job/21637793255#step:10:11707
* https://github.com/apache/flink/actions/runs/7925038538/job/21637804837#step:10:11677
* https://github.com/apache/flink/actions/runs/7925038538/job/21637791544#step:10:11707;;;","16/Feb/24 10:36;Sergey Nuyanzin;{quote}
you mean switching back to forked integration test execution for the table-planner module (FLINK-34432).
{quote}
yep;;;","19/Feb/24 08:01;lincoln.86xy;1.20(master): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57573&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","19/Feb/24 12:16;mapohl;[~qingyue] Do you have capacity to look into this topic? To be fair, it's most likely not a blocker for 1.19. It would be just good to resolve it for stabilziing master.;;;","20/Feb/24 01:43;qingyue;Hi [~mapohl], sorry for the late reply, I just noticed your message. I'll take a look now.;;;","20/Feb/24 07:55;mapohl;Much appreciated!
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57642&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11539]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57647&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11599]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57647&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11508];;;","21/Feb/24 08:19;mapohl;https://github.com/apache/flink/actions/runs/7983071058/job/21797854525#step:10:11592;;;","21/Feb/24 09:34;mapohl;https://github.com/apache/flink/actions/runs/7985075288/job/21803136609#step:10:11560;;;","21/Feb/24 16:17;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57722&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11860;;;","22/Feb/24 07:41;mapohl;* https://github.com/apache/flink/actions/runs/7998412179/job/21844789710#step:10:11564
* https://github.com/apache/flink/actions/runs/7998412179/job/21844781436#step:10:11488
* https://github.com/apache/flink/actions/runs/7998412179/job/21844769564#step:10:11615
* https://github.com/apache/flink/actions/runs/7998749202/job/21845587274#step:10:11630;;;","23/Feb/24 07:14;mapohl;https://github.com/apache/flink/actions/runs/8013740455/job/21891468505#step:10:11620;;;","23/Feb/24 07:16;mapohl;* https://github.com/apache/flink/actions/runs/8002589654/job/21856592263#step:10:11502
* https://github.com/apache/flink/actions/runs/8002447771/job/21856070038#step:10:11600
* https://github.com/apache/flink/actions/runs/8000170732/job/21849420123#step:10:11460;;;","26/Feb/24 07:39;mapohl;* https://github.com/apache/flink/actions/runs/8019572934/job/21908055413#step:10:11434
* https://github.com/apache/flink/actions/runs/8027473891/job/21931661418#step:10:11563
* https://github.com/apache/flink/actions/runs/8027473891/job/21931649532#step:10:11322
* https://github.com/apache/flink/actions/runs/8034840540/job/21946978450#step:10:11410
* https://github.com/apache/flink/actions/runs/8034840540/job/21946986383#step:10:11564
* https://github.com/apache/flink/actions/runs/8034840540/job/21946982284#step:10:11452
* https://github.com/apache/flink/actions/runs/8034840540/job/21947010268#step:10:11541
* https://github.com/apache/flink/actions/runs/8034840540/job/21946977316#step:10:11538
* https://github.com/apache/flink/actions/runs/8042676359/job/21964270013#step:10:11589
* https://github.com/apache/flink/actions/runs/8042676359/job/21964318809#step:10:11447
* https://github.com/apache/flink/actions/runs/8044095032/job/21967373619#step:10:11582;;;","26/Feb/24 07:42;mapohl;Hi [~qingyue], any updates? Or are you busy and we should look for someone else to check the instability?;;;","27/Feb/24 06:30;lincoln.86xy;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57863&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11999;;;","27/Feb/24 07:27;qingyue;Hi [~mapohl], sorry for the late reply due to a tight time budget. I've added some debug code and found some clues.

I think the root cause lies in the way to generate the staging directory for the filesystem sink. Please see [FileSystemTableSink.java#L377|https://github.com/apache/flink/blob/1070c6e9e0f9f00991bdeb34f0757e4f0597931e/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/FileSystemTableSink.java#L377]
System.currentTimeMillis() might lead to the same value in some rare conditions, which leads to staging dir conflicts.
I added some debug logs, and here are the details.

First, I changed @TempDir to CleanupMode.NEVER, and then noticed that for a failed case, only one file generated.

!image-2024-02-27-15-23-49-494.png|width=799,height=414!

 

Then, I added the log and found the staging dir conflicts.

!image-2024-02-27-15-26-07-657.png|width=793,height=462!

 

I think we can improve the staging dir generation method to fix this problem.;;;","27/Feb/24 07:33;qingyue;Update: I added a random UUID to the staging dir, and now the tests run well within 500 repeats.

!image-2024-02-27-15-32-48-317.png|width=1036,height=307!;;;","27/Feb/24 07:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57876&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=11537;;;","27/Feb/24 07:35;mapohl;Thanks for your efforts. That will reduce the amount of maintenance on the CI side quite a bit. :);;;","27/Feb/24 07:45;qingyue;Only this particular case encounters this issue because it writes to the same sink table using a statement set, and the table is non-partitioned. Normally, people wouldn't do that.;;;","27/Feb/24 09:27;qingyue;h3. A short summary

Prior to the commit, files written to the sink are temporarily stored under the staging directory, with the following file structure.
{code:java}
target_dir/
├─ .staging_timestamp/
│  ├─ task-${subtaskId}-attempt-${attemptNumber}/{code}

When employing statement set syntax or alternative methods to write multiple sink outputs, specifying a singular target path can inadvertently result in various sink streams sharing the same staging directory. This scenario arises in rare cases where System.currentTimeMillis() returns identical values across different sinks.

In the commit phase, the staging directory is deleted once the commit is finished. Consequently, this may lead to a situation where another sink task, attempting to commit concurrently, fails to locate its corresponding staging directory. The absence of the staging directory impedes the sink task's ability to commit correctly, potentially leading to erroneous computation outcomes.;;;","28/Feb/24 07:20;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57890&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11537
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57897&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11858
* https://github.com/apache/flink/actions/runs/8074215483/job/22059412218#step:10:11572
* https://github.com/apache/flink/actions/runs/8074215483/job/22059400878#step:10:11622;;;","29/Feb/24 01:55;hackergin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57954&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Feb/24 02:35;hackergin;[~qingyue] Can we fix this issue by creating different Sink Tables? If so, I am willing to help fix it.;;;","29/Feb/24 06:19;qingyue;[~hackergin] Different sink paths could avoid the unstable case. However, the problem lies in the way of generating the staging dir path. It's unreliable to rely solely on the timestamp as a path postfix.;;;","29/Feb/24 06:32;hackergin;[~qingyue] Thank you for your reply. Indeed, as you said, this is not an issue with unit testing but a problem with the generation of the staging directory.;;;","29/Feb/24 07:17;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57940&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11858
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57956&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11598
* https://github.com/apache/flink/actions/runs/8079627963/job/22074788185#step:10:11525
* https://github.com/apache/flink/actions/runs/8081916042/job/22082067075#step:10:11603
* https://github.com/apache/flink/actions/runs/8089966279/job/22107054311#step:10:11704;;;","29/Feb/24 09:14;mapohl;This time, there's not only {{testTableHintWithLogicalTableScanReuse}} that's failing but also {{testTableHint}}:
https://github.com/apache/flink/actions/runs/8092184514/job/22112722761#step:10:11663;;;","01/Mar/24 10:25;mapohl;* https://github.com/apache/flink/actions/runs/8095915970/job/22124076903#step:10:11609
* https://github.com/apache/flink/actions/runs/8105496391/job/22154178984#step:10:11477
* https://github.com/apache/flink/actions/runs/8105496391/job/22154177115#step:10:11631
* https://github.com/apache/flink/actions/runs/8105496391/job/22154146130#step:10:11459
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58009&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11537;;;","04/Mar/24 13:55;mapohl;* https://github.com/apache/flink/actions/runs/8119607813/job/22195868118#step:10:11553
* https://github.com/apache/flink/actions/runs/8119607813/job/22195889799#step:10:11535
* https://github.com/apache/flink/actions/runs/8127069864/job/22212009995#step:10:11528
* https://github.com/apache/flink/actions/runs/8134965216/job/22228831511#step:10:11371
* https://github.com/apache/flink/actions/runs/8134965216/job/22228875875#step:10:11686;;;","05/Mar/24 07:44;mapohl;* https://github.com/apache/flink/actions/runs/8149964578/job/22275704169#step:10:11371
* https://github.com/apache/flink/actions/runs/8149964578/job/22275706440#step:10:11535;;;","06/Mar/24 10:22;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58085&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11818
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58090&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11858
* https://github.com/apache/flink/actions/runs/8165693225/job/22323490948#step:10:11445
* https://github.com/apache/flink/actions/runs/8165883809/job/22323975553#step:10:11677
* https://github.com/apache/flink/actions/runs/8165883809/job/22323984154#step:10:11381
* https://github.com/apache/flink/actions/runs/8165883809/job/22324086188#step:10:11668
* https://github.com/apache/flink/actions/runs/8165883809/job/22323990828#step:10:11411
* https://github.com/apache/flink/actions/runs/8166578931/job/22325716285#step:10:11521;;;","08/Mar/24 10:45;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58161&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11999]
 * [https://github.com/apache/flink/actions/runs/8170940974/job/22338553670#step:10:11708]
 * [https://github.com/apache/flink/actions/runs/8181761715/job/22372293331#step:10:11678]
 * [https://github.com/apache/flink/actions/runs/8185748493/job/22384090359#step:10:11706]
 * [https://github.com/apache/flink/actions/runs/8187405144/job/22388171974#step:10:11706]
 * [https://github.com/apache/flink/actions/runs/8197679675/job/22420266062#step:10:11680]
 * https://github.com/apache/flink/actions/runs/8197679675/job/22420250277#step:10:11708;;;","11/Mar/24 01:58;masteryhx;master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58170&view=ms.vss-test-web.build-test-results-tab&runId=4035576&resultId=115533&paneView=debug;;;","11/Mar/24 14:19;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58178&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11858
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58201&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11509
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58201&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=11514
* {{testTableHint}} & {{testTableHintWithLogicalTableScanReuse}}: https://github.com/apache/flink/actions/runs/8201145220/job/22429638792#step:10:11387
* https://github.com/apache/flink/actions/runs/8204750658/job/22440490061#step:10:11553
* https://github.com/apache/flink/actions/runs/8201338090/job/22430169517#step:10:11597
* https://github.com/apache/flink/actions/runs/8211401122/job/22460446876#step:10:11549
* https://github.com/apache/flink/actions/runs/8218856722/job/22476301876#step:10:11357
* https://github.com/apache/flink/actions/runs/8218856722/job/22476316255#step:10:11572
* https://github.com/apache/flink/actions/runs/8218856722/job/22476344555#step:10:11680
* https://github.com/apache/flink/actions/runs/8226669379/job/22494228516#step:10:11553
* https://github.com/apache/flink/actions/runs/8226669379/job/22493766797#step:10:11610
* https://github.com/apache/flink/actions/runs/8228497559/job/22498277501#step:10:11643
* https://github.com/apache/flink/actions/runs/8230355449/job/22504508857#step:10:11393;;;","12/Mar/24 07:40;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58229&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11858
* https://github.com/apache/flink/actions/runs/8242516657/job/22541862142#step:10:11400
* https://github.com/apache/flink/actions/runs/8242516657/job/22541858062#step:10:11543;;;","13/Mar/24 09:52;qingyue;Fixed in master 7d0111dfab640f2f590dd710d76de927c86cf83e;;;","13/Mar/24 10:05;qingyue;Not sure whether it's a good timing to pick into release-1.19 considering that it's already being prepared for release.;;;","13/Mar/24 15:05;mapohl;It shouldn't be a problem as far as I see: if the RC is accepted, it will end up in 1.19.1. If it's not accepted it would even make it into 1.19.0 (you could mention that in the vote thread, if a new RC would be created). It's up to the release managers to accept it in or not. It's still a valid change because it's a bugfix (which is allowed to be merged during the feature freeze).;;;","14/Mar/24 02:59;qingyue;Thanks [~mapohl], I've opened a cherry-pick [PR|https://github.com/apache/flink/pull/24492], and it would be great if you could help review it.;;;","14/Mar/24 11:48;qingyue;Fixed in release-1.19 4d5327d40a3ca3e6845f779ca6508733fd630bae;;;","14/Mar/24 14:21;rskraba;Just for completeness, these two CI errors occurred _*before*_ the fix was cherry-picked to master and can be disregarded.  Thanks for the flx!
 * [https://github.com/apache/flink/actions/runs/8258416220/job/22590904525#step:10:11611]
 * [https://github.com/apache/flink/actions/runs/8258416220/job/22590878792#step:10:11677];;;","14/Mar/24 14:23;leonard;Hey, [~qingyue] [~mapohl]
I'm verifying 1.19 rc2, should we change this issue priority from blocker to critical or change the fixed version? otherwise a blocker issue will cancel the ongoing rc2 from my understanding.;;;","14/Mar/24 14:25;mapohl;[~pnowojski] was your intention to have this issue being included in 1.19.0? Or did you mark this one as a blocker because of the constant test failures happening in CI?;;;","14/Mar/24 14:28;mapohl;[~qingyue] For the current state of the release (RC2 being voted on and most likely being accepted) it would mean that this Jira issue will end up in 1.19.1 instead of 1.19.0. I'm gonna update the fixVersion accordingly.;;;","14/Mar/24 14:34;qingyue;Sorry for the inconvenience, and thanks for correcting the version. cc [~leonard], [~pnowojski] and [~mapohl];;;","19/Mar/24 07:53;qingyue;Fixed in release-1.17 0b430c2e614a7a9936e5eedf49e87393d8bc7a77

Fixed in release-1.18 c2a85ac15003f03682979c617424c74875e19137;;;",,,,,,,,,,,,,,,,,,,,,
Join hint with invalid table name mixed with valid names will not raise an error,FLINK-29113,13478757,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,26/Aug/22 07:27,14/Sep/22 02:20,04/Jun/24 20:41,14/Sep/22 02:20,1.16.0,,,,,,,1.16.0,1.17.0,,,Table SQL / Planner,,,,0,pull-request-available,,,," 

add a  BROADCAST hint to tpch q2 with a non exists table 'supp' works fine, while a single invalid table name 'supp' will raise an error

 

```sql

explain  SELECT /*+ BROADCAST(partsupp,supp) */ 

s_acctbal,
  s_name,
  n_name,
  p_partkey,
  p_mfgr,
  s_address,
  s_phone,
  s_comment
FROM
  part,
  supplier,
  partsupp,
  nation,
  region
WHERE
  p_partkey = ps_partkey
  AND s_suppkey = ps_suppkey
  AND p_size = 15
  AND p_type LIKE '%BRASS'
  AND s_nationkey = n_nationkey
  AND n_regionkey = r_regionkey
  AND r_name = 'EUROPE'
  AND ps_supplycost = (
    SELECT min(ps_supplycost)
    FROM
      partsupp, supplier,
      nation, region
    WHERE
      p_partkey = ps_partkey
      AND s_suppkey = ps_suppkey
      AND s_nationkey = n_nationkey
      AND n_regionkey = r_regionkey
      AND r_name = 'EUROPE'
  )
ORDER BY
  s_acctbal DESC,
  n_name,
  s_name,
  p_partkey
LIMIT 100

```

!image-2022-08-26-15-26-33-305.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28969,,,,,,,"26/Aug/22 07:26;lincoln.86xy;image-2022-08-26-15-26-33-305.png;https://issues.apache.org/jira/secure/attachment/13048619/image-2022-08-26-15-26-33-305.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 02:20:14 UTC 2022,,,,,,,,,,"0|z1836g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 02:20;godfreyhe;Fixed in master: d518086f475ec92a18592ec3c423bf6398e776cf
in 1.16.0: 0e02c082037979766b61da90e37f5fc555d71770;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print the lookup join hint on the node `Correlate` in the origin RelNode tree for easier debuging,FLINK-29112,13478755,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,26/Aug/22 07:20,02/Sep/22 08:04,04/Jun/24 20:41,02/Sep/22 08:04,,,,,,,,1.16.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28968,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 02 08:04:48 UTC 2022,,,,,,,,,,"0|z18360:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 08:04;godfrey;Fixed in mater: 69872241788f112cb3b9148269c3c494487d7bc4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deprecate 'lookup.async' in hbase connector to expose both async and sync lookup capabilities to planner,FLINK-29111,13478754,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,lincoln.86xy,lincoln.86xy,26/Aug/22 07:16,21/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,Connectors / HBase,,,,0,auto-deprioritized-major,pull-request-available,,,"FLIP-234 introduced a LOOKUP hint which support suggest planner to use sync or async lookup function by hint option 'async' = 'false' or 'true', also the planner prefers an async lookup when a connector has both sync and async lookup capabilities.

As discussed in the mailing list before ([https://lists.apache.org/thread/9k1sl2519kh2n3yttwqc00p07xdfns3h]), this 'lookup.async' option should be deprecated and refactor the hbase table source to expose both async and sync lookup capabilities to planner, so that users can switch to use the new query hint",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28968,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:26 UTC 2023,,,,,,,,,,"0|z1835s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to mount a dynamically-created pvc for JM and TM in standalone mode with StatefulSet.,FLINK-29110,13478739,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Later,,kevin_123,kevin_123,26/Aug/22 04:19,24/Aug/23 07:48,04/Jun/24 20:41,24/Aug/23 07:48,,,,,,,,,,,,Kubernetes Operator,,,,0,auto-deprioritized-major,pull-request-available,,,"Use StatefulSet instead of Deployment to deploy JM and TM to support mount a dynamically-created PersistentVolumeClaim.

add volumeClaimTemplates to JobManagerSpec and TaskManagerSpec:

JobManagerSpec:
{code:java}
public class JobManagerSpec {
    /** Resource specification for the JobManager pods. */
    private Resource resource;

    /** Number of JobManager replicas. Must be 1 for non-HA deployments. */
    private int replicas = 1;

    /** Volume Claim Templates for JobManager stateful set. Just for standalone mode. */
    private List<PersistentVolumeClaim> volumeClaimTemplates = new ArrayList<>();

    /** JobManager pod template. It will be merged with FlinkDeploymentSpec.podTemplate. */
    private Pod podTemplate;
}
 {code}
TaskManagerSpec:
{code:java}
public class TaskManagerSpec {
    /** Resource specification for the TaskManager pods. */
    private Resource resource;

    /** Number of TaskManager replicas. If defined, takes precedence over parallelism */
    @SpecReplicas private Integer replicas;

    /** Volume Claim Templates for TaskManager stateful set. Just for standalone mode. */
    private List<PersistentVolumeClaim> volumeClaimTemplates = new ArrayList<>();

    /** TaskManager pod template. It will be merged with FlinkDeploymentSpec.podTemplate. */
    private Pod podTemplate;
} {code}
 

volumeClaimTemplates just available in standalone mode[1].

CR Example:
{code:java}
kind: FlinkDeployment
metadata:
  namespace: default
  name: basic-example
spec:
  image: flink:1.14.3
  flinkVersion: v1_14
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""2""
  serviceAccount: flink
  jobManager:
    replicas: 1
    resource:
      memory: ""2048m""
      cpu: 1
    volumeClaimTemplates:
      - metadata:
          name: log
        spec:
          accessModes: [ ""ReadWriteOnce"" ]
          storageClassName: ""alicloud-local-lvm""
          resources:
            requests:
              storage: 10Gi
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        name: job-manager-pod-template
      spec:
        containers:
          - name: flink-main-container
            volumeMounts:
              - name: log
                mountPath: /opt/flink/log
  taskManager:
    replicas: 1 // (only needed for standalone clusters)*     
    resource:
      memory: ""2048m""
      cpu: 1
    volumeClaimTemplates: 
      - metadata:
          name: log
        spec:
          accessModes: [ ""ReadWriteOnce"" ]
          storageClassName: ""alicloud-local-lvm""
          resources:
            requests:
              storage: 10Gi
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        name: task-manager-pod-template
      spec:
        containers:
          - name: flink-main-container
            volumeMounts:
              - name: log
                mountPath: /opt/flink/log
  mode: standalone {code}
[1]. [FLIP-225: Implement standalone mode support in the kubernetes operator - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-225%3A+Implement+standalone+mode+support+in+the+kubernetes+operator]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 07:48:17 UTC 2023,,,,,,,,,,"0|z1832g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 04:23;kevin_123;[~wangyang0918] [~gyfora] [~usamj] ;;;","26/Aug/22 04:42;gyfora;What would be the benefit of adding support for these persistent volumes?;;;","26/Aug/22 07:17;kevin_123;[~gyfora] 

When I use custom StorageClass create ReadWriteOnce PVC for many TMs, it failed. 

The differences is:

With {*}Deployments{*}:

We can either create a single PVC for all TMs, and all TM share one single volume performance.

Or we can manually create n PVCs for n TMs  and map them accordingly (this imitates the behavior of volumeClaimTemplates and opens us to all sorts of problems because of the ""manual"" approach)

With {*}StatefulSets{*}:
when a pod is started it creates a claim based on the template and its name (achieving the needed results without any fancy management from the operator).
 

An example for the problem k8s faces with Deployment here:
Lets say you have 30 TMs and need 30 PVCs for them (lvm volume for each TM where the local rocksDB is written to),
How will k8s know to map a PVC to a TM? (if you don't use the template mechanism in StatefulSet, you will have to specify a name in the PVC and map it to each TM) and when a pod crashes and gets built again by k8s, how can we use the previous PVC for it? or delete the old one and map it to it? those things seems to be impossible for a Deployment and require us to use StatefulSet.

Mounting external volume for each TM is a essential, and there are two advantages that cannot be ignored:
 * disk space - Large states storage
 * disk performance - Each TM can use the disk independently;;;","29/Aug/22 02:50;wangyang0918;If the kubelet starts with a big local disk for rootDir, then using the emptyDir for the logs and rocksdb local state will be the best choice. However, I agree that using the StatefulSet will have more benefits. For example, accelerate the recovery by using working directory[1], mount a dedicated PV for each TM to get a better performance.

 

There's a related ticket FLINK-24332 for the native support. Since it is not a critical requirement, we have not put much efforts on it. Please be aware that it is not very easy to support working directory for native K8s mode because we always assume that the TaskManager pod will never be restarted and re-registered.

 

But I am not against with having a try on the standalone mode first.

 

[1]. https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/standalone/working_directory/;;;","29/Aug/22 07:05;gyfora;+1 from my side, seems there are many potential benefits without downsides. [~kevin_123] would you like to also work on this ticket?

In any case we should try to finish this before 1.2.0 (end of september) as that will be the first release supporting standalone deployment mode;;;","29/Aug/22 11:41;kevin_123;[~gyfora] I'm very excited about this ticket, and in fact, I'm already working on it.

Please assign this ticket to me!;;;","29/Aug/22 11:52;gyfora;That's the spirit :D [~kevin_123] I have assigned this ticket to you;;;","31/Aug/22 08:44;gyfora;[@Grypse|https://github.com/Grypse] this seems like a fairly large architecture decision that we underestimated at first. We would like to kindly ask you to prepare your proposal in a FLIP format and post it on the DEV mailing list.

Please describe the following points in the FLIP:
 * What is the benefit of using StatefulSet for TaskManagers?

 ** Does this only affect local recovery? What is the performance gain compared to Deployments?
 ** What benefit do we have compared to the PVC available in the podTemplate? Seems like ReadWriteMany PVCs already provide the same benefits for Deployments
 ** What is the implication of TaskManager startup time? Deployments start pods faster as they dont need to wait for termination
 ** Should we make Deployment / StatefulSet configurable?
 * What is the benefit of using StatefulSet for JobManagers?

 ** What data would the jobmanager store in PVC? What is the performance gain?
 ** Does this only affect state handles? That is very small data.
 ** Why can't we do this already with the podTemplate?;;;","31/Aug/22 13:43;kevin_123;[~gyfora] Okay, I will make a FLIP to describe the proposal.;;;","06/Sep/22 15:57;Yanfei Lei;[~kevin_123], as we know,  PVs and PVCs are preserved when scaling down a StatefulSet, what are you going to do with the remaining PVCs and the data on PVs when scaling down? Will there be a program responsible for managing their lifecycle?;;;","28/Nov/22 09:36;gyfora;[~kevin_123]  are you still working on this?;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","24/Aug/23 07:48;gyfora;I am closing this for now [~kevin_123] please feel free to reopen the ticket if you would like to investigate further / create a FLIP;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint path conflict with stateless upgrade mode,FLINK-29109,13478724,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,26/Aug/22 00:59,04/Dec/22 20:49,04/Jun/24 20:41,04/Dec/22 20:49,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,Kubernetes Operator,,,,0,pull-request-available,,,,"A stateful job with stateless upgrade mode (yes, there are such use cases) fails with checkpoint path conflict due to constant jobId and FLINK-19358 (applies to Flink < 1.16x). Since with stateless upgrade mode the checkpoint id resets on restart the job is going to write to previously used locations and fail. The workaround is to rotate the jobId on every redeploy when the upgrade mode is stateless. While this can be worked around externally it is best done in the operator itself because reconciliation resolves when a restart is actually required while rotating jobId externally may trigger unnecessary restarts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 03 15:19:26 UTC 2022,,,,,,,,,,"0|z182z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 15:18;gyfora;merged to main 6f0914bbd296c9daf2664afe0a77d1df4f2e157e;;;","23/Nov/22 08:11;gyfora;[~thw] , looking at this again, it seems that we might need this logic for all Flink versions (not just before 1.16) in 1.16 you get a generated jobId based on the clusterid but in our case thats also fixed, so it will lead to the same issues.

What do you think?;;;","03/Dec/22 15:19;thw;[~gyfora] thanks for catching this. Because the jobId assigned by Flink is deterministic (HighAvailabilityOptions.HA_CLUSTER_ID), we will also need to apply the random jobId for stateless upgrade mode for Flink version >= 1.16 to avoid the checkpoint path collisions. 

https://github.com/apache/flink/blob/e70fe68dea764606180ca3728184c00fc63ea0ff/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L227;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator: Support queryable state,FLINK-29108,13478719,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,rcrocker@newrelic.com,rcrocker@newrelic.com,25/Aug/22 21:25,08/Jun/23 16:29,04/Jun/24 20:41,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,,"Enable the kubernetes operator to deploy jobs where queryable state is desired.

When queryable state is desired, the operator should configure the deployed job with
 # The deployed job has {{queryable-state.enabled:}} {{true}} applied to it.
 # Configure the Queryable State proxy and Queryable State server (via the {{queryable-state.proxy}} and {{queryable-state.server}} configuration sections respectively). If these sections aren't provided, then the default configuration is used.

The operator will need to create a Kubernetes service fronting the Task Managers {{QueryableStateClientProxy}} port (as configured by the above).

Tearing down the job also tears down the service.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 10:22:34 UTC 2023,,,,,,,,,,"0|z182y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 03:16;masteryhx;Hi, [~rcrocker@newrelic.com] . I see ""Querable state"" has been marked as ""Reaching end-of-life""[1].
You want to support Querable based on it ?

[1] https://flink.apache.org/roadmap.html;;;","26/Aug/22 16:35;rcrocker@newrelic.com;[~masteryhx] In a word, yes.

There's two things in play:
 # While its ""reaching end of life,"" Queryable State remains a supported capability in all of the Flink versions that the operator currently supports. My company would like to use this Kubernetes operator to manage our Flink jobs, and some of those jobs require Queryable State. We can't use this operator for those jobs until it supports Queryable State for some of those jobs. 
 # I'm trying to rescue Queryable State from deprecation. In [my recent presentation at Flink Forward|[http://example.com|https://www.slideshare.net/FlinkForward/using-queryable-state-for-fun-and-profit]] I made what I'd claim is a fairly strong argument for keeping queryable state in the Flink feature set. ({_}TLDR: Using Flink Queryable State, I can save >90% of the cost of the equivalent Redis-based solution{_})

I'm looking for allies in the fight to keep Queryable State alive.;;;","28/Aug/22 18:19;gyfora;I understand that this is an important feature for you. But putting something like this into the CRD and adding it to the operator logic is not necessarily something we should do for a deprecated feature.

If you manage to convince the community to ""undeprecate"" it and support it forward then it's a slightly different situation but even then it might be an overkill to bake the support into the operator.

What I suggest as a first step and a middle-ground is to add some plugin mechanism to be able to extend the core opertaor logic in a way that something like this can be implemented. We can then have this as a reference implementation / example in the repo.;;;","28/Aug/22 18:20;gyfora;We already have some plugin mechanisms, such as validators, resource listeners that allows users to extend the operator for their own purposes. This could be something very similar :) ;;;","28/Aug/22 23:22;rcrocker@newrelic.com;[~gyfora] I can accept starting with ""extend operator via plugin mechanism"". ;;;","17/Apr/23 10:22;Alcántara;[~rcrocker@newrelic.com] There is no recording available for your presentation right? Queryable State looks like a good idea, I would be interested in keeping it alive too for observability purposes. On a related note, some people are opting for externalizing the local state instead, see, e.g.:
 - [https://engineering.contentsquare.com/2022/flink-external-rocksdb-state-to-aerospike/]
 - [https://www.youtube.com/watch?v=ZWq_TzsXssM (FlinkNDB)|https://www.youtube.com/watch?v=ZWq_TzsXssM]

How likely is to add an external state backend to Flink in the short term? In the meantime, it would be nice to have some guidelines / recommendations for when to rely on Queryable State vs an external/global DB. Also, the current situation of Queryable State is a bit confusing probably...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade spotless version to improve spotless check efficiency,FLINK-29107,13478665,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yu Chen,Yu Chen,Yu Chen,25/Aug/22 14:12,30/Jan/24 21:43,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,Build System,,,,0,,,,,"I noticed a [discussion|https://github.com/diffplug/spotless/issues/927] in the spotless GitHub repository that we can improve the efficiency of spotless checks significantly by upgrading the version of spotless and enabling the `upToDateChecking`.

I have made a simple test locally and the improvement of the spotless check after the upgrade is shown in the figure.
!image-2022-08-25-22-10-54-453.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/22 14:10;Yu Chen;image-2022-08-25-22-10-54-453.png;https://issues.apache.org/jira/secure/attachment/13048599/image-2022-08-25-22-10-54-453.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 21:43:04 UTC 2024,,,,,,,,,,"0|z182m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 15:50;yunta;An interesting feature. [~Yu Chen] , already assigned to you, please go ahead. ;;;","25/Aug/22 20:29;chesnay;we cant upgrade spotless. The current version we use is the last version supported by the intellij plugin that runs on java 8. beyond that we run into formatting inconsistencies.;;;","26/Aug/22 05:52;yunta;[~chesnay]  Unfortunately to hear this:(, could you share the ticket or issue links of this inconsistencies formatting problem?;;;","30/Jan/24 21:43;jhughes;[~chesnay] Do you know if the IntelliJ plugin has improved in the last year?  

If so, I'd suggest we update the Maven Spotless plugin to version 2.35.0.  That is the last version which does not require changing the `google-java-format`. 

```
Execution default-cli of goal com.diffplug.spotless:spotless-maven-plugin:2.36.0:apply failed: You are running Spotless on JVM 11. This requires google-java-format of at least 1.8 (you are using 1.7).
```

[~yunta] as a work around, you can add `-Dspotless.version=2.35.0` to your Maven commands to override the version locally. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
value of metric 'idleTimeMsPerSecond'  more than 1000,FLINK-29106,13478632,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,stupid_pig,stupid_pig,25/Aug/22 11:18,25/Aug/22 11:55,04/Jun/24 20:41,25/Aug/22 11:55,1.11.3,,,,,,,,,,,Runtime / Metrics,,,,0,,,,," As the picture shown below, the value of metric 'idleTimeMsPerSecond' is more than 1000.

It's obviously unreasonable!

 

 

!image-2022-08-25-19-18-52-755.png!",flink 1.11.3,,,,,,,,,,,,,,,,,,,,FLINK-19174,,,,,,,,,,,,,,,,,,,"25/Aug/22 11:18;stupid_pig;image-2022-08-25-19-18-52-755.png;https://issues.apache.org/jira/secure/attachment/13048590/image-2022-08-25-19-18-52-755.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-25 11:18:03.0,,,,,,,,,,"0|z182ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesStateHandleStoreTest.testAddAndLockShouldNotThrowAlreadyExistExceptionWithSameContents failed with AssertionFailedError,FLINK-29105,13478588,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wangyang0918,hxb,hxb,25/Aug/22 07:38,27/Aug/22 10:20,04/Jun/24 20:41,27/Aug/22 10:20,1.16.0,,,,,,,1.16.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-25T04:19:22.1429302Z Aug 25 04:19:22 [ERROR] Tests run: 25, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.827 s <<< FAILURE! - in org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest
2022-08-25T04:19:22.1447098Z Aug 25 04:19:22 [ERROR] org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.testAddAndLockShouldNotThrowAlreadyExistExceptionWithSameContents  Time elapsed: 0.031 s  <<< FAILURE!
2022-08-25T04:19:22.1447862Z Aug 25 04:19:22 org.opentest4j.AssertionFailedError: 
2022-08-25T04:19:22.1448236Z Aug 25 04:19:22 
2022-08-25T04:19:22.1448561Z Aug 25 04:19:22 expected: 2
2022-08-25T04:19:22.1448893Z Aug 25 04:19:22  but was: 0
2022-08-25T04:19:22.1449330Z Aug 25 04:19:22 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-25T04:19:22.1450330Z Aug 25 04:19:22 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-25T04:19:22.1451114Z Aug 25 04:19:22 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-25T04:19:22.1452006Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.retryWithFirstFailedK8sOperation(KubernetesStateHandleStoreTest.java:1218)
2022-08-25T04:19:22.1452956Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.access$600(KubernetesStateHandleStoreTest.java:59)
2022-08-25T04:19:22.1453863Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest$5.lambda$null$0(KubernetesStateHandleStoreTest.java:245)
2022-08-25T04:19:22.1454744Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.kubeclient.TestingFlinkKubeClient.checkAndUpdateConfigMap(TestingFlinkKubeClient.java:182)
2022-08-25T04:19:22.1455619Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.updateConfigMap(KubernetesStateHandleStore.java:634)
2022-08-25T04:19:22.1456553Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStore.addAndLock(KubernetesStateHandleStore.java:219)
2022-08-25T04:19:22.1457435Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest$5.lambda$new$1(KubernetesStateHandleStoreTest.java:258)
2022-08-25T04:19:22.1458482Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityTestBase$Context.runTest(KubernetesHighAvailabilityTestBase.java:107)
2022-08-25T04:19:22.1459383Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest$5.<init>(KubernetesStateHandleStoreTest.java:237)
2022-08-25T04:19:22.1460391Z Aug 25 04:19:22 	at org.apache.flink.kubernetes.highavailability.KubernetesStateHandleStoreTest.testAddAndLockShouldNotThrowAlreadyExistExceptionWithSameContents(KubernetesStateHandleStoreTest.java:235)
2022-08-25T04:19:22.1461357Z Aug 25 04:19:22 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-25T04:19:22.1461965Z Aug 25 04:19:22 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-25T04:19:22.1462653Z Aug 25 04:19:22 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-25T04:19:22.1463280Z Aug 25 04:19:22 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-25T04:19:22.1463903Z Aug 25 04:19:22 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-25T04:19:22.1464604Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-25T04:19:22.1465397Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-25T04:19:22.1466211Z Aug 25 04:19:22 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-25T04:19:22.1466943Z Aug 25 04:19:22 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-25T04:19:22.1467701Z Aug 25 04:19:22 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-08-25T04:19:22.1468531Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-25T04:19:22.1469350Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-25T04:19:22.1470180Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-25T04:19:22.1471027Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-25T04:19:22.1471919Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-25T04:19:22.1472727Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-25T04:19:22.1473455Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-25T04:19:22.1474161Z Aug 25 04:19:22 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-25T04:19:22.1474946Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-25T04:19:22.1475761Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1476561Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-25T04:19:22.1477360Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-25T04:19:22.1478133Z Aug 25 04:19:22 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-25T04:19:22.1478911Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-25T04:19:22.1479701Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1480486Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-25T04:19:22.1481214Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-25T04:19:22.1481944Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-25T04:19:22.1482797Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1483563Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-25T04:19:22.1484287Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-25T04:19:22.1485176Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-25T04:19:22.1486258Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-25T04:19:22.1487314Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-25T04:19:22.1488228Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-25T04:19:22.1489017Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1489801Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-25T04:19:22.1490525Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-25T04:19:22.1491248Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-25T04:19:22.1492090Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1492846Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-25T04:19:22.1493581Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-25T04:19:22.1494474Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-25T04:19:22.1495547Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-25T04:19:22.1496450Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-25T04:19:22.1497242Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1498115Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-25T04:19:22.1498842Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-25T04:19:22.1499553Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-25T04:19:22.1500336Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-25T04:19:22.1501101Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-25T04:19:22.1501835Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-25T04:19:22.1502727Z Aug 25 04:19:22 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-25T04:19:22.1503654Z Aug 25 04:19:22 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-25T04:19:22.1504262Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-25T04:19:22.1504872Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-25T04:19:22.1505502Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-25T04:19:22.1506135Z Aug 25 04:19:22 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-25T04:19:22.1506610Z Aug 25 04:19:22  {code}
https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/40355/logs/956",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28265,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 27 10:20:43 UTC 2022,,,,,,,,,,"0|z18254:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 07:51;wangyang0918;I will have a look.;;;","26/Aug/22 06:13;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40394&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","27/Aug/22 10:20;wangyang0918;Fixed via:

master(1.16): 7b394a3ddd57e1bf88426fad92090f93fcdf6e4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check mutability of CoreOptions,FLINK-29104,13478555,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,25/Aug/22 06:54,29/Aug/22 05:20,04/Jun/24 20:41,29/Aug/22 05:20,table-store-0.3.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"We've checked some options but in a case-by-case way, and it's time for CoreOptions to form a thorough way of checking mutability. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 05:20:00 UTC 2022,,,,,,,,,,"0|z181xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 05:20;lzljs3620320;master: cd708bac08efc4db380534aa18d0a1584ac135f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException: No such file or directory: s3://,FLINK-29103,13478537,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Bug,,hehetown,hehetown,25/Aug/22 03:41,25/Aug/22 08:53,04/Jun/24 20:41,25/Aug/22 08:53,1.15.1,,,,,,,1.14.4,,,,Connectors / FileSystem,,,,0,,,,,"The following error is reported when the application is released on k8s, but it is normal in 1.14.4.

2022-08-25 11:09:40,008 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.
2022-08-25 11:09:40,078 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Stopping DefaultJobGraphStore.
2022-08-25 11:09:40,081 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkRuntimeException: Could not retrieve JobResults of globally-terminated jobs from JobResultStore
    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_342]
    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) [?:1.8.0_342]
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606) [?:1.8.0_342]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_342]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_342]
    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_342]
Caused by: org.apache.flink.util.FlinkRuntimeException: Could not retrieve JobResults of globally-terminated jobs from JobResultStore
    at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResults(SessionDispatcherLeaderProcess.java:192) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.supplyUnsynchronizedIfRunning(AbstractDispatcherLeaderProcess.java:198) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResultsIfRunning(SessionDispatcherLeaderProcess.java:184) ~[flink-dist-1.15.1.jar:1.15.1]
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_342]
    ... 3 more
Caused by: java.io.FileNotFoundException: No such file or directory: s3://flink/ods/rdemo/recovery/job-result-store/ods-cs-192
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2344) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:1961) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$9(S3AFileSystem.java:1940) ~[?:?]
    at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:1940) ~[?:?]
    at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?]
    at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.listStatus(PluginFileSystemFactory.java:141) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.highavailability.FileSystemJobResultStore.getDirtyResultsInternal(FileSystemJobResultStore.java:179) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.highavailability.AbstractThreadsafeJobResultStore.withReadLock(AbstractThreadsafeJobResultStore.java:118) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.highavailability.AbstractThreadsafeJobResultStore.getDirtyResults(AbstractThreadsafeJobResultStore.java:100) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResults(SessionDispatcherLeaderProcess.java:190) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.dispatcher.runner.AbstractDispatcherLeaderProcess.supplyUnsynchronizedIfRunning(AbstractDispatcherLeaderProcess.java:198) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess.getDirtyJobResultsIfRunning(SessionDispatcherLeaderProcess.java:184) ~[flink-dist-1.15.1.jar:1.15.1]
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) ~[?:1.8.0_342]
    ... 3 more
2022-08-25 11:09:40,180 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.cluster-id, ","docker images: flink 1.15

minio+k8s",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 08:52:07 UTC 2022,,,,,,,,,,"0|z181ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 08:52;hehetown;{{flink-s3-fs-hadoop}}  registers for {color:#FF0000}*_s3a://_*  {color}

{{flink-s3-fs-presto}} also registers for {color:#FF0000}*_s3p://_*{color}

 

If you use s3 ://, an error will be reported 【Caused by: java.io.FileNotFoundException: No such file or directory: s3://】.

It must be clear that you use {color:#FF0000}*_s3a_*{color} or {color:#FF0000}*_s3p_*{color}. I use {{{}flink-s3-fs-presto{}}}, so setting {color:#FF0000}*_s3p://_* {color}is normal;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogLocalRecoveryITCase.testRestartTM failed with AssertionFailedError,FLINK-29102,13478532,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,hxb,hxb,25/Aug/22 03:13,14/Sep/22 07:48,04/Jun/24 20:41,14/Sep/22 07:48,1.16.0,,,,,,,1.16.0,,,,Runtime / State Backends,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-25T02:10:19.1407131Z Aug 25 02:10:19 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 20.126 s <<< FAILURE! - in org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase
2022-08-25T02:10:19.1410088Z Aug 25 02:10:19 [ERROR] ChangelogLocalRecoveryITCase.testRestartTM  Time elapsed: 12.501 s  <<< FAILURE!
2022-08-25T02:10:19.1411734Z Aug 25 02:10:19 org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
2022-08-25T02:10:19.1413028Z Aug 25 02:10:19 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:43)
2022-08-25T02:10:19.1414753Z Aug 25 02:10:19 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:146)
2022-08-25T02:10:19.1416077Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForAllTaskRunning$3(CommonTestUtils.java:213)
2022-08-25T02:10:19.1417807Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
2022-08-25T02:10:19.1419455Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
2022-08-25T02:10:19.1421184Z Aug 25 02:10:19 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:208)
2022-08-25T02:10:19.1422893Z Aug 25 02:10:19 	at org.apache.flink.test.checkpointing.ChangelogLocalRecoveryITCase.testRestartTM(ChangelogLocalRecoveryITCase.java:149)
2022-08-25T02:10:19.1424381Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-25T02:10:19.1425730Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-25T02:10:19.1427428Z Aug 25 02:10:19 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-25T02:10:19.1428842Z Aug 25 02:10:19 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-25T02:10:19.1430173Z Aug 25 02:10:19 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-25T02:10:19.1431683Z Aug 25 02:10:19 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-25T02:10:19.1433150Z Aug 25 02:10:19 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-25T02:10:19.1434760Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-25T02:10:19.1436296Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-25T02:10:19.1437704Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-25T02:10:19.1439268Z Aug 25 02:10:19 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-25T02:10:19.1441412Z Aug 25 02:10:19 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-25T02:10:19.1442792Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-25T02:10:19.1444190Z Aug 25 02:10:19 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-25T02:10:19.1445593Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-25T02:10:19.1446946Z Aug 25 02:10:19 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-25T02:10:19.1448568Z Aug 25 02:10:19 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-25T02:10:19.1449909Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-25T02:10:19.1451376Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-25T02:10:19.1452655Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-25T02:10:19.1454003Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-25T02:10:19.1455239Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-25T02:10:19.1456511Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-25T02:10:19.1458149Z Aug 25 02:10:19 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-08-25T02:10:19.1460189Z Aug 25 02:10:19 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-08-25T02:10:19.1461574Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-25T02:10:19.1462877Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-25T02:10:19.1464471Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-25T02:10:19.1465869Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-25T02:10:19.1467177Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-25T02:10:19.1468801Z Aug 25 02:10:19 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-25T02:10:19.1470145Z Aug 25 02:10:19 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-25T02:10:19.1471536Z Aug 25 02:10:19 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-08-25T02:10:19.1472951Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-25T02:10:19.1474218Z Aug 25 02:10:19 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-25T02:10:19.1475390Z Aug 25 02:10:19 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-25T02:10:19.1476566Z Aug 25 02:10:19 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-25T02:10:19.1478077Z Aug 25 02:10:19 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-25T02:10:19.1480014Z Aug 25 02:10:19 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-25T02:10:19.1481463Z Aug 25 02:10:19 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-25T02:10:19.1482776Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-25T02:10:19.1486583Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-25T02:10:19.1488264Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-25T02:10:19.1489877Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-25T02:10:19.1491691Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-25T02:10:19.1492986Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-25T02:10:19.1494236Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-25T02:10:19.1495567Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-25T02:10:19.1496961Z Aug 25 02:10:19 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-25T02:10:19.1498390Z Aug 25 02:10:19 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-25T02:10:19.1499796Z Aug 25 02:10:19 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-25T02:10:19.1501267Z Aug 25 02:10:19 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-25T02:10:19.1502560Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-25T02:10:19.1503777Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-25T02:10:19.1504924Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-25T02:10:19.1506127Z Aug 25 02:10:19 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-25T02:10:19.1507702Z Aug 25 02:10:19 Caused by: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=10)
2022-08-25T02:10:19.1509606Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-25T02:10:19.1511280Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-25T02:10:19.1512791Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-25T02:10:19.1514116Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-25T02:10:19.1515412Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-25T02:10:19.1516738Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-25T02:10:19.1518192Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-25T02:10:19.1519748Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-08-25T02:10:19.1521544Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1619)
2022-08-25T02:10:19.1522964Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1137)
2022-08-25T02:10:19.1524151Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1077)
2022-08-25T02:10:19.1525332Z Aug 25 02:10:19 	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:778)
2022-08-25T02:10:19.1526624Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195)
2022-08-25T02:10:19.1528105Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182)
2022-08-25T02:10:19.1529562Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SharedSlot.lambda$release$4(SharedSlot.java:270)
2022-08-25T02:10:19.1530816Z Aug 25 02:10:19 	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
2022-08-25T02:10:19.1531900Z Aug 25 02:10:19 	at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683)
2022-08-25T02:10:19.1532921Z Aug 25 02:10:19 	at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010)
2022-08-25T02:10:19.1533628Z Aug 25 02:10:19 	at org.apache.flink.runtime.scheduler.SharedSlot.release(SharedSlot.java:270)
2022-08-25T02:10:19.1534360Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152)
2022-08-25T02:10:19.1535210Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:483)
2022-08-25T02:10:19.1536322Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeAndReleaseSlots(DefaultDeclarativeSlotPool.java:475)
2022-08-25T02:10:19.1537411Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:446)
2022-08-25T02:10:19.1538422Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.internalReleaseTaskManager(DeclarativeSlotPoolService.java:275)
2022-08-25T02:10:19.1539375Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.releaseTaskManager(DeclarativeSlotPoolService.java:231)
2022-08-25T02:10:19.1540193Z Aug 25 02:10:19 	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:533)
2022-08-25T02:10:19.1540913Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-25T02:10:19.1541778Z Aug 25 02:10:19 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-25T02:10:19.1542700Z Aug 25 02:10:19 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-25T02:10:19.1543354Z Aug 25 02:10:19 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-25T02:10:19.1544039Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-25T02:10:19.1544853Z Aug 25 02:10:19 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-25T02:10:19.1545661Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-25T02:10:19.1546413Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-25T02:10:19.1547179Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-25T02:10:19.1548110Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-25T02:10:19.1548869Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-25T02:10:19.1549682Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-25T02:10:19.1550579Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-25T02:10:19.1551529Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-25T02:10:19.1552170Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-25T02:10:19.1552821Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-25T02:10:19.1553467Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-25T02:10:19.1554125Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-25T02:10:19.1554732Z Aug 25 02:10:19 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-25T02:10:19.1555398Z Aug 25 02:10:19 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-25T02:10:19.1555993Z Aug 25 02:10:19 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-25T02:10:19.1556612Z Aug 25 02:10:19 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-25T02:10:19.1557203Z Aug 25 02:10:19 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-25T02:10:19.1557942Z Aug 25 02:10:19 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-25T02:10:19.1558512Z Aug 25 02:10:19 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-25T02:10:19.1559032Z Aug 25 02:10:19 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-25T02:10:19.1559620Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-25T02:10:19.1560282Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-25T02:10:19.1561023Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-25T02:10:19.1561690Z Aug 25 02:10:19 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-25T02:10:19.1562440Z Aug 25 02:10:19 Caused by: org.apache.flink.util.FlinkExpectedException: org.apache.flink.util.FlinkExpectedException: The TaskExecutor is shutting down.
2022-08-25T02:10:19.1563197Z Aug 25 02:10:19 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.onStop(TaskExecutor.java:456)
2022-08-25T02:10:19.1563908Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:238)
2022-08-25T02:10:19.1564671Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.lambda$terminate$0(AkkaRpcActor.java:578)
2022-08-25T02:10:19.1565497Z Aug 25 02:10:19 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-25T02:10:19.1566406Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:577)
2022-08-25T02:10:19.1567177Z Aug 25 02:10:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:196)
2022-08-25T02:10:19.1568114Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-25T02:10:19.1568721Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-25T02:10:19.1569337Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-25T02:10:19.1569944Z Aug 25 02:10:19 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-25T02:10:19.1570568Z Aug 25 02:10:19 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-25T02:10:19.1571302Z Aug 25 02:10:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-25T02:10:19.1571808Z Aug 25 02:10:19 	... 13 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40355&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9",,,,,,,,,,,,,,,,,,,,FLINK-29147,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 07:48:50 UTC 2022,,,,,,,,,,"0|z181so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 03:13;hxb;[~Yanfei Lei] Could you help take a look? Thx.;;;","25/Aug/22 03:17;Yanfei Lei;[~hxb]  Thanks for reporting, I will fix it later.;;;","13/Sep/22 03:29;hxb;Hi [~Yanfei Lei] any updates on this issue?;;;","14/Sep/22 07:48;hxb;Merged into master via 545ce28da098eaa7be2b0f1549336db5d939d023

Merged into release-1.16 via 8fb13a803904485e0d44ffee8e1510b14543129f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PipelinedRegionSchedulingStrategy benchmark shows performance degradation,FLINK-29101,13478530,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,25/Aug/22 03:00,14/Sep/22 14:57,04/Jun/24 20:41,14/Sep/22 14:57,1.16.0,,,,,,,1.16.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"Throw TPC-DS and flink-benchmark testing, we found that PipelinedRegionSchedulingStrategy has performance degradation. By investigation, I can confirm that this was introduced by FLINK-28799 which introduce HYBRID type edge support for scheduling strategy.

The key to the problem is for blocking ALL_TO_ALL type edges should only enter the scheduling method when the last execution becomes finished, but the current implementation ignores this fact, resulting in the complexity of O(n ^ 2) in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 14:57:36 UTC 2022,,,,,,,,,,"0|z181s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 14:57;xtsong;- master (1.17): c0165a8a7e3ccc6e82df7a30c67497a10e281153
- release-1.16: eefeb6a539186e2bfe49f72d346f00474229547c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deployment with last-state upgrade mode stuck after initial error,FLINK-29100,13478515,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,thw,thw,thw,25/Aug/22 01:48,31/Aug/22 17:06,04/Jun/24 20:41,31/Aug/22 17:06,kubernetes-operator-1.1.0,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,"A deployment with last_state upgrade mode that never succeeds will be stuck in deploying state because no HA data exists. This can be reproduced by creating a deployment with invalid image or exception in entry point. Update to the CR that corrects the issue won't be reconciled due to ""o.a.f.k.o.r.d.ApplicationReconciler [INFO ] [default.basic-checkpoint-ha-example] Job is not running yet and HA metadata is not available, waiting for upgradeable state"". This forces manual intervention to delete the CR.

Instead,  operator should check if this is the initial deployment and if so skip the HA metadata check.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29159,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-25 01:48:41.0,,,,,,,,,,"0|z181ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock for Single Subtask in Kinesis Consumer,FLINK-29099,13478475,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,sethsaperstein,sethsaperstein,sethsaperstein,24/Aug/22 19:05,28/Nov/22 23:37,04/Jun/24 20:41,28/Nov/22 23:36,1.10.3,1.11.6,1.12.7,1.13.6,1.14.5,1.15.3,1.9.3,1.17.0,,,,Connectors / Kinesis,,,,0,connector,consumer,kinesis,pull-request-available,"Deadlock is reached as the result of:
 * max lookahead reached for local watermark
 * idle state for subtask

The lookahead prevents the RecordEmitter from emitting a new record. The idle state prevents the global watermark from being updated.

To exit this deadlock state, we need to complete the [TODO here|https://github.com/apache/flink/blob/221d70d9930f72147422ea24b399f006ebbfb8d7/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java#L1268] which updates the global watermark while the subtask is marked idle, which will then allow us to emit a record again as the lookahead is no longer reached.

 

*Context:*

We reached this scenario at Lyft as a result of prolonged CPU throttling on all FlinkKinesisConsumer threads for multiple minutes.

Walking through the series of events for a single subtask:
 * prolonged CPU throttling occurs and no logs are seen from any FlinkKinesisConsumer thread for up to 15 minutes
 * after CPU throttling the subtask is marked idle
 * the subtask has reached the lookahead for its local watermark relative to the global watermark
 * WatermarkSyncCallback indicates the subtask as idle and does not update the global watermark
 * emitQueue fills to max
 * RecordEmitter cannot emit records due to the max lookahead
 * Deadlock on subtask

At this point, we had not realized what had happened and processing of all other shards/subtasks had continued for multiple days. When we finally restarted the application, we saw the following behavior:
 * global watermark recalculated after all subtasks consumed data based on the last kinesis record sequence number
 * global watermark moved back in time multiple days, to when the subtask was first marked idle
 * the single subtask processed data while all others remained idle due to the lookahead

This would have continued until the subtask had caught up to the others and thus the global watermark is within reach of the lookahead for other subtasks.

 

*Repro:*

Too difficult to repro the exact scenario.",,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,java,Mon Nov 28 23:37:01 UTC 2022,,,,,,,,,,"0|z181g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 20:55;premsantosh;cc [~thomasWeise];;;","28/Nov/22 23:37;thw;[~sethsaperstein] thanks for the thorough investigation and fix!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StoreWriteOperator#prepareCommit should let logSinkFunction flush first before fetching offset,FLINK-29098,13478424,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,24/Aug/22 13:58,24/Sep/22 02:38,04/Jun/24 20:41,29/Aug/22 05:21,table-store-0.3.0,,,,,,,table-store-0.2.1,table-store-0.3.0,,,Table Store,,,,0,pull-request-available,,,,The cause for unstable CompositePkAndMultiPartitionedTableITCase#testEnableLogAndStreamingReadWriteMultiPartitionedRecordsWithMultiPk was found that the StoreWriteOperator#prepareCommit may get an empty log offset due to some condition where KafkaProducer#flush are not called.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 05:21:55 UTC 2022,,,,,,,,,,"0|z1814o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 05:21;lzljs3620320;master: 50b1b2951fcdfbfc139e21950086dddb3e01c690
release-0.2: db55ca641654a75e69f56ae3c34ccc08ec90f205;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moving json se/deserializers from sql-gateway-api to sql-gateway,FLINK-29097,13478402,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Wencong Liu,Wencong Liu,24/Aug/22 12:51,30/Aug/22 01:50,04/Jun/24 20:41,30/Aug/22 01:49,1.16.0,,,,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,,"Considering that the current json se/deserialization rules for results returned by SqlGateway are only used in Rest Endpoint, we migrated the serialization related tools from the flink-sql-gateway-api to the flink-sql-gateway package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 01:49:49 UTC 2022,,,,,,,,,,"0|z180zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 03:34;fsk119;I think we'd better to move it into the sql-gateway package rather than leave this into the sql-gateway-api package. Currently the sql-gateway-api is used to build an endpoint. If we leave the rest endpoint implementation, it may influence endpoints.  

In the future, we will modify the sql-client package and introduce a basic rest endpoint client for users to use. User can rely on the rest client to submit their SQL.

 ;;;","30/Aug/22 01:49;fsk119;Merged into master: e18782f9bb8ae9320742fb0cedad889661f82c78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"json_value when the path has blank, the result is not right",FLINK-29096,13478401,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,24/Aug/22 12:33,26/Aug/22 07:35,04/Jun/24 20:41,26/Aug/22 07:35,1.16.0,,,,,,,1.16.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"!https://aone.alipay.com/v2/api/workitem/adapter/file/url?fileIdentifier=workitem%2Falipay%2Fdefault%2F1661334308971image.png!

 

 

!image-2022-08-24-20-33-59-052.png!

 

!https://aone.alipay.com/v2/api/workitem/adapter/file/url?fileIdentifier=workitem%2Falipay%2Fdefault%2F1661334330457image.png!

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/22 12:34;jackylau;image-2022-08-24-20-33-59-052.png;https://issues.apache.org/jira/secure/attachment/13048513/image-2022-08-24-20-33-59-052.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 07:35:48 UTC 2022,,,,,,,,,,"0|z180zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 12:35;jackylau;the sql escape is ' , so i think we should add test for it. ;;;","26/Aug/22 07:35;jark;Fixed in master: 60e594414a7a0ee0930b63d8e859e8d6000b1f76 to 3149c621671605ccd676cbbe393074ae25715773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve logging in SharedStateRegistry ,FLINK-29095,13478383,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,jingge,jingge,24/Aug/22 10:13,18/Nov/22 21:26,04/Jun/24 20:41,08/Nov/22 06:55,1.16.0,,,,,,,1.17.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,,"with the incremental checkpoint, conceptually, state files that are never used by any checkpoint will be deleted/GC . In practices, state files might be deleted when they are still somehow required by the failover which will lead to Flink job fails.

We should add the log for trouble shooting.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 08 06:54:48 UTC 2022,,,,,,,,,,"0|z180vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 03:42;Yanfei Lei;[~jingge]  There is already a [trace log|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/IncrementalRemoteKeyedStateHandle.java#L252] when deleting {{{}IncrementalRemoteKeyedStateHandle，{}}}I think we can use this trace log to debug.;;;","30/Aug/22 15:09;roman;There is some logging already but it could be improved.

For example, SharedStateRegistry [logs on key duplication|https://github.com/apache/flink/blob/2220f24925ab5146d5771c3782ed8c0837bb0bc4/flink-runtime/src/main/java/org/apache/flink/runtime/state/SharedStateRegistryImpl.java#L134]:
{code:java}
Identified duplicate state registration under key 0d7c41ca-954d-49a2-97b9-4c42e9db1ad8-KeyGroupRange{startKeyGroup=64, endKeyGroup=127}-000066.sst. New state org.apache.flink.runtime.state.PlaceholderStreamStateHandle@7fcdb6ee was determined to be an unnecessary copy of existing state File State: <some path> <some size> and will be dropped.
{code}
Here,
1. The actual object to discard (scheduledStateDeletion) might not be logged 
2. entry.confirmed is not logged
3. The message is the same for two branches
4. Placeholder.toString can be overriden
5. The existing state is less interesting (it must have been already logged earlier)

Besides that, nothing is logged if a different object representing the same state is registered twice.
Usually, this isn't a problem; however, in some cases it might indicate a bug (e.g. SharedStateRegistry and CheckpointStore use different java objects for the same state on recovery).

So I'm going to change the issue to Improvement.;;;","03/Nov/22 05:52;Yanfei Lei;I'd like to continue this work, could you please assign it to me?;;;","03/Nov/22 07:35;roman;Sure, assigned to you. Thanks for volunteering!;;;","08/Nov/22 06:54;klion26;merged into master 9a4250d248e93f3e87b211df98ce3d3c66aabca0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot stop Python job with savepoint when using Kafka Consumer,FLINK-29094,13478382,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bartoszdeepbi,bartoszdeepbi,24/Aug/22 10:03,17/Oct/22 01:41,04/Jun/24 20:41,,1.15.0,1.15.1,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"We want to gracefully stop a Python job reading from Kafka. However, running `flink stop --savepointPath` causes an exception to be thrown and the job fails. Find the exception in the [Gist|https://gist.github.com/BartMiki/6eab470a49d8a935ed3e7e544c3f278f#file-exception-txt]. The type of savepoint also doesn't matter both canonical and native fail. This Stack Overflow [answer|https://stackoverflow.com/a/68280329] also does not work. 

However, checkpoints work fine. This leads to weird scenarios when you want to stop a Python job, stopping fails with an exception, and the job is restarted from the latest checkpoint.

The setup was tested in fresh default Flink 1.15.1 installation (however we also tried 1.15.0 with the same error).  The error occurred regardless of the amount of data processed from Kafka (could be even none). [Gist with minimal code required to trigger the exception|https://gist.github.com/BartMiki/6eab470a49d8a935ed3e7e544c3f278f].","Local Environment
 * OS: Ubuntu 20.04
 * Java: openjdk version ""11.0.12"" 2021-07-20
 * Flink: 1.15.1 (clean instalation)
 * Kafka: kafka_2.13-2.8.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-24 10:03:45.0,,,,,,,,,,"0|z180vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LookupJoinITCase failed with InternalCompilerException,FLINK-29093,13478353,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,smiralex,hxb,hxb,24/Aug/22 07:43,29/Sep/22 03:39,04/Jun/24 20:41,23/Sep/22 14:32,1.16.0,,,,,,,1.16.0,,,,Table SQL / Runtime,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-24T03:45:02.5915521Z Aug 24 03:45:02 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-08-24T03:45:02.5916823Z Aug 24 03:45:02 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-08-24T03:45:02.5919320Z Aug 24 03:45:02 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-08-24T03:45:02.5920833Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-08-24T03:45:02.5922361Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-08-24T03:45:02.5923733Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-24T03:45:02.5924922Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-24T03:45:02.5926191Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-08-24T03:45:02.5927677Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-24T03:45:02.5929091Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-24T03:45:02.5930430Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-24T03:45:02.5931966Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-24T03:45:02.5933293Z Aug 24 03:45:02 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-08-24T03:45:02.5934708Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-08-24T03:45:02.5936228Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-24T03:45:02.5937998Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-08-24T03:45:02.5939627Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-24T03:45:02.5941051Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-24T03:45:02.5942650Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-24T03:45:02.5944203Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-24T03:45:02.5945740Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-08-24T03:45:02.5947020Z Aug 24 03:45:02 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-08-24T03:45:02.5948130Z Aug 24 03:45:02 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-08-24T03:45:02.5949255Z Aug 24 03:45:02 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-08-24T03:45:02.5950405Z Aug 24 03:45:02 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-08-24T03:45:02.5951638Z Aug 24 03:45:02 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-08-24T03:45:02.5953564Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-08-24T03:45:02.5955214Z Aug 24 03:45:02 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-08-24T03:45:02.5956587Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-08-24T03:45:02.5958037Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-08-24T03:45:02.5959448Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-08-24T03:45:02.5960679Z Aug 24 03:45:02 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-08-24T03:45:02.5962183Z Aug 24 03:45:02 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-08-24T03:45:02.5963694Z Aug 24 03:45:02 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-08-24T03:45:02.5965024Z Aug 24 03:45:02 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-08-24T03:45:02.5966227Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-08-24T03:45:02.5967444Z Aug 24 03:45:02 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-08-24T03:45:02.5968642Z Aug 24 03:45:02 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-08-24T03:45:02.5969913Z Aug 24 03:45:02 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-08-24T03:45:02.5971340Z Aug 24 03:45:02 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-08-24T03:45:02.5972824Z Aug 24 03:45:02 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-08-24T03:45:02.5974100Z Aug 24 03:45:02 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-08-24T03:45:02.5975389Z Aug 24 03:45:02 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-08-24T03:45:02.5976670Z Aug 24 03:45:02 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-08-24T03:45:02.5978063Z Aug 24 03:45:02 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-08-24T03:45:02.5979479Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-24T03:45:02.5980728Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-24T03:45:02.5982143Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-24T03:45:02.5983410Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-24T03:45:02.5984613Z Aug 24 03:45:02 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-08-24T03:45:02.5986005Z Aug 24 03:45:02 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-24T03:45:02.5988034Z Aug 24 03:45:02 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-24T03:45:02.5989611Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-24T03:45:02.5990833Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-24T03:45:02.5992309Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-24T03:45:02.5993616Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-24T03:45:02.5994883Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-24T03:45:02.5996320Z Aug 24 03:45:02 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-08-24T03:45:02.5997572Z Aug 24 03:45:02 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-08-24T03:45:02.5998837Z Aug 24 03:45:02 	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
2022-08-24T03:45:02.6000095Z Aug 24 03:45:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-24T03:45:02.6001334Z Aug 24 03:45:02 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-24T03:45:02.6002879Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-24T03:45:02.6004484Z Aug 24 03:45:02 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-24T03:45:02.6006045Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-24T03:45:02.6007510Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-24T03:45:02.6009009Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-24T03:45:02.6010458Z Aug 24 03:45:02 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-24T03:45:02.6011933Z Aug 24 03:45:02 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-24T03:45:02.6013136Z Aug 24 03:45:02 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-24T03:45:02.6014334Z Aug 24 03:45:02 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-24T03:45:02.6015516Z Aug 24 03:45:02 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-24T03:45:02.6016746Z Aug 24 03:45:02 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-24T03:45:02.6017988Z Aug 24 03:45:02 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-24T03:45:02.6019207Z Aug 24 03:45:02 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-24T03:45:02.6020453Z Aug 24 03:45:02 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-24T03:45:02.6021664Z Aug 24 03:45:02 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-24T03:45:02.6022813Z Aug 24 03:45:02 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-24T03:45:02.6023947Z Aug 24 03:45:02 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-24T03:45:02.6025133Z Aug 24 03:45:02 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-24T03:45:02.6026232Z Aug 24 03:45:02 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-24T03:45:02.6027190Z Aug 24 03:45:02 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-24T03:45:02.6028082Z Aug 24 03:45:02 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-24T03:45:02.6029101Z Aug 24 03:45:02 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-24T03:45:02.6030155Z Aug 24 03:45:02 	... 4 more
2022-08-24T03:45:02.6032269Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-08-24T03:45:02.6033990Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.LookupFullCache.getIfPresent(LookupFullCache.java:74)
2022-08-24T03:45:02.6035675Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.CachingLookupFunction.lookup(CachingLookupFunction.java:122)
2022-08-24T03:45:02.6037117Z Aug 24 03:45:02 	at org.apache.flink.table.functions.LookupFunction.eval(LookupFunction.java:52)
2022-08-24T03:45:02.6038277Z Aug 24 03:45:02 	at LookupFunction$109548.flatMap(Unknown Source)
2022-08-24T03:45:02.6039528Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.doFetch(LookupJoinRunner.java:92)
2022-08-24T03:45:02.6041280Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:79)
2022-08-24T03:45:02.6043077Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34)
2022-08-24T03:45:02.6044507Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)
2022-08-24T03:45:02.6045903Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)
2022-08-24T03:45:02.6047336Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)
2022-08-24T03:45:02.6048783Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
2022-08-24T03:45:02.6050162Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
2022-08-24T03:45:02.6051690Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
2022-08-24T03:45:02.6053386Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
2022-08-24T03:45:02.6055157Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
2022-08-24T03:45:02.6056876Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
2022-08-24T03:45:02.6058505Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
2022-08-24T03:45:02.6059974Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-08-24T03:45:02.6061347Z Aug 24 03:45:02 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-08-24T03:45:02.6063003Z Aug 24 03:45:02 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-08-24T03:45:02.6065141Z Aug 24 03:45:02 Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-08-24T03:45:02.6066557Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
2022-08-24T03:45:02.6067945Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
2022-08-24T03:45:02.6069310Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1643)
2022-08-24T03:45:02.6070702Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
2022-08-24T03:45:02.6072177Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-24T03:45:02.6073457Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-24T03:45:02.6074907Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-24T03:45:02.6076156Z Aug 24 03:45:02 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-24T03:45:02.6077752Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: Failed to reload lookup 'FULL' cache.
2022-08-24T03:45:02.6079119Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:105)
2022-08-24T03:45:02.6080604Z Aug 24 03:45:02 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-08-24T03:45:02.6081691Z Aug 24 03:45:02 	... 5 more
2022-08-24T03:45:02.6083708Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: Failed to load data into the lookup 'FULL' cache from InputSplit org.apache.flink.table.runtime.functions.table.fullcache.FullCacheTestInputFormat$QueueInputSplit@67854815
2022-08-24T03:45:02.6085871Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.run(InputSplitCacheLoadTask.java:94)
2022-08-24T03:45:02.6087785Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputFormatCacheLoader.reloadCache(InputFormatCacheLoader.java:102)
2022-08-24T03:45:02.6089522Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.CacheLoader.run(CacheLoader.java:90)
2022-08-24T03:45:02.6090630Z Aug 24 03:45:02 	... 6 more
2022-08-24T03:45:02.6092180Z Aug 24 03:45:02 Caused by: java.lang.RuntimeException: Could not instantiate generated class 'KeyProjection$109542'
2022-08-24T03:45:02.6093614Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
2022-08-24T03:45:02.6095161Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:52)
2022-08-24T03:45:02.6096823Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:29)
2022-08-24T03:45:02.6098583Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.run(InputSplitCacheLoadTask.java:79)
2022-08-24T03:45:02.6099844Z Aug 24 03:45:02 	... 8 more
2022-08-24T03:45:02.6101114Z Aug 24 03:45:02 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-08-24T03:45:02.6102880Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
2022-08-24T03:45:02.6104217Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-08-24T03:45:02.6105525Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-08-24T03:45:02.6106608Z Aug 24 03:45:02 	... 11 more
2022-08-24T03:45:02.6108117Z Aug 24 03:45:02 Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-08-24T03:45:02.6110087Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
2022-08-24T03:45:02.6111720Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-08-24T03:45:02.6113370Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-08-24T03:45:02.6114888Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-08-24T03:45:02.6115931Z Aug 24 03:45:02 	... 13 more
2022-08-24T03:45:02.6117177Z Aug 24 03:45:02 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
2022-08-24T03:45:02.6118673Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
2022-08-24T03:45:02.6120123Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
2022-08-24T03:45:02.6121778Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
2022-08-24T03:45:02.6123576Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
2022-08-24T03:45:02.6125251Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
2022-08-24T03:45:02.6126991Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
2022-08-24T03:45:02.6128591Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
2022-08-24T03:45:02.6130111Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
2022-08-24T03:45:02.6131763Z Aug 24 03:45:02 	at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
2022-08-24T03:45:02.6133369Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
2022-08-24T03:45:02.6134777Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
2022-08-24T03:45:02.6136236Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
2022-08-24T03:45:02.6137497Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:52)
2022-08-24T03:45:02.6138796Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.keyselector.GenericRowDataKeySelector.getKey(GenericRowDataKeySelector.java:29)
2022-08-24T03:45:02.6140234Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.functions.table.lookup.fullcache.inputformat.InputSplitCacheLoadTask.run(InputSplitCacheLoadTask.java:79)
2022-08-24T03:45:02.6141457Z Aug 24 03:45:02 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-08-24T03:45:02.6146427Z Aug 24 03:45:02 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-24T03:45:02.6147685Z Aug 24 03:45:02 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-24T03:45:02.6149049Z Aug 24 03:45:02 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-24T03:45:02.6150192Z Aug 24 03:45:02 	at java.lang.Thread.run(Thread.java:748)
2022-08-24T03:45:02.6153609Z Aug 24 03:45:02 Caused by: org.codehaus.janino.InternalCompilerException: Compiling ""KeyProjection$109542"": Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-08-24T03:45:02.6156044Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:382)
2022-08-24T03:45:02.6157265Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
2022-08-24T03:45:02.6158569Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
2022-08-24T03:45:02.6159872Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
2022-08-24T03:45:02.6161097Z Aug 24 03:45:02 	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
2022-08-24T03:45:02.6162659Z Aug 24 03:45:02 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
2022-08-24T03:45:02.6163829Z Aug 24 03:45:02 	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
2022-08-24T03:45:02.6165126Z Aug 24 03:45:02 	at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
2022-08-24T03:45:02.6166156Z Aug 24 03:45:02 	... 19 more
2022-08-24T03:45:02.6168806Z Aug 24 03:45:02 Caused by: java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
2022-08-24T03:45:02.6171268Z Aug 24 03:45:02 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:184)
2022-08-24T03:45:02.6173345Z Aug 24 03:45:02 	at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.loadClass(FlinkUserCodeClassLoaders.java:192)
2022-08-24T03:45:02.6174695Z Aug 24 03:45:02 	at java.lang.Class.forName0(Native Method)
2022-08-24T03:45:02.6175663Z Aug 24 03:45:02 	at java.lang.Class.forName(Class.java:348)
2022-08-24T03:45:02.6176871Z Aug 24 03:45:02 	at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:89)
2022-08-24T03:45:02.6178209Z Aug 24 03:45:02 	at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:312)
2022-08-24T03:45:02.6179482Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:8556)
2022-08-24T03:45:02.6180766Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6601)
2022-08-24T03:45:02.6182178Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
2022-08-24T03:45:02.6183437Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
2022-08-24T03:45:02.6184758Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
2022-08-24T03:45:02.6186120Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
2022-08-24T03:45:02.6187399Z Aug 24 03:45:02 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
2022-08-24T03:45:02.6188610Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
2022-08-24T03:45:02.6189872Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
2022-08-24T03:45:02.6191091Z Aug 24 03:45:02 	at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
2022-08-24T03:45:02.6192407Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
2022-08-24T03:45:02.6193644Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215)
2022-08-24T03:45:02.6194938Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$34.getInterfaces2(UnitCompiler.java:10002)
2022-08-24T03:45:02.6196169Z Aug 24 03:45:02 	at org.codehaus.janino.IClass.getInterfaces(IClass.java:497)
2022-08-24T03:45:02.6197312Z Aug 24 03:45:02 	at org.codehaus.janino.IClass.getIMethods(IClass.java:263)
2022-08-24T03:45:02.6198461Z Aug 24 03:45:02 	at org.codehaus.janino.IClass.getIMethods(IClass.java:237)
2022-08-24T03:45:02.6199646Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:492)
2022-08-24T03:45:02.6200857Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
2022-08-24T03:45:02.6202211Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
2022-08-24T03:45:02.6203556Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
2022-08-24T03:45:02.6205035Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
2022-08-24T03:45:02.6206461Z Aug 24 03:45:02 	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
2022-08-24T03:45:02.6207873Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
2022-08-24T03:45:02.6209088Z Aug 24 03:45:02 	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
2022-08-24T03:45:02.6210038Z Aug 24 03:45:02 	... 26 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40327&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=21331",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29463,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 14:32:12 UTC 2022,,,,,,,,,,"0|z180ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 03:03;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40564&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","01/Sep/22 03:03;hxb;[~renqs] Any progress on this issue？;;;","06/Sep/22 09:04;smiralex;Hi [~hxb]. I made a PR that should fix this issue. I think it's connected either with concurrent creating of Project instances from generated code (maybe #newInstance method is not thread-safe) or with race condition during closing of LookupFullCache (currently #close method doesn't wait until the end of active reload, if it's happening). Both of these problems are fixed in the MR.;;;","07/Sep/22 01:52;hxb;Thanks [~smiralex] for the fix.;;;","19/Sep/22 14:02;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41125&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","21/Sep/22 09:40;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41202&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","23/Sep/22 14:32;renqs;Fixed on master: 340b100f2de5e0d90ba475aa8a00e359a61442ce 

release-1.16: c10a727990668b1a0d706f16e4f5220780422ed9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopPathBasedPartFileWriterTest.testWriteFile failed with AssertionError,FLINK-29092,13478347,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kurt.ding,hxb,hxb,24/Aug/22 07:24,29/Nov/22 12:15,04/Jun/24 20:41,29/Nov/22 12:15,1.15.2,1.16.0,1.17.0,,,,,1.15.4,1.16.1,1.17.0,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-24T02:00:01.1670618Z Aug 24 02:00:01 [ERROR] org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.testWriteFile  Time elapsed: 2.311 s  <<< FAILURE!
2022-08-24T02:00:01.1671250Z Aug 24 02:00:01 java.lang.AssertionError: 
2022-08-24T02:00:01.1671626Z Aug 24 02:00:01 
2022-08-24T02:00:01.1672001Z Aug 24 02:00:01 Expected size: 1 but was: 2 in:
2022-08-24T02:00:01.1673656Z Aug 24 02:00:01 [DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--02; isDirectory=true; modification_time=1661306400000; access_time=1661306400396; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false},
2022-08-24T02:00:01.1676131Z Aug 24 02:00:01     DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--01; isDirectory=true; modification_time=1661306400000; access_time=1661306400326; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}]
2022-08-24T02:00:01.1677339Z Aug 24 02:00:01 	at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.validateResult(HadoopPathBasedPartFileWriterTest.java:107)
2022-08-24T02:00:01.1678274Z Aug 24 02:00:01 	at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriterTest.testWriteFile(HadoopPathBasedPartFileWriterTest.java:97)
2022-08-24T02:00:01.1679017Z Aug 24 02:00:01 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-24T02:00:01.1679666Z Aug 24 02:00:01 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-24T02:00:01.1680369Z Aug 24 02:00:01 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-24T02:00:01.1681019Z Aug 24 02:00:01 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-24T02:00:01.1681666Z Aug 24 02:00:01 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-24T02:00:01.1682385Z Aug 24 02:00:01 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-24T02:00:01.1683094Z Aug 24 02:00:01 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-24T02:00:01.1683965Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-24T02:00:01.1684713Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-24T02:00:01.1685956Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2022-08-24T02:00:01.1687185Z Aug 24 02:00:01 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2022-08-24T02:00:01.1688347Z Aug 24 02:00:01 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-24T02:00:01.1689324Z Aug 24 02:00:01 	at java.lang.Thread.run(Thread.java:748)
2022-08-24T02:00:01.1690062Z Aug 24 02:00:01 
2022-08-24T02:00:03.4727706Z Aug 24 02:00:03 Formatting using clusterid: testClusterID
2022-08-24T02:00:07.9860626Z Aug 24 02:00:07 [INFO] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.712 s - in org.apache.flink.formats.hadoop.bulk.committer.HadoopRenameCommitterHDFSTest
2022-08-24T02:00:08.4139747Z Aug 24 02:00:08 [INFO] 
2022-08-24T02:00:08.4140678Z Aug 24 02:00:08 [INFO] Results:
2022-08-24T02:00:08.4141326Z Aug 24 02:00:08 [INFO] 
2022-08-24T02:00:08.4142008Z Aug 24 02:00:08 [ERROR] Failures: 
2022-08-24T02:00:08.4144242Z Aug 24 02:00:08 [ERROR]   HadoopPathBasedPartFileWriterTest.testWriteFile:97->validateResult:107 
2022-08-24T02:00:08.4145317Z Aug 24 02:00:08 Expected size: 1 but was: 2 in:
2022-08-24T02:00:08.4147711Z Aug 24 02:00:08 [DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--02; isDirectory=true; modification_time=1661306400000; access_time=1661306400396; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false},
2022-08-24T02:00:08.4150885Z Aug 24 02:00:08     DeprecatedRawLocalFileStatus{path=file:/tmp/junit3893779198554813459/junit1595046776902782406/2022-08-24--01; isDirectory=true; modification_time=1661306400000; access_time=1661306400326; owner=; group=; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}] {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40324&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17594,,,,FLINK-27185,,,,,,,,,,,"05/Sep/22 09:31;kurt.ding;image-2022-09-05-17-31-44-813.png;https://issues.apache.org/jira/secure/attachment/13048958/image-2022-09-05-17-31-44-813.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 29 12:15:34 UTC 2022,,,,,,,,,,"0|z180nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 07:26;hxb;[~slinkydeveloper] Could you help take a look? Thx.;;;","05/Sep/22 02:32;kurt.ding;Let me try , [~hxb] ;;;","05/Sep/22 09:31;kurt.ding;Ha,Ha，I found the cause . The date format string pattern is ""yyyy-MM-dd–HH"" ,so if the first part in  stream source  come in at  2022-08-24-01:xx:xx and the second part of stream source come in at 2022-08-24-02:xx:xx  . So the date time assigner will assign file into two different file . Here is  the proof in log trace .

!image-2022-09-05-17-31-44-813.png!;;;","05/Sep/22 09:47;kurt.ding;Pr is avaliable, [~hxb] 

 ;;;","05/Sep/22 09:51;hxb;[~kurt.ding] Good job. I have assigned it to you.;;;","08/Nov/22 06:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42896&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=12322;;;","29/Nov/22 12:15;mapohl;master: ed46cb2fd64f1cb306ae5b7654d2b4d64ab69f22
1.16: 73dfd61858b7762e7d979bb8b09051abc0d82734
1.15: 84ec72f2cade11251c263a1a367634521461d225;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the determinism declaration of the rand function to be consistent with current behavior,FLINK-29091,13478325,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,24/Aug/22 05:21,04/Sep/22 13:36,04/Jun/24 20:41,04/Sep/22 13:36,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"RAND and RAND_INTEGER are declared as dynamic function (isDynamicFuntion returns true), as the declaration it should only evaluate once at query-level (not per record) for batch mode, FLINK-21713 did the similar fix for temporal functions.

But current behavior is completely a non-deterministic function which evaluated per record for both batch and streaming mode, it's not a good choice to break current behavior,  and the determinism of RAND function are also different across vendors:

[1] evaluated at query-level though it is treated as non-deterministic function [https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-ver16#built-in-function-determinism|https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-ver16#built-in-function-determinism)]

[2][ evaluated at row level:  [https://dev.mysql.com/doc/refman/5.7/en/mathematical-functions.html#function_rand]|https://dev.mysql.com/doc/refman/5.7/en/mathematical-functions.html#function_rand)]

[3] evaluated at row level if not specifies a seed,  e.g., DBMS_RANDOM.normal, DBMS_RANDOM.value(1,10)  [https://docs.oracle.com/database/timesten-18.1/TTPLP/d_random.htm#TTPLP71231|https://docs.oracle.com/database/timesten-18.1/TTPLP/d_random.htm#TTPLP71231)]

So just fix the determinism declaration of the rand function to be consistent with the current behavior and make it clear in the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 04 13:36:57 UTC 2022,,,,,,,,,,"0|z180io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/22 13:36;godfrey;Fixed in master: 4b15bc900eb60b1830bc406975ce974ad6050f98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the code gen for ColumnarMapData and ColumnarArrayData,FLINK-29090,13478307,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,danny0405,danny0405,24/Aug/22 02:22,28/Sep/22 06:13,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,,"!image-2022-08-24-10-15-11-824.png|width=589,height=284!

Currently, the code generation for {{MapData}} assumes that it is the {{{}GenericMapData{}}}, but the new introduced {{ColumnarMapData}} and {{ColumnarArrayData}} can not be casted to {{{}GenericMapData{}}}.

{{ColumnarMapData}} and {{ColumnarArrayData}} are introduced in
FLINK-24614 [https://github.com/apache/flink/commit/5c731a37e1a8f71f9c9e813f6c741a1e203fa1a3]

How to reproduce:
{code:sql}
create table parquet_source (
  f_map map<varchar(20), int>
) with (
  'connector' = 'filesystem',
  'format' = 'parquet'
);

select f_map['k1'] from table parquet_source;

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/22 02:15;danny0405;image-2022-08-24-10-15-11-824.png;https://issues.apache.org/jira/secure/attachment/13048465/image-2022-08-24-10-15-11-824.png","08/Sep/22 13:47;lsy;image-2022-09-08-21-47-47-325.png;https://issues.apache.org/jira/secure/attachment/13049092/image-2022-09-08-21-47-47-325.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 08 13:44:38 UTC 2022,,,,,,,,,,"0|z180ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/22 06:57;lsy;Thanks for report, I will take a look.;;;","08/Sep/22 13:44;lsy;I didn't reproduce the exception through the following sql case in master branch:

```java
@Test
publicvoidtestMap() throws Exception

{ super.tableEnv() .executeSql( String.format( ""create table parquet_source (\n"" + ""f1 int,\n"" + "" f_map map<varchar(20), int>\n"" + "") with (\n"" + "" 'connector' = 'filesystem',\n"" + "" 'path' = '%s',\n"" + "" 'format' = 'parquet'\n"" + "");"", super.resultPath())); super.tableEnv().executeSql(""insert into parquet_source select 1, map['k1', 1]"").await(); List<Row> results = CollectionUtil.iteratorToList( super.tableEnv() .executeSql(""select f1, f_map['k1'] from parquet_source"") .collect()); assertThat(results.size()).isEqualTo(1); }

```

In the codegen code, the ColumnarMapData will be convert to BinaryMapData before get it, the codegen related code as following:

!image-2022-09-08-21-47-47-325.png!

 

Can you give me more context about reproducing the bug?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when run test case in Windows,FLINK-29089,13478248,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hjw,hjw,hjw,23/Aug/22 15:50,24/Aug/22 08:10,04/Jun/24 20:41,,1.15.1,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"When I run mvn clean install ,It will run Flink test case .
However , I get Error：
[ERROR] Failures:
[ERROR]   KubernetesClusterDescriptorTest.testDeployApplicationClusterWithNonLocalSchema:155 Previous method call should have failed but it returned: org.apache.flink.kubernetes.KubernetesClusterDescriptor$$Lambda$839/1619964974@70e5737f
[ERROR]   AbstractKubernetesParametersTest.testGetLocalHadoopConfigurationDirectoryFromHadoop1HomeEnv:132->runTestWithEmptyEnv:149->lambda$testGetLocalHadoopConfigurationDirectoryFromHadoop1HomeEnv$3:141
Expected: is ""C:\Users\10104\AppData\Local\Temp\junit5662202040601670287/conf""
     but: was ""C:\Users\10104\AppData\Local\Temp\junit5662202040601670287\conf""
[ERROR]   AbstractKubernetesParametersTest.testGetLocalHadoopConfigurationDirectoryFromHadoop2HomeEnv:117->runTestWithEmptyEnv:149->lambda$testGetLocalHadoopConfigurationDirectoryFromHadoop2HomeEnv$2:126
Expected: is ""C:\Users\10104\AppData\Local\Temp\junit7094401822178578683/etc/hadoop""
     but: was ""C:\Users\10104\AppData\Local\Temp\junit7094401822178578683\etc\hadoop""
[ERROR]   KubernetesUtilsTest.testLoadPodFromTemplateWithNonExistPathShouldFail:110
Expected: Expected error message is ""Pod template file /path/of/non-exist.yaml does not exist.""
     but: The throwable <org.apache.flink.util.FlinkRuntimeException: Pod template file \path\of\non-exist.yaml does not exist.> does not contain the expected error message ""Pod template file /path/of/non-exist.yaml does not exist.""
 
I judge the error occurred due to different fileSysyem(unix,Windows..etc) separators.","deploy env: Windows10

flink version:1.15

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 15:52:21 UTC 2022,,,,,,,,,,"0|z1801s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 15:52;hjw;[~wangyang0918] I'd like to file a PR to fix it. Please help assign the ticket to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project push down cause the source reuse can not work,FLINK-29088,13478242,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,aitozi,aitozi,aitozi,23/Aug/22 15:22,03/Oct/23 08:42,04/Jun/24 20:41,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,stale-assigned,,,"It can be reproduce by 

{code:java}
    util.addTable(
      s""""""
         |create table newX(
         |  a int,
         |  b bigint,
         |  c varchar
         |) with (
         |  'connector' = 'values'
         |  ,'enable-projection-push-down' = 'true'
         |)
       """""".stripMargin)
    val sqlQuery =
      """"""
        | SELECT b from newX WHERE a > 10
        | UNION ALL
        | SELECT b from newX WHERE b > 10
      """""".stripMargin
    util.verifyExecPlan(sqlQuery)
{code}

if 'enable-projection-push-down' set to true, the source will not be reused. If set to false, the source will be reused. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 10:35:10 UTC 2023,,,,,,,,,,"0|z1800g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 11:50;aitozi;I think we can push the union project of both project to source. By this, we can save the bandwidth of the source scan for this case. what do you think [~godfreyhe] [~twalthr] ? ;;;","26/Aug/22 00:32;aitozi;I propose to support this level reuse by this way:

1) Detect the same TableScan with different fields but to the same table.
2) Create a new TableScan with the common fields to replace both, so that they will have the same digest
3) Create extra project to the TableScan if the scan fields changed

I would like give a try on this, also looking forward to your ideas [~godfreyhe] [~twalthr];;;","26/Aug/22 08:22;twalthr;Thanks for identifying the problem [~aitozi]. Reusing sources as much as possible is generally a good idea. I guess the implementation can be a nicely separable logical rule. +1 to your proposal.;;;","26/Aug/22 09:01;aitozi;[~twalthr] Thanks for your inputs. 

> I guess the implementation can be a nicely separable logical rule

I'm not sure whether we can detect a same source node in the Logical rule, My current solution is working around the {{SubplanReuser}} to find the similar source except the project list. And then recreate a new source with the union fields of all the projects. 

Do you think it's a reasonable solution?;;;","29/Aug/22 09:43;aitozi;After some research, here is some progress:

When using the logical rule to detect the same source node, due to the calcite can only optimize the single root, so it can only reuse the source node in the same tree. The same source node in different {{RelRoot}} will be skipped.

When trying to detect the same source after the physical optimize. It becomes much complicated because there also need to deal with the output type of the source table (which may be also affects the PushDownWaterMark and so on)

So I lean to follow your suggestion to add a logical rule to optimize in the single tree first. I have pass the poc verify and will prepare a PR for it ASAP.



;;;","10/Sep/22 15:08;aitozi;Hi [~twalthr] , [~godfreyhe]  I have opened a PR to solve this. I'm not sure whether it is the most suitable solution to it now, but it works in our production now. Looking forward to your valuable comments on it. 


Besides, during work on this PR I also notice that other push down behavior to the source may also affect the source reuse, I'm interested to explore how to solve most of them after this issue addressed.;;;","30/Sep/22 03:43;aitozi;Hi, [~twalthr] looking forward to your feedback, thanks :);;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jdbc connector sql ITCase failed when run in idea,FLINK-29087,13478219,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xuyangzhong,lsy,lsy,23/Aug/22 12:53,31/Aug/22 01:44,04/Jun/24 20:41,31/Aug/22 01:44,1.16.0,,,,,,,1.16.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,,"java.lang.NoSuchFieldError: CORRELATE

    at org.apache.flink.table.planner.hint.FlinkHintStrategies.createHintStrategyTable(FlinkHintStrategies.java:91)
    at org.apache.flink.table.planner.delegation.PlannerContext.lambda$getSqlToRelConverterConfig$1(PlannerContext.java:288)
    at java.util.Optional.orElseGet(Optional.java:267)
    at org.apache.flink.table.planner.delegation.PlannerContext.getSqlToRelConverterConfig(PlannerContext.java:283)
    at org.apache.flink.table.planner.delegation.PlannerContext.createFrameworkConfig(PlannerContext.java:146)
    at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:124)
    at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:121)
    at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:65)
    at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.create(TableEnvironmentImpl.java:308)
    at org.apache.flink.table.api.TableEnvironment.create(TableEnvironment.java:93)
    at org.apache.flink.connector.jdbc.catalog.MySqlCatalogITCase.setup(MySqlCatalogITCase.java:159)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
    at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 01:44:56 UTC 2022,,,,,,,,,,"0|z17zvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 12:54;lsy;cc [~xuyangzhong] ;;;","23/Aug/22 12:58;xuyangzhong;I'll try to fix it.;;;","31/Aug/22 01:44;leonard;Fixed in master(1.16): d55be6850dc2a4e0291c0a4853fa5aa7a51a1d10;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the Helm chart's Pod env reference,FLINK-29086,13478209,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,23/Aug/22 12:15,26/Aug/22 08:01,04/Jun/24 20:41,26/Aug/22 08:01,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,We need to add a `quote` pipeline to the env params reference.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/354,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 08:01:14 UTC 2022,,,,,,,,,,"0|z17zt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 08:01;gyfora;merged to main 849c52897f3bd2a2212591e3fa6103a9bf8a7afc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the name for test as hint for the current test case in BuiltInFunctionTestBase,FLINK-29085,13478204,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,aitozi,aitozi,23/Aug/22 11:52,18/Dec/22 07:20,04/Jun/24 20:41,18/Dec/22 07:20,,,,,,,,,,,,Tests,,,,0,,,,,"when running tests extends the {{BuiltInFunctionTestBase}}, I found it's hard to distinguish the failure tests, I think it will be easy to add the name prefix for the {{TestItem}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 11:54:12 UTC 2022,,,,,,,,,,"0|z17zs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 11:54;aitozi;cc [~twalthr] [~slinkydeveloper] do you think it's reasonable ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Program argument containing # (pound sign) mistakenly truncated in Kubernetes mode,FLINK-29084,13478197,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,kyledong,kyledong,23/Aug/22 11:23,29/Dec/22 08:14,04/Jun/24 20:41,29/Dec/22 08:14,1.13.6,1.14.5,1.15.1,,,,,,,,,Deployment / Kubernetes,,,,0,,,,,"We have found that when submitting jobs in native-Kubernetes mode, the main arguments of the Flink program would be truncated if it contains a # character.

For example, if we pass 'ab#cd' as the argument for Flink programs, Flink actually gets only 'ab' from the variable `$internal.application.program-args` at runtime.

After searching into the code, we found the reason might be that when `org.apache.flink.kubernetes.kubeclient.decorators.FlinkConfMountDecorator#buildAccompanyingKubernetesResources` transform Flink config data `Map` into `ConfigMap`, fabric8 Kubernetes client converts it to YAML internally, without any escaping procedures. Afterwards, when there is a # character in the YAML line, the decoder treats it as the start of a comment, thus the substring after the # character is ignored erroneously.","Flink 1.13.6

Native Kubernetes (Application Mode)",,,,,,,,,,,,,,,,,,,,FLINK-15358,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 08:14:55 UTC 2022,,,,,,,,,,"0|z17zqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 11:38;kyledong;One possible solution is to base64-encode the program arguments and decode them later, but we may need to change _flink-clients_ module as well, like _org.apache.flink.client.deployment.application.ApplicationConfiguration#applyToConfiguration_ and {_}org.apache.flink.client.deployment.application.ApplicationConfiguration#fromConfiguration{_}.

Hi [~wangyang0918] do you have time to have a look at this issue? Thanks : );;;","24/Aug/22 08:13;wangyang0918;This is the same issue with FLINK-15358. Right?;;;","24/Aug/22 11:49;kyledong;Oh yes, but that issue is still left unresolved. So probably we need to address this issue to prevent further potential ""bugs"" like this :D;;;","29/Dec/22 08:14;xtsong;Closing as duplicating FLINK-15358;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to split String to Array<String> with scalar functions,FLINK-29083,13478186,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zaobao,zaobao,23/Aug/22 09:53,29/Aug/22 14:06,04/Jun/24 20:41,29/Aug/22 14:06,,,,,,,,,,,,,,,,0,,,,,"as hive

split('a,b,c,d',',') -> [""a"",""b"",""c"",""d""]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 14:06:54 UTC 2022,,,,,,,,,,"0|z17zo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 14:06;martijnvisser;Please ask these type of questions to the Flink community directly instead of opening a Jira ticket. See https://flink.apache.org/community.html for more information;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean-up Leftovers for changelog pre-uploading files after failover,FLINK-29082,13478180,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ym,ym,23/Aug/22 09:31,23/Aug/22 09:55,04/Jun/24 20:41,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,,"Since ChangelogRegistry is not persisted, registration information is lost after failover.

It may lead to changelog leftovers.

We can use a similar way proposed in FLINK-24852  for this.

Or other ways like TM-owned/managed files.",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25511,,,,FLINK-24852,FLINK-25842,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 09:31:59.0,,,,,,,,,,"0|z17zmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join Hint cannot be identified by lowercase,FLINK-29081,13478168,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,23/Aug/22 08:43,25/Aug/22 09:03,04/Jun/24 20:41,25/Aug/22 09:03,1.16.0,,,,,,,1.16.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"The following sql can reproduce this bug:

select /*+ bRoadCasT(t1) */* from t1 join t1 as t3 on t1.a = t3.a;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 09:03:18 UTC 2022,,,,,,,,,,"0|z17zk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 09:03;godfrey;Fixed in master: fcaa4f77e0b3253ea902fbfad0bc1b2046ff814d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate all tests from managed table to catalog-based tests,FLINK-29080,13478162,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,qingyue,qingyue,23/Aug/22 08:25,19/Mar/23 05:47,04/Jun/24 20:41,19/Mar/23 05:47,table-store-0.3.0,,,,,,,table-store-0.4.0,,,,Table Store,,,,0,,,,,To get rid of ManagedTableFactory and enable test on -Pflink-1.14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 08:25:07.0,,,,,,,,,,"0|z17zio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for show statement of Hive dialect,FLINK-29079,13478155,13477344,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,23/Aug/22 08:04,20/Sep/22 08:04,04/Jun/24 20:41,20/Sep/22 08:04,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,,,,0,,,,,"Add a page of show statment for HiveDialect. As our Hive dialect is compatible to Hive, so we can take some from Hive docs",,,,,,,,,,,,,,,FLINK-29025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 08:04:37.0,,,,,,,,,,"0|z17zh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for drop statement of Hive dialect,FLINK-29078,13478154,13477344,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,23/Aug/22 08:03,20/Sep/22 08:04,04/Jun/24 20:41,20/Sep/22 08:04,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Documentation,,,0,,,,,"Add a page of drop statment for HiveDialect. As our Hive dialect is compatible to Hive, so we can take some from Hive docs",,,,,,,,,,,,,,,FLINK-29025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 08:03:22.0,,,,,,,,,,"0|z17zgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for create statement of Hive dialect,FLINK-29077,13478153,13477344,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,23/Aug/22 08:02,20/Sep/22 08:03,04/Jun/24 20:41,20/Sep/22 08:03,,,,,,,,1.16.0,,,,Connectors / Hive,,,,0,,,,,"Add a page of create statment for HiveDialect. As our Hive dialect is compatible to Hive, so we can take some from Hive docs",,,,,,,,,,,,,,,FLINK-29025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 08:02:07.0,,,,,,,,,,"0|z17zgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for alter statement of Hive dialect,FLINK-29076,13478152,13477344,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,23/Aug/22 07:58,20/Sep/22 08:04,04/Jun/24 20:41,20/Sep/22 08:04,1.16.0,,,,,,,1.16.0,,,,Documentation,,,,0,,,,,"Add a page of alter statment for HiveDialect. As our Hive dialect is compatible to Hive, so we can take some from [Hive docs|#LanguageManualDDL]]",,,,,,,,,,,,,,,FLINK-29025,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-23 07:58:31.0,,,,,,,,,,"0|z17zgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RescaleBucketITCase#testSuspendAndRecoverAfterRescaleOverwrite is not stable,FLINK-29075,13478148,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,qingyue,qingyue,23/Aug/22 07:35,27/Oct/22 07:35,04/Jun/24 20:41,27/Oct/22 03:55,table-store-0.2.0,,,,,,,table-store-0.3.0,,,,Table Store,,,,0,pull-request-available,,,,"[https://github.com/apache/flink-table-store/runs/7964774584?check_suite_focus=true]

!image-2022-08-23-15-35-59-499.png|width=576,height=370!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/22 07:36;qingyue;image-2022-08-23-15-35-59-499.png;https://issues.apache.org/jira/secure/attachment/13048431/image-2022-08-23-15-35-59-499.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 27 07:35:07 UTC 2022,,,,,,,,,,"0|z17zfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 03:55;lzljs3620320;master: f3b1e813684194fa50d57663fea2f06d747f5f96;;;","27/Oct/22 07:35;TsReaper;release-0.2: 8377a51c76509dcdfbaa3bf28e806b50b2459814;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"use 'add jar' in sql client throws ""Could not find any jdbc dialect factories that implement""",FLINK-29074,13478131,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,xuyangzhong,xuyangzhong,23/Aug/22 06:03,07/Sep/22 02:35,04/Jun/24 20:41,07/Sep/22 02:35,1.16.0,,,,,,,1.16.0,1.17.0,,,Table SQL / Planner,,,,0,pull-request-available,,,,"The following step can reproduce this bug:

1、 create a source table 't1' in sql-client using jdbc(mysql)

2、add a jar with jdbc connector

3、select * from 't1'

then an exception throws:

java.lang.IllegalStateException: Could not find any jdbc dialect factories that implement 'org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory' in the classpath.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 02:35:18 UTC 2022,,,,,,,,,,"0|z17zbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 06:22;lsy;[~xuyangzhong] Thanks for report, I will take a look;;;","07/Sep/22 02:35;jark;Fixed in 
 - master: 523546101f0180999f11d68269aad53c59134064 to 481ed78bec4211561e78be7586a102bd37a4dfb1
 - release-1.16: 3760a370ed7e4d92ff1cb14035bcc6abb00e02f8 to 38088230bb57486f81bb089a96aa3fa1e3f414f7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-91] Support SQL Gateway(Part 2),FLINK-29073,13478114,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,23/Aug/22 03:28,24/Nov/22 07:13,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,Table SQL / Client,Table SQL / Gateway,,,0,,,,,Issue continues improving the SQL Gateway and allows the SQL Client submit jobs to the SQL Gateway.,,,,,,,,,,,,,,,,,,,,,,,FLINK-15472,FLINK-29941,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 24 07:13:49 UTC 2022,,,,,,,,,,"0|z17z80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/22 19:09;eric.xiao;Hi there, are any of the tasks above good for someone new to the community to start working on?;;;","23/Nov/22 03:28;yzl;[~eric.xiao] Hi, it's very nice that you can provide help. Recently, I'm working on the SQL Gateway and Client. Because I will change some public API in SQL gateway, so I am writing a FLIP now and it is near completion. I think we can have some talk after my proposal is accepted. 

Moreover, I have PRs to the subtask 6 & 7, if you are interested in, maybe you can take a look. And the subtask 5 (add rest api for 7) is still open, it would be nice if you can have a look at this ticket. The FLINK-15472 has more information about the SQL Gateway and rest api related changes.;;;","23/Nov/22 19:50;eric.xiao;Thanks [~yzl]! Yes let's connect after the FLIP is accepted as well :).

Yes my team and I are interested in working on #5, is there some sort of timeline that the community had in mind to get these issues closed by?

I will take a look at your PRs to see what is needed to expose the `completeStatement` to the REST API.;;;","24/Nov/22 07:13;yzl;Hi, [~eric.xiao] , I estimate that I will finish the work of client in mid-December. I hope that the features of gateway can be ready at that time because they are the basis of client.

Anyway, feel free to talk to me if you have any problem. You can find me through this ticket or yuzelin.yzl@gmail.com.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate DDL-related schema handling to the new Schema framework,FLINK-29072,13478113,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qingyue,qingyue,23/Aug/22 03:27,21/Nov/22 03:21,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,Table SQL / API,,,,0,,,,,"[FLIP-164|https://cwiki.apache.org/confluence/display/FLINK/FLIP-164%3A+Improve+Schema+Handling+in+Catalogs] introduces the new Schema framework. Yet all DDL-related functionalities (such as CREATE TABLE, CREATE TABLE LIKE, ALTER TABLE SET/RESET, ALTER TABLE ADD/DROP CONSTRAINT, SHOW CREATE TABLE, etc.) are based on the deprecated CatalogTableImpl and TableSchema.

The FLIP said, ""For backward compatibility, we leave Catalog#createTable and Catalog#alterTable untouched."" Therefore, issues like FLINK-18958, FLINK-28690, etc., cannot be resolved. And new functionalities like ALTER TABLE ADD/MODIFY face a dilemma of implementing against the new framework but lose backward consistency.

Fully migrating to the new framework takes a lot of effort and maybe a long-term plan, but at least it's the time to put it on the agenda to have a track.",,,,,,,,,,,,FLINK-21634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 21 02:55:03 UTC 2022,,,,,,,,,,"0|z17z7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 03:23;luoyuxia;Notice the deprecated TableSchema spreads in many modules , would it be better to create some subtasks to track so that it'll be clear that what's should be done in each subtask?

Btw, I have created a FLINK-29585 for migrate the TableSchema to new Schema for Hive connector.

 ;;;","21/Nov/22 02:55;liyubin117;Hi, [~qingyue] [~luoyuxia], I have implemented the feature in https://issues.apache.org/jira/browse/FLINK-29679, migrate created table to new schema framework,  so we can make all flink tables including hive connector to have new version schema, would bring a lot of benefits like get column comments, now ci has passed, would you please give a review?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Table Store Hive CDH support,FLINK-29071,13478110,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,23/Aug/22 03:08,23/Aug/22 06:06,04/Jun/24 20:41,23/Aug/22 06:06,table-store-0.2.0,table-store-0.3.0,,,,,,table-store-0.2.0,,,,Table Store,,,,0,pull-request-available,,,,Currently Table Store Hive catalog and connectors cannot run against Hive 2.1 CDH 6.3. We should fix this support.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 06:06:55 UTC 2022,,,,,,,,,,"0|z17z74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 06:06;lzljs3620320;master: fe57dfa704117ff9f14dcf39bdb1dcba6e826972
release-0.2: aa85abe38963796904c77dd5062ee84d569bf9a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a option to force the removal of the normalize node when streaming read,FLINK-29070,13478107,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,23/Aug/22 02:40,23/Aug/22 03:12,04/Jun/24 20:41,23/Aug/22 03:12,,,,,,,,table-store-0.2.0,,,,Table Store,,,,0,pull-request-available,,,,"There are many jobs that do not require a downstream normalize node, and the key is that the node has a very large cost.

Introduce a option:
Whether to force the removal of the normalize node when streaming read. Note: This is dangerous and is likely to cause data errors if downstream is used to calculate aggregation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 03:12:41 UTC 2022,,,,,,,,,,"0|z17z6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 03:12;lzljs3620320;master: b4f187f370d3e47c436bc9c0aafa5f6e381e6387
release-0.2: fa11e8748c763fcc897f0b688c9d2e6d7c1334a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-218 Create table as select syntax,FLINK-29069,13478098,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Resolved,renqs,lsy,lsy,23/Aug/22 01:51,07/Sep/22 08:13,04/Jun/24 20:41,07/Sep/22 08:13,1.16.0,,,,,,,1.16.0,,,,Table SQL / API,,,,0,release-testing,,,,"This issue aims to verify FLIP-218: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=199541185

We can verify it in SQL client after we build the flink-dist package.
 # Creating a source table firstly
 # Creating a target table using the source table
 # Verify the result when the query is executed successfully/failed/canceled.
 # Verify the ctas query in streaming and batch mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 07 07:40:53 UTC 2022,,,,,,,,,,"0|z17z4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 02:10;hxb;Hi [~renqs] any progress on the issue?;;;","07/Sep/22 07:40;renqs;# Used CTAS syntax to create a Kafka sink table from Kafka source table in SQL client. The job was submitted and run successfully, but SQL client got stuck without emitting new prompts and had to use SIGTERM to kill the client. From the log of SQL client it looked like SQL client was waiting for the job to finish.
 # Used CTAS with hive catalog to create Kafka sink table from Kafka source table in SQL client. The job was submitted and run successfully. Still SQL client encountered the issue above. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-214 Create function using jar syntax,FLINK-29068,13478097,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Resolved,renqs,lsy,lsy,23/Aug/22 01:48,06/Sep/22 09:23,04/Jun/24 20:41,06/Sep/22 09:23,1.16.0,,,,,,,1.16.0,,,,Table SQL / API,,,,0,release-testing,,,,"This issue aims to verify FLIP-214: [https://cwiki.apache.org/confluence/display/FLINK/FLIP-214+Support+Advanced+Function+DDL]

We can verify it in SQL client after we build the flink-dist package.

*verify create function using jar syntax*
 # Preparing a jar that contains the udf class implementation, the jar path should be a local or remote path such as hdfs/s3/oss.
 # Creating a catalog function/temporary catalog function/temporary system function using the jar provided jar path.
 # Using `show jars` to verify whether the function has been created successfully. 
 # Write a query which refer to the above udf, and then execute it. Verify the query execution and result correctness.

 

*verify add jar syntax*

We have ported the implementation of add jar/show jars from SQL client to TableEnvironment in FLIP-214, and the customed jar class load using user classloader instead of thread context classloader now, so we also should verify the add jar syntax.
 # Preparing a jar that contains the udf or catalog implementation, the jar path should be a local or remote path such as hdfs/s3/oss.
 # Using add jar syntax by the provided jar resource.
 # Creating a catalog(such as jdbc/hive) using the added jar and execute other ddls within the catalog
 # Creating a udf using the added jar and write a query referring to the udf, then submit it.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 08:15:43 UTC 2022,,,,,,,,,,"0|z17z48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 02:10;hxb;Hi [~renqs] , any progress on the issue?;;;","30/Aug/22 02:14;renqs;[~hxb] I'll start testing this feature today. Thanks for the reminder!;;;","06/Sep/22 08:15;renqs;CREATE FUNCTION USING JAR: 
 # Prepared a JAR with UDF implementation
 # Verified creating an UDF with JAR in local filesystem
 # Verified `SHOW JARS`
 # Verified executing query with the function created above

ADD JAR:
 # Using Kafka connector JAR for validation, including:
 ## Kafka SQL connector uber JAR
 ## Kafka connector JAR + Kafka clients JAR
 # Verified `ADD JAR`
 # Verified creating Kafka source and sink table
 # Verified executing query with Kafka source to Kafka sink, including using UDTF created by `USING JAR` syntax and custom sink partitioner added by `ADD JAR` syntax. 
 # Verified using multiple UDFs in the query;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace deprecated Calcite's SqlParser#configBuilder with SqlParser#config,FLINK-29067,13478088,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/22 22:21,09/Sep/22 13:13,04/Jun/24 20:41,09/Sep/22 13:13,1.17.0,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,{{SqlParser#configBuilder}} is deprecated and probably will be removed in future versions. It is better to replace it with SqlParser#config,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 13:13:50 UTC 2022,,,,,,,,,,"0|z17z2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 13:13;mapohl;master: 3a91ded289105684e107dfaf462159974d89456c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reconsider the runtime property of the BuiltInFunctionDefinition,FLINK-29066,13478053,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,aitozi,aitozi,22/Aug/22 16:18,18/Dec/22 07:20,04/Jun/24 20:41,18/Dec/22 07:20,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"I found a bit confused when implementing the inner built in functions when dealing with the runtime property. Currently, it has three types of the runtime property:
1) runtimeclass which means flink provide a class to define the runtime implementation
2) runtimeProvider which means the runtime class is code generated 
3) runtimeDefered which means it will use the calcite's sql operator to mapping the codegen

After some research, I found that we have 4 situations to deal:
1) non new stack operators.
2) new stack with own runtime class provided. eg: {{IFNULL}} -> runtimeClass
3) new stack translate to sql operator to provide runtime call gen.  eg:{{IS_NOT_TRUE}} -> runtimeDefered
4) new stack can not mapping to calcite's operator (mainly flink internal functions) without runtime class need mapping to the runtime callgen. eg: {{CURRENT_WATERMARK}}. -> runtimeProvided

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-22 16:18:38.0,,,,,,,,,,"0|z17yv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink v1.15.1 contains netty(version:3.10.6). There are many vulnerabilities, like CVE-2021-21409 etc. please confirm these version and fix. thx",FLINK-29065,13478049,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liuhb86,liuhb86,22/Aug/22 16:05,21/Aug/23 10:35,04/Jun/24 20:41,,1.11.3,1.12.2,1.13.0,1.15.1,,,,,,,,Runtime / Coordination,,,,0,auto-deprioritized-major,auto-deprioritized-minor,,,"Though FLINK-22441 states it's fixed, we can still see Netty 3.10.6 is used in the latest version: [https://github.com/apache/flink/blob/master/flink-rpc/flink-rpc-akka/pom.xml#L102] and it show up in the security scan results:

 
|Netty Project|3.10.6.Final|BDSA-2018-4022|MEDIUM|4.7|
|Netty Project|3.10.6.Final|BDSA-2019-2642|MEDIUM|6.5|
|Netty Project|3.10.6.Final|BDSA-2019-2643|MEDIUM|6.7|
|Netty Project|3.10.6.Final|BDSA-2019-2649|MEDIUM|6.5|
|Netty Project|3.10.6.Final|BDSA-2019-2610|HIGH|7.2|
|Netty Project|3.10.6.Final|CVE-2019-16869 (BDSA-2019-3119)|HIGH|7.5|
|Netty Project|3.10.6.Final|BDSA-2020-0130|HIGH|8.8|
|Netty Project|3.10.6.Final|CVE-2019-20444 (BDSA-2019-4231)|CRITICAL|9.1|
|Netty Project|3.10.6.Final|CVE-2019-20445 (BDSA-2019-4230)|CRITICAL|9.1|
|Netty Project|3.10.6.Final|BDSA-2020-0666|MEDIUM|6.5|
|Netty Project|3.10.6.Final|CVE-2021-21290 (BDSA-2021-0311)|MEDIUM|5.5|
|Netty Project|3.10.6.Final|CVE-2021-21295 (BDSA-2021-0589)|MEDIUM|5.9|
|Netty Project|3.10.6.Final|CVE-2021-21409 (BDSA-2021-0828)|MEDIUM|5.9|
|Netty Project|3.10.6.Final|CVE-2021-37136|HIGH|7.5|
|Netty Project|3.10.6.Final|CVE-2021-37137|HIGH|7.5|
|Netty Project|3.10.6.Final|CVE-2021-43797 (BDSA-2021-3741)|MEDIUM|6.5|
|Netty Project|3.10.6.Final|CVE-2022-24823|MEDIUM|5.5|",,,,,,,,,,,,,,FLINK-22441,FLINK-28372,,,,,,FLINK-31217,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:26 UTC 2023,,,,,,,,,,"0|z17yug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 08:06;martijnvisser;This can't be fixed since Akka doesn't support Netty 4 and there's no newer version of Netty 3. This can only be resolved if FLINK-28372 is possible and if so, has been completed. ;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ClassCastException thrown while reporting influxdb metrics,FLINK-29064,13478013,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,evgenyfish,evgenyfish,22/Aug/22 13:49,22/Aug/22 15:51,04/Jun/24 20:41,22/Aug/22 15:51,1.15.0,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"We report our own metrics to InfluxDb using Flink metric reporter extension: [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/metric_reporters/] , everything is working properly, we also see flink and kafka metrics.

But if we use kafkasink ( as a side output ) we get the following ClassCastException(s) for some kafka metrics:

13:40:30,568 WARN org.apache.flink.runtime.metrics.MetricRegistryImpl [] - Error while reporting metrics
java.lang.ClassCastException: class java.lang.Long cannot be cast to class java.lang.Double *(java.lang.Long and java.lang.Double* are in module java.base of loader 'bootstrap')
at org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.getValue(KafkaMetricMutableWrapper.java:37) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
at org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.getValue(KafkaMetricMutableWrapper.java:27) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
at org.apache.flink.metrics.influxdb.MetricMapper.map(MetricMapper.java:36) ~[flink-metrics-influxdb-1.15.0.jar:1.15.0]
at org.apache.flink.metrics.influxdb.InfluxdbReporter.buildReport(InfluxdbReporter.java:141) ~[flink-metrics-influxdb-1.15.0.jar:1.15.0]
at org.apache.flink.metrics.influxdb.InfluxdbReporter.report(InfluxdbReporter.java:127) ~[flink-metrics-influxdb-1.15.0.jar:1.15.0]
at org.apache.flink.runtime.metrics.MetricRegistryImpl$ReporterTask.run(MetricRegistryImpl.java:495) [flink-runtime-1.15.0.jar:1.15.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]

 

13:06:56,838 WARN org.apache.flink.runtime.metrics.MetricRegistryImpl [] - Error while reporting metrics
java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Double *(java.lang.String and java.lang.Double* are in module java.base of loader 'bootstrap')
at org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.getValue(KafkaMetricMutableWrapper.java:37) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
at org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricMutableWrapper.getValue(KafkaMetricMutableWrapper.java:27) ~[flink-connector-kafka-1.15.0.jar:1.15.0]
at org.apache.flink.metrics.influxdb.MetricMapper.map(MetricMapper.java:36) ~[flink-metrics-influxdb-1.15.0.jar:1.15.0]
at org.apache.flink.metrics.influxdb.InfluxdbReporter.buildReport(InfluxdbReporter.java:141) ~[flink-metrics-influxdb-1.15.0.jar:1.15.0]
at org.apache.flink.metrics.influxdb.InfluxdbReporter.report(InfluxdbReporter.java:127) ~[flink-metrics-influxdb-1.15.0.jar:1.15.0]
at org.apache.flink.runtime.metrics.MetricRegistryImpl$ReporterTask.run(MetricRegistryImpl.java:495) [flink-runtime-1.15.0.jar:1.15.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) [?:?]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) [?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
at java.lang.Thread.run(Thread.java:834) [?:?]

 

 ","Java 11.0.15

OS : Ubuntu 20 or Mac OS 

 ",,,,,,,,,,,,,,,,,,,,FLINK-27487,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-22 13:49:50.0,,,,,,,,,,"0|z17ymo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileFormatTest has some wrong assertion,FLINK-29063,13478003,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,klistopad,klistopad,klistopad,22/Aug/22 12:55,23/Aug/22 08:52,04/Jun/24 20:41,23/Aug/22 08:52,table-store-0.2.0,,,,,,,table-store-0.2.0,,,,Table Store,,,,0,pull-request-available,,,,org.apache.flink.table.store.file.FileFormatTest#testWriteRead reads the same value from file twice.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 08:52:08 UTC 2022,,,,,,,,,,"0|z17ykg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 08:52;lzljs3620320;master: 3e4445611c97fde9b796f5be46385f945998f1c3
release-0.2: 287fe32f652780f116b7c1084f8b362ed56cfacb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protobuf-plugin should download protoc via maven on flink-protobuf module,FLINK-29062,13478000,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,22/Aug/22 12:34,29/Aug/22 14:11,04/Jun/24 20:41,29/Aug/22 14:11,1.15.1,,,,,,,1.16.0,,,,Build System,,,,0,pull-request-available,,,,"[ERROR] Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.11.4:run (default) on project flink-protobuf: Error extracting protoc for version 3.21.2: Unsupported platform: protoc-3.21.2-osx-x86_64.exe -> [Help 1]

This issue is similar to FLINK-23661 but on the different module.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/os72/protoc-jar-maven-plugin/issues/68#issuecomment-455424245,,,,,,,,,,9223372036854775807,,,,Mon Aug 29 14:11:47 UTC 2022,,,,,,,,,,"0|z17yjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 14:11;martijnvisser;Fixed in master: 7ac37c08918bc76ff67f196e6ef46b53027957b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup dead code in StringCallGen,FLINK-29061,13477991,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,22/Aug/22 11:23,21/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,Table SQL / Planner,,,,0,auto-deprioritized-major,pull-request-available,,,"In the https://issues.apache.org/jira/browse/FLINK-13522, we dropped some function in the blink planner. There are still some dead codes for it now. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:27 UTC 2023,,,,,,,,,,"0|z17yhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not set rewriteBatchedStatements parameter for JDBC catalog for MySQL,FLINK-29060,13477985,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,macdoor615,macdoor615,22/Aug/22 11:00,11/Mar/24 12:43,04/Jun/24 20:41,,1.15.1,,,,,,,1.20.0,,,,Connectors / JDBC,,,,0,,,,,"execute following code in sql-client.sh
{code:java}
CREATE CATALOG mysql_bnpmp3 WITH(
 'type' = 'jdbc',
 'default-database' = 'gem_tmp',
 'username' = 'bnpmp',
 'password' = '*********',
 'base-url' = 'jdbc:mysql://hb3-prod-mysql:32759?rewriteBatchedStatements=true'
);{code}
output an error
{code:java}
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant com.mysql.cj.conf.BooleanPropertyDefinition.AllowableValues.""TRUE""/GEM_TMP{code}
rewriteBatchedStatements is a very import parameter, without it, data insertion performance will be very poor. 


I found a trick to overcome this problem. I changed jdbcUrl format and it works.
{code:java}
CREATE CATALOG mysql_bnpmp WITH(
    'type' = 'jdbc',
    'default-database' = 'gem_tmp',
    'username' = 'bnpmp',
    'password' = '*******',
    'base-url' = 'jdbc:mysql://(host=hb3-prod-mysql,port=32759,rewriteBatchedStatements=true)'
);
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-22 11:00:25.0,,,,,,,,,,"0|z17ygg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The existing column stats are deleted incorrectly when analyze table for partial columns,FLINK-29059,13477977,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,22/Aug/22 10:27,25/Aug/22 14:03,04/Jun/24 20:41,25/Aug/22 14:02,1.16.0,,,,,,,1.16.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"If there are three columns named `a, b, c` with column stats already exists,  I just analyze column `a` using `Analyze table xxx FOR COLUMNS a`, the existing column stats of `b, c` will be reset back to empty.",,,,,,,,,,,,,,,,FLINK-28939,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 14:02:10 UTC 2022,,,,,,,,,,"0|z17yeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 10:28;337361684@qq.com;This Jira  is related to [FLINK-28939|https://issues.apache.org/jira/browse/FLINK-28939];;;","25/Aug/22 14:02;godfrey;Fixed in master: fe392645421d10923c75cd5438b91d9ed55900d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyze table xxx For columns will thrown error while encounter String type column with null value,FLINK-29058,13477976,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,22/Aug/22 10:20,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,"When String/varchar type column `a` have `null` value, `Analyze table xxx FOR ALL COLUMNS/ COLUMNS a` may throw error:
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.thrift.protocol.TProtocolException: Required field 'maxColLen' is unset! Struct:StringColumnStatsData(maxColLen:0, avgColLen:0.0, numNulls:1, numDVs:0) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 10:25:07 UTC 2022,,,,,,,,,,"0|z17yeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 10:25;337361684@qq.com;This Jira is realated to [FLINK-28939|https://issues.apache.org/jira/browse/FLINK-28939];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not set rewriteBatchedStatements parameter for JDBC catalog for MySQL,FLINK-29057,13477975,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,macdoor615,macdoor615,22/Aug/22 10:19,22/Aug/22 10:57,04/Jun/24 20:41,22/Aug/22 10:57,1.15.0,,,,,,,,,,,Connectors / JDBC,,,,0,,,,,"execute following code in sql-client.sh
{code:java}
CREATE CATALOG mysql_bnpmp3 WITH(
 'type' = 'jdbc',
 'default-database' = 'gem_tmp',
 'username' = 'bnpmp',
 'password' = '*********',
 'base-url' = 'jdbc:mysql://hb3-prod-mysql:32759?rewriteBatchedStatements=true'
);{code}
output error
{code:java}
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant com.mysql.cj.conf.BooleanPropertyDefinition.AllowableValues.""TRUE""/GEM_TMP
{code}
rewriteBatchedStatements is a very import parameter, without it, data insertion performance will be very poor. 

I found a trick to overcome this problem. I changed jdbcUrl format and it works.
{code:java}
CREATE CATALOG mysql_bnpmp WITH(
    'type' = 'jdbc',
    'default-database' = 'gem_tmp',
    'username' = 'bnpmp',
    'password' = '*******',
    'base-url' = 'jdbc:mysql://(host=hb3-prod-mysql,port=32759,rewriteBatchedStatements=true)'
);
 
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-22 10:19:23.0,,,,,,,,,,"0|z17ye8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw PartitionNotFoundException if the partition file is not readable for hybrid shuffle.,FLINK-29056,13477971,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,22/Aug/22 10:04,26/Aug/22 14:38,04/Jun/24 20:41,26/Aug/22 14:38,1.16.0,,,,,,,1.16.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"If data file is not readable especially data loss, throw PartitionNotFoundException to mark this result partition failed. Otherwise, the partition data is not regenerated, so failover can not recover the job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 14:38:06 UTC 2022,,,,,,,,,,"0|z17ydc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 14:38;xtsong;master (1.16): c643a2953ba44b3b316ba52983932329dc0162e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.4] Provide a way to exclude the build-in stepDecorator,FLINK-29055,13477954,13432972,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,bzhaoop,bzhaoop,bzhaoop,22/Aug/22 08:22,16/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,Deployment / Kubernetes,,,,0,pull-request-available,stale-assigned,,,"The basic Idea is from Gyula Fora's comment .

Looks we need a way to exclude the build-in K8S stepDecorators.

From my original thought, whether we can provide a Configuration Option for exclude a sets of build-in StepDecorators. I think a full path Class list would help for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 16 10:35:10 UTC 2023,,,,,,,,,,"0|z17y9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure tests failed with Temporary failure resolving 'archive.ubuntu.com',FLINK-29054,13477916,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,hxbks2ks,hxbks2ks,22/Aug/22 06:23,23/Aug/22 07:00,04/Jun/24 20:41,23/Aug/22 07:00,1.16.0,,,,,,,,,,,Build System / Azure Pipelines,Test Infrastructure,,,0,test-stability,,,,"{code:java}
2022-08-22T02:30:50.8451433Z Err:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 libio-pty-perl amd64 1:1.08-1.1build1
2022-08-22T02:30:50.8452217Z   Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8505305Z Err:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libipc-run-perl all 0.94-1
2022-08-22T02:30:50.8506106Z   Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8561081Z Err:3 http://archive.ubuntu.com/ubuntu xenial/universe amd64 moreutils amd64 0.57-1
2022-08-22T02:30:50.8561877Z   Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8640593Z E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.08-1.1build1_amd64.deb  Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8641098Z 
2022-08-22T02:30:50.8641965Z E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_0.94-1_all.deb  Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8642685Z 
2022-08-22T02:30:50.8643536Z E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.57-1_amd64.deb  Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8643964Z 
2022-08-22T02:30:50.8644767Z E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40233&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 09:22:48 UTC 2022,,,,,,,,,,"0|z17y14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 06:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40230&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=42
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40232&view=results
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40233&view=results
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40236&view=results;;;","22/Aug/22 09:12;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40224&view=logs&j=cb526a4c-d76b-582c-cc98-6707d16ea573&t=0821312f-2360-5f19-d0ce-2482844f43b6;;;","22/Aug/22 09:22;chesnay;Ill bake this dependency into the image.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle has concurrent modification of buffer when compression is enabled,FLINK-29053,13477898,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,22/Aug/22 06:04,01/Sep/22 03:36,04/Jun/24 20:41,01/Sep/22 03:36,1.16.0,,,,,,,1.16.0,,,,Runtime / Network,,,,0,pull-request-available,,,,"When the downstream thread obtains the buffer and consuming it, if the data is compressed in the spilling thread and copied to the original buffer in the same time, since the two threads share the same memory data, the consuming thread will consume incorrect data, causing problems such as deserialize the data disorder.
Considering that the downstream consumption is prohibited during compression, or block spilling thread when the downstream consumption is not completed will have a great impact on performance. I think we should move the compression operation to the write thread and store the compressed buffer directly in memory.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 01 03:36:00 UTC 2022,,,,,,,,,,"0|z17xx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 03:36;xtsong;master (1.16): 8b8245ba46b25c2617d91cff3d3a44b99879d9f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support TO_TIMESTAMP  built-in function in Table API,FLINK-29052,13477892,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hehuiyuan,hehuiyuan,22/Aug/22 05:37,21/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,Table SQL / API,,,,0,auto-deprioritized-major,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:35:27 UTC 2023,,,,,,,,,,"0|z17xvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable dependency-reduced-pom creation in quickstarts,FLINK-29051,13477889,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Aug/22 05:13,29/Sep/22 14:20,04/Jun/24 20:41,22/Aug/22 17:53,1.15.1,,,,,,,1.16.0,,,,Quickstarts,,,,0,pull-request-available,,,,"now maven-shade-plugin puts it in project folder and version control detects as a change.

The proposal is to put it to target folder as it's done for table-planner",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 17:54:07 UTC 2022,,,,,,,,,,"0|z17xv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 17:53;chesnay;master: 65046d573bc7a82e65a74db5f17a208c2faa0efe;;;","22/Aug/22 17:54;chesnay;Re-purposed to not create the pom in the first place because it's usually unnecessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-hadoop-compatibility,FLINK-29050,13477839,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,21/Aug/22 14:20,25/Apr/24 01:49,04/Jun/24 20:41,25/Apr/24 01:32,,,,,,,,1.20.0,,,,Connectors / Hadoop Compatibility,Tests,,,0,pull-request-available,stale-assigned,starter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 01:32:07 UTC 2024,,,,,,,,,,"0|z17xk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 11:09;RocMarshal;Hello, sorry for the late update.
Could someone help to review it ? I'd appreciated it with your help. :D;;;","15/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Dec/23 08:06;RocMarshal;Based on [https://github.com/apache/flink/pull/20990#discussion_r1292783464]

We'd like to do the following sub-tasks for the current jira.
 - Rename  AbstractTestBase, JavaProgramTestBase MultipleProgramsTestBase to XXXXJUnit4,     
 - Use jUnit5 to re-write the implementations for the above classes & tag XXXXJUnit4 classes as deprecated 

 - Use junit5 implementation classes to migrate the Module: flink-hadoop-compatibility

 - Use junit5 implementation to make adaption for the sub-classes of XXXXJUnit4 (Maybe this part of the work needs to be recorded and promoted in other jiras);;;","25/Apr/24 01:32;fanrui;Merged to master(1.20.0) via:
 * ffa639a31c769880dceb39234700a75f0945ec65
 * 29a045574efd498ba0cb760845a459928c722531
 * 92eef24d4cc531d6474252ef909fc6d431285dd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the deprecated classes in the flink-java module.,FLINK-29049,13477836,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,RocMarshal,RocMarshal,21/Aug/22 13:33,22/Aug/22 02:06,04/Jun/24 20:41,22/Aug/22 02:06,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 21 14:34:10 UTC 2022,,,,,,,,,,"0|z17xjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/22 14:34;Sergey Nuyanzin;this is a duplicate of https://issues.apache.org/jira/browse/FLINK-15130;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WatermarksWithIdleness does not work with FLIP-27 sources,FLINK-29048,13477801,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,leeys.1,leeys.1,21/Aug/22 06:16,21/Aug/22 06:19,04/Jun/24 20:41,21/Aug/22 06:19,1.15.1,,,,,,,,,,,API / DataStream,,,,0,,,,,"In org.apache.flink.streaming.api.operators.SourceOperator, there are separate instances of WatermarksWithIdleness created for each split output and the main output. There is multiplexing of watermarks between split outputs but no multiplexing between split output and main output in org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.
 
For a source such as org.apache.flink.connector.kafka.source.KafkaSource, {color:#353833}there is only output from splits and no output from main. Hence the main output will (after an initial timeout) be marked as idle.{color}
{color:#353833} {color}
{color:#353833}The implementation of {color}WatermarksWithIdleness is such that once an output is idle, it will periodically re-mark the output as idle. Since there is no multiplexing between split outputs and main output, the idle marks coming from main output will repeatedly set the output to idle even though there are events from the splits. Result is that the entire source is repeatedly marked as idle.
 
One solution i can think of is to multiplex split and main output in org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks but I am not sure if there are other considerations.",,,,,,,,,,,,,,,,,,,,,FLINK-28975,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-21 06:16:13.0,,,,,,,,,,"0|z17xbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3.1] Shade the Flink-Kubernetes Fabric8 Kubernetes dependency with org.apache.flink.shaded prefix,FLINK-29047,13477733,13432972,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bzhaoop,bzhaoop,bzhaoop,20/Aug/22 12:58,24/Nov/22 15:35,04/Jun/24 20:41,09/Sep/22 04:16,,,,,,,,1.17.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,,"For supporting stepDecorators plugin mechanism, we load the plugin via existing Flink plugin from plugins directory. And make it like SPI.

 

So we need to shade the Flink kubernetes(Fabric8) as a whitelist for loading the said Classes during SPI loading instance. Due to some/most part of plugins need depends on the existing Flink Fabric8 dependency. So we need to load the said classes in parent classloader, won't load all classes from Plugin jars.",,,,,,,,,,,,,,,,,,,,FLINK-24273,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 09 04:16:01 UTC 2022,,,,,,,,,,"0|z17wwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 04:16;wangyang0918;Fixed via:

master(1.17): 17d7c39bb2a9fcbaac1ead42073c099a52171d7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceStatisticsReportTest fails with Hadoop 3,FLINK-29046,13477609,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,337361684@qq.com,chesnay,chesnay,19/Aug/22 14:01,26/Aug/22 08:26,04/Jun/24 20:41,26/Aug/22 08:26,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Tests,,,0,pull-request-available,,,,"
{code:java}
2022-08-19T13:35:56.1882498Z Aug 19 13:35:56 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport  Time elapsed: 9.442 s  <<< FAILURE!
2022-08-19T13:35:56.1883817Z Aug 19 13:35:56 org.opentest4j.AssertionFailedError: 
2022-08-19T13:35:56.1884543Z Aug 19 13:35:56 
2022-08-19T13:35:56.1890435Z Aug 19 13:35:56 expected: TableStats{rowCount=3, colStats={f_boolean=ColumnStats(nullCount=1), f_smallint=ColumnStats(nullCount=0, max=128, min=100), f_decimal5=ColumnStats(nullCount=0, max=223.45, min=123.45), f_array=null, f_binary=null, f_decimal38=ColumnStats(nullCount=1, max=123433343334333433343334333433343334.34, min=123433343334333433343334333433343334.33), f_map=null, f_float=ColumnStats(nullCount=1, max=33.33300018310547, min=33.31100082397461), f_row=null, f_tinyint=ColumnStats(nullCount=0, max=3, min=1), f_decimal14=ColumnStats(nullCount=0, max=123333333355.33, min=123333333333.33), f_date=ColumnStats(nullCount=0, max=1990-10-16, min=1990-10-14), f_bigint=ColumnStats(nullCount=0, max=1238123899121, min=1238123899000), f_timestamp3=ColumnStats(nullCount=0, max=1990-10-16 12:12:43.123, min=1990-10-14 12:12:43.123), f_double=ColumnStats(nullCount=0, max=10.1, min=1.1), f_string=ColumnStats(nullCount=0, max=def, min=abcd), f_int=ColumnStats(nullCount=1, max=45536, min=31000)}}
2022-08-19T13:35:56.1902811Z Aug 19 13:35:56  but was: TableStats{rowCount=3, colStats={f_boolean=ColumnStats(nullCount=1), f_smallint=ColumnStats(nullCount=0, max=128, min=100), f_decimal5=ColumnStats(nullCount=0, max=223.45, min=0), f_array=null, f_binary=null, f_decimal38=ColumnStats(nullCount=1, max=123433343334333433343334333433343334.34, min=123433343334333433343334333433343334.33), f_map=null, f_float=ColumnStats(nullCount=1, max=33.33300018310547, min=33.31100082397461), f_row=null, f_tinyint=ColumnStats(nullCount=0, max=3, min=1), f_decimal14=ColumnStats(nullCount=0, max=123333333355.33, min=0), f_date=ColumnStats(nullCount=0, max=1990-10-16, min=1990-10-14), f_bigint=ColumnStats(nullCount=0, max=1238123899121, min=1238123899000), f_timestamp3=ColumnStats(nullCount=0, max=1990-10-16 12:12:43.123, min=1990-10-14 12:12:43.123), f_double=ColumnStats(nullCount=0, max=10.1, min=1.1), f_string=ColumnStats(nullCount=0, max=def, min=abcd), f_int=ColumnStats(nullCount=1, max=45536, min=31000)}}
2022-08-19T13:35:56.1908634Z Aug 19 13:35:56 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-19T13:35:56.1910402Z Aug 19 13:35:56 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-19T13:35:56.1912266Z Aug 19 13:35:56 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-19T13:35:56.1913257Z Aug 19 13:35:56 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.assertHiveTableOrcFormatTableStatsEquals(HiveTableSourceStatisticsReportTest.java:339)
2022-08-19T13:35:56.1914512Z Aug 19 13:35:56 	at org.apache.flink.connectors.hive.HiveTableSourceStatisticsReportTest.testFlinkOrcFormatHiveTableSourceStatisticsReport(HiveTableSourceStatisticsReportTest.java:118)
2022-08-19T13:35:56.1915444Z Aug 19 13:35:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-19T13:35:56.1916130Z Aug 19 13:35:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-19T13:35:56.1916856Z Aug 19 13:35:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-19T13:35:56.1917571Z Aug 19 13:35:56 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-19T13:35:56.1918278Z Aug 19 13:35:56 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-19T13:35:56.1919020Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-19T13:35:56.1919923Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-19T13:35:56.1920841Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-19T13:35:56.1921877Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-19T13:35:56.1922778Z Aug 19 13:35:56 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-08-19T13:35:56.1923726Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-19T13:35:56.1924761Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-19T13:35:56.1925690Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-19T13:35:56.1926590Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-19T13:35:56.1927507Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-19T13:35:56.1928422Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-19T13:35:56.1929216Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-19T13:35:56.1930018Z Aug 19 13:35:56 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-19T13:35:56.1930866Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-19T13:35:56.1931868Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1932794Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-19T13:35:56.1933757Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-19T13:35:56.1934645Z Aug 19 13:35:56 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-19T13:35:56.1935581Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-19T13:35:56.1936483Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1937381Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1938153Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1938980Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1939899Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1940713Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1941642Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1942726Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1943944Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-19T13:35:56.1945074Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-19T13:35:56.1946207Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-19T13:35:56.1947104Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1947941Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1948776Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1949613Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1950509Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1951326Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1952371Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1953430Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1954545Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-19T13:35:56.1955575Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-19T13:35:56.1956466Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1957359Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-19T13:35:56.1958137Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-19T13:35:56.1959059Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-19T13:35:56.1959962Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-19T13:35:56.1960783Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-19T13:35:56.1961688Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-19T13:35:56.1962766Z Aug 19 13:35:56 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-19T13:35:56.1963674Z Aug 19 13:35:56 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-19T13:35:56.1964372Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-19T13:35:56.1965093Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-19T13:35:56.1965758Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-19T13:35:56.1966500Z Aug 19 13:35:56 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40205&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27988,HIVE-26492,,,,,,,,,,"22/Aug/22 13:06;337361684@qq.com;image-2022-08-22-21-06-29-980.png;https://issues.apache.org/jira/secure/attachment/13048394/image-2022-08-22-21-06-29-980.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 08:26:31 UTC 2022,,,,,,,,,,"0|z17w54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 02:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40211&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=25486;;;","22/Aug/22 02:01;hxbks2ks;[~337361684@qq.com] Could you help take a look? Thx.;;;","22/Aug/22 13:11;337361684@qq.com;Hi, [~hxbks2ks] and [~chesnay] , I  tried to reproduce the error of these two failed tests. I found that the problem was not produced by HiveTableSourceStatisticsReport itself. It is caused by _orc.apache.orc.impl.writer.StructTreeWriter_ in {_}hive-exec-3.1.1{_}. In this class,  method _writeFileStatistics_ will create a wrong column stats min value when encounter decimal type data like:

!image-2022-08-22-21-06-29-980.png|width=395,height=273!

Also, I found that there is no test cases to cover decimal type data for hive 3.1.1 or upper version in Flink. So, I think the best solution now is to add tests to cover the decimal type stats report of hive 3.x for orc format, and create issue in hive to fix this error.;;;","22/Aug/22 13:16;337361684@qq.com;In this version, I could add a parameter in _HiveTableSourceStatisticsReport_ to distinguish between hive 3. X and hive 2. X. By default, hive 3. X will produce wrong results.;;;","22/Aug/22 13:17;337361684@qq.com;Could you assign this to me! Thanks a lot.

 ;;;","22/Aug/22 14:06;337361684@qq.com;Hive issue: https://issues.apache.org/jira/browse/HIVE-26492;;;","24/Aug/22 01:18;luoyuxia;Seems like a bug of orc 1.5.6 which Hive3 depends. And I also found a similar issue [ORC-516|https://issues.apache.org/jira/browse/ORC-517].  I think we can skip check decimal in Hive3. ;;;","26/Aug/22 08:26;godfreyhe;Fixed in master: d501b88be5599e14a0da578e3083ef53a6392b11;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize error message in Flink SQL Client and Gateway when try to use Hive Dialect,FLINK-29045,13477592,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,19/Aug/22 11:52,21/Sep/22 01:48,04/Jun/24 20:41,21/Sep/22 01:48,,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,0,pull-request-available,,,,"Since Flink 1.15 , if users want to use HiveDialect, they have to swap flink-table-planner-loader located in /lib with flink-table-planner_2.12 located in /opt

Noticing it bothers some users as reported in [FLINK-27020| https://issues.apache.org/jira/browse/FLINK-27020], [FLINK-28618|https://issues.apache.org/jira/browse/FLINK-28618] .

Althogh the document has noted it, but some users may still miss it.  It would be better to show the detail error message  and tell user how to deal with  such case in Flink SQL client.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 14:29:29 UTC 2022,,,,,,,,,,"0|z17w1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 14:29;jark;Fixed in 
 - master: 791d8396163a8eb045493f7333218c5d881cc6ff
 - release-1.16: 9e16d54b9ea0422a97bcbe20ebb244be54dc1c3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for DCT,FLINK-29044,13477559,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,yunfengzhou,yunfengzhou,19/Aug/22 08:24,07/Sep/22 01:42,04/Jun/24 20:41,07/Sep/22 01:42,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-19 08:24:17.0,,,,,,,,,,"0|z17vu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve ML iteration efficiency,FLINK-29043,13477551,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,19/Aug/22 07:42,23/Aug/22 14:31,04/Jun/24 20:41,23/Aug/22 14:31,ml-2.1.0,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,0,pull-request-available,,,,"Currently, in Github Action, it takes about one minute to execute the unit tests of each algorithm that uses flink ml's iteration mechanism, including KMeansTest, LinearRegressionTest, LinearSVCTest, and LogisticRegressionTest[1]. We need to figure out which components in flink-ml-iteration have caused this phenomenon and seek to make corresponding improvements.

 [1] [https://github.com/apache/flink-ml/runs/7892649470?check_suite_focus=true]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-19 07:42:42.0,,,,,,,,,,"0|z17vs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support lookup join for es connector,FLINK-29042,13477550,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,19/Aug/22 07:38,12/Sep/23 03:30,04/Jun/24 20:41,11/Sep/23 20:32,1.16.0,,,,,,,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,stale-assigned,,,"Now es connector could only be used as a sink, but in many business scenarios, we treat es as a index database, we should support to make it lookupable in flink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 20:33:15 UTC 2023,,,,,,,,,,"0|z17vs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 07:48;liyubin117;I have implemented the feature, Could someone assign this to me?;;;","30/Aug/22 07:34;martijnvisser;[~liyubin117] Can you please make sure that you point your PR to both the Flink repo and to the external connector repo? https://github.com/apache/flink-connector-elasticsearch;;;","30/Aug/22 08:10;liyubin117;[~martijnvisser] Thanks for your kind reminds, I will take care of it.;;;","17/Oct/22 07:16;liyubin117;[~martijnvisser] Given flink-connector-elasticsearch has been moved to the external connector repo, I have just implement the feature in the repo, please have a look, thanks!;;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Sep/23 20:33;Sergey Nuyanzin;Merged as [fef5c964f4a2fc8b61c482b0bc8504757581adfb|https://github.com/apache/flink-connector-elasticsearch/commit/fef5c964f4a2fc8b61c482b0bc8504757581adfb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add utility to test POJO compliance without any Kryo usage,FLINK-29041,13477547,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,19/Aug/22 07:28,24/Aug/22 07:35,04/Jun/24 20:41,24/Aug/22 07:35,,,,,,,,1.16.0,,,,API / DataStream,Tests,,,0,pull-request-available,,,,Add a variant of the test util added in FLINK-28636 that additionally asserts that Kryo is not used for any contained field.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 24 07:35:36 UTC 2022,,,,,,,,,,"0|z17vrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 07:35;chesnay;master: 254b276c79a5adce21269fae6722e3bf3ac15b78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using the JDBC Catalog, the custom cannot be applied because it is fixed in the code",FLINK-29040,13477541,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,gaara,gaara,gaara,19/Aug/22 06:36,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Connectors / JDBC,,,,0,pull-request-available,stale-assigned,,,"使用JDBC catalog 时，自定义的无法应用，因为代码中是固定的。

When using the JDBC Catalog, the custom cannot be applied because it is fixed in the code.

我在做ClickHouse的JDBC catalog测试时，无法直接使用发行版的代码。

When I was testing ClickHouse's JDBC Catalog, I couldn't use the distribution's code directly.

JDBC catalog未来应该会持续拓展，所以建议采用别的方式来实例化JdbcCatalog。

The JDBC Catalog should continue to expand in the future, so it is recommended to instantiate JdbcCatalog in a different way.

稍后我会提交一个PR，希望可以采用这种方式，或者类似方式来实例化JdbcCatalog。

I will submit a PR later and hopefully instantiate JdbcCatalog this way, or something similar.",flink-1.16-SNAPSHOT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/22 06:36;gaara;截图_选择区域_20220819143458.png;https://issues.apache.org/jira/secure/attachment/13048307/%E6%88%AA%E5%9B%BE_%E9%80%89%E6%8B%A9%E5%8C%BA%E5%9F%9F_20220819143458.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Wed Aug 16 10:35:11 UTC 2023,,,,,,,,,,"0|z17vq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RowData produced by LineBytesInputFormat is reused, but DeserializationSchemaAdapter#Reader only shallow copies produced data, thus result will always be the last row value",FLINK-29039,13477521,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ruanhang1993,mvillalobos,mvillalobos,19/Aug/22 02:56,15/Aug/23 02:59,04/Jun/24 20:41,,1.15.1,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,,"RowData produced by LineBytesInputFormat is reused, but DeserializationSchemaAdapter#Reader only shallow copies produced data, thus result will always be the last row value.

 

Given this program:
{code:java}
package mvillalobos.bug;

import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.TableResult;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

import static org.apache.flink.table.api.Expressions.$;

public class IsThisABatchSQLBug {  
   public static void main(String[] args) {
     final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
     env.setRuntimeMode(RuntimeExecutionMode.BATCH);
     final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
     tableEnv.executeSql(""CREATE TABLE historical_raw_source_template(\n"" +
           ""        `file.path`              STRING NOT NULL METADATA,\n"" +
           ""        `file.name`              STRING NOT NULL METADATA,\n"" +
           ""        `file.size`              BIGINT NOT NULL METADATA,\n"" +
           ""        `file.modification-time` TIMESTAMP_LTZ(3) NOT NULL METADATA,\n"" +
           ""        line                    STRING\n"" +
           ""      ) WITH (\n"" +
           ""        'connector' = 'filesystem', \n"" +
           ""        'format' = 'raw'\n"" +
           ""      );"");
     tableEnv.executeSql(""CREATE TABLE historical_raw_source\n"" +
           ""      WITH (\n"" +
           ""        'path' = '/Users/minmay/dev/mvillalobos/historical/data'\n"" +
           ""      ) LIKE historical_raw_source_template;"");     final TableResult output = tableEnv.from(""historical_raw_source"").select($(""line"")).execute();
     output.print();
  }
} {code}
and this sample.csv file in the '/Users/minmay/dev/mvillalobos/historical/data' directory:
{code:java}
one
two
three
four
five
six
seven
eight
nine
ten {code}
{{The print results are:}}
{code:java}
+----+--------------------------------+
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
| +I |                            ten |
+----+--------------------------------+
10 rows in set {code}
 ",This issue was discovered on MacOS Big Sur.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 15 02:59:39 UTC 2023,,,,,,,,,,"0|z17vlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 07:39;martijnvisser;[~godfrey] [~jark] Any idea who can look into this ticket? ;;;","19/Jul/23 03:00;ruanhang1993;Hi, all.

I would like to help. Please assign this issue to me. Thanks.;;;","15/Aug/23 02:59;ruanhang1993;This bug is the same as https://issues.apache.org/jira/browse/FLINK-25132.

We cannot get the right result when the deserializer reuses the object and the connector put deserialized records in a collection. This will cause the collection contains the same object.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncWaitOperatorTest.testProcessingTimeRepeatedCompleteOrderedWithRetry failed with AssertionError,FLINK-29038,13477515,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lincoln.86xy,hxbks2ks,hxbks2ks,19/Aug/22 02:15,06/Feb/23 03:43,04/Jun/24 20:41,06/Feb/23 03:43,1.16.0,1.17.0,,,,,,1.16.2,1.17.0,,,API / DataStream,,,,0,pull-request-available,test-stability,,,"{code:java}
2022-08-18T15:58:28.8029934Z Aug 18 15:58:28 [INFO] Results:
2022-08-18T15:58:28.8030287Z Aug 18 15:58:28 [INFO] 
2022-08-18T15:58:28.8030644Z Aug 18 15:58:28 [ERROR] Failures: 
2022-08-18T15:58:28.8032800Z Aug 18 15:58:28 [ERROR]   AsyncWaitOperatorTest.testProcessingTimeRepeatedCompleteOrderedWithRetry:1203->testProcessingTimeWithRetry:1253 ORDERED Output was not correct.: array lengths differed, expected.length=3 actual.length=2; arrays first differed at element [2]; expected:<Record @ 6 : 12> but was:<end of array>
2022-08-18T15:58:28.8033826Z Aug 18 15:58:28 [INFO] 
2022-08-18T15:58:28.8034265Z Aug 18 15:58:28 [ERROR] Tests run: 2061, Failures: 1, Errors: 0, Skipped: 28
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40164&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27878,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 03:43:23 UTC 2023,,,,,,,,,,"0|z17vk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 02:24;hxbks2ks;[~lincoln.86xy] Could you help take a look? Thx.;;;","19/Aug/22 02:37;lincoln.86xy;[~hxbks2ks] sure, I'll lookup into this.;;;","22/Aug/22 03:35;lincoln.86xy;The failure record is the last input record of test data, it doesn't hit retry condition and was expected as a normal process and directly outputs '12,6', can't find the reason the record lost after `StreamTaskMailboxTestHarness#processAll` (no close operation before getOutput) so far, I will continue to track this problem and try to find someone that familiar with the harness test to help lookup at this.

 ;;;","29/Aug/22 09:43;lincoln.86xy;This failure most probably cause by slow execution of internal execution service used by the test async function. We will improve the execution deadline from 1s to 10s so as to reduce the failure possiblilty(In most cases, it will only take less than 1s).  Thanks [~gaoyunhaii] for helping investigating this!;;;","30/Aug/22 02:47;hxb;Thanks [~lincoln.86xy] and [~gaoyunhaii] for the fix.;;;","30/Aug/22 02:48;hxb;Merged into master via a1d74c131b0fd10f34436463077fd5c7a7984a2b;;;","02/Feb/23 08:55;mapohl;I'm reopening this issue since it appeared again in a 1.17 build.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9203]

[~lincoln.86xy] or does it deserve a new ticket?;;;","02/Feb/23 12:20;lincoln.86xy;[~mapohl] Looks like the timeout is still occurring under the current test machine load after the time was previously turned up. Let's watch for a while, and if the failure of this case continues to occur I'll turn up the timeout again, WDYT?;;;","03/Feb/23 08:11;mapohl;[~lincoln.86xy]: I didn't check the fix earlier. I did a pass over it now: The test class itself doesn't follow Flink's coding guidelines in terms of timeouts which leaves it vulnerable to test instabilities. We agreed on not using timeouts in tests and let the be shutdown by the test watchdog process which will print a thread dump at the end (see [coding guidelines|https://flink.apache.org/contributing/code-style-and-quality-common.html#avoid-timeouts-in-junit-tests]). If we really have an issue with tests running into timeouts, printing the thread dump in the end might help investigating the issue in comparison to letting the test fail early. I feel like using the 10s deadline in [AsyncWaitOperatorTest:1242|https://github.com/apache/flink/blob/a1d74c131b0fd10f34436463077fd5c7a7984a2b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/operators/async/AsyncWaitOperatorTest.java#L1242] is not necessary. The same applies to other tests in this class where we use timeouts.;;;","03/Feb/23 09:15;lincoln.86xy;[~mapohl] thanks for pointing out the testing guideline violation, agree with you not using local timeout and let the test watchdog process the timeout can be better. I've submitted a fix for this.;;;","03/Feb/23 13:12;lincoln.86xy;fixed in master: b138c82a464d825ec51587e57f136f91128d6bba;;;","06/Feb/23 03:43;lincoln.86xy;fixed in 1.16: db524dfb46673bc98d4cf93b9e84ed9a4b76d837;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Histogram not emitting sum while using Metrics Reporter,FLINK-29037,13477460,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,svadali,svadali,18/Aug/22 16:30,11/Nov/22 14:12,04/Jun/24 20:41,11/Nov/22 14:11,1.13.6,,,,,,,,,,,Runtime / Metrics,,,,1,,,,,"I am currently registering a Flink Histogram and using the Prometheus Metrics Reporter to send this metric to our Time Series Data Storage. When Prometheus grabs this metric and converts it to the ""summary"" type, there is no sum found (only the streaming quantiles and count). This is causing an issue when our metrics agent is attempting to capture the Flink Histogram/Prometheus Summary.

I was wondering if in a newer version (than 1.13.6) the histogram sum is computed by Flink and what that version would be? If not, is there any work around so that a Flink histogram can emit all 3 elements (quantiles, sum, and count) in Prometheus format?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 11 14:12:26 UTC 2022,,,,,,,,,,"0|z17v80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 07:39;martijnvisser;[~svadali] Can you please verify this with either Flink 1.14 or Flink 1.15, since Flink 1.13 is no longer supported by the community?;;;","10/Nov/22 16:40;qingwei91;I believe this is still not supported in Flink 1.15 and above.

If I read it correctly, this is the relevant code: [https://github.com/apache/flink/blob/master/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/AbstractPrometheusReporter.java#L365];;;","10/Nov/22 17:04;qingwei91;I had a closer look, and I think sum is not supported because flink uses Dropwizard Histogram under the hood, and it does not maintain a sum.;;;","11/Nov/22 14:11;chesnay;We don't calculate sums for histograms.;;;","11/Nov/22 14:12;chesnay;(and it doesn't make sense to change that because the in practice the only histogram implementations that exist go through dropwizard which doesn't support sums);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code examples on the Data Sources page have errors,FLINK-29036,13477441,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,18/Aug/22 14:00,23/Aug/22 09:18,04/Jun/24 20:41,23/Aug/22 09:18,,,,,,,,1.16.0,,,,Documentation,,,,0,pull-request-available,,,,"While reviewing the  [Data Source|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/datastream/sources/], some examples are slightly out of date.

As an example, FutureNotifier doesn't exist any more.

This page (as well as some javadoc) could be reviewed for correctness.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 09:18:01 UTC 2022,,,,,,,,,,"0|z17v3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 14:01;rskraba;I'm reading a bunch of the doc and picking up errors as i go along -- can you assign this to me please?;;;","23/Aug/22 09:18;chesnay;master: 58296bfd98751904525cac9f4a94fc2951b087b6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpressionReducer does not work with jar resources,FLINK-29035,13477433,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,twalthr,twalthr,18/Aug/22 13:01,26/Aug/22 11:59,04/Jun/24 20:41,26/Aug/22 11:59,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,"It seems the code generation for expression reduction uses an invalid class loader that does not contain the jar resource.

Reproducible example:

{code}

CREATE TEMPORARY SYSTEM FUNCTION myLower AS '%s' USING JAR '%s'

SELECT myLower('HELLO')

{code}

 

fails with

{code}

java.lang.RuntimeException: Could not instantiate generated class 'ExpressionReducer$4'

    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
    at org.apache.flink.table.planner.codegen.ExpressionReducer.reduce(ExpressionReducer.scala:97)
    at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressionsInternal(ReduceExpressionsRule.java:759)
    at org.apache.calcite.rel.rules.ReduceExpressionsRule.reduceExpressions(ReduceExpressionsRule.java:699)
    at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:306)


Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.

Caused by: org.codehaus.commons.compiler.CompileException: Line 13, Column 37: Cannot determine simple type name ""LowerUDF46""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 26 11:59:22 UTC 2022,,,,,,,,,,"0|z17v20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 13:02;twalthr;CC [~lsy] ;;;","19/Aug/22 09:20;lsy;[~twalthr] Thanks for report, I will take a look.;;;","26/Aug/22 11:59;jark;Fixed in master: 0ade193d39326dd5b84334348a4b6ce76c4a915a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HYBRID_FULL result partition type is not yet reConsumable,FLINK-29034,13477410,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,18/Aug/22 10:27,19/Aug/22 09:45,04/Jun/24 20:41,19/Aug/22 09:45,1.16.0,,,,,,,1.16.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,"HYBRID_FULL partitions can be consumed repeatedly, but it does not support concurrent consumption. So re-consumable is false, but double calculation can be avoided during failover. If we regard it as re-consumable, there will be problems when the partition is reused. Therefore, we temporarily set this field false, and reset it to true when HsResultPartition supports downstream concurrent consumption of multiple identical subpartitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 09:45:16 UTC 2022,,,,,,,,,,"0|z17uww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 09:45;xtsong;master (1.16): f0a6c0cbd8313de8146c9c2610bb3db98bacaea0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add timestamp to operator resource listener context,FLINK-29033,13477409,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,18/Aug/22 10:19,22/Aug/22 05:40,04/Jun/24 20:41,22/Aug/22 05:40,,,,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,,In order to make deterministic listener logic easier to implement we should expose a timestamp through the context for both events and status updates.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 05:40:24 UTC 2022,,,,,,,,,,"0|z17uwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 05:40;gyfora;merged to main 964bedefc1e6011110c5a45ecc007494cd531300;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Consume from timestamp catch exception : Caused by: java.lang.IllegalArgumentException: Invalid negative offset,FLINK-29032,13477391,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,HunterHunter,HunterHunter,18/Aug/22 09:06,30/Aug/22 07:41,04/Jun/24 20:41,30/Aug/22 07:41,1.15.1,,,,,,,,,,,Connectors / Kafka,,,,0,,,,,"/*+ OPTIONS(
'scan.startup.mode' = 'timestamp',
'scan.startup.timestamp-millis'='1660809660000',
) */;
{code:java}
org.apache.flink.util.FlinkRuntimeException: Failed to initialize partition splits due to 
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.handlePartitionSplitChanges(KafkaSourceEnumerator.java:299) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83) ~[flink-dist-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-dist-1.15.1-vip.jar:1.15.1-vip]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_201]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_201]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_201]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_201]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_201]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_201]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]
Caused by: java.lang.IllegalArgumentException: Invalid negative offset
	at org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ~[?:1.8.0_201]
	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:1.8.0_201]
	at java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1699) ~[?:1.8.0_201]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_201]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_201]
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_201]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_201]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_201]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.initializePartitionSplits(KafkaSourceEnumerator.java:272) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator.lambda$checkPartitionChanges$0(KafkaSourceEnumerator.java:242) ~[flink-sql-connector-kafka-1.15.1-vip.jar:1.15.1-vip]
	at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80) ~[flink-dist-1.15.1-vip.jar:1.15.1-vip]
	... 7 more {code}",,,,,,,,,,,,,,,,,,,,FLINK-28185,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 30 07:41:21 UTC 2022,,,,,,,,,,"0|z17uso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 07:41;martijnvisser;I believe this is a duplicate;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKinesisConsumerTest.testSourceSynchronization failed with AssertionFailedError,FLINK-29031,13477386,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hxbks2ks,hxbks2ks,18/Aug/22 08:52,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Connectors / Kinesis,,,,0,test-stability,,,,"
{code:java}
2022-08-18T03:58:00.0197521Z Aug 18 03:58:00 [ERROR] org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.testSourceSynchronization  Time elapsed: 10.191 s  <<< FAILURE!
2022-08-18T03:58:00.0198736Z Aug 18 03:58:00 org.opentest4j.AssertionFailedError: 
2022-08-18T03:58:00.0199434Z Aug 18 03:58:00 [first record received] 
2022-08-18T03:58:00.0200022Z Aug 18 03:58:00 expected: 1
2022-08-18T03:58:00.0200577Z Aug 18 03:58:00  but was: 0
2022-08-18T03:58:00.0201285Z Aug 18 03:58:00 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-18T03:58:00.0202337Z Aug 18 03:58:00 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-18T03:58:00.0203442Z Aug 18 03:58:00 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-18T03:58:00.0205001Z Aug 18 03:58:00 	at org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumerTest.testSourceSynchronization(FlinkKinesisConsumerTest.java:1149)
2022-08-18T03:58:00.0206078Z Aug 18 03:58:00 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-18T03:58:00.0206994Z Aug 18 03:58:00 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-18T03:58:00.0208019Z Aug 18 03:58:00 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-18T03:58:00.0208952Z Aug 18 03:58:00 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-18T03:58:00.0209816Z Aug 18 03:58:00 	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:68)
2022-08-18T03:58:00.0211029Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:326)
2022-08-18T03:58:00.0212264Z Aug 18 03:58:00 	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:89)
2022-08-18T03:58:00.0213266Z Aug 18 03:58:00 	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:97)
2022-08-18T03:58:00.0214530Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:310)
2022-08-18T03:58:00.0216259Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:131)
2022-08-18T03:58:00.0217769Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.access$100(PowerMockJUnit47RunnerDelegateImpl.java:59)
2022-08-18T03:58:00.0219348Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner$TestExecutorStatement.evaluate(PowerMockJUnit47RunnerDelegateImpl.java:147)
2022-08-18T03:58:00.0220610Z Aug 18 03:58:00 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-18T03:58:00.0221543Z Aug 18 03:58:00 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-18T03:58:00.0222807Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.evaluateStatement(PowerMockJUnit47RunnerDelegateImpl.java:107)
2022-08-18T03:58:00.0224339Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82)
2022-08-18T03:58:00.0226110Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:298)
2022-08-18T03:58:00.0227637Z Aug 18 03:58:00 	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:87)
2022-08-18T03:58:00.0228583Z Aug 18 03:58:00 	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:50)
2022-08-18T03:58:00.0229767Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:218)
2022-08-18T03:58:00.0231130Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
2022-08-18T03:58:00.0232443Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
2022-08-18T03:58:00.0233582Z Aug 18 03:58:00 	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
2022-08-18T03:58:00.0237704Z Aug 18 03:58:00 	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
2022-08-18T03:58:00.0238913Z Aug 18 03:58:00 	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
2022-08-18T03:58:00.0240041Z Aug 18 03:58:00 	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
2022-08-18T03:58:00.0241152Z Aug 18 03:58:00 	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
2022-08-18T03:58:00.0242137Z Aug 18 03:58:00 	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
2022-08-18T03:58:00.0242895Z Aug 18 03:58:00 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-18T03:58:00.0243587Z Aug 18 03:58:00 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-18T03:58:00.0244365Z Aug 18 03:58:00 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-18T03:58:00.0245751Z Aug 18 03:58:00 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-18T03:58:00.0246649Z Aug 18 03:58:00 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-18T03:58:00.0247581Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-18T03:58:00.0248597Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-18T03:58:00.0249773Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-18T03:58:00.0250859Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-18T03:58:00.0251912Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-18T03:58:00.0252845Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-18T03:58:00.0253723Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-18T03:58:00.0254688Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-18T03:58:00.0256266Z Aug 18 03:58:00 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-18T03:58:00.0257318Z Aug 18 03:58:00 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-08-18T03:58:00.0258206Z Aug 18 03:58:00 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-08-18T03:58:00.0259059Z Aug 18 03:58:00 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-08-18T03:58:00.0260066Z Aug 18 03:58:00 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-18T03:58:00.0261078Z Aug 18 03:58:00 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-08-18T03:58:00.0262026Z Aug 18 03:58:00 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-18T03:58:00.0262910Z Aug 18 03:58:00 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-18T03:58:00.0263736Z Aug 18 03:58:00 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-18T03:58:00.0264551Z Aug 18 03:58:00 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-18T03:58:00.0265138Z Aug 18 03:58:00 
2022-08-18T03:58:00.3848848Z Aug 18 03:58:00 [INFO] 
2022-08-18T03:58:00.3849425Z Aug 18 03:58:00 [INFO] Results:
2022-08-18T03:58:00.3849823Z Aug 18 03:58:00 [INFO] 
2022-08-18T03:58:00.3850546Z Aug 18 03:58:00 [ERROR] Failures: 
2022-08-18T03:58:00.3851183Z Aug 18 03:58:00 [ERROR]   FlinkKinesisConsumerTest.testSourceSynchronization:1149 [first record received] 
2022-08-18T03:58:00.3851804Z Aug 18 03:58:00 expected: 1
2022-08-18T03:58:00.3852206Z Aug 18 03:58:00  but was: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40126&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27185,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 23:18:42 UTC 2023,,,,,,,,,,"0|z17urk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 08:55;hxbks2ks;Hi [~slinkydeveloper], could you help take a loot at this unstable test ? Thx.;;;","06/Sep/22 06:57;martijnvisser;[~dannycranmer] Can you have a look at this one?;;;","13/Sep/22 03:23;hxb;Given that it hasn't appeared for a month, I will downgrade the priority to Major.;;;","24/Jul/23 23:18;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51556&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=37890;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print a log message if a Pojo/Tuple contains a generic type,FLINK-29030,13477378,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Aug/22 08:34,24/Aug/22 07:40,04/Jun/24 20:41,24/Aug/22 07:40,,,,,,,,1.16.0,,,,API / Type Serialization System,,,,0,pull-request-available,,,,"Users are encouraged to use POJO types, that will be serialized by the PojoSerializer which supports schema evolution.
If a user does not use a POJO we print an info message, linking to the docs and citing potential performance issues.

However, no such message is printed if a POJO _contains_ a generic type.

As a result there may be users out there believing to have optimal performance and support for schema evolution since, after all, they are able to use the POJO serializer, when this may not be the case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29016,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 24 07:40:35 UTC 2022,,,,,,,,,,"0|z17ups:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 08:44;chesnay;This in particular happens if a POJO field has an interface type, like {{List}}, or has a type hierarchy exceeding 1 level (e.g., Type1 extends (Type2 extends Type3)).;;;","24/Aug/22 07:40;chesnay;master: 01fb742d0946d6cf81ea6a2945a4f77c341a0cf1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug of CSV format doesn't work in Thread Mode,FLINK-29029,13477368,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Aug/22 07:33,25/Aug/22 09:13,04/Jun/24 20:41,25/Aug/22 09:13,1.16.0,,,,,,,1.16.0,,,,API / Python,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 25 09:13:38 UTC 2022,,,,,,,,,,"0|z17unk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 09:13;hxb;Merged into master via 0af535a9134aaf8ff86e936c7fd0a09c33e037a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the missing cache api in Python DataStream API,FLINK-29028,13477357,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,18/Aug/22 06:15,22/Aug/22 01:53,04/Jun/24 20:41,22/Aug/22 01:52,1.16.0,,,,,,,1.16.0,,,,API / Python,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 01:52:55 UTC 2022,,,,,,,,,,"0|z17ul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 01:52;dianfu;Merged to master via 97519d1683f3f5f086ce613d59aa5ba46cf38a30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveTableSink failed to report statistic to hive metastore in stream mode,FLINK-29027,13477355,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,18/Aug/22 06:04,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,,1.20.0,,,,Connectors / Hive,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-18 06:04:09.0,,,,,,,,,,"0|z17uko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs for HiveServer2 integration,FLINK-29026,13477351,13477344,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,luoyuxia,luoyuxia,18/Aug/22 04:31,20/Sep/22 08:05,04/Jun/24 20:41,20/Sep/22 08:05,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Documentation,Table SQL / Gateway,,0,,,,,,,,,,,,,,,,,,,,FLINK-29148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-18 04:31:59.0,,,,,,,,,,"0|z17ujs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for Hive Dialect,FLINK-29025,13477350,13477344,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,18/Aug/22 04:31,20/Sep/22 08:10,04/Jun/24 20:41,20/Sep/22 01:46,1.16.0,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,Documentation,,,0,pull-request-available,,,,Moving some stuff from connectors/table/hive/hive_dialect.,,,,,,,,,,,,,,,,FLINK-29077,FLINK-29076,FLINK-29078,FLINK-29079,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 20 01:46:51 UTC 2022,,,,,,,,,,"0|z17ujk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 01:46;jark;Fixed in 
 - master: 99e892736178993ad2beafcec7a44c0f4d507758..7c9f5ec1845c4a2332ebda944d3ef6e3a805d1a3
 - release-1.16: 53194166ee70c71aae55bedf1030f44aaf50ca3d..100979d89613ca30b07c905fc8b3c444aa43e3c0
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an overview page for Hive Compatibility,FLINK-29024,13477349,13477344,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,luoyuxia,luoyuxia,18/Aug/22 04:28,28/Sep/22 08:55,04/Jun/24 20:41,28/Sep/22 08:55,1.16.0,,,,,,,1.16.0,,,,Connectors / Hive,Documentation,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 08:55:26 UTC 2022,,,,,,,,,,"0|z17ujc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 08:55;luoyuxia;Seems we don't need it again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating Jar statement document,FLINK-29023,13477348,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,18/Aug/22 04:19,19/Sep/22 14:16,04/Jun/24 20:41,19/Sep/22 14:16,1.16.0,,,,,,,1.16.0,1.17.0,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 19 14:16:09 UTC 2022,,,,,,,,,,"0|z17uj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 14:16;jark;Fixed in 
 - master: f6520ec16536c8e2380acbb70630c111a8f714da and 65907bc5470bc43f0227ab287d2a6f150ba0bc29
 - release-1.16: 59f1c344456fcec49e1c805ae6eac10d05ead34e and d18fc95cfd3ba7e5aa91810380de65198cfea9d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for CREATE FUNCTION USING JAR feature,FLINK-29022,13477346,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,18/Aug/22 04:05,06/Sep/22 09:20,04/Jun/24 20:41,06/Sep/22 09:20,1.16.0,,,,,,,1.16.0,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 06 09:20:12 UTC 2022,,,,,,,,,,"0|z17uio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 09:20;jark;Fixed in 
 - master: 666ef1865360496bb3436a17e9ce68d325acba1fb2e61
 - release-1.16: 905a9654558a97705eed40db268fffc5af1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Add docs for Hive Compatibility,FLINK-29021,13477344,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,18/Aug/22 04:03,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,,1.20.0,,,,Documentation,,,,0,documentation,,,,"This Jira is for adding some docs for Hive Compatibility.

Expect to add a new section below `dev/table/function` with title 'Hive Compatibility',  and including three children pages in it:

1:  Overview

2:  Hive Dialect

3:  HiveServer2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-08-18 04:03:11.0,,,,,,,,,,"0|z17ui8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for CTAS feature,FLINK-29020,13477343,13436816,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,lsy,lsy,18/Aug/22 03:55,11/Oct/22 08:54,04/Jun/24 20:41,30/Sep/22 09:01,1.16.0,,,,,,,1.16.0,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 11 08:54:51 UTC 2022,,,,,,,,,,"0|z17ui0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 05:34;jark;Fixed in 
 - master: 8ce056c59439c1a3cedd6b32c0a98a14febc7ffb;;;","11/Oct/22 08:54;jark;Fixed in release-1.16: 29d95797ac170fa4ab4038dd063766171d8d7fe4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating parquet format document that support read complex type,FLINK-29019,13477342,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,18/Aug/22 03:52,31/Aug/22 08:14,04/Jun/24 20:41,31/Aug/22 08:14,1.16.0,,,,,,,1.16.0,,,,Documentation,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 31 08:14:19 UTC 2022,,,,,,,,,,"0|z17uhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 08:14;martijnvisser;Fixed in master: 279856570807ba3818c8a5302cc7e3401256c64d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaITCase.testMultipleSourcesOnePartition failed with TopicExistsException,FLINK-29018,13477333,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,hxbks2ks,hxbks2ks,18/Aug/22 02:10,13/Sep/22 02:22,04/Jun/24 20:41,13/Sep/22 02:22,1.16.0,,,,,,,1.16.0,,,,Connectors / Kafka,,,,0,test-stability,,,,"{code:java}
2022-08-17T22:52:03.5376972Z Aug 17 22:52:03 [ERROR] Tests run: 23, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 133.5 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.KafkaITCase
2022-08-17T22:52:03.5378664Z Aug 17 22:52:03 [ERROR] org.apache.flink.streaming.connectors.kafka.KafkaITCase.testMultipleSourcesOnePartition  Time elapsed: 3.967 s  <<< FAILURE!
2022-08-17T22:52:03.5380231Z Aug 17 22:52:03 java.lang.AssertionError: Create test topic : manyToOneTopic failed, org.apache.kafka.common.errors.TopicExistsException: Topic 'manyToOneTopic' already exists.
2022-08-17T22:52:03.5381136Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:207)
2022-08-17T22:52:03.5381983Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
2022-08-17T22:52:03.5382935Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:217)
2022-08-17T22:52:03.5384026Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runMultipleSourcesOnePartitionExactlyOnceTest(KafkaConsumerTestBase.java:1053)
2022-08-17T22:52:03.5385057Z Aug 17 22:52:03 	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testMultipleSourcesOnePartition(KafkaITCase.java:105)
2022-08-17T22:52:03.5385741Z Aug 17 22:52:03 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T22:52:03.5386344Z Aug 17 22:52:03 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T22:52:03.5387038Z Aug 17 22:52:03 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T22:52:03.5387658Z Aug 17 22:52:03 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T22:52:03.5388350Z Aug 17 22:52:03 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T22:52:03.5389118Z Aug 17 22:52:03 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T22:52:03.5389875Z Aug 17 22:52:03 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T22:52:03.5390556Z Aug 17 22:52:03 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-17T22:52:03.5391564Z Aug 17 22:52:03 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2022-08-17T22:52:03.5392325Z Aug 17 22:52:03 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2022-08-17T22:52:03.5392985Z Aug 17 22:52:03 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-17T22:52:03.5393687Z Aug 17 22:52:03 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40124&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37261",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29153,,,,FLINK-24119,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 13 02:21:50 UTC 2022,,,,,,,,,,"0|z17ufs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 02:10;hxbks2ks;[~renqs] Could help take a look? Thx.;;;","19/Aug/22 02:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40164&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","24/Aug/22 07:27;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40324&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","24/Aug/22 08:54;godfrey;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40290&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","26/Aug/22 06:11;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40394&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf;;;","29/Aug/22 02:32;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40398&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","30/Aug/22 07:54;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40505&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc;;;","30/Aug/22 07:54;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40505&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf;;;","30/Aug/22 07:55;hxb;[~renqs]  Any progress on this issue?;;;","31/Aug/22 03:46;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40524&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","31/Aug/22 06:08;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40540&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f7d83ad5-3324-5307-0eb3-819065cdcb38&l=8272;;;","31/Aug/22 07:03;renqs;Thanks for sharing the info [~hxb]! This is caused by a recent version bump of Kafka client. I created FLINK-29153 for fixing this. ;;;","13/Sep/22 02:21;renqs;Should be fixed by FLINK-29153. Please reopen the issue if the unstable case pops up again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some github links in released doc point to master,FLINK-29017,13477275,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,17/Aug/22 17:05,19/Aug/22 07:43,04/Jun/24 20:41,19/Aug/22 07:43,,,,,,,,1.16.0,,,,Documentation,,,,0,pull-request-available,,,,"For example:
{code:java}
The following example uses the example schema [testdata.avsc](https://github.com/apache/flink/blob/master/flink-formats/flink-parquet/src/test/resources/avro/testdata.avsc):
{code}
should be using the {{gh_link}} shortcode to automatically create the link to the the correct github branch:
{code:java}
The following example uses the example schema {{< gh_link file=""flink-formats/flink-parquet/src/test/resources/avro/testdata.avsc"" name=""testdata.avsc"" >}}:
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 19 07:43:36 UTC 2022,,,,,,,,,,"0|z17u2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 17:06;rskraba;Could you please assign this to me?;;;","19/Aug/22 07:43;chesnay;master: f6fbb8ac185bb3c3800e369f809ef31cd0426384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify Kryo limitations w.r.t. schema evolution,FLINK-29016,13477269,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Aug/22 16:26,24/Aug/22 07:40,04/Jun/24 20:41,24/Aug/22 07:40,,,,,,,,1.16.0,,,,API / Type Serialization System,,,,0,,,,,"While the schema evolution docs do state that Kryo isn't supported for schema evolution they don't really convey the gravity of the situation.
They should clarify that if Kryo is used for anything, even some data-structure containing POJOs (like an ArrayList, scala option/collection), that schema evolution will not be possible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29030,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 24 07:40:11 UTC 2022,,,,,,,,,,"0|z17u1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 07:40;chesnay;master: 4409d96514ba016bf79e1c2fbf79381013bd72f6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink stop with/without savepoint does not work with kinesis consumer,FLINK-29015,13477243,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,vvararu,vvararu,17/Aug/22 13:31,17/Aug/22 15:20,04/Jun/24 20:41,17/Aug/22 14:52,1.15.1,,,,,,,,,,,Connectors / Kinesis,,,,0,,,,,"Trying to migrate the job from Flink 1.14 to 1.15.1, observed that stop with/without savepoint  does not work anymore. 

The checkpointing/savepointing by themselves are working OK.

 

The exception when stopping the job seems to come from the kinesis connector.
{code:java}
2022-08-17 12:57:16,266 INFO  org.apache.flink.streaming.connectors.kinesis.internals.KinesisDataFetcher [] - Starting shutdown of shard consumer threads and AWS SDK resources of subtask 2 ...
java.lang.InterruptedException: sleep interrupted
    at java.lang.Thread.sleep(Native Method) ~[?:?]
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.adjustRunLoopFrequency(PollingRecordPublisher.java:216) ~[blob_p-956a630701b5709969a8029f6fefe9d3cf05a778-c2e9ace4f09a95dc67692581fea51a5b:?]
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:124) ~[blob_p-956a630701b5709969a8029f6fefe9d3cf05a778-c2e9ace4f09a95dc67692581fea51a5b:?]
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:102) ~[blob_p-956a630701b5709969a8029f6fefe9d3cf05a778-c2e9ace4f09a95dc67692581fea51a5b:?]
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114) [blob_p-956a630701b5709969a8029f6fefe9d3cf05a778-c2e9ace4f09a95dc67692581fea51a5b:?]
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [?:?]
    at java.util.concurrent.FutureTask.run(Unknown Source) [?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
    at java.lang.Thread.run(Unknown Source) [?:?]{code}",Flink 1.15.1. Same version for flink-connector-kinesis 1.15.1,,,,,,,,,,,,,,,,,,,FLINK-23528,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 17 15:20:08 UTC 2022,,,,,,,,,,"0|z17tvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 14:52;dannycranmer;This is a known issue and fixed in 1.15.2: 

- https://issues.apache.org/jira/browse/FLINK-23528;;;","17/Aug/22 15:20;vvararu;Thanks! Waiting for 1.15.2 :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use setting instead of merging for pipeline.jars in StreamExecutionEnvironment.configure,FLINK-29014,13477229,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,twalthr,twalthr,twalthr,17/Aug/22 11:24,23/Aug/22 12:26,04/Jun/24 20:41,23/Aug/22 12:26,,,,,,,,1.16.0,,,,API / DataStream,Table SQL / API,,,0,pull-request-available,,,,"See discussion in FLINK-28213. We should restore a ""setting"" behavior instead of a ""merging"" one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 23 12:26:51 UTC 2022,,,,,,,,,,"0|z17tsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 12:26;twalthr;Fixed in master: eb69b11532e6ac3fafd74849ed5f8a2c1273b71c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
