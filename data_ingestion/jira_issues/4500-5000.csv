Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Inward issue link (Dependent),Outward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Session window aggregation cannot trigger window using event time,FLINK-31013,13524188,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,qingyue,qingyue,10/Feb/23 09:52,29/Mar/23 02:00,04/Jun/24 20:41,29/Mar/23 02:00,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"{code:sql}
-- test against Flink 1.16.0

create catalog fscat with (
    'type' = 'table-store',
    'warehouse' = 'file:///tmp/fscat'
);


use catalog fscat;
create table events (
  `id` int, 
  `type` string, 
  `date` TIMESTAMP(3), 
  watermark for `date` AS `date`);
  
insert into events 
values (1, 'T1', to_timestamp('2018-01-24', 'yyyy-MM-dd')), 
(2, 'T1', to_timestamp('2018-01-26', 'yyyy-MM-dd')), 
(1, 'T2', to_timestamp('2018-01-28', 'yyyy-MM-dd')), 
(1, 'T2', to_timestamp('2018-01-28', 'yyyy-MM-dd'));  

-- no output
select `id`,
    `type`, 
    COUNT(1) as event_cnt, 
    session_start(`date`, interval '1' DAY) as ss, 
    session_end(`date`, interval '1' DAY) as se 
from events group by `id`, `type`, session(`date`, interval '1' DAY); 

-- explain plan
== Abstract Syntax Tree ==
LogicalProject(id=[$0], type=[$1], event_cnt=[$3], ss=[SESSION_START($2)], se=[SESSION_END($2)])
+- LogicalAggregate(group=[{0, 1, 2}], event_cnt=[COUNT()])
   +- LogicalProject(id=[$0], type=[$1], $f2=[$SESSION($2, 86400000:INTERVAL DAY)])
      +- LogicalWatermarkAssigner(rowtime=[date], watermark=[$2])
         +- LogicalTableScan(table=[[fscat, default, events]])


== Optimized Physical Plan ==
Calc(select=[id, type, event_cnt, w$start AS ss, w$end AS se])
+- GroupWindowAggregate(groupBy=[id, type], window=[SessionGroupWindow('w$, date, 86400000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[id, type, COUNT(*) AS event_cnt, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[hash[id, type]])
      +- TableSourceScan(table=[[fscat, default, events, watermark=[date]]], fields=[id, type, date])


== Optimized Execution Plan ==
Calc(select=[id, type, event_cnt, w$start AS ss, w$end AS se])
+- GroupWindowAggregate(groupBy=[id, type], window=[SessionGroupWindow('w$, date, 86400000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[id, type, COUNT(*) AS event_cnt, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[hash[id, type]])
      +- TableSourceScan(table=[[fscat, default, events, watermark=[date]]], fields=[id, type, date])

-- however, if switch to filesystem source, the window can be triggered normally

CREATE TEMPORARY TABLE `fscat`.`default`.`event_file_source` (
  `id` INT,
  `type` VARCHAR(2147483647),
  `date` TIMESTAMP(3),
  WATERMARK FOR `date` AS `date`
) WITH (
  'format' = 'csv',
  'path' = '/tmp/events.csv',
  'source.monitor-interval' = '1 min',
  'connector' = 'filesystem'
);

// cat events.csv                                
1,T1,2018-01-24 00:00:00.000
2,T1,2018-01-26 00:00:00.000
1,T2,2018-01-28 00:00:00.000
1,T2,2018-01-28 00:00:00.000


-- same query using filesystem source
select `id`, `type`, COUNT(1) as event_cnt, session_start(`date`, interval '1' DAY) as ss, session_end(`date`, interval '1' DAY) as se from event_file_source group by `id`, `type`, session(`date`, interval '1' DAY);

-- output

          id                           type            event_cnt                      ss                      se
           1                             T1                    1 2018-01-24 00:00:00.000 2018-01-25 00:00:00.000
           2                             T1                    1 2018-01-26 00:00:00.000 2018-01-27 00:00:00.000{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 06:49:49 UTC 2023,,,,,,,,,,"0|z1fud4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 01:43;xzw0223;I refer you to the documentation [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/sql/queries/window-agg/#group-window-aggregation]  on Windows

Instead of dividing the window by the specified interval, the sessionwindow will fire only after the specified interval window has no data and is inactive.

 

This is your data
1,T1,2018-01-24 00:00:00.000
2,T1,2018-01-26 00:00:00.000
1,T2,2018-01-28 00:00:00.000
1,T2,2018-01-28 00:00:00.000
 

When the first data is accepted and the second data is received, the second data event time is longer than the activity interval of the window, then the window will be triggered.

When three pieces of data are received, his event event is greater than the window interval, triggering the window calculation.

When the fourth piece of data is received, it has the same time as the third piece of data, and they will be divided into the same sessionwindow. Since no new data is continuously received, the sessionwindow will not trigger calculation until the received data It will only be triggered when the event time of is greater than the window interval.

 

 ;;;","17/Feb/23 06:49;qingyue;[~xzw0223] What's your point?


According to the table schema
||id||type||date||
|1|T1|2018-01-24 00:00:00.000|
|2|T1|2018-01-26 00:00:00.000|
|1|T2|2018-01-28 00:00:00.000|
|1|T2|2018-01-28 00:00:00.000|

 
And the query with a session window gap of 1 DAY, the first two windows should be triggered.
{code:sql}
select `id`,
    `type`, 
    COUNT(1) as event_cnt, 
    session_start(`date`, interval '1' DAY) as ss, 
    session_end(`date`, interval '1' DAY) as se 
from events group by `id`, `type`, session(`date`, interval '1' DAY); 
{code}
 

Expected Output
||id||type||event_cnt||ss||se||
|1|T1|1|2018-01-24 00:00:00.000|2018-01-25 00:00:00.000|
|2|T1|1|2018-01-26 00:00:00.000|2018-01-27 00:00:00.000|

 

Table Store Actual Output

No output;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update docs for files table in table store,FLINK-31012,13524187,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,10/Feb/23 09:44,10/Feb/23 10:42,04/Jun/24 20:41,10/Feb/23 10:42,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Update docs to add partition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 10:42:41 UTC 2023,,,,,,,,,,"0|z1fucw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 10:42;lzljs3620320;master: d325dcef41c6d3dc815189983abded5cf1b19830;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade hiverunner version,FLINK-31011,13524184,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhangjun,zhangjun,10/Feb/23 09:31,29/Mar/23 02:02,04/Jun/24 20:41,29/Mar/23 02:02,table-store-0.3.1,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"The current HiveRunner Test framework (version 4) has some bugs. for example, when we rename a table, it does not rename the location, I test that, the last version (6) is fine, so we should upgrade the HiveRunner to last version ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-10 09:31:33.0,,,,,,,,,,"0|z1fuc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for GBTClassifier and GBTRegressor,FLINK-31010,13524180,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hongfanxo,hongfanxo,10/Feb/23 08:56,19/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,auto-deprioritized-major,pull-request-available,,,"Add Transformer and Estimator for GBTClassifier and GBTRegressor.

They are supposed to cover most features in SparkML provides, except for the following features:

 
 # Support early-stopping with validation set.
 # Support weights.
 # Support leaf ID.
 # Support feature importance.
 # Support 1-order gradient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:08 UTC 2023,,,,,,,,,,"0|z1fubc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add recordCount to snapshot meta,FLINK-31009,13524175,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,10/Feb/23 08:07,27/Feb/23 08:39,04/Jun/24 20:41,27/Feb/23 08:39,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Record count represents the total number of data records. It is simply added by the number of data records of all files.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 08:39:40 UTC 2023,,,,,,,,,,"0|z1fua8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 05:29;nicholasjiang;[~lzljs3620320], I'm working for this feature. Please assign to me.;;;","27/Feb/23 08:39;lzljs3620320;master: d79b39a63bcd550a516d6b6caa8e9e8a092dac01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink][Table Store] The Split allocation of the same bucket in ContinuousFileSplitEnumerator may be out of order,FLINK-31008,13524166,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,10/Feb/23 07:27,13/Feb/23 11:31,04/Jun/24 20:41,13/Feb/23 11:31,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"There are two places in {{ContinuousFileSplitEnumerator}} that add {{FileStoreSourceSplit}} to {{{}bucketSplits{}}}: {{addSplitsBack}} and {{{}processDiscoveredSplits{}}}. {{processDiscoveredSplits}} will continuously check for new splits and add them to the queue.  At this time, the order of the splits is in order.
{code:java}
private void addSplits(Collection<FileStoreSourceSplit> splits) {
    splits.forEach(this::addSplit);
}

private void addSplit(FileStoreSourceSplit split) {
    bucketSplits
            .computeIfAbsent(((DataSplit) split.split()).bucket(), i -> new LinkedList<>())
            .add(split);
}{code}
However, when the task failover, the splits that have been allocated before will be returned. At this time, these returned splits are also added to the end of the queue, which leads to disorder in the allocation of splits.

 

I think these returned splits should be added to the head of the queue to ensure the order of allocation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 11:31:13 UTC 2023,,,,,,,,,,"0|z1fu88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 07:35;Ming Li;hi, [~lzljs3620320], if you have time, please help to take a look at this issue, thank you.;;;","10/Feb/23 07:38;lzljs3620320;[~Ming Li] Thanks for reporting!
Wow, you're right. This should be a blocker issue.
Do you want to contribute this jira? ;;;","10/Feb/23 07:43;Ming Li;[~lzljs3620320] Yes, thanks a lot, I've been doing some related work recently that I think will help fix this issue.

In addition, if you have time, please help to take a look at this issue([FLINK-30985|https://issues.apache.org/jira/browse/FLINK-30985]), I think it is also helpful for split allocation. ;;;","10/Feb/23 08:59;lzljs3620320;[~Ming Li] Assigned to u~;;;","10/Feb/23 12:06;Ming Li;[~lzljs3620320] I have created a pull request, please review it if you have time. Thanks.;;;","13/Feb/23 11:31;lzljs3620320;master: e9875b4d0bb9d3ccd05b51349ffe7a1ea78510e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The code generated by the IF function throws NullPointerException,FLINK-31007,13524163,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,xzw0223,tivan,tivan,10/Feb/23 07:01,14/Apr/23 01:53,04/Jun/24 20:41,14/Apr/23 01:53,1.15.2,1.15.3,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,,,,,"Caused by: java.lang.NullPointerException
    at StreamExecCalc$19.processElement_split1(Unknown Source)
    at StreamExecCalc$19.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at org.apache.flink.table.runtime.operators.source.InputConversionOperator.processElement(InputConversionOperator.java:128)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
    at org.apache.flink.streaming.api.functions.source.FromElementsFunction.run(FromElementsFunction.java:231)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)","{code:java}
// code placeholder
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

final DataStream<Tuple2> tab =
        env.fromCollection(Arrays.asList(
                new Tuple2<>(1L, ""a_b_c""),
                new Tuple2<>(-1L, ""a_b_c"")));

final Table tableA = tableEnv.fromDataStream(tab);

tableEnv.executeSql(""SELECT if(f0 = -1, '', split_index(f1, '_', 0)) as id FROM "" + tableA)
        .print(); {code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-31653,,,,FLINK-30559,,,,,,,,,,,,,,,"10/Feb/23 07:18;tivan;StreamExecCalc$19.java;https://issues.apache.org/jira/secure/attachment/13055330/StreamExecCalc%2419.java","10/Feb/23 09:20;xzw0223;image-2023-02-10-17-20-51-619.png;https://issues.apache.org/jira/secure/attachment/13055338/image-2023-02-10-17-20-51-619.png","13/Feb/23 10:24;xzw0223;image-2023-02-13-18-24-08-380.png;https://issues.apache.org/jira/secure/attachment/13055395/image-2023-02-13-18-24-08-380.png","13/Feb/23 10:25;xzw0223;image-2023-02-13-18-25-32-357.png;https://issues.apache.org/jira/secure/attachment/13055396/image-2023-02-13-18-25-32-357.png","13/Feb/23 10:27;xzw0223;image-2023-02-13-18-27-27-856.png;https://issues.apache.org/jira/secure/attachment/13055397/image-2023-02-13-18-27-27-856.png",,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 06:50:08 UTC 2023,,,,,,,,,,"0|z1fu7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 09:22;xzw0223;I found out why.
!image-2023-02-10-17-20-51-619.png!

In the generated if function, this line throws an NullPointerException ,because result$9 is equal to null.

 ;;;","10/Feb/23 09:25;xzw0223;I think I can solve this problem, can this be assigned to me.
Thanks;;;","10/Feb/23 20:40;martijnvisser;[~lincoln.86xy] WDYT?;;;","13/Feb/23 06:26;lincoln.86xy;This is a bug for IfCallGen, all operands initialization and null check should be processed well.

[~xzw0223] assigned to you, welcome for contributing!;;;","13/Feb/23 10:31;xzw0223;[~lincoln.86xy]  Hello, I would like to share with you the results of my investigation.

The specific reason is that the return result of true_value is char (0) type, and the false type is string type.



The specific reason is that the return result of true_value is char (0) type, and the false type is string type, which leads to type inconsistency, because there are some conflicts between the conversion type code generated in the middle and the previous judgment code, resulting in a NullPointerException problem when judging.

 

If the return type of true_value is char(0), and the result we return is false_value, the following situations will occur:
eg1.
  if(1=2, '' , 'abc' ) return ===> result =''
eg2.
  if(1=2, 'a' , 'cde' ) return ===> result ='c'

He will return according to the field type of true_value.

nit : For this situation, I think it is wrong, it will affect our normal results.

 

In my test, I found a problem. If the true_value and false_value types are the same, there will be no NullPointerException problem,the cause of the problem was explained at the very beginning.

 

Below is the result of the code in my test.

 

Compare Figure 1

!image-2023-02-13-18-24-08-380.png|width=658,height=263!

!image-2023-02-13-18-25-32-357.png|width=675,height=256!

Compare Figure 2

!image-2023-02-13-18-27-27-856.png|width=580,height=209!

 

 

My idea is that I think it is necessary to unify the return types of true_value and false_value, because different type conversions may lead to incorrect data, such as converting long to int may cause loss of precision

[~lincoln.86xy]  I want to hear your opinions and opinions, thank you.

 

 ;;;","13/Feb/23 14:01;lincoln.86xy;[~xzw0223] thank you for investigating this! I found other two related issues(FLINK-30559 & FLINK-30966), please help to check if these issues cover your case;;;","14/Feb/23 01:09;xzw0223;[~lincoln.86xy]  Yes, the problem is the same, the reason is because of a bug in the func generated by IfCallGen;;;","14/Feb/23 06:46;lincoln.86xy;[~xzw0223] thank you for confirming this issue! Would you mind marking this jira duplicated since the other two have smaller jira number? And you can continue help reviewing/contributing their patches.;;;","14/Feb/23 06:50;xzw0223;[~lincoln.86xy]  Ok, no problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
job is not finished when using pipeline mode to run bounded source like kafka/pulsar,FLINK-31006,13524153,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,10/Feb/23 05:24,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,,,1.20.0,,,,,,,,,,,,,,,,Connectors / Kafka,Connectors / Pulsar,,,,0,pull-request-available,stale-assigned,,,"when i do failover works like kill jm/tm when using  pipeline mode to run bounded source like kafka, i found job is not finished, when every partition data has consumed.

 

After dig into code, i found this logical not run when JM recover. the partition infos are not changed. so noMoreNewPartitionSplits is not set to true. then this will not run 

 

!image-2023-02-10-13-23-38-430.png!

 

!image-2023-02-10-13-24-46-929.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/23 05:20;jackylau;image-2023-02-10-13-20-52-890.png;https://issues.apache.org/jira/secure/attachment/13055328/image-2023-02-10-13-20-52-890.png","10/Feb/23 05:23;jackylau;image-2023-02-10-13-23-38-430.png;https://issues.apache.org/jira/secure/attachment/13055327/image-2023-02-10-13-23-38-430.png","10/Feb/23 05:24;jackylau;image-2023-02-10-13-24-46-929.png;https://issues.apache.org/jira/secure/attachment/13055326/image-2023-02-10-13-24-46-929.png","03/Mar/23 17:04;taoran;image-2023-03-04-01-04-18-658.png;https://issues.apache.org/jira/secure/attachment/13056018/image-2023-03-04-01-04-18-658.png","03/Mar/23 17:05;taoran;image-2023-03-04-01-05-25-335.png;https://issues.apache.org/jira/secure/attachment/13056019/image-2023-03-04-01-05-25-335.png","03/Mar/23 17:07;taoran;image-2023-03-04-01-07-04-927.png;https://issues.apache.org/jira/secure/attachment/13056020/image-2023-03-04-01-07-04-927.png","03/Mar/23 17:07;taoran;image-2023-03-04-01-07-36-168.png;https://issues.apache.org/jira/secure/attachment/13056021/image-2023-03-04-01-07-36-168.png","03/Mar/23 17:08;taoran;image-2023-03-04-01-08-29-042.png;https://issues.apache.org/jira/secure/attachment/13056022/image-2023-03-04-01-08-29-042.png","03/Mar/23 17:09;taoran;image-2023-03-04-01-09-24-199.png;https://issues.apache.org/jira/secure/attachment/13056023/image-2023-03-04-01-09-24-199.png",,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 16:45:32 UTC 2024,,,,,,,,,,"0|z1fu5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 20:42;martijnvisser;[~renqs] WDYT?;;;","13/Feb/23 02:19;syhily;Hi [~jackylau], can you give me a clear reproduce steps on using {{flink-connector-pulsar}}?;;;","13/Feb/23 02:29;syhily;No more splits is only used in Pulsar connector for discovering the topic partitions change only once. The stop of the pipeline is only determined by {{StopCurosr}}. After reading the whole issue you submitted. I don't think this is a bug in {{flink-connector-pulsar}}. I'm still waiting for your further reply.;;;","03/Mar/23 18:16;taoran;[~syhily] [~renqs] hi, guys. I think Qingsheng's respond in github([https://github.com/apache/flink/pull/21909).] is right. I got another problem and found this case.

1.Kafka source runs in bounded mode.
2.The enumerator starts and noMoreNewPartitionSplits is set to true, then the initial partition discovery is triggered.
3.As the partition discovery runs asynchronously in the worker thread, it's possible that a reader can register on the enumerator before the partition discovery finishes.

If we set `noMoreNewPartitionSplits = true;` when partitionDiscovery is disabled.
because *context.callAsync* to get partitions is a async call. If call is not finished, it will quit early. it means we can not consume bounded source. However we expect it should read first partitions and consume and then quit.

so, i think currently the PR can not work. however the issue may be a special case like race condition we should keep on eye.;;;","05/Mar/23 09:08;syhily;[~lemonjing] Yep, I agree with you.;;;","24/Mar/23 21:00;tzulitai;Is this subsumed by https://issues.apache.org/jira/browse/FLINK-31319 (merged)? If yes, can we close this ticket as a duplicate?;;;","25/Mar/23 04:19;taoran;[~tzulitai] Not the same problem, this issue is the problem that noMoreSplit was not reset during jm fo. [~jackylau] because kafka in flink main repo is code freezing, could you close this PR and create a new one in the flink-connector-kafka repo if you reproduce it. WDYT?;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","08/Jan/24 16:31;jwdk;Hi!

Is this issue is still being worked on? I am still able to reproduce it and would like to work on a solution.

I have created a suggestion as a pull request [https://github.com/apache/flink-connector-kafka/pull/71] but would very much like to open up a new round of discussions?;;;","08/Jan/24 16:45;martijnvisser;[~tzulitai] Can you take a look at the PR?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-281 Supports speculative execution of sinks,FLINK-31005,13524149,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,wanglijie,zhuzh,zhuzh,10/Feb/23 04:01,21/Feb/23 07:54,04/Jun/24 20:41,21/Feb/23 07:54,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,"This task aims to verify [FLIP-281 Supports speculative execution of sinks|https://issues.apache.org/jira/browse/FLINK-30725].
The documentation can be found [here|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/speculative_execution/#enable-sinks-for-speculative-execution] .

Things to verify:
1. If a sink implements the decorative interface {{SupportsConcurrentExecutionAttempts}, Speculative executions can be performed, otherwise not. Sinks to verify includes SinkFunction, OutputFormat and Sink(V2).
2. These built-in sinks supports speculative execution: DiscardingSink, PrintSinkFunction, PrintSink, FileSink, FileSystemOutputFormat, HiveTableSink

It's important to verify that the final data produced are correct and the operator chain is as expected, i.e. no unexpected chain-break.

If it's hard to construct a case that speculative execution would happen, especially for those built-in sinks, the speculative execution configuration can be tuned to allow it easier to happen, e.g. set {{slow-task-detector.execution-time.baseline-lower-bound}} and {{slow-task-detector.execution-time.baseline-ratio}} to {{0}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30725,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 07:54:06 UTC 2023,,,,,,,,,,"0|z1fu4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 08:11;wanglijie;I' ll take this ticket :D;;;","20/Feb/23 10:02;wanglijie;I have tested all cases in the description through a simple word count job, everything met expectations.

1. I tested 6 kind of sinks:
 * speculative SinkFunction
 * non-speculative SinkFunction
 * speculative OutputFormat
 * non-speculative OutputFormat
 * speculative SinkV2
 * non-speculative SinkV2

and let task 0 significantly slower than other tasks by adding sleep in the code. Finally, in the case of {{speculative SinkFunction}} {{speculative OutputFormat}} and {{{}speculative SinkV2{}}}, task 0 was detected as a slow vertex, and Flink created a new speculative execution for it.


2. I ran all the built-in sinks: DiscardingSink, PrintSinkFunction, PrintSink, FileSink and HiveTableSink (depending on FileSystemOutputFormat), and confirmed that they all support speculative execution. I checked the results of FileSink and HiveTableSink, and the results are as expected (same as without speculative execution).;;;","21/Feb/23 07:54;zhuzh;Thanks for helping with the testing! [~wanglijie];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce data input and output stream for table store,FLINK-31004,13524148,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,10/Feb/23 03:58,11/Feb/23 09:06,04/Jun/24 20:41,11/Feb/23 09:06,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Introduce data input/output stream for table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 11 09:06:14 UTC 2023,,,,,,,,,,"0|z1fu4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 09:06;lzljs3620320;master: 03f1885c3e6e6e6af7931357780cbebe85fe9a8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL IF / CASE WHEN Funcation incorrect,FLINK-31003,13524147,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,weiqinpan,weiqinpan,10/Feb/23 03:57,13/Feb/23 08:31,04/Jun/24 20:41,13/Feb/23 08:31,1.15.0,1.15.1,1.15.2,1.15.3,1.16.0,1.16.1,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"When I execute the below sql using sql-client，i found something wrong.

 
{code:java}
CREATE TEMPORARY TABLE source (
  mktgmsg_biz_type STRING,
  marketing_flow_id STRING,
  mktgmsg_campaign_id STRING
)
WITH
(
  'connector' = 'filesystem',
  'path' = 'file:///Users/xxx/Desktop/demo.json',
  'format' = 'json'
); 

-- return correct value('marketing_flow_id') 
SELECT IF(`marketing_flow_id` IS NOT NULL, `marketing_flow_id`, '') FROM source;

-- return incorrect value('')
SELECT IF(`marketing_flow_id` IS  NULL, '', `marketing_flow_id`) FROM source;{code}
The demo.json data is 

 
{code:java}
{""mktgmsg_biz_type"": ""marketing_flow"", ""marketing_flow_id"": ""marketing_flow_id"", ""mktgmsg_campaign_id"": ""mktgmsg_campaign_id""} {code}
 

 

BTW, use case when + if / ifnull also have something wrong.

 
{code:java}
-- return wrong value(''), expect return marketing_flow_id
select CASE
  WHEN `mktgmsg_biz_type` = 'marketing_flow'     THEN IF(`marketing_flow_id` IS NULL, `marketing_flow_id`, '')
  WHEN `mktgmsg_biz_type` = 'mktgmsg_campaign'   THEN IF(`mktgmsg_campaign_id` IS NULL, '', `mktgmsg_campaign_id`)
  ELSE ''
  END AS `message_campaign_instance_id` FROM source;

-- return wrong value('')
select CASE
  WHEN `mktgmsg_biz_type` = 'marketing_flow'     THEN IFNULL(`marketing_flow_id`, '')
  WHEN `mktgmsg_biz_type` = 'mktgmsg_campaign'   THEN IFNULL(`mktgmsg_campaign_id`, '')
  ELSE ''
  END AS `message_campaign_instance_id` FROM source;

-- return correct value, the difference is [else return ' ']
select CASE
  WHEN `mktgmsg_biz_type` = 'marketing_flow'     THEN IFNULL(`marketing_flow_id`, '')
  WHEN `mktgmsg_biz_type` = 'mktgmsg_campaign'   THEN IFNULL(`mktgmsg_campaign_id`, '')
  ELSE ' '
  END AS `message_campaign_instance_id` FROM source;
{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 02:44:56 UTC 2023,,,,,,,,,,"0|z1fu48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 03:59;weiqinpan;Only find this in Flink 1.15.0 +, everything is right before 1.15.0.;;;","10/Feb/23 15:13;martijnvisser;[~lincoln.86xy] WDYT?;;;","10/Feb/23 16:15;weiqinpan;The value of marketing_flow_id is not null, but the result  of below sql  is  empty. So unbelievable.
{code:java}
SELECT IF(`marketing_flow_id` IS  NULL, '', `marketing_flow_id`) FROM source;
{code};;;","11/Feb/23 02:50;csq;Hi [~martijnvisser][~weiqinpan], I think it is the same issue as [FLINK-30966 |https://issues.apache.org/jira/browse/FLINK-30966], that
when normalizing arguments in IfCallGen, it always align to the type of ARG1, like IF(1 > 2, 'true', 'false')
the result will be string 'fals' which length is the same as 'true'.;;;","13/Feb/23 01:56;weiqinpan;BTW, CASE WHEN + IFNULL also have logic problem.;;;","13/Feb/23 02:35;luoyuxia;Hi, everyone. For me,  it seems the same issue of FLINK-30559. for which a pr is available.

Is that right?;;;","13/Feb/23 02:44;csq;[~luoyuxia] +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide data sampling query,FLINK-31002,13524128,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,10/Feb/23 01:45,19/Mar/23 05:39,04/Jun/24 20:41,19/Mar/23 05:39,,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"Want to take several randomly from each partition, but the limit is always fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:39:09 UTC 2023,,,,,,,,,,"0|z1fu00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 08:42;zhangjun;I test that. ,and found  this function has been implemented by spark sql .like 
{code:java}
select * from word_count TABLESAMPLE (50 PERCENT)
or
select * from word_count TABLESAMPLE (BUCKET 4 OUT OF 10){code};;;","10/Feb/23 11:03;lzljs3620320;[~zhangjun] I think we can implement this by table option.;;;","19/Mar/23 05:39;lzljs3620320;this should be implemented by flink sql;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Hive writer,FLINK-31001,13524126,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,10/Feb/23 01:44,29/Mar/23 03:10,04/Jun/24 20:41,29/Mar/23 03:10,,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-10 01:44:37.0,,,,,,,,,,"0|z1ftzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade test units in flink-table-store-common to junit5,FLINK-31000,13524125,13516872,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,10/Feb/23 01:44,22/Mar/23 02:20,04/Jun/24 20:41,22/Mar/23 02:20,table-store-0.4.0,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-10 01:44:19.0,,,,,,,,,,"0|z1ftzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce flink-table-store-test-utils for table store,FLINK-30999,13524124,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,10/Feb/23 01:41,10/Feb/23 10:48,04/Jun/24 20:41,10/Feb/23 10:48,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Introduce flink-table-store-test-utils module for table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 10:48:02 UTC 2023,,,,,,,,,,"0|z1ftz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 10:48;lzljs3620320;master: 065328efea4f5efc27ac6f6de8c663cf6266aa24;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add optional exception handler to flink-connector-opensearch,FLINK-30998,13524091,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lilyevsky,lilyevsky,lilyevsky,09/Feb/23 21:13,03/Nov/23 09:05,04/Jun/24 20:41,11/Jul/23 23:11,1.16.1,,,,,,opensearch-1.1.0,,,,,,,,,,,,,,,,Connectors / Opensearch,,,,,0,pull-request-available,,,,"Currently, when there is a failure coming from Opensearch, the FlinkRuntimeException is thrown from OpensearchWriter.java code (line 346). This makes the Flink pipeline fail. There is no way to handle the exception in the client code.

I suggest to add an option to set a failure handler, similar to the way it is done in elasticsearch connector. This way the client code has a chance to examine the failure and handle it.

Here is the use case example when it will be very useful. We are using streams on Opensearch side, and we are setting our own document IDs. Sometimes these IDs are duplicated; we need to ignore this situation and continue (this way it works for us with Elastisearch).
However, with opensearch connector, the error comes back, saying that the batch failed (even though most of the documents were indexed, only the ones with duplicated IDs were rejected), and the whole flink job fails.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31068,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 23:36:13 UTC 2023,,,,,,,,,,"0|z1ftrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 22:24;lilyevsky;I now found that previously the required functionality was there in the org.apache.flink.streaming.connectors.opensearch package, but it is all now deprecated and not quite usable, because it indeed is missing other important setter functions on the builder object.

So the issue is about moving the ActionRequestFailureHandler interface to the current org.apache.flink.connector.opensearch.sink package and implementing the related logic.;;;","09/Feb/23 23:20;lilyevsky;I actually looked at the code related to ActionRequestFailureHandler in org.apache.flink.streaming.connectors.opensearch package.

It is quite complicated, processing every single element there. Maybe this is why it was not yet ported to the new package.

I think it does not have to be this complicated. Maybe a simple interface like this will do:



public interface FailureHandler extends Serializable {

    void onFailure(Throwable failure);
}

 

I even tried to implement it myself, it looks fine; only that in my environment I have issues cleanly build the whole project (failing tests, some quarantined dependencies, etc.). I used the [dependabot/maven/flink-connector-opensearch/org.opensearch-opensearch-2.5.0|https://github.com/apache/flink-connector-opensearch/tree/dependabot/maven/flink-connector-opensearch/org.opensearch-opensearch-2.5.0] branch because I need support for Opensearch 2.
So maybe somebody can technically do these enhancements, I would really appreciate that. We can work together, I can share the code fragments that I changed. ;;;","10/Feb/23 15:01;lilyevsky;This morning I successfully tested my solution. The changes overall are pretty minor.

Please let's discuss so we can implement it.;;;","10/Feb/23 20:43;martijnvisser;[~reta] WDYT?;;;","10/Feb/23 21:09;reta;[~martijnvisser] [~lilyevsky] sure, happy to help here;;;","11/Feb/23 02:24;lilyevsky;[~reta] Thanks, please let me know how to submit my change. I can do it on Monday.;;;","11/Feb/23 02:43;reta;[~lilyevsky] thanks, you could send the pull request to [https://github.com/apache/flink-connector-opensearch/] and we could work it through. Any other options are more comfortable for you? Thanks!;;;","12/Feb/23 00:37;lilyevsky;[~reta] here is my pull request: [https://github.com/apache/flink-connector-opensearch/pull/8] .

Please let me know what is next. If you take my changes in the branch, I guess, you will do a build. I would like to get that artifact and re-test it in my environment, just to make sure nothing got broken.

Also I noticed on the branch you still have Flink version as 1.16.0, while in main it is 1.16.1, so probably you are gong to correct that.

Also question: are you going to maintain two variants of this connector? One for Opensearch 1.3.0 and another for 2.5.0? I see that the differences between the branches are very minor. ;;;","13/Feb/23 13:15;reta;Thanks [~lilyevsky] , looking into it

 

> Also I noticed on the branch you still have Flink version as 1.16.0, while in main it is 1.16.1, so probably you are gong to correct that.

Hm ... this should not be the case [https://github.com/apache/flink-connector-opensearch/commit/17f5fcafdb393b0b460cbe5e56906e24576221f1] , could you please point out where you still see 1.16.0?

 

> Also question: are you going to maintain two variants of this connector? One for Opensearch 1.3.0 and another for 2.5.0? 

The OpenSearch connector should work with 1.x and 2.x clusters. The 1.x is the baseline since 2.x clients need JDK-11 at least, Apache Flink has JDK-8 baseline. ;;;","13/Feb/23 14:21;lilyevsky;Hi [~reta] ,

Thanks for your help.

The commit that you mentioned where you upgraded 1.16.0 to 1.16.1 was applied to main branch only, the [https://github.com/apache/flink-connector-opensearch/blob/dependabot/maven/flink-connector-opensearch/org.opensearch-opensearch-2.5.0/pom.xml] still shows 1.16.0. I guess, this is not a problem, as ultimately you are going to merge all updates to main branch.

I know that the connector built from main branch will work with 2.x cluster, but only if in my project I explicitly upgrade opensearch version to 2.5.0 . Again, this is not a problem at all. Maybe you just should mention this fact in README.

Without setting opensearch version to 2.5.0 it failed on parsing some responses from the cluster, complaining about one missing field.

Also, a minor technical difficulty with unit tests. As I mentioned, I had to do some small fixes there to make it compile with opensearch 2.5.0.;;;","13/Feb/23 14:41;reta;Hi [~lilyevsky] 

Ah, the dependabot pull request has to be closed (I sadly cannot do that) [~martijnvisser] could please help here?

> I know that the connector built from main branch will work with 2.x cluster, but only if in my project I explicitly upgrade opensearch version to 2.5.0 . Again, this is not a problem at all. Maybe you just should mention this fact in README.

This is a good idea, I will update the documentation & README.md, thank you.

> Without setting opensearch version to 2.5.0 it failed on parsing some responses from the cluster, complaining about one missing field.

Sorry, I didn't get the context for this one. You mean using 1.x client (the connector's default) with OpenSearch 2.x cluster, is that right?

> Also, a minor technical difficulty with unit tests. As I mentioned, I had to do some small fixes there to make it compile with opensearch 2.5.0.

That is correct, it has been discussed here [https://github.com/apache/flink-connector-opensearch/pull/4#issuecomment-1370984907|https://github.com/apache/flink-connector-opensearch/pull/4#issuecomment-1370984907,] , the dependency of unit tests should not impact the connector;;;","13/Feb/23 21:33;lilyevsky;Hi [~reta] ,

> Sorry, I didn't get the context for this one. You mean using 1.x client (the connector's default) with OpenSearch 2.x cluster, is that right?

Correct. The 1.x client when used against 2.x server failed on parsing the response. I fixed it in my project by explicitly specifying 2.5.0 dependency.;;;","15/Feb/23 13:36;lilyevsky;[~reta] Please let me know if you need me to fix anything in the PR [https://github.com/apache/flink-connector-opensearch/pull/9] .;;;","15/Feb/23 23:01;lilyevsky;[~reta] here is the new PR: [https://github.com/apache/flink-connector-opensearch/pull/11] .

Hopefully this looks cleaner. Please let me know if I missed anything.;;;","11/Jul/23 16:37;martijnvisser;Fixed in:

apache/flink-connector-opensearch:main d853e3d6be3e0f15e25c1220800b7d5fcf152c43
apache/flink-connector-opensearch:v1.0 a7f0ade240dbba04a41ae793684ede4285ca959b;;;","02/Aug/23 13:41;leonidilyevsky;[~reta] [~martijnvisser]  Thanks for merging my PR. What are the plans for releasing the latest version?;;;","07/Aug/23 23:36;reta;Thank you for contribution, [~leonidilyevsky] ! Please refer to [1] that describes the release process for connectors, short brief summary below (you probably could nominate [~martijnvisser] since I am not a committer):

> Anybody can propose a release on the dev@ mailing list, giving a solid argument and nominating a committer as the Release Manager (including themselves).

Thank you.

[1] https://cwiki.apache.org/confluence/display/FLINK/Creating+a+flink-connector+release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor tests in connector to extends AbstractTestBase,FLINK-30997,13523993,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,09/Feb/23 14:52,14/Feb/23 01:30,04/Jun/24 20:41,14/Feb/23 01:30,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Refactor tests in connector to extends `AbstractTestBase`,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 01:30:33 UTC 2023,,,,,,,,,,"0|z1ft60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 01:30;lzljs3620320;master: 74cc7ce1e516e96349021fa7a4c96f2e992d1b28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync Kafka 1.17 commits from apache/flink repo to flink-connector-kafka,FLINK-30996,13523990,13503028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,09/Feb/23 14:29,12/Apr/23 17:01,04/Jun/24 20:41,10/Mar/23 07:35,kafka-4.0.0,,,,,,kafka-3.0.0,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 07:55:10 UTC 2023,,,,,,,,,,"0|z1ft5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 07:32;mason6345;[~martijnvisser] Looks like this is done. Should we work on https://issues.apache.org/jira/browse/FLINK-30859 to remove from master/release-1.17? I can help if needed;;;","10/Mar/23 07:35;martijnvisser;[~mason6345] Oh duh, I forgot to close this. Yes, let's work on FLINK-30859. Do you want to do it and I review, or do you want me to do it and you review?;;;","10/Mar/23 07:35;martijnvisser;Fixed via https://github.com/apache/flink-connector-kafka/pull/4;;;","10/Mar/23 07:55;mason6345;Let's save you some time ;). I'll help do that–I have some spare cycles. This will effect the Flink 1.17 rc1 btw;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ByteSerializer for Table Store,FLINK-30995,13523986,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,09/Feb/23 14:16,11/Feb/23 08:34,04/Jun/24 20:41,11/Feb/23 08:34,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,Introduce ByteSerializer for Table Store{+}{+},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 11 08:34:45 UTC 2023,,,,,,,,,,"0|z1ft4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 08:34;lzljs3620320;master: 677313714323a2f745cfe15497e7de7c8f866069;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce DoubleSerializer for Table Store,FLINK-30994,13523985,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,09/Feb/23 14:15,11/Feb/23 08:35,04/Jun/24 20:41,11/Feb/23 08:35,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,Introduce DoubleSerializer for Table Store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 11 08:35:36 UTC 2023,,,,,,,,,,"0|z1ft48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 08:35;lzljs3620320;master: 8363dc6f57ae053f9b34cf7403c734b9e6c66a3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FloatSerializer for Table Store,FLINK-30993,13523984,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,09/Feb/23 14:15,11/Feb/23 04:30,04/Jun/24 20:41,11/Feb/23 04:30,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,28/Feb/23 00:00,0,,,,,Introduce FloatSerializer for Table Store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 11 04:30:57 UTC 2023,,,,,,,,,,"0|z1ft40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 04:30;lzljs3620320;master: c62992cef568274ec777c2d6e264a12f077b0925;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ShortSerializer for Table Store,FLINK-30992,13523983,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,09/Feb/23 14:14,11/Feb/23 04:30,04/Jun/24 20:41,11/Feb/23 04:30,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,Introduce ShortSerializer for Table Store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 11 04:30:05 UTC 2023,,,,,,,,,,"0|z1ft3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 04:30;lzljs3620320;master: f285db56b8a440ef4ef08d15b5a48d58b7c10e01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce LongSerializer for Table Store,FLINK-30991,13523982,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,09/Feb/23 14:11,11/Feb/23 04:29,04/Jun/24 20:41,11/Feb/23 04:29,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,28/Feb/23 00:00,0,,,,,Introduce LongSerializer for Table Store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Sat Feb 11 04:29:13 UTC 2023,,,,,,,,,,"0|z1ft3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Feb/23 04:29;lzljs3620320;master: 430df6c595c229a7128a43c5a1dc831d3f91f905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance of deployDownstreamTasks.BATCH has worsened,FLINK-30990,13523970,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Not A Problem,,martijnvisser,martijnvisser,09/Feb/23 12:42,10/Feb/23 03:34,04/Jun/24 20:41,09/Feb/23 13:30,1.17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"See http://codespeed.dak8s.net:8000/timeline/#/?exe=8&ben=deployDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200

It appears that the trend has now stabilized upwards, from 60-70 ms/op to 80-90 ms/op, while less is better. 

Not sure if this has meaningful impact in a production setup, but I would like to get this verified. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 13:28:44 UTC 2023,,,,,,,,,,"0|z1ft0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 12:51;mapohl;[~zhuzh] can you have a look at this?;;;","09/Feb/23 13:17;zhuzh;Thanks for reporting!

This regression should have been caused by some recent improvements to the scheduler. 

This is not a blocker to take 80-90ms to deploy 4000 tasks. As a reference, before the improvement to the scheduling performance(FLINK-21110), the number of this benchmark is tens of seconds.;;;","09/Feb/23 13:28;martijnvisser;[~zhuzh] OK so this is considered a false-positive. Thanks for verifying. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration table.exec.spill-compression.block-size not take effect in batch job,FLINK-30989,13523959,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,shenjiaqi,shenjiaqi,09/Feb/23 11:59,07/Apr/23 08:07,04/Jun/24 20:41,07/Apr/23 03:05,1.16.1,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,,,Runtime / Configuration,Table SQL / Runtime,,,,0,pull-request-available,,,,"h1. Description

I tried to config table.exec.spill-compression.block-size in TableEnv in my job and failed. I  attached to TaskManager and found conf passed to constructor of [BinaryExternalSorter|https://github.com/apache/flink/blob/release-1.16.1/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sort/BinaryExternalSorter.java#L204] is empty:

!image-2023-02-09-19-37-44-927.png|width=306,height=185!
h1. How to reproduce

A simple code to reproduce this problem:
{code:java}
// App.java

package test.flink403;

import static org.apache.flink.configuration.ExecutionOptions.RUNTIME_MODE;
import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE;

import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.configuration.AlgorithmOptions;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.api.config.ExecutionConfigOptions;

import java.util.Arrays; public class App {

  public static void main(String argc[]) throws Exception {

    Configuration config = new Configuration();
    config.set(RUNTIME_MODE, RuntimeExecutionMode.BATCH);
    config.set(ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED, true);
    config.set(AlgorithmOptions.HASH_JOIN_BLOOM_FILTERS, true);
    config.setString(TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE.key(), ""32 m""); // <---- cannot take effect
    config.set(AlgorithmOptions.SORT_SPILLING_THRESHOLD, Float.valueOf(0.5f));
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1, config);

    final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
    tableEnv.getConfig().set(""table.exec.spill-compression.block-size"", ""32 m""); // <---- cannot take effect
    final DataStream<Order> orderA =
        env.fromCollection(
            Arrays.asList(
                new Order(1L, ""beer"", 3),
                new Order(1L, ""diaper"", 4),
                new Order(3L, ""rubber"", 2)));

    final Table tableA = tableEnv.fromDataStream(orderA);

    final Table result =
        tableEnv.sqlQuery(
            ""SELECT * FROM ""
                + tableA
                + "" ""
                + "" order by user"");

    tableEnv.toDataStream(result, Order.class).print();
    env.execute();
  }
}

// ---------------------------------------------------------------
// Order.java
package test.flink403;

public class Order {
  public Long user;
  public String product;
  public int amount;

  // for POJO detection in DataStream API
  public Order() {}

  // for structured type detection in Table API
  public Order(Long user, String product, int amount) {
    this.user = user;
    this.product = product;
    this.amount = amount;
  }

  @Override
  public String toString() {
    return ""Order{""
        + ""user=""
        + user
        + "", product='""
        + product
        + '\''
        + "", amount=""
        + amount
        + '}';
  }
}{code}
 

I think it is because [SortOperator|https://github.com/apache/flink/blob/release-1.16.1/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sort/SortOperator.java#L88] try to get conf from JobConfiguration, which should be set in JobGraph. 
Following are the Classes use the same method to get conf from JobConfiguration:
 * BinaryExternalSorter
 ** ExecutionConfigOptions.TABLE_EXEC_SORT_ASYNC_MERGE_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SORT_MAX_NUM_FILE_HANDLES
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE
 * BinaryHashTable，BaseHybridHashTable
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE
 * SortDataInput
 ** AlgorithmOptions.SORT_SPILLING_THRESHOLD
 ** AlgorithmOptions.SPILLING_MAX_FAN
 ** AlgorithmOptions.USE_LARGE_RECORDS_HANDLER",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/23 11:37;shenjiaqi;image-2023-02-09-19-37-44-927.png;https://issues.apache.org/jira/secure/attachment/13055305/image-2023-02-09-19-37-44-927.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 07 08:07:18 UTC 2023,,,,,,,,,,"0|z1fsyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 04:29;Weijie Guo;cc [~lsy] ;;;","17/Feb/23 05:50;lsy;Thanks for repport it. For the table module, I will take a look.;;;","17/Feb/23 07:45;Weijie Guo;After offline discuss, the configuration does not take effect in both the streaming module and the table module. [~lsy] want to fix the table module, the others leave to me.;;;","02/Mar/23 13:31;godfrey;Fixed in

1.18.0: b4d43b47c993b7b4d5e4f7a78610c54124fcbcb4

1.17.0: 333088113993f4607038dae391863b5c30d0bc95;;;","04/Apr/23 09:59;Weijie Guo;[~lsy] Can you confirm whether the changes related to table part need backport to release-1.16 branch?;;;","05/Apr/23 14:23;Weijie Guo;Summary:

Fixed in streaming module:
master(1.18) via ccd0fe2d75a26a158ad64ab25bb5063a7031d428.
release-1.17 via 40e9501a5fcd7a71af4a7e79cd1556e190488137
release-1.16 via 75fab2759a1a2a2664d6b9f4a006a56f1a65d2fe.

Fixed in table module:
master(1.18) via b4d43b47c993b7b4d5e4f7a78610c54124fcbcb4.
release-1.17 via 333088113993f4607038dae391863b5c30d0bc95.;;;","07/Apr/23 08:07;lsy;[~Weijie Guo] Yes, we need to pick it back to release-1.16.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate TableStoreManagedFactory,FLINK-30988,13523952,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,09/Feb/23 11:31,10/Feb/23 04:26,04/Jun/24 20:41,10/Feb/23 04:26,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 04:26:56 UTC 2023,,,,,,,,,,"0|z1fsww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 04:26;lzljs3620320;master: 0cc23940b274945c1b69a6f65fdfc4134acab77b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
output source exception for SocketStreamIterator,FLINK-30987,13523926,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,09/Feb/23 08:49,09/Feb/23 08:49,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Sometime we could meet some error when using `org.apache.flink.streaming.experimental.SocketStreamIterator` for testing or output.

 

Howerver, we can't got the source exception on the log info. May be we could throw the source exception directly ?

 

!image-2023-02-09-16-47-49-928.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/23 08:47;stupid_pig;image-2023-02-09-16-47-49-928.png;https://issues.apache.org/jira/secure/attachment/13055297/image-2023-02-09-16-47-49-928.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-09 08:49:08.0,,,,,,,,,,"0|z1fsr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor CreateTableITCase and DropTableITCase to get rid of managed table,FLINK-30986,13523922,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,09/Feb/23 08:27,10/Feb/23 01:38,04/Jun/24 20:41,10/Feb/23 01:38,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 01:38:16 UTC 2023,,,,,,,,,,"0|z1fsq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 01:38;lzljs3620320;master: 0a0013c4f6091986314df78a1206f39a7ece8082;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink][Table Store] Change the Splits allocation algorithm of ContinuousFileSplitEnumerator in TableStore to a fair algorithm.,FLINK-30985,13523916,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,09/Feb/23 08:05,15/Feb/23 05:07,04/Jun/24 20:41,15/Feb/23 05:07,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Currently, {{assignSplits}} of {{ContinuousFileSplitEnumerator}} in {{TableStore}} is performed by traversing the {{{}HashMap{}}}, but since the number of buckets is fixed, the order of traversal is also fixed.
{code:java}
private void assignSplits() {
    bucketSplits.forEach(
            (bucket, splits) -> {
                if (splits.size() > 0) {
                    // To ensure the order of consumption, the data of the same bucket is given
                    // to a task to be consumed.
                    int task = bucket % context.currentParallelism();
                    if (readersAwaitingSplit.remove(task)) {
                        // if the reader that requested another split has failed in the
                        // meantime, remove
                        // it from the list of waiting readers
                        if (!context.registeredReaders().containsKey(task)) {
                            return;
                        }
                        context.assignSplit(splits.poll(), task);
                    }
                }
            });
}{code}
Assume that a {{task}} consumes multiple {{{}buckets{}}}, and there is enough split in each {{bucket}} , so that the first {{bucket}} will always be assigned to the task, and other buckets may not be consumed for a long time, resulting in uneven consumption and difficulty in advancing {{{}watermark{}}}. So I think we should change the split allocation algorithm to a fair algorithm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 05:07:47 UTC 2023,,,,,,,,,,"0|z1fsow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 09:17;lzljs3620320;[~Ming Li] Thanks for reporting.
This ticket may be related to https://issues.apache.org/jira/browse/FLINK-31008
The reason why same bucket consumed by same task is that we need to make sure the order of the consuming of one bucket.;;;","10/Feb/23 09:48;Ming Li;[~lzljs3620320] This is somewhat different from [FLINK-31008|https://issues.apache.org/jira/browse/FLINK-31008]. Considering the situation where a task consumes multiple buckets at the same time, we always allocate the split of the first bucket, which may cause other buckets to not consume for a long time.;;;","10/Feb/23 10:03;lzljs3620320;[~Ming Li] I can not get it. Can you provide some example? Under what cases would this happen.;;;","10/Feb/23 11:53;Ming Li;[~lzljs3620320] Consider extreme cases, for example, I only have one source task, and there are 2 buckets in the table store.

The split of {{snapshot-1 }}of {{bucket-1}} is allocated for the first time. When the task consumption is completed, it will request to allocate the next split. Assuming that the consumption speed is lower than the production speed, a new split is generated in {{bucket-1}} of {{snapshot-2}} at this time. According to the allocation algorithm, we will allocate the split of {{bucket-1}} instead of {{{}bucket-2{}}}, which causes {{bucket-2}} to not be consumed (or we cannot consume the complete data of {{snapshot-1}} ?).

Please let me know if you notice any mistakes or omissions. Thanks.;;;","10/Feb/23 12:06;lzljs3620320;[~Ming Li] Thanks for the explaination. I see, this is a problem. This problem may occur under the condition of back pressure.
Maybe we can assign task for splits at the time of `addSplits`.;;;","10/Feb/23 12:23;Ming Li;[~lzljs3620320] The {{RequestSplit}} event may not have occurred when {{{}addSplits{}}}, should we do eagerly allocation? My idea is to calculate the mapping relationship between {{split}} and {{taskid}} during {{{}addSplits{}}}, and change the key of {{bucketSplits}} to {{{}taskid{}}}, so that consumption can be performed in the order of discovery.;;;","15/Feb/23 05:07;lzljs3620320;master: 2503d61b2bcf45e69e8d626fb55e41a75da95e64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove explicit cast required by 3.1.x janino ,FLINK-30984,13523914,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,09/Feb/23 08:03,16/Aug/23 07:16,04/Jun/24 20:41,18/Jul/23 12:26,,,,,,,1.18.0,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"This is a follow up task.

Currently in 3.1.x Janino there is  [https://github.com/janino-compiler/janino/issues/188] leading to fail several Flink tests. Once it is fixed on janino side WAs should be removed together with janino's update",,,,,,,,,,,,,,,,,,,,,,,,FLINK-32874,,,,,,,,,,FLINK-27995,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 12:26:30 UTC 2023,,,,,,,,,,"0|z1fsog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/23 12:26;Sergey Nuyanzin;Merged to master as [36b4531ab42751d3d3a2831ca39ddfe222c18f01|https://github.com/apache/flink/commit/36b4531ab42751d3d3a2831ca39ddfe222c18f01];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the security.ssl.algorithms configuration does not take effect in rest ssl,FLINK-30983,13523913,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,lyssg,lyssg,09/Feb/23 07:59,08/Mar/23 03:36,04/Jun/24 20:41,06/Mar/23 05:46,1.16.0,,,,,,1.17.0,1.18.0,,,,,,,,,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,,"The security.ssl.algorithms configuration does not take effect in rest ssl.

 

SSLUtils#createRestNettySSLContext does not call SslContextBuilder#ciphers as  SSLUtils#createInternalNettySSLContext.

!image-2023-02-09-15-58-36-254.png!

 

!image-2023-02-09-15-58-43-963.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/23 07:58;lyssg;image-2023-02-09-15-58-36-254.png;https://issues.apache.org/jira/secure/attachment/13055289/image-2023-02-09-15-58-36-254.png","09/Feb/23 07:58;lyssg;image-2023-02-09-15-58-43-963.png;https://issues.apache.org/jira/secure/attachment/13055288/image-2023-02-09-15-58-43-963.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 05:46:24 UTC 2023,,,,,,,,,,"0|z1fso8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:10;tanyuxin;[~lyssg] Hi, I will take a look at the issue.;;;","06/Mar/23 05:46;Weijie Guo;master(1.18) via 84f532e65498164bc03529dc387a852f0e18d31d.
release-1.17 via 8ff09cbb9a1bef6a5b6644e9d06035663f10535b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support checkpoint mechanism in GBT,FLINK-30982,13523906,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,09/Feb/23 07:28,10/Feb/23 08:49,04/Jun/24 20:41,10/Feb/23 08:49,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-09 07:28:21.0,,,,,,,,,,"0|z1fsmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
explain_sql throws java method not exist,FLINK-30981,13523900,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,09/Feb/23 07:19,13/Feb/23 03:38,04/Jun/24 20:41,13/Feb/23 03:38,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"Execute `t_env.explainSql(""ANY VALID SQL"")` will throw error:
{code:java}
Traceback (most recent call last):
  File ""ISSUE/FLINK-25622.py"", line 42, in <module>
    main()
  File ""ISSUE/FLINK-25622.py"", line 34, in main
    print(t_env.explain_sql(
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 799, in explain_sql
    return self._j_tenv.explainSql(stmt, j_extra_details)
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/py4j/protocol.py"", line 330, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o11.explainSql. Trace:
org.apache.flink.api.python.shaded.py4j.Py4JException: Method explainSql([class java.lang.String, class [Lorg.apache.flink.table.api.ExplainDetail;]) does not exist
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:274)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.base/java.lang.Thread.run(Thread.java:829) {code}
[30668|https://issues.apache.org/jira/browse/FLINK-30668] changed TableEnvironment#explainSql to an interface default method, while both TableEnvironmentInternal and TableEnvironmentImpl not overwriting it, it triggers a bug in py4j, see [https://github.com/py4j/py4j/issues/506] .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 03:38:14 UTC 2023,,,,,,,,,,"0|z1fslc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 03:38;dianfu;Fixed in:
- master via a92892fea747f81f0e8a6cd4ec4ee207c95fa625
- release-1.17 via c33ee8decd7733436bec3ed102c6422e9083c558;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support s3.signer-type for S3,FLINK-30980,13523898,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,09/Feb/23 07:13,09/Feb/23 09:25,04/Jun/24 20:41,09/Feb/23 09:25,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Currently, s3.signer-type should be s3a.signing-algorithm, we can also support s3.signer-type configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:25:08 UTC 2023,,,,,,,,,,"0|z1fskw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 09:25;lzljs3620320;master: 6202410e995cb0ab8176a544eaa9f6d2db1357be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The buckets of the secondary partition should fall on different tasks,FLINK-30979,13523897,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,09/Feb/23 07:09,14/Feb/23 01:29,04/Jun/24 20:41,14/Feb/23 01:29,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"In Flink Streaming Job, sink to table store.
Considering that I only set one bucket now, but there are many secondary partitions, I expect to use multiple parallelism tasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 01:29:06 UTC 2023,,,,,,,,,,"0|z1fsko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 01:29;lzljs3620320;master: 219c4de518c419eade41e99fc419d7f2ccfed213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorImplITCase.testInterruptExecution hangs waiting for SQL gateway service closing,FLINK-30978,13523894,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,renqs,renqs,09/Feb/23 06:52,06/Mar/23 10:17,04/Jun/24 20:41,06/Mar/23 10:17,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,test-stability,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45921&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=44674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 12:28:04 UTC 2023,,,,,,,,,,"0|z1fsk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 09:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46290&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15744

{code}
Feb 19 02:06:14 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007f38910e8000 nid=0x2250 waiting on condition [0x00007f3798588000]
Feb 19 02:06:14    java.lang.Thread.State: TIMED_WAITING (sleeping)
Feb 19 02:06:14 	at java.lang.Thread.sleep(Native Method)
Feb 19 02:06:14 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Feb 19 02:06:14 	at org.apache.flink.table.client.gateway.ExecutorImplITCase.testInterrupting(ExecutorImplITCase.java:626)
Feb 19 02:06:14 	at org.apache.flink.table.client.gateway.ExecutorImplITCase.testInterruptExecution(ExecutorImplITCase.java:513)
Feb 19 02:06:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 19 02:06:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 19 02:06:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code};;;","20/Feb/23 09:22;mapohl;[~fsk119] I'm increasing the priority for that one to blocker. May you provide details on whether that's a test code issue or actually relevant for production?;;;","27/Feb/23 10:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46558&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18102;;;","28/Feb/23 08:11;fsk119;Hello [~mapohl]. Thanks for sharing this with me. It's very wired to me the case fails. I need to add some logs about this part and test it in my private pipeline. But I don't think it's a blocking issue here because the failed tests only tell us the `Executor` fails to close the Operation. But the Gateway will automatically detect the session whether are idle and close used resources by the idle session. ;;;","01/Mar/23 07:21;fsk119;After discussing with colleagues, I find the problem is because the `RestClient#sendRequest` and `CompletableFuture#get` are not atomic. The Client may send the request and the Server returns the response immediately before  ExecutorImpl tries to invoke `CompletableFuture#get`. In this case, the Executor will not check the interrupted flag on the thread and continue execution here. 

I will fix this after I figure out other blocking issues.;;;","01/Mar/23 08:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46643&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=16997;;;","02/Mar/23 10:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46687&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=18050;;;","03/Mar/23 12:28;fsk119;Merged into master: d96bb2f66d71fecdc5dba183ad04c9ba75e40845
Merged into release-1.17: 5dddc0dba2be20806e67769314eecadf56b87a53;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink tumbling window stream converting to pandas dataframe not work,FLINK-30977,13523893,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Joekwal,Joekwal,09/Feb/23 06:50,02/Mar/23 09:04,04/Jun/24 20:41,02/Mar/23 09:04,,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"I want to know if tumbling window supported to convert to pandas?
{code:java}
code... #create env

kafka_src = """"""
CREATE TABLE if not exists `kafka_src` (
...
`event_time` as CAST(`end_time` as TIMESTAMP(3)),
WATERMARK FOR event_time as event_time - INTERVAL '5' SECOND
)
with (
'connector' = 'kafka',
'topic' = 'topic',
'properties.bootstrap.servers' = '***',
'properties.group.id' = '***',
'scan.startup.mode' = 'earliest-offset',
'value.format' = 'debezium-json'
);
""""""  
  
t_env.execute_sql(kafka_src)
table = st_env.sql_query(""SELECT columns,`event_time`  \
    FROM TABLE(TUMBLE(TABLE table_name, DESCRIPTOR(event_time), INTERVAL '1' MINUTES))"")

table.execute().print()  #could print the result

df = table.to_pandas()

#schema is correct!
schema = DataTypes.ROW([DataTypes.FIELD(""column1"", DataTypes.STRING()),
                        .......
                            ])
table = st_env.from_pandas(df,schema=schema)
st_env.create_temporary_view(""view_table"",table)

st_env.sql_query(""select * from view_table"").execute().print() # Not work!Can't print the result {code}
Tumbling window stream from kafka source convert to pandas dataframe and it can't print the result.The schema is right.I have tested in another job with using batch stream from jdbc source.It can print the result.The only different thing is the input stream.Is tumbling windows supported to convert to Pandas?",pyflink1.15.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-09 06:50:27.0,,,,,,,,,,"0|z1fsjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_404_check fails occasionally,FLINK-30976,13523892,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,09/Feb/23 06:48,09/Feb/23 15:03,04/Jun/24 20:41,09/Feb/23 14:18,1.17.0,1.18.0,,,,,pulsar-4.0.0,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,test-stability,,,"We've seen the docs_404_check failing in nightly builds (only the cron stage but not the ci stage):
{code}
Re-run Hugo with the flag --panicOnWarning to get a better error message.
ERROR 2023/02/09 01:27:27 ""docs/connectors/datastream/pulsar.md"": Invalid use of artifact shortcode. Unknown flag `4.0.0-SNAPSHOT`
ERROR 2023/02/09 01:27:34 ""docs/connectors/datastream/pulsar.md"": Invalid use of artifact shortcode. Unknown flag `4.0.0-SNAPSHOT`
Error: Error building site: logged 2 error(s)
Total in 12945 ms
Error building the docs
{code}
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45909&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=133
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45906&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=132",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 14:18:47 UTC 2023,,,,,,,,,,"0|z1fsjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 06:49;mapohl;It seems to be fixed again. But I'm still wondering what caused and what fixed it and why it didn't cause a test failure in the CI stage.;;;","09/Feb/23 07:17;renqs;{{docs_404_check}} is only run in PR triggered and cron jobs, so most CI runs skip this stage. 

This could be reproduced in my own Azure pipeline:

[https://dev.azure.com/renqs/Apache%20Flink/_build/results?buildId=438&view=logs&j=c5d67f7d-375d-5407-4743-f9d0c4436a81&t=38411795-40c9-51fa-10b0-bd083cf9f5a5&l=133] ;;;","09/Feb/23 08:10;mapohl;I see, we only run the docs check for the PR if the PR touches the docs: https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/tools/azure-pipelines/build-apache-repo.yml#L87
I'm still puzzled, though, why your change triggered the docs build, [~renqs]. You didn't touch the docs.;;;","09/Feb/23 08:20;renqs;[~mapohl] My guess is that personal CIs use {{azure-pipelines.yml}} instead of the {{{}tools/azure-pipelines/build-apache-repo.yml{}}}. The former one doesn't have the doc touching check.;;;","09/Feb/23 08:47;mapohl;[~martijnvisser] do you have time to look into this?;;;","09/Feb/23 09:07;chesnay;You just have to replace references to the {{artifact}} shortcode with {{connector_artifact}} in flink-connector-pulsar.;;;","09/Feb/23 14:18;mapohl;pulsar/main: 6d5629f23f6306993a2782919fd392032f798088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate aws sdk from v1 to v2,FLINK-30975,13523890,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,samrat007,samrat007,09/Feb/23 06:44,28/Sep/23 15:59,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,"This issue is created to address migration of aws sdk v1 in flink to aws sdk v2 
As Aws annouced deprecation of sdk v1 , long term support and changes will be offered on v2. 

So, it will be good for flink to move its dependencies on aws sdk v2 keeping all functionalities intact.

 

[changelog-details |https://github.com/aws/aws-sdk-java-v2/blob/master/docs/LaunchChangelog.md]  of aws sdk v2 

 ",,,,,,,,,,,,,,,,,,,,,,,FLINK-30946,FLINK-33157,,,,FLINK-33157,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:08 UTC 2023,,,,,,,,,,"0|z1fsj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add commons-io to flink-sql-connector-hbase,FLINK-30974,13523887,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xichaomin,xichaomin,09/Feb/23 06:35,19/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,"I use flink sql to join hbase table, when I set lookup.asycn = true, I get an error.

!image-2023-02-09-14-43-06-187.png|width=1385,height=549!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/23 06:43;xichaomin;image-2023-02-09-14-43-06-187.png;https://issues.apache.org/jira/secure/attachment/13055280/image-2023-02-09-14-43-06-187.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:09 UTC 2023,,,,,,,,,,"0|z1fsig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e test prepare crashes with libssl dependency not being available,FLINK-30973,13523879,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,mapohl,mapohl,09/Feb/23 06:06,09/Feb/23 06:29,04/Jun/24 20:41,09/Feb/23 06:29,1.15.3,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,test-stability,,,,"We're experiencing a test instability where the preparation of the e2e tests fails because of a 404:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45907&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=11b0df07-3e5e-58da-eb81-03003e470195&l=1830

{code}
--2023-02-09 00:23:32--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-02-09 00:23:33 ERROR 404: Not Found.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30972,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 06:29:47 UTC 2023,,,,,,,,,,"0|z1fsgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 06:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45910&view=results;;;","09/Feb/23 06:13;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45912&view=results;;;","09/Feb/23 06:14;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45909&view=results;;;","09/Feb/23 06:27;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45906&view=results;;;","09/Feb/23 06:29;mapohl;I don't know why I didn't manage to find FLINK-30972 before creating this issue. 8) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"E2e tests always fail in phase ""Prepare E2E run""",FLINK-30972,13523877,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,wanglijie,wanglijie,09/Feb/23 05:27,06/Mar/23 09:57,04/Jun/24 20:41,09/Feb/23 08:10,1.15.4,1.16.2,1.17.0,1.18.0,,,1.15.4,1.16.2,1.17.0,1.18.0,,,,,,,,,,,,,Build System / CI,Tests,,,,0,test-stability,,,,"{code:java}
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-02-09 04:38:47--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-02-09 04:38:47 ERROR 404: Not Found.


WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline
##[error]Bash exited with code '100'.
Finishing: Prepare E2E run
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30973,,,,FLINK-31334,,,,,,,,,FLINK-30965,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 06:55:03 UTC 2023,,,,,,,,,,"0|z1fsg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 05:28;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45926&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=11b0df07-3e5e-58da-eb81-03003e470195;;;","09/Feb/23 05:30;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45913&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=11b0df07-3e5e-58da-eb81-03003e470195;;;","09/Feb/23 06:20;renqs;Thanks for reporting this [~wanglijie] ! The required package was removed from the Ubuntu repository. I'll make a hotfix first and try to find a permanent solution later. ;;;","09/Feb/23 07:44;martijnvisser;[~renqs] This always happens when a new version of OpenSSL has been released; fix should be easy;;;","09/Feb/23 08:09;renqs;Fixed on master: 4029f730e454fb4c13e92d1ada5253c80ea645ba

1.17: f4410f0f97ec6e857f246c9649833b6eb15f6d2a

1.16: d5bd173f35844bbb56d32b9315073c59d12fe067

1.15: 998b7a1d1e4335fe8771150ed6b7a08fa08f6da4;;;","10/Feb/23 06:55;mapohl;The following builds didn't contain the fixes mentioned above, yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45934&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45919&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45922&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45979&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46021&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46030&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46034&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold',FLINK-30971,13523871,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,09/Feb/23 03:55,02/Mar/23 09:52,04/Jun/24 20:41,17/Feb/23 07:18,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"In our test environment, we set the default parallelism to  1 and got the most appropriate default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold'  is 5000000. However, for these batch jobs with high parallelism in produce environment,  the amount of data in single parallelism is almost less than 5000000. Therefore, after testing, we found that set to 500000 can get better results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 07:18:22 UTC 2023,,,,,,,,,,"0|z1fsew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 07:18;godfrey;Fixed in master: 55b927b0e6eb2d5d71487b9bb2c4dab80017a7e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SQL Client doc still uses old style output,FLINK-30970,13523861,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fsk119,fsk119,fsk119,09/Feb/23 02:54,11/Mar/24 12:43,04/Jun/24 20:41,,1.17.0,,,,,,1.20.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-09 02:54:53.0,,,,,,,,,,"0|z1fsco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pyflink table example throws ""module 'pandas' has no attribute 'Int8Dtype'""",FLINK-30969,13523860,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,09/Feb/23 02:52,13/Feb/23 03:43,04/Jun/24 20:41,13/Feb/23 03:43,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"After apache-beam is upgraded to 2.43.0 in 1.17, running `python pyflink/examples/table/basic_operations.py` will throw error:
{code:java}
Traceback (most recent call last):
  File ""pyflink/examples/table/basic_operations.py"", line 484, in <module>
    basic_operations()
  File ""pyflink/examples/table/basic_operations.py"", line 29, in basic_operations
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 1640, in _open
    startup_loopback_server()
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 1631, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 31, in <module>
    from apache_beam.options.pipeline_options import DebugOptions
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/__init__.py"", line 92, in <module>
    from apache_beam import coders
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>
    from apache_beam.coders.coders import *
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/coders/coders.py"", line 59, in <module>
    from apache_beam.coders import coder_impl
  File ""apache_beam/coders/coder_impl.py"", line 63, in init apache_beam.coders.coder_impl
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/typehints/__init__.py"", line 31, in <module>
    from apache_beam.typehints.pandas_type_compatibility import *
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/typehints/pandas_type_compatibility.py"", line 81, in <module>
    (pd.Int8Dtype(), Optional[np.int8]),
AttributeError: module 'pandas' has no attribute 'Int8Dtype' {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 03:43:48 UTC 2023,,,,,,,,,,"0|z1fscg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 03:43;dianfu;Fixed in:
- master via c096c03df70648b60b665a09816635b956b201cc
- release-1.17 via 19c05ef0c864644512e5643589fe550f7e281254;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql-client supports dynamic config to open session,FLINK-30968,13523856,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,09/Feb/23 02:09,21/Feb/23 11:59,04/Jun/24 20:41,21/Feb/23 11:59,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"Currently sql client will open session with configuration in flink-conf.yaml when it creates connection to gateway. For the convenience of users, it can supports dynamic config with `--conf` as `bin/sql-client.sh gateway --endpoint host:port --conf k1=v1 --conf k2=v2`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 11:59:54 UTC 2023,,,,,,,,,,"0|z1fsbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 11:59;fsk119;Merged into master: da00b9c7de7a496d2fe42d0f94836fd7451676aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add MongoDB connector documentation,FLINK-30967,13523798,13071642,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,chesnay,chesnay,08/Feb/23 16:02,14/Feb/23 10:38,04/Jun/24 20:41,14/Feb/23 10:38,,,,,,,mongodb-1.0.0,,,,,,,,,,,,,,,,Connectors / MongoDB,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 10:38:28 UTC 2023,,,,,,,,,,"0|z1fryo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:38;chesnay;main: ae0881d56e9a621a08ad85b708162481f850520a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL IF FUNCTION logic error,FLINK-30966,13523766,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,hiscat,hiscat,08/Feb/23 13:20,20/Jul/23 04:21,04/Jun/24 20:41,06/Jun/23 02:54,1.16.0,1.16.1,,,,,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"my data is 
{code:java}
//
{ ""before"": { ""status"": ""sent"" }, ""after"": { ""status"": ""succeed"" }, ""op"": ""u"", ""ts_ms"": 1671926400225, ""transaction"": null } {code}
my sql is 

 
{code:java}
CREATE TABLE t
(
    before ROW (

        status varchar (32)

        ),
    after ROW (

        status varchar (32)

        ),
    ts_ms                bigint,
    op                   string,
    kafka_timestamp      timestamp METADATA FROM 'timestamp',
--     @formatter:off
    proctime AS PROCTIME()
--     @formatter:on
) WITH (
    'connector' = 'kafka',
--     'topic' = '',
    'topic' = 'test',
    'properties.bootstrap.servers' = ' ',
    'properties.group.id' = '',
    'format' = 'json',
    'scan.topic-partition-discovery.interval' = '60s',
    'scan.startup.mode' = 'earliest-offset',
    'json.ignore-parse-errors' = 'true'
 );
create table p
(
    status                  STRING ,
    before_status                  STRING ,
    after_status                  STRING ,
    metadata_operation      STRING COMMENT '源记录操作类型',
    dt                      STRING
)WITH (
    'connector' = 'print'
 );
INSERT INTO p
SELECT
       IF(op <> 'd', after.status, before.status),
        before.status,
        after.status,
       op                                         AS metadata_operation,
       DATE_FORMAT(kafka_timestamp, 'yyyy-MM-dd') AS dt
FROM t;

 {code}
 my local env output is 

 

 
{code:java}
+I[null, sent, succeed, u, 2023-02-08] {code}
 

 my produtionc env output is 
{code:java}
+I[sent, sent, succeed, u, 2023-02-08]  {code}
why?  
This look like a bug.

 ",,,,,,,,,,,,,,,,,,,FLINK-30559,FLINK-30018,,,,FLINK-31653,FLINK-30559,,,FLINK-30559,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 09:48:11 UTC 2023,,,,,,,,,,"0|z1frrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 02:38;csq;Hi [~hiscat], I have reproduced the same error and it seems a bug in IFCallGen.
After investigated the generated code, there are two problems:
1. It perform the result term casting before the calculation logic, and finally the actual result always refer to a non-initialized field.
2. when normalizing arguments, it always align to the type of ARG1, like IF(1 >  2, 'true', 'false')
the result will be string 'fals' which length is the same as 'true'.

I would like to help fix it. ;;;","10/Feb/23 02:41;csq;BTW, the expected result of your query might be:
+I[succeed, sent, u, 2023-02-08];;;","13/Feb/23 02:37;luoyuxia;Seems same to FLINK-31003? ;;;","13/Feb/23 02:48;csq;[~luoyuxia]The matter of [FLINK-31003|https://issues.apache.org/jira/browse/FLINK-31003] is return type inferencing, while there is another issue that the code block position of result term casting might be wrong.;;;","13/Feb/23 07:15;lincoln.86xy;[~csq] assigned to you;;;","31/May/23 03:45;luoyuxia;Hi, [~csq] I have reviewed your pr. Hope you can find sometime to have a look again.;;;","31/May/23 03:51;csq;[~luoyuxia]Sorry for the delay, I would continue to work on the issue and update the PR  as soon as possible.;;;","05/Jun/23 09:48;luoyuxia;master: 6663c8b29f672b961d34baed314940621f3754ca

1.17: 8e82359be4982c8534b271259c7e649e4c12df1d

1.16: 6bdf2ba59678289b517b208ac5173c8e58b50690;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
git-repo-sync doesn't pick up the 2nd-most-recently published Flink version (even though it's still supported) after a new release branch is cut,FLINK-30965,13523765,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,08/Feb/23 13:18,16/Feb/23 09:39,04/Jun/24 20:41,16/Feb/23 09:39,,,,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,,,,,"I noticed that we're always synchronizing the most-recent release branches (see [flink-ci/git-repo-sync:git-repo-synch:27|https://github.com/flink-ci/git-repo-sync/blob/7c0c2ed4b8f1cdf343e75021ca89e9dcc9421b93/sync_repo.sh#L27]). That means that we stop running CI on the Flink version that is soon to be deprecated after a new release branch is cut, e.g.: 1.16 & 1.15 are supported and we cut {{release-1.17}} to start the release process for 1.17. From this point onwards, we are synchronizing 1.17 and 1.16 but skip 1.15 eventhough 1.15 is still supported.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30972,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 09:39:50 UTC 2023,,,,,,,,,,"0|z1frrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 09:32;mapohl;1.15 doesn't get updated anymore: We see the issue of FLINK-30972 being fixed but it's not picked up anymore:
 * [20230210.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45979&view=results]
 * [20230211.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46021&view=results]
 * [20230212.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46030&view=results]
 * [20230213.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46034&view=results]
 * [20230214.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46085&view=results];;;","13/Feb/23 14:48;mapohl;[~chesnay] I still don't understand why the build is retriggered if there's no new commit sync'd to the {{flink-mirror}} repo. Is there some build pipeline configuration that triggers the nightly? The YAML config in the code looks like we're only running {{master}} even if there's no change pushed in the meantime (see [tools/azure-pipelines/build-apache-repo.yml|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/tools/azure-pipelines/build-apache-repo.yml#L32]).

May you have a look at the repo-sync script change? I'm wondering whether ignoring the oldest release branch was intentionally done in the past or just missed.;;;","13/Feb/23 15:28;chesnay;nightlies are configured in the azure UI.

I don't believe we have concluded to not create a final 1.15 release. Hence the nightlies should keep running, as should the sync.;;;","13/Feb/23 15:30;chesnay;Interestingly enough in the UI we did set that nightlies should run if something changed, so it's weird that it runs them anyway. #JustAzureThings;;;","16/Feb/23 09:39;mapohl;The PR was merged and redeployed. I verified that the {{release-1.15}} is sync'd now again in [flink-ci/flink-mirror:release-1.15|https://github.com/flink-ci/flink-mirror/commits/release-1.15].

CI also succeed with the fix of FLINK-30972 being considered as well: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46200&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-mirror repo sync release branches failed,FLINK-30964,13523761,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,leonard,leonard,08/Feb/23 12:38,08/Feb/23 13:40,04/Jun/24 20:41,08/Feb/23 13:39,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Build System / CI,,,,,0,,,,,"we use https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh to sync the master the latest 2 release-X branches  from apache/flink to flink-ci/flink-mirror.  
 but the scripts https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh exists a wrong condition judgement which lead the release-1.17 sync fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 13:39:23 UTC 2023,,,,,,,,,,"0|z1frqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 13:39;leonard;Fixed in https://github.com/flink-ci/git-repo-sync master: 9563d8a8564b5709607122ad146753c925e05827;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch binary downloads to archive.apache.org,FLINK-30963,13523754,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Feb/23 11:05,05/May/23 09:09,04/Jun/24 20:41,08/Feb/23 15:57,,,,,,,cassandra-3.0.1,cassandra-3.1.0,elasticsearch-3.0.1,elasticsearch-3.1.0,gcp-pubsub-3.0.1,hbase-3.0.0,hive-3.0.0,jdbc-3.0.1,jdbc-3.1.0,kafka-3.0.0,mongodb-1.0.0,opensearch-1.0.1,pulsar-3.0.1,pulsar-4.0.0,rabbitmq-3.0.1,redis-1.0.0,Build System / CI,Connectors / Common,,,,0,pull-request-available,,,,"archive.apache.org is the only stable download link for binaries.
Now that we properly fixed the binary caching in the connector workflows it should be fine to make use of it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 11:08:27 UTC 2023,,,,,,,,,,"0|z1frow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 11:08;chesnay;ci_utils:
81b1f9a2d156ebb875eb6a10c8b13d0ca9bdf79d
662b2fb266943effb8f3408bcfb12ac6b8427ea8

jdbc:
main: 67909c1eee18a171aafeb51669e8632ec640a2ba
v3.0: f0f91330d1918ba17bdf89e9fcd644ebbd2924b1

mongodb:
main: tbd

pulsar:
main: 8496061aa9c6b6aa61ed212e713346934611a7d2
v3.0: 343acebd3ec00410747a9c2af3af2dccc87b5f93

opensearch:
main: e049c42d665da39201d2ccb171d4011e9b93ede2

gcp:
main: 0537108a1dc31f54b619f8a72f0fd165a7e8c8fa
v3.0: b0c65846f31aa27ca53511912f072caba402a02f

elasticsearch:
main: e6088c1189d4bfc77a4429dba9d90e7db4b6d8d4
v3.0: 8e57135c2a5d848c9ba38c45fe080a3b91a77d48

kafka:
main: b7af760003a2bcc7e48f1aa3b96a7e5fbfea990b
v3.0: 32319332ce9aef84793efc89dc552e3462db2127

cassandra:
main: 6f83d2286af7bc10bf235982b39632ae712108
v3.0: b7c7673380491518fdc86e3b3f7fcd13e4531d75e1

rabbitmq:
main: b5f30adabc35c73b1f022e6195978c9aa62375f5
v3.0: 2929f7c4fa35378721688ccb9d046694eb8ef896

hive:
main: 6a10a97936cfe04d1d8e284d102c893adc8955d2

hbase:
main: ec660e81b042b64220a2753039c75e47b23e4268
v3.0: 90c0d9d44ac9171067d7dae4ef9a143489b35cef

redis:
main: dd7304bf359d219a1e00367da00ac25c930fc263;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error messaging when launching py4j gateway server,FLINK-30962,13523749,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,08/Feb/23 10:04,14/Feb/23 02:46,04/Jun/24 20:41,14/Feb/23 02:46,,,,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"In some cases, e.g. JVM_ARGS does not match the java executable version that pyflink uses when launching py4j gateway server, there'll be only a general message ""Exception: Java gateway process exited before sending its port number"". It would be easier to investigate the error cause if we expose stderr from java process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 02:46:57 UTC 2023,,,,,,,,,,"0|z1frns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 02:46;dianfu;Merged to:
- master via 50c522f1d817ede56a8532987b819abf06e9df86
- release-1.17 via 44e6cfb87c1b2a5f4106df61cd06c4870d4802f8
- release-1.16 via c57699cb47134a39b082139beaaab4173acd1fdc
- release-1.15 via b6cbe5403d86d42b26b2105d5be9d016384bf736;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated tests_require in setup.py,FLINK-30961,13523744,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,08/Feb/23 09:47,17/Feb/23 07:43,04/Jun/24 20:41,14/Feb/23 02:51,,,,,,,1.17.0,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"According to [https://github.com/pypa/setuptools/pull/1878] , `python setup.py test` command is deprecated, thus related `tests_require` could be removed from setup.py.

BTW, PyCharm will complain that pytest==4.4.1 is not installed, but 4.4.1 is actually for python < 3.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 02:51:41 UTC 2023,,,,,,,,,,"0|z1frmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 02:51;dianfu;Merged to master via e67b141f2b2414cf9b6ddc43e817d4250dd38319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemory error using jdbc sink,FLINK-30960,13523735,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,shenbo,shenbo,shenbo,08/Feb/23 08:56,15/Jan/24 16:52,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"Here in production I have an environment consisting of a kafka source, to which third-party providers push a few thousand messages every hour, and I use flink jdbc sink to store those messages into a mysql database.

Recently there are a few taskmanager process crashes. My investigation suggests that the cause boils down to the way exceptions are handled in jdbc batched mode.
https://issues.apache.org/jira/browse/FLINK-30960#
 

When writing to JDBC failed in batched mode due to some error like DataTuncation, the exception is stored in field ""flushException"" waiting to be processed by the task main thread.

This exception is only checked on the next call to ""writeRecords"". In my case, if the exception happens to occur when processing the last batch of records, and there are no further record comming from source for the next hour, the flushing thread simply repeatly throws a new RuntimeException wrapping the last ""flushException"" as cause. The new RuntimeException is stored as the new ""flushException"". 

Hence, every time the RuntimeException is thrown, the exception hierachy gets bigger, and eventually before the exception is processed on the main thread, the jvm runs out of memory, which causes an unexpected process crash. 

!image-2023-02-08-16-58-27-765.png!

 

 ","Using SQL-Client + TableAPI

Processing Kafka messages to MySQL databases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/23 15:23;shenbo;heap-memory-metrics.png;https://issues.apache.org/jira/secure/attachment/13055254/heap-memory-metrics.png","08/Feb/23 08:56;shenbo;image-2023-02-08-16-58-27-765.png;https://issues.apache.org/jira/secure/attachment/13055241/image-2023-02-08-16-58-27-765.png","08/Feb/23 15:17;shenbo;taskmanager.log;https://issues.apache.org/jira/secure/attachment/13055253/taskmanager.log",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 01 12:13:34 UTC 2023,,,,,,,,,,"0|z1frko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 10:34;martijnvisser;[~shenbo] Please verify this with a more recent version of Flink, given that Flink 1.13 is no longer supported in the community. It's best to verify this with Flink 1.16. Especially since the JDBC connector is no longer bundled with Flink, but is externalized at https://github.com/apache/flink-connector-jdbc;;;","08/Feb/23 15:16;shenbo;[~martijnvisser] OK, will keep that in mind.

By looking at the code in the main branch of [https://github.com/apache/flink-connector-jdbc] ""JdbcOutputFormat.java"", the code concerned are mostly the same as the version in v1.13.6

The same issue is reproduced using Flink 1.16. I have uploaded a full run log.;;;","13/Feb/23 09:46;leonard;[~ShawnHx] Would you like to look into this ticket?;;;","13/Feb/23 17:32;shenbo;Just to share what I thought on this issue:

The issue is probably due to the assumption that the data stream is mostly continuous, the exception thrown in the ""flushing"" thread is cached for for processing in the main task thread.

In the main task thread, is where the exception can be thrown up along the abstraction layers, and ultimately caught by the task failure exception handling logic.

There seems no way for the other ""flushing"" thread to properly interact with the main task thread. (maybe interrupt it?). I tried to utilize the ""mailbox"" mechanism in StreamTask, but the abstration layers are pretty thick to penetrate.

 

The problem as far as I could see, in a pulse-like data stream, is two fold:
 # Processing of exception in ""flushing"" thread is delayed until there's new data incoming. This delay could be hours or days depending on the use case. Idealy it would better for the task to ""fail-fast"".
 # ""flushing"" thread repeatly throws a new RuntimeException wrapping the last exception as the cause. This builds a growing exception hierachy and eats up memory. When new data arrives, the exception stringify operation could easily cause stack overflow or out of memory.

 

If it's acceptable, I would like to contribute a patch for the latter issue, which could be easily fixed by storing the exception thrown by the ""flushing"" thread for comparison, and just don't throw it again.

 

For the former issue, there might need some major work done on the excepton handling mechanism across thread boundaries. Hope someone more familiar with the project could shed some light on this.

 ;;;","01/Nov/23 12:13;shenbo;This issue still exists in the latest master branch of flink-jdbc-connector.

I would like to contribute a PR for a quick fix if this is OK:
 # Introduce a new FlushingException extends RuntimeException, and throw this new type instead in ""checkFlushException"".
 # In JdbcOutputFormat's scheduler thread, add a type check before replacing existing flushException. If newly caught exception is of ""FlushingException"", ignore it.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the documentation of UNIX_TIMESTAMP for different argument format,FLINK-30959,13523720,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,ruanhang1993,yunfengzhou,yunfengzhou,08/Feb/23 07:24,20/Mar/24 08:22,04/Jun/24 20:41,20/Mar/24 08:22,1.16.1,,,,,,1.16.2,,,,,,,,,,,,,,,,Documentation,Table SQL / API,,,,0,pull-request-available,stale-assigned,,,"When running the following pyflink program

 
{code:python}
import pandas as pd
from pyflink.datastream import StreamExecutionEnvironment, HashMapStateBackend
from pyflink.table import StreamTableEnvironment

if __name__ == ""__main__"":
    input_data = pd.DataFrame(
        [
            [""Alex"", 100.0, ""2022-01-01 08:00:00.001 +0800""],
            [""Emma"", 400.0, ""2022-01-01 00:00:00.003 +0000""],
            [""Alex"", 200.0, ""2022-01-01 08:00:00.005 +0800""],
            [""Emma"", 300.0, ""2022-01-01 00:00:00.007 +0000""],
            [""Jack"", 500.0, ""2022-01-01 08:00:00.009 +0800""],
            [""Alex"", 450.0, ""2022-01-01 00:00:00.011 +0000""],
        ],
        columns=[""name"", ""avg_cost"", ""time""],
    )

    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_state_backend(HashMapStateBackend())
    t_env = StreamTableEnvironment.create(env)

    input_table = t_env.from_pandas(input_data)

    t_env.create_temporary_view(""input_table"", input_table)

    time_format = ""yyyy-MM-dd HH:mm:ss.SSS X""

    output_table = t_env.sql_query(
        f""SELECT *, UNIX_TIMESTAMP(`time`, '{time_format}') AS unix_time FROM input_table""
    )

    output_table.execute().print()

{code}

The actual output is 


{code}
+----+--------------------------------+--------------------------------+--------------------------------+----------------------+
| op |                           name |                       avg_cost |                           time |            unix_time |
+----+--------------------------------+--------------------------------+--------------------------------+----------------------+
| +I |                           Alex |                          100.0 |  2022-01-01 08:00:00.001 +0800 |           1640995200 |
| +I |                           Emma |                          400.0 |  2022-01-01 00:00:00.003 +0000 |           1640995200 |
| +I |                           Alex |                          200.0 |  2022-01-01 08:00:00.005 +0800 |           1640995200 |
| +I |                           Emma |                          300.0 |  2022-01-01 00:00:00.007 +0000 |           1640995200 |
| +I |                           Jack |                          500.0 |  2022-01-01 08:00:00.009 +0800 |           1640995200 |
| +I |                           Alex |                          450.0 |  2022-01-01 00:00:00.011 +0000 |           1640995200 |
+----+--------------------------------+--------------------------------+--------------------------------+----------------------+
{code}

While the expected result is


{code:java}
+----+--------------------------------+--------------------------------+--------------------------------+----------------------+
| op |                           name |                       avg_cost |                           time |            unix_time |
+----+--------------------------------+--------------------------------+--------------------------------+----------------------+
| +I |                           Alex |                          100.0 |  2022-01-01 08:00:00.001 +0800 |           1640995200 |
| +I |                           Emma |                          400.0 |  2022-01-01 00:00:00.003 +0000 |           1640966400 |
| +I |                           Alex |                          200.0 |  2022-01-01 08:00:00.005 +0800 |           1640995200 |
| +I |                           Emma |                          300.0 |  2022-01-01 00:00:00.007 +0000 |           1640966400 |
| +I |                           Jack |                          500.0 |  2022-01-01 08:00:00.009 +0800 |           1640995200 |
| +I |                           Alex |                          450.0 |  2022-01-01 00:00:00.011 +0000 |           1640966400 |
+----+--------------------------------+--------------------------------+--------------------------------+----------------------+
{code}




 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 20 08:22:02 UTC 2024,,,,,,,,,,"0|z1frhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 09:15;twalthr;Are you sure this is a problem with UNIX_TIMESTAMP? Unfortunately, the format of UNIX_TIMESTAMP is not documented. We should change this. It might be that a {{2022-01-01 08:00:00.001 +0800}} format is currently not supported.;;;","08/Feb/23 09:16;godfrey;Thanks for reporting this issue. [~yunfengzhou] 

 

Currently, the behavior of data with timezone is not defined, the description of \{{UNIX_TIMESTAMP(string1[, string2])}} in Flink document is C{{{}onverts date time string string1 in format string2 (by default: yyyy-MM-dd HH:mm:ss if not specified) to Unix timestamp (in seconds), using the specified timezone in table config. {}}}which means we should always use the specified timezone in table. config to parse data. I think the behavior for \{{yyyy-MM-dd HH:mm:ss.SSS X}} is not considered before, and I tend to use the time zone in the record if the format and the record both have time zone. If that, the result is correct.

need more discussion determine the behavior.  cc [~Leonard] [~jark] [~twalthr] 

 ;;;","08/Feb/23 09:25;twalthr;To avoid confusion: UNIX_TIMESTAMP does not need to read from table config in this case. The input is a string and the output is long. So there is no TIMESTAMP_LTZ involved and internally UNIX_TIMESTAMP should potentially interpret the timezone. I guess this is just not implemented yet.;;;","08/Feb/23 09:33;yunfengzhou;cc [~twalthr] [~godfrey] Thanks for the comments.

I think that `UNIX_TIMESTAMP` has a `format` argument as described in [~godfrey] 's comment. According to the document of this function, the timezone should be decided by flink's configurations, while the actual behavior is using the timezone information in the format argument. This gap in the document and the actual behavior is what confuses me now.;;;","08/Feb/23 09:37;twalthr;""2022-01-01 08:00:00.001"" has *no* timezone, so it uses the one defined in table config.
""2022-01-01 08:00:00.001 +0000"" has *a* timezone, so it can calculate the UNIX timestamp independently.

The format string must define whether ""2022-01-01 08:00:00.001"" == ""2022-01-01 08:00:00.001 +0000"" or not.;;;","08/Feb/23 10:09;leonard;Thanks [~yunfengzhou] for reporting this issue.
(1) Both UNIX_TIMESTAMP()  and UNIX_TIMESTAMP(string1[, string2])  are not defined in ANSI SQL, every DB/SQL vendor handles this differently. We didn't consider ""yyyy-MM-dd HH:mm:ss.SSS X"" pattern before and thus our document missed this case.
(2) As ""yyyy-MM-dd HH:mm:ss.SSS X"" pattern is also valid format for ISO 8601, I tend to we should parse the the date and time follows the ISO format rule i.e. : adopting the time zone in the record firstly to get a parsed instant and then obtain the unix timestamp(epoch seconds) from passed instant. Btw, Spark and Hive have same function UNIX_TIMESTAMP(string1[, string2]), I didn't test them yet, but they're good input for our decision.;;;","08/Feb/23 10:19;luoyuxia;Hi, I just test with Hive3/Spark3, the results are as follows:

Hive3:
{code:java}
Alex    100.0    2022-01-01 08:00:00.001 +0800    1640995200
Emma    400.0    2022-01-01 00:00:00.003 +0000    1640995200
Alex    200.0    2022-01-01 08:00:00.005 +0800    1640995200
Emma    300.0    2022-01-01 00:00:00.007 +0000    1640995200
Jack    500.0    2022-01-01 08:00:00.009 +0800    1640995200
Alex    450.0    2022-01-01 00:00:00.011 +0000    1640995200 {code}
Spark3
{code:java}
Alex    100.0    2022-01-01 08:00:00.001 +0800    1640995200
Emma    400.0    2022-01-01 00:00:00.003 +0000    1640995200
Alex    200.0    2022-01-01 08:00:00.005 +0800    1640995200
Emma    300.0    2022-01-01 00:00:00.007 +0000    1640995200
Jack    500.0    2022-01-01 08:00:00.009 +0800    1640995200
Alex    450.0    2022-01-01 00:00:00.011 +0000    1640995200 {code}
Seems Flink's result is same to Hive3&Spark3.;;;","08/Feb/23 10:50;leonard;Thanks [~luoyuxia] for the tests, cool! ;;;","08/Feb/23 13:10;leonard;Alright, looks like current behavior is expected and aligned with Hive&Spark, I plan to to convert this issue to a document improvement ticket to cover the  ""yyyy-MM-dd HH:mm:ss.SSS X"" pattern if no objections. WDYT? [~twalthr][~godfrey][~yunfengzhou];;;","09/Feb/23 01:13;yunfengzhou;Hi [~leonard] , Thanks for driving this issue. I agree with it that the current behavior is better, given that it is better aligned with the common practice taken by other existing projects. We should modify the document of this function to reflect its behavior in this kind of case.;;;","09/Feb/23 16:27;twalthr;Better documentation definitely sounds good to me. It sounds like it interprets {{UTC+X}}? How does the function react if you add something completely unsupported?;;;","10/Feb/23 08:53;leonard;> It sounds like it interprets UTC+X? How does the function react if you add something completely unsupported?

Yes, it interprets UTC+X. An IllegalArgumentException will be thrown If the format pattern is invalid, the default value Long.MIN_VALUE will be returned if the timestamp string can not parse.;;;","12/Mar/23 08:03;ruanhang1993;Hi, all,

I would like to help to improve the document. Please assign this to me if possible. Thanks~;;;","15/Mar/23 15:37;leonard;Resolved in: 
 * master: f3ec5733f4fbc85d7a6bd27484ac10185a36b1ed
 * release-1.17: 7e9cf7af50f87d997dd660bc6300eb791bc0a9cf
 * release-1.16: 3d84e591fa172f7520d6c9fc7d2335a7bf67ecd2;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Mar/24 08:22;leonard;Sorry I forgot close this one ,close as resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest API doc generation failure caused by JobClientHeartbeatHeaders,FLINK-30958,13523715,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,08/Feb/23 06:34,08/Feb/23 10:56,04/Jun/24 20:41,08/Feb/23 10:56,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / REST,,,,,0,pull-request-available,,,,`JobClientHeartbeatHeaders` should override `operationId` since `getHttpMethod` returns `POST`. Otherwise `UnsupportedOperationException` is thrown at `OpenApiSpecGenerator` when generating the REST API doc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29640,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 10:56:00 UTC 2023,,,,,,,,,,"0|z1frg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 10:56;xtsong;* master (1.18): e33034f8aa359c9233e5c1fa570e5270c7a03737
* release-1.17: 353c8016f06c350b05121dc905a37daf33fd1727;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support other missing features (see description),FLINK-30957,13523698,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,08/Feb/23 02:41,10/Feb/23 08:50,04/Jun/24 20:41,10/Feb/23 08:50,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,"# Support weights.
 # Support leaf ID.
 # Support feature importance.
 # Support 1-order gradient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 02:41:11.0,,,,,,,,,,"0|z1frcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Python implementation and documents of GBTClassifier and GBTRegressor.,FLINK-30956,13523697,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,08/Feb/23 02:39,10/Feb/23 08:49,04/Jun/24 20:41,10/Feb/23 08:49,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 02:39:36.0,,,,,,,,,,"0|z1frc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support early stopping with validation set.,FLINK-30955,13523694,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,08/Feb/23 02:27,10/Feb/23 08:49,04/Jun/24 20:41,10/Feb/23 08:49,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 02:27:16.0,,,,,,,,,,"0|z1frbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add estimator and transformer for GBTRegressor,FLINK-30954,13523689,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,08/Feb/23 01:59,10/Feb/23 08:49,04/Jun/24 20:41,10/Feb/23 08:49,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 01:59:29.0,,,,,,,,,,"0|z1frag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add estimator and transformer for GBTClassifier,FLINK-30953,13523688,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,08/Feb/23 01:59,10/Feb/23 08:49,04/Jun/24 20:41,10/Feb/23 08:49,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 01:59:03.0,,,,,,,,,,"0|z1fra8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add main training and transforming part,FLINK-30952,13523687,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,08/Feb/23 01:58,10/Feb/23 08:48,04/Jun/24 20:41,10/Feb/23 08:48,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,Implement main training and transforming part.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-08 01:58:24.0,,,,,,,,,,"0|z1fra0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-29635 Hive sink should support merge files in batch mode,FLINK-30951,13523686,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,luoyuxia,luoyuxia,08/Feb/23 01:54,21/Feb/23 09:10,04/Jun/24 20:41,21/Feb/23 09:10,,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,"The issue aims to verfiy FLINK-29635.

Please verify in batch mode, the document is in [https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/hive/hive_read_write/#file-compaction]:

 

1: enable auto-compaction, write some data to a Hive table which results in the average size of files is less than compaction.small-files.avg-size(16MB by default), verfiy these files should be merged.

2:  enable auto-compaction, set compaction.small-files.avg-size to a smaller values, then write some data to a Hive table which results in the average size of files is greater thant the compaction.small-files.avg-size, verfiy these files shouldn't be merged.

3. set sink.parallelism manually, check the parallelism of the compact operator is equal to sink.parallelism.

4. set compaction.parallelism manually, check the parallelism of the compact operator is equal to compaction.parallelism.

5. set compaction.file-size, check the size of the each target file merged is about the `compaction.file-size`.

 

We shoud verify it with writing non-partitioned table, static partition table, dynamic partition table.

We can find the example sql for how to create & write hive table in the codebase  [HiveTableCompactSinkITCase|[https://github.com/apache/flink/blob/0915c9850d861165e283acc0f60545cd836f0567/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableCompactSinkITCase.java]].

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29635,,,,,,FLINK-31132,,,,,,"20/Feb/23 09:56;fsk119;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13055628/screenshot-1.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 10:17:02 UTC 2023,,,,,,,,,,"0|z1fr9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 09:56;fsk119;1. I submit a job to write records data with/without auto-compaction. With auto-compaction, the records are merged into a target file. 
2. I adjust the target file size and submit again, the compacted file is as expected. 
3. I set the compacter parallelism and the sink parallelism 8, but it only works for the compacter. 
4. I adjust the avg size of the file with a smaller value, the compaction doesn't happens.


About the case 3: the graph in Flink WebUI

 !screenshot-1.png! 
;;;","20/Feb/23 10:17;luoyuxia;[~fsk119] Thanks for testing. If only set sink.parallelism, the compact parallelism should be same to  sink.parallelism. But now, it seems the compact parallelism won't follow sink.parallelism, it looks like a bug. I'll have a look.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-connector-aws-base from Flink main repo,FLINK-30950,13523640,13523636,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,07/Feb/23 23:30,03/Apr/23 08:56,04/Jun/24 20:41,03/Apr/23 08:56,,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,pull-request-available,,,,Remove flink-connector-aws-base from Flink main repo,,,,,,,,,,,FLINK-30949,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 08:56:05 UTC 2023,,,,,,,,,,"0|z1fqzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 08:56;dannycranmer;Merged commit [{{7d29e1b}}|https://github.com/apache/flink/commit/7d29e1beb721ac28fe4b803bce6e8af931e62044] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync flink-connector-aws-base from Flink main to flink-connector-aws,FLINK-30949,13523639,13523636,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,liangtl,liangtl,07/Feb/23 23:29,14/Mar/23 20:34,04/Jun/24 20:41,14/Mar/23 20:34,,,,,,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,,,,,Sync flink-connector-aws-base from Flink main to flink-connector-aws,,,,,,,,,,,,FLINK-30950,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 20:33:54 UTC 2023,,,,,,,,,,"0|z1fqzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 20:33;liangtl;Did a diff between flink/flink-connectors/flink-connector-aws-base and flink-connector-aws/flink-connector-aws-base. Turns out they are the same, excluding some simple renaming;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-avro-glue-schema-registry and flink-json-glue-schema-registry from Flink main repo,FLINK-30948,13523638,13523636,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,07/Feb/23 23:26,06/Mar/23 10:15,04/Jun/24 20:41,21/Feb/23 08:57,,,,,,,1.17.0,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"Remove flink-avro-glue-schema-registry and flink-json-glue-schema-registry from Flink main repo, along with associated end-to-end tests",,,,,,,,,,FLINK-30947,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 08:55:39 UTC 2023,,,,,,,,,,"0|z1fqz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 08:55;martijnvisser;Fixed in:

master: 29f009b7e8c714cd5af0626e9725eb8538a4bd0f
release-1.17: d58829335557dac6ce428df5a80d4244fccf4491;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sync flink-avro-glue-schema-registry and flink-json-glue-schema-registry to flink-connector-aws,FLINK-30947,13523637,13523636,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,liangtl,liangtl,liangtl,07/Feb/23 23:25,22/Feb/23 16:39,04/Jun/24 20:41,22/Feb/23 16:39,,,,,,,aws-connector-4.1.0,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"Copy flink-avro-glue-schema-registry and flink-json-glue-schema-registry, along with associated end-to-end tests to flink-connector-aws",,,,,,,,,,,,FLINK-30948,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 16:38:55 UTC 2023,,,,,,,,,,"0|z1fqyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 16:38;dannycranmer;Merged commit [{{98cde3d}}|https://github.com/apache/flink-connector-aws/commit/98cde3de0c1c16b610c1e3672f1623bc9f5ca486] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate AWS connector formats to flink-connector-aws,FLINK-30946,13523636,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,07/Feb/23 23:23,30/Jun/23 18:42,04/Jun/24 20:41,30/Jun/23 18:41,,,,,,,aws-connector-4.2.0,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,,"Migrate flink-avro-glue-schema-registry and flink-json-glue-schema-registry to flink-connector-aws.
h4.  ",,,,,,,,,,,,,,,,,,,,,,FLINK-30975,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 18:41:48 UTC 2023,,,,,,,,,,"0|z1fqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/23 18:41;dannycranmer;All subtasks complete. Resolving, thanks [~liangtl] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FTS does not support multiple writers into the same table and topic,FLINK-30945,13523528,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,vicky_papavas,vicky_papavas,07/Feb/23 15:43,29/Mar/23 03:06,04/Jun/24 20:41,29/Mar/23 03:06,,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"When creating two different streaming jobs that INSERT INTO the same table and kafka topic, the second job is never able to make progress as the transaction gets constantly aborted due to the producer getting fenced.

FTS should set the transactionalIdPrefix to avoid transactions of different jobs clashing.
{code:java}
2023-02-06 17:13:36,088 WARN org.apache.flink.runtime.taskmanager.Task [] - Writer -> Global Committer -> Sink: end (1/1)#0 (8cf4197af9716623c3c19e7fa3d7c071_b5c8d46f3e7b141acf271f12622e752b_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:323) at org.apache.flink.table.store.connector.sink.StoreWriteOperator.notifyCheckpointComplete(StoreWriteOperator.java:175) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:479) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:413) at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1412) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$15(StreamTask.java:1353) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$18(StreamTask.java:1392) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:383) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:345) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) at java.lang.Thread.run(Thread.java:750) Caused by: org.apache.flink.table.store.shaded.org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one. {code}
Sample queries:
 
 
{code:java}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store'
 );
USE CATALOG table_store_catalog;
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count_kafka (
     word STRING PRIMARY KEY NOT ENFORCED,
     cnt BIGINT
 ) WITH (
     'log.system' = 'kafka',
     'kafka.bootstrap.servers' = 'broker:9092',
     'kafka.topic' = 'word_count_log'
 );
CREATE TEMPORARY TABLE word_table (
     word STRING
 ) WITH (
     'connector' = 'datagen',
     'fields.word.length' = '1'
 );
{code}
 

And the two INSERT jobs:
{code:java}
INSERT INTO word_count_kafka SELECT word, COUNT(*) FROM word_table GROUP BY word;{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 03:06:53 UTC 2023,,,,,,,,,,"0|z1fqao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 07:39;lzljs3620320;[~vicky_papavas] Thanks for reporting! I think this needs to be improved!;;;","17/Feb/23 20:08;binh;[~lzljs3620320] would you assign this to me?;;;","29/Mar/23 03:06;lzljs3620320;https://github.com/apache/incubator-paimon/issues/745;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraphPartitionReleaseTest leaks threads,FLINK-30944,13523524,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,mapohl,mapohl,07/Feb/23 15:08,13/Feb/23 08:30,04/Jun/24 20:41,13/Feb/23 08:30,1.15.3,1.16.1,1.17.0,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,{{ExecutionGraphPartitionReleaseTest}} leaks threads through {{ExecutionGraphPartitionReleaseTest.scheduledExecutorService}}. The {{ScheduledExecutorService}} is instantiated but never shut down.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27518,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 08:30:58 UTC 2023,,,,,,,,,,"0|z1fq9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 15:09;mapohl;We identified the thread leaking while reviewing FLINK-27518;;;","08/Feb/23 03:44;Weijie Guo;Thanks [~mapohl] for reporting this, I'd like to do this work.;;;","13/Feb/23 08:30;mapohl;* master: df23acfc0f124476365370b5e9945e7c957e4fce (with prereq hotfix commits 82ba1f7ab2168b0b0e84bdfe66bb9971b55a7338 and 7bdc7549fdfb4002d09ae390bdef8e4e565eb433)
* 1.17: a5536f2f2f5b0574d8e8ecb768afd049667a1fba (with prereq hotfix commits 8e8aa2aac165620529ef674e7dc5d8e39ddfeae0 and fa85ca20ac5919c52a548a069551d04b00885db1)
* 1.16: 26a8fe566b792d6e49c974978b6cc2347ea8606a (with prereq hotfix commits d4242f11bed61eeba71ac562ecd048489691f93c and 080ddecbb1884eea9deed51f3ed2bc4a48b2de6c);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hide sensitive command-line configurations,FLINK-30943,13523514,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Bo Cui,Bo Cui,07/Feb/23 14:09,14/Feb/23 05:05,04/Jun/24 20:41,09/Feb/23 09:54,,,,,,,,,,,,,,,,,,,,,,,Command Line Client,,,,,0,pull-request-available,,,,"When a job is submitted from the command line, log prints jvm options and application dynamic parameters. 
now only dynamic parameters (`-Dxx=yy`) are masked, but the key-value separation parameter (`--key value`) is not masked.",,,,,,,,,,,,,,,,,,,FLINK-19523,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 05:05:47 UTC 2023,,,,,,,,,,"0|z1fq7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 14:57;martijnvisser;Are we sure that command line configurations are always considered sensitive? I don't think that's always the case. ;;;","07/Feb/23 15:01;martijnvisser;I would argue that providing sensitive data via the command line is insecure by default https://smallstep.com/blog/command-line-secrets/ - So I don't see an obvious reason for Flink to hide already exposed data;;;","09/Feb/23 09:38;pltbkd;Hiding sensitive configurations is already supported if the key and its value are connected. The jira tries to hidden the value who's separated from its key.
However I also agree that we don't need to support such cases. Users can easily change the argument list to the supported format. And the neighboring argument may doesn't have any relation, in which case this feature may cause users confused.;;;","14/Feb/23 05:05;Bo Cui;The jira tries to hidden the value who's separated from its key

--> yes, like the command: flink run-application -Dtaskmanager.cpu.cores=2 --class xxx.xxx.xxx.JAVACLASS --jobParameter1 1 --jobParameter2 2 --jobParameter1 --jobPassword 1234

 

Users can easily change the argument list to the supported format

--> how to change jobPassword ?

thx;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug that the decided parallelism by adaptive batch scheduler may be larger than the max parallelism,FLINK-30942,13523499,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,07/Feb/23 12:23,17/Feb/23 05:54,04/Jun/24 20:41,10/Feb/23 10:45,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Currently, when using the adaptive batch scheduler, the vertex parallelism decided by  forward group may be larger than the global max parallelism(which is configured by option {{parallelism.default}} or {{execution.batch.adaptive.auto-parallelism.max-parallelism}}, see FLINK-30686 for details), which will cause the following exception:

{code:java}
Caused by: java.lang.IllegalArgumentException: Vertex's parallelism should be smaller than or equal to vertex's max parallelism.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.runtime.scheduler.DefaultVertexParallelismInfo.setParallelism(DefaultVertexParallelismInfo.java:95)
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.setParallelism(ExecutionJobVertex.java:317)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.changeJobVertexParallelism(AdaptiveBatchScheduler.java:385)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.initializeVerticesIfPossible(AdaptiveBatchScheduler.java:284)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.onTaskFinished(AdaptiveBatchScheduler.java:183)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:745)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	... 30 more
{code}

Following code can reproduce the above exception:

{code:java}
final Configuration configuration = new Configuration();
configuration.setString(RestOptions.BIND_PORT, ""0"");
configuration.setLong(JobManagerOptions.SLOT_REQUEST_TIMEOUT, 5000L);
configuration.setInteger(
        BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_MAX_PARALLELISM, 2);
configuration.set(
        BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_AVG_DATA_VOLUME_PER_TASK,
        MemorySize.parse(""150kb""));
configuration.set(TaskManagerOptions.MEMORY_SEGMENT_SIZE, MemorySize.parse(""4kb""));
configuration.set(TaskManagerOptions.NUM_TASK_SLOTS, 1);

final StreamExecutionEnvironment env =
        StreamExecutionEnvironment.createLocalEnvironment(configuration);
env.setRuntimeMode(RuntimeExecutionMode.BATCH);
env.setParallelism(4);

final DataStream<Long> source =
        env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                .setParallelism(4)
                .name(""source"")
                .slotSharingGroup(""group1"");

source.forward().map(new NumberCounter()).name(""map"").slotSharingGroup(""group2"");
env.execute();
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31114,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 13:33:12 UTC 2023,,,,,,,,,,"0|z1fq48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 13:33;zhuzh;master:
5a4e0ea31aef51dfc50b9c401c91c7b869a154c4

release-1.17:
7c7156bb486d7b480ead19c0c486c82afcb92c3a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when downloading the flink binaries into the Docker container,FLINK-30941,13523492,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,07/Feb/23 11:55,19/Aug/23 10:35,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,auto-deprioritized-major,test-stability,,,"Creating the Docker container failed because it failed to download the Flink binaries:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5253

{code}
Feb 06 09:00:46 Step 10/14 : RUN set -ex;   wget -nv -O flink.tgz ""$FLINK_TGZ_URL"";     if [ ""$CHECK_GPG"" = ""true"" ]; then     wget -nv -O flink.tgz.asc ""$FLINK_ASC_URL"";     export GNUPGHOME=""$(mktemp -d)"";     for server in ha.pool.sks-keyservers.net $(shuf -e                             hkp://p80.pool.sks-keyservers.net:80                             keyserver.ubuntu.com                             hkp://keyserver.ubuntu.com:80                             pgp.mit.edu) ; do         gpg --batch --keyserver ""$server"" --recv-keys ""$GPG_KEY"" && break || : ;     done &&     gpg --batch --verify flink.tgz.asc flink.tgz;     gpgconf --kill all;     rm -rf ""$GNUPGHOME"" flink.tgz.asc;   fi;     tar -xf flink.tgz --strip-components=1;   rm flink.tgz;     chown -R flink:flink .;     sed -i 's/rest.address: localhost/rest.address: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i 's/rest.bind-address: localhost/rest.bind-address: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i 's/jobmanager.bind-host: localhost/jobmanager.bind-host: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i 's/taskmanager.bind-host: localhost/taskmanager.bind-host: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i '/taskmanager.host: localhost/d' $FLINK_HOME/conf/flink-conf.yaml;
Feb 06 09:00:46  ---> Running in f4bc5a29798a
Feb 06 09:00:46 + wget -nv -O flink.tgz localhost:9999/flink.tgz
Feb 06 09:00:46 failed: Connection refused.
Feb 06 09:00:46 failed: Connection refused.
The command '/bin/sh -c set -ex;   wget -nv -O flink.tgz ""$FLINK_TGZ_URL"";     if [ ""$CHECK_GPG"" = ""true"" ]; then     wget -nv -O flink.tgz.asc ""$FLINK_ASC_URL"";     export GNUPGHOME=""$(mktemp -d)"";     for server in ha.pool.sks-keyservers.net $(shuf -e                             hkp://p80.pool.sks-keyservers.net:80                             keyserver.ubuntu.com                             hkp://keyserver.ubuntu.com:80                             pgp.mit.edu) ; do         gpg --batch --keyserver ""$server"" --recv-keys ""$GPG_KEY"" && break || : ;     done &&     gpg --batch --verify flink.tgz.asc flink.tgz;     gpgconf --kill all;     rm -rf ""$GNUPGHOME"" flink.tgz.asc;   fi;     tar -xf flink.tgz --strip-components=1;   rm flink.tgz;     chown -R flink:flink .;     sed -i 's/rest.address: localhost/rest.address: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i 's/rest.bind-address: localhost/rest.bind-address: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i 's/jobmanager.bind-host: localhost/jobmanager.bind-host: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i 's/taskmanager.bind-host: localhost/taskmanager.bind-host: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml;   sed -i '/taskmanager.host: localhost/d' $FLINK_HOME/conf/flink-conf.yaml;' returned a non-zero code: 4
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30921,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 10:35:09 UTC 2023,,,,,,,,,,"0|z1fq2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:56;mapohl;I'm linking FLINK-30921 because we had the Azure Ubuntu mirror instabilities along this build failure. But it doesn't look like the Ubuntu mirrors not being accessible has something to do with this error.;;;","07/Feb/23 12:36;chesnay;Further up in the logs we can see this:

{code}
Feb 06 08:41:36 + wget -nv -O flink.tgz localhost:9999/flink.tgz
Feb 06 08:41:36 failed: Connection refused.
127.0.0.1 - - [06/Feb/2023 08:41:36] ""GET /flink.tgz HTTP/1.1"" 200 -
Feb 06 08:41:36 2023-02-06 08:41:36 URL:http://localhost:9999/flink.tgz [470024767/470024767] -> ""flink.tgz"" [1]
Feb 06 08:41:36 + [ false = true ]
Feb 06 08:41:36 + tar -xf flink.tgz --strip-components=1
Terminated
{code}

To me it looks like the local webserver crashes for some reason.;;;","07/Feb/23 13:41;mapohl;There's also a ""address already in use"" error in between:
{code}
Feb 06 08:52:45 Step 5/14 : ENV FLINK_TGZ_URL=localhost:9999/flink.tgz     FLINK_ASC_URL=     GPG_KEY=     CHECK_GPG=false
Feb 06 08:52:45  ---> Running in 562dc136a900
Traceback (most recent call last):
  File ""/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
    httpd = socketserver.TCPServer(("""", 9999), handler)
  File ""/usr/lib/python3.8/socketserver.py"", line 452, in __init__
    self.server_bind()
  File ""/usr/lib/python3.8/socketserver.py"", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4971

And a test_docker_embedded_job test failure
{code}
Feb 06 08:52:12 + gpg --batch --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4
Feb 06 08:52:18 The command 'docker build --no-cache --network=host -t test_docker_embedded_job dev/test_docker_embedded_job-ubuntu' (pid: 230499) did not finish after 600 seconds.
Terminated
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4922

{{git clone}} also fails because a previous run wasn't properly cleaned up
{code}
Feb 06 08:42:03 Executing command: git clone https://github.com/apache/flink-docker.git --branch dev-1.16 --single-branch
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Feb 06 08:42:03 Retry 1/5 exited 128, retrying in 1 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Feb 06 08:42:04 Retry 2/5 exited 128, retrying in 2 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Feb 06 08:42:06 Retry 3/5 exited 128, retrying in 4 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Feb 06 08:42:10 Retry 4/5 exited 128, retrying in 8 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Feb 06 08:42:18 Retry 5/5 exited 128, no more retries left.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4709

It feels like this build has generally quite some inconsistencies;;;","22/May/23 10:48;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49065&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=5519;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InterruptedException in ExecutorImplITCase which doesn't fail the test,FLINK-30940,13523491,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,mapohl,mapohl,07/Feb/23 11:50,09/Feb/23 12:01,04/Jun/24 20:41,09/Feb/23 12:01,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,test-stability,,,"We're experiencing a test failure {{CliClientITCase.testSqlStatements}} which might be caused by an {{InterruptedException}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45828&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=46490

{code}
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to fetch results.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$fetchUtilResultsReady$4(ExecutorImpl.java:375)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:398)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:368)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:234)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptExecution$7(ExecutorImplITCase.java:507)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:396)
	... 5 more
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to fetch results.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$fetchUtilResultsReady$4(ExecutorImpl.java:375)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:398)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:368)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:234)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptFetching$8(ExecutorImplITCase.java:515)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:396)
	... 5 more
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to get response.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$executeStatement$2(ExecutorImpl.java:228)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:429)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:210)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptSubmitting$6(ExecutorImplITCase.java:502)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:424)
	... 4 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 12:01:01 UTC 2023,,,,,,,,,,"0|z1fq2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 08:51;fsk119;Hi. Thanks for pointing it out. When a thread is interrupted, it will print the interrupted exception to the {{System.out}} by default. I will fix this;;;","09/Feb/23 12:01;fsk119;Merged into mater: 8ff06420312b87377e36aa43be20390f5a7acf7d

Merged into release-1.17: 72cf4eaa42379ad38fd6bc7cb52330af070cfe63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add preprocessor for GBT algorithms.,FLINK-30939,13523490,13523483,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,07/Feb/23 11:48,10/Feb/23 08:46,04/Jun/24 20:41,10/Feb/23 08:46,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,Add preprocessor for GBT algorithms to transform data to the format booster can handle. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-07 11:48:57.0,,,,,,,,,,"0|z1fq28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-29766 Adaptive Batch Scheduler should also work with hybrid shuffle mode,FLINK-30938,13523489,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,syhily,Weijie Guo,Weijie Guo,07/Feb/23 11:46,21/Feb/23 02:40,04/Jun/24 20:41,21/Feb/23 02:40,1.17.0,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,release-testing,,,,"This ticket aims for verifying FLINK-29766: Adaptive Batch Scheduler should also work with hybrid shuffle mode.

More details about this feature and how to use it can be found in this [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/batch_shuffle/#hybrid-shuffle].

The verification is divided into two parts:

Part I: Verify hybrid shuffle can work with AdaptiveBatchScheduler

Write a simple Flink batch job using hybrid shuffle mode and submit this job. Note that in flink-1.17, AdaptiveBatchScheduler is the default scheduler for batch job, so you do not need other configuration.

Suppose your job's topology like source -> map -> sink, if your cluster have enough slots, you should find that source and map are running at the same time.

Part II: Verify hybrid shuffle can work with Speculative Execution

Write a Flink batch job using hybrid shuffle mode which has a subtask running much slower than others (e.g. sleep indefinitely if it runs on a certain host, the hostname can be retrieved via InetAddress.getLocalHost().getHostName(), or if its (subtaskIndex + attemptNumer) % 2 == 0)

Modify Flink configuration file to enable speculative execution and tune the configuration as you like

Submit the job. Checking the web UI, logs, metrics and produced result.

You should find that once a producer task's one subtask finished, all its consumer tasks can be scheduled in log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29766,,,,,,,,,,,,"20/Feb/23 16:53;syhily;AdaptiveBatchScheduler-job-graph.png;https://issues.apache.org/jira/secure/attachment/13055640/AdaptiveBatchScheduler-job-graph.png","20/Feb/23 16:57;syhily;AdaptiveBatchScheduler-timeline.png;https://issues.apache.org/jira/secure/attachment/13055641/AdaptiveBatchScheduler-timeline.png","20/Feb/23 17:17;syhily;Speculative-Execution-timeline.png;https://issues.apache.org/jira/secure/attachment/13055643/Speculative-Execution-timeline.png","20/Feb/23 16:19;syhily;flink-1.17-branch-log.png;https://issues.apache.org/jira/secure/attachment/13055635/flink-1.17-branch-log.png","20/Feb/23 16:23;syhily;flink-dashborad-version.png;https://issues.apache.org/jira/secure/attachment/13055637/flink-dashborad-version.png","20/Feb/23 16:46;syhily;taskmanager-slots.png;https://issues.apache.org/jira/secure/attachment/13055638/taskmanager-slots.png","08/Feb/23 10:42;Weijie Guo;testAdaptiveBatchJob;https://issues.apache.org/jira/secure/attachment/13055246/testAdaptiveBatchJob","08/Feb/23 10:42;Weijie Guo;testSpeculativeExecution;https://issues.apache.org/jira/secure/attachment/13055247/testSpeculativeExecution",,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 18:19:17 UTC 2023,,,,,,,,,,"0|z1fq20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 09:23;xuzifu;i will get the Jira [~Weijie Guo] ;;;","08/Feb/23 09:24;Weijie Guo;[~xuzifu] Thank you for volunteering, you are assigned.;;;","08/Feb/23 10:43;Weijie Guo;[~xuzifu] I have uploaded the example test jobs, you can refer to it for testing.;;;","08/Feb/23 11:05;xuzifu;ok，thanks  [~Weijie Guo] ;;;","20/Feb/23 06:59;Weijie Guo;Through offline communication, unassigned [~xuzifu] as he has no time to continue to do this.;;;","20/Feb/23 16:23;syhily;Thanks for assigning this test task to me. I have finished the verification by following the instructions from [~Weijie Guo]. The {{AdaptiveBatchScheduler}} and {{SpeculativeExecution}} are working as expert. Here are the detailed test process and report from my side.

h2. Building the flink on {{release-1.17}} branch

I build the flink locally on {{release-1.17}} branch. The latest commit in my local repository is {{6b4745}}. To get the build more faster. I skipped all the code quality check and tests by using command {{mvn clean install -DskipTests -Pfast}}.

 !flink-1.17-branch-log.png|width=500! 

The running flink dashboard also proven this is a build from the {{6b4745}} commit.

 !flink-dashborad-version.png|width=550! 

We start the local standalone flink cluster for simplifying the test. The task manager is configured with 4 slots.

 !taskmanager-slots.png|width=700! 

h2. Test hybrid shuffle with {{AdaptiveBatchScheduler}}

The default config value for {{execution.batch-shuffle-mode}} is {{ALL_EXCHANGES_BLOCKING}}, which means that this is a blocking shuffle. To get the hybrid shuffle mode enabled. We change this value to {{ALL_EXCHANGES_HYBRID_SELECTIVE}}, which means the data could be consumed immediately when the downstream task is available. No need to persist the data to disk.

Given the AdaptiveBatchScheduler is enabled by default. The default {{jobmanager.partition.hybrid.partition-data-consume-constraint}} in this scheduler is {{UNFINISHED_PRODUCERS}}. This means that the produced data can be consumed immediately, no need to wait for the producer finished.

To get hybrid shuffle feature testable. The last thing we need to do is setting enough slots for a task manager. The default flink standalone cluster only has on job manager and one task manager. The test code for this verification is shown in attachments: [^testAdaptiveBatchJob]. It will start a source, a sink and two map functions. So we set the slot to 4 by setting {{taskmanager.numberOfTaskSlots: 4}} in {{flink.yml}}.

Finally, we submit the job [^testAdaptiveBatchJob] to flink cluster. 

h3. Hybrid shuffle in {{AdaptiveBatchScheduler}} report

Since all the map operator will sleep for 5 seconds. The first thing we can find in job graph is that the map operator and sink operator are running simultaneously. If this is a default blocking shuffle, the sink should start after all the map operators have been finished.

!AdaptiveBatchScheduler-job-graph.png|width=600!

The screenshot of the execution timeline also shows that the source, map and sink are almost running in the same time.

!AdaptiveBatchScheduler-timeline.png|width=600!

Finally, the log also confirm this.

{code}
2023-02-20 23:59:16,558 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Sequence Source (1/1)#0 (fb0eccc08c3516b1b8ed16977616b147_bc764cd8ddf7a0cff126f51c16239658_0_0) switched from INITIALIZING to RUNNING.
2023-02-20 23:59:16,563 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: Unnamed (1/1)#0 (fb0eccc08c3516b1b8ed16977616b147_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from INITIALIZING to RUNNING.
2023-02-20 23:59:16,564 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Map (2/2)#0 (fb0eccc08c3516b1b8ed16977616b147_0a448493b4782967b150582570326227_1_0) switched from INITIALIZING to RUNNING.
2023-02-20 23:59:16,564 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Map (1/2)#0 (fb0eccc08c3516b1b8ed16977616b147_0a448493b4782967b150582570326227_0_0) switched from INITIALIZING to RUNNING.
{code}

h2. Test hybird shuffle in Speculative Execution

To get the *Speculative Execution* enabled, we add the {{execution.batch.speculative.enabled: true}} to {{flink.yml}}. We also change the {{execution.batch-shuffle-mode}} to {{ALL_EXCHANGES_HYBRID_FULL}} for persisting all the datas.

The test code [^testSpeculativeExecution] in this verification is that the map task will sleep forever when the index of task plus the number of attempts is even. To get the speculative execution detects such slept tasks. We also add a baseline for detecting this more faster.

{code}
slow-task-detector.execution-time.baseline-ratio: 0.2
slow-task-detector.execution-time.baseline-lower-bound: 0
slow-task-detector.execution-time.baseline-multiplier: 1
execution.batch.speculative.block-slow-node-duration: 0
{code}

Finally, we restart the flink cluster with new configurations and submit the [^testSpeculativeExecution] job to flink cluster. 

h3. Hybrid shuffle in Speculative Execution report

The default value of data consumption constraints in speculative execution is {{ONLY_FINISHED_PRODUCERS}}. This means that if a upstream producer has been finished. We will start a down stream consumer for consuming the data produced by upstream. The screenshot of execution timeline confirms this.

!Speculative-Execution-timeline.png|width=600!

We can find that once the only one source task has been finished, we start two map tasks simultaneously. The first one will hang forever. And the second one will finish soon. So the downstream sink task started on the half way of the map task running time. Because the second map task has been finished.

{code:java}
2023-02-21 00:11:37,993 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Sequence Source (1/1) (ee83d53171224378cb74bdd5681a1399_bc764cd8ddf7a0cff126f51c16239658_0_0) switched from RUNNING to FINISHED.
2023-02-21 00:11:37,996 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job c9562d7c3c921ac7d2f0383c7e250827
2023-02-21 00:11:38,000 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0) switched from CREATED to SCHEDULED.
2023-02-21 00:11:38,001 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2023-02-21 00:11:38,001 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0) switched from SCHEDULED to DEPLOYING.
2023-02-21 00:11:38,001 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Map (1/2) (attempt #0) with attempt id ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0 and vertex id 0a448493b4782967b150582570326227_0 to localhost:41185-563559 @ localhost (dataPort=38645) with allocation id 772ba5994150e188cf2ed4743eafb537
2023-02-21 00:11:38,005 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_1_0) switched from CREATED to SCHEDULED.
2023-02-21 00:11:38,006 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
2023-02-21 00:11:38,068 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0) switched from DEPLOYING to INITIALIZING.
2023-02-21 00:11:38,074 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_1_0) switched from SCHEDULED to DEPLOYING.
2023-02-21 00:11:38,074 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Map (2/2) (attempt #0) with attempt id ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_1_0 and vertex id 0a448493b4782967b150582570326227_1 to localhost:41185-563559 @ localhost (dataPort=38645) with allocation id c3cc3ffeb221d7cd327722feaa5d5557
2023-02-21 00:11:38,104 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_1_0) switched from DEPLOYING to INITIALIZING.
2023-02-21 00:11:38,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0) switched from INITIALIZING to RUNNING.
2023-02-21 00:11:38,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_1_0) switched from INITIALIZING to RUNNING.
2023-02-21 00:11:38,167 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (2/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_1_0) switched from RUNNING to FINISHED.
2023-02-21 00:11:38,168 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2023-02-21 00:11:38,168 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (ee83d53171224378cb74bdd5681a1399_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from CREATED to SCHEDULED.
2023-02-21 00:11:38,169 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
2023-02-21 00:11:38,169 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (ee83d53171224378cb74bdd5681a1399_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from SCHEDULED to DEPLOYING.
2023-02-21 00:11:38,169 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Sink: Unnamed (1/1) (attempt #0) with attempt id ee83d53171224378cb74bdd5681a1399_ea632d67b7d595e5b851708ae9ad79d6_0_0 and vertex id ea632d67b7d595e5b851708ae9ad79d6_0 to localhost:41185-563559 @ localhost (dataPort=38645) with allocation id c3cc3ffeb221d7cd327722feaa5d5557
2023-02-21 00:11:38,187 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (ee83d53171224378cb74bdd5681a1399_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from DEPLOYING to INITIALIZING.
2023-02-21 00:11:38,200 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (ee83d53171224378cb74bdd5681a1399_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from INITIALIZING to RUNNING.
2023-02-21 00:11:38,335 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Map (1/2) (0a448493b4782967b150582570326227_0) is detected as a slow vertex, create and deploy 1 new speculative executions for it.
2023-02-21 00:11:38,339 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_1) switched from CREATED to SCHEDULED.
2023-02-21 00:11:38,340 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
2023-02-21 00:11:38,415 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_1) switched from SCHEDULED to DEPLOYING.
2023-02-21 00:11:38,415 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying Map (1/2) (attempt #1) with attempt id ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_1 and vertex id 0a448493b4782967b150582570326227_0 to localhost:41185-563559 @ localhost (dataPort=38645) with allocation id c053b04b2b8a67a771a10bb4339cbc2b
2023-02-21 00:11:38,435 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_1) switched from DEPLOYING to INITIALIZING.
2023-02-21 00:11:38,443 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_1) switched from INITIALIZING to RUNNING.
2023-02-21 00:11:38,463 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_1) switched from RUNNING to FINISHED.
2023-02-21 00:11:38,466 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
2023-02-21 00:11:38,474 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Canceling 1 un-finished executions of 0a448493b4782967b150582570326227_0 because one of its executions has finished.
2023-02-21 00:11:38,475 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0) switched from RUNNING to CANCELING.
2023-02-21 00:11:38,492 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Unnamed (1/1) (ee83d53171224378cb74bdd5681a1399_ea632d67b7d595e5b851708ae9ad79d6_0_0) switched from RUNNING to FINISHED.
2023-02-21 00:11:38,493 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job c9562d7c3c921ac7d2f0383c7e250827: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
2023-02-21 00:11:38,499 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Map (1/2) (ee83d53171224378cb74bdd5681a1399_0a448493b4782967b150582570326227_0_0) switched from CANCELING to CANCELED.
2023-02-21 00:11:38,500 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Test Speculative Execution Job (c9562d7c3c921ac7d2f0383c7e250827) switched from state RUNNING to FINISHED.
{code}

The execution log shows a detailed view of the speculative execution. The {{Deploying Map (1/2)}} has shown twice because its task index is 0. It will sleep forever in the first execution. And when we redeployed this map task, the attempt number is 1. This make the sum of task index and attempt number is 1, which isn't a even number. So the map task can bypass the sleep logic and finished as expect. Because the {{full spilling strategy}} we enabled before, only the map task will be restarted.;;;","20/Feb/23 17:45;syhily;[~Weijie Guo] Can you confirm my report is correct?;;;","20/Feb/23 18:19;Weijie Guo;Thanks [~syhily] very much. What a perfect and excellent test report!  Which can be used as a reference example for future release testing work. There is no doubt that your test process is completely correct. I know that there may be a lot of imperfections in the document, especially for a person who has not much experience in the basic modules related to runtime. I'm sorry if it causes confusion to you and I will continuously improve the quality of related documents. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for GBTClassifier and GBTRegressor,FLINK-30937,13523483,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,hongfanxo,hongfanxo,07/Feb/23 11:38,10/Feb/23 08:57,04/Jun/24 20:41,10/Feb/23 08:57,,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,"Add Transformer and Estimator for GBTClassifier and GBTRegressor.

They are supposed to cover most features in SparkML provides, except for the following features:

 
 # Support early-stopping with validation set.
 # Support weights.
 # Support leaf ID.
 # Support feature importance.
 # Support 1-order gradient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-07 11:38:12.0,,,,,,,,,,"0|z1fq0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLINK-29941 Support SQL Client connects to SQL Gateway,FLINK-30936,13523468,13523226,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,luoyuxia,renqs,renqs,07/Feb/23 09:33,22/Feb/23 01:39,04/Jun/24 20:41,22/Feb/23 01:38,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,release-testing,,,,"The issue aims to verify [FLIP-275 Support SQL Client connects to SQL Gateway|https://cwiki.apache.org/confluence/display/FLINK/FLIP-275%3A+Support+Remote+SQL+Client+Based+on+SQL+Gateway]

Please verify:

In embedded mode: 
 # All CLI parameter still work in the embedded mode including -i, -f, --jar, --libary
 # Please submit DDL/DML to verify SQL Client can execute SQL correctly
 # Please submit a query and use CTRL+C to cancel the query to make sure SQL Client can cancel the job
 # Please submit a SQL with error to make sure the Client can display exception clearly with/without {{sql-client.verbose}} option

In  gateway mode:

      1. Please verify SQL Client can connect to the gateway with --endpoint parameter
      2. When connecting to release-1.16 Gateway, the SQL Client stop with clear exception.
      3. Leave the SQL Client alone for a long time, the SQL Client is still alive and can submit SQL.

If you meet any problems, it's welcome to ping me directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29941,,,,,,FLINK-31136,FLINK-31137,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 06:40:31 UTC 2023,,,,,,,,,,"0|z1fpxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 09:57;mapohl;[~fsk119] did you intentionally assign you to this task? I guess, the intention is that someone who didn't contribute the feature actually does the testing.;;;","10/Feb/23 02:29;fsk119;Yes, you are right. I have unassigned me.;;;","16/Feb/23 04:22;luoyuxia;[~fsk119] If you don't mind, I would like to take it. ;;;","20/Feb/23 06:40;luoyuxia;Verify embedded mode:

1: try with different CLI parameters such as -i, -f, --jar, it still works

2: sumbit DDL/DML, it works

3: CTRL+C can cancel job

4: with/without {{sql-client.verbose}} option will show excpetion, {{sql-client.verbose will also the full exception stack.}}

 

In  gateway mode:

1: SQL Client can connect to the gateway with --endpoint parameter, and sumbit DDL/DML, it works

2: When connecting to release-1.16 Gateway, the SQL Client stop with the message ""Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Currently, SQL Client only supports to connect to the REST endpoint with API version larger than V1""

3: Leave the SQL Client alone for a long time, the SQL Client is still alive and can submit SQL.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add KafkaSerializer deserialize check when using SimpleVersionedSerializer,FLINK-30935,13523452,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,taoran,taoran,07/Feb/23 08:14,04/Apr/24 12:06,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,auto-deprioritized-major,pull-request-available,,,"Current kafka many implemented serializers do not deal with version check while other implementations of SimpleVersionedSerializer supports it.

we can add it like many other connectors's implementation in case of incompatible or corrupt state when restoring from checkpoint.

 
{code:java}
@Override
public int getVersion() {
    return CURRENT_VERSION;
}

@Override
public KafkaPartitionSplit deserialize(int version, byte[] serialized) throws IOException {
    try (ByteArrayInputStream bais = new ByteArrayInputStream(serialized);
            DataInputStream in = new DataInputStream(bais)) {
        String topic = in.readUTF();
        int partition = in.readInt();
        long offset = in.readLong();
        long stoppingOffset = in.readLong();
        return new KafkaPartitionSplit(
                new TopicPartition(topic, partition), offset, stoppingOffset);
    }
} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 10 22:35:05 UTC 2023,,,,,,,,,,"0|z1fpts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 10:01;becket_qin;Sounds good to me.;;;","07/Mar/23 11:11;taoran;HI.  [~becket_qin]  PTAL. [https://github.com/apache/flink/pull/21904] thanks.;;;","02/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor ComputedColumnAndWatermarkTableITCase and StreamingWarehouseITCase to get rid of managed table,FLINK-30934,13523429,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,07/Feb/23 06:59,09/Feb/23 11:22,04/Jun/24 20:41,09/Feb/23 08:13,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 08:13:33 UTC 2023,,,,,,,,,,"0|z1fpoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 08:13;lzljs3620320;master: f9025336ef19ee1fe4964498971c2186d49458b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Result of join inside iterationBody loses max watermark,FLINK-30933,13523406,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,07/Feb/23 06:06,19/Apr/23 01:38,04/Jun/24 20:41,19/Apr/23 01:38,ml-2.0.0,ml-2.1.0,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"Currently if we execute a join inside an iteration body, the following program produces empty output. (In which the right result should be a list with \{2}.
{code:java}
public class Test {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream<Tuple2<Long, Integer>> input1 =
                env.fromElements(Tuple2.of(1L, 1), Tuple2.of(2L, 2));

        DataStream<Tuple2<Long, Long>> input2 =
                env.fromElements(Tuple2.of(1L, 2L), Tuple2.of(2L, 3L));

        DataStream<Tuple2<Long, Long>> iterationJoin =
                Iterations.iterateBoundedStreamsUntilTermination(
                                DataStreamList.of(input1),
                                ReplayableDataStreamList.replay(input2),
                                IterationConfig.newBuilder()
                                        .setOperatorLifeCycle(
                                                IterationConfig.OperatorLifeCycle.PER_ROUND)
                                        .build(),
                                new MyIterationBody())
                        .get(0);

        DataStream<Long> left = iterationJoin.map(x -> x.f0);
        DataStream<Long> right = iterationJoin.map(x -> x.f1);
        DataStream<Long> result =
                left.join(right)
                        .where(x -> x)
                        .equalTo(x -> x)
                        .window(EndOfStreamWindows.get())
                        .apply((JoinFunction<Long, Long, Long>) (l1, l2) -> l1);

        List<Long> collectedResult = IteratorUtils.toList(result.executeAndCollect());
        List<Long> expectedResult = Arrays.asList(2L);
        compareResultCollections(expectedResult, collectedResult, Long::compareTo);
    }

    private static class MyIterationBody implements IterationBody {
        @Override
        public IterationBodyResult process(
                DataStreamList variableStreams, DataStreamList dataStreams) {
            DataStream<Tuple2<Long, Integer>> input1 = variableStreams.get(0);
            DataStream<Tuple2<Long, Long>> input2 = dataStreams.get(0);

            DataStream<Long> terminationCriteria = input1.flatMap(new TerminateOnMaxIter(1));

            DataStream<Tuple2<Long, Long>> res =
                    input1.join(input2)
                            .where(x -> x.f0)
                            .equalTo(x -> x.f0)
                            .window(EndOfStreamWindows.get())
                            .apply(
                                    new JoinFunction<
                                            Tuple2<Long, Integer>,
                                            Tuple2<Long, Long>,
                                            Tuple2<Long, Long>>() {
                                        @Override
                                        public Tuple2<Long, Long> join(
                                                Tuple2<Long, Integer> longIntegerTuple2,
                                                Tuple2<Long, Long> longLongTuple2)
                                                throws Exception {
                                            return longLongTuple2;
                                        }
                                    });

            return new IterationBodyResult(
                    DataStreamList.of(input1), DataStreamList.of(res), terminationCriteria);
        }
    }
}
{code}
 

There are two possible reasons:
 * The timer in `HeadOperator` is not a daemon process and it does not exit even flink job finishes.
 * The max watermark from the iteration body is missed.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:38:45 UTC 2023,,,,,,,,,,"0|z1fpjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:38;lindong;Merged to apache/flink-ml master branch 80fd4dfb843aee1d9cfd93130cfff016a9966b7b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enabling producer metrics for KafkaSink is not documented,FLINK-30932,13523384,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,07/Feb/23 02:18,26/Jan/24 12:54,04/Jun/24 20:41,,1.17.0,,,,,,kafka-4.0.0,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,,,,,Users can enable producer metrics by setting `register.producer.metrics` to True. We should expose this as a ConfigOption to automate it with Flink's documentation process. Kafka Table connector config options are already auto-generated,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-07 02:18:45.0,,,,,,,,,,"0|z1fpeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive scheduler wastes slots for multiple slot sharing groups with different max parallelism,FLINK-30931,13523257,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mxm,mxm,06/Feb/23 15:19,06/Feb/23 15:21,04/Jun/24 20:41,06/Feb/23 15:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"The adaptive scheduler assumes all slot sharing groups have the same maximum parallelism which can yield to allocating too many slots for smaller slot sharing groups.

See https://github.com/apache/flink/blob/2ae5df278958073fee63b2bf824a53a28a21701b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/allocator/SlotSharingSlotAllocator.java#L97",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30895,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-06 15:19:20.0,,,,,,,,,,"0|z1fomg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatically determine Flink binary download URL from version,FLINK-30930,13523237,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/Feb/23 13:29,07/Feb/23 14:54,04/Jun/24 20:41,07/Feb/23 14:54,,,,,,,,,,,,,,,,,,,,,,,Build System / CI,,,,,0,pull-request-available,,,,"Currently the shared connector CI workflow accepts a binary url and caching flag.
This isn't bad and should be kept, but for all current repos we can determine the URL within the workflow from the passed Flink version.
This would make management a bit easier.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 14:54:03 UTC 2023,,,,,,,,,,"0|z1foi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 14:54;chesnay;ci_utils: 08c68947b9c76fbb959cb4b3cbd6ea8df4adb7f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add helpful message when ElasticsearchSink.Builder.build() throws an IllegalArgumentException.,FLINK-30929,13523235,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kennywgx,kennywgx,06/Feb/23 13:00,07/Feb/23 08:22,04/Jun/24 20:41,07/Feb/23 03:29,1.13.6,,,,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,0,,,,,"When I test flink-connector-elasticsearch on my project. And *I got an IllegalArgumentException with nothing helpful message.* 

*Here's the exception message:* 

 
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException
    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
    at org.apache.flink.streaming.connectors.elasticsearch7.Elasticsearch7ApiCallBridge.<init>(Elasticsearch7ApiCallBridge.java:61)
    at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink.<init>(ElasticsearchSink.java:74)
    ... {code}
 

*I could not see which exactly the argument was illegal.*

After I read the code in (Elasticsearch7ApiCallBridge.java:61), I realized that proberbly  the ES hosts was empty. And finally fixed it.

!image-2023-02-06-21-28-37-835.png!

I think helpful message should be printed when such an important argument is illegal and fails to build the connector.

And I'd love to improve it. thanks

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/23 13:28;kennywgx;image-2023-02-06-21-28-37-835.png;https://issues.apache.org/jira/secure/attachment/13055182/image-2023-02-06-21-28-37-835.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 03:20:38 UTC 2023,,,,,,,,,,"0|z1fohk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 15:33;martijnvisser;[~kennywgx] Please validate that this is still the case with a supported version of Flink, especially given that in Flink 1.15 the Elasticsearch connectors have been ported to new interfaces https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/#port-elasticsearch-sink-to-new-unified-sink-api-flip-143;;;","07/Feb/23 03:20;kennywgx;[~martijnvisser] I have checked the latest version. The case,  as well as other property check statements, has been improved in the new implement already, thx. I will close this ticket
{code:java}
public B setHosts(HttpHost... hosts) {
    Preconditions.checkNotNull(hosts);
    Preconditions.checkState(hosts.length > 0, ""Hosts cannot be empty."");
    this.hosts = Arrays.asList(hosts);
    return this.self();
}{code}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use temurin JRE in test hadoop docker image,FLINK-30928,13523232,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,06/Feb/23 12:49,15/Feb/23 16:57,04/Jun/24 20:41,15/Feb/23 16:57,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,,,,"OpenJDK is deprecated, please see: https://hub.docker.com/_/openjdk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 16:57:54 UTC 2023,,,,,,,,,,"0|z1fogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 16:57;gyfora;merged to main d7be0ca8cfe8059c4701292be785c63b0b9d256a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Several tests started generate output with two non-abstract methods  have the same parameter types, declaring type and return type",FLINK-30927,13523230,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,KristoffSC,Sergey Nuyanzin,Sergey Nuyanzin,06/Feb/23 12:45,09/Feb/23 09:33,04/Jun/24 20:41,09/Feb/23 09:33,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"e.g. org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase#testUserDefinedFunctions

 

it seems during code splitter it starts generating some methods with same signature

 

{noformat}

org.codehaus.janino.InternalCompilerException: Compiling ""MatchRecognizePatternProcessFunction$77"": Two non-abstract methods ""default void MatchRecognizePatternProcessFunction$77.processMatch_0(java.util.Map, org.apache.flink.cep.functions.PatternProcessFunction$Context, org.apache.flink.util.Collector) throws java.lang.Exception"" have the same parameter types, declaring type and return type

{noformat}

 

Probably could be a side effect of https://issues.apache.org/jira/browse/FLINK-27246",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:33:08 UTC 2023,,,,,,,,,,"0|z1fogg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 13:54;KristoffSC;I already have fix for this, will provide PR shortly.
It's caused by https://github.com/apache/flink/pull/21393.

Could someone assign this ticket to me?;;;","06/Feb/23 14:02;mapohl;Thanks, [~KristoffSC]. I assigned the issue to you.;;;","06/Feb/23 14:25;KristoffSC;Pull request available
https://github.com/apache/flink/pull/21871;;;","06/Feb/23 14:44;KristoffSC;Provided PR above is fixing the reported issue. 

However CI build was not failing due to this problem. The reason why it was not failing is that code splitter has a safety net, that whenever rewritten code fails the compilation, Flink tries to use original code + print failing class into the logs. That is how the problem was spotted. 

Maybe it would worth to add an enhancement such this issue would in fact failed the build? A separate issue?;;;","06/Feb/23 20:56;KristoffSC;OK,
CI build is green for provided PR, also I dont see any `InternalCompilerException ... Two non-abstract methods` exception in table_ci_table nor other tests from flink-table-planer.

I would appreciate for review for this small Bug FIX PR and sorry for any inconvenience caused by this.;;;","07/Feb/23 09:37;KristoffSC;PR needs to be merged to 1.17 branch aswell. ;;;","07/Feb/23 10:56;KristoffSC;master: 96a296db723575d64857482a1278744e4c41201f

PR for 1.17 - https://github.com/apache/flink/pull/21879;;;","09/Feb/23 09:33;TsReaper;master: 96a296db723575d64857482a1278744e4c41201f
release-1.17: fd2ccf2d9586e2ffb92d8a6ccb5a5a303d32ef2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Test Flink Release 1.17,FLINK-30926,13523226,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,renqs,renqs,06/Feb/23 11:57,14/Dec/23 11:47,04/Jun/24 20:41,27/Feb/23 10:39,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Tests,,,,,0,release-testing,,,,"This is an umbrella ticket for the Flink 1.17 testing efforts. Please prepare for the release testing by creating child testing tasks for the new features.

Tickets for testing tasks should be opened with:
 * Priority: Blocker
 * Fix Version: 1.17.0
 * Label: release-testing

At the meantime, please update column value of `X-team verified` in the [1.17 Release Wiki page|https://cwiki.apache.org/confluence/display/FLINK/1.17+Release].",,,,,,,,,,,,,FLINK-33828,FLINK-31150,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:39:04 UTC 2023,,,,,,,,,,"0|z1fofk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 10:39;mapohl;I'm closing this issue considering all release-testing tasks to be finalized.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs for the SQL Client gateway mode,FLINK-30925,13523224,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,06/Feb/23 11:46,08/Feb/23 02:40,04/Jun/24 20:41,08/Feb/23 02:40,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,,,,,,,,,Documentation,Table SQL / Client,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 02:40:20 UTC 2023,,,,,,,,,,"0|z1fof4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 02:40;fsk119;Merged into master: 
76a6edc7ff2f416920b6ff67d2e2b69d21ffcbab
bd33e02547ec1faef9195fd6968803f76917c55e

Merged into release-1.17:
78e2c9a13043d926bb1c4efb62f45cf4ef38561f
ace89597edb5074a7455d6d69b0d087bdfb8704e
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Conversion issues between timestamp and bingint,FLINK-30924,13523222,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,hackergin,hackergin,06/Feb/23 11:40,10/Feb/23 08:16,04/Jun/24 20:41,10/Feb/23 07:40,1.16.1,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"When casting to convert timestamp and bigint, the following exception is thrown: 
{code:java}
//代码占位符
org.apache.flink.table.api.ValidationException: The cast from NUMERIC type to TIMESTAMP type is not allowed. It's recommended to use TO_TIMESTAMP(FROM_UNIXTIME(numeric_col)) instead, note the numeric is in seconds.

{code}
However, the FROM_UNIXTIME function will use the local time zone for conversion, but the TO_TIMESTAMP function will not use the local time zone but will use the UTC time zone conversion, so that the actual result  in the  wrong result.

 

The following is an example of the results of the test
{code:java}
//代码占位符

Flink SQL> SET 'table.local-time-zone' = 'Asia/Shanghai';
Flink SQL> select TO_TIMESTAMP(FROM_UNIXTIME(0));

// result 
                 EXPR$0
 1970-01-01 08:00:00.000

{code}
  

 

UNIX_TIMESTAMP(CAST(timestamp_col AS STRING)) has the same problem. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 07:19:54 UTC 2023,,,,,,,,,,"0|z1foeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 15:37;martijnvisser;[~hackergin] The used timezone in {{FROM_UNIXTIME}} should depend on the defined session time zone as mentioned in the docs https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/

{code:java}
The return value is expressed in the session time zone (specified in TableConfig). E.g., FROM_UNIXTIME(44) returns ‘1970-01-01 00:00:44’ if in UTC time zone, but returns ‘1970-01-01 09:00:44’ if in ‘Asia/Tokyo’ time zone.
{code}

When trying the example from the doc, I get the result as specified returned. 

;;;","06/Feb/23 16:54;hackergin;[~martijnvisser]   Yes, FROM_UNIXTIME should use the session time zone. But  I don't think we should use session time zone when convert bigint(timestamp mills) to timestamp (TIMESTAMP WITH OUT TIMEZONE) . 

when convert 0 to TIMESTAMP,  we should alway get  TIMESTAMP '1970-01-01 00:00:00' ? ;;;","07/Feb/23 13:51;martijnvisser;[~hackergin] 

> when convert 0 to TIMESTAMP,  we should alway get  TIMESTAMP '1970-01-01 00:00:00' ? 

That's not always being returned for me. When doing this in Flink SQL client on 1.16.1:

After starting SQL Client:

{code:sql} select TO_TIMESTAMP(FROM_UNIXTIME(0)); {code} 

Returns:

| 1970-01-01 01:00:00.000

After that, running:

{code:sql}
SET 'table.local-time-zone' = 'Asia/Shanghai';
select TO_TIMESTAMP(FROM_UNIXTIME(0)); 
{code} 

Returns:

| 1970-01-01 08:00:00.000

That looks correct to me. ;;;","08/Feb/23 17:04;hackergin;[~martijnvisser]   The result for TO_TIMESTAMP(FROM_UNIXTIME(0)) is exactly correct.  But the question is, When converting BIGINT type to TIMESTAMP type

 , should we recommend users to use TO_TIMESTAMP(FROM_UNIXTIME(xxx)) ? 

 

There is the result when  running `cast 0 to  TIMESTAMP` on Flink1.12: 

Running: 
{code:sql}
select cast(0 as TIMESTAMP);{code}
Returns: 
{panel}
 1970-01-01T00:00
{panel}
 

Running: 
{code:sql}
 SET 'table.local-time-zone' = 'Asia/Shanghai';
 select cast(0 as timestamp); {code}
Returns: 
{panel}
 1970-01-01T00:00
{panel}
too. 

The data of BIGINT type is usually an absolute number of seconds, it is time zone independent, it should not be changed in any time zone, and the data of TIMESTAMP type is also time zone independent data, when we convert these two types, We should not rely on session time zone。  Maybe my understanding is wrong, but at least this is the behavior in 1.12。 

 

 ;;;","08/Feb/23 17:52;martijnvisser;I don't think that a BIGINT value is always time zone independent. It depends on the incoming data, right? ;;;","09/Feb/23 14:43;hackergin;[~martijnvisser] 

> I don't think that a BIGINT value is always time zone independent. It depends on the incoming data, right?

Maybe that's right, But most of the scenarios I've come across where the BIGINT value is an absolute number of seconds.

 

And， Here is a description of the TO_TIMESTAMP function,  It is clearly stated here that the string should under the 'UTC+0'  time zone

> Converts date time string string1 with format string2 (by default: ‘yyyy-MM-dd HH:mm:ss’) under the ‘UTC+0’ time zone to a timestamp.;;;","09/Feb/23 14:56;martijnvisser;[~hackergin] I would argue that ""under the ‘UTC+0’ time zone"" is indeed misleading and could be replaced with something like ""without taking a timezone into consideration"". ;;;","09/Feb/23 15:14;hackergin;[~martijnvisser]   That's ture, I took a closer look at the definition of the TIMESTAMP type, which should not carry any time zone meaning, just a wrapper of STRING (year-month-day).

 

Thank you for your patience, I didn't really understand the meaning of TIMESTAMP before.   But at least, we found a small problem in the documentation. 

 

 

 ;;;","09/Feb/23 16:28;twalthr;[~leonard] has put a lot of this. Maybe he has some input?;;;","10/Feb/23 07:00;leonard;{code:java}
Flink SQL> SET 'table.local-time-zone' = 'Asia/Shanghai';
Flink SQL> select TO_TIMESTAMP(FROM_UNIXTIME(0));
// result 
 1970-01-01 08:00:00.000
{code}
Hi, [~hackergin] 
The result is also reasonable and correct from my understanding. Time is a complex topic, and I'll try to explain some part of it and make your case simpler to understand.

When we use a long value with BIGINT(or numeric SQL type) to represent a time, it usually comes from the upstream system's unix time (e.g. System.currentMills() in java, unix time in Linux OS). We may have some question about the long value.

*Q1: What's the meaning of the long value?*
 * It means the *past period* since a fixed point in the absolute timeline, the fixed point people called it epoch,
the wall-clock is 1970-01-01 00:00:00 in region whose time zone is UTC+0 when epoch point arrived，
the wall-clock is 1970-01-01 01:00:00 in region whose time zone is UTC+1 when epoch point arrived, and so on.

*Q2:Thus, a epoch 4 seconds means the time past 4 seconds since the epoch point, what the wall-clock is in UTC+0 region and UTC+1 region at this time point？*
I guess you could give the right wall-clocks:
1970-01-01 00:00:04 in UTC+0 regions, 
1970-01-01 01:00:04 in UTC+1 regions.

*Q3: Is the epoch seconds related to timezone?* 
No, it means the past time period , the past time period is same and equal in any region，it usually represented in a long value in various systems as well as Flink SQL.

*Q4:How we interpret the epoch seconds to a human readable timestamp expression like a wall-clock ?* 
Yes, we need timezone here. 
We interpret epoch 4 seconds to wall-clock 1970-01-01 01:00:04 in these regions with UTC+1 timezone, 
We interpret epoch 4 seconds to wall-clock 1970-01-01 08:00:04 in these regions with UTC+8 timezone.
Here you should understand why the conversion between timestamp and bingint in Flink SQL is correct.

I didn't explain the SQL type TIMESTAMP and TIMESTAMP_LTZ, nor the concept Instant, time zone. If you want to dig more about the type and concept , I can give the conclusion as following :
 * l{*}ong epoch second{*} matches concept *Instant*  matches SQL type *TIMESTAMP_LTZ*
 * *timestamp string* matches concept *wall-clock* matches SQL type *TIMESTAMP*;;;","10/Feb/23 07:19;hackergin;[~leonard]    Thank you very much for the answer.  In my past knowledge, I thought that TIMESTAMP is the UTC-0 time zone by default, but in fact it should not be bound to any time zone meaning, it can only represent the year, month, day, hour, minute, and second.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide single script for installing Hugo,FLINK-30923,13523220,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ericbrzezenski,mapohl,mapohl,06/Feb/23 11:30,23/Aug/23 13:47,04/Jun/24 20:41,23/Aug/23 13:47,1.17.0,,,,,,1.19.0,,,,,,,,,,,,,,,,Documentation,,,,,0,auto-deprioritized-major,pull-request-available,starter,,"Currently, we have multiple locations to install hugo. In the past this caused problems because of different Hugo versions being used for building the docs. It depends on which script triggered the docs build (see [ci/docs.sh|https://github.com/apache/flink/blob/de368acf0038328a751507a2fa7cb0989d6312e7/tools/ci/docs.sh#L20] and [.github/workflows/docs.sh:26|https://github.com/apache/flink/blob/278642219ebf4b68a02a3901fe06a7cb006d105b/.github/workflows/docs.sh#L26]). We could have a {{docs/setup_hugo.sh}} next to {{docs/setup_docs.sh}} that provides this functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26658,,,,,,,,,,,FLINK-32902,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 13:47:02 UTC 2023,,,,,,,,,,"0|z1foe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 03:43;ericbrzezenski;Would you mind assigning this ticket to me please? i think i understood the description and pushed out a change: https://github.com/apache/flink/pull/22198;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Aug/23 13:47;mapohl;master (1.19): 5c1e9f3b1449cb77276d578b344d9a69c7cf9a3c

Thanks for your contribution, [~ericbrzezenski] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL validate fail in parsing writable metadata,FLINK-30922,13523211,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,tanjialiang,tanjialiang,06/Feb/23 10:35,18/Oct/23 01:20,04/Jun/24 20:41,06/Mar/23 14:32,1.16.1,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When i tried an simple demo sql with writing metadata to the kafka in flink sql client
{code:java}
CREATE TABLE KafkaTable (
  `user_id` BIGINT,
  `item_id` BIGINT,
  `behavior` STRING,
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
) WITH (
  'connector' = 'kafka',
  'topic' = 'user_behavior',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'testGroup',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'csv'
)

INSERT INTO KafkaTable(user_id, ts) SELECT '1', CURRENT_TIMESTAMP; {code}
 

it will be throw an error
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to parse statement: INSERT INTO KafkaTable(user_id, ts) SELECT '1', CURRENT_TIMESTAMP;
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:174) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl$$Lambda$364/1900307803.apply(Unknown Source) ~[?:?]
        at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:295) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:280) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:228) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16.1.jar:1.16.1]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 33 to line 1, column 34: Unknown target column 'ts'
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 33 to line 1, column 34: Unknown target column 'ts'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_41]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_41]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_41]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[?:1.8.0_41]
        at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:401) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:389) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:172) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$$$Lambda$610/614335089.apply(Unknown Source) ~[?:?]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike$$Lambda$329/456314134.apply(Unknown Source) ~[?:?]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50) ~[?:?]
        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown target column 'ts'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_41]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_41]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_41]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[?:1.8.0_41]
        at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) ~[?:?]
        at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:401) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:389) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:172) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$$$Lambda$610/614335089.apply(Unknown Source) ~[?:?]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike$$Lambda$329/456314134.apply(Unknown Source) ~[?:?]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50) ~[?:?]
        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more{code}
 ",,,,,,,,,,,,,,,,,,,FLINK-28650,,,,,FLINK-31313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 14:32:16 UTC 2023,,,,,,,,,,"0|z1foc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 08:48;csq;Hi [~tanjialiang], thank you for reporting the issue. I have reproduced the same error with the code you provided. 
According to [FLIP-107|https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Handling+of+metadata+in+SQL+connectors], it is possible to write metadata columns in SQL.

The cause of this error is a bug that it excludes all computed columns and metadata columns when doing appendPartitionAndNullsProjects in PreValidateReWriter. Actually, it is expected to include all persisted columns. I would like to fix it. ;;;","09/Feb/23 03:00;csq;I have created a pull request to fixed the issue, Anyone who help review the PR will be highly appreciated.;;;","10/Feb/23 09:03;tanjialiang;Great! Thanks for [~csq] 's contribute. Hope to merge it soon.;;;","06/Mar/23 14:32;lincoln.86xy;fixed in master: a2d78e60a96616ad9f575c7a3391f7322cb220af
release-1.17: 865a05678a0cee4a3d013ddc8685d9420cc59b12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Too many CI failed due to ""Could not connect to azure.archive.ubuntu.com""",FLINK-30921,13523205,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,fanrui,fanrui,06/Feb/23 09:59,03/May/23 08:49,04/Jun/24 20:41,03/May/23 08:49,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,,,,,,,,,Test Infrastructure,,,,,0,pull-request-available,test-stability,,,"!image-2023-02-06-17-59-20-019.png!

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a",,,,,,,,,,,,,,,,,,,FLINK-25292,,,,,,,,,FLINK-30152,,,,,,,,,FLINK-30941,,,,,,"06/Feb/23 09:59;fanrui;image-2023-02-06-17-59-20-019.png;https://issues.apache.org/jira/secure/attachment/13055175/image-2023-02-06-17-59-20-019.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 03 08:49:54 UTC 2023,,,,,,,,,,"0|z1foaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 10:06;fanrui;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45764&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","06/Feb/23 12:24;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45759&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=bea52777-eaf8-5663-8482-18fbc3630e81;;;","06/Feb/23 12:41;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45778&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=34;;;","06/Feb/23 12:42;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45761&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=34;;;","06/Feb/23 12:46;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45745&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=11410;;;","06/Feb/23 12:55;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","06/Feb/23 13:01;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45763&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","06/Feb/23 13:10;leonard;This is an azure mirror crash issue, please see more info here https://github.com/actions/runner-images/issues/675,
I think we can avoid use azure mirror to fix this ticket;;;","06/Feb/23 16:19;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45778&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=35;;;","06/Feb/23 16:20;mapohl;Same build, different jobs:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45788&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5035
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45788&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=35;;;","06/Feb/23 16:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45779&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=39;;;","06/Feb/23 16:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45772&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5364;;;","06/Feb/23 16:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45761&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=33;;;","06/Feb/23 16:39;mapohl;There's [actions/runner-images:#7048|https://github.com/actions/runner-images/issues/7048] where users are reporting issues with the Ubuntu mirrors ;;;","06/Feb/23 19:49;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45800&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=25;;;","07/Feb/23 02:20;fanrui;I see it's recovered, the new CI is green.;;;","07/Feb/23 02:53;leonard;I downgrade the issue priority to Critical as it's an external(azure infra) service crash and has recovered, we have use this service for a long time and thus it should not block our release.

 Sure that we need to fix this in our community.;;;","06/Mar/23 08:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46730&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=35;;;","06/Mar/23 09:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46766&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=35;;;","28/Mar/23 09:53;mapohl;Slightly different error but same outcome: 
{code}
After this operation, 771 kB of additional disk space will be used.
Err:1 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libio-pty-perl amd64 1:1.12-1
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:2 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libipc-run-perl all 20180523.0-2
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:3 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 libtime-duration-perl all 1.21-1
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:4 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libtimedate-perl all 2.3200-1
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:5 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 moreutils amd64 0.63-1
  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.12-1_amd64.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_20180523.0-2_all.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/libt/libtime-duration-perl/libtime-duration-perl_1.21-1_all.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libt/libtimedate-perl/libtimedate-perl_2.3200-1_all.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.63-1_amd64.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47642&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=25;;;","03/May/23 08:49;mapohl;master: 742685b76c7f001a08799a539cad2bb683d5d29d
1.17: 13fc226d4c049a63efcce57460f7dedc331f088f
1.16: d3f93d0fa1c25a4525dbd411523c891bed2d95d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"K8 cluster autoscaler | exclude operator ids from scaler",FLINK-30920,13523202,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaurav726,gaurav726,gaurav726,06/Feb/23 09:54,19/Apr/23 14:50,04/Jun/24 20:41,28/Feb/23 11:08,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,1,pull-request-available,,,,"Sometime in cases of sink operator ids, where logic is heavy group aggregation and scan mode is earliest, flink k8 operator tries to scale/downscale sink operator ids as well, there should be a way where user can give list of operator ids/vertices where cluster autoscaler doesn't perform any scaling action on a configurable list of operator ids/vertices

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:57;gaurav726;sample-logs;https://issues.apache.org/jira/secure/attachment/13055174/sample-logs",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 28 11:08:52 UTC 2023,,,,,,,,,,"0|z1foa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 12:53;gyfora;Could you please explain why it is a problem to downscale the sink? Maybe based on the metrics it seems that it doesn't need that much parallelism;;;","07/Feb/23 04:34;gaurav726;yes you are right, but in our cases, load came suddenly, for which we need sink to be of define parallelism, although I have found out property `kubernetes.operator.job.autoscaler.vertex.min-parallelism` which can be set, then autoscaler will not trigger downscale below this threshold, but it seems to me as we are scaling vertices and we already have list of vertices, why not user can control which vertices he didn't want autoscaler to perform any action, your thoughts ?;;;","07/Feb/23 12:27;gyfora;I agree it would make sense to be able to exclude some operators from scaling. However jobvertexid is a bit clumsy to configure as it can change after a job upgrade in some cases.[~mxm] what do you think?;;;","07/Feb/23 16:10;mxm;I think we could add an ignore list based on the vertex id. That would be an easy fix but I agree that it is not optimal.

A better solution would be to allow annotations on each operator. An operator annotated with ""NO_AUTOSCALING"" would not be chained to other operators and subsequently be ignored by the autoscaler.;;;","07/Feb/23 16:11;mxm;Please note that the autoscaler operates on the task ids, not the operator ids. A task consists of multiple chained operators. The chaining behavior can be configured.;;;","07/Feb/23 17:17;gaurav726;I think it is vertex id, as i can see in logs, [~mxm]  correct me if i'm wrong, https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/autoscaler/ScalingExecutor.java#L140;;;","09/Feb/23 13:42;mxm;Task id / Vertex id are used interchangeably. Vertex ID generally refers to the Flink JobGraph / ExecutionGraph, whereas TaskId is used on task managers which executes one or more vertices. There is a 1:1 mapping between a given vertex and task and their ids are identical.;;;","09/Feb/23 15:31;gaurav726;[~mxm] [~gyfora] then do i have head's up to make changes for it ?;;;","09/Feb/23 17:24;mxm;No objections from my side. It should be a fairly straight-forward change.;;;","28/Feb/23 11:08;gyfora;merged to main f7e353c26254c3ccc4a8c2a291da7697f1fbec5d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in the document of User-defined Sources & Sinks,FLINK-30919,13523200,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,06/Feb/23 09:29,09/Jun/23 11:28,04/Jun/24 20:41,09/Jun/23 11:28,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,Just find some typo in the aibility part of the user-defined Sources & Sinks document ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 08:16:28 UTC 2023,,,,,,,,,,"0|z1fo9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 08:16;luoyuxia;master: 191ec6ca3943d7119f14837efe112e074d815c47

1.17: eb36d472ce2d4ec67ac1a31ea0e5933198bbb178

1.16: 378746c511a48df3d28694a303d27f7618d5a2d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor CompositePkAndMultiPartitionedTableITCase to get rid of managed table,FLINK-30918,13523193,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,06/Feb/23 09:08,09/Feb/23 11:23,04/Jun/24 20:41,09/Feb/23 05:36,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 05:36:36 UTC 2023,,,,,,,,,,"0|z1fo88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 05:36;lzljs3620320;master: ac01ddc333372dc92bf9bc3080b7dd4d90d26d01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The user configured max parallelism does not take effect when using adaptive batch scheduler,FLINK-30917,13523189,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,06/Feb/23 08:43,15/Feb/23 06:19,04/Jun/24 20:41,15/Feb/23 06:19,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Currently, the adaptive batch scheduler only respects the global maximum parallelism(which is configured by option {{parallelism.default}} or {{execution.batch.adaptive.auto-parallelism.max-parallelism}}, see FLINK-30686 for details) when deciding parallelism for job vertices, the maximum parallelism of vertices configured by the user through {{setMaxParallelism}} will not be respected.

In this ticket, we will change the behavior so that the user-configured max parallelism also be respected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 06:18:01 UTC 2023,,,,,,,,,,"0|z1fo7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 06:05;wanglijie;master: b3998324b685afc779954f7e54cc0d8f281267ec
release-1.17 : e1c6352d18a19403e3eb80736c58b842de21bc88;;;","15/Feb/23 06:18;wanglijie;release-1.16: d83044908a019d2c7f73a6c823276968b9d2ce23;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBStateUploaderTest.testUploadedSstCanBeCleanedUp failed with assertion,FLINK-30916,13523186,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,fanrui,mapohl,mapohl,06/Feb/23 08:19,07/Feb/23 03:26,04/Jun/24 20:41,07/Feb/23 03:26,1.17.0,,,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,test-stability,,,"{{RocksDBStateUploaderTest.testUploadedSstCanBeCleanedUp}} failed due to an assertion:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45730&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=10328

{code}
Feb 06 02:54:01 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.427 s <<< FAILURE! - in org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest
Feb 06 02:54:01 [ERROR] org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.testUploadedSstCanBeCleanedUp  Time elapsed: 0.115 s  <<< FAILURE!
Feb 06 02:54:01 java.lang.AssertionError: 
Feb 06 02:54:01 
Feb 06 02:54:01 Expecting empty but was: [""379065d4-dd29-4455-b30f-4dbc53336ea2""]
Feb 06 02:54:01 	at org.apache.flink.contrib.streaming.state.RocksDBStateUploaderTest.testUploadedSstCanBeCleanedUp(RocksDBStateUploaderTest.java:166)
Feb 06 02:54:01 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30461,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 03:25:16 UTC 2023,,,,,,,,,,"0|z1fo6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 08:25;mapohl;I'm linking FLINK-30461 because this one introduced the test. I'm leaving the priority at Critical because FLINK-30461 seems to fix a bug that's also present in already released versions.

[~fanrui] can you have a look at this?;;;","06/Feb/23 08:31;fanrui;Hi [~mapohl] , thanks for report this bug.

I‘m working on this, could you assign this ticket to me?;;;","07/Feb/23 03:25;ym;Resolved in FLINK-30461;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`flink-fs-hadoop-s3` connector is unable to find IRSA credentials,FLINK-30915,13523184,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,06/Feb/23 08:15,06/Feb/23 10:29,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,," 

""flink-fs-s3-hadoop"" connectors were not able to find the credentials just fine when configured with the {{WebIdentityTokenCredentialsProvider. }}

 

when I try to use Flink's s3 connector I get access denied, so then I made sure to set the correct identity provider in my flink-conf, which was set to the following

 

 
{code:java}
hadoop.fs.s3a.aws.credentials.provider: ""com.amazonaws.auth.WebIdentityTokenCredentialsProvider""{code}



{code:java}
2023-02-02 21:02:06,214 INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator        [] - Remoting shut down.
2023-02-02 21:02:06,293 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Could not start cluster entrypoint KubernetesApplicationClusterEntrypoint.
org.apache.flink.runtime.entrypoint.ClusterEntrypointException: Failed to initialize the cluster entrypoint KubernetesApplicationClusterEntrypoint.
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:255) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729) [flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.kubernetes.entrypoint.KubernetesApplicationClusterEntrypoint.main(KubernetesApplicationClusterEntrypoint.java:86) [flink-dist-1.16.0.jar:1.16.0]
Caused by: org.apache.flink.util.FlinkException: Could not create the ha services from the instantiated HighAvailabilityServicesFactory org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:299) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:285) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:145) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:439) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:382) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229) ~[flink-dist-1.16.0.jar:1.16.0]
    ... 2 more
Caused by: java.nio.file.AccessDeniedException: s3://<bucket>/flink-ha/basic-example/blob: getFileStatus on s3://<bucket>/flink-ha/basic-example/blob: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: CXJ8Y79Z8SYTBEFM; S3 Extended Request ID: 1234567/1234567; Proxy: null), S3 Extended Request ID:123454321/123232:AccessDenied
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3858) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem$MkdirOperationCallbacksImpl.probePathStatus(S3AFileSystem.java:3455) ~[?:?]
    at org.apache.hadoop.fs.s3a.impl.MkdirOperation.probePathStatusOrNull(MkdirOperation.java:135) ~[?:?]
    at org.apache.hadoop.fs.s3a.impl.MkdirOperation.getPathStatusExpectingDir(MkdirOperation.java:150) ~[?:?]
    at org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:80) ~[?:?]
    at org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:45) ~[?:?]
    at org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76) ~[?:?]
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[?:?]
    at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[?:?]
    at org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3428) ~[?:?]
    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2449) ~[?:?]
    at org.apache.flink.fs.s3hadoop.common.HadoopFileSystem.mkdirs(HadoopFileSystem.java:183) ~[?:?]
    at org.apache.flink.core.fs.PluginFileSystemFactory$ClassLoaderFixingFileSystem.mkdirs(PluginFileSystemFactory.java:162) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.blob.FileSystemBlobStore.<init>(FileSystemBlobStore.java:64) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:108) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:86) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory.createHAServices(KubernetesHaServicesFactory.java:41) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:296) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createCustomHAServices(HighAvailabilityServicesUtils.java:285) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:145) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:439) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:382) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) ~[flink-dist-1.16.0.jar:1.16.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229) ~[flink-dist-1.16.0.jar:1.16.0]
    ... 2 more{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 10:29:30 UTC 2023,,,,,,,,,,"0|z1fo68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 10:29;gaborgsomogyi;Please have a look how it needs to be configured: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/s3/#hadooppresto-s3-file-systems-plugins;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding failed,FLINK-30914,13523183,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,mapohl,mapohl,mapohl,06/Feb/23 08:14,06/Feb/23 15:02,04/Jun/24 20:41,06/Feb/23 15:02,1.15.3,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{{ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding}} failed:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45731&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=7436

{code}
Feb 06 02:32:27 [ERROR] org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding  Time elapsed: 29.997 s  <<< FAILURE!
Feb 06 02:32:27 java.lang.AssertionError: 
Feb 06 02:32:27 
Feb 06 02:32:27 Expected: Expected failure cause is <org.apache.flink.util.FlinkRuntimeException: testUnExpectedErrorForwarding>
Feb 06 02:32:27      but: The throwable <org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss> does not contain the expected failure cause <org.apache.flink.util.FlinkRuntimeException: testUnExpectedErrorForwarding>
Feb 06 02:32:27 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Feb 06 02:32:27 	at org.junit.Assert.assertThat(Assert.java:964)
Feb 06 02:32:27 	at org.junit.Assert.assertThat(Assert.java:930)
Feb 06 02:32:27 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding(ZooKeeperLeaderElectionTest.java:724)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28523,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 15:02:04 UTC 2023,,,,,,,,,,"0|z1fo60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 12:14;mapohl;The fsync took longer (29858ms) than the configured session timeout (2000ms):
{code}
02:32:26,386 [        SyncThread:0] WARN  org.apache.zookeeper.server.persistence.FileTxnLog           [] - fsync-ing the write ahead log in SyncThread:0 took 29858ms which will adversely effect operation latency. File size is 67108880 bytes. See the ZooKeeper troubleshooting guide
{code};;;","06/Feb/23 12:16;mapohl;I'm gonna harden the test by accepting {{ConnectionLossException}} as a valid response that should be handled by the FatalErrorHandler. Forwarding this kind of exceptions should be considered as expected behavior.;;;","06/Feb/23 15:02;mapohl;[~chesnay] reminded me that we had already FLINK-28523 to fix this issue more generally by increasing the session timeout. FLINK-28523 wasn't backported. That's why we see this error still in the {{release-1.15}} branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlinkStreamUserDefinedFunctionTests.test_chaining_scalar_function failed due to missing response from the Java side,FLINK-30913,13523182,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,06/Feb/23 08:13,21/Aug/23 11:04,04/Jun/24 20:41,21/Aug/23 11:04,1.15.3,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45731&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=29792
{code}
Feb 06 04:45:58 =================================== FAILURES ===================================
Feb 06 04:45:58 _____ PyFlinkStreamUserDefinedFunctionTests.test_chaining_scalar_function ______
Feb 06 04:45:58 
Feb 06 04:45:58 self = <pyflink.table.tests.test_udf.PyFlinkStreamUserDefinedFunctionTests testMethod=test_chaining_scalar_function>
Feb 06 04:45:58 
Feb 06 04:45:58     def test_chaining_scalar_function(self):
Feb 06 04:45:58         add_one = udf(lambda i: i + 1, result_type=DataTypes.BIGINT())
Feb 06 04:45:58         subtract_one = udf(SubtractOne(), result_type=DataTypes.BIGINT())
Feb 06 04:45:58     
Feb 06 04:45:58         table_sink = source_sink_utils.TestAppendSink(
Feb 06 04:45:58             ['a', 'b', 'c'],
Feb 06 04:45:58             [DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.INT()])
Feb 06 04:45:58         self.t_env.register_table_sink(""Results"", table_sink)
Feb 06 04:45:58     
Feb 06 04:45:58         t = self.t_env.from_elements([(1, 2, 1), (2, 5, 2), (3, 1, 3)], ['a', 'b', 'c'])
Feb 06 04:45:58 >       t.select(add(add_one(t.a), subtract_one(t.b)), t.c, expr.lit(1)) \
Feb 06 04:45:58             .execute_insert(""Results"").wait()
Feb 06 04:45:58 
Feb 06 04:45:58 pyflink/table/tests/test_udf.py:89: 
Feb 06 04:45:58 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Feb 06 04:45:58 pyflink/table/table.py:1088: in execute_insert
Feb 06 04:45:58     return TableResult(self._j_table.executeInsert(table_path_or_descriptor, overwrite))
Feb 06 04:45:58 .tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py:1321: in __call__
Feb 06 04:45:58     return_value = get_return_value(
Feb 06 04:45:58 pyflink/util/exceptions.py:146: in deco
Feb 06 04:45:58     return f(*a, **kw)
Feb 06 04:45:58 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Feb 06 04:45:58 
Feb 06 04:45:58 answer = 'x'
Feb 06 04:45:58 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7fef18871160>
Feb 06 04:45:58 target_id = 'o21847', name = 'executeInsert'
Feb 06 04:45:58 
Feb 06 04:45:58     def get_return_value(answer, gateway_client, target_id=None, name=None):
Feb 06 04:45:58         """"""Converts an answer received from the Java gateway into a Python object.
Feb 06 04:45:58     
Feb 06 04:45:58         For example, string representation of integers are converted to Python
Feb 06 04:45:58         integer, string representation of objects are converted to JavaObject
Feb 06 04:45:58         instances, etc.
Feb 06 04:45:58     
Feb 06 04:45:58         :param answer: the string returned by the Java gateway
Feb 06 04:45:58         :param gateway_client: the gateway client used to communicate with the Java
Feb 06 04:45:58             Gateway. Only necessary if the answer is a reference (e.g., object,
Feb 06 04:45:58             list, map)
Feb 06 04:45:58         :param target_id: the name of the object from which the answer comes from
Feb 06 04:45:58             (e.g., *object1* in `object1.hello()`). Optional.
Feb 06 04:45:58         :param name: the name of the member from which the answer comes from
Feb 06 04:45:58             (e.g., *hello* in `object1.hello()`). Optional.
Feb 06 04:45:58         """"""
Feb 06 04:45:58         if is_error(answer)[0]:
Feb 06 04:45:58             if len(answer) > 1:
Feb 06 04:45:58                 type = answer[1]
Feb 06 04:45:58                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
Feb 06 04:45:58                 if answer[1] == REFERENCE_TYPE:
Feb 06 04:45:58                     raise Py4JJavaError(
Feb 06 04:45:58                         ""An error occurred while calling {0}{1}{2}.\n"".
Feb 06 04:45:58                         format(target_id, ""."", name), value)
Feb 06 04:45:58                 else:
Feb 06 04:45:58                     raise Py4JError(
Feb 06 04:45:58                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
Feb 06 04:45:58                         format(target_id, ""."", name, value))
Feb 06 04:45:58             else:
Feb 06 04:45:58 >               raise Py4JError(
Feb 06 04:45:58                     ""An error occurred while calling {0}{1}{2}"".
Feb 06 04:45:58                     format(target_id, ""."", name))
Feb 06 04:45:58 E               py4j.protocol.Py4JError: An error occurred while calling o21847.executeInsert
Feb 06 04:45:58 
Feb 06 04:45:58 .tox/py38-cython/lib/python3.8/site-packages/py4j/protocol.py:334: Py4JError
Feb 06 04:45:58 ------------------------------ Captured log call -------------------------------
Feb 06 04:45:58 ERROR    root:java_gateway.py:1055 Exception while sending command.
Feb 06 04:45:58 Traceback (most recent call last):
Feb 06 04:45:58   File ""/__w/1/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1224, in send_command
Feb 06 04:45:58     raise Py4JNetworkError(""Answer from Java side is empty"")
Feb 06 04:45:58 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30630,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 11:04:21 UTC 2023,,,,,,,,,,"0|z1fo5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 11:04;mapohl;I'm gonna close this one as a duplicate of FLINK-30630. The stacktraces look the same even though the failing tests are different.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CreateTableAsITCase seem to have timed out on Azure,FLINK-30912,13523177,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,mapohl,mapohl,06/Feb/23 08:04,14/Aug/23 09:01,04/Jun/24 20:41,14/Aug/23 09:01,1.16.1,,,,,,,,,,,,,,,,,,,,,,Table SQL / Ecosystem,,,,,0,stale-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45722&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678&l=1001

There is no additional log file because Azure failed to upload logs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 14 09:01:40 UTC 2023,,,,,,,,,,"0|z1fo4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 09:01;mapohl;Closing that issue because it didn't reappear and there is no means to investigate it due to the limited artifacts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkE2ECase.testStartFromSavepoint fails with TimeoutException because the topic doesn't become available in the meta file,FLINK-30911,13523174,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,mapohl,mapohl,06/Feb/23 08:00,16/Oct/23 06:53,04/Jun/24 20:41,16/Oct/23 06:53,1.16.1,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,test-stability,,,,"{{KafkaSinkE2ECasetestStartFromSavepoint}} with timeout exception after the topic didn't end up in the meta file after 60s.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45722&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34&l=15563

{code}
Feb 05 05:28:58 Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to send data to Kafka kafka-single-topic-9151082293470264886--1@-1 with FlinkKafkaInternalProducer{transactionalId='null', inTransaction=false, closed=false} 
Feb 05 05:28:58 	at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.throwException(KafkaWriter.java:436)
Feb 05 05:28:58 	at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.lambda$onCompletion$0(KafkaWriter.java:417)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:383)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:345)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
Feb 05 05:28:58 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
Feb 05 05:28:58 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
Feb 05 05:28:58 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
Feb 05 05:28:58 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
Feb 05 05:28:58 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
Feb 05 05:28:58 	at java.lang.Thread.run(Thread.java:750)
Feb 05 05:28:58 Caused by: org.apache.kafka.common.errors.TimeoutException: Topic kafka-single-topic-9151082293470264886 not present in metadata after 60000 ms.
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 07:52:17 UTC 2023,,,,,,,,,,"0|z1fo40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 08:01;mapohl;This sounds like it's caused by the general instability of Kafka tests again. Still, we need to start collecting all of this under a single umbrella ticket.;;;","20/Mar/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47349&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap fails with assertion,FLINK-30910,13523169,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,06/Feb/23 07:56,08/Feb/23 07:51,04/Jun/24 20:41,08/Feb/23 07:51,1.15.3,1.16.1,1.17.0,,,,1.15.4,1.16.2,1.17.0,1.18.0,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"A build failure in {{ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9831

{code}
Feb 05 01:13:44 [ERROR] Tests run: 30, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.174 s <<< FAILURE! - in org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest
Feb 05 01:13:44 [ERROR] org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap  Time elapsed: 2.026 s  <<< FAILURE!
Feb 05 01:13:44 org.opentest4j.AssertionFailedError: 
Feb 05 01:13:44 
Feb 05 01:13:44 Expecting value to be true but was false
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Feb 05 01:13:44 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Feb 05 01:13:44 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap(ApplicationDispatcherBootstrapTest.java:361)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 07:51:57 UTC 2023,,,,,,,,,,"0|z1fo2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 11:20;mapohl;[~chesnay] do you have capacity to look into this issue?;;;","06/Feb/23 12:42;mapohl;{quote}
Chesnay Schepler do you have capacity to look into this issue?
{quote}

Never mind, the other investigation went faster than expected. I'm gonna pick this one as well. Let me know if you already started on it.;;;","06/Feb/23 13:04;mapohl;It looks like the bootstrap code finished faster than the test executed the stop call. This results in the {{applicationExecutionFuture}} future completing and not being cancelled. This can be reproduced by adding a {{Thread.sleep(1000)}} before the [stop call|https://github.com/apache/flink/blob/6da5d36243329497b3b005b1900ba01f1a9d8935/flink-clients/src/test/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrapTest.java#L350];;;","08/Feb/23 07:51;mapohl;master: a6de5f23a30fcd580f4d005ade0fb2eba0e901dc
1.17: 5ded7b320eff4fe7587d9eeb1fe5fa87e217718b
1.16: cb83c268df09426060d1d45cc7fde9a999637691
1.15: eaf35c80342ab0df2fd70ef730e9299521b5a9d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running HA (hashmap, sync) end-to-end test",FLINK-30909,13523167,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,06/Feb/23 07:53,19/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / Coordination,,,,0,auto-deprioritized-critical,test-stability,,,"A build failure in {{Running HA (hashmap, sync) end-to-end test}} appeared:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45721&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=13213

{code}
Feb 05 02:52:21 FAILURE: A JM did not execute the job.
Feb 05 02:52:21 One or more tests FAILED.
Feb 05 02:52:21 Stopping job timeout watchdog (with pid=304915)
Feb 05 02:52:21 Killing JM watchdog @ 306748
Feb 05 02:52:21 Killing TM watchdog @ 307208
Feb 05 02:52:21 [FAIL] Test script contains errors.
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30732,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:04 UTC 2023,,,,,,,,,,"0|z1fo2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46810&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62;;;","03/Apr/23 06:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47795&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=13242;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail,FLINK-30908,13523165,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,mapohl,mapohl,06/Feb/23 07:28,03/May/23 15:03,04/Jun/24 20:41,20/Feb/23 02:12,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Deployment / YARN,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"There's a build failure in {{YARNSessionFIFOSecuredITCase.testDetachedMode}} which is caused by a fatal error in the ResourceManager:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45720&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=29869

{code}
Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1480) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1422) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-3.2.3.jar:?]
Feb 05 02:41:58 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Feb 05 02:41:58 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Feb 05 02:41:58 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Feb 05 02:41:58 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-yarn-client-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:311) [hadoop-yarn-client-3.2.3.jar:?]
Feb 05 02:41:58 Caused by: java.lang.InterruptedException
Feb 05 02:41:58 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Feb 05 02:41:58 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1180) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1475) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	... 17 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31169,FLINK-31974,FLINK-22312,,,FLINK-31609,,,,,,"06/Feb/23 10:41;mapohl;mvn-1.FLINK-30908.log;https://issues.apache.org/jira/secure/attachment/13055177/mvn-1.FLINK-30908.log",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 02:01:53 UTC 2023,,,,,,,,,,"0|z1fo20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:52;mapohl;Not sure, yet, whether that's related but there's a {{ApplicationAttemptNotFoundException}} which causes application {{application_1675564836997_0002}} to be killed:
{code}
02:41:17,442 [IPC Server handler 9 on default port 46716] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl [] - Stopping container with container Id: container_1675564836997_0002_01_000002
02:41:17,458 [IPC Server handler 2 on default port 45213] ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService [] - Application attempt appattempt_1675564836997_0002_000001 doesn't exist in ApplicationMasterService cache.
02:41:17,459 [IPC Server handler 2 on default port 45213] INFO  org.apache.hadoop.ipc.Server                                 [] - IPC Server handler 2 on default port 45213, call Call#8 Retry#0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate from 192.168.144.2:35386
org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1675564836997_0002_000001 doesn't exist in ApplicationMasterService cache.
        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:407) ~[hadoop-yarn-server-resourcemanager-3.2.3.jar:?]
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60) ~[hadoop-yarn-common-3.2.3.jar:?]
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99) ~[hadoop-yarn-api-3.2.3.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:549) ~[hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:518) ~[hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086) ~[hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029) [hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957) [hadoop-common-3.2.3.jar:?]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_292]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) [hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957) [hadoop-common-3.2.3.jar:?]
02:41:17,560 [Listener at 2c636c5cfb18/37932] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1675564836997_0002
{code}

Application container {{container_1675564836997_0002_01_000002}} is used for the TaskManager ({{container_1675564836997_0002_01_000001}} for the JobManager).;;;","06/Feb/23 09:58;mapohl;The above mentioned error happens closely (time-wise) to the fatal error in Flink's ResourceManager:
{code}
2023-02-05 02:41:17,459 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeat
java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
    at org.apache.hadoop.ipc.Client.call(Client.java:1480) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.Client.call(Client.java:1422) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:231) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.3.jar:?]
    at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]
    at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-3.2.3.jar:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.3.jar:?]
    at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]
    at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-yarn-client-3.2.3.jar:?]
    at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:311) [hadoop-yarn-client-3.2.3.jar:?]
Caused by: java.lang.InterruptedException
    at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
    at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
    at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1180) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.Client.call(Client.java:1475) ~[hadoop-common-3.2.3.jar:?]
    ... 17 more
{code} ;;;","06/Feb/23 10:31;mapohl;The WordCount job finishes successfully. This triggers the shutdown from the Flink side.
* Application finishes at {{2023-02-05 02:41:17,116}} where the JobMaster is removed from the ResourceManager
* YarnTestBase sends an application status at {{02:41:17,357}} and triggers the shutdown afterwards
* Kill request received on the Yarn side at {{02:41:17,359}}
* ClusterEntrypoint receives SIGTERM at {{02:41:17,381}};;;","06/Feb/23 10:32;mapohl;-There is an KRB error reported on INFO log level by the KdcHandler at {{02:41:13:755}} Just to be sure: [~bamrabi]: This issue is actual not a problem, is it? ...considering that we worked on Kerberos support in 1.17.-
{code:java}
02:41:13,755 [     pool-3-thread-1] INFO  org.apache.kerby.kerberos.kerb.server.request.KdcRequest     [] - The preauth data is empty.
02:41:13,755 [     pool-3-thread-1] INFO  org.apache.kerby.kerberos.kerb.server.KdcHandler             [] - KRB error occurred while processing request:Additional pre-authentication required
02:41:13,808 [     pool-3-thread-1] INFO  org.apache.kerby.kerberos.kerb.server.request.AsRequest      [] - AS_REQ ISSUE: authtime 1675564873807,hadoop/localhost@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
{code}
{*}Update{*}: The concern shared above shouldn't be the reason for the cause. The error appears during the graceful shutdown of the JobManager process/ResourceManager. It looks like a race condition between YARN and Flink. There's s still communication happening between the ResourceManager and the TaskManager while the TaskManager is shut down by YARN.;;;","06/Feb/23 10:50;mapohl;[~xtsong], [~huwh] may you have a look at it? Could that be an issue related to your work on FLINK-20988 subtasks?;;;","06/Feb/23 11:13;mapohl;I've found a comment from Xintong in FLINK-22312 on this error:
{quote}
The InterruptedIOException is expected. It can happen if AMRMClient happens to be waiting for a heartbeat response when the cluster is shutdown.
I'll add it to the whitelist for the log prohibited string checking.
{quote}

We also whitelist this error in [YarnTestBase:172-174|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L172-L174]. Do we need to find a way to avoid triggering the fatal error handler for this issue? ;;;","06/Feb/23 11:15;mapohl;Can we assume based on FLINK-22312 that it's a ongoing issue that is not necessarily only an issue in 1.17 meaning that this issue isn't a blocker for the 1.17 release?;;;","06/Feb/23 11:44;xtsong;This is indeed related to FLINK_20988. The {{InterruptedIOException}} is expected and is included in the whitelist for prohibited string checking. However, this should not be handled as a fatal error.

I think this is a blocker. Depending on which process finishes first (the gracefully shutdown and the fatal error handling), it may cause the process to terminate with an error exit code, which lead to restarting of the application being shutdown.

I'll provide a fix for it asap.;;;","07/Feb/23 03:48;xtsong;After looking more into the logs and Hadoop codes, we believe FLINK-20988 is not the cause of this failure.

The test failure is caused by:
1. {{AMRMClientAsync}} sends an {{InterruptedIOException}} to the callback handler ({{YarnContainerEventHandler}}) after being stopped.
2. All errors sent to {{YarnContainerEventHandler}} are treated as fatal error in Flink.

This is not a newly introduced issue. 1) exists in Hadoop 2.9+ versions (https://issues.apache.org/jira/browse/YARN-5999), and 2) is the behavior since yarn deployment is supported. FLINK-20988 did introduce another chance for exceptions during shutdown to be handled as fatal error, but that is not the cause of this test failure. Given that this issue already exist in previous releases, I'm downgrading this ticket to Critical priority.

The proper fix might be to ignore the exceptions in {{YarnContainerEventHandler}} after being terminated. I'll update the PR and fix this.;;;","07/Feb/23 03:59;wangyang0918;+1 for Xintong's analysis and proposal.

YARN-5999 introduced a side effect that {{CallbackHandler#onError}} will have a chance to be executed when stopping the AMRMAyncClient.;;;","20/Feb/23 02:12;xtsong;- master (1.18): 6b47f45ffd22e0ff332a528dccfbd8b664e28702
- release-1.17: 6b47f45ffd22e0ff332a528dccfbd8b664e28702;;;","20/Feb/23 11:22;mapohl;Thanks for providing a fix for that issue, [~xtsong]. Just to fix things (the matching commit hash caught my attention) - here's an updated of the commits per branch:
* master (1.18): 
** 296ee78efdd5c19308a2967f5b7a2f994324b0f4
** 9b92a89c1853e140be3889df47c12bc36f42a96b
* release-1.17: 
** aecefe756df39b44b18574483a02b6753b8092fc
** 6b47f45ffd22e0ff332a528dccfbd8b664e28702 ;;;","21/Feb/23 02:01;xtsong;Nice catch, thanks [~mapohl].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveSinkCompactionITCase.testSingleParallelism fails with test timeout,FLINK-30907,13523163,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,06/Feb/23 07:22,21/Aug/23 10:47,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,auto-deprioritized-critical,test-stability,,,"We experience the a test failure in {{HiveSinkCompactionITCase.testSingleParallelism}} due to a {{TestTimeoutException}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45708&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=26356

{code}
Feb 04 01:56:55 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 187.918 s <<< FAILURE! - in org.apache.flink.connectors.hive.HiveSinkCompactionITCase
Feb 04 01:56:55 [ERROR] HiveSinkCompactionITCase.testSingleParallelism  Time elapsed: 90.009 s  <<< ERROR!
Feb 04 01:56:55 org.junit.runners.model.TestTimedOutException: test timed out after 90 seconds
Feb 04 01:56:55 	at java.io.FileDescriptor.sync(Native Method)
Feb 04 01:56:55 	at org.apache.derby.impl.io.DirRandomAccessFile.sync(Unknown Source)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 10:47:02 UTC 2023,,,,,,,,,,"0|z1fo1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 10:47;mapohl;The 90s timeout is coming from [CompactionITCaseBase:50|https://github.com/apache/flink/blob/7fe549e56a4c4c2a1d05a0013282d2716e34b0e9/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/CompactionITCaseBase.java#L50]. This timeout should be removed to allow proper stacktrace generation as suggested in the [Flink Coding Guidelines|https://flink.apache.org/how-to-contribute/code-style-and-quality-common/#avoid-timeouts-in-junit-tests].

Anyway, the actual issue is that the File sync operation didn't return. I suspect it to be an infrastructure issue. But I'm gonna keep this one open to observe it a bit longer (to see whether this issue pops up again). We could remove the timeout anyway.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TwoInputStreamTask passes wrong configuration object when creating input processor,FLINK-30906,13523161,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,06/Feb/23 07:21,13/Sep/23 10:26,04/Jun/24 20:41,13/Sep/23 10:26,1.16.1,1.17.0,,,,,1.19.0,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"It seems _StreamTwoInputProcessorFactory.create_ is passed with wrong configuration object: the taskManagerConfiguration should be __ 
_getEnvironment().getTaskManagerInfo().getConfiguration()._ 
 
And in the following logic, it seems to indeed try to load taskmanager options from this configuration object, like state-backend and taskmanager.memory.managed.consumer-weights 
 

[1]https://github.com/apache/flink/blob/111342f37bdc0d582d3f7af458d9869f0548299f/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java#L98",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 13 10:26:24 UTC 2023,,,,,,,,,,"0|z1fo14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 10:26;Weijie Guo;master(1.19) via 70f4c40f15f38ed404d8e031a08d534326535ced.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"doc generation fails with ""concurrent map read and map write""",FLINK-30905,13523159,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,mapohl,mapohl,06/Feb/23 07:13,06/Feb/23 11:37,04/Jun/24 20:41,06/Feb/23 11:34,1.15.3,1.16.1,1.17.0,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,Documentation,Test Infrastructure,,,,0,pull-request-available,test-stability,,,"We experience a build failure in {{master}} (but since it looks like a Hugo issue, I added already released version to the affected versions as well) with a {{concurrent map read and map write}} within hugo:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98

{code}
Start building sites … 
fatal error: concurrent map read and map write

goroutine 233 [running]:
runtime.throw(0x23054e4, 0x21)
	/usr/local/go/src/runtime/panic.go:1116 +0x72 fp=0xc0016ea860 sp=0xc0016ea830 pc=0x4f5ff2
runtime.mapaccess1_faststr(0x1f71280, 0xc000764a20, 0xc000aa60e1, 0x18, 0xcd)
	/usr/local/go/src/runtime/map_faststr.go:21 +0x465 fp=0xc0016ea8d0 sp=0xc0016ea860 pc=0x4d29c5
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 11:34:24 UTC 2023,,,,,,,,,,"0|z1fo0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:32;chesnay;https://github.com/gohugoio/hugo/issues/8210 mentions some data race being fixed in 0.81, but the docs run on 0.80. Let's just bump it to some later version and see what happens.;;;","06/Feb/23 11:34;chesnay;master: 5d44d591b6f7bc1488b6b7b6d61a642350b60af1
1.16: 3f615fb998e51ebfd4f83cf1ca12b65ea4e6d02d
1.15: 582ea29f1713fa799af1cf9660dd146ff0461e83;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the documentation and configuration description of slow task detector,FLINK-30904,13523156,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Done,xiasun,zhuzh,zhuzh,06/Feb/23 07:06,10/Feb/23 09:41,04/Jun/24 20:41,10/Feb/23 09:41,,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,Runtime / Configuration,,,,0,pull-request-available,,,,FLINK-30707 improved the slow task detecting. The previous documentation and configuration descriptions of SlowTaskDetector need to be updated for it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 09:41:51 UTC 2023,,,,,,,,,,"0|z1fo00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 07:21;xiasun;I'd like to continue this work, please assign this ticket to me, thanks!;;;","06/Feb/23 07:33;zhuzh;I have assigned you the ticket :) [~xiasun];;;","10/Feb/23 09:41;zhuzh;master:
ffecb68b11e2cd914f92699112b5adb62a48724e

release-1.17:
04e3542ccb5138afc94cf6bca12ef1152deaf441;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The max parallelism used in adaptive batch scheduler doesn't fallbacks to default parallelism,FLINK-30903,13523148,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,06/Feb/23 06:06,07/Feb/23 08:09,04/Jun/24 20:41,07/Feb/23 08:09,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In FLINK-30684 we mark the vertices which use the default parallelism, and in AdaptiveBatchScheduler we allow users to use parallelism.default as the max parallelism if they don't configure the configuration item ""execution.batch.adaptive.auto-parallelism.max-parallelism"". This issue will fix the fallback logic.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 08:09:17 UTC 2023,,,,,,,,,,"0|z1fny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 08:09;zhuzh;Fixed via 4bc6d6dac29b1d7d89850a512b8283858824ae21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition reuse does not take effect on edges of hybrid selective type,FLINK-30902,13523143,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Feb/23 04:19,09/Feb/23 03:49,04/Jun/24 20:41,09/Feb/23 03:49,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Partition reuse only take effect for re-consumable edge, but hybrid selective result partition is not re-consumable. This optimization is very important to reduce the cost of the shuffle write phase. In the previous implementation, we will only force the broadcast edge to be of hybrid full(re-consumable) in the 'ResultPartitionTypeFactory'. As a result, for ALL_ EXCHANGE_HYBRID_SELECTIVE job, partition reuse cannot take effect for non-broadcast edges.
In fact, we expected to replace all the edges that can be reused with hybrid full result partition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 03:49:03 UTC 2023,,,,,,,,,,"0|z1fnx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 03:49;xtsong;- master (1.18): 4fb5f5e48aeeedba724dd1b44f2b819b40011944
- release-1.17: e35c97e45ba2bbeeb36c0947ad09533e362dc0c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The jobVertex's parallelismConfigured is incorrect when chaining with source operators,FLINK-30901,13523123,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,06/Feb/23 03:21,07/Feb/23 11:17,04/Jun/24 20:41,07/Feb/23 11:17,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When creating OperatorChainInfo in StreamingJobGenerator, the chained source are not included in OperatorChainInfo#chainedNodes, because they are not added to OperatorChainInfo via #addNodeToChain().

This will affect jobVertex which has a MultiInput operator chained with sources. The vertex's parallelismConfigured will be false even if the chained sources have a parallelism configured. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30685,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 11:17:32 UTC 2023,,,,,,,,,,"0|z1fnso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:17;zhuzh;master:
10ced269a8ec3970d2f567109e8983eb7202d45c

release-1.17:
5e198e57df35a9bc0b979f9e79e22872e753211c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce utils for table store,FLINK-30900,13523119,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,06/Feb/23 02:58,12/Feb/23 07:54,04/Jun/24 20:41,12/Feb/23 07:54,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Introduce utils from flink-core for table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 12 07:54:04 UTC 2023,,,,,,,,,,"0|z1fnrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/23 07:54;lzljs3620320;master: 8a9005db51dd07e0457e52640006701f467c73f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystemTableSource with CSV format incorrectly selects fields if filtering for partition,FLINK-30899,13523077,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mateczagany,mateczagany,mateczagany,04/Feb/23 16:04,21/Aug/23 22:35,04/Jun/24 20:41,,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,pull-request-available,stale-assigned,,,"In my testing it only affected csv and testcsv formats.

 

I think it's caused by `FileSystemTableSource` calling `DeserializationFormatFactory#createRuntimeDecoder` with wrong `physicalDataType`. The files won't contain the partitioned field values, but in case of a projection pushdown (which can happen during planning phase if we filter the partition field by a constant value) the final `physicalDataType` passed to the deserializer by `FileSystemTableSource` will contain the partitioned fields as well. As described in `DecodingFormat`, every field in the `physicalDataType` parameter will have to be present in the serialized record.

 

Example:
{code:java}
CREATE TABLE test_table (
  f0 INT,
  f1 INT,
  f2 INT,
  f3 INT
) PARTITIONED BY (f0,f1) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///path/to/whatever',
  'format' = 'csv'
)

SELECT * FROM test_table WHERE f0 = 1;
-- !!!! should be 1,4,7,10 !!!! 
+-------------+-------------+-------------+-------------+
|          f0 |          f1 |          f2 |          f3 |
+-------------+-------------+-------------+-------------+
|           1 |           4 |          10 |           0 |
+-------------+-------------+-------------+-------------+

SELECT * FROM test_table;
+-------------+-------------+-------------+-------------+
|          f0 |          f1 |          f2 |          f3 |
+-------------+-------------+-------------+-------------+
|           2 |           5 |           8 |          11 |
|           1 |           4 |           7 |          10 |
|           3 |           6 |           9 |          12 |
+-------------+-------------+-------------+-------------+

SELECT * FROM test_table WHERE f0>0;
+-------------+-------------+-------------+-------------+
|          f0 |          f1 |          f2 |          f3 |
+-------------+-------------+-------------+-------------+
|           1 |           4 |           7 |          10 |
|           3 |           6 |           9 |          12 |
|           2 |           5 |           8 |          11 |
+-------------+-------------+-------------+-------------+

SELECT * FROM test_table WHERE f0 = 1 AND f1 = 4;
...
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 4
    at org.apache.flink.types.parser.IntParser.parseField(IntParser.java:49)
    at org.apache.flink.types.parser.IntParser.parseField(IntParser.java:27)
    at org.apache.flink.types.parser.FieldParser.resetErrorStateAndParse(FieldParser.java:101)
    at org.apache.flink.formats.testcsv.TestCsvDeserializationSchema.deserialize(TestCsvDeserializationSchema.java:92)
    at org.apache.flink.formats.testcsv.TestCsvDeserializationSchema.deserialize(TestCsvDeserializationSchema.java:42)
    at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82)
... {code}
At [https://github.com/apache/flink/blob/b1e70aebd3e248d68cf41a43db385ec9c9b6235a/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/FileSystemTableSource.java#L147] the `physicalRowDataType` will contain the partition fields as well, but `partitionKeysToExtract` will not contain it since `producedDataType` has been modified in the `applyProjection` method, so it will result in an empty projection. Then on line 154 we construct the final `physicalDataType`, but since `partitionKeysProjections` is empty, it will result with the same value as `physicalDataType` which contains the partition fields too.

By changing
{code:java}
 final Projection partitionKeysProjections = Projection.fromFieldNames(physicalDataType, partitionKeysToExtract);{code}
to
{code:java}
 final Projection partitionKeysProjections = Projection.fromFieldNames(physicalDataType, partitionKeys);{code}
the issue can be solved. I have verified this solution with 1 and 2 partition keys, with and without metadata columns, with and without virtual columns. But I still need to test this change with other formats.

 

If this solution seems correct and a committer could assign me to the JIRA I can start working on it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 22:35:14 UTC 2023,,,,,,,,,,"0|z1fnig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 07:08;luoyuxia;[~mateczagany] Thanks for reporting. I think it's the right solution. Please go head and open a pr, we will happy to help review. 

I don't have right to assign ticket, may [~martijnvisser] help assign ticket.

 ;;;","09/Feb/23 10:02;mateczagany;Thank you [~luoyuxia] 

I have created a PR, can you please take a look at it when you have the time?;;;","21/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not include and build examples in operator image,FLINK-30898,13522950,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,03/Feb/23 12:53,09/Feb/23 07:04,04/Jun/24 20:41,09/Feb/23 07:04,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The docker build has slowed down substantially over time. We include many things in the image that are not necessary.

We should not include examples at all.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 07:04:14 UTC 2023,,,,,,,,,,"0|z1fmq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 07:04;gyfora;merged to main a89081fc7ac2abb492a01d71c211ffe4b2ed51ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid timeouts in JUnit tests,FLINK-30897,13522946,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,lincoln.86xy,lincoln.86xy,lincoln.86xy,03/Feb/23 12:29,13/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,stale-assigned,,,"As our [testing guideline|https://flink.apache.org/contributing/code-style-and-quality-common.html#avoid-timeouts-in-junit-tests] says we should 'Avoid timeouts in JUnit tests' but rather depend on the global timeout in Azure. There're 10 itcases throughout the project that use the 'Timeout Rule' and 22 tests use the 'Deadline' to set local timeouts. We need to check if we can change this dependency one by one

List of related test classes:

'Timeout Rule':
{code}

flink-end-to-end-tests-common-kafka  (1 usage found)
            org.apache.flink.tests.util.kafka  (1 usage found)
                SQLClientSchemaRegistryITCase.java  (1 usage found)
                    78 @ClassRule public static final Timeout TIMEOUT = new Timeout(10, TimeUnit.MINUTES);
        flink-glue-schema-registry-avro-test_2.12  (1 usage found)
            org.apache.flink.glue.schema.registry.test  (1 usage found)
                GlueSchemaRegistryAvroKinesisITCase.java  (1 usage found)
                    74 @ClassRule public static final Timeout TIMEOUT = new Timeout(10, TimeUnit.MINUTES);
        flink-glue-schema-registry-json-test  (1 usage found)
            org.apache.flink.glue.schema.registry.test.json  (1 usage found)
                GlueSchemaRegistryJsonKinesisITCase.java  (1 usage found)
                    68 @ClassRule public static final Timeout TIMEOUT = new Timeout(10, TimeUnit.MINUTES);
        flink-runtime  (1 usage found)
            org.apache.flink.runtime.io.disk  (1 usage found)
                BatchShuffleReadBufferPoolTest.java  (1 usage found)
                    41 @Rule public Timeout timeout = new Timeout(60, TimeUnit.SECONDS);
        flink-streaming-java  (1 usage found)
            org.apache.flink.streaming.api.operators.async  (1 usage found)
                AsyncWaitOperatorTest.java  (1 usage found)
                    117 @Rule public Timeout timeoutRule = new Timeout(100, TimeUnit.SECONDS);
        flink-tests  (5 usages found)
            org.apache.flink.runtime.operators.lifecycle  (3 usages found)
                BoundedSourceITCase.java  (1 usage found)
                    75 @Rule public Timeout timeoutRule = new Timeout(10, TimeUnit.MINUTES);
                PartiallyFinishedSourcesITCase.java  (1 usage found)
                    79 @Rule public Timeout timeoutRule = new Timeout(10, TimeUnit.MINUTES);
                StopWithSavepointITCase.java  (1 usage found)
                    103 @Rule public Timeout timeoutRule = new Timeout(10, TimeUnit.MINUTES);
            org.apache.flink.test.runtime  (2 usages found)
                JoinDeadlockITCase.java  (1 usage found)
                    39 @Rule public Timeout globalTimeout = new Timeout(120 * 1000); // Set timeout for deadlocks
                SelfJoinDeadlockITCase.java  (1 usage found)
                    46 @Rule public Timeout globalTimeout = new Timeout(120 * 1000); // Set timeout for deadlocks

{code}

'Deadline':
{code}
flink-table/flink-sql-gateway/src/test/java/org/apache/flink/table/gateway/service/session/SessionManagerImplTest.java:2
flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTaskTest.java:2
flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/io/checkpointing/CheckpointedInputGateTest.java:2
flink-metrics/flink-metrics-jmx/src/test/java/org/apache/flink/runtime/jobmanager/JMXJobManagerMetricTest.java:2
flink-runtime-web/src/test/java/org/apache/flink/runtime/webmonitor/WebFrontendITCase.java:4
flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test/java/org/apache/flink/tests/util/kafka/SQLClientSchemaRegistryITCase.java:2
flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test/java/org/apache/flink/tests/util/kafka/SQLClientKafkaITCase.java:2
flink-end-to-end-tests/flink-end-to-end-tests-hbase/src/test/java/org/apache/flink/tests/util/hbase/SQLClientHBaseITCase.java:2
flink-end-to-end-tests/flink-metrics-availability-test/src/test/java/org/apache/flink/metrics/tests/MetricsAvailabilityITCase.java:6
flink-tests/src/test/java/org/apache/flink/test/recovery/JobManagerHAProcessFailureRecoveryITCase.java:3
flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java:2
flink-tests/src/test/java/org/apache/flink/test/accumulators/AccumulatorLiveITCase.java:3
flink-tests/src/test/java/org/apache/flink/test/checkpointing/UnalignedCheckpointStressITCase.java:2
flink-tests/src/test/java/org/apache/flink/test/checkpointing/SavepointITCase.java:3
flink-tests/src/test/java/org/apache/flink/test/checkpointing/RescalingITCase.java:5
flink-dstl/flink-dstl-dfs/src/test/java/org/apache/flink/changelog/fs/BatchingStateChangeUploadSchedulerTest.java:3
flink-dstl/flink-dstl-dfs/src/test/java/org/apache/flink/changelog/fs/RetryingExecutorTest.java:2
flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/BlobsCleanupITCase.java:2
flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java:2
flink-runtime/src/test/java/org/apache/flink/runtime/io/disk/FileChannelManagerImplTest.java:2
flink-runtime/src/test/java/org/apache/flink/runtime/leaderelection/ZooKeeperLeaderElectionTest.java:2
flink-runtime/src/test/java/org/apache/flink/runtime/highavailability/AbstractHAJobRunITCase.java:2
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 10:35:05 UTC 2023,,,,,,,,,,"0|z1fmpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce usage of CatalogViewImpl in planner,FLINK-30896,13522943,13359276,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,03/Feb/23 11:35,16/Mar/23 23:29,04/Jun/24 20:41,16/Mar/23 23:29,,,,,,,1.18.0,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Most of the work was done under https://issues.apache.org/jira/browse/FLINK-21801

 

However there are still some usages of {{CatalogViewImpl}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 23:29:45 UTC 2023,,,,,,,,,,"0|z1fmoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 23:29;Sergey Nuyanzin;Merged as [81f77a9d0e7f49a1786eb21e0adb1290306b89c5|https://github.com/apache/flink/commit/81f77a9d0e7f49a1786eb21e0adb1290306b89c5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlotSharingSlotAllocator may waste slots,FLINK-30895,13522930,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,03/Feb/23 10:30,20/Feb/23 11:20,04/Jun/24 20:41,16/Feb/23 13:51,1.16.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The allocated evenly distributes slots across slot sharing groups independent of how many slots the vertices in that group actually need.

This can cause slots to be unused.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30931,,,,,,,,,,,,,FLINK-31119,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 13:51:42 UTC 2023,,,,,,,,,,"0|z1fmls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 13:51;chesnay;master: 97e1dbc90cc3ea6e6e48e901813ab3bfbb693c6a
1.17: 9e1cf08e24677108758a227a42384a482885b371;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Serializer to serialize internal data structure,FLINK-30894,13522923,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,03/Feb/23 09:18,09/Feb/23 03:01,04/Jun/24 20:41,09/Feb/23 03:01,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,31/Mar/23 00:00,0,,,,,"Introduce Serializer to store itself. Unlike Flink's TypeSerializer, only a few methods are needed here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Feb 09 03:01:02 UTC 2023,,,,,,,,,,"0|z1fmkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 03:01;lzljs3620320;master: a17d9001de447629775fccfb7a6789d974e42c26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace not specifying a driver in Minikube by something else,FLINK-30893,13522914,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,victoryu930909,mapohl,mapohl,03/Feb/23 07:55,14/Nov/23 02:41,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,auto-deprioritized-major,pull-request-available,starter,,"Quoting [~wangyang0918] here from a [Github PR comment|https://github.com/apache/flink/pull/21837#pullrequestreview-1282199706]:
{quote}
We need to find a more elegant way to avoid none driver of minikube in the future. And then we could rely on the latest minikube version and find some potential incompatible problems with new K8s versions.
{quote}

Starting point for this issue might be {{setup_kubernetes_for_linux}} function in [flink-end-to-end-tests/test-scripts/common_kubernetes.sh:32ff|https://github.com/apache/flink/blob/b1c3c7579e18cf11c809bf09c05b0774e972f1fc/flink-end-to-end-tests/test-scripts/common_kubernetes.sh#L32]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,,,,FLINK-30881,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 08 09:57:12 UTC 2023,,,,,,,,,,"0|z1fmig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 07:57;mapohl;I'm going to label this one as {{starter}} because fixing the minikube setup doesn't require in-depth knowledge of Flink.;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","08/Oct/23 09:57;victoryu930909;  Hi [Matthias Pohl|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl], I just submit a PR of this issue.If you have time, could you please help review it? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean codes for memory and tests,FLINK-30892,13522911,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,fengwang,fengwang,03/Feb/23 07:13,03/Feb/23 09:58,04/Jun/24 20:41,03/Feb/23 09:58,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,31/Mar/23 00:00,0,pull-request-available,,,,Clean up some useless code and adjust the code call of legacy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Feb 03 09:58:33 UTC 2023,,,,,,,,,,"0|z1fmhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 09:58;lzljs3620320;master: fa4a69913987416d7fd6b3ebe21d0eb39945fff2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Preconditions for table store,FLINK-30891,13522866,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,03/Feb/23 04:54,03/Feb/23 06:14,04/Jun/24 20:41,03/Feb/23 06:14,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Introduce Preconditions for table store from flink-core,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 06:14:57 UTC 2023,,,,,,,,,,"0|z1fm7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 06:14;lzljs3620320;master: b19efb2dcf19a536f6815e93b05c2a34c5a13aac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backup codespeed‘s data regularly,FLINK-30890,13522865,13493900,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,Yanfei Lei,Yanfei Lei,03/Feb/23 04:42,20/Oct/23 08:25,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,,,,,"The Codespeed of flink benchmark is a website hosted on jenkins master node. It uses the built-in SQLite to store running data.

In order to prevent data loss caused by downtime, the data should be regularly backed up to reliable remote storage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33052,,,,,,"28/Mar/23 12:59;Yanfei Lei;image-2023-03-28-20-59-08-040.png;https://issues.apache.org/jira/secure/attachment/13056863/image-2023-03-28-20-59-08-040.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 20 08:25:36 UTC 2023,,,,,,,,,,"0|z1fm7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 13:00;Yanfei Lei;Hi [~pnowojski] , I create a [pipeline|http://codespeed.dak8s.net:8080/job/codespeed-backup/] to backup codespeed's data to slack, due to the limitations of the slack api, the slack plugin requires the botUser mode to use the[file upload|[https://github.com/jenkinsci/slack-plugin#file-upload]] feature, I temporarily backed up the data to my private slack workspace. This pipeline will work like the figure below, maybe we can create a botUser in the workspace of Apache Flink to upload files, what do you think?

!image-2023-03-28-20-59-08-040.png|width=649,height=184!;;;","28/Mar/23 14:15;pnowojski;That's neat!. As an alternative or complementary solution, have you considered backuping whole machine in the EC2? That would also include codespeed/jenkins installation and configuration.;;;","29/Mar/23 12:51;Yanfei Lei;> have you considered backuping whole machine in the EC2? 

I haven't purchased AWS EC2. I'm not sure if Ververica has such a public machine available. How about backing it up to a worker node(hetzner node)?;;;","30/Mar/23 13:54;pnowojski;Codespeed WebUI, the database and Jenkins are hosted on a single AWS EC2 machine from Ververica account. There might be an easy way to backup everything.

The worker node doesn't have anything of value. The only thing that's configured there is the ssh account so that the jenkins can log in and start a job. ;;;","20/Oct/23 08:25;zakelly;The master node of new machines hosted on Aliyun also needs a backup procedure. I would like to work on this.

The backup will consist two parts:

  1. A snapshot of the cloud disk will be taken every day. 

  2. The database files will be uploaded to a object storage (Aliyun OSS) daily.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the key of configuration SPECULATIVE_ENABLED.,FLINK-30889,13522861,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,03/Feb/23 03:41,03/Feb/23 09:33,04/Jun/24 20:41,03/Feb/23 08:54,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Configuration,,,,,0,pull-request-available,,,,"In FLINK-30686 we refined the adaptive batch configuration. However, the key of a configuration item `SPECULATIVE_ENABLED` has been modified incorrectly. According to flip-283, it should be modified to `execution.batch.speculative.enabled`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30682,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 09:33:44 UTC 2023,,,,,,,,,,"0|z1fm6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 08:39;mapohl;master: 8efd5e8c809d607529bb251f6f034893c918905e;;;","03/Feb/23 08:41;mapohl;[~JunRuiLi] can we close this issue? The build on {{master}} succeeded (ignoring the e2e1 module which is caused by FLINK-30881): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45664&view=results

The [next build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45669&view=results] on {{master}} will include your change and the FLINK-30881: That one should, hopefully turn green. Or maybe, just close this issue after the next build is done. ...to be on the safe side.;;;","03/Feb/23 08:54;JunRuiLi;Thanks [~mapohl] , I'll close this issue.;;;","03/Feb/23 09:00;mapohl;Thanks [~JunRuiLi]. Just on a side note: I removed the release note entry again. ""Release note"" is meant to be filled by a description that would end up in the release notes of the corresponding release (in this case 1.17.0). But it looks like you accidentally added the version there. If you think that this change deserves release notes, you might come up with something more meaningful to the user describing this fix. :-);;;","03/Feb/23 09:33;JunRuiLi;[~mapohl] Thank you for reminding me. I accidentally made a mistake.:D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Doc(zh version) has misspelled words ,FLINK-30888,13522845,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,marlin,marlin,03/Feb/23 00:42,17/Apr/23 03:59,04/Jun/24 20:41,13/Apr/23 03:27,,,,,,,1.18.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"[https://github.com/apache/flink/blob/master/docs/content.zh/docs/dev/datastream/overview.md]

Iterative streaming 程序实现了 +{color:#de350b}setp function{color}+ 并将其嵌入到 {{IterativeStream}} 。由于 DataStream 程序可能永远不会完成，因此没有最大迭代次数。相反，你需要指定流的哪一部分反馈给迭代，哪一部分使用[旁路输出](\{{< ref ""docs/dev/datastream/side_output"" >}})或{{{}过滤器{}}}转发到下游。

==> step function

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 03:28:00 UTC 2023,,,,,,,,,,"0|z1fm34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:02;paul8263;Hi all,

I would like to fix this.;;;","13/Apr/23 03:28;tanyuxin;I found the typo has been fixed. 
https://github.com/apache/flink/blob/master/docs/content.zh/docs/dev/datastream/overview.md;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI workflow for externalized connectors doesn't cache Flink's binary download,FLINK-30887,13522790,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,02/Feb/23 13:25,09/Feb/23 16:05,04/Jun/24 20:41,02/Feb/23 13:36,,,,,,,,,,,,,,,,,,,,,,,Build System / CI,,,,,0,pull-request-available,,,,The current CI workflow for PRs that runs on externalized connectors doesn't cache Flink binary downloads properly. We should fix this. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 13:36:16 UTC 2023,,,,,,,,,,"0|z1flqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 13:36;martijnvisser;Fixed in ci_utils:
7227de526e09f0a891a4a8ec05f7054c1f4bac52
9141a06c6a6d44d6a483bf602fe8f9dfd03e82ac
2f47df76f83d5e07e5e736b0344585d77cf51c85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Content area is too narrow to show all content,FLINK-30886,13522788,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,arihuttunen,arihuttunen,02/Feb/23 13:15,07/Feb/23 02:12,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,"If you open a page like this, you notice that the main content is not fully visible.

[https://nightlies.apache.org/flink/flink-docs-master/api/python/reference/pyflink.table/table_environment.html]

Here's a screenshot. You can see that the right-most characters are cut off. The screenshot is of Vivaldi, but it looks like that on Safari as well.

!Screenshot 2023-02-02 at 15.13.57.png|width=1498,height=833!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/23 13:15;arihuttunen;Screenshot 2023-02-02 at 15.13.57.png;https://issues.apache.org/jira/secure/attachment/13055057/Screenshot+2023-02-02+at+15.13.57.png","07/Feb/23 02:12;hxb;image-2023-02-07-10-12-09-237.png;https://issues.apache.org/jira/secure/attachment/13055199/image-2023-02-07-10-12-09-237.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 02:12:20 UTC 2023,,,,,,,,,,"0|z1flqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 13:49;martijnvisser;[~hxbks2ks] [~dianfu] Is this fixable? ;;;","03/Feb/23 09:42;arihuttunen;I took a short look at the page contents, and it uses this [https://github.com/pydata/pydata-sphinx-theme]. This seems related to the issue: https://github.com/pydata/pydata-sphinx-theme/issues/17;;;","07/Feb/23 02:12;hxb;T he content that is too long needs to be viewed by scrolling, we can improve this style.

!image-2023-02-07-10-12-09-237.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optional group pattern starts with non-optional looping pattern get wrong result on followed-by,FLINK-30885,13522782,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,02/Feb/23 12:31,17/Feb/23 07:43,04/Jun/24 20:41,14/Feb/23 09:14,1.15.3,1.16.1,,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,Library / CEP,,,,,0,,,,,"{code:java}
Pattern.begin(""A"")
  .followedBy(
    Pattern.begin(""B"").oneOrMore().greedy().consecutive()
      .next(""C""))
  .optional()
  .next(""D""){code}
This can match ""a1 e1 d1"", which is not the expected behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 09:14:14 UTC 2023,,,,,,,,,,"0|z1flp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 09:14;dianfu;Fixed in:
- master via 845d04d07d623e93a5bc3276eab45eed9edec264
- release-1.17 via cf448f05c5605f37dd170f8709f28d836feba5a6
- release-1.16 via b13a11c04c0fe96ee5a05878fbab99df7ec3478f
- release-1.15 via 1ac1e0d7dd078a85cb0f127aa75e161e03b2fd41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GetTable from Flink catalog should be judged whether it is a sink table,FLINK-30884,13522780,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,heigebupahei,heigebupahei,02/Feb/23 12:24,04/Feb/23 06:19,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,,0,,,,,"I want to say that when I use a third-party persistent catalog to manage the metadata of my persistent table,

I may want to judge the options of the table I need to generate by whether it is a sink table

For example when using kafka connector

When I use kafka as source table,

The following parameters are required:

offset, properties.group.id, etc.

When I use kafka as the sink representation, I will pass in some parameters only about the sink table, for example:

sink.delivery-guarantee

sink.partitioner

So why can't we add a switch to tell the catalog this information, which is very useful in platform development! !

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Sat Feb 04 06:19:14 UTC 2023,,,,,,,,,,"0|z1floo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 12:29;heigebupahei;[~fsk119] [~godfrey]  ;;;","03/Feb/23 10:25;luoyuxia;Hi, [~heigebupahei] Could you please explain about the use scenario about catalog need to know whether the table is a source or sink?

From my side, a table stored in catalog can be either source or sink.  I really don't think catalog may need this information.;;;","04/Feb/23 04:40;heigebupahei;In one case, we used an external catalog to manage my kafka tables, preserving information such as:

The topic format and columns and columnttypes

But the kafka table can actually be used to read or write. When it is used as a write table, I may add some other parameters such as:

sink.delivery-guarantee

sink.partitioner

The properties. The sasl. Jaas. Config authentication information, etc

As source I need to add these parameters:

scan.startup.mode

offset

The properties. The sasl. Jaas. Config authentication information, etc

I need to ask some external systems to generate this information. Therefore, I want to know whether gettable is a sink table so that I can determine how to generate these options;;;","04/Feb/23 06:19;luoyuxia;I think now I understand your case. It's valid, but I still don't think it's Catalog's responsibility. Catalog is designed to store table ignoring table source or sink. 

For you case, I think you can implement it in DynamicTableSink/ DynamicTableSource, you can generate these information  in method getScanRuntimeProvider. 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing JobID caused the k8s e2e test to fail,FLINK-30883,13522775,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,02/Feb/23 11:57,04/Oct/23 10:04,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,Runtime / Coordination,,,,0,auto-deprioritized-critical,test-stability,,,"We've experienced a test failure in {{Run kubernetes application HA test}} due to a {{CliArgsException}}:
{code}
Feb 01 15:03:15 org.apache.flink.client.cli.CliArgsException: Missing JobID. Specify a JobID to cancel a job.
Feb 01 15:03:15 	at org.apache.flink.client.cli.CliFrontend.cancel(CliFrontend.java:689) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
Feb 01 15:03:15 	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1107) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
Feb 01 15:03:15 	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
Feb 01 15:03:15 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28) [flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
Feb 01 15:03:15 	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189) [flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
Feb 01 15:03:15 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157) [flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45569&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=9866",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30788,,"02/Feb/23 13:58;mapohl;e2e_test_failure.log;https://issues.apache.org/jira/secure/attachment/13055063/e2e_test_failure.log","02/Feb/23 13:58;mapohl;flink-vsts-client-fv-az378-840.log;https://issues.apache.org/jira/secure/attachment/13055064/flink-vsts-client-fv-az378-840.log","02/Feb/23 13:58;mapohl;jobmanager.0.log;https://issues.apache.org/jira/secure/attachment/13055062/jobmanager.0.log","02/Feb/23 13:58;mapohl;jobmanager.1.log;https://issues.apache.org/jira/secure/attachment/13055061/jobmanager.1.log","02/Feb/23 13:58;mapohl;taskmanager.log;https://issues.apache.org/jira/secure/attachment/13055060/taskmanager.log",,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 04 10:04:46 UTC 2023,,,,,,,,,,"0|z1flnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 12:03;mapohl;It was discovered while working on FLINK-30788. It's quite unlikely that this failure is related to FLINK-30788 because FLINK-30788 only did a minor refactoring moving a constructor call from the implementing subclasses into {{AbstractHaServices}}.;;;","02/Feb/23 13:04;mapohl;{code}
Feb 01 14:53:49 deployment.apps/flink-native-k8s-application-ha-1 condition met
Feb 01 14:53:49 Waiting for job (flink-native-k8s-application-ha-1-65d85b768b-7q5nr) to have at least 3 completed checkpoints ...
Feb 01 14:55:57 Waiting for jobmanager pod flink-native-k8s-application-ha-1-65d85b768b-7q5nr ready.
Feb 01 14:55:57 pod/flink-native-k8s-application-ha-1-65d85b768b-7q5nr condition met
Feb 01 14:55:57 Waiting for log ""Restoring job  from Checkpoint""...
Feb 01 14:56:31 Log ""Restoring job  from Checkpoint"" shows up.
{code}
It appears that the {{job_id}} wasn't properly extracted in [test_kubernetes_application_ha.sh:71|https://github.com/apache/flink/blob/6cce68dcdc1baf4be2a9e1549983d010644b5ee3/flink-end-to-end-tests/test-scripts/test_kubernetes_application_ha.sh#L71]. {{jm_job_name}} in provided (flink-native-k8s-application-ha-1-65d85b768b-7q5nr) ;verifyable through the logs shown above). The most probable reason is that the job wasn't submitted;;;","02/Feb/23 14:06;mapohl;I extracted the available logs into dedicated files. It appears that the jobmanager restarted at least once. The strange thing is that even in {{jobmanager.0.log}} it states that the job was recovered.

Source: jobmanager.0.log
{code:java}
Feb 01 15:03:03 2023-02-01 14:55:28,715 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job 0000000068e961ca0000000000000000 was recovered successfully.
{code}
It looks like there was more than one JobManager restart happening with the expected log line ""Job \{jobId} is submitted"" probably being located in the logs of the missing JobManager run. But I struggle to find evidence for this theory: No additional logs are provided.;;;","02/Feb/23 14:09;mapohl;{{jobmanager.0.log}} appears to be the JobManager run before the kill command is executed from within the test. The SIGTERM signal is received in {{jobmanager.0.log}} at 14:55:57,715 :
{code}
Feb 01 15:03:03 2023-02-01 14:55:57,715 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
{code}
which matches the timestamps of the logs that are printed around the time the kill command is triggered in the test code (see {{e2e_test_failure.log}}):
{code}
[...]
Feb 01 14:55:57 Waiting for jobmanager pod flink-native-k8s-application-ha-1-65d85b768b-7q5nr ready.
Feb 01 14:55:57 pod/flink-native-k8s-application-ha-1-65d85b768b-7q5nr condition met
Feb 01 14:55:57 Waiting for log ""Restoring job  from Checkpoint""...
Feb 01 14:56:31 Log ""Restoring job  from Checkpoint"" shows up.
Feb 01 14:56:31 Waiting for job (flink-native-k8s-application-ha-1-65d85b768b-7q5nr) to have at least 1 completed checkpoints ...
Feb 01 14:57:02 Missing JobID. Specify a JobID to cancel a job.
[...]
{code};;;","02/Feb/23 14:11;mapohl;The watchdog logs reveal the number of restarts:
{code:java}
Feb 01 15:03:02 NAME                                                     READY   STATUS    RESTARTS       AGE
Feb 01 15:03:02 pod/flink-native-k8s-application-ha-1-65d85b768b-7q5nr   1/1     Running   2 (7m4s ago)   9m16s
Feb 01 15:03:02 pod/flink-native-k8s-application-ha-1-taskmanager-1-1    1/1     Running   0              6m30s {code}
We should have expected 1 instead of 2 restarts, I guess.;;;","02/Feb/23 14:14;mapohl;Essentially, we're missing the logs of the first run that would have been used to retrieve the job ID.;;;","02/Feb/23 15:24;chesnay;Overall the log retrieval of the k8s e2e tests is incredibly poor. For example, if the application job test doesn't finish in an hour we dont get _any_ logs from the JM.;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Sep/23 03:32;Fei Feng;We encountered this problem in our product environment as well.

There is job with fixed restart strategy that job will restart at most 3 times. After 3 times, the job status will be FAILED. And then the failed job information will be deleted from the job manager UI after 1 hour. When we stop the SessionJob  CR , we will miss cancel job failed exception because the jobid is missing ...;;;","04/Oct/23 10:04;mapohl;Hi [~Fei Feng] , could you provide logs of your scenario? Is it reproducible?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce partition.expiration-time to automatically delete expired partitions,FLINK-30882,13522773,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Feb/23 11:49,05/Feb/23 14:26,04/Jun/24 20:41,05/Feb/23 14:26,,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Same to snapshot expiration, we can also introduce partition expiration to automatically delete expired partitions in commit node.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 05 14:26:32 UTC 2023,,,,,,,,,,"0|z1fln4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Feb/23 14:26;lzljs3620320;master: 42bb9a49d48e35a15d07f75c48aac80c4ef5b8fa
release-0.3: 5283f98ed3b2faf85699c4e661f0ebe258062b7f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crictl/Minikube version mismatch causes errors in k8s setup,FLINK-30881,13522764,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,02/Feb/23 11:12,03/Feb/23 08:55,04/Jun/24 20:41,03/Feb/23 07:45,1.15.3,1.16.1,1.17.0,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,Deployment / Kubernetes,Test Infrastructure,,,,0,pull-request-available,test-stability,,,"We observed constant failures in the e2e k8s tests with permission issues. This was initially accidentally reported through FLINK-29671. But FLINK-29671 actually covers a different instability.

Here are the build failures initially reported in FLINK-29671:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4972]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4900]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5597]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4818]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=5852]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45591&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4915]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45598&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4921]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4991]

{code:java}
Feb 01 11:00:45 Starting minikube ...
Feb 01 11:00:45 * minikube v1.29.0 on Ubuntu 20.04
Feb 01 11:00:45 * Using the none driver based on existing profile
Feb 01 11:00:45 * Starting control plane node minikube in cluster minikube
Feb 01 11:00:45 * Restarting existing none bare metal machine for ""minikube"" ...
Feb 01 11:00:45 * OS release is Ubuntu 20.04.5 LTS
Feb 01 11:01:22 
X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo /usr/local/bin/crictl version: exit status 1
stdout:
[...]  
Feb 01 11:01:22 
E0201 11:01:22.809164  241870 root.go:80] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
Feb 01 11:01:22 
X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
* Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
* Related issue: https://github.com/kubernetes/minikube/issues/9165
[...]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,,,FLINK-30893,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 08:55:55 UTC 2023,,,,,,,,,,"0|z1fll4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 11:14;mapohl;[~wangyang0918] reported in FLINK-29671:

{quote}
It seems that the minikube in the azure pipeline upgraded from v1.28.0 to v1.29.0. So it might be with incompatible crictl@v1.24.2.
{quote};;;","02/Feb/23 11:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4951;;;","02/Feb/23 13:19;mapohl;Are you going to provide a fix, [~wangyang0918]?;;;","02/Feb/23 14:17;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45616&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5878
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45617&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4857
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45618&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5126;;;","02/Feb/23 15:09;mapohl;There's a Minikube Github issue talking about problem: https://github.com/kubernetes/minikube/issues/15758 ;;;","02/Feb/23 15:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45625&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5050;;;","02/Feb/23 23:06;mapohl;[~wangyang0918] I fixed the Minikube version to 1.28.0 now. I think having a fixed Minikube version in general instead of relying on whatever Azure's container is offering is a good thing for more stable builds, anyway. Feel free to approve and merge the PRs if you agree.;;;","03/Feb/23 06:09;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5674]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5658]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5088]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45652&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5639] 

 ;;;","03/Feb/23 07:45;mapohl;master: 0e4fb6bbfbcf9bb623a2b26826e53d720b74c898
1.16: e13a73f36e8f3582c60253bfda81823c85b6f3ca
1.15: 2cef2cb3f9cfd331bbf493f0d1648b688f9e9c65;;;","03/Feb/23 08:55;mapohl;Some pre-fix build failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45660&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5007
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45659&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4943
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45664&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4970;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI for externalized connectors is failing because it doesn't point to the binary archive,FLINK-30880,13522759,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,eskabetxe,eskabetxe,02/Feb/23 10:41,04/Apr/23 15:28,04/Jun/24 20:41,29/Mar/23 08:20,,,,,,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,pull-request-available,,,,"CI is falling as binaries from 1.16.0 are no more available

we could update to 1.16.1 or define the download link to archive (it is much slower than release link)

https://archive.apache.org/dist/flink/flink-1.16.0/flink-1.16.0-bin-scala_2.12.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 08:20:56 UTC 2023,,,,,,,,,,"0|z1flk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 10:37;eskabetxe;[~martijnvisser] I think this could be closed..;;;","29/Mar/23 08:20;chesnay;Subsumed by FLINK-30963.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaShuffleITCase failed with multiple test failures due to a TimeoutException while creating topics,FLINK-30879,13522724,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,mapohl,mapohl,02/Feb/23 08:51,16/Oct/23 07:08,04/Jun/24 20:41,16/Oct/23 07:08,1.17.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,test-stability,,,,"The test topic creation failed for multiple tests in {{{}KafkaShuffleITCase{}}}.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=36821
{code:java}
Feb 02 02:45:42 java.lang.AssertionError: Create test topic : test_assigned_to_partition-c7f57154-bf5c-4270-9d26-8f54394573b7_IngestionTime failed, org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics
Feb 02 02:45:42 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:199)
Feb 02 02:45:42 	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
Feb 02 02:45:42 	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:213)
Feb 02 02:45:42 	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.testAssignedToPartition(KafkaShuffleITCase.java:295)
Feb 02 02:45:42 	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.testAssignedToPartitionIngestionTime(KafkaShuffleITCase.java:115)
Feb 02 02:45:42 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 02 02:45:42 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 02 02:45:42 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 02 02:45:42 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
Feb 02 02:45:42 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 02 02:45:42 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 02 02:45:42 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 02 02:45:42 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 02 02:45:42 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Feb 02 02:45:42 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Feb 02 02:45:42 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Feb 02 02:45:42 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Feb 02 02:45:42 	at java.base/java.lang.Thread.run(Thread.java:829) {code}
The original cause might be a {{{}TimeoutException{}}}:
{code:java}
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: createTopics
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:175)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:213)
	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.testAssignedToPartition(KafkaShuffleITCase.java:295)
	at org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase.testAssignedToPartitionIngestionTime(KafkaShuffleITCase.java:115) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30503,FLINK-30298,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 18:01:59 UTC 2023,,,,,,,,,,"0|z1flc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 08:53;mapohl;Other kafka-related tests with timeout or topic creation issues are linked;;;","02/Feb/23 09:21;gaborgsomogyi;I'm 99% sure that this is a Kafka issue but since we don't see the Kafka pod logs hard to say why it's hanging...;;;","02/Feb/23 09:25;mapohl;Yeah, I remember you were talking about something like that in another issue already (which I cannot find right now). I'm reluctant to merge all the test failures into a single issue right now. But maybe, it's time to collect all these issue under one umbrella ticket which also covers adding the kafka logs to the build artifacts similarly to what we do with the ZooKeeper logs. ;;;","02/Feb/23 09:38;gaborgsomogyi;Good idea (y);;;","22/Feb/23 14:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46420&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36958
A different stacktrace but still caused by a request timeout:
{code}
[...]
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:258)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:249)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:242)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:749)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:726)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)
	at akka.actor.ActorCell.invoke(ActorCell.scala:547)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: org.apache.kafka.common.KafkaException: org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The request timed out.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593)
	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735)
	at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:159)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:650)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.abortTransactions(FlinkKafkaProducer.java:1290)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.initializeState(FlinkKafkaProducer.java:1216)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:189)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:171)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:95)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:274)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:741)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:717)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:684)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The request timed out.
	at org.apache.kafka.clients.producer.internals.TransactionManager$InitProducerIdHandler.handleResponse(TransactionManager.java:1418)
	at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1322)
	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
	at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:583)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:575)
	at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:418)
	at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:316)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
	... 1 more
{code};;;","27/Feb/23 18:01;mapohl;Another one with unexpected error in {{InitProducerIdResponse}} which was caused by a timeout:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46596&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36982

This time, {{KafkaWriterITCase.testAbortOnClose}} was affected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock,FLINK-30878,13522716,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,02/Feb/23 08:30,02/Feb/23 11:03,04/Jun/24 20:41,02/Feb/23 11:03,1.15.4,1.16.2,1.17.0,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,Deployment / Kubernetes,Runtime / Coordination,,,,0,pull-request-available,test-stability,,,"We're seeing a test failure in {{KubernetesHighAvailabilityRecoverFromSavepointITCase}} due to a deadlock:
{code:java}
2023-02-01T18:53:35.5540322Z ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]
2023-02-01T18:53:35.5540900Z    java.lang.Thread.State: TIMED_WAITING (parking)
2023-02-01T18:53:35.5541272Z 	at sun.misc.Unsafe.park(Native Method)
2023-02-01T18:53:35.5541932Z 	- parking to wait for  <0x00000000d14d7b60> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-01T18:53:35.5542496Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2023-02-01T18:53:35.5543088Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)
2023-02-01T18:53:35.5543672Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-02-01T18:53:35.5544240Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)
2023-02-01T18:53:35.5544801Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-01T18:53:35.5545632Z 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)
2023-02-01T18:53:35.5546409Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=61916]

The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30474,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 11:03:06 UTC 2023,,,,,,,,,,"0|z1flag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 10:30;mapohl;{code:java}
Java stack information for the threads listed above:
===================================================
""config-map-watch-handler-thread-2851"":
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback.lambda$run$0(KubernetesSharedInformer.java:243)
        - waiting to lock <0x00000000d14d1928> (a java.lang.Object)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback$$Lambda$879/261416860.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
""config-map-watch-handler-thread-4"":
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.notifyAllKnownLeaderInformation(DefaultMultipleComponentLeaderElectionService.java:264)
        - waiting to lock <0x00000000d14d6bd0> (a java.lang.Object)
        at org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver$ConfigMapCallbackHandlerImpl.onModified(KubernetesMultipleComponentLeaderElectionDriver.java:238)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$EventHandler.lambda$null$3(KubernetesSharedInformer.java:208)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$EventHandler$$Lambda$945/45359867.accept(Unknown Source)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback.lambda$run$0(KubernetesSharedInformer.java:246)
        - locked <0x00000000d14d1928> (a java.lang.Object)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback$$Lambda$879/261416860.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
""leadershipOperationExecutor-thread-1"":
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onLeaderInformationChange(DefaultLeaderElectionService.java:254)
        - waiting to lock <0x00000000d14d6bf8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.lambda$sendLeaderInformationChange$3(DefaultMultipleComponentLeaderElectionService.java:254)
        - locked <0x00000000d14d6bd0> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService$$Lambda$948/665822281.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
""mini-cluster-io-thread-2"":
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.hasLeadership(DefaultMultipleComponentLeaderElectionService.java:180)
        - waiting to lock <0x00000000d14d6bd0> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter.hasLeadership(MultipleComponentLeaderElectionDriverAdapter.java:51)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.hasLeadership(DefaultLeaderElectionService.java:155)
        - locked <0x00000000d14d6bf8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.confirmLeadership(DefaultLeaderElectionService.java:120)
        - locked <0x00000000d14d6bf8> (a java.lang.Object)
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner.lambda$forwardConfirmLeaderSessionFuture$2(DefaultDispatcherRunner.java:175)
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner$$Lambda$886/1947395283.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
        at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
{code}
The deadlock occurs due to the lock being acquired in FLINK-30474 before triggering the {{onLeaderInformationChange}} in [DefaultMultipleComponentLeaderElectionService.java|https://github.com/apache/flink/pull/21537/commits/dbce30603bc9436032a98ae1b9d33f99e0790099#diff-8e1b9ed8178f117c8e5f3be67ce6f5b341862d06ce27f14d52901b503f6a3a0bR254]
[:254|https://github.com/apache/flink/pull/21537/commits/dbce30603bc9436032a98ae1b9d33f99e0790099#diff-8e1b9ed8178f117c8e5f3be67ce6f5b341862d06ce27f14d52901b503f6a3a0bR254];;;","02/Feb/23 10:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45592&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=34767;;;","02/Feb/23 11:03;mapohl;master: e3cd3b311c1c8a6a0e0cdc849d7c951ef8beea5c
1.16: 3366c1f941c45e7fc47d57dc46ca7728bc1df33b
1.15: 8ee1ceb2f532f06d3cceefefc204772fb9b83594;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for @Before / @After for JUnit 4 and 5,FLINK-30877,13522713,13443518,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gaoyunhaii,gaoyunhaii,02/Feb/23 08:22,02/Feb/23 08:22,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,,,,,"Based on the new migration tests infrastructure, we might also support @Before / @After for both junit 4 and 5. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-02 08:22:26.0,,,,,,,,,,"0|z1fl9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode,FLINK-30876,13522711,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,02/Feb/23 08:21,06/Feb/23 03:23,04/Jun/24 20:41,06/Feb/23 03:23,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"Now, ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode. This may cause error while creating StreamGraph for BatchExecMultiInput due to different id of rootNode and inputNode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 02:58:30 UTC 2023,,,,,,,,,,"0|z1fl9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 02:58;godfrey;Fixed in master: 111342f37bdc0d582d3f7af458d9869f0548299f

1.16.2: 00a4ab9011cf13facde364c52a170a7a897cdcce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix usages of legacy AdaptiveBatchScheduler configuration,FLINK-30875,13522705,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,02/Feb/23 07:43,03/Feb/23 10:28,04/Jun/24 20:41,03/Feb/23 10:28,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In FLINK-30686, we deprecated the JobManagerOptions's AdaptiveBatchScheduler configuration. However, these configuration items still have some calls. And we should change these calls to new configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 10:28:06 UTC 2023,,,,,,,,,,"0|z1fl80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 10:28;zhuzh;Fixed via 851771035ed964063a9bfec8a59b4669a58d8aa1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce configuration for table store,FLINK-30874,13522686,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zjureel,zjureel,02/Feb/23 03:56,12/Feb/23 07:55,04/Jun/24 20:41,12/Feb/23 07:55,table-store-0.4.0,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,Introduce configuration related classes similar to flink in flink-core for table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 12 07:55:08 UTC 2023,,,,,,,,,,"0|z1fl48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 02:48;zhoupeijie;Hello, can this ticket be assigned to me? I'm interested in flink table store.;;;","03/Feb/23 04:59;zjureel;[~zhoupeijie] Sure! Hi [~lzljs3620320] can you help to assign it?;;;","12/Feb/23 07:55;lzljs3620320;Thanks [~zhoupeijie] and [~zjureel], this is duplicated to FLINK-31023, I will close this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSystem Table API connector doesn't discover new files when partitions are specified,FLINK-30873,13522597,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sap1ens,sap1ens,01/Feb/23 21:20,09/Feb/23 10:35,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,,,,,"When *source.monitor-interval* is configured I expect the source connector to discover new files. However, if the source table was created with partitions (using PARTITIONED BY), it only discovers new files in the partition folders that existed during the startup; it doesn't discover new partitions. 

I believe the problem is [here|https://github.com/apache/flink/blob/5f2d088a2713ced5c6ce072db92f4378f73bc739/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/FileSystemTableSource.java#L276-L286], only these file paths are monitored internally, no partition discovery is currently implemented. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 10:35:20 UTC 2023,,,,,,,,,,"0|z1fkko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 10:19;mateczagany;If you're not working on it yet and a committer could assign this to me, I'd gladly pick it up. I looked at it briefly and as you said currently FileSystemTableSource initially finds all partition directories in the root path and uses those to initialize FileSource.

By only passing the root path to FileSource it seems to solve this shortcoming, but it definitely needs extensive testing to ensure it doesn't break current functionality.;;;","02/Feb/23 16:51;sap1ens;[~mateczagany] feel free to pick it up! Personally, I just removed the PARTITIONED BY clause and everything started working, which is similar to what you're proposing. However, I think there was probably a reason to implement partitioning this way, so it definitely needs extensive testing. ;;;","09/Feb/23 10:35;mateczagany;I've made some further investigation into this:
 * Assigning the partition columns to the result rows is done by `FileInfoExtractorBulkFormat` and if you set PARTITIONED BY clause it will always try to retrieve the partitioned columns using `PartitionPathUtils#extractPartitionSpecFromPath`. So extracting the partition values from the splits should not be affected by changing the paths we initialize the `FileSource` with. It's only affected by the `BulkFormat` we create `FileSource` with would not be changed with my proposed solution.
 * `FileEnumerator` would only be called with the single root path instead of all the initial partition paths found, you can see the implementation for enumerating the paths in `NonSplittingRecursiveEnumerator#enumerateSplits`. It always recursively enumerates through all the paths given and all its sub directories, so in an ideal environment this should not be an issue either, but it will result in more I/O operations.
 * `FileSystemTableSource` implements `SupportsPartitionPushDown` which can restrict the number of partitions that will be read during planning phase based on the filters of the query (see `PushPartitionIntoTableSourceScanRule` for more info). This can cause issues as this restricts the partitions the connector can read and in case of a dynamic filter (e.g. `partitionField > 20`) we won't be able to respect this optimization in case there are new partition folders.


One (pretty bad) solution I could think of:
Add a new property, e.g. `source.monitor-partitions` which if true, the `FileSystemTableSource` will not scan the root folder for partitions, but it will pass the root folder to `FileSource` which will automatically iterate through its subfolders and parse the partitions. If this property is true, throw an exception if the partitions get pruned by the planner (if the user specifies any filters for the partitions).


But it would be easier for the user to create a new table without partitions like you did, since it will have the exact same functionality as this solution. The only advantage of this solution might be that the user does not have to create two seperate table if they write and read to/from it as well.

Best solution in my opinion would be to explain this in the documentation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move GCP example to GCP connector repo,FLINK-30872,13522567,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,01/Feb/23 18:21,03/Feb/23 09:25,04/Jun/24 20:41,03/Feb/23 09:25,,,,,,,gcp-pubsub-3.0.1,,,,,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 09:25:59 UTC 2023,,,,,,,,,,"0|z1fke0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 09:25;martijnvisser;Fixed in main: 7d8090b72ffb75727d877534d670a52e3eaa039b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add bloom filter for orc ,FLINK-30871,13522538,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,01/Feb/23 16:07,10/Feb/23 06:56,04/Jun/24 20:41,10/Feb/23 06:56,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 06:56:49 UTC 2023,,,,,,,,,,"0|z1fk7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 06:56;lzljs3620320;master: bd8b6c3d9f71f0fdc3b86df860be8634d62c55cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regressions notifications in Slack are cut off,FLINK-30870,13522480,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,martijnvisser,martijnvisser,01/Feb/23 13:15,10/Feb/23 07:41,04/Jun/24 20:41,10/Feb/23 07:41,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,"Example from today at https://apache-flink.slack.com/archives/C0471S0DFJ9/p1675253720571659

{code}
Performance regression
mapRebalanceMapSink.F27_UNBOUNDED median=17231.7398765 recent_median=16165.0549395
multiInputOneIdleMapSink median=11254.5329375 recent_median=10727.7280915
calculateRegionToRestart.BATCH median=12.881527 recent_median=12.096391
partitionRelease.BATCH median=23.2130145 recent_median=21.4858475
checkpointMultiInput median=2.6094395 recent_median=2.477736
checkpointSingleInput.UNALIGNED median=339.229515 recent_median=67.2695295
checkpointSingleInput.UNALIGNED_1 median=215.2789775 recent_median=40.1294965
fireProcessingTimers median=50.9977185 recent_median=44.0925955
globalWindow median=5459.689767 recent_median=5045.436655
<http…
{code}

As you can see, the last part is cut off with {{<htt...}} listed. I'm not sure if this is because there are more regressions, but they aren't posted to Slack (could be) or if this is just a mistake in the output and the {{globalWindow}} was the last regression to be reported. It would be great if this could be validated. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/23 04:26;Yanfei Lei;image-2023-02-06-12-26-57-268.png;https://issues.apache.org/jira/secure/attachment/13055171/image-2023-02-06-12-26-57-268.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 07:41:49 UTC 2023,,,,,,,,,,"0|z1fjuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 13:05;martijnvisser;[~Yanfei Lei] Could you help clarify if this is indeed an issue or if this is as expected? ;;;","03/Feb/23 02:42;Yanfei Lei;[~martijnvisser] Thanks for the ping, this is a known issue with the slack plugin of jenkinsci [https://github.com/jenkinsci/slack-plugin/issues/735#issuecomment-774131103] , because the notification messages are relatively long recently, exceeding the limit of 2048. I'm going to try it with an attachment instead of a message.;;;","03/Feb/23 03:43;leonard;I downgrade the issue priority to Major as it's a known slack plugin issue according to Yanfei's comment;;;","06/Feb/23 04:27;Yanfei Lei;The message after updating the Jenkins script became normal:

!image-2023-02-06-12-26-57-268.png|width=552,height=255!;;;","10/Feb/23 07:41;martijnvisser;Fixed in flink-benchmarks master: 1a6da18e0933b6e43297c1fc44a556c0c162b3e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove remote branches not being used anymore,FLINK-30869,13522471,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,01/Feb/23 12:50,06/Jul/23 15:59,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Build System,,,,,0,,,,,"{code}
$ git branch -a | grep 'remotes/origin' | grep -v '/release-'
  remotes/origin/FLINK-30191
  remotes/origin/blink
  remotes/origin/cron-master-dependency_check
  remotes/origin/cron-master-maven_compat
  remotes/origin/dependabot/npm_and_yarn/flink-runtime-web/web-dashboard/ua-parser-js-0.7.33
  remotes/origin/exp_github_actions
  remotes/origin/experiment_gha_docs
  remotes/origin/master
{code}
There are various remote branches next to {{master}} and the release branches. We should either clean those up or document them (maybe in the [Release Management docs|https://cwiki.apache.org/confluence/display/FLINK/Flink+Release+Management].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31034,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 15:59:48 UTC 2023,,,,,,,,,,"0|z1fjso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 12:55;mapohl;* {{FLINK-30191}} was probably accidentally created. I reached out to the contributor in the ticket.
 * {{master}} is needed for obvious reasons
 * {{{}exp_github_actions{}}}, {{experiment_gha_docs}} is used for the Github Actions migrations work (see related [ML thread|https://lists.apache.org/thread/cvqh07m7b45mzk0pyj9t1yv9m04wkrff] where Chesnay stated that those shouldn't be deleted).
 * {{blink}} contains legacy code from blink. It was requested to keep this code for historical reasons (see related [ML thread|https://lists.apache.org/thread/cvqh07m7b45mzk0pyj9t1yv9m04wkrff] with Yun Tang's post.
 * {{dependabot/*}} are temporary branches that are created by dependabot for dependency version updates.

I'm gonna document the branches that should stay in [Flink Release Management|https://cwiki.apache.org/confluence/display/FLINK/Flink+Release+Management].;;;","01/Feb/23 12:56;mapohl;{{cron-master-dependency_check}} and {{cron-master-maven_compat}} where apparently used by Travis in the past. [~rmetzger] & [~chesnay] confirmed that these branches could be removed.;;;","10/Feb/23 13:05;mapohl;I pushed a copy of {{cron-master-maven_compat}} named {{FLINK-30869-cron-master-maven_compat}} and a copy of {{cron-master-dependency_check}} named {{FLINK-30869-cron-master-dependency_check}} before deleting the original branches:
{code}
➜  flink-master git:(cron-master-maven_compat) ✗ git push origin --delete cron-master-dependency_check
remote: 
remote: GitHub found 15 vulnerabilities on apache/flink's default branch (4 critical, 3 high, 5 moderate, 3 low). To find out more, visit:
remote:      https://github.com/apache/flink/security/dependabot
remote: 
To github.com:apache/flink.git
 - [deleted]                 cron-master-dependency_check
➜  flink-master git:(cron-master-maven_compat) ✗ git push origin --delete cron-master-maven_compat    
remote: 
remote: GitHub found 15 vulnerabilities on apache/flink's default branch (4 critical, 3 high, 5 moderate, 3 low). To find out more, visit:
remote:      https://github.com/apache/flink/security/dependabot
remote: 
To github.com:apache/flink.git
 - [deleted]                 cron-master-maven_compat
{code};;;","10/Feb/23 13:06;mapohl;I'm gonna leave the two branches for a bit to see whether nothing comes up that would require recreating them under the old name.;;;","06/Jul/23 14:03;mapohl;I deleted {{FLINK-30869-cron-master-dependency_check}} and {{FLINK-30869-cron-master-maven_compat}}:
{code:java}
$ git push origin --delete FLINK-30869-cron-master-dependency_check               
To github.com:apache/flink.git
 - [deleted]                 FLINK-30869-cron-master-dependency_check
$ git push origin --delete FLINK-30869-cron-master-maven_compat
To github.com:apache/flink.git
 - [deleted]                 FLINK-30869-cron-master-maven_compat {code};;;","06/Jul/23 14:06;mapohl;In the meantime two other branches where added. Where those branches accidentally added to the Apache Flink repository?
[~tzulitai] {{remove-kafka-connector}}
[~Weijie Guo] {{origin/process-func-api-poc-weijie}};;;","06/Jul/23 14:14;Weijie Guo;Thanks [~mapohl] for reminding me! Sorry, I accidentally pushed it to the origin when push this branch to my own fork... I have been deleted it.;;;","06/Jul/23 15:06;mapohl;No worries, happened to me, too. :-) In case, you didn't know, yet: You can use {{no_push}} in your git configuration for {{master}} if you would like to avoid such a situation in the future:
{code}
[remote ""origin""]
        url = git@github.com:apache/flink.git
        fetch = +refs/heads/*:refs/remotes/origin/*
        pushurl = no_push
{code};;;","06/Jul/23 15:59;Weijie Guo;Interesting, I got it!(y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert to use LongSerializer for seralization in the TimeIndicatorTypeInfo,FLINK-30868,13522467,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,01/Feb/23 12:22,06/Feb/23 07:13,04/Jun/24 20:41,06/Feb/23 07:13,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 07:13:59 UTC 2023,,,,,,,,,,"0|z1fjrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 07:13;fsk119;Merged into master: 48a2e5a440067bcab429fd9cf90e3888e0498473;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support streaming read changes from overwrite,FLINK-30867,13522465,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,01/Feb/23 12:05,03/Feb/23 13:45,04/Jun/24 20:41,03/Feb/23 13:45,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 13:45:59 UTC 2023,,,,,,,,,,"0|z1fjrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 13:45;lzljs3620320;master: 0b2662dd9f31b6310b645e570528fed20722bec5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FileIO for table store,FLINK-30866,13522459,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,01/Feb/23 11:41,02/Feb/23 12:48,04/Jun/24 20:41,02/Feb/23 12:48,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"FileIO aims to make table-store and the FileSystem of Flink independent, In this way, we can provide different FileSystem support in the Flink cluster, such as other S3 buckets. In addition, different engines can provide the same FileIO experience (such as configuration and usage)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 12:48:45 UTC 2023,,,,,,,,,,"0|z1fjq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 12:48;lzljs3620320;master: b9811b730c6160149ed520dc31ad5f8b075f1be0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make pyflink-ml modules consistent with Flink ML Java library packages,FLINK-30865,13522457,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,lindong,lindong,01/Feb/23 11:26,06/Feb/23 10:47,04/Jun/24 20:41,06/Feb/23 10:46,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"For some classes in pyflink-ml, its module is inconsistent with the package of the corresponding class in Flink ML Java library.

For example, the Stage class in pyflink-ml is currently under the module pyflink.ml.core.api, whereas the Stage class in Flink ML Java library is under the package org.apache.flink.ml.api.

It would be useful to make their package consistent, e.g. moving the Stage class in pyflink-ml to be under the module pyflink.ml.core.api, so that the experience of using pyflink-ml and Flink ML Java library can be as similar as possible.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 10:47:32 UTC 2023,,,,,,,,,,"0|z1fjpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 10:47;lindong;Added to the apache/flink-ml master branch 3f62235a65ed905028101c3ad8e6c9e9923ffaf4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optional pattern at the start of a group pattern not working,FLINK-30864,13522456,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,01/Feb/23 11:25,17/Feb/23 07:44,04/Jun/24 20:41,15/Feb/23 01:40,1.15.3,1.16.1,,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,Library / CEP,,,,,0,pull-request-available,,,,"The optional pattern at the start of a group pattern turns out be ""not optional"", e.g.
{code:java}
Pattern.<String>begin(""A"").next(Pattern.<String>begin(""B"").optional().next(""C"")).next(""D"")
{code}
cannot match sequence ""a1 c1 d1"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 01:40:42 UTC 2023,,,,,,,,,,"0|z1fjpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 09:14;dianfu;Fixed in:
- master via 845d04d07d623e93a5bc3276eab45eed9edec264
- release-1.17 via cf448f05c5605f37dd170f8709f28d836feba5a6
- release-1.16 via b13a11c04c0fe96ee5a05878fbab99df7ec3478f
- release-1.15 via 1ac1e0d7dd078a85cb0f127aa75e161e03b2fd41;;;","14/Feb/23 12:55;mapohl;This change causes compiler errors in {{release-1.16}} and {{release-1.15}}.

Reverts happened for the following branches:

1.16: 20da7a9f09b48b61c1bc6c73527e8e58aac0331f
1.15: 735609c44261d631527e492dfdfa68f0dcbd51b0;;;","15/Feb/23 01:40;dianfu;Thanks [~mapohl].

Have fixed in:
- release-1.16: 14531dac756b87eb12a6864a0850415496a6bab4
- release-1.15: 66dc917542419ff351593412d1ddde69bb624f27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register local recovery files of changelog before notifyCheckpointComplete(),FLINK-30863,13522454,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,01/Feb/23 10:24,20/Oct/23 09:09,04/Jun/24 20:41,29/Aug/23 08:39,1.17.0,,,,,,1.19.0,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,0,pull-request-available,,,,"If TM is materialized before receiving confirm(), the previously uploaded queue in `FsStateChangelogWriter` will be cleared, so the local files of the completed checkpoint will not be registered again, while the JM owned files are registered before confirm(), and do not depend on the uploaded queue, so the local files are deleted, and the DFS files are still there. 

 

We have encountered the following situation, the job cannot find the local recovery files, but can restore from the DFS files:
{code:java}
2023-01-18 17:21:13,412 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - SlidingProcessingTimeWindows (37/48)#1 #1 (fa12cfa3b811a351e031b036b0e85d91) switched from DEPLOYING to INITIALIZING.
2023-01-18 17:21:13,440 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.runtime.state.TaskLocalStateStoreImpl       [] - Found registered local state for checkpoint 11599 in subtask (2daf1d9bc9ed40ecb191303db813b0de - 0a448493b4782967b150582570326227 - 36) : TaskOperatorSubtaskStates{subtaskStatesByOperatorID={0a448493b4782967b150582570326227=SubtaskState{operatorStateFromBackend=StateObjectCollection{[]}, operatorStateFromStream=StateObjectCollection{[]}, keyedStateFromBackend=StateObjectCollection{[org.apache.flink.runtime.state.changelog.ChangelogStateBackendLocalHandle@38aa46db]}, keyedStateFromStream=StateObjectCollection{[]}, inputChannelState=StateObjectCollection{[]}, resultSubpartitionState=StateObjectCollection{[]}, stateSize=1764644202, checkpointedSize=1997682}}, isTaskDeployedAsFinished=false, isTaskFinished=false}
2023-01-18 17:21:13,442 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting managed memory shared cache for RocksDB.
2023-01-18 17:21:13,446 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 1438814063 bytes
2023-01-18 17:21:13,447 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation [] - Starting to restore from state handle: IncrementalLocalKeyedStateHandle{metaDataState=File State: file:/opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/localState/aid_45af7e6b612dad10b60554d81323d5f3/jid_2daf1d9bc9ed40ecb191303db813b0de/vtx_0a448493b4782967b150582570326227_sti_36/chk_125/0d082666-bd31-4ebe-9977-545c0d9b18a5 [1187 bytes]} DirectoryKeyedStateHandle{directoryStateHandle=DirectoryStateHandle{directory=/opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/localState/aid_45af7e6b612dad10b60554d81323d5f3/jid_2daf1d9bc9ed40ecb191303db813b0de/vtx_0a448493b4782967b150582570326227_sti_36/chk_125/b3e1d20f164d4c5baed291f5d1224183}, keyGroupRange=KeyGroupRange{startKeyGroup=96, endKeyGroup=98}} without rescaling.
2023-01-18 17:21:13,495 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation [] - Finished restoring from state handle: IncrementalLocalKeyedStateHandle{metaDataState=File State: file:/opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/localState/aid_45af7e6b612dad10b60554d81323d5f3/jid_2daf1d9bc9ed40ecb191303db813b0de/vtx_0a448493b4782967b150582570326227_sti_36/chk_125/0d082666-bd31-4ebe-9977-545c0d9b18a5 [1187 bytes]} DirectoryKeyedStateHandle{directoryStateHandle=DirectoryStateHandle{directory=/opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/localState/aid_45af7e6b612dad10b60554d81323d5f3/jid_2daf1d9bc9ed40ecb191303db813b0de/vtx_0a448493b4782967b150582570326227_sti_36/chk_125/b3e1d20f164d4c5baed291f5d1224183}, keyGroupRange=KeyGroupRange{startKeyGroup=96, endKeyGroup=98}} without rescaling.
2023-01-18 17:21:13,495 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation [] - restore rocksdb cost 48 ms.
2023-01-18 17:21:13,495 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Finished building RocksDB keyed state-backend at /opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/tmp/job_2daf1d9bc9ed40ecb191303db813b0de_op_WindowOperator_0a448493b4782967b150582570326227__37_48__uuid_2cbcf5ff-4451-4788-8762-158077c8368e.
2023-01-18 17:21:13,501 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.changelog.fs.FsStateChangelogStorage        [] - createWriter for operator WindowOperator_0a448493b4782967b150582570326227_(37/48)/KeyGroupRange{startKeyGroup=96, endKeyGroup=98}: 00000000-0000-0000-0000-000000000001
2023-01-18 17:21:13,502 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation [] - read changelog handle start, total state size=190851072 .
2023-01-18 17:21:13,502 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'filesystem' to restore from 'ChangelogStateHandleStreamImpl'.
2023-01-18 17:21:13,529 [Source Data Fetcher for Source: KafkaWindowSource (37/48)#1] INFO  org.apache.kafka.clients.Metadata                            [] - [Consumer clientId=xr_cl_1-36, groupId=xr_cl_1] Cluster ID: 56sVc6RESJ63Jh6BnsMjkA
2023-01-18 17:21:13,515 [SlidingProcessingTimeWindows (37/48)#1] WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for WindowOperator_0a448493b4782967b150582570326227_(37/48) from alternative (1/2), will retry while more alternatives are available.
java.lang.RuntimeException: java.io.FileNotFoundException: /opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/localState/aid_45af7e6b612dad10b60554d81323d5f3/jid_2daf1d9bc9ed40ecb191303db813b0de/taskowned/cc3bac5d-020c-4ee0-8999-d661f4b9beac (No such file or directory)
    at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:121) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:89) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:267) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:701) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:677) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:644) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954) [flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923) [flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746) [flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568) [flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at java.lang.Thread.run(Thread.java:834) [?:1.8.0_102]
Caused by: java.io.FileNotFoundException: /opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/localState/aid_45af7e6b612dad10b60554d81323d5f3/jid_2daf1d9bc9ed40ecb191303db813b0de/taskowned/cc3bac5d-020c-4ee0-8999-d661f4b9beac (No such file or directory)
    at java.io.FileInputStream.open0(Native Method) ~[?:1.8.0_102]
    at java.io.FileInputStream.open(FileInputStream.java:195) ~[?:1.8.0_102]
    at java.io.FileInputStream.<init>(FileInputStream.java:138) ~[?:1.8.0_102]
    at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:141) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:89) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:72) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85) ~[flink-dist_2.12-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
    ... 21 more
2023-01-18 17:21:13,545 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting managed memory shared cache for RocksDB.
2023-01-18 17:21:13,545 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 1438814063 bytes
2023-01-18 17:21:13,546 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation [] - Starting to restore from state handle: IncrementalRemoteKeyedStateHandle{backendIdentifier=b3e1d20f-164d-4c5b-aed2-91f5d1224183, stateHandleId=f404ffdb-715e-4f95-a850-f459639a30e6, keyGroupRange=KeyGroupRange{startKeyGroup=96, endKeyGroup=98}, checkpointId=125, sharedState={001388.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/bd7103e0-fe66-4400-a2a2-e4f3dda01b71 [51353250 bytes], 001383.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5e937d12-e1d5-4427-9401-80f5db6af2ee [67393725 bytes], 001314.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c2d374dc-88ba-4003-bad5-81590e56963d [67407704 bytes], 001403.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a7ec0f13-3ef1-4e69-a50e-071b9f6b092b [67411564 bytes], 001416.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9c82f81a-33d6-4b8f-a237-2150d0c311d8 [10391374 bytes], 001384.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b591513d-b850-4025-a126-452137b4a6fa [67397014 bytes], 001413.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5e2a0408-f9b4-4875-8594-2dc59df3bc66 [5307477 bytes], 001400.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/16bf5cda-4f04-40c9-8b54-ca1c393028f9 [67953551 bytes], 001316.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9543bc5c-dd9d-47b6-9626-159559b9ee45 [67406146 bytes], 001315.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b6099b46-bbef-486a-8c2f-ef232eefdeff [67409984 bytes], 001408.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/66354b67-97b2-41e8-ae48-678af553d2a7 [16861835 bytes], 001404.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a550c63b-003c-48d5-9bc2-45341ce4e641 [67413763 bytes], 001406.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/55822a0c-8448-48d2-a0c1-0bbccfaf31ca [67414722 bytes], 001317.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2b1f4dcc-76e2-4492-8470-bd512031a0ab [67407862 bytes], 001401.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/42a7b773-95d6-4a8b-bb18-64f083e9c627 [20184247 bytes], 001414.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/ec3a3d64-c0c1-4a5e-b541-dfc01a0f028d [19636706 bytes], 001385.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2ef14224-f229-4852-b97e-50268979f2a3 [67395048 bytes], 001399.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a4550160-1c0c-4bb7-9eb9-26e2a6d9ed43 [67931628 bytes], 001381.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c6278757-9048-4e9d-a67d-5b9fd46c2c4f [67404982 bytes], 001368.sst=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/4497322c-c46b-4d42-9313-1f36d929c577', dataBytes=1354}, 001407.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/3308882e-bf0f-4f26-9f3a-3d3dfb05fc85 [67409775 bytes], 001405.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5d5e1529-b109-483d-afa6-5d44297bec6d [67415504 bytes], 001318.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/7f9a7aa5-db80-4448-b987-1f9fdb88f5d1 [67406909 bytes], 001386.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2cf3a080-37d5-4133-b9d2-83e2ad49fc24 [67395104 bytes], 001411.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/66d3176b-2257-403a-be82-03c49162c05f [19605638 bytes], 001410.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/108bb1c1-99da-4920-acab-15f254326ed0 [11733963 bytes], 001387.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9027f480-b895-4336-80b9-dea244ce1572 [67397047 bytes], 001382.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/398f2188-6336-4efe-a014-6bd3bf2ce8c6 [67396271 bytes], 001415.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c73af6f6-18e2-424e-90f9-586809dfbba5 [1931553 bytes], 001402.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/f80462bb-eef1-4e3a-845c-97dea160e306 [67410281 bytes], 001313.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/88251386-e092-4a5e-b8bf-9c2e65148f5a [67408879 bytes]}, privateState={OPTIONS-000013=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b32ad6b5-4540-4b3e-9a82-14cebe231898', dataBytes=17286}, MANIFEST-000004=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c94b6317-b50b-4bf1-938d-83d7aad6ed6a [179781 bytes], CURRENT=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/3665ebc2-52ee-4a2e-87b6-6e0cc8458c37', dataBytes=16}}, metaStateHandle=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9af5edc4-c589-4f0b-91d7-515e068a3454', dataBytes=1187}, registered=false} without rescaling.
2023-01-18 17:22:08,867 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.RocksDBStateDownloader [] - download IncrementalRemoteKeyedStateHandle{backendIdentifier=b3e1d20f-164d-4c5b-aed2-91f5d1224183, stateHandleId=f404ffdb-715e-4f95-a850-f459639a30e6, keyGroupRange=KeyGroupRange{startKeyGroup=96, endKeyGroup=98}, checkpointId=125, sharedState={001388.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/bd7103e0-fe66-4400-a2a2-e4f3dda01b71 [51353250 bytes], 001383.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5e937d12-e1d5-4427-9401-80f5db6af2ee [67393725 bytes], 001314.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c2d374dc-88ba-4003-bad5-81590e56963d [67407704 bytes], 001403.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a7ec0f13-3ef1-4e69-a50e-071b9f6b092b [67411564 bytes], 001416.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9c82f81a-33d6-4b8f-a237-2150d0c311d8 [10391374 bytes], 001384.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b591513d-b850-4025-a126-452137b4a6fa [67397014 bytes], 001413.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5e2a0408-f9b4-4875-8594-2dc59df3bc66 [5307477 bytes], 001400.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/16bf5cda-4f04-40c9-8b54-ca1c393028f9 [67953551 bytes], 001316.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9543bc5c-dd9d-47b6-9626-159559b9ee45 [67406146 bytes], 001315.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b6099b46-bbef-486a-8c2f-ef232eefdeff [67409984 bytes], 001408.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/66354b67-97b2-41e8-ae48-678af553d2a7 [16861835 bytes], 001404.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a550c63b-003c-48d5-9bc2-45341ce4e641 [67413763 bytes], 001406.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/55822a0c-8448-48d2-a0c1-0bbccfaf31ca [67414722 bytes], 001317.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2b1f4dcc-76e2-4492-8470-bd512031a0ab [67407862 bytes], 001401.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/42a7b773-95d6-4a8b-bb18-64f083e9c627 [20184247 bytes], 001414.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/ec3a3d64-c0c1-4a5e-b541-dfc01a0f028d [19636706 bytes], 001385.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2ef14224-f229-4852-b97e-50268979f2a3 [67395048 bytes], 001399.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a4550160-1c0c-4bb7-9eb9-26e2a6d9ed43 [67931628 bytes], 001381.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c6278757-9048-4e9d-a67d-5b9fd46c2c4f [67404982 bytes], 001368.sst=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/4497322c-c46b-4d42-9313-1f36d929c577', dataBytes=1354}, 001407.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/3308882e-bf0f-4f26-9f3a-3d3dfb05fc85 [67409775 bytes], 001405.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5d5e1529-b109-483d-afa6-5d44297bec6d [67415504 bytes], 001318.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/7f9a7aa5-db80-4448-b987-1f9fdb88f5d1 [67406909 bytes], 001386.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2cf3a080-37d5-4133-b9d2-83e2ad49fc24 [67395104 bytes], 001411.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/66d3176b-2257-403a-be82-03c49162c05f [19605638 bytes], 001410.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/108bb1c1-99da-4920-acab-15f254326ed0 [11733963 bytes], 001387.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9027f480-b895-4336-80b9-dea244ce1572 [67397047 bytes], 001382.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/398f2188-6336-4efe-a014-6bd3bf2ce8c6 [67396271 bytes], 001415.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c73af6f6-18e2-424e-90f9-586809dfbba5 [1931553 bytes], 001402.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/f80462bb-eef1-4e3a-845c-97dea160e306 [67410281 bytes], 001313.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/88251386-e092-4a5e-b8bf-9c2e65148f5a [67408879 bytes]}, privateState={OPTIONS-000013=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b32ad6b5-4540-4b3e-9a82-14cebe231898', dataBytes=17286}, MANIFEST-000004=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c94b6317-b50b-4bf1-938d-83d7aad6ed6a [179781 bytes], CURRENT=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/3665ebc2-52ee-4a2e-87b6-6e0cc8458c37', dataBytes=16}}, metaStateHandle=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9af5edc4-c589-4f0b-91d7-515e068a3454', dataBytes=1187}, registered=false}, state size = 1573793130, cost 55319 ms.
2023-01-18 17:22:08,909 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation [] - Finished restoring from state handle: IncrementalRemoteKeyedStateHandle{backendIdentifier=b3e1d20f-164d-4c5b-aed2-91f5d1224183, stateHandleId=f404ffdb-715e-4f95-a850-f459639a30e6, keyGroupRange=KeyGroupRange{startKeyGroup=96, endKeyGroup=98}, checkpointId=125, sharedState={001388.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/bd7103e0-fe66-4400-a2a2-e4f3dda01b71 [51353250 bytes], 001383.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5e937d12-e1d5-4427-9401-80f5db6af2ee [67393725 bytes], 001314.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c2d374dc-88ba-4003-bad5-81590e56963d [67407704 bytes], 001403.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a7ec0f13-3ef1-4e69-a50e-071b9f6b092b [67411564 bytes], 001416.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9c82f81a-33d6-4b8f-a237-2150d0c311d8 [10391374 bytes], 001384.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b591513d-b850-4025-a126-452137b4a6fa [67397014 bytes], 001413.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5e2a0408-f9b4-4875-8594-2dc59df3bc66 [5307477 bytes], 001400.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/16bf5cda-4f04-40c9-8b54-ca1c393028f9 [67953551 bytes], 001316.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9543bc5c-dd9d-47b6-9626-159559b9ee45 [67406146 bytes], 001315.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b6099b46-bbef-486a-8c2f-ef232eefdeff [67409984 bytes], 001408.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/66354b67-97b2-41e8-ae48-678af553d2a7 [16861835 bytes], 001404.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a550c63b-003c-48d5-9bc2-45341ce4e641 [67413763 bytes], 001406.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/55822a0c-8448-48d2-a0c1-0bbccfaf31ca [67414722 bytes], 001317.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2b1f4dcc-76e2-4492-8470-bd512031a0ab [67407862 bytes], 001401.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/42a7b773-95d6-4a8b-bb18-64f083e9c627 [20184247 bytes], 001414.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/ec3a3d64-c0c1-4a5e-b541-dfc01a0f028d [19636706 bytes], 001385.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2ef14224-f229-4852-b97e-50268979f2a3 [67395048 bytes], 001399.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/a4550160-1c0c-4bb7-9eb9-26e2a6d9ed43 [67931628 bytes], 001381.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c6278757-9048-4e9d-a67d-5b9fd46c2c4f [67404982 bytes], 001368.sst=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/4497322c-c46b-4d42-9313-1f36d929c577', dataBytes=1354}, 001407.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/3308882e-bf0f-4f26-9f3a-3d3dfb05fc85 [67409775 bytes], 001405.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/5d5e1529-b109-483d-afa6-5d44297bec6d [67415504 bytes], 001318.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/7f9a7aa5-db80-4448-b987-1f9fdb88f5d1 [67406909 bytes], 001386.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/2cf3a080-37d5-4133-b9d2-83e2ad49fc24 [67395104 bytes], 001411.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/66d3176b-2257-403a-be82-03c49162c05f [19605638 bytes], 001410.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/108bb1c1-99da-4920-acab-15f254326ed0 [11733963 bytes], 001387.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9027f480-b895-4336-80b9-dea244ce1572 [67397047 bytes], 001382.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/398f2188-6336-4efe-a014-6bd3bf2ce8c6 [67396271 bytes], 001415.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c73af6f6-18e2-424e-90f9-586809dfbba5 [1931553 bytes], 001402.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/f80462bb-eef1-4e3a-845c-97dea160e306 [67410281 bytes], 001313.sst=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/88251386-e092-4a5e-b8bf-9c2e65148f5a [67408879 bytes]}, privateState={OPTIONS-000013=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/b32ad6b5-4540-4b3e-9a82-14cebe231898', dataBytes=17286}, MANIFEST-000004=File State: oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/c94b6317-b50b-4bf1-938d-83d7aad6ed6a [179781 bytes], CURRENT=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/3665ebc2-52ee-4a2e-87b6-6e0cc8458c37', dataBytes=16}}, metaStateHandle=ByteStreamStateHandle{handleName='oss://cluster-serving/flink-jobs/namespaces/state-test-default/deployments/1b1f8910-047f-4e51-a1bc-eea91e57600d/checkpoints/jobs/2daf1d9b-c9ed-40ec-b191-303db813b0de/2daf1d9bc9ed40ecb191303db813b0de/taskowned/9af5edc4-c589-4f0b-91d7-515e068a3454', dataBytes=1187}, registered=false} without rescaling.
2023-01-18 17:22:08,911 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation [] - restore rocksdb cost 55365 ms.
2023-01-18 17:22:08,912 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Finished building RocksDB keyed state-backend at /opt/flink/flink-tmp-dir/tm_job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31/tmp/job_2daf1d9bc9ed40ecb191303db813b0de_op_WindowOperator_0a448493b4782967b150582570326227__37_48__uuid_1404e597-c96c-4d7c-99b7-303fd98f80bd.
2023-01-18 17:22:08,915 [SlidingProcessingTimeWindows (37/48)#1] WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'ChangelogStateBackend.lastFullSizeOfMaterialization'. Metric will not be reported.[192.168.32.162, taskmanager, job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31, Flink Streaming Job, SlidingProcessingTimeWindows, 36]
2023-01-18 17:22:08,915 [SlidingProcessingTimeWindows (37/48)#1] WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'ChangelogStateBackend.lastIncSizeOfMaterialization'. Metric will not be reported.[192.168.32.162, taskmanager, job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31, Flink Streaming Job, SlidingProcessingTimeWindows, 36]
2023-01-18 17:22:08,915 [SlidingProcessingTimeWindows (37/48)#1] WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'ChangelogStateBackend.lastFullSizeOfNonMaterialization'. Metric will not be reported.[192.168.32.162, taskmanager, job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31, Flink Streaming Job, SlidingProcessingTimeWindows, 36]
2023-01-18 17:22:08,915 [SlidingProcessingTimeWindows (37/48)#1] WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'ChangelogStateBackend.lastIncSizeOfNonMaterialization'. Metric will not be reported.[192.168.32.162, taskmanager, job-2daf1d9b-c9ed-40ec-b191-303db813b0de-taskmanager-1-31, Flink Streaming Job, SlidingProcessingTimeWindows, 36]
2023-01-18 17:22:08,915 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.changelog.fs.FsStateChangelogStorage        [] - createWriter for operator WindowOperator_0a448493b4782967b150582570326227_(37/48)/KeyGroupRange{startKeyGroup=96, endKeyGroup=98}: 00000000-0000-0000-0000-000000000002
2023-01-18 17:22:08,915 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation [] - read changelog handle start, total state size=190851072 .
2023-01-18 17:22:08,919 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.fs.osshadoop.StsFetcherCredentialsProvider  [] - Old credential is going to expire. Fetch a new one.
2023-01-18 17:22:38,158 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation [] - read read changelog handle end, cost 29243 ms.
2023-01-18 17:22:38,158 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.common.PeriodicMaterializationManager [] - Task SlidingProcessingTimeWindows (37/48)#1 starts periodic materialization
2023-01-18 17:22:38,158 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.common.PeriodicMaterializationManager [] - Task SlidingProcessingTimeWindows (37/48)#1 schedules the next materialization in 82 seconds
2023-01-18 17:22:38,176 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - SlidingProcessingTimeWindows (37/48)#1 #1 (fa12cfa3b811a351e031b036b0e85d91) switched from INITIALIZING to RUNNING.
2023-01-18 17:22:39,057 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - snapshot of SlidingProcessingTimeWindows (37/48)#1 for checkpoint 11601, change range: 0..2, materialization ID 125
2023-01-18 17:22:43,779 [Source Data Fetcher for Source: KafkaWindowSource (37/48)#1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator [] - [Consumer clientId=xr_cl_1-36, groupId=xr_cl_1] Discovered group coordinator 192.168.47.158:9092 (id: 2147483546 rack: null)
2023-01-18 17:22:44,100 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - snapshot of SlidingProcessingTimeWindows (37/48)#1 for checkpoint 11602, change range: 0..11, materialization ID 125
2023-01-18 17:22:47,531 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - snapshot of SlidingProcessingTimeWindows (37/48)#1 for checkpoint 11603, change range: 0..17, materialization ID 125
2023-01-18 17:22:50,837 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - snapshot of SlidingProcessingTimeWindows (37/48)#1 for checkpoint 11604, change range: 0..21, materialization ID 125
2023-01-18 17:22:53,580 [SlidingProcessingTimeWindows (37/48)#1] INFO  org.apache.flink.state.changelog.ChangelogKeyedStateBackend  [] - snapshot of SlidingProcessingTimeWindows (37/48)#1 for checkpoint 11605, change range: 0..23, materialization ID 125 {code}
The above log can be simplified to the following scenario:
{code:java}
- cp1 trigger: file1,file1'(local)
- JM: register [file1] to sharedRegistry
- cp1 complete: stopTracking [file1], register [file1'] to localRegistry
- cp2 trigger: file1,file1',file2,file2'
- JM: register [file1,file2] to sharedRegistry
- cp2 complete: stopTracking [file1, file2], register [file1',file2'] to localRegistry
- cp1 subsume
- cp3 trigger: file1,file1',file2,file2',file3,file3'
- materialization: uploaded.clear()
- JM: register [file1,file2,file3] to sharedRegistry
- cp3 complete: stopTracking [file3], register [file3] to localRegistry
- cp2 subsume: [file1', file2'] are discarded
- if restore from cp3: local file1',file2' are not found{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/23 11:44;Yanfei Lei;tm-log_fail_cl_local_recovery.txt;https://issues.apache.org/jira/secure/attachment/13055934/tm-log_fail_cl_local_recovery.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 29 08:38:55 UTC 2023,,,,,,,,,,"0|z1fjow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 15:16;roman;Thanks for filing the ticket and opening the PR [~Yanfei Lei] 

Could you clarify my understanding?
 # The issue doesn't cause the data loss currently; but it might make local recovery impossible after checkpoint abortion
 # With the proposed fix, in case of many subsequent aborted checkpoints, local state accumulates. This is a degradation

Right?;;;","10/Feb/23 07:11;Yanfei Lei;[~roman] Thanks for your reply.
 # Yes, this issue might make local recovery fail after checkpoint abortion, and then the job would recovery from remote DFS. This issue doesn't cause data loss.
 # In case of many subsequent aborted checkpoints, all aborted local state will not be deleted until the next completed checkpoint. Right, this is a degradation in some case.  As [~xiarui] [suggested|https://github.com/apache/flink/pull/21822#issuecomment-1418605498] in PR, I'm going to use reference counting to decide when to delete a file.;;;","10/Feb/23 10:01;roman;Thanks [~Yanfei Lei], reference counting makes sense to me.;;;","01/Mar/23 12:25;Yanfei Lei;Looking back at the fileNotFound problem of local recovery again, I found that my previous analysis was incorrect:

For a checkpoint, notifyAbort() is impossible to come after notifyComplete() on TM.

If TM is materialized before receiving confirm(), the previously uploaded queue in `FsStateChangelogWriter` will be cleared, so the local files of the completed checkpoint will not be registered again, while the JM owned files are registered before confirm(), and do not depend on the uploaded queue, so the local files are deleted, and the DFS files are still there.

I added  `testLocalFileAfterMaterialize` to simulate this scenario, and I think local files should be registered before confirm() to avoid this problem. 
[~roman]  [~Feifan Wang] could you please take a look again?

 ;;;","13/Mar/23 11:39;assassinj;Hello Yanfei Lei, I found another situation that also triggers FileNotFoundException in local recovery mode, which has nothing to do with the materialization process.
The specific description is as follows:
When a checkpoint is completed, the sendAcknowledgeMessages method will be called to notify each ExecutionVertex that the checkpoint is completed, and finally FsStateChangelogWriter#confirm() is called. When there are two or more operators in one tm, one operator will accidentally delete the changelog local files of other operators when executing the confirm() method.

[~Yanfei Lei] Please help to confirm whether this issue is discussed in this issue, or I am raising a separate issue？Thanks;;;","13/Mar/23 11:39;assassinj;The above description can be simplified to the following scenario:
- cp1
    - filter ：f-file001
    - window ：w-file001
- localChangelogRegistry：f-file001，w-file001

- cp2
    - filter ：f-file001 
        - rum confirm（）first，and delete w-file001
    - window ：w-file002
- localChangelogRegistry：f-file001，w-file002

- if restore from cp2: local w-file001 are not found;;;","13/Mar/23 13:01;Yanfei Lei;[~assassinj] Thanks for sharing a new situation, I think it's possible, because the lastUsedCheckpointID of `w-file001`  will not be updated until the window operator confirm(), so it's deleted when the filter operator confirm().

I think the current PR can solve this problem, WDYT?

 ;;;","14/Mar/23 03:15;assassinj;[~Yanfei Lei]  Thanks for your reply.

I don't think the current pr will fix this, and I verified that it does.
The reason is that localChangelogRegistry in [ChangelogKeyedStateBackend.java|https://github.com/apache/flink/pull/21822/files/b782fef3ed194d098d1f7fe4cc45355d2b95f1f4#diff-e071e8a89527c24be4ee5ee342ad7d47c870170ef915d1407d18e998f7847f16] is a new object.

For this problem, I think of two other possible solutions:
Method 1. Change the logic of LocalChangelogRegistryImpl and add a mapping relationship between operator and file. When confirm() cleans up, only the files of its own operator are cleaned up;
Method 2. In LocalChangelogRegistryImpl#discardUpToCheckpoint(), change entry.f1 < upTo to entry.f1 < upTo -1.

Compare the two solutions: method 1 has relatively large changes; method 2 is a trick method with small changes, but it is not easy to understand. At the same time, there will be one more checkpoint file storage occupation. WDYT?;;;","14/Mar/23 07:23;Feifan Wang;Hi [~Yanfei Lei] , sorry for the late reply, I agree to register before confirm() to avoid the both problems mentioned by you and [~assassinj] . Further, I think we should do the registration operation in persist(), because that's where the reference management is originally generated. To achieve this, I suggest pass checkpoint id to persist method. 

WDYT [~roman] ?;;;","16/Mar/23 06:36;Yanfei Lei;[~assassinj] Thanks for your investigation and solutions, you're right, I overlooked that every subtask has its own `localChangelogRegistry`. Can adjusting the scope of localChangelogRegistry to TM solve this problem?

For two other possible solutions:

I prefer method 1 and I think changing the scope of localChangelogRegistry based on the current PR can achieve the same effect.

Why does method-2 work? I guess it's because the next checkpoint won't start until all subtasks of this checkpoint are confirmed, it's really not easy to understand as you mentioned😂.

 

[~Feifan Wang] Thanks for your reply and suggestion, I'll try to refactor the registration to persist().;;;","20/Apr/23 03:34;Feifan Wang;There are two known issues with dstl file cleanup:
 # local dstl files still in use be deleted (as described earlier in the current ticket)
 # remote dstl files cannot be cleaned up when checkpoint cannot be completed, even if materialization has completed

According to Tang Yun's reivew feedback, in order to avoid introducing half-baked solutions, I would like to initiate a discussion on dstl file cleanup. 

*Here is my proposal :*

divide the deletion of dstl files into two categories :
 # *delete by subsumed checkpoint*
 ## remote files : delete by JobManager , nothing to do with TaskManager (keep status quo)
 ## local files : delete by LocalChangelogRegistry. Local checkpoint only used for job failover, so only retain one completed checkpoint. I suggest : Register the latest checkpointId referencing the dstl file to the LocalChangelogRegistry, regardless of whether the checkpoint is completed or not. When the checkpoint is complete, call LocalChangelogRegistry#discardUpToCheckpoint() method to delete the local file whose checkpointId is less than the completed checkpontId. In this way, the LocalChangelogRegistry can ensure that the latest completed local checkpoint is available, and local files only referenced by non-latest checkpoints can be deleted in time. (this PR does exactly that)
 # *delete by not used StateChangeSet*
The ""not used StateChangeSet"" refers to those StateChangelogSet that are not used by any checkpoint, or the checkpoint that uses it has aborted, and the StateChangeSet has been materialized. In this case, remote files and local files should be handled uniformly, and both are handled by TaskChangelogRegistry. We can introduce a ""StateChangeUsageTracker"" to find out those not used StateChangeSets based on checkpoint execution and materialization execution, and then notify TaskChangelogRegistry to release the reference count of the corresponding file.

In the method described above, the LocalChangelogRegistry is only responsible for the deletion of local files when checkpoint subsumed, just as implemented in this PR. Since each TaskManager only stores its own checkpoint files locally, and these files will be cleaned up when the TaskManager exits, I think the problem with local dstl files accumulation is milder than remote dstl files. So I suggest do the above modification in two steps :
 # in this ticket, make LocalChangelogRegistry only delete local files when checkpoint completed. (just as the PR do)
 # open a new ticket to address deletion of not used both remote and local files.

[~Yanfei Lei] , [~yunta] , [~roman] , [~pnowojski], [~yuanmei]  WDYT ?;;;","20/Apr/23 03:57;Yanfei Lei;[~Feifan Wang] Thanks for your proposal!  “2. delete by not used StateChangeSet” can partially solve the problem of dstl files accumulating without any checkpoint completing, but it depends on at least one materialization that can be completed. This is better than doing nothing on abort(), +1 for opening a new ticket to address deletion of not used both remote and local files.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","14/Aug/23 02:11;Feifan Wang;Since [~Yanfei Lei]  is still working on the ticket, I removed the 'stale-assigned' tag.;;;","29/Aug/23 08:38;Yanfei Lei;Merged via c23a3002b837ecffa2d561a405c17c979c814c6e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config doc generation should keep @deprecated ConfigOption,FLINK-30862,13522442,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jingge,jingge,jingge,01/Feb/23 09:24,04/Nov/23 17:00,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Currently the content will be removed once the ConfigOption is marked as @deprecated. The content should be kept and marked with a deprecated flag, since the ConfigOption is only deprecated and still be used. The content should be only removed when the ConfigOption has been removed.

 

If we just remove the fresh deprecated option in the document, user will be confused and think the option is gone and does not work anymore, which means the backward compatibility is broken.

 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33240,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 13:44:44 UTC 2023,,,,,,,,,,"0|z1fjmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 11:04;chesnay;I disagree.

There are 2 possible scenarios:
a) an option is deprecated because the related features was removed, and is only kept for source compatibility. There's no point to document this option.
b) an option _key_ is deprecated, being subsumed by a new key. There are certain option keys that go back half a decade;the config page is overloaded as-is, so adding more just makes things even worse

I will say that there should be better documentation for _migrating_ from deprecated options, but this to me is quite different than unilaterally showing all deprecated options in the docs.;;;","01/Feb/23 11:15;jingge;Fair enough. Thanks for the info. The question is why we didn't clean up and remove all deprecated options regularly, like e.g. after one/two release cycles?

 

Btw. NIT, I didn't make the description clear enough. The content of deprecated ConfigOption will be generated with a ""deprecated"" flag. I will update the description. 

 ;;;","01/Feb/23 12:34;dwysakowicz;I second [~chesnay]. I don't think we should document deprecated options as they should not be used.

Listing deprecated options gives an impression it can still be used.;;;","01/Feb/23 12:38;dwysakowicz;An alternative, that've just popped in my mind is to have a separate section/page with all deprecated options.;;;","01/Feb/23 13:44;chesnay;??The question is why we didn't clean up and remove all deprecated options regularly, like e.g. after one/two release cycles???

If you are referring to a), then source/binary compatibility.
If you are referring to b), then because there's usually no immediate gain for us (or users for that matter).

??An alternative, that've just popped in my mind is to have a separate section/page with all deprecated options.???

Yes, but such a page is imo only useful if you can attach more information to it, like ""this option is deprecated in favor of X"" or ""this option has no longer an effect"".
This isn't as trivial as documenting new options because many of these deprecated keys are defined in the {{ConfigConstants}}, i.e. don't use {{ConfigOption}} at all.

I do wonder though who would look at a page purely about deprecated options.
That's not really what users are searching for, right?
Rather they want to see a migration page, but isn't that what the release notes are for (which FYI aren't even linked anywhere in the docs and only accessible via the search).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive Catalog throws java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList under certain environment,FLINK-30861,13522413,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,01/Feb/23 07:16,01/Feb/23 08:05,04/Jun/24 20:41,01/Feb/23 08:05,table-store-0.3.1,table-store-0.4.0,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Table Store Hive Catalog throws {{java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList}} under certain environment. We need to package {{hive-storage-api}} dependency.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 08:05:36 UTC 2023,,,,,,,,,,"0|z1fjg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 08:05;TsReaper;master: 5e09dfec910c2aa2dfcf3eca4adcb8191524bf04
release-0.3: 78fdd09077431a7284cd8519cca5ae06c7acb60e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for hybrid shuffle with adaptive batch scheduler,FLINK-30860,13522406,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,01/Feb/23 06:42,14/Feb/23 05:02,04/Jun/24 20:41,14/Feb/23 03:25,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,Add document for hybrid shuffle with adaptive batch scheduler.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 03:24:53 UTC 2023,,,,,,,,,,"0|z1fjeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 03:24;Weijie Guo;master(1.18) via 5ceda3da20ec5ed50849d89a629744fe9d4010d8

release-1.17 via 58ca4d49c772bcdc73036a5efe0b6b0039ea75e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-connector-kafka from master branch,FLINK-30859,13522401,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,mason6345,mason6345,01/Feb/23 06:16,04/Oct/23 23:14,04/Jun/24 20:41,27/Jun/23 06:37,1.18.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,pull-request-available,,,,Remove flink-connector-kafka from master branch since the repo has now been externalized and 1.17 commits have been sync'ed.,,,,,,,,,,,,FLINK-31731,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 06:37:17 UTC 2023,,,,,,,,,,"0|z1fjdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 06:14;mason6345;[~martijnvisser] [~tzulitai] [~thw@apache.org] I took at stab at this a while back and it seems that there are still a few components that still rely on the flink-connector-kafka.

I'll try to enumerate them here:
 * confluent-schema-registry-end-to-end-tests
 * flink-examples
 * flink-examples-build-helper

The other dependencies can be deleted due to the externalization.

I think we can consider externalizing the confluent-schema-registry and confluent-schema-registry-end-to-end-tests since they are closely related to Kafka.

However, I'm unsure about the examples since users refer to that. If we want to keep it in the repo, then we need a release of the flink-connector-kafka–I think we left the earlier release candidate to be expired.

These are just my initial thoughts and I'm happy to start a discussion in the mailing list if required. Thanks!;;;","23/Mar/23 13:49;thw;The examples would lead to a chicken and egg problem: We would first need to release core, then the connector based on that core release, then the examples based on the connector release. That seems to suggest that the Kafka example should also live in the Kafka connector repo?;;;","23/Mar/23 14:24;chesnay;??the Kafka example should also live in the Kafka connector repo??

yes.;;;","23/Mar/23 17:31;tzulitai;Thanks for the summary [~mason6345]!

+1 to separating out the Flink examples that use Kafka and move that to apache/flink-connector-kafka

> I think we can consider externalizing the confluent-schema-registry and confluent-schema-registry-end-to-end-tests since they are closely related to Kafka.

Yes to this as well.;;;","23/Mar/23 17:32;tzulitai;[~mason6345] following the discussion in [https://github.com/apache/flink/pull/22150,] do you think you can tackle this ticket first? That would resolve the confusion with the correct merge procedure.

If you currently don't have free cycles to do that soon, I'll be happy to work on removing the code in apache/flink:main as well. Let me know :);;;","23/Mar/23 17:45;mason6345;[~tzulitai] I'm happy to help out on this in parallel, will take a stab tonight. Due to the nature of the bug, I feel that it is best to address and merge it immediately and I can also take up the task to sync it to flink-connector-kafka:main (I also see that other commits have been merged there).;;;","23/Mar/23 17:58;tzulitai;[~mason6345] yeh for FLINK-31305 we don't have to block it on the code removal in apache/flink:main

I'll try to do a review pass and merge that already before tomorrow.

After I merge that, there will be conflicts with your parallel work on removing code, but that should be easily resolvable.;;;","24/Mar/23 14:25;tzulitai;Reassigned to [~mason6345] as we agreed that he will be doing most of the work for removing the code in apache/flink:main;;;","27/Jun/23 06:37;tzulitai;Merged via apache/flink:149a5e34c1ed8d8943c901a98c65c70693915811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator does not update reconciled generation,FLINK-30858,13522370,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,thw,thw,01/Feb/23 00:50,04/Feb/23 02:10,04/Jun/24 20:41,04/Feb/23 02:10,kubernetes-operator-1.3.1,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Kubernetes manages the generation field as part of the spec metadata. It will be increased when changes are made to the resource. The counterpart in status is ""observed generation"", provided by a controller. By comparing the two, the client can determine that the controller has processed the spec and in conjunction with other status information conclude that a change has been reconciled.

The Flink operator currently tracks the generation as part of reconciled and stable specs but these cannot be used as ""observed generation"" to perform the check. The value isn't updated in cases where operator determines that there are no changes to the spec that require deployment. This can be reproduced through PUT/replace with the same spec or a change in upgrade mode.

The operator should provide the observed spec, which in conjunction with deployment state can then be used by clients to determine that the spec has been reconciled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 04 02:10:48 UTC 2023,,,,,,,,,,"0|z1fj74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 01:08;thw;https://lists.apache.org/thread/8y1zp4ogssy8ltsl42ppzvbo64dlzc3v;;;","01/Feb/23 08:09;gyfora;I discussed this with Max yesterday, and it should also be possible to use the current generation metadata in the status to track the last observed generation with a slight change to the reconciler logic.

I just want to point out that the original intention with the generation field was to be able to match deployed (jobmanager generation label) with the last reconciled spec for resiliency to failures during deploy operations. This is not currently intended as a user facing information.

With this change however it would be easier to is for users, I agree :);;;","04/Feb/23 02:10;mbalassi;b45122c in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create table does not create topic with multiple partitions,FLINK-30857,13522296,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,vicky_papavas,vicky_papavas,31/Jan/23 20:01,29/Mar/23 02:04,04/Jun/24 20:41,29/Mar/23 02:04,,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,," 
{code:java}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store'
 );
USE CATALOG table_store_catalog;
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count_kafka (
     word STRING PRIMARY KEY NOT ENFORCED,
     cnt BIGINT
 ) WITH (
     'log.system' = 'kafka',
     'kafka.bootstrap.servers' = 'broker:9092',
     'kafka.topic' = 'word_count_log',
     'bucket'='4'
 );
{code}
 

The created topic has only one partition
{code:java}
Topic: word_count_log    TopicId: udeJwBIkRsSybkf1EerphA    PartitionCount: 1    ReplicationFactor: 1    Configs:
    Topic: word_count_log    Partition: 0    Leader: 1    Replicas: 1    Isr: 1{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 02:04:33 UTC 2023,,,,,,,,,,"0|z1fiqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 08:48;paul8263;Hi [~vicky_papavas] ,

Could you please tell me what version of Kafka are you currently using? Thanks.;;;","07/Feb/23 08:30;paul8263;Hi all,

I am currently investigating this issue.

It seems that the onCreateTable method in flink-table-store-kafka/src/main/java/org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java invokes AdminClient of Kafka to create topic with number of partitions assigned. The relevant codes are listed below:
{code:java}
@Override
public void onCreateTable(Context context, int numBucket, boolean ignoreIfExists) {
    Configuration options = Configuration.fromMap(context.getCatalogTable().getOptions());
    try (AdminClient adminClient = AdminClient.create(toKafkaProperties(options))) {
        Map<String, String> configs = new HashMap<>();
        options.getOptional(LOG_RETENTION)
                .ifPresent(
                        retention ->
                                configs.put(
                                        TopicConfig.RETENTION_MS_CONFIG,
                                        String.valueOf(retention.toMillis())));
        NewTopic topicObj =
                new NewTopic(topic(context), Optional.of(numBucket), Optional.empty())
                        .configs(configs);
        adminClient.createTopics(Collections.singleton(topicObj)).all().get();

// ...

} {code}
However, onCreateTable method never has the chance to be executed, as Flink table store catalog(implemented by flink-table-store-connector/src/main/java/org/apache/flink/table/store/connector/FlinkCatalog.java) does not support managed table (it does not override thesupportsManagedTable method in org/apache/flink/table/catalog/Catalog.java).

I tried to make it support managed table and updated the sql as below:
{code:java}
                String.format(
                        ""CREATE TABLE T (a STRING, b STRING, c STRING) WITH (""
                                + ""'log.system'='kafka', ""
                                + ""'root-path'='/path/to/tablestore-data',""
                                + ""'kafka.bootstrap.servers'='%s',""
                                + ""'kafka.transaction.timeout.ms'='300000',""
                                + ""'table.type'='MANAGED_TABLE',""
                                + ""'bucket'='9'""
                                + "")"",
                        ""kafka1:9092:kafka2:9092,kafka3:9092"", ""flink-demo"")); {code}
(As managed table does not allow customizing kafka topic so kafka.topic option was removed. )

Now the onCreateTable method is able to be invoked but I got another exception:
{code:java}
Creating topics with default partitions/replication factor are only supported in CreateTopicRequest version 4+ {code}
It says default values of partitions or replication factor are not supported while creating new topic. I made another change:

From: 
{code:java}
new NewTopic(topic(context), Optional.of(numBucket), Optional.empty()) .configs(configs); {code}
To:
{code:java}
// Optional.of((short) 3) is just a value for test purpose only
 NewTopic topicObj = new NewTopic(topic(context), Optional.of(numBucket), Optional.of((short) 3))
                            .configs(configs);{code}
Finally the topic with correct partitions and replication factor was successfully created.

 

I think there are 2 questions need some further discussion:
 # Should FlinkCatalog support managed table? Even if it is working as supporting external tables, the Kafka topic also has to be created via AdminClient.
 # Creating new topic with default partitions/replication factor by AdminClient seems to have limited support. We have to assign default partitions/replication factor in Flink configuration or SQL with clause.

 ;;;","07/Feb/23 18:57;vicky_papavas;Hi [~paul8263] ! 

Thank you for looking into this.

I am using AK 3.2.0

 ;;;","09/Feb/23 08:42;paul8263;Hi community,

After further investigation, I think FlinkCatalog should support managed table as it does not have a connector option. However org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java does not support customized topic. The relevant codes are:
{code:java}
@Override
public Map<String, String> enrichOptions(Context context) {
    Map<String, String> options = new HashMap<>(context.getCatalogTable().getOptions());
    Preconditions.checkArgument(
            !options.containsKey(TOPIC.key()),
            ""Managed table can not contain custom topic. ""
                    + ""You need to remove topic in table options or session config."");

    String topic = context.getObjectIdentifier().asSummaryString();
    options.put(TOPIC.key(), topic);
    return options;
} {code}
I suggest it should support customized topic name. If the topic was not specified, it could be context.getObjectIdentifier().asSummaryString() instead.

As the issue of Kafka AdminClient, we might add default replication factor for better compatibility.

Correct me if I am wrong.

 

 

 ;;;","13/Feb/23 02:59;paul8263;Hi [~lzljs3620320] ,

As the latest hotfix removed managed table related method in LogStoreFactoryTable, it seems that it is not planned to support managed table in the future. Should we support creating Kafka topic explicitly with specified replication factor/partitions?;;;","29/Mar/23 02:04;lzljs3620320;Kafka topic should be created by users..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator - allow specifying job name,FLINK-30856,13522252,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,VinceChenal,VinceChenal,31/Jan/23 15:35,31/Jan/23 16:24,04/Jun/24 20:41,31/Jan/23 16:24,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"It seems like it's not possible to define a job name in JobSpec.
Instead we always get ""Flink Streaming Job"" (cf. Attachment)

It would be nice to have the possibility to name a job.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 15:34;VinceChenal;flink_job_name.png;https://issues.apache.org/jira/secure/attachment/13054951/flink_job_name.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 16:24:57 UTC 2023,,,,,,,,,,"0|z1fih4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 16:24;gyfora;This is intentional. You can use `pipeline.name` flink configuration easily for this purpose.
However keep in mind that the name may be override programatically from the application;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-connector-gcp-pubsub from master branch,FLINK-30855,13522241,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,31/Jan/23 14:51,02/Feb/23 12:20,04/Jun/24 20:41,02/Feb/23 12:20,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,,0,pull-request-available,,,,"With Google PubSub being synchronized to https://github.com/apache/flink-connector-gcp-pubsub we can now remove the GCP PubSub connector from master so it won't be included in the 1.17 release anymore

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 12:20:38 UTC 2023,,,,,,,,,,"0|z1fieo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 12:20;martijnvisser;Fixed in master: 30dd8178dd0f010fe6c6e675fd5dfe992232d0ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose periodic compaction to RocksdbCompactFilterCleanupStrategy,FLINK-30854,13522239,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,31/Jan/23 14:49,25/Aug/23 02:38,04/Jun/24 20:41,25/Aug/23 02:38,,,,,,,1.19.0,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,stale-assigned,,,"In 6.20.3-ververica-2.0 rocksdb version flink uses from 1.17[1], we introduce periodic compaction option[2].

it deserves to expose this into RocksdbCompactFilterCleanupStrategy or some confs to make users use it conveniently.

cc [~yunta] 

[1] [https://github.com/apache/flink/pull/21747]
[2] https://github.com/ververica/frocksdb/pull/57",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30457,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 02:38:06 UTC 2023,,,,,,,,,,"0|z1fie8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 02:56;yunta;I think this is a valid ticket, already assigned to you.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","25/Aug/23 02:38;masteryhx;merge [435ae4ff|https://github.com/apache/flink/commit/435ae4fff269589954eadc758eb1429d76e56bfe] and [be3e75ef|https://github.com/apache/flink/commit/be3e75ef9bf8bca8ff409b9a1a85097207b973cf] into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make clear the public API for flink-avro,FLINK-30853,13522234,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liangtl,liangtl,31/Jan/23 14:25,31/Jan/23 14:25,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,,"flink-avro provides users with methods to register the Apache Avro format in their connectors. However, it is currently not clear which classes are @Public, @PublicEvolving and @Internal. 

 

We want to make clear the classes / interfaces that are @Public or @PublicEvolving so that users can know whether the API is expected to be stable.

 

For classes that are already indirectly exposed as @Public or @PublicEvolving, we should ensure that they are not marked as anything lower (e.g. @Internal)

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 14:25:15.0,,,,,,,,,,"0|z1fid4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTest.testCleanupWhenSwitchToInitializationFails reports AssertionError but doesn't fail,FLINK-30852,13522217,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalash,mapohl,mapohl,31/Jan/23 12:00,21/Apr/23 11:50,04/Jun/24 20:41,21/Apr/23 11:50,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,test-stability,,,"While investigating FLINK-30844, I noticed that {{TaskTest.testCleanup}} reports an AssertionError in the logs but doesn't fail:
{code}
00:59:01,886 [                main] ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Error while canceling task Test Task (1/1)#0.
java.lang.AssertionError: This should not be called
        at org.junit.Assert.fail(Assert.java:89) ~[junit-4.13.2.jar:4.13.2]
        at org.apache.flink.runtime.taskmanager.TaskTest$TestInvokableCorrect.cancel(TaskTest.java:1304) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1529) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:796) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.TaskTest.testCleanupWhenSwitchToInitializationFails(TaskTest.java:184) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
[...]
{code}

[~akalashnikov] is this expected?

The affected build is https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30844,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 11:50:25 UTC 2023,,,,,,,,,,"0|z1fi9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 14:49;akalashnikov;Oh, It is not the correct test. It actually expects that this method would be called. I will change that.

Thanks for the report.;;;","14/Apr/23 13:57;akalash;merged to master: a6f0165c;;;","20/Apr/23 13:35;mapohl;The issue's version is misleading. It appears that you've merged the change into {{master}} (which means that it was fixed in 1.18.0). I'm gonna update the fixVersion accordingly. May you also provide a 1.17 backport? ;;;","20/Apr/23 14:06;akalash;Yeh, sorry, I missed up with versions. I will backport it to 1.17 shortly.;;;","21/Apr/23 11:50;akalash;merged to release-1.17: aa47c3f862414511e92637d9816f6908c86b4cf6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator - operator image should provide s3 support for jarUri,FLINK-30851,13522212,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,VinceChenal,VinceChenal,31/Jan/23 11:11,31/Aug/23 06:41,04/Jun/24 20:41,31/Aug/23 06:41,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"The docker image does not embed filesystem plugins.
Neither in plugins/ neither in opt/ folders.

I'd like to use jarUri this way:
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: myjob
spec:
  deploymentName: mydeployment
  job:
    jarURI: s3://somebucket/myjar.jar {code}
But I'm getting this error:
{code:java}
Could not find a file system implementation for scheme 's3'. The scheme is directly supported by Flink through the following plugins: flink-s3-fs-hadoop, flink-s3-fs-presto. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://ci.apache.org/projects/flink/flink-docs-stable/ops/plugins.html for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/. {code}

I was able to make it work by putting flink 1.16 s3-presto-fs jar within the flink-kubernetes-operator image but it would make sense to have it natively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 15:02:29 UTC 2023,,,,,,,,,,"0|z1fi88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 11:17;gaborgsomogyi;If we add S3 then we must add all other available plugins + connectors. How do you plan to decide what should be added and what not?
As a first glance I think it would be better to use the existing image for custom docker creation where any plugin can be added.;;;","31/Jan/23 11:48;VinceChenal;I agree that if we add s3 why not adding other ones as well.. I guess it's not for me to decide

What do you mean by "" I think it would be better to use the existing image for custom docker creation"" ?

Right now this is what I do but it does not feel right to have to copy the jar from another image
{code:java}
FROM apache/flink:1.16.0-scala_2.12-java11 AS builder

FROM apache/flink-kubernetes-operator:1.3.1

RUN mkdir -p /opt/flink/plugins/s3-fs-presto
COPY --from=builder /opt/flink/plugins/s3-fs-presto/* /opt/flink/plugins/s3-fs-presto/{code}

With flink images, as explained in the doc we must copy plugins from opt/ folder to plugins/ folder but in flink-kubernetes-operator image there's no opt/ folder;;;","31/Jan/23 13:01;gaborgsomogyi;https://mvnrepository.com/artifact/org.apache.flink/flink-s3-fs-presto;;;","31/Jan/23 13:24;fabiowanner;Thanks a lot for opening this ticket! We did not think about using the S3 plugin but went with using presigned S3 URLs instead but ran into this problem: FLINK-30845. But of course if you only have one S3 endpoint the plugin makes much sense!;;;","31/Jan/23 15:02;VinceChenal;I said it worked but actually it's not working anymore with operator version 1.3.1 and {color:#000000}flink-s3-fs-presto-1.16.0.jar{color}
I get this errors:
{code:java}
2023-01-31 14:26:47,052 c.a.m.CsmConfigurationProviderChain [DEBUG][newflink/newflink-job] Unable to load configuration from com.amazonaws.monitoring.ProfileCsmConfigurationProvider@6c96d3c0: Unable to load config file
Exception in thread ""pool-5-thread-9"" java.lang.NoClassDefFoundError: org/apache/flink/core/fs/RefCountedTmpFileCreator
        at org.apache.flink.fs.s3.common.FlinkS3FileSystem.<init>(FlinkS3FileSystem.java:100)
        at org.apache.flink.fs.s3presto.FlinkS3PrestoFileSystem.<init>(FlinkS3PrestoFileSystem.java:51)
        at org.apache.flink.fs.s3presto.S3FileSystemFactory.createFlinkFileSystem(S3FileSystemFactory.java:76)
        at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:155)
        at org.apache.flink.core.fs.PluginFileSystemFactory.create(PluginFileSystemFactory.java:62)
        at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508)
        at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
        at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274)
        at org.apache.flink.kubernetes.operator.artifact.FileSystemBasedArtifactFetcher.fetch(FileSystemBasedArtifactFetcher.java:41)
        at org.apache.flink.kubernetes.operator.artifact.ArtifactManager.fetch(ArtifactManager.java:63)
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.uploadJar(AbstractFlinkService.java:711)
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.submitJobToSessionCluster(AbstractFlinkService.java:197)
        at org.apache.flink.kubernetes.operator.reconciler.sessionjob.SessionJobReconciler.deploy(SessionJobReconciler.java:107)
        at org.apache.flink.kubernetes.operator.reconciler.sessionjob.SessionJobReconciler.deploy(SessionJobReconciler.java:44)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:116)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:65)
        at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:99)
        at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:51)
        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:136)
        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:94)
        at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
        at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:93)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:130)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:110)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:81)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:54)
        at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source) {code}
It was working with operator version 1.2.0 and {color:#000000}flink-s3-fs-presto-1.15.2.jar{color}

So I'm afraid there could incompatibilities between filesystems plugins and operator, what would be the best approach?

EDIT: I just tried to rebuild operator 1.3.1 with {color:#000000}flink-s3-fs-presto-1.15.2.jar{color} and it's working;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update GCP E2E tests to updated setup so they no longer publish their JARs,FLINK-30850,13522209,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,31/Jan/23 11:04,03/Nov/23 10:59,04/Jun/24 20:41,27/Jun/23 18:31,gcp-pubsub-3.0.0,,,,,,gcp-pubsub-3.0.2,gcp-pubsub-3.1.0,,,,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 18:31:56 UTC 2023,,,,,,,,,,"0|z1fi7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 18:31;martijnvisser;Fixed in:

apache/flink-connector-gcp-pubsub:main - d7ed02b76be15afdff0f9c8fc0805bc30ce6c559
apache/flink-connector-gcp-pubsub:v3.0 - 262bf05db456bfff22094eeeb9ae3fa903d9127e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
udaf validated failed with TableEnvironment#executeSql but work correctly with StreamTableEnrivorment#registerFunction,FLINK-30849,13522200,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,lam167,lam167,31/Jan/23 09:54,31/Jan/23 11:18,04/Jun/24 20:41,31/Jan/23 11:18,,,,,,,,,,,,,,,,,,,,,,,API / Core,Table SQL / API,,,,0,,,,,"We have a udaf which has some overloaded methods and it can work in flink 1.12.2 with deprecated api StreamTableEnrivorment#registerFunction, but when use TableEnvironment#executeSql in flink 1.12.2 or flink 1.16, it will throw an exception as follows:
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Considering all hints, the method should comply with the signature:
accumulate(_, java.lang.String, java.lang.Object, java.lang.Object)
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:333)
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:328)
    at org.apache.flink.table.types.extraction.FunctionMappingExtractor.createMethodNotFoundError(FunctionMappingExtractor.java:535) {code}
In flink 1.16, the method
<T, ACC> void registerFunction(String name, AggregateFunction<T, ACC> aggregateFunction); does noe exist anymore, and I want to how to rewrite the udaf to make it works.
The test code is as follows: 
{code:java}
TableEnvironment tableEnvironment = StreamTableEnvironment.create(StreamExecutionEnvironment.getExecutionEnvironment());
//        tableEnvironment.registerFunction(""MultiAggToJsonArrayV2"", new MultiAggToJsonArrayV2());
        tableEnvironment.executeSql(""CREATE FUNCTION `MultiAggToJsonArrayV2` AS 'com.sankuai.flink.streaming.udf.MultiAggToJsonArrayV2'"");
        tableEnvironment.executeSql(""CREATE TABLE `grocery_udf_test`(`a` VARCHAR,`b` INTEGER,`c` VARCHAR,`d` VARCHAR) WITH "" +
                ""('connector'='datagen')\n"");
        tableEnvironment.executeSql(""CREATE TABLE `grocery_udf_test_sink`(`a` VARCHAR,`res` VARCHAR) WITH ('connector'='blackhole')\n"" );
        tableEnvironment.executeSql(""INSERT INTO `grocery_udf_test_sink` SELECT `a`, `MultiAggToJsonArrayV2`('b', '', '', '') AS `res` FROM `grocery_udf_test` GROUP BY `a`""); {code}
[^MultiAggToJsonArrayV2.java]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 09:51;lam167;MultiAggToJsonArrayV2.java;https://issues.apache.org/jira/secure/attachment/13054933/MultiAggToJsonArrayV2.java",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 09:54:59.0,,,,,,,,,,"0|z1fi5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable CatalogTableITCase#testAlterTable,FLINK-30848,13522191,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,31/Jan/23 08:41,01/Feb/23 02:22,04/Jun/24 20:41,01/Feb/23 02:22,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"This test has been ignored since FLINK-22315 because, at that moment, the `CREATE TABLE` and `ALTER TABLE`statements are creating deprecated CatalogTableImpl and DefaultCatalogTable interchangeably,  which causes CI failure [https://dev.azure.com/qingyuecqy/Flink/_build/results?buildId=102&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f]

 

While after resolving FLINK-29679, this test should be enabled again.",,,,,,,,,,,,,,,,,,FLINK-29679,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 02:22:23 UTC 2023,,,,,,,,,,"0|z1fi3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 02:22;fsk119;Merged into master: 009a9e5ce7dc967cb3a1e7f5d997835707e05ccd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some rel rules violate Calcite guidance rule about instance creation,FLINK-30847,13522190,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Sergey Nuyanzin,Sergey Nuyanzin,31/Jan/23 08:38,31/Jan/23 08:38,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"This is a follow up ticket based on review [https://github.com/apache/flink/pull/21519#pullrequestreview-1267744410]

 

Calcite guidance rule 5 says that (https://calcite.apache.org/javadocAggregate/org/apache/calcite/plan/RelRule.html)
{quote}5. Do not create an INSTANCE constant inside your rule. Instead, create a named instance of your rule, with default configuration, in a holder class. The holder class must not be a sub-class of RelOptRule (otherwise cyclic class-loading issues may arise). Generally it will be called XxxRules, for example CsvRules. The rule instance is named after your rule, and is based on the default config (Config.DEFAULT, or DEFAULT_CONFIG for converter rules):
{code:java}
     /** Rule that matches a {@code Project} on a
      * {@code CsvTableScan} and pushes down projects if possible. */
     public static final CsvProjectTableScanRule PROJECT_SCAN =
         CsvProjectTableScanRule.Config.DEFAULT.toRule();
{code}
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 08:38:07.0,,,,,,,,,,"0|z1fi3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource fails,FLINK-30846,13522183,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,mapohl,mapohl,31/Jan/23 07:57,07/Aug/23 09:46,04/Jun/24 20:41,04/Feb/23 02:13,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"{{SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource}} is timing out
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8599
{code}
Jan 31 02:02:28 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007fcf74f2b800 nid=0x5476 waiting on condition [0x00007fce2b078000]
Jan 31 02:02:28    java.lang.Thread.State: WAITING (parking)
Jan 31 02:02:28 	at sun.misc.Unsafe.park(Native Method)
Jan 31 02:02:28 	- parking to wait for  <0x00000000a22933e0> (a java.util.concurrent.CompletableFuture$Signaller)
Jan 31 02:02:28 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 31 02:02:28 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 31 02:02:28 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:216)
Jan 31 02:02:28 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:162)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,FLINK-32768,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 07:17:22 UTC 2023,,,,,,,,,,"0|z1fi1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 06:07;wanglijie;If we don't set {{jobmangaer.scheduler}} manually and set the system properties {{flink.tests.enable-adaptive-scheduler}}, it will use the adaptive scheduler for streaming jobs and use the default scheduler for batch jobs, this ticket occured in this scenario.;;;","01/Feb/23 06:10;wanglijie; In FLINK-30683, we removed the logic of manually configuring the adaptive batch scheduler, which led to this test running with default scheduler (we expected this test must run with adaptive batch scheduler).;;;","01/Feb/23 06:15;wanglijie;[~JunRuiLi] Would you take a look?;;;","01/Feb/23 06:20;JunRuiLi;[~wanglijie] This issue will not be a block to production availability, it will only appear in the test. I will fix it immediately, thanks!;;;","01/Feb/23 09:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=10711;;;","01/Feb/23 11:15;mapohl;I'm lowering the priority of this issue to Critical based on [Junrui's comment|https://issues.apache.org/jira/browse/FLINK-30846?focusedCommentId=17682866&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17682866] that it's only a test-related issue and doesn't indicate a bug in production.;;;","02/Feb/23 08:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8899;;;","03/Feb/23 06:09;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8893] ;;;","04/Feb/23 02:13;zhuzh;Fixed via b1e70aebd3e248d68cf41a43db385ec9c9b6235a;;;","06/Feb/23 07:17;mapohl;The following failed build didn't include the above mentioned fix, yet: 
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Params in jarURI end up in file name,FLINK-30845,13522182,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fabiowanner,fabiowanner,fabiowanner,31/Jan/23 07:55,07/Feb/23 11:29,04/Jun/24 20:41,07/Feb/23 11:29,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,starter,,,"*Context*

Jar files for jobs are submitted to the operator by supplying a URI to the .jar file. This URI can be a file system path or a URI to some HTTP resource. If a HTTP URI is given, the file will be fetched using the {{{}HttpArtifactFetcher{}}}. 

There are cases where the supplied URI will contain additional params. For example if pre-signed S3 URLs are used.

Example:
{code:java}
https://some-domain.example.com/some.jar?some=params{code}
*Problem*

When the HttpArtifactFetcher determines the name of the .jar file it does also use the params as part of the file name. In the example from above the resulting file name would be:  {{some.jar?some=params}}

Submitting this job to Flink will result in an error as it will be checked for the file name to end with {{.jar}}

*Possible Solution*
In the {{HttpArtifactFetcher}} it would be enough to replace:
{code:java}
String fileName = FilenameUtils.getName(url.getFile());{code}
with
{code:java}
String fileName = FilenameUtils.getName(url.getPath());{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 11:29:58 UTC 2023,,,,,,,,,,"0|z1fi1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:29;gyfora;merged to main f0578a57f6395634b9ad301c7ab7176a45f9c438;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTest.testInterruptibleSharedLockInInvokeAndCancel causes a JVM shutdown with exit code 239,FLINK-30844,13522181,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,mapohl,mapohl,31/Jan/23 07:50,30/Aug/23 14:24,04/Jun/24 20:41,06/Apr/23 15:02,1.16.2,1.17.0,,,,,1.16.3,1.17.1,,,,,,,,,,,,,,,Runtime / Coordination,Runtime / Task,,,,0,pull-request-available,test-stability,,,"We're experiencing a fatal crash in {{TaskTest}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8334
{code}
[...]
Jan 31 01:03:12 [ERROR] Process Exit Code: 239
Jan 31 01:03:12 [ERROR] Crashed tests:
Jan 31 01:03:12 [ERROR] org.apache.flink.runtime.taskmanager.TaskTest
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Jan 31 01:03:12 [ERROR] at java.lang.Thread.run(Thread.java:748)
Jan 31 01:03:12 [ERROR] -> [Help 1]
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 31 01:03:12 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 31 01:03:12 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] After correcting the problems, you can resume the build with the command
Jan 31 01:03:12 [ERROR]   mvn <goals> -rf :flink-runtime
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32972,,,,,FLINK-30852,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 08:17:15 UTC 2023,,,,,,,,,,"0|z1fi1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 12:02;mapohl;An AssertionError is reported that should be independent from this build failure. I created FLINK-30852 to cover the issue.;;;","31/Jan/23 12:14;mapohl;{{TaskTest.testInterruptibleSharedLockInInvokeAndCancel}} caused the failure 
{code}
00:59:02,291 [Cancellation Watchdog for Test Task (1/1)#0 (003bbd51a0b61b0ff2925c31e749f53e_00000000000000000000000000000000_0_0).] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'Cancellation Watchdog for Test Task (1/1)#0 (003bbd51a0b61b0ff2925c31e749f53e_00000000000000000000000000000000_0_0).' produced an uncaught exception. Stopping the process...
org.apache.flink.util.FlinkRuntimeException: Error in Task Cancellation Watch Dog
        at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1801) ~[classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.lang.RuntimeException: Unexpected FatalError notification
        at org.apache.flink.runtime.taskmanager.TaskTest$ProhibitFatalErrorTaskManagerActions.notifyFatalError(TaskTest.java:1278) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1798) ~[classes/:?]
        ... 1 more
{code}

The {{TaskCancelerWatchDog}} causes the System.exit when the executor thread is still alive (see [Task:1781|https://github.com/apache/flink/blob/3b6d08e57f644cddcdac1fb5a110d44172652c3a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L1781]).

 [~piotr.nowicki] [~akalashnikov] May one of you have a look at this?;;;","31/Jan/23 15:02;piotr.nowicki;[~mapohl] I believe I'm not the person you wanted to notify here :);;;","31/Jan/23 15:12;mapohl;ah, typo! 🤦‍♂️  sorry for the spam, Piotr Nowicki. I meant [~pnowojski];;;","31/Jan/23 18:11;akalashnikov;I don't see a problem here.  I just see that the thread is finishing longer than 50ms but I see it isn't stuck and make progress which is good. Normally, it should take about 1ms but 50ms  is also not something extraordinal on the overloaded machine. The only thing is a synchronization inside the finishing loop which can delay the actual finish but I have no evidence that it is a reason. 
I will try to check more ideas but if nothing works I will just increase the waiting interval for this test.(I actually think that 50ms is low anyway);;;","01/Feb/23 11:13;mapohl;thanks for sharing your thoughts. It sounds reasonable. I'm gonna lower the priority for that one to Major.;;;","20/Mar/23 07:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47318&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8351;;;","31/Mar/23 06:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47748&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8807;;;","31/Mar/23 08:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47750&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8385;;;","06/Apr/23 15:02;akalash;In conclusion, I haven't found any deadlocks or other suspicious things. I was able to reproduce it locally it seems it indeed just works pretty slowly on an overloaded machine. So I just increased the timeout. We will see how it will be.

merged to master: 6e95bfaf;;;","26/May/23 08:17;Sergey Nuyanzin;Merged to 1.16 as [141b47a80092134f95dfdf6b3d0e7051d4fec6bb|https://github.com/apache/flink/commit/141b47a80092134f95dfdf6b3d0e7051d4fec6bb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Notifier For Autoscaler,FLINK-30843,13522180,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Later,,gaurav726,gaurav726,31/Jan/23 07:36,24/Aug/23 08:53,04/Jun/24 20:41,24/Aug/23 08:53,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,auto-deprioritized-major,feature,,,"We can add Notifier abstraction and slack as the first cut for notification in case auto scaler performs the scaling activity. Let me know if this is feasible from your side, then I will start working on it",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 08:44:17 UTC 2023,,,,,,,,,,"0|z1fi14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 08:21;gyfora;Hi [~gaurav726] !

We have recently added event triggering as part of scaling: [https://github.com/apache/flink-kubernetes-operator/commit/53d22d8a3e33f3971df43a29735471b7a8ccefaf]

You could already combine this with the resource listener interface [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/plugins/#custom-flink-resource-listeners] to get the desired functionality. What do you think?;;;","31/Jan/23 08:52;gaurav726;Yes it makes sense, if you guys want, we can open source slack flink custom resource listener, or is it not needed?;;;","31/Jan/23 08:57;gyfora;I have never thought about a slack listener but it could actually be interesting for the users :) Especially if we make it configurable for the types of events to report to slack;;;","31/Jan/23 09:00;gaurav726;I can start working on that if we want to add this custom plugin as part of k8 flink release? what you think, also will need your help to setup base package or you can guide me with some points here to start with;;;","03/Feb/23 09:11;gaurav726;Hi [~gyfora]  waiting for your input, read [https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/concepts/controller-flow/] also to understand control flow;;;","03/Feb/23 09:26;gyfora;I am not familiar with the slack plugin licensing etc, but if licence permits it would be nice to have an example like this in the operator repo. ;;;","03/Feb/23 09:44;gaurav726;I think we can make it configurable via some config, just like cruise control does it, if the user wants to see all messages on slack for all event record messages or configurable for some types of events, what you think [~gyfora] , slack licensing etc. are not coming in picture here :thinking:

 

https://github.com/linkedin/cruise-control/pull/629/files;;;","06/Feb/23 07:18;gaurav726;[~gyfora]  waiting for your comment;;;","06/Feb/23 12:54;gyfora;Thanks [~gaurav726] , I think you can provide a prototype implementation of such a notifier module in the examples directory.;;;","07/Feb/23 04:36;gaurav726;Ok, will do it and raise PR;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","24/Aug/23 07:47;gyfora;[~gaurav726] are you still working on this? If not we can close this ticket as this will probably be covered by some other work.;;;","24/Aug/23 08:44;gaurav726;yes we can close this now, I will pick it up later;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for sink supports speculative execution,FLINK-30842,13522177,13519977,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,SleePy,SleePy,SleePy,31/Jan/23 07:15,06/Feb/23 09:50,04/Jun/24 20:41,06/Feb/23 09:50,,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,Runtime / Coordination,,,,0,pull-request-available,,,,Add document to describe how sink supports speculative execution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 09:50:09 UTC 2023,,,,,,,,,,"0|z1fi0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:50;zhuzh;Done via 7522405d13c638967ff47a0ae2f421c8b12a042c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect calc merge generate wrong plan,FLINK-30841,13522174,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,31/Jan/23 07:00,03/Feb/23 13:38,04/Jun/24 20:41,03/Feb/23 13:38,1.16.1,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"currently we have a `FlinkCalcMergeRuleTest`, take one test as example:
{code:java}
  @Test
  def testCalcMergeWithNonDeterministicExpr1(): Unit = {
    val sqlQuery = ""SELECT a, a1 FROM (SELECT a, random_udf(a) AS a1 FROM MyTable) t WHERE a1 > 10""
    util.verifyRelPlan(sqlQuery)
  }
{code}
the current final optimized plan will be wrong:
{code:java}
Calc(select=[a, random_udf(b) AS a1], where=[(random_udf(b) > 10)])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}
the merged calc contains two `random_udf` call, users may encounter the result satisfied by where predicate (>10) but the selected column <= 10, that's counter-intuitive for users

the expected plan is:
{code:java}
Calc(select=[a, a1], where=[(a1 > 10)])
+- Calc(select=[a, random_udf(b) AS a1])
   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 13:38:39 UTC 2023,,,,,,,,,,"0|z1fhzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 13:38;lincoln.86xy;fixed in master: c3a376f5380ee85c9d34a8e72806e6ce6893be6a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix resource leak when creating Executor,FLINK-30840,13522173,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,31/Jan/23 06:53,06/Feb/23 07:12,04/Jun/24 20:41,06/Feb/23 07:12,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 07:12:29 UTC 2023,,,,,,,,,,"0|z1fhzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 07:12;fsk119;Merged into master: 152edf6a5e6f67a67a3d09815557b304bcd7b1a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Relaxed looping group pattern make the following strict contiguity relaxed too,FLINK-30839,13522172,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Juntao Hu,Juntao Hu,31/Jan/23 06:48,17/Feb/23 07:44,04/Jun/24 20:41,,1.15.3,1.16.1,,,,,,,,,,,,,,,,,,,,,Library / CEP,,,,,0,,,,,"If a group pattern loops with relaxed contiguity, the strict contiguity between group pattern and next pattern turns out to be as the same effect as relaxed contiguity.

E.g. for pattern:
{code:java}
Pattern.begin(Pattern.begin(""A"").next(""B"")).oneOrMore().next(""C"")
{code}
sequence (a b d c) is also a matched one, which is abviously wrong.

As part of the NFA below shows, the ignore edge on A state causes the unexpected ignoration of event ""d"".

!looping-group-pattern-relaxed-1.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/23 06:46;Juntao Hu;looping-group-pattern-relaxed-1.png;https://issues.apache.org/jira/secure/attachment/13054931/looping-group-pattern-relaxed-1.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 06:48:10.0,,,,,,,,,,"0|z1fhzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation about the AdaptiveBatchScheduler,FLINK-30838,13522171,13518520,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,31/Jan/23 06:45,09/Feb/23 06:49,04/Jun/24 20:41,09/Feb/23 03:19,,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,Documentation is needed to update to help users how to enable the AdaptiveBatchScheduler and properly configuring it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 03:19:35 UTC 2023,,,,,,,,,,"0|z1fhz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 03:19;zhuzh;master:
a6c84cd2cb36f37aed06ded1af3cf4b52471bdfc

release-1.17:
11724b5cb66b7efb0111aa4d7feaab204f86fa8b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove dependency of MutableByteArrayInputStream from flink-avro-glue-schema-registry,FLINK-30837,13522168,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,liangtl,liangtl,liangtl,31/Jan/23 06:34,08/Feb/23 10:20,04/Jun/24 20:41,08/Feb/23 10:20,1.16.1,,,,,,1.17.0,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,"MutableByteArrayInputStream is a class implemented in flink-avro. We want to make this @Internal if possible, so we can reduce the surface area of interface that we support. At the moment, it is only used in flink-avro and flink-avro-glue-schema-registry.

As such, we explore if we can remove the use of MutableByteArrayInputStream from flink-avro-glue-schema-registry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 10:20:08 UTC 2023,,,,,,,,,,"0|z1fhyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 10:20;chesnay;master: bf68aa2fd9331d2df66a3aede6064d18544f3675
1.17: bf68aa2fd9331d2df66a3aede6064d18544f3675;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release FRocksDB 6.20.3-ververica-2.0,FLINK-30836,13522163,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yanfei Lei,Yanfei Lei,Yanfei Lei,31/Jan/23 05:32,01/Feb/23 13:46,04/Jun/24 20:41,31/Jan/23 05:42,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,,,,,"Compiled files for Linux x86, Linux arm, Linux ppc64le, MacOS x86,
MacOS arm, and Windows are included in FRocksDB 6.20.3-ververica-2.0
jar,  There are the following 4 points of improvement:
- [FLINK-30457] Add periodic_compaction_seconds option to RocksJava[1].
- [FLINK-30321] Upgrade ZLIB of FRocksDB to 1.2.13[2].
- Avoid expensive ToString() call when not in debug[3].
- [FLINK-24932] Support build FRocksDB Java on Apple silicon[4].

Maven artifacts for FRocksDB can be found at:
[https://mvnrepository.com/artifact/com.ververica/frocksdbjni]

 

[1] https://issues.apache.org/jira/browse/FLINK-30457
[2] https://issues.apache.org/jira/browse/FLINK-30321
[3] [https://github.com/ververica/frocksdb/pull/55]
[4] https://issues.apache.org/jira/browse/FLINK-24932",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 05:32:14.0,,,,,,,,,,"0|z1fhxc:",9223372036854775807,"FRocksDB has been upgraded to 6.20.3-ververica-2.0. Compared to the previous  6.20.3-ververica-1.0, the new version contains vulnerability fixes, Apple M1 platform supported, and some features/improvements. According to the results of fink-benchmarks, the performance of this version is basically the same as 6.20.3-ververica-1.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add smoke tests for flink connector,FLINK-30835,13522153,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Weijie Guo,Weijie Guo,31/Jan/23 04:22,19/Aug/23 22:35,04/Jun/24 20:41,,1.18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,Umbrella,,,"As discussed in dev mail-list, we should introducing smoke test for connectors to test Datastream&SQL job at the same time.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:01 UTC 2023,,,,,,,,,,"0|z1fhv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortAggITCase.testLeadLag is unstable in ALL_EXCHANGES_BLOCKING mode,FLINK-30834,13522151,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhuzh,zhuzh,31/Jan/23 04:05,31/Jan/23 04:09,04/Jun/24 20:41,,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"When investigating the problem of FLINK-30828, we found that the SortAggITCase.testLeadLag is working well in ALL_EXCHANGES_PIPELINED mode. However, it becomes unstable when runs in ALL_EXCHANGES_BLOCKING mode, which is unexpected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30828,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-31 04:05:54.0,,,,,,,,,,"0|z1fhuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMetricGroup supports being closed,FLINK-30833,13522135,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mason6345,mason6345,30/Jan/23 23:53,07/Feb/23 09:22,04/Jun/24 20:41,,1.16.1,,,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,Tests,,,,0,,,,,"For testing purposes, closing a metric group is required to verify logic that maintains metrics. The TestMetricGroup needs to extend AbstractMetricGroup<> to support this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 09:22:34 UTC 2023,,,,,,,,,,"0|z1fhrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 23:54;mason6345;[~renqs] wdyt about adding this functionality? I can take it on if I can get your review. It's a small change;;;","02/Feb/23 09:03;renqs;[~mason6345] Please go ahead as it's just a test utilization, but I'm afraid I don't quite get the motivation of the improvement. The original purpose of creating {{MetricListener}} is to trap metrics registered under a group and check if their values are as expected (Mainly used for testing connector metrics but of course it's a general one). Could you elaborate more about ""closing metric groups""? I assume this is not relevant to {{{}MetricListener{}}}.;;;","02/Feb/23 10:23;chesnay;I'd rather see us amending the TestMetricGroup or similar to support this. The listener relies on metric (de)registration hitting the metric registry, but groups being closed doesn't even reach that.;;;","07/Feb/23 02:31;mason6345;For context, I was writing a connector that depends on closing some metric groups. The closed metric group can be created independently of MetricListener although other tests can continue to use it. [~chesnay] 's described approach makes more sense and I will update the Jira ticket to reflect that. Thanks!;;;","07/Feb/23 09:22;chesnay;Do you even need to test the closing? Wouldn't the metric being registered again suffice?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator helm chart - allow specifying operator pod resources requests/limits for flink-webhook container,FLINK-30832,13522093,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,VinceChenal,VinceChenal,30/Jan/23 16:20,02/Feb/23 16:00,04/Jun/24 20:41,02/Feb/23 16:00,kubernetes-operator-1.3.1,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,flink-kubernetes-operator helm chart does not allow specifying resources requests/limits for operator pod on flink-webhook container. It would be nice having such configuration in environments where you are forced to specify them.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 16:00:20 UTC 2023,,,,,,,,,,"0|z1fhi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 16:00;gyfora;merged to main 91df8ba7f53b5d552bad9fc822f3e9a071757dd2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving e2e test output in case errors/exceptions are found,FLINK-30831,13522088,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,flaviu.cicio,mapohl,mapohl,30/Jan/23 15:50,15/Oct/23 10:23,04/Jun/24 20:41,,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,auto-deprioritized-major,starter,,,"Some e2e tests parse the Flink logs for exceptions using {{grep}}. We then print the first 500 lines of the each log file in case an exception that shouldn't be ignored is found (see [internal_check_logs_for_exceptions|https://github.com/apache/flink/blob/c9e87fe410c42f7e7c19c81456d4212a58564f5e/flink-end-to-end-tests/test-scripts/common.sh#L449] or [check_logs_for_errors|https://github.com/apache/flink/blob/c9e87fe410c42f7e7c19c81456d4212a58564f5e/flink-end-to-end-tests/test-scripts/common.sh#L387]). Instead, we could use {{grep -C200}} to actually print the context of the exception.

This would help especially in those situations where the exception doesn't appear in the first 500 lines.

This issue does not necessarily only include the two aforementioned code locations. One should check the scripts for other code with a similar issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30328,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 04 14:46:14 UTC 2023,,,,,,,,,,"0|z1fhh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Sep/23 09:55;flaviu.cicio;[~mapohl] Can I have this ticket as my first one?;;;","04/Oct/23 14:46;mapohl;Thanks for volunteering. i assigned the issue to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Temporal Table Function Samples for Java Do Not Work As-Is, And Some Functions Are Deprecated",FLINK-30830,13522086,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jeremyber,jeremyber,jeremyber,30/Jan/23 15:44,06/Feb/23 16:11,04/Jun/24 20:41,06/Feb/23 16:11,1.16.1,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,developer-experience,pull-request-available,,,"In the java documentation for the [Temporal Table Function|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/temporal_table_function/], there are two issues with the APIs as-is.
 * Defining the temporal table calls `tEnv.registerFunction` which is a deprecated API since v1.11, favoring `createTemporarySystemFunction` instead.
 * In the Temporal Table Function Join syntax, calling `joinLateral($(""rates(order_time)"")` elicits the following error:

 

{{Cannot resolve field [rates(order_time)] }}

 

 

For point 1, I am proposing we replace `tEnv.registerFunction()` with `tEnv.createTemporarySystemFunction`.

 

For point 2, I am proposing we replace:

 

 

{{{}Table result = orders{}}}{{{}.joinLateral($(""rates(order_time)""), $(""orders.currency = rates.currency"")){}}}{{{}.select($(""(o_amount * r_rate).sum as amount""));{}}}

 

with

 

{{{}Table result = orders.joinLateral(call(""rates"", $(""o_proctime"")), $(""o_currency"").isEqual($(""r_currency""))){}}}{{{}.select($(""(o_amount"").times($(""r_rate"")).sum().as(""amount"")); {}}}

 

{{Sources for corrections:}}
 * {{[https://nightlies.apache.org/flink/flink-docs-release-1.11/api/java/org/apache/flink/table/api/TableEnvironment.html#registerFunction-java.lang.String-org.apache.flink.table.functions.ScalarFunction-]}}
 * {{{}[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/tableapi/#join-with-temporal-table]{}}}{{{{}}{}}}",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,Mon Feb 06 16:11:34 UTC 2023,,,,,,,,,,"0|z1fhgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 16:11;dannycranmer;Merged commit [{{1d4d255}}|https://github.com/apache/flink/commit/1d4d255147dae0072e5f6da77ad1fc25faa823f8] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the backpressure tab could be sort by the backpressure level,FLINK-30829,13522082,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Zhanghao Chen,Zhanghao Chen,30/Jan/23 14:49,10/Jul/23 09:06,04/Jun/24 20:41,08/Mar/23 06:54,1.17.0,1.18.0,,,,,1.18.0,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,1,pull-request-available,,,,"[FLINK-29998|https://issues.apache.org/jira/browse/FLINK-29998] enables user to sort the backpressure tab to see which task is busiest. Another common scenario for backpressure analysis is to find which tasks are backpressured. We should add support to sort the backpressure tab by backpressure level as well.

 
h4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32568,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 06:54:14 UTC 2023,,,,,,,,,,"0|z1fhfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 14:55;Zhanghao Chen;Hi, [~yunta]. If you think this is valid, could you assign it to me? I'd be willing to prepare a PR for it.;;;","31/Jan/23 05:50;yunta;[~Zhanghao Chen] What do you mean ""backpressure level"", do you mean the ""Backpressure Status"" tab? We use the newly added busy metrics to find which subtask is the bottleneck, and the previous existing ""Backpressure Status"" tab is used to find which subtask is mostly backpressured but cannot find which subtask is the bottleneck in the downstream side.
From my point of view, since we already introduced the busy tab, which makes sorting by backpressured status a bit meaningless.;;;","27/Feb/23 13:54;Wencong Liu;I think if the busy/idle/backpressure is displayed in three columns and each of them can be sorted separately, it will be more clear to users. WDYT? cc [~yunta] [~xtsong] ;;;","27/Feb/23 14:38;Zhanghao Chen;After a second thought, I agree with [~yunta] that sorting by backpressured status a bit meaningless. Sorting each of the busy/idle/backpressure columns would be clearer for users but the marginal benefit is also limited.;;;","28/Feb/23 10:56;xtsong;Sorting subtasks by busy/idle/backpressure time sounds to be a nice improvement to me, especially for workloads with high parallelism. I'd be +1 for it if the required frontend changes are not too heavy.;;;","01/Mar/23 13:16;Wencong Liu;For jobs with high parallelism, it will be very convenient to analyze job status according to sorted busy/idle/backpressure columns. Would you like to continue this? If you don't have time, I can take over. More discussion is also necessary. cc [~Zhanghao Chen] ;;;","01/Mar/23 13:45;yunta;I think we already have the ability to sort according to the busy percentage in FLINK-29998, what else do we need here?;;;","02/Mar/23 08:37;Wencong Liu;I think it's necessary to sort the three columns separately. The calculation strategy is different between busy/idle/backpressure, and it may be updated in future. Therefore, it will be inaccurate to infer the top value of backpressure or idle percentage by sorted busy columns. WDYT? [~yunta] ;;;","08/Mar/23 06:54;Weijie Guo;master(1.18) via ed30a4f906f35654745ca2f694b540ed50e0e14e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortAggITCase.testLeadLag failed,FLINK-30828,13522075,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,mapohl,mapohl,30/Jan/23 13:30,31/Jan/23 10:27,04/Jun/24 20:41,31/Jan/23 02:57,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12560

{code}
Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase
Jan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag  Time elapsed: 0.547 s  <<< FAILURE!
Jan 30 11:03:32 java.lang.AssertionError: 
Jan 30 11:03:32 
Jan 30 11:03:32 Results do not match for query:
Jan 30 11:03:32   
Jan 30 11:03:32 SELECT
Jan 30 11:03:32   a,
Jan 30 11:03:32   b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
Jan 30 11:03:32   c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
Jan 30 11:03:32   d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
Jan 30 11:03:32   e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
Jan 30 11:03:32   f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
Jan 30 11:03:32   g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
Jan 30 11:03:32   h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
Jan 30 11:03:32   i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
Jan 30 11:03:32   j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
Jan 30 11:03:32   k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
Jan 30 11:03:32   l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
Jan 30 11:03:32   m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
Jan 30 11:03:32   n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag
Jan 30 11:03:32 
Jan 30 11:03:32 FROM UnnamedTable$230
Jan 30 11:03:32 order by a
Jan 30 11:03:32 
Jan 30 11:03:32 
Jan 30 11:03:32 Results
Jan 30 11:03:32  == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
Jan 30 11:03:32  +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]
Jan 30 11:03:32  +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]
Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]
Jan 30 11:03:32         
Jan 30 11:03:32 Plan:
Jan 30 11:03:32   == Abstract Syntax Tree ==
Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
Jan 30 11:03:32    +- LogicalUnion(all=[true])
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30826,,,,,,,,,,,,,FLINK-30834,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 10:27:23 UTC 2023,,,,,,,,,,"0|z1fhe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 13:57;mapohl;[~shengkai] [~zhengyunhong97] may you have a look at it?;;;","30/Jan/23 16:29;zhuzh;I can re-produce this problem locally.
This problem started to appear after FLINK-30683 which changed the SQL test to test against {{BatchShuffleMode.ALL_EXCHANGES_BLOCKING}} 
 instead of {{BatchShuffleMode.ALL_EXCHANGES_PIPELINED}}.
By changing it back to test against {{BatchShuffleMode.ALL_EXCHANGES_PIPELINED}}, the problem is gone. So seems the case is unstable with blocking shuffle.
I will change the case to run with {{BatchShuffleMode.ALL_EXCHANGES_PIPELINED}} first to unblock CI.

cc [~godfreyhe] [~lzljs3620320]
;;;","31/Jan/23 02:57;zhuzh;Fixed via 9a5c2dbb85f1143c2ae24f92cd7033d8e35c69d9;;;","31/Jan/23 04:06;zhuzh;I opened a separate ticket FLINK-30834 to track the issue that SortAggITCase.testLeadLag becomes unstable in ALL_EXCHANGES_BLOCKING mode.;;;","31/Jan/23 10:27;mapohl;The following build is based on [dcbb2068|https://github.com/apache/flink/commit/dcbb2068] didn't contain the aforementioned fix [9a5c2dbb|https://github.com/apache/flink/commit/9a5c2dbb85f1143c2ae24f92cd7033d8e35c69d9], yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45445&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12565;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the SQL Client code structure,FLINK-30827,13522074,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,30/Jan/23 13:11,01/Feb/23 10:10,04/Jun/24 20:41,01/Feb/23 10:10,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 10:10:27 UTC 2023,,,,,,,,,,"0|z1fhe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 10:10;fsk119;Merged into master: 615fde6667a9a57be8cf308b1118171f8c812f13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashAggITCase.testLeadLag is failing,FLINK-30826,13522060,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,30/Jan/23 12:25,31/Jan/23 13:53,04/Jun/24 20:41,31/Jan/23 11:50,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,"With jdk8 it is green.
At the same time it is constantly failing for jdk11 like
{noformat}
[ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.testLeadLag  Time elapsed: 0.336 s  <<< FAILURE!
java.lang.AssertionError: 

Results do not match for query:
  
SELECT
  a,
  b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
  c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
  d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
  e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
  f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
  g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
  h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
  i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
  j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
  k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
  l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
  m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
  n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag

FROM UnnamedTable$18
order by a


Results
 == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
!+I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null, null]
!+I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null, null]
!+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99]
        
Plan:
  == Abstract Syntax Tree ==
LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
+- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
   +- LogicalUnion(all=[true])
      :- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST(1:TINYINT):TINYINT, CAST(1:SMALLINT):SMALLINT, CAST(2):INTEGER, CAST(9223:BIGINT):BIGINT, CAST(-2.3E0:FLOAT):FLOAT, CAST(9.9E0:DOUBLE):DOUBLE, CAST(true):BOOLEAN, CAST(_UTF-16LE'varchar':VARCHAR(20) CHARACTER SET ""UTF-16LE""):VARCHAR(20) CHARACTER SET ""UTF-16LE"", CAST(_UTF-16LE'char                '):CHAR(20) CHARACTER SET ""UTF-16LE"", CAST(2021-08-03):DATE, CAST(20:08:17):TIME(0), CAST(2021-08-03 20:08:29:TIMESTAMP(6)):TIMESTAMP(6), CAST(9.99E0:DECIMAL(3, 2)):DECIMAL(3, 2)]])
      :  +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
      :- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:TINYINT, null:SMALLINT, null:INTEGER, null:BIGINT, null:FLOAT, null:DOUBLE, null:BOOLEAN, null:VARCHAR(20) CHARACTER SET ""UTF-16LE"", null:CHAR(20) CHARACTER SET ""UTF-16LE"", null:DATE, null:TIME(0), null:TIMESTAMP(6), null:DECIMAL(3, 2)]])
      :  +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
      +- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST(1:TINYINT):TINYINT, CAST(1:SMALLINT):SMALLINT, CAST(2):INTEGER, CAST(9223:BIGINT):BIGINT, CAST(-2.3E0:FLOAT):FLOAT, CAST(9.9E0:DOUBLE):DOUBLE, CAST(true):BOOLEAN, CAST(_UTF-16LE'varchar':VARCHAR(20) CHARACTER SET ""UTF-16LE""):VARCHAR(20) CHARACTER SET ""UTF-16LE"", CAST(_UTF-16LE'char                '):CHAR(20) CHARACTER SET ""UTF-16LE"", CAST(2021-08-03):DATE, CAST(20:08:17):TIME(0), CAST(2021-08-03 20:08:29:TIMESTAMP(6)):TIMESTAMP(6), CAST(9.99E0:DECIMAL(3, 2)):DECIMAL(3, 2)]])
         +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])

== Optimized Logical Plan ==
Sort(orderBy=[a ASC])
+- Calc(select=[a, b, w0$o0 AS bLead, w0$o1 AS bLag, c, w0$o2 AS cLead, w0$o3 AS cLag, d, w0$o4 AS dLead, w0$o5 AS dLag, e, w0$o6 AS eLead, w0$o7 AS eLag, f, w0$o8 AS fLead, w0$o9 AS fLag, g, w0$o10 AS gLead, w0$o11 AS gLag, h, w0$o12 AS hLead, w0$o13 AS hLag, i, w0$o14 AS iLead, w0$o15 AS iLag, j, w0$o16 AS jLead, w0$o17 AS jLag, k, w0$o18 AS kLead, w0$o19 AS kLag, l, w0$o20 AS lLead, w0$o21 AS lLag, m, w0$o22 AS mLead, w0$o23 AS mLag, n, w0$o24 AS nLead, w0$o25 AS nLag])
   +- OverAggregate(orderBy=[a ASC], window#0=[LEAD(b, 1) AS w0$o0, LAG(b, 1) AS w0$o1, LEAD(c, 1) AS w0$o2, LAG(c, 1) AS w0$o3, LEAD(d, 1) AS w0$o4, LAG(d, 1) AS w0$o5, LEAD(e, 1) AS w0$o6, LAG(e, 1) AS w0$o7, LEAD(f, 1) AS w0$o8, LAG(f, 1) AS w0$o9, LEAD(g, 1) AS w0$o10, LAG(g, 1) AS w0$o11, LEAD(h, 1) AS w0$o12, LAG(h, 1) AS w0$o13, LEAD(i, 1) AS w0$o14, LAG(i, 1) AS w0$o15, LEAD(j, 1) AS w0$o16, LAG(j, 1) AS w0$o17, LEAD(k, 1) AS w0$o18, LAG(k, 1) AS w0$o19, LEAD(l, 1) AS w0$o20, LAG(l, 1) AS w0$o21, LEAD(m, 1) AS w0$o22, LAG(m, 1) AS w0$o23, LEAD(n, 1) AS w0$o24, LAG(n, 1) AS w0$o25 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, d, e, f, g, h, i, j, k, l, m, n, w0$o0, w0$o1, w0$o2, w0$o3, w0$o4, w0$o5, w0$o6, w0$o7, w0$o8, w0$o9, w0$o10, w0$o11, w0$o12, w0$o13, w0$o14, w0$o15, w0$o16, w0$o17, w0$o18, w0$o19, w0$o20, w0$o21, w0$o22, w0$o23, w0$o24, w0$o25])
      +- Sort(orderBy=[a ASC])
         +- Exchange(distribution=[single])
            +- Union(all=[true], union=[a, b, c, d, e, f, g, h, i, j, k, l, m, n])
               :- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, CAST(1 AS TINYINT) AS b, CAST(1 AS SMALLINT) AS c, CAST(2 AS INTEGER) AS d, CAST(9223 AS BIGINT) AS e, CAST(-2.3E0 AS FLOAT) AS f, CAST(9.9E0 AS DOUBLE) AS g, CAST(true AS BOOLEAN) AS h, CAST('varchar' AS VARCHAR(20)) AS i, CAST('char                ' AS CHAR(20)) AS j, CAST(2021-08-03 AS DATE) AS k, CAST(20:08:17 AS TIME(0)) AS l, CAST(2021-08-03 20:08:29 AS TIMESTAMP(6)) AS m, CAST(9.99E0 AS DECIMAL(3, 2)) AS n])
               :  +- Values(tuples=[[{ 0 }]], values=[ZERO])
               :- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, null:TINYINT AS b, null:SMALLINT AS c, null:INTEGER AS d, null:BIGINT AS e, null:FLOAT AS f, null:DOUBLE AS g, null:BOOLEAN AS h, null:VARCHAR(20) AS i, null:CHAR(20) AS j, null:DATE AS k, null:TIME(0) AS l, null:TIMESTAMP(6) AS m, null:DECIMAL(3, 2) AS n])
               :  +- Values(tuples=[[{ 0 }]], values=[ZERO])
               +- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, CAST(1 AS TINYINT) AS b, CAST(1 AS SMALLINT) AS c, CAST(2 AS INTEGER) AS d, CAST(9223 AS BIGINT) AS e, CAST(-2.3E0 AS FLOAT) AS f, CAST(9.9E0 AS DOUBLE) AS g, CAST(true AS BOOLEAN) AS h, CAST('varchar' AS VARCHAR(20)) AS i, CAST('char                ' AS CHAR(20)) AS j, CAST(2021-08-03 AS DATE) AS k, CAST(20:08:17 AS TIME(0)) AS l, CAST(2021-08-03 20:08:29 AS TIMESTAMP(6)) AS m, CAST(9.99E0 AS DECIMAL(3, 2)) AS n])
                  +- Values(tuples=[[{ 0 }]], values=[ZERO])

       
	at org.junit.Assert.fail(Assert.java:89)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1(BatchTestBase.scala:152)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1$adapted(BatchTestBase.scala:145)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.testLeadLag(AggregateITCaseBase.scala:958)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30828,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 13:53:25 UTC 2023,,,,,,,,,,"0|z1fhaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 16:33;zhuzh;I think it has the same cause of FLINK-30828. The test is a bit unstable though.;;;","31/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12842;;;","31/Jan/23 08:01;zhuzh;This problem should have been fixed in FLINK-30828.

The failed test above did not include the fix.;;;","31/Jan/23 08:12;zhuzh;[~Sergey Nuyanzin] is the problem still happening with JDK11 when using the latest Flink master?;;;","31/Jan/23 11:50;Sergey Nuyanzin;I tested it against latest master with jdk11 and it seems the issue is fixed

Thank you.;;;","31/Jan/23 13:53;zhuzh;Thanks for reporting this issue and verifying the fix! [~Sergey Nuyanzin];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ingress creation failed on K8s with version < 1.19,FLINK-30825,13522052,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,idealities,idealities,30/Jan/23 11:52,09/Feb/23 08:56,04/Jun/24 20:41,09/Feb/23 08:56,kubernetes-operator-1.2.0,kubernetes-operator-1.3.1,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Currently flink-kubernetes-operator only use io.fabric8.kubernetes.api.model.networking.v1.Ingress, but this is only available in and after K8s 1.19 ([https://kubernetes.io/docs/reference/using-api/deprecation-guide/] use the *networking.k8s.io/v1* API version, available since v1.19.)

With K8s before 1.19, we can use Ingress in *extensions/v1beta1* or {*}networking.k8s.io/v1beta1{*}, to avoid Ingress creation failure.","K8s 1.18

 ",1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Feb 09 08:56:32 UTC 2023,,,,,,,,,,"0|z1fh94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 08:56;gyfora;merged to main 33771a5b6748e951448f80f1627d502c4e77a67b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for new introduced hive agg option,FLINK-30824,13522051,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,30/Jan/23 11:47,20/Feb/23 12:54,04/Jun/24 20:41,20/Feb/23 12:54,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 12:54:18 UTC 2023,,,,,,,,,,"0|z1fh8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 12:54;godfrey;Fixed in 1.17.0: afdc079465c393d98bf2b3607a75b1fc9d58d281;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable speculative execution for some of the typical built-in sinks,FLINK-30823,13522030,13519977,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,SleePy,SleePy,SleePy,30/Jan/23 10:00,31/Jan/23 11:19,04/Jun/24 20:41,31/Jan/23 11:19,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"After the Sink supports speculative execution, here we enable speculative execution for some built-in sinks. For each type of Sink (SinkV2, SinkFunction, OutputFormat) we picked some typical sinks that do not require any or only minor changes.

In this ticket, we would enable DiscardingSink, PrintSinkFunction, PrintSink, FileSink, FileSystemOutputFormat to support speculative execution.

Speculative execution might not be meaningful for some of these sinks, like DiscardingSink, PrintSinkFunction and PrintSink. However, when these sinks are chained with other operators, the chained task could not support speculative execution if there is any operator in chain does not support speculative execution. That's the main idea behind this.

FileSink and FileSystemOutputFormat are the most typical implementation for SinkV2 and OutputFormat. We would not enable speculative execution for the StreamingFileSink, because it's marked as deprecated. The HiveTableSink without compaction could also support speculative, because it depends on FileSystemOutputFormat.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 11:19:33 UTC 2023,,,,,,,,,,"0|z1fh48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 11:19;zhuzh;Done via
7894fe56cd92d423a20302ea488b075507c20bbd
886b1f155313207128ddb2a242c2d1587c6d8642
6913f91107f606db2db8e23c21ac74c4aaf63de0
401aefdcf16992bde636eb617decdfa1ffd36622
1420ee7ed69a3ce72eab8027570cc3af9fa828a3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkITCase fails in setup and teardown method because the topic couldn't be created/deleted,FLINK-30822,13522006,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,30/Jan/23 07:59,16/Oct/23 06:53,04/Jun/24 20:41,16/Oct/23 06:53,1.16.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,test-stability,,,,"We experience build failures in {{KafkaSinkITCase}} due to the topic already existing when creating it and failing to delete it because a partition doesn't exist:
{code:java}
Jan 28 01:45:32 [ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 89.58 s <<< FAILURE! - in org.apache.flink.connector.kafka.sink.KafkaSinkITCase
Jan 28 01:45:32 [ERROR] org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testWriteRecordsToKafkaWithNoneGuarantee  Time elapsed: 14.387 s  <<< FAILURE!
Jan 28 01:45:32 org.opentest4j.MultipleFailuresError: 
Jan 28 01:45:32 Multiple Failures (2 failures)
Jan 28 01:45:32 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Jan 28 01:45:32 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Jan 28 01:45:32 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Jan 28 01:45:32 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
Jan 28 01:45:32 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Jan 28 01:45:32 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Jan 28 01:45:32 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Jan 28 01:45:32 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Jan 28 01:45:32 	Suppressed: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TopicExistsException: Topic 'b5de1eb2-175c-4b96-96c9-30ff23db2ca7' already exists.
Jan 28 01:45:32 		at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Jan 28 01:45:32 		at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 28 01:45:32 		at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Jan 28 01:45:32 		at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.createTestTopic(KafkaSinkITCase.java:447)
Jan 28 01:45:32 		at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.setUp(KafkaSinkITCase.java:162)
Jan 28 01:45:32 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 28 01:45:32 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 28 01:45:32 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 28 01:45:32 		at java.lang.reflect.Method.invoke(Method.java:498)
Jan 28 01:45:32 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jan 28 01:45:32 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jan 28 01:45:32 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jan 28 01:45:32 		at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
Jan 28 01:45:32 		at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
Jan 28 01:45:32 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jan 28 01:45:32 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 28 01:45:32 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 28 01:45:32 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jan 28 01:45:32 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jan 28 01:45:32 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jan 28 01:45:32 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jan 28 01:45:32 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jan 28 01:45:32 		... 34 more
Jan 28 01:45:32 	Caused by: org.apache.kafka.common.errors.TopicExistsException: Topic 'b5de1eb2-175c-4b96-96c9-30ff23db2ca7' already exists.
Jan 28 01:45:32 	Suppressed: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Jan 28 01:45:32 		at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Jan 28 01:45:32 		at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 28 01:45:32 		at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
Jan 28 01:45:32 		at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.deleteTestTopic(KafkaSinkITCase.java:453)
Jan 28 01:45:32 		at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.tearDown(KafkaSinkITCase.java:167)
Jan 28 01:45:32 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 28 01:45:32 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 28 01:45:32 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 28 01:45:32 		at java.lang.reflect.Method.invoke(Method.java:498)
Jan 28 01:45:32 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jan 28 01:45:32 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jan 28 01:45:32 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jan 28 01:45:32 		at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
Jan 28 01:45:32 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
Jan 28 01:45:32 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 28 01:45:32 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 28 01:45:32 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jan 28 01:45:32 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jan 28 01:45:32 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jan 28 01:45:32 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jan 28 01:45:32 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jan 28 01:45:32 		... 34 more
Jan 28 01:45:32 	Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45251&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37261",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29914,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 09:47:53 UTC 2023,,,,,,,,,,"0|z1fgz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 08:02;mapohl;FLINK-29914 already tried solving this issue for this specific test class by waiting for the topic creation in [9e7ebdc6|https://github.com/apache/flink/commit/9e7ebdc671386a8127ddf5affb66e997d877cb7b] The error is odd because we're generating a random topic here.;;;","30/Jan/23 08:30;gaborgsomogyi;The issue can be on Kafka side too... Please see this old but still open issue: https://issues.apache.org/jira/browse/KAFKA-6221
It would be good to dump Kafka server log in case of failed tests in general. This is not tightly related to this issue but it could help to analyze most of the Kafka test failures. I'm pretty sure this can't be avoided in mid/long term...;;;","13/Feb/23 09:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46031&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37727;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The optimized exec plan generated by OverAggregateTest#testDiffPartitionKeysWithDiffOrderKeys2 in the case of all-blocking is not as expected,FLINK-30821,13521961,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,JunRuiLi,JunRuiLi,30/Jan/23 02:47,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,1.20.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"The optimized exec plan generated by OverAggregateTest#testDiffPartitionKeysWithDiffOrderKeys2 in the case of all-blocking is that
{code:java}
Calc(select=[CASE((w0$o0 > 0), w0$o1, null:INTEGER) AS EXPR$0, w1$o0 AS EXPR$1, w2$o0 AS EXPR$2, w0$o2 AS EXPR$3, CAST((CASE((w3$o0 > 0), w3$o1, null:INTEGER) / w3$o0) AS INTEGER) AS EXPR$4])
+- OverAggregate(partitionBy=[c], orderBy=[a ASC], window#0=[MAX(a) AS w0$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0, w0$o1, w3$o0, w3$o1, w2$o0, w0$o0])
   +- Exchange(distribution=[forward])
      +- Sort(orderBy=[c ASC, a ASC])
         +- Exchange(distribution=[hash[c]])
            +- OverAggregate(partitionBy=[b], orderBy=[c ASC], window#0=[COUNT(a) AS w3$o0, $SUM0(a) AS w3$o1 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], window#1=[RANK(*) AS w2$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0, w0$o1, w3$o0, w3$o1, w2$o0])
               +- Exchange(distribution=[forward])
                  +- Sort(orderBy=[b ASC, c ASC])
                     +- Exchange(distribution=[hash[b]])
                        +- OverAggregate(orderBy=[c ASC, a ASC], window#0=[MIN(a) AS w0$o1 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0, w0$o1])
                           +- Exchange(distribution=[forward])
                              +- Sort(orderBy=[c ASC, a ASC])
                                 +- Exchange(distribution=[forward])
                                    +- OverAggregate(orderBy=[b ASC], window#0=[COUNT(a) AS w0$o2, $SUM0(a) AS w1$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0])
                                       +- Sort(orderBy=[b ASC])
                                          +- Exchange(distribution=[single])
                                             +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 {code}
However, the expected plan is that
{code:java}
Calc(select=[CASE((w0$o0 > 0), w0$o1, null:INTEGER) AS EXPR$0, w1$o0 AS EXPR$1, w2$o0 AS EXPR$2, w0$o2 AS EXPR$3, CAST((CASE((w3$o0 > 0), w3$o1, null:INTEGER) / w3$o0) AS INTEGER) AS EXPR$4])
+- OverAggregate(partitionBy=[c], orderBy=[a ASC], window#0=[MAX(a) AS w0$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0, w0$o1, w3$o0, w3$o1, w2$o0, w0$o0])
   +- Exchange(distribution=[forward])
      +- Sort(orderBy=[c ASC, a ASC])
         +- Exchange(distribution=[hash[c]])
            +- OverAggregate(partitionBy=[b], orderBy=[c ASC], window#0=[COUNT(a) AS w3$o0, $SUM0(a) AS w3$o1 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], window#1=[RANK(*) AS w2$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0, w0$o1, w3$o0, w3$o1, w2$o0])
               +- Exchange(distribution=[forward])
                  +- Sort(orderBy=[b ASC, c ASC])
                     +- Exchange(distribution=[hash[b]])
                        +- OverAggregate(orderBy=[c ASC, a ASC], window#0=[MIN(a) AS w0$o1 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0, w0$o1])
                           +- Exchange(distribution=[forward])
                              +- Sort(orderBy=[c ASC, a ASC])
                                 +- Exchange(distribution=[forward])
                                    +- OverAggregate(orderBy=[b ASC], window#0=[COUNT(a) AS w0$o2, $SUM0(a) AS w1$o0 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, w0$o2, w1$o0])
                                       +- Exchange (distribution=[forward])                                   
                                          +- Sort(orderBy=[b ASC])
                                             +- Exchange(distribution=[single])
                                                +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 10:44:32 UTC 2023,,,,,,,,,,"0|z1fgp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 02:49;JunRuiLi;cc [~zhuzh] [~godfrey] ;;;","12/Jul/23 10:44;nuafonso;Hello [~JunRuiLi],

 

I have a couple of questions about the issue:
 * Are both plans equivalent (they both produce the same result)?
 * Is this a case where the executed plan is not as optimised as the reported one?

 

Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support varchar and char for spark in table store,FLINK-30820,13521958,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,30/Jan/23 02:07,31/Jan/23 04:23,04/Jun/24 20:41,31/Jan/23 04:23,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Support convert varchar and char type in SparkTypeUtils,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:23:38 UTC 2023,,,,,,,,,,"0|z1fgog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 04:23;lzljs3620320;master: 
aeeed9a490ae35589662e96b035fc732fdf4364a
0c53630f1c762b67ebb1accfae20864ba0a41cd2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix sql client print an empty line between the multi-line statements,FLINK-30819,13521924,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,29/Jan/23 12:10,01/Feb/23 07:15,04/Jun/24 20:41,01/Feb/23 07:15,,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 07:15:52 UTC 2023,,,,,,,,,,"0|z1fggw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 07:15;fsk119;Fixed in mater: db82f82bd94a0af059bcfd6e85b3ae31bdc5c76e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add metrics to measure the count of window,FLINK-30818,13521920,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,29/Jan/23 10:06,29/Jan/23 10:07,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,,,,,"When recovering a job which using window operator,  taskmanager will need more resouce(cpu/memory) to process incoming data because there are too many created window on the cenario of data backlog.

Thus, maybe we should add metrics to measure the count of window.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-29 10:06:44.0,,,,,,,,,,"0|z1fgg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException in TestValuesTableFactory.TestValuesScanTableSourceWithoutProjectionPushDown,FLINK-30817,13521913,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,csq,csq,29/Jan/23 08:43,18/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-minor,pull-request-available,,,"When applying partitions in TestValuesScanTableSourceWithoutProjectionPushDown with no partition provided, the following code will cause ClassCastException

{code:java}
 remainingPartitions = (List<Map<String, String>>) Collections.emptyMap();
 this.data.put(Collections.emptyMap(), Collections.emptyList());
{code}


{code:java}
java.lang.ClassCastException: java.util.Collections$EmptyMap cannot be cast to java.util.List

	at org.apache.flink.table.planner.factories.TestValuesTableFactory$TestValuesScanTableSourceWithoutProjectionPushDown.applyPartitions(TestValuesTableFactory.java:1222)
	at org.apache.flink.table.planner.plan.abilities.source.PartitionPushDownSpec.apply(PartitionPushDownSpec.java:57)
	at org.apache.flink.table.planner.plan.rules.logical.PushPartitionIntoTableSourceScanRule.onMatch(PushPartitionIntoTableSourceScanRule.java:183)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:343)
{code}


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:03 UTC 2023,,,,,,,,,,"0|z1fgeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 06:22;luoyuxia;[~csq] Thanks for reporting. Would you like to help fix it ?;;;","01/Feb/23 06:22;luoyuxia;Introuduced by FLINK-24717;;;","01/Feb/23 11:52;csq;[~luoyuxia] I would like to help fix the issue, please assign the jira ticket to me.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Sql Client always uses the packaged highests version to connect to gateway,FLINK-30816,13521911,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,29/Jan/23 08:19,06/Feb/23 11:46,04/Jun/24 20:41,30/Jan/23 02:46,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 02:46:11 UTC 2023,,,,,,,,,,"0|z1fge0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 02:46;fsk119;Merged into master: cc6315a3e82c570e631684793d08ca9cb5403521;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchTestBase/BatchAbstractTestBase are using JUnit4 while some child tests are using JUnit5,FLINK-30815,13521897,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,zhuzh,zhuzh,29/Jan/23 04:24,27/Aug/23 17:10,04/Jun/24 20:41,10/May/23 03:44,1.16.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,,,,"BatchTestBase/BatchAbstractTestBase are using Junit4, while some child tests (e.g. DynamicFilteringITCase) are using JUnit5. This may break some assumption and hide some problems.
For example, the child test will create a MiniCluster by itself, instead of using the MiniCluster(TM=1, slots=3)  created in BatchAbstractTestBase. The created MiniCluster may  have more slots and hide resource deadlock issues.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31674,,,,,,,,FLINK-32048,,,,,FLINK-32055,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 03:44:04 UTC 2023,,,,,,,,,,"0|z1fgaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 06:37;tanyuxin;[~zhuzh] Thanks for starting the issue, I will take a look at it.;;;","10/May/23 03:44;godfrey;Fixed in master:

ed9ee279e50781b7bd2d85f1486721c02fc7e32b

7a423666d0f8452382ad5fe2635de5ad1475dd46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The parallelism&maxParallelism of sort after a global partitioning is not forced to be 1,FLINK-30814,13521895,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfrey,zhuzh,zhuzh,29/Jan/23 03:16,30/Jan/23 01:55,04/Jun/24 20:41,30/Jan/23 01:55,1.16.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"The parallelism&maxParallelism of sort after a global partitioning is not forced to be 1. The may lead to the parallelism to be changed by adaptive batch scheduler, which is unexpected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 01:55:35 UTC 2023,,,,,,,,,,"0|z1fgag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 07:54;godfrey;[~zhuzh] thanks for reporting this issue.

Currently, the parallelism of global sort is set as 1, but the max parallelism is not set because ExecNodeBase#inputsContainSingleton does not work for global sort.  This works fine for the default scheduler, but may occur wrong result if  adaptive batch scheduler changes the global sort parallelism.

 

I will fix it.

 ;;;","30/Jan/23 01:55;godfrey;Fixed in 1.17.0: 916ff76b61a0bfa8283d52be53cd33e317d0d550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Residual zk data when using the kubernetes session mode,FLINK-30813,13521853,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,spoon-lz,spoon-lz,28/Jan/23 10:04,23/Feb/23 08:56,04/Jun/24 20:41,23/Feb/23 08:56,,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,,"    If we use kubernetes session mode and use Zookeeper(ZK) as the HA service, the HA data on ZK is not cleared after the session stops.

    Because  when deleting a session, only call this method:
{code:java}
kubernetesClusterDescriptor.killCluster(clusterId);
{code}
    However, this method only deletes the deployment associated with the clusterId. If ZK is used as the HA service, data on ZK will be left over when the HA stops, resulting in more and more data on zk.

 

Maybe we need to add
{code:java}
ClusterClient#shutDownCluster(){code}
or
{code:java}
HighAvailabilityServices#closeAndCleanupAllData(){code}
When using session mode (native kubernetes)",,,,,,,,,,,,,,,,,,,,,,,,FLINK-20219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:08:42 UTC 2023,,,,,,,,,,"0|z1fg1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 04:08;wangyang0918;This might be a duplicate of FLINK-20219.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN with S3 resource storage fails for Hadoop 3.3.2,FLINK-30812,13521849,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mateczagany,mateczagany,mateczagany,28/Jan/23 09:48,02/Sep/23 07:04,04/Jun/24 20:41,02/Sep/23 07:04,1.16.0,1.17.0,1.18.0,,,,1.19.0,,,,,,,,,,,,,,,,Deployment / YARN,,,,,0,pull-request-available,stale-assigned,,,"In HADOOP-17139 S3AFileSystem#copyFromLocalFile was refactored and expects the local source Hadoop Path object to have a scheme specified which the YarnClusterDescriptor uploading the local files won't have.

When uploading files to S3 CopyFromLocalOperation#getFinalPath compares the passed source Hadoop Path with the file it found(which will have file:// scheme) using URI.relativize but it will fail because of the scheme difference and throw PathIOException as can be seen in this exception:

 
{code:java}
org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:478) ~[flink-yarn-1.16.0.jar!/:1.16.0]        ......
Caused by: org.apache.hadoop.fs.PathIOException: `Cannot get relative path for URI:file:///tmp/application_1674531932229_0030-flink-conf.yaml587547081521530798.tmp': Input/output error
	at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.getFinalPath(CopyFromLocalOperation.java:360) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.uploadSourceFromFS(CopyFromLocalOperation.java:222) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:169) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$25(S3AFileSystem.java:3920) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499) ~[hadoop-common-3.3.3.jar!/:?]
	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444) ~[hadoop-common-3.3.3.jar!/:?]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:3913) ~[flink-s3-fs-hadoop-1.16.0.jar!/:1.16.0]
	at org.apache.flink.yarn.YarnApplicationFileUploader.copyToRemoteApplicationDir(YarnApplicationFileUploader.java:397) ~[flink-yarn-1.16.0.jar!/:1.16.0]
	at org.apache.flink.yarn.YarnApplicationFileUploader.uploadLocalFileToRemote(YarnApplicationFileUploader.java:202) ~[flink-yarn-1.16.0.jar!/:1.16.0]
	at org.apache.flink.yarn.YarnApplicationFileUploader.registerSingleLocalResource(YarnApplicationFileUploader.java:181) ~[flink-yarn-1.16.0.jar!/:1.16.0]
	at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1047) ~[flink-yarn-1.16.0.jar!/:1.16.0]
	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:623) ~[flink-yarn-1.16.0.jar!/:1.16.0]
	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:471) ~[flink-yarn-1.16.0.jar!/:1.16.0]
	... 35 more {code}
 

The possibly easiest solution would be to somehow add the file:// scheme in YarnApplicationFileUploader#copyToRemoteApplicationDir

The other solution would be to change all calls uploading local files to use ""new Path(file.toURI())"" instead of ""new Path(file.getAbsolutePath())"" but it might not be as future-proof as the other solution

Email thread: [https://lists.apache.org/thread/oo5rlyo3jr7kds2y6wwnfo1yhnk0fx4c]

 

If a committer can assign this ticket to me I can start working on this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 02 07:04:35 UTC 2023,,,,,,,,,,"0|z1fg0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 08:31;martijnvisser;Is this is a trivial update for 1.16, given that in 1.17 we'll have FLINK-29710 which updated the minimum Hadoop versions to 2.10.2 and 3.2.3? ;;;","30/Jan/23 09:25;mateczagany;I have found an easy fix by making sure the passed 'localSrcPath' contains the 'file://' scheme in 'YarnApplicationFileUploader#copyToRemoteApplicationDir' if it doesn't contain any scheme already.

I manually tested this fix on a local single-node 3.3.4 Hadoop cluster with 'fs.defaultFS' pointing to a local Minio S3 storage and it worked well.

I'm trying to come up with a unit test for this change and I'll create a PR then.

Also, it affects all Hadoop versions starting from 3.3.2;;;","31/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","02/Sep/23 07:04;gaborgsomogyi;9507dd6 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix sql gateway can not stop job correctly,FLINK-30811,13521842,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,fsk119,fsk119,28/Jan/23 08:01,07/Feb/23 02:32,04/Jun/24 20:41,07/Feb/23 02:32,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,FLINK-30538,,,,,,,,,,,,,,,,,,,,,,,FLINK-27344,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 02:32:52 UTC 2023,,,,,,,,,,"0|z1ffyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 13:05;qingyue;I'd like to fix this issue, cc [~fsk119] ;;;","31/Jan/23 08:21;qingyue;*Phenomenon*
STOP JOB <job_identifier> occasionally not working as expected, and the following exception stacktrace can be found in the sql-client.log
{code:java}
2023-01-30 23:40:16,456 WARN org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel [] - Force-closing a channel whose registration task was not accepted by an event loop: [id: 0xe1c04c6e]
java.util.concurrent.RejectedExecutionException: event executor terminated
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:934) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:351) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:344) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:836) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute0(SingleThreadEventExecutor.java:827) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:817) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe.register(AbstractChannel.java:483) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:89) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:83) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.register(MultithreadEventLoopGroup.java:86) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.AbstractBootstrap.initAndRegister(AbstractBootstrap.java:323) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.doResolveAndConnect(Bootstrap.java:155) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:139) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:123) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:471) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:394) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:308) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$38(RestClusterClient.java:962) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:580) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202]
2023-01-30 23:40:16,464 ERROR org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?
java.util.concurrent.RejectedExecutionException: event executor terminated
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:934) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:351) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:344) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:836) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute0(SingleThreadEventExecutor.java:827) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:817) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:499) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:184) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:475) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:394) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:308) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$38(RestClusterClient.java:962) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:580) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202] {code}
*Root cause*
OperationExecutor#runClusterAction manages RestClusterClient lifecycle via try-with-resource; thus, the client will be closed immediately after leaving the code block, which shuts down the netty NioEventLoopGroup.

 

*Proposed fix*

Block the job cancel call until timeout.;;;","01/Feb/23 06:52;Paul Lin;[~qingyue] [~fsk119] Sorry for introducing this bug, I should have fixed this via FLINK-30538.;;;","03/Feb/23 09:51;qingyue;Hi [~Paul Lin], yes, the PR has been merged. This issue is discovered when reviewing FLINK-29945. You can reach the discussion at https://github.com/apache/flink/pull/21717#discussion_r1089640498.
I've rebased your fix and added the test case to verify that all problems have been fixed.;;;","07/Feb/23 02:32;fsk119;Merged into master: bbd5a7876eb1542ff89f05f5f5d82bb8bd41b7bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rework the CliClientITCase to extend AbstractSqlGatewayStatementITCase,FLINK-30810,13521835,13500273,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zjureel,fsk119,fsk119,28/Jan/23 06:05,13/Aug/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,Tests,,,,0,pull-request-available,stale-assigned,,,We should always use the AbstractSqlGatewayStatementITCase to cover the statement tests in the sql-client/sql-gateway. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 22:35:21 UTC 2023,,,,,,,,,,"0|z1ffxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 04:13;zjureel;Hi [~fsk119] I'd like to fix this issue, can you assign it to me? Thanks;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-elasticsearch7 updates data pipeline does not work,FLINK-30809,13521834,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,iduanyingjie,iduanyingjie,28/Jan/23 05:45,30/Mar/23 12:56,04/Jun/24 20:41,,elasticsearch-3.0.0,,,,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,0,,,,,"create elasticsearch in docker
{code:yaml}
version: '2.1'
services:
 elasticsearch:
   image: docker.elastic.co/elasticsearch/elasticsearch:7.17.7
   environment:
     - cluster.name=docker-cluster
     - bootstrap.memory_lock=true
     - ""ES_JAVA_OPTS=-Xms512m -Xmx512m""
     - discovery.type=single-node
   ports:
     - ""9200:9200""
     - ""9300:9300""
   ulimits:
     memlock:
       soft: -1
       hard: -1
     nofile:
       soft: 65536
       hard: 65536
 kibana:
   image: docker.elastic.co/kibana/kibana:7.17.7
   ports:
     - ""5601:5601""
{code}
create table: records in mysql
{code:sql}
CREATE TABLE records (
 id bigint unsigned NOT NULL AUTO_INCREMENT,
 user_id bigint unsigned NOT NULL,
 create_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,
 PRIMARY KEY (id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
{code}
insert some datas
{code:sql}
INSERT INTO test.records (id, user_id, create_time) VALUES(default, 123, '2023-01-20 12:25:11');
INSERT INTO test.records (id, user_id, create_time) VALUES(default, 456, '2023-01-20 12:25:30');
INSERT INTO test.records (id, user_id, create_time) VALUES(default, 789, '2023-01-20 12:25:37');
{code}
create pipeline in es:
{code:java}
PUT /_ingest/pipeline/set_ingest_timestamp_fields
{
 ""processors"": [
   {
     ""set"": {
       ""field"": ""ingest_timestamp"",
       ""value"": ""{{_ingest.timestamp}}""
     }
   }
 ]
}{code}
create index in es:
{code:java}
PUT enriched_records
{
 ""settings"": {
   ""default_pipeline"": ""set_ingest_timestamp_fields"",
   ""number_of_shards"": ""1"",
   ""number_of_replicas"": ""0""
 }
}{code}
excute flink sql:
{code:sql}
CREATE TABLE records (
   id INT,
   user_id INT,
   create_time TIMESTAMP(3),
   proc_time AS PROCTIME(),
   operation_time TIMESTAMP_LTZ(3) METADATA FROM 'op_ts' VIRTUAL,
   PRIMARY KEY (id) NOT ENFORCED
) WITH (
   'connector' = 'mysql-cdc',
   'hostname' = 'localhost',
   'port' = '3306',
   'username' = 'root',
   'password' = '123456',
   'database-name' = 'test',
   'table-name' = 'records',
   'server-time-zone' = 'UTC'
);
CREATE TABLE enriched_records (
   id INT,
   user_id INT,
   create_time TIMESTAMP(3),
   proc_time TIMESTAMP_LTZ(3),
   operation_time TIMESTAMP_LTZ(3),
   PRIMARY KEY (id) NOT ENFORCED
) WITH (
   'connector' = 'elasticsearch-7',
   'hosts' = 'http://localhost:9200',
   'index' = 'enriched_records'
);
INSERT INTO enriched_records
SELECT
   o.id,
   o.user_id,
   o.create_time,
   o.proc_time,
   o.operation_time
FROM records AS o; 
{code}
We query the data in Elasticsearch use GET /enriched_records/_search and we find that each record has an ingest_timestamp field and the value is the recent time.
{code:json}
{
    ""_index"":""enriched_records"",
    ""_type"":""_doc"",
    ""_id"":""3"",
    ""_score"":1,
    ""_source"":{
        ""operation_time"":""1970-01-01 00:00:00Z"",
        ""create_time"":""2023-01-20 12:25:37"",
        ""user_id"":789,
        ""ingest_timestamp"":""2023-01-28T05:21:40.539754251Z"",
        ""id"":3,
        ""proc_time"":""2023-01-28 05:21:40.233Z""
    }
} {code}
When we modify a record in MySQL, the value of the ingest_timestamp field does not change, and it seems that the pipeline set for this index is not working at this moment.
{code:json}
{
    ""_index"":""enriched_records"",
    ""_type"":""_doc"",
    ""_id"":""3"",
    ""_score"":1,
    ""_source"":{
        ""operation_time"":""2023-01-28 05:25:05Z"",
        ""create_time"":""2023-01-20 12:25:37"",
        ""user_id"":987,
        ""ingest_timestamp"":""2023-01-28T05:21:40.539754251Z"",
        ""id"":3,
        ""proc_time"":""2023-01-28 05:25:05.529Z""
    }
}
{code}
If we directly modify a field in Elasticsearch, we can find that the value of the ingest_timestamp field will change.

 ","Flink Version: 1.15.3
Flink-CDC Version: 2.3.0
Mysql Version: 5.7
Elasticsearch Version: 7.17.7

During the test, these jar packages were added under flink/lib：
flink-sql-connector-elasticsearch7-1.15.3.jar
flink-sql-connector-mysql-cdc-2.3.0.jar
mysql-connector-java-8.0.27.jar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 12:56:29 UTC 2023,,,,,,,,,,"0|z1ffxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 12:56;martijnvisser;How have you determined that the problem is the Elasticsearch sink, and not the MySQL CDC source? Is updated record in MySQL actually visible in your test? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleInputITCase failed with AdaptiveBatch Scheduler,FLINK-30808,13521831,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,28/Jan/23 03:43,29/Jan/23 08:09,04/Jun/24 20:41,29/Jan/23 08:09,1.16.0,1.17.0,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"MultipleInputITCase#testRelatedInputs failed with AdaptiveBatch Scheduler.
{code:java}
java.lang.UnsupportedOperationException: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[10]-14 parallelism: 1, downstream operation: HashJoin[15]-20 parallelism: 3 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 08:09:21 UTC 2023,,,,,,,,,,"0|z1ffwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 03:45;JunRuiLi;cc [~zhuzh] ;;;","28/Jan/23 04:04;JunRuiLi;The failure is because currently in StreamGraph#createActualEdge, if the partitioner is a ForwardPartitioner, then the operator parallelism of the upstream and downstream of the partitioner will be required to be consistent. However, for the ForwardForConsecutiveHashPartitioner(which is a subclass of ForwardPartitioner) used in this case, it is not required that the parallelism of upstream and downstream operators must be consistent. So we can fix this bug by fixing the StreamGraph#createActualEdge.;;;","29/Jan/23 08:09;zhuzh;master:
a55ae1c6b426acdf499fc4df766fc43daa2dcce4

release-1.16:
f8e2b618c73814236873e4d74173072acc9282d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State Processor API - Overwrite Support for Savepoints,FLINK-30807,13521806,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rionmonster,rionmonster,27/Jan/23 20:26,01/Feb/23 11:59,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,API / State Processor,,,,,0,,,,,"Currently there is no overwrite support when using the State Processor API to create a savepoint at a given location. For applications that may run or generate a given savepoint on a periodic basis (e.g. cron job, nightly process, etc.) this can result in an exception if the job was previously run.

This ticket proposes amending the existing `SavePointWriter` class to support passing the preferred overwrite mode as an optional parameter when writing the savepoint similar to the example below:
{code:java}
SavepointWriter
    .newSavepoint(env, new HashMapStateBackend(), maxParallelism)
    .withOperator(OperatorIdentifier.forUid(""uid1""), transformation1)
    .withOperator(OperatorIdentifier.forUid(""uid2""), transformation2)
    .write(savepointPath, FileSystem.WriteMode.OVERWRITE); {code}
This coincides with the underlying writer class which explicitly declares the use of  `FileSystem.WriteMode.NO_OVERWRITE` within the `FileCopyFunction` class as seen below:
{code:java}
public final class FileCopyFunction implements OutputFormat<Path> {
    ...
    @Override
    public void writeRecord(Path sourcePath) throws IOException {
        Path destPath = new Path(path, sourcePath.getName());
        try (FSDataOutputStream os =
                        destPath.getFileSystem()
                                .create(destPath, FileSystem.WriteMode.NO_OVERWRITE);
                FSDataInputStream is = sourcePath.getFileSystem().open(sourcePath))
{             IOUtils.copyBytes(is, os);         }
    }
    ...
} {code}
An alternative solution might be to explicitly check for the existence of the file at the destination and deleting it, although the above seems much more elegant.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 11:59:44 UTC 2023,,,,,,,,,,"0|z1ffrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 06:41;Yanfei Lei;Hi [~rionmonster], I think it's a good idea to support Overwrite Mode for savepoint in State Processor API, and I'm a bit curious why State Processor API  is used to do savepoint periodically, is the job a batch job? 

If the job is a streaming job, have you tried to use  [rest api |https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-savepoints] to do savepoint periodically? 

 ;;;","30/Jan/23 06:58;masteryhx;It's a bit strange to descirbe whether a savepoint directory not a file is Overwirte.
It may cause some problems if just using this flag to control parts of files to be overwirted.

I'd like to suggest to keep every savepoint seprated by generating different paths, and manage the life cycle just like k8s operator does --- FLINK-26179

Could it meet your needs ?;;;","30/Jan/23 14:10;rionmonster;[~Yanfei Lei] 

This particular use case acts as a bootstrapping job for another separate Flink job. During a deployment, the bootstrap job uses the State Processor API to create a savepoint that is used by a different job. This process interacts with a few different persistent data stores and constructs the savepoints. I've found this has been a perfect use-case for the State Processor API, however when a new deployment is run, the bootstrapping job will fail since the state already exists at the target location.

The proposed solution (however it is accomplished) would support a bootstrap job such as this one to be run in a idempotent manner by simply overwriting the existing savepoint as opposed to throwing an IOException since the file already exists. ;;;","01/Feb/23 11:59;Yanfei Lei;Thanks for sharing your use case. I'm not sure if it's worth adding an option for this requirement, State Processor  module is relatively independent from flink. If the demand is urgent, maybe a local jar package can be compiled after modifying FileSystem.WriteMode.NO_OVERWRITE to  FileSystem.WriteMode.OVERWRITE.

Let's try to hear [~chesnay] ‘s  opinion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump protobuf-java dependency version for aws connectors,FLINK-30806,13521794,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,27/Jan/23 17:14,01/Feb/23 10:35,04/Jun/24 20:41,01/Feb/23 10:35,,,,,,,aws-connector-4.1.0,,,,,,,,,,,,,,,,Connectors / AWS,,,,,0,pull-request-available,,,,"Update protobuf-java dependency version to at least 3.19.6 to match version used in kinesis-client-library.

Use 3.21.7 to match version used in Flink if there are no compatibility issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 10:33:18 UTC 2023,,,,,,,,,,"0|z1ffow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 10:33;dannycranmer;Merged commit [{{639ab72}}|https://github.com/apache/flink-connector-aws/commit/639ab72085f9b907acf4257997de271a3b2270fc] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitEnumerator#handleSplitRequest() should be called automatically,FLINK-30805,13521792,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,echauchot,echauchot,27/Jan/23 17:06,30/Mar/23 08:26,04/Jun/24 20:41,30/Mar/23 08:26,,,,,,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,pull-request-available,,,,"SplitEnumerator#handleSplitRequest() is not called automatically by the new source framework which could be surprising to a source author. Right now a source author would have to call it himself when a split is finished or early when the reader gets created. 
IMHO it would be good if we could find a way for the framework to call it when a split is finished automatically",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 02:55:01 UTC 2023,,,,,,,,,,"0|z1ffog:",9223372036854775807,as discussed in the PR,,,,,,,,,,,,,,,,,,,"08/Mar/23 14:23;echauchot;[~MartijnVisser] can you assign this ticket to me ?;;;","15/Mar/23 02:55;mason6345;[~echauchot] left a comment, I'm not sure this change makes sense;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator helm chart - allow specifying operator pod resources requests/limits,FLINK-30804,13521784,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,VinceChenal,VinceChenal,27/Jan/23 15:29,30/Jan/23 09:44,04/Jun/24 20:41,30/Jan/23 09:13,kubernetes-operator-1.3.1,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,flink-kubernetes-operator helm chart doe snot allow specifying resources requests/limits for operator pod. It would be nice having such configuration.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 09:13:10 UTC 2023,,,,,,,,,,"0|z1ffmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 09:13;gyfora;merged to main 1a9aae4f45c042476b81c8344038893fd6535c4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink mishandles script dependencies,FLINK-30803,13521780,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,nuafonso,nuafonso,27/Jan/23 15:04,02/Feb/23 06:13,04/Jun/24 20:41,02/Feb/23 06:13,1.15.2,1.15.3,1.16.0,,,,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"h2. Summary

Since Flink 1.15, PyFlink is unable to run scripts that import scripts under other directories. For instance, if _main.py_ imports {_}job/word_count.py{_}, PyFlink will fail due to not finding the _job_ directory.

The issue seems to have started after a [refactoring of _PythonDriver_|https://github.com/apache/flink/commit/330aae0c6e0811f50888d17830f10f7a29efe7d7] to address FLINK-26847. The path to the Python script is removed, which forces PyFlink to use the copy in its temporary directory. When files are copied to this directory, the original directory structure is not maintained and ends up breaking the imports.
h2. Testing

To confirm the regression, I ran the attached application in both Flink 1.14.6 and 1.15.3 clusters.
h3. Flink 1.14.6

Application was able to start after being submitted via CLI:

 
{code:java}
% ./bin/flink run --python ~/sandbox/word_count_split/main.py
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/.../flink-1.14.6/lib/flink-dist_2.12-1.14.6.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Job has been submitted with JobID 6f7be21072384ca3a314af10860c4ba8 {code}
 
h3. Flink 1.15.3

Application did not start due to not finding the _job_ directory:

 
{code:java}
% ./bin/flink run --python ~/sandbox/word_count_split/main.py
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/tmp/pyflink/40c649c3-24af-46ef-ae27-e0019cb55769/3673dd18-adff-40e0-bb11-06a3f00ba29c/main.py"", line 5, in <module>
    from job.word_count import word_count
ModuleNotFoundError: No module named 'job'
org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:140)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:841)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:240)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1085)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1163)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1163)
Caused by: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:130)
        ... 13 more {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:46;nuafonso;word_count_split.zip;https://issues.apache.org/jira/secure/attachment/13054841/word_count_split.zip",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 06:13:02 UTC 2023,,,,,,,,,,"0|z1ffls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/23 15:56;nuafonso;Hello [~dianfu],

I am wondering if you can provide some context about your changes for FLINK-26847.
The quickest fix would be to keep the path to the script, but it looks like it might revert the fix to YARN.

Thank you,

Nuno;;;","29/Jan/23 03:14;dianfu;[~nuafonso]
Could you try if the following command works:
./bin/flink run --python ~/sandbox/word_count_split/main.py --pyFiles ~/sandbox/word_count_split;;;","30/Jan/23 17:37;nuafonso;Hello [~dianfu],

Thank you for the quick reply.

I can confirm that the application is able to run after adding the _--pyFiles_ argument.
{code:java}
[flink-1.15.3]  
% ./bin/flink run --python ~/sandbox/word_count_split/main.py --pyFiles ~/sandbox/word_count_split
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/.../flink-1.15.3/lib/flink-dist-1.15.3.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Job has been submitted with JobID b8e00aa7a58d1622cf8df25d414c8a0a{code}
Thank you,

Nuno;;;","31/Jan/23 07:19;dianfu;[~nuafonso] Thanks for the confirmation. You could work around it using the above approach. I will figure out if there is a better way to handle this.;;;","31/Jan/23 12:55;dianfu;[~nuafonso] Let me explain a bit more about the changes made in FLINK-26847. It's introduced to make sure `-py` works in YARN application mode. That's, users could submit PyFlink jobs using the following command:
{code:java}
./bin/flink run-application -t yarn-application \
-Djobmanager.memory.process.size=1024m \
-Dtaskmanager.memory.process.size=1024m \
-Dyarn.application.name=<ApplicationName> \
-Dyarn.ship-files=/path/to/shipfiles \
-pyarch shipfiles/venv.zip \
-pyclientexec venv.zip/venv/bin/python3 \
-pyexec venv.zip/venv/bin/python3 \
-py shipfiles/word_count.py {code}

The reason is that, for {*}-py shipfiles/word_count.py{*}, as it will create a temporary directory as the working directory of the Python process which is responsible to compile the job, so the path `shipfiles/word_count.py ` will not be found during compiling as it's not inside the working directory.

After rethinking about this issue, I think a better approach would be let users to use `-pym` explicitly in YARN application mode. So I'd like to revert the changes made in FLINK-26847. After that, users should submit PyFlink jobs using the following command in YARN application mode:
{code:java}
./bin/flink run-application -t yarn-application \
      -Djobmanager.memory.process.size=1024m \
      -Dtaskmanager.memory.process.size=1024m \
      -Dyarn.application.name=<ApplicationName> \
      -Dyarn.ship-files=/path/to/shipfiles \
      -pyarch shipfiles/venv.zip \
      -pyclientexec venv.zip/venv/bin/python3 \
      -pyexec venv.zip/venv/bin/python3 \
      -pyfs shipfiles \
      -pym word_count {code}
 ;;;","02/Feb/23 06:13;dianfu;Fixed in:
- master via 68c78dcd0a17f3d13d82b09b61246808e85961c8
- release-1.16 via 592815cec5ed5f33ae503194fccfe88c7df2eaa5
- release-1.15 via a2a7b664e827118c2d54a936e010c63b8acb2668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document that SplitReader#fetch() may exit early and resume later,FLINK-30802,13521762,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,27/Jan/23 14:21,14/Mar/23 11:31,04/Jun/24 20:41,14/Mar/23 11:30,,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Common,Documentation,,,,0,pull-request-available,,,,"[SplitReader#fetch()|https://nightlies.apache.org/flink/flink-docs-master/api/java/] lacks details on the fact that source authors can decide to interrupt it (for ex for performance reasons) for it to be resumed later based on its state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 11:30:53 UTC 2023,,,,,,,,,,"0|z1ffhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 11:30;chesnay;master: 17b805bc9afcca8776a46c15c3785d9df067ec7e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate LeaderElection-related unit tests to JUnit5,FLINK-30801,13521721,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,27/Jan/23 11:24,07/Feb/23 14:07,04/Jun/24 20:41,07/Feb/23 14:07,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"To prepare the merge of the {{MultipleComponentLeaderElectionService}}-related tests with the legacy test, we want to align the JUnit versin they are using.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 14:07:56 UTC 2023,,,,,,,,,,"0|z1ff8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 14:07;mapohl;master: cba2e1e433e7fa0674834daed7823f6b4fa231b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc entry how to re-generate java doc,FLINK-30800,13521704,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,27/Jan/23 08:10,30/Jan/23 09:14,04/Jun/24 20:41,30/Jan/23 09:14,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,At the moment only the CI notifies ppls.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 09:14:44 UTC 2023,,,,,,,,,,"0|z1ff54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 09:14;gyfora;merged to main 6d0bda96b7d9c51464c9ee499a88d9df3362807f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make SinkFunction support speculative execution for batch jobs,FLINK-30799,13521684,13519977,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,SleePy,SleePy,SleePy,27/Jan/23 02:48,29/Jan/23 13:20,04/Jun/24 20:41,29/Jan/23 13:20,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In this ticket, it would make SinkFunction based sink run with speculative execution for batch jobs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 13:20:42 UTC 2023,,,,,,,,,,"0|z1ff0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 13:20;zhuzh;Done via:
6e445064f6a20caed16b99b3d06398a51c404e84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make OutputFormat support speculative execution for batch jobs,FLINK-30798,13521683,13519977,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,SleePy,SleePy,SleePy,27/Jan/23 02:47,14/Dec/23 15:15,04/Jun/24 20:41,31/Jan/23 02:53,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,This issue would make OutputFormat based sink run with speculative execution for batch jobs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31167,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 02:53:46 UTC 2023,,,,,,,,,,"0|z1ff0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 02:53;zhuzh;Done via
642d28ec83c915ddc29037737d89f6698f99161e
f6694ca057ed01221968e4fdedec9dc4e3fc62e5
9a5c2dbb85f1143c2ae24f92cd7033d8e35c69d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump json5 from 1.0.1 to 1.0.2,FLINK-30797,13521662,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,26/Jan/23 20:21,26/Jan/23 20:23,04/Jun/24 20:41,26/Jan/23 20:23,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"Dependabot has created https://github.com/apache/flink/pull/21617

This is the corresponding Jira ticket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 26 20:23:27 UTC 2023,,,,,,,,,,"0|z1fevs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/23 20:23;martijnvisser;Fixed in master: eb8a33fb0cb7a07443df835a3142a1a139206ed6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Slf4jReporter less noisy when no/few metrics exist,FLINK-30796,13521617,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,26/Jan/23 13:39,31/Jan/23 11:52,04/Jun/24 20:41,31/Jan/23 11:52,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 11:52:04 UTC 2023,,,,,,,,,,"0|z1fels:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 11:52;chesnay;master: e856bd8b5f834ad2dfd784940cc6186fde5b6c3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingWithStateTestBase of win not correct,FLINK-30795,13521616,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,JinxinTang,JinxinTang,26/Jan/23 13:22,10/Feb/23 11:32,04/Jun/24 20:41,10/Feb/23 11:32,,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,,,,"Windows path such as

""

[file://C:\Users\xx\AppData\Local\Temp\junit373749850266957074\junit7014045318909690439|file:///C:/UsersxxAppDataLocalTempjunit373749850266957074junit7014045318909690439]

""

will throw 

 
new IllegalArgumentException(""Cannot use the root directory for checkpoints."");","Windows path such as

""

file://C:\Users\xx\AppData\Local\Temp\junit373749850266957074\junit7014045318909690439

""

will throw 

 
new IllegalArgumentException(""Cannot use the root directory for checkpoints."");",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 11:32:31 UTC 2023,,,,,,,,,,"0|z1felk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 11:32;chesnay;We no longer account for developing Flink on Windows. Only unix environments are supported.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable dependency convergence check for night connector builds,FLINK-30794,13521597,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,26/Jan/23 09:32,28/Apr/23 14:04,04/Jun/24 20:41,28/Apr/23 14:04,,,,,,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,,,,,,,,,,,,,,,,FLINK-31416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-26 09:32:45.0,,,,,,,,,,"0|z1fehc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink YARN per-job on Docker test fails on Azure due to permission issues,FLINK-30793,13521589,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,26/Jan/23 07:50,04/Apr/24 15:59,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Hadoop Compatibility,Deployment / YARN,,,,0,auto-deprioritized-major,test-stability,,,"The following build failed due to some hdfs/yarn permission issues in  PyFlink YARN per-job on Docker e2e test:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=10587

{code}
[...]
Jan 26 02:17:31 23/01/26 02:12:20 FATAL hs.JobHistoryServer: Error starting JobHistoryServer
Jan 26 02:17:31 org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://master.docker-hadoop-cluster-network:9000/tmp/hadoop-yarn/staging/history/done]
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.tryCreatingHistoryDirs(HistoryFileManager.java:698)
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.createHistoryDirs(HistoryFileManager.java:634)
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:595)
Jan 26 02:17:31 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:96)
Jan 26 02:17:31 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Jan 26 02:17:31 	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:152)
Jan 26 02:17:31 	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:228)
Jan 26 02:17:31 	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:238)
Jan 26 02:17:31 Caused by: org.apache.hadoop.security.AccessControlException: Permission denied: user=mapred, access=WRITE, inode=""/"":hdfs:hadoop:drwxr-xr-x
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:350)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:251)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:189)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1756)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1740)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1699)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3007)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1141)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:659)
Jan 26 02:17:31 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24434,,,,,,,,,"04/Apr/24 15:59;rskraba;log.txt;https://issues.apache.org/jira/secure/attachment/13067860/log.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 04 15:59:18 UTC 2024,,,,,,,,,,"0|z1fefk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/23 07:51;mapohl;I'm linking FLINK-24434 because it has the same stacktrace. But  with FLINK-24434 the cause seems to be disk usage of 100% which is not the case here.;;;","10/Feb/23 07:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45970&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=11207;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","04/Apr/24 15:59;rskraba;1.20 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58716&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=13845

Note that the final symptom in the logs is a bit misleading:

{code}
24/04/04 00:12:50 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.18.0.3:8032
24/04/04 00:12:50 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.18.0.3:10200
YARN application ID: 
24/04/04 00:12:51 INFO client.RMProxy: Connecting to ResourceManager at master.docker-hadoop-cluster-network/172.18.0.3:8032
24/04/04 00:12:51 INFO client.AHSProxy: Connecting to Application History server at master.docker-hadoop-cluster-network/172.18.0.3:10200
options parsing failed: Missing argument for option: applicationId
Apr 04 00:12:52 
Apr 04 00:12:52 ==== YARN application logs begin ====
Apr 04 00:12:52 Retrieve logs for YARN applications.
Apr 04 00:12:52 usage: yarn logs -applicationId <application ID> [OPTIONS]
Apr 04 00:12:52 
{code}

There actually isn't a missing argument -- the application was never run.  This might be an opportunity to add a better error message in the test script, but that isn't a priority;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up not uploaded state changes after materialization complete,FLINK-30792,13521574,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Feifan Wang,Feifan Wang,26/Jan/23 02:07,17/Feb/23 02:23,04/Jun/24 20:41,17/Feb/23 02:23,1.16.0,,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"We should clean up not uploaded state changes after materialization completed, otherwise it may cause FileNotFoundException.

Since state changes before completed materialization in FsStateChangelogWriter#notUploaded will not be used in any subsequent checkpoint, I suggest clean up it while handle materialization result. 

How do you think about this ? [~ym] , [~roman] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/23 03:30;Yanfei Lei;image-2023-02-03-11-30-40-198.png;https://issues.apache.org/jira/secure/attachment/13055074/image-2023-02-03-11-30-40-198.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 06:32:06 UTC 2023,,,,,,,,,,"0|z1fec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 07:34;masteryhx;IIUC, notuploaded will be truncated after materialization competed and checkpoint subsuming currently in theory ?

Do you have seen some exceptions that could be shared ?;;;","31/Jan/23 02:41;Feifan Wang;Hi [~masteryhx] , you are right, but my suggestion is to delete the state changes before the materialization trigger in notuploaded after the materialization is completed. Otherwise this part of the state change will be uploaded to the same file with subsequent state changes when triggering an pre-emptive upload (with {_}BatchingStateChangeUploadScheduler{_}) . This is one of the causes of changelog file not found.

From the implementation of FsStateChangelogWriter and BatchingStateChangeUploadScheduler, there should be another situation that will cause the changelog file to not be found.

I will try to write test cases to reproduce these two cases.;;;","03/Feb/23 03:30;Yanfei Lei;[~Feifan Wang] Thanks for the investigation, after discussing offline, this case would indeed result in FileNotFound.

As described in https://issues.apache.org/jira/browse/FLINK-28440?focusedCommentId=17682912&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17682912 , this will cause FileNotFound may be encountered in all checkpoints after chk-4 and before the next materialization.

We are going to use activeSqn directly during pre-emptive to avoid changesets that have been materialized but not uploaded from being uploaded and then deleted.

!image-2023-02-03-11-30-40-198.png|width=661,height=155!;;;","09/Feb/23 15:58;roman;Do I understand correctly that the issue is the same as in FLINK-28440?

Couldn't we rely on the same solution then (ref counting state changes per state handle)?

 

Per my understanding, if materialization fails or is too slow; then the next checkpoint will have to upload those state changes (before activeSqn), causing performance regression, won't it?;;;","10/Feb/23 06:32;Feifan Wang;Thanks for your reply [~roman] , you are right, ref counting state changes per state handle can indeed solve the problem of changelog file not found mentioned above. The changes in this PR are only intended to reduce useless data uploads.

As for the performance regression problem you mentioned, I really didn't think about it carefully before. Now I'm also not sure if this is causing a performance regression. On the one hand, this will indeed reduce the amount of data uploaded; on the other hand, it will indeed require more data to be uploaded when the checkpoint is triggered.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Codespeed machine is not responding,FLINK-30791,13521536,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,pnowojski,pnowojski,25/Jan/23 16:43,25/Jan/23 20:06,04/Jun/24 20:41,25/Jan/23 18:29,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,,,,,"Neither speedcenter: [http://codespeed.dak8s.net:8000/]

nor jenkins: [http://codespeed.dak8s.net:8080|http://codespeed.dak8s.net:8080/]

are responding. Both services were hosted on the same EC2 machine from Ververica's AWS account.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 20:06:53 UTC 2023,,,,,,,,,,"0|z1fe40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/23 16:56;jingge;Thanks for creating this ticket. We are working on it.;;;","25/Jan/23 17:46;jingge;the machine is running with no issue, there are some issues with the dns.;;;","25/Jan/23 18:29;jingge;fixed, the dns is back.;;;","25/Jan/23 20:06;pnowojski;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Connectors/Jdbc] Refactor of testings,FLINK-30790,13521532,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eskabetxe,eskabetxe,eskabetxe,25/Jan/23 16:27,24/May/23 09:42,04/Jun/24 20:41,24/May/23 09:42,,,,,,,jdbc-3.2.0,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"This is one attempt to get better results executing tests on jdbc connector..

In current master branch it takes like 15m to execute all tests on CI (it will vary from 10m to 20m)

 
{code:java}
[INFO] Reactor Summary for Flink : Connectors : JDBC : Parent 3.1-SNAPSHOT:
[INFO] 
[INFO] Flink : Connectors : JDBC : Parent ................. SUCCESS [ 22.019 s]
[INFO] Flink : Connectors : JDBC .......................... SUCCESS [13:45 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  14:07 min
[INFO] Finished at: 2022-12-23T09:22:46Z {code}
 

 

The main problems that we see on testings are:
 * Parameterized tests that setup database containers (this tends to be more time consuming as another databases are added)
 * Creation of containers by class (this is fine as we want test to be independents, but the database could be setup 1time and cleaned at end of each class)
 * No easy way to extend a test to another database, at the end we copy a lot of code
 * A lot of code for create and populate tables for testing. We have JdbcTestBase that use JdbcTestFixture to create a kind of 'Book Store', but this is used in a lot of test without the use of all tables that are implemented on store. 

 

 ",,,,,,,,,,,,,,,,,,,FLINK-31770,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 09:42:16 UTC 2023,,,,,,,,,,"0|z1fe34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 22:08;eskabetxe;h3. Change log level to off on CI

This reduce the flood of logs from more than 40.000 lines to less than 400 lines.. reducing the time to less than 10m

If debug is needed the log can be activated on the corresponding branch

Commits: [[FLINK-30790] Change log level to off on CI|https://github.com/apache/flink-connector-jdbc/pull/18/commits/a6ab3e73222af6d5f3824e12aaece22158fac1d1]
{code:java}
[INFO] Flink : Connectors : JDBC : Parent ................. SUCCESS [ 21.437 s]
[INFO] Flink : Connectors : JDBC .......................... SUCCESS [09:15 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  09:37 min
[INFO] Finished at: 2023-01-28T21:59:24Z {code};;;","30/Jan/23 11:48;eskabetxe;h3. Refactor parametrised class with multiple containers

This should reduce the time to start this classes, spreading the time for multiple classes.. This change will have little effect, but is important to improve latter the time and number of times container will be created.

Also will allow to define basic extension rules to tests that we want implemented on all databases.

Commits:

[[FLINK-30790] Refactor JdbcDataTypeTest create tests by dialect|https://github.com/apache/flink-connector-jdbc/pull/18/commits/31aae786b67f734af27572579630172a6ce37ec0]

[[FLINK-30790] Refactor JdbcExactlyOnceSinkE2eTest create tests by dialect|https://github.com/apache/flink-connector-jdbc/pull/18/commits/3db75f5e7c281a1bdd42ab96afe17906ab6245b0]

[[FLINK-30790] Refactor MySqlCatalogITCase create tests by database version|https://github.com/apache/flink-connector-jdbc/pull/18/commits/12b51a19c1e615481f0b9b94a017a2c1ef0a6c11]

 
{code:java}
[INFO] Flink : Connectors : JDBC : Parent ................. SUCCESS [ 24.647 s]
[INFO] Flink : Connectors : JDBC .......................... SUCCESS [08:41 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  09:16 min
[INFO] Finished at: 2023-01-30T11:37:35Z
[INFO] ------------------------------------------------------------------------ {code};;;","13/Feb/23 12:56;martijnvisser;Fixed in main:

45306bbdeb8f056b6c3cab89e7ee27a573672025 until 753ea0e87d05e0eb7b4419cd1653bd36ba986aa5;;;","13/Feb/23 13:45;martijnvisser;Re-opening since there are more upcoming changes to complete this ticket;;;","24/May/23 09:42;martijnvisser;Fixed in main: b6991d8e888ed3504fefcbedf8a47277c5e088f9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge MultipleComponentsLeaderElectionDriver methods notifyAllKnownLeaderInformation and notifyLeaderInformationChange into a single method and ,FLINK-30789,13521531,13542119,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,25/Jan/23 16:27,10/Jul/23 07:57,04/Jun/24 20:41,10/Jul/23 07:57,1.17.0,,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,The new interface proposed in FLIP-285 shall provide only a single method for writing data into a the HA backend.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-25 16:27:14.0,,,,,,,,,,"0|z1fe2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor redundant code in AbstractHaServices ,FLINK-30788,13521506,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,25/Jan/23 15:15,07/Feb/23 10:51,04/Jun/24 20:41,07/Feb/23 10:50,1.17.0,1.18.0,,,,,1.18.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"{{AbstractHaServices.createLeaderElectionService}} returns {{{}LeaderElectionService{}}}. All implementations return {{{}DefaultLeaderElectionService{}}}. The actual implementation-specific code creates the {{{}LeaderElectionDriverFactory{}}}.

We can remove the redundant code here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30883,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 10:50:53 UTC 2023,,,,,,,,,,"0|z1fdxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 10:50;mapohl;master: 54967e9124b668448faac018e8fa339779426d45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dmesg fails to save data to file due to permissions,FLINK-30787,13521356,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,25/Jan/23 11:39,30/Jan/23 11:51,04/Jun/24 20:41,30/Jan/23 11:50,1.15.3,1.16.0,1.17.0,,,,1.17.0,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,pull-request-available,,,,"We're not collecting the {{dmesg}} output due to a permission issue in any build:
{code}
2023-01-12T10:10:25.1598207Z dmesg: read kernel buffer failed: Operation not permitted
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13978,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 11:50:52 UTC 2023,,,,,,,,,,"0|z1fd00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/23 11:41;mapohl;I linked the FLINK-19699 PR that introduced this feature in the CI pipeline. [~rmetzger] can you recall whether we checked that it actual worked? Or can we assume that it was broken from the start and nobody noticed before?;;;","25/Jan/23 11:49;rmetzger;I'm pretty sure this worked before. Maybe it only worked on the alibaba machines, not on the Azure machines (or vice versa);;;","25/Jan/23 12:13;mapohl;good point, I'm gonna double-check;;;","30/Jan/23 11:50;mapohl;Backports were omitted because the issue actually doesn't create any harm besides creating an error message.

master: 5f5abb69187524de9a2d0e1f6380bed3d69e3fa1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support merge by name for podTemplate array fields,FLINK-30786,13521255,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,25/Jan/23 09:56,29/Mar/23 14:18,04/Jun/24 20:41,29/Mar/23 14:18,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The operator currently merges the hierarchical pod template array fields (containers, volumes, volume mounts etc) by index.

In many cases these array fields already have a name attribute that could be used to merge by name. We should allow this configurable option.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 14:18:53 UTC 2023,,,,,,,,,,"0|z1fcdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 14:12;applike-ss;I am also facing this issue when using the flink kubernetes operator.

I was trying to add a persistent volume claim and its volume to the task manager template, because our task manager recently got evicted due to use of local disk space.;;;","29/Mar/23 14:18;gyfora;merged to main 625b5ae48b8a57bb77511ca1ead09d466de5e8eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB Memory Management end-to-end test failed due to unexpected exception,FLINK-30785,13521245,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,mapohl,mapohl,25/Jan/23 09:09,27/Nov/23 09:23,04/Jun/24 20:41,10/Feb/23 09:40,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,test-stability,,,"We see a test instability with {{RocksDB Memory Management end-to-end test}}. The test failed because an exception was detected in the logs:
{code}
2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...
2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.
2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...
2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:
[...]
2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...
2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.
2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 
2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

The only exception being reported in the Flink logs is due to a warning:
{code}
2023-01-25 02:47:38,242 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5117",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33661,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 09:40:40 UTC 2023,,,,,,,,,,"0|z1fcbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 14:05;mapohl;[~roman] [~yuanmei] can someone look into this issue?;;;","30/Jan/23 15:28;roman;Thanks [~mapohl] , 

The test failed because the logs contained exceptions. Those are CheckpointException and CancellationException.

According to [this pattern|https://github.com/apache/flink/blob/c9e87fe410c42f7e7c19c81456d4212a58564f5e/flink-end-to-end-tests/test-scripts/common.sh#L405], they both should be ignored.

However, one of the stacktraces contained ""completeExceptionally"", which I think is not ignored:
{code:java}
  2023-01-25 02:47:38,530 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: EventSource -> Timestamps/Watermarks (1/2)#0 - asynchronous part of checkpoint 1 could
   not be completed.
  java.util.concurrent.CancellationException: null
          at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276) ~[?:1.8.0_352]
          at org.apache.flink.runtime.state.StateUtil.discardStateFuture(StateUtil.java:78) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.lambda$cancel$0(OperatorSnapshotFutures.java:173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.shaded.guava30.com.google.common.io.Closer.close(Closer.java:213) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.cancel(OperatorSnapshotFutures.java:185) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.cleanup(AsyncCheckpointRunnable.java:391) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.close(AsyncCheckpointRunnable.java:356) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:295) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.util.IOUtils.closeAllQuietly(IOUtils.java:282) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:547) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$cancel$10(StreamTask.java:985) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_352]
          at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_352]
          at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_352]
          at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_352]
          at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:344) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]{code}
So this looks like a false positive. I think we should add ""completeExceptionally"" to the ignore pattern.

WDYT?

 

cc: [~chesnay] ;;;","30/Jan/23 15:40;mapohl;Good point. That sounds reasonable.;;;","10/Feb/23 09:40;roman;Merged as d7bbb763ded6c0c8cd99560eeafbfd4ce800a8cf into master,

as 05e687cdea804b74534b991057b40b100aefefb3 into 1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceITCase.testPartitionFilter  failed with assertion error,FLINK-30784,13521235,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,25/Jan/23 08:47,20/Feb/24 07:53,04/Jun/24 20:41,,1.17.0,1.18.1,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,auto-deprioritized-critical,test-stability,,,"We see a test failure in {{HiveTableSourceITCase.testPartitionFilter}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=23909

{code}
Jan 25 01:14:55 [ERROR] org.apache.flink.connectors.hive.HiveTableSourceITCase.testPartitionFilter  Time elapsed: 2.212 s  <<< FAILURE!
Jan 25 01:14:55 org.opentest4j.AssertionFailedError: 
Jan 25 01:14:55 
Jan 25 01:14:55 Expecting value to be false but was true
Jan 25 01:14:55 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jan 25 01:14:55 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Jan 25 01:14:55 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jan 25 01:14:55 	at org.apache.flink.connectors.hive.HiveTableSourceITCase.testPartitionFilter(HiveTableSourceITCase.java:314)
Jan 25 01:14:55 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

There's a similar test stability issue still open with FLINK-20975. The stacktraces don't match. That's why I decided to open a new one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20975,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 07:53:12 UTC 2024,,,,,,,,,,"0|z1fc94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Jan/24 15:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56672&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22161;;;","20/Feb/24 07:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57626&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22553;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add pull request template for AWS Connectors repo,FLINK-30783,13521227,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,liangtl,liangtl,liangtl,25/Jan/23 08:07,27/Jan/23 14:48,04/Jun/24 20:41,27/Jan/23 14:48,,,,,,,aws-connector-4.1.0,,,,,,,,,,,,,,,,Connectors / AWS,,,,,0,pull-request-available,,,,"Add a pull request template for Apache Flink AWS Connectors [https://github.com/apache/flink-connector-aws]

 

This improves our pull request and commit quality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 27 14:47:40 UTC 2023,,,,,,,,,,"0|z1fc7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:47;dannycranmer;Merged commit [{{81c175e}}|https://github.com/apache/flink-connector-aws/commit/81c175e9fb7ede2d387b016b75a2e6f4df8eebac] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use https for schemaLocations,FLINK-30782,13521172,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,Sergey Nuyanzin,Sergey Nuyanzin,24/Jan/23 21:47,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-minor,pull-request-available,,,"In poms 
{code:xml}
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
{code}
use https for xsd like  https://maven.apache.org/xsd/maven-4.0.0.xsd",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:03 UTC 2023,,,,,,,,,,"0|z1fbv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add completed checkpoint check to cluster health check,FLINK-30781,13521145,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,24/Jan/23 16:58,30/Jan/23 14:51,04/Jun/24 20:41,30/Jan/23 14:51,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 14:51:27 UTC 2023,,,,,,,,,,"0|z1fbp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 14:51;gyfora;merged to main fc1b901c42fcd838f12254e2b47dd471f5eef8b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Develop ElasticCatalog to connect Flink with Elastic tables and ecosystem,FLINK-30780,13521125,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,grzegorz.kolakowski,grzegorz.kolakowski,24/Jan/23 14:17,24/Jan/23 14:19,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,,,,,"As a follow-up to the introduction of Elastic Dialect, we may implement a catalog which maps Elastic indices and datastreams to Flink SQL tables.",,,,,,,,,,,FLINK-30702,,,,,,,,,,FLINK-15350,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-24 14:17:42.0,,,,,,,,,,"0|z1fbkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump josdk version to 4.2.6,FLINK-30779,13521122,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,24/Jan/23 13:50,09/Feb/23 07:04,04/Jun/24 20:41,09/Feb/23 07:04,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,We should upgrade to the latest version to get some important improvements,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-24 13:50:02.0,,,,,,,,,,"0|z1fbk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Configuration into FunctionContext,FLINK-30778,13521104,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,smiralex,smiralex,24/Jan/23 10:25,24/Jan/23 15:33,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,,0,,,,,"Currently {color:#000000}UserDefinedFunction {color}{color:#000000}is opened with instance of {color}{color:#000000}FunctionContext to obtain global runtime information about the context in which the function is executed. It provides job parameters by key, but doesn't provide Configuration object as a whole. However, user might want to open another Flink entities in {color}{color:#000000}UserDefinedFunction{color}{color:#000000}, such as InputFormats, SourceFunctions, etc., which requires Configuration in their #open method. {color}

{color:#000000}
Practical example is {color}{color:#000000}CachingLookupFunction with enabled Full caching, which need to open InputFormat that was passed from {color}{color:#000000}FullCachingLookupProvider. Currently there is no way to open InputFormat in FullCachingLookupProvider with Configuration corresponding to actual job parameters.
{color}

{color:#000000} {color}

{color:#000000} {color}
 
{color:#000000} {color}

{color:#000000} {color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 15:33:33 UTC 2023,,,,,,,,,,"0|z1fbg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 15:33;twalthr;Even though `Configuration` is not deprecated, we should avoid using this class going forward. `ReadableConfig` is the right abstraction to be passed as it allows for layered configuration, easy filtering and other transforms. We should rather try to improve downstream interfaces do not require a full `Configuration` anymore. This class is potentially mutable which makes not much sense.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Kinesis Table API Connector to specify shard assigner,FLINK-30777,13521049,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,karthitect,karthitect,karthitect,24/Jan/23 00:07,16/Mar/23 11:11,04/Jun/24 20:41,16/Mar/23 11:11,,,,,,,aws-connector-4.1.0,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"As a follow up to the fix involving UniformShardAssigner (https://issues.apache.org/jira/browse/FLINK-24639), it would be good to allow users to pick something other than the default shard assigner, such as the recently introduced UniformShardAssigner, in the Kinesis Table API Connector. 

This can be accomplished by introducing a new connector option called 'shard-assigner.' Something like below; note the introduction of a new option called 'shard-assigner'

 
 
{{CREATE TABLE KinesisTable}}

( 

`user_id` BIGINT, 

{{`item_id` BIGINT,}}

{{`category_id` BIGINT,}}

{{`behavior` STRING,}}

{{`ts` TIMESTAMP(3))}}

{{PARTITIONED BY (user_id, item_id)}}

{{WITH}}

{{(}}

{{'connector' = 'kinesis',}}

{{'stream' = 'user_behavior',}}

{{'aws.region' = 'us-east-2',}}

{{'scan.stream.initpos' = 'LATEST',}}

{{'format' = 'csv',}}

{{'shard-assigner' = 'uniform'}}

{{);}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 22:32:22 UTC 2023,,,,,,,,,,"0|z1fb48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 22:32;dannycranmer;Merged commit [{{7477266}}|https://github.com/apache/flink-connector-aws/commit/747726631dd0f9eec004db6f5e8ef68e747cf2e4] into main (AWS 4.1.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move autoscaler code into a separate module and make it pluggable,FLINK-30776,13521016,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,23/Jan/23 16:43,15/Feb/23 11:04,04/Jun/24 20:41,15/Feb/23 11:04,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The autoscaling code is tightly coupled with the main operator code. In an effort to make it pluggable or run independently, we should move the code into a dedicated Maven module.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30775,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-23 16:43:58.0,,,,,,,,,,"0|z1fax4:",9223372036854775807,Allow plugging in custom autoscaler implementations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support pluggable reconciler modules,FLINK-30775,13521015,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mxm,mxm,23/Jan/23 16:39,15/Feb/23 14:45,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,Users may want to perform custom reconciler actions in the operator depending on their setup. We can achieve this via pluggable / configurable reconciler classes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30776,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-23 16:39:25.0,,,,,,,,,,"0|z1faww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce flink-utils module,FLINK-30774,13521013,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,23/Jan/23 16:24,12/Sep/23 22:35,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Build System,,,,,0,auto-deprioritized-major,starter,,,"Currently, utility methods generic utility classes like {{Preconditions}} or {{AbstractAutoCloseableRegistry}} are collected in {{flink-core}}. The flaw of this approach is that we cannot use those classes in modules like {{fink-migration-test-utils}}, {{flink-test-utils-junit}}, {{flink-metrics-core}} or {{flink-annotations}}.

We might want to have a generic {{flink-utils}} analogously to {{flink-test-utils}} that collects Flink-independent utility functionality that can be access by any module {{flink-core}} is depending on to make this utility functionality available in any Flink-related module.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 22:35:06 UTC 2023,,,,,,,,,,"0|z1fawg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 03:59;Wencong Liu;Hello [~mapohl]. I think your proposal is reasonable! But I have a question. Currently many utility classes exists in the path ""flink-core/src/main/java/org/apache/flink/util"". Some of them are only used in the module flink-core, others are used in the modules depending on flink-core. How do we sift the classes moved to the flink-utils module? Or we just simply move all of them to the module flink-util.;;;","24/Jan/23 07:15;mapohl;I guess that could be done on a use-case basis. Like we have utils packages in other classes we could have a dedicated utils package for certain classes within flink-core as well. If they are not necessarily used in other packages. I came across this issue when reviewing FLINK-27518. There I noticed that we cannot use Preconditions in the newly added module {{flink-migration-tests}} rightaway.

{{flink-utils}} should be a module that does have plain Java code utility methods/classes that are useful in multiple modules. I would let the usercode-classloader related code, for instance, leave in {{flink-core}} as this is something that closer to the actual Flink functionality. 

[~chesnay] do you see problems adding such a module?;;;","24/Jan/23 10:05;chesnay;The idea of splitting flink-core has been has been floating for quite a while, and I'd go a step further than you proposed and move everything that isn't tied to the processing APIs into a separate module.
Classloader stuff should go into this module since it is relevant for anything loaded with plugins. The core goal here is to not have auxiliary features rely on modules related to data processing.;;;","27/Jun/23 06:25;alokh;Hi folks. Would love to work on this in case we want to go ahead with this.;;;","06/Jul/23 07:08;mapohl;Hi [~alokh] thanks for volunteering. There's a discussion about Flink 2.0 going right now (see [corresponding wiki page|https://cwiki.apache.org/confluence/display/FLINK/2.0+Release] and [the mailing list thread|https://lists.apache.org/thread/l3dkdypyrovd3txzodn07lgdwtwvhgk4]) which might also include reorganizing the module structure of Flink in some way. You might want to follow the discussion on the mailing list regarding this and other similar contributions. Therefore, I suggest to wait with working on this issue until the next steps are clarified.;;;","04/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","12/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add API for rescaling of jobs based on per-vertex parallelism overrides,FLINK-30773,13521010,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,23/Jan/23 16:02,28/Apr/23 10:04,04/Jun/24 20:41,28/Apr/23 10:04,,,,,,,1.18.0,,,,,,,,,,,,,,,,Autoscaler,Runtime / Coordination,Runtime / REST,,,0,,,,,"FLINK-29501 introduced a way to rescale jobs via a user-provided parallelism overrides map. This feature is already used today by the Autoscaler of the Flink Kubernetes operator. However, it requires a full restart of the Flink job and only supports the application deployment mode.

In a K8s environment, this is inefficient because all pods for a deployment will be surrendered. Upon restart, they have to be re-acquired. In addition to being slow, this can also lead to situations where resource constraints prevent a restart from executing properly.

Ideally, we would would want the following to happen on receiving a rescale request:
 # Rescale API receives a request with a parallelism overrides map (vertexId => parallelism) for a jobId
 # Compute the number of required task slots using the overrides and the current JobGraph
 ## If the total number of task slots for the cluster is less than the required number of task slots of the rescale, acquire the missing task slots. Otherwise, do nothing
 ## Wait for new task slots to become available
 ## Abort rescale request on timeout
 # Redeploy the JobGraph / Tasks with the updated parallelisms
 # Surrender any unused task slots in case of scaling down

 

There is an existing ""Rescaling"" API which is currently disabled. We have to evaluate whether reusing it makes sense.",,,,,,,,,,,,,,,,,,FLINK-31316,,,,,,,,,,,,,,,,FLINK-29501,FLINK-30260,,,,,,,,"23/Jan/23 16:59;pcless;meces.patch;https://issues.apache.org/jira/secure/attachment/13054779/meces.patch",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 28 10:04:28 UTC 2023,,,,,,,,,,"0|z1favs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 17:00;pcless;For other ideas around efficient scaling of Flink jobs that could be looked into in the future:
[https://www.usenix.org/system/files/atc22-gu-rong.pdf]

Code implementation: [https://github.com/ATC2022No63/Meces]
Patch with the differences for easier study: [^meces.patch];;;","23/Jan/23 18:08;Swathi_C;The logic looks great, this would be applicable only for flink native isn't it ?? ;;;","23/Jan/23 18:21;pcless;Disclaimer: I am neither the author or contributor of Meces nor a Flink expert. 

As I see it it would apply to Flink native such that Kubernetes would then scale based only on load of the task managers. There is a point between no more vertices behind ""allocatable"" in task managers & Kubernetes having to provision new instances. Don't know enough about Kubernetes or Flink to know how to make that transition. 

Maybe metric-based scaling for the Flink K8s deployment that Flink itself would know (based on Meces logic) to generate.;;;","09/Feb/23 16:52;mxm;Thanks for the pointers! Indeed, the rescaling logic can be much improved. I think we will just go with an upfront reservation of the resources and then perform a full job restart once the new resources are acquired, but I agree that it would be great to optimize the rescaling further by intelligently migrating the application state during a rescale operation. Note that we already have an autoscaling implementation in the Flink Kubernetes operator which was added in FLINK-30260.;;;","28/Apr/23 03:35;Jiangang;[~mxm] The rescaling api is useful in many situations such as:
 * Increase the parallelism when the job load is high. As latency is important for streaming systems, the rescale api can help us resolve this problem quickly.
 * Decrease the parallelism when the job needn't so much resource. When the cluster is big, this way can help us save much resources which means much money.
 * The rescaling api can decrease the stop-the-world time when changing the job's parallelism. This is important for the long-running streaming computation.

In fact, we have implemented the rescaling api in our company for default scheduler. The general idea is similar with yours. I understand your long considerations for state migration for operators. But this may need a long time to wait. A rescaling api may be valuable for may users. Can we consider to implement it in the recent time？

Looking forward to your reply. Thanks.

One more word, I wonder whether we can make the rescaling a common api for all schedulers?;;;","28/Apr/23 04:29;gyfora;I think the features outlined in this ticket are covered (and implemented) already by https://issues.apache.org/jira/browse/FLINK-31316


[~mxm] should we close this ticket?;;;","28/Apr/23 10:04;mxm;Thanks [~Jiangang] and [~gyfora]! The Rescaling API is now available as part of the Rest API and backed by the AdaptiveScheduler. Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink Shaded dependencies to the latest versions,FLINK-30772,13521003,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Jan/23 15:38,10/Jul/23 18:17,04/Jun/24 20:41,31/Mar/23 06:40,1.18.0,,,,,,shaded-17.0,,,,,,,,,,,,,,,,BuildSystem / Shaded,,,,,0,,,,,It would be smart to update the Flink Shaded components to their latest versions before we release Flink 1.18,,,,,,,,,,,,,,,FLINK-32032,,,,FLINK-29173,,,,,,,,,,,,,,,FLINK-28372,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 13:42:02 UTC 2023,,,,,,,,,,"0|z1fau8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 15:33;martijnvisser;[~mapohl] I've talked to [~chesnay] on this offline and he wouldn't do a dependency update so close for the release cutting, given that it might introduce new instabilities. I'm currently checking if which CVEs might be a reason to still do this update. ;;;","31/Jan/23 15:54;mapohl;I see, that was then a misconception on my side, then: I actually thought of not doing the dependency updates too early to avoid doing redundant work. 🤦‍♂️ I actually had the curator update in mind (FLINK-29173) because 5.4.0 solves some test instabilities on the Flink side (FLINK-30484). But it's not occurring that often - so, it's not a must-must-have.;;;","27/Mar/23 11:45;mapohl;[~martijnvisser] are you planning to work on that? I'd like to see the curator version being bumped rather sooner than later.;;;","27/Mar/23 14:49;martijnvisser;[~mapohl] The PR was already there, but the Flinkbot didn't pick it up. I've pinged you in the PR;;;","31/Mar/23 06:40;martijnvisser;Fixed in master: 

[FLINK-30772][Build/Shaded] Update Netty to 4.1.90.Final d00429a35fdd5b781bcf26d99f2c7cf8ff80f8f7
[FLINK-30772][Build/Shaded] Update Jackson to 2.14.2 b09266f1befca067b8090177fbc1b0f46cf21d0d
[FLINK-30772][Build/Shaded] Update Netty TCNative to 2.0.59.Final b5c59db3b6c2bdefb0bcb4a7495880d5ef9ab3b8
[FLINK-30772][Build/Shaded] Update Swagger to 2.2.8 9b1265fdcf0a80aca134a4e0ed381d897fe805bc
[FLINK-30772][Build/Shaded] Update Zookeeper 3.6 to 3.6.4 14142c380007836d68247471ddb73e053d15895c
[FLINK-30772][Build/Shaded] Update Snakeyaml to 2.0 1da20d4df35af41b7042fbf8e5e23bc570897603
[FLINK-30772][Build/Shaded] Update Guava to 31.1-jre 2822733762a49b535279d6c2bdb9e083fbc2480c
[FLINK-30772][Build/Shaded] Update Zookeeper 3.8 to 3.8.1 d68885c61387f19b4407e67afd21edccd932e623
[FLINK-30772][Build/Shaded] Update Curator to 5.4.0 c567aa7d8f183e7264c1a08c675f511e8f8b075a
[FLINK-30772][Build/Shaded] Update ASM to 9.5 b7ac6ad55c110ebba48417522d4f2defca10ac95
[FLINK-30772][Build/Shaded] Rename Guava module from flink-shaded-guava-30 to flink-shaded-guava-31 c37343c80e8f82ff9473fc978597a40314b9b864;;;","17/Apr/23 13:03;mapohl;[~martijnvisser] will you work on releasing flink-shaded 17.0 as well?;;;","17/Apr/23 13:40;martijnvisser;[~mapohl] Yes, it's on my list :D ;;;","17/Apr/23 13:42;mapohl;Great, let me know if I can help in any way. (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
different performance of conversion between datetime type in mysql and timestamp in flink,FLINK-30771,13521002,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,solofire,solofire,23/Jan/23 15:08,29/Jan/23 09:30,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,,,,,"in the attachment below named 'my_issue.sql', you can repeat my problem in eight steps。in simple terms，i think the conversion between datetime type in mysql and timestamp in flink need redefine carefully . after the research of source code ，i find something as below:
 # the timestime type in flink represent localdatetime in java
 # in  class 'com.mysql.cj.jdbc.result.ResultSetImpl' , there are two overloaded function :function one is  'Object getObject(int columnIndex)' , function two is '<T> T getObject(int columnIndex, Class<T> type)' . the chosen is function one by flink ,however the most popular ORM project like JPA/Mybatis choose function two.
 # since version  8.0.23 of mysql connector ，the function one has been refactored. and this refactor affect the conversion from datetime type in mysql to localdatetime in java. this conversion means fetch data from source mysql .
 # when the version of mysql connector is less than or equal with 8.0.22, the conversion is affected by the JDBC connection property ""serverTimezone"" which is defined in the WITH clause in the CREATE TABLE statment.
 # But when the version is greater than or equal with 8.0.23 , the result is different
 # in class 'org.apache.flink.connector.jdbc.internal.converter.AbstractJdbcRowConverter' , i am not sure if the localdatime type is cast into timestamp type, However i am pretty sure that  no matter the version of mysql connector ,the conversion from  localdatetime in java to datetime type in mysql is always affected by the JDBC connection property ""serverTimezone"" which is defined in the WITH clause in the CREATE TABLE statment.  this conversion means sink data to mysql .

       in my option ,i suggest we should refer to the ORM project , neither the fetch from mysql or sink to mysql proccess should affect by the property ""serverTimezone"" .

 ","System timezone is UTC and mysql version is 5.7.37;
my laptap is window and i build my flink in wsl 2(Windows Subsystem for Linux) ;
my mysql client is dbeaver;",1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/23 15:05;solofire;my_issue.sql;https://issues.apache.org/jira/secure/attachment/13054777/my_issue.sql",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,sql,Sun Jan 29 09:30:47 UTC 2023,,,,,,,,,,"0|z1fau0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 15:27;martijnvisser;[~solofire] Thanks for opening the ticket. For me it's not clear what is the outcome of the 2nd function, is that the result of the first function prior to 8.023 or the result of the first function with 8.023 and higher? ;;;","24/Jan/23 00:23;solofire;function one and two are overloaded functions . in the script named my_issue.sql , i create a table with 3 columns .  i find the function one was executed 3 times .  just see the source code as below:

when  version <= 8.0.22: 

Lines from  1218 to 1220 in method 'Object com.mysql.cj.jdbc.result.ResultSetImpl.getObject(int columnIndex)'  .

case TIMESTAMP:
case DATETIME:
return this.getTimestamp(columnIndex);

 

when version >=  8.0.23:

Lines from  1230 to 1233 in method 'Object com.mysql.cj.jdbc.result.ResultSetImpl.getObject(int columnIndex)'  

case TIMESTAMP:
return this.getTimestamp(columnIndex);
case DATETIME:
return this.getLocalDateTime(columnIndex);;;;","24/Jan/23 00:30;solofire;in ORM framework JAP/Mybatis, they choose the function two , '<T> T getObject(int columnIndex, Class<T> type)' .

the key statement is as below:

{color:#FF0000}else if (type.equals(Timestamp.class)) {                --line 1255{color}
{color:#FF0000} return this.getTimestamp(columnIndex);{color}
{color:#FF0000}}{color}

{color:#FF0000}} else if (type.equals(LocalDateTime.class)) {         --line 1277{color}
{color:#FF0000} return this.getLocalDateTime(columnIndex);           – line 1278{color}
{color:#FF0000}}{color}

so when the target type is LocalDateTime , the second arg was set to LocalDateTime.class , thus the line 1278 will be  executed.;;;","29/Jan/23 09:30;solofire;[~martijnvisser] Do I need to provide additional information?  Plz feel free to let me know。
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OUTER join generates partial results,FLINK-30770,13520998,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Workaround,,terlenbach,terlenbach,23/Jan/23 13:42,03/Feb/23 14:30,04/Jun/24 20:41,03/Feb/23 14:30,1.13.5,1.15.2,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"Hi,

 

I would like to share a scenario with you that seems to reveal a bug to me:

Given the data,
{code:java}
// file /tmp/input_table.jsonl

{""event_time_utc"": ""2023-01-23T10:02:41.677798"", ""match_id"": ""100""}
 {code}
{code:java}
// file /tmp/matches_state.jsonl
 
{""match_id"": ""100"", ""status"": ""PREMATCH"", ""event_time_utc"": ""2023-01-23T10:02:40.000000""}
{""match_id"": ""100"", ""status"": ""CLOSE"", ""event_time_utc"": ""2023-01-23T10:02:50.000000""}
{""match_id"": ""100"", ""status"": ""ENDED"", ""event_time_utc"": ""2023-01-23T10:02:55.000000""}
{""match_id"": ""100"", ""status"": ""ENDED-1"", ""event_time_utc"": ""2023-01-23T10:03:55.000000""}
{""match_id"": ""100"", ""status"": ""ENDED-2"", ""event_time_utc"": ""2023-01-23T10:04:55.000000""}
{""match_id"": ""100"", ""status"": ""ENDED-3"", ""event_time_utc"": ""2023-01-23T10:05:55.000000""}
 

{code}
 

and the scenario,
{code:java}
// scenario.sql

SET 'execution.runtime-mode' = 'streaming';
SET 'sql-client.execution.result-mode' = 'tableau';

CREATE TABLE input_table (
  match_id STRING NOT NULL, 
  event_time_utc TIMESTAMP(3) NOT NULL
)                              
WITH ( 
  'format' = 'json',
  'json.timestamp-format.standard' = 'ISO-8601',
  'path' = '/tmp/input_table.jsonl',
  'connector' = 'filesystem'
);

CREATE TABLE matches_state (
  match_id STRING NOT NULL,    
  status STRING NOT NULL,
  event_time_utc TIMESTAMP(3) NOT NULL
)
WITH (
  'format' = 'json',
  'json.timestamp-format.standard' = 'ISO-8601',
  'path' = '/tmp/matches_state.jsonl',
  'connector' = 'filesystem'
);

WITH last_matches_state AS (
    SELECT match_id, status
    FROM (
        SELECT
            match_id,
            status,
            ROW_NUMBER() OVER (PARTITION BY match_id ORDER BY event_time_utc DESC) AS rownum
        FROM matches_state
    )
    WHERE rownum = 1
)
, agg_match AS (
    SELECT
        match_id,
        COUNT(*) AS bet_count
    FROM input_table
    GROUP BY match_id
)
SELECT
    agg_match.match_id,
    agg_match.bet_count,
    last_matches_state.status
FROM
    agg_match
LEFT JOIN last_matches_state
ON last_matches_state.match_id = agg_match.match_id;
  {code}
 

Here is what I get from the query:
{code:java}
// Result

+----+--------------------------------+----------------------+--------------------------------+
| op |                       match_id |            bet_count |                         status |
+----+--------------------------------+----------------------+--------------------------------+
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |
| -U |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE |
| -U |                            100 |                    1 |                          CLOSE |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          ENDED |
| -U |                            100 |                    1 |                          ENDED |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                        ENDED-1 |
| -U |                            100 |                    1 |                        ENDED-1 |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                        ENDED-2 |
| -U |                            100 |                    1 |                        ENDED-2 |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                        ENDED-3 |
+----+--------------------------------+----------------------+--------------------------------+
Received a total of 23 rows {code}
 

The first part of the CTE query computes the last status of a match, the second part computes the number of rows by match_id, and then there is a left outer join query to link that count with the match status.

However, on match updates, I see that *some rows are generated with a status set to NULL* whereas there was already a status set for the corresponding match.

 

Interestingly, replacing the left outer join by an inner join, generates a different output:

 
{code:java}
WITH last_matches_state AS (
    SELECT match_id, status
    FROM (
        SELECT
            match_id,
            status,
            ROW_NUMBER() OVER (PARTITION BY match_id ORDER BY event_time_utc DESC) AS rownum
        FROM matches_state
    )
    WHERE rownum = 1
)
, agg_match AS (
    SELECT
        match_id,
        COUNT(*) AS bet_count
    FROM input_table
    GROUP BY match_id
)
SELECT
    agg_match.match_id,
    agg_match.bet_count,
    last_matches_state.status
FROM
    agg_match
INNER JOIN last_matches_state
ON last_matches_state.match_id = agg_match.match_id;

+----+--------------------------------+----------------------+--------------------------------+
| op |                       match_id |            bet_count |                         status |
+----+--------------------------------+----------------------+--------------------------------+
| +I |                            100 |                    1 |                       PREMATCH |
| -U |                            100 |                    1 |                       PREMATCH |
| +U |                            100 |                    1 |                          CLOSE |
| -U |                            100 |                    1 |                          CLOSE |
| +U |                            100 |                    1 |                          ENDED |
| -U |                            100 |                    1 |                          ENDED |
| +U |                            100 |                    1 |                        ENDED-1 |
| -U |                            100 |                    1 |                        ENDED-1 |
| +U |                            100 |                    1 |                        ENDED-2 |
| -U |                            100 |                    1 |                        ENDED-2 |
| +U |                            100 |                    1 |                        ENDED-3 |
+----+--------------------------------+----------------------+--------------------------------+ {code}
I would have expected to get the same results from the query with the left outer join (except the first entry when there is no status yet).

 

Is there any reason for receiving these partial results with the left outer join query?

 

I'd be happy to help should you need more details!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 12:59:47 UTC 2023,,,,,,,,,,"0|z1fat4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 13:51;martijnvisser;[~lincoln.86xy] [~godfrey] Is this a bug? ;;;","30/Jan/23 02:08;lincoln.86xy;AFAIK, this behavior is by design for now.
Both the inner join and left join will be processed as a [regular join|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/joins/#regular-joins]， 
each input record of the join(both the left and right input) will trigger the join operation and output current result, and left join may output partial result here 
(if no match record from right side, for above example, the 1st result '| +I |   100 |   1 |  <NULL> |' is the case)

because right input of the join uses the deduplicate(last row):

{code}

WITH last_matches_state AS (
    SELECT match_id, status
    FROM (
        SELECT
            match_id,
            status,
            ROW_NUMBER() OVER (PARTITION BY match_id ORDER BY event_time_utc DESC) AS rownum
        FROM matches_state
    )
    WHERE rownum = 1
)

{code}

 

it'll output detail changes, let's take the first two records as an example input:

{code}

{""match_id"": ""100"", ""status"": ""PREMATCH"", ""event_time_utc"": ""2023-01-23T10:02:40.000000""}
{""match_id"": ""100"", ""status"": ""CLOSE"", ""event_time_utc"": ""2023-01-23T10:02:50.000000""}

{code}

after deduplication will output:

{code}

+I 100 PREMATCH
-U 100 PREMATCH
+U 100 CLOSE

{code}

then for the join operator, each record will trigger the join and output two result (one retract old and one new, though these are all partial results, and join operator didn't distinguish the 'equivalent' message types, e.g., -D vs -U and +I vs +U, see https://issues.apache.org/jira/browse/FLINK-17337 )

{code}

| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |
| -U |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE |    

{code}

hope this helpful.;;;","30/Jan/23 08:28;terlenbach;Hi [~lincoln.86xy] , thanks for this comment!

Let me clarify my concern:

You said ""each record will trigger the join and output two result (one retract old and one new"" and I agree, that's what I would have expected.

However in the output, the second match status update (from PREMATCH to CLOSE) generate 4 records:
{code:java}
| -U |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE | {code}
and the two records in the middle are the ones that seem problematic to me as they have NULL values and the match status never went back to NULL.

WDYT?;;;","30/Jan/23 12:47;lincoln.86xy;hi [~terlenbach], in fact, each record as I explained before includes the intermediate results generated by the deduplication, it means that the first two original records (PREMATCH becomes CLOSE) are input to the deduplication operator, which outputs three intermediate results:
{code}
+I 100 PREMATCH
-U 100 PREMATCH
+U 100 CLOSE
{code}

then these three intermediate results input to the join operator which outputs these 6 records
{code}
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |
| -U |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE |
{code}

the approximate topology might be as follows:
{code}
left input 
                \
                  join - 
                /
            dedup
             /
right input 
{code};;;","30/Jan/23 14:53;terlenbach;Yes the dedup operation and its ouput are perfectly fine to me.

I just don't get the output of the join, If I try to link the events together, would this be the right sequence?

 
{code:java}
+I 100 PREMATCH -->

| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |{code}
 

 
{code:java}
-U 100 PREMATCH -->

| -U |                            100 |                    1 |                       PREMATCH |{code}
 

 
{code:java}
+U 100 CLOSE -->

| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE |{code}
 

and if so, why does the +U event generate these first two records with the the null values?;;;","31/Jan/23 06:03;lincoln.86xy;[~terlenbach] I see your confusion, for the left join, if the record from right table missing match, it will output the join result that pad null to the corresponding fields (from right table), so the input & output mapping is:
{code:java}
+I 100 PREMATCH -->

| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |{code}
 
{code:java}
-U 100 PREMATCH -->

| -U |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                         <NULL> | {code}
after '-U 100 PREMATCH' comes, the right match is missing, so here the join will output '| +I | 100 | 1 | <NULL> |' to follow the left join semantic

 
{code:java}
+U 100 CLOSE -->
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE |{code};;;","31/Jan/23 14:35;terlenbach;Ok, thanks for making things clearer.

 

To give you more context, in my project I consume this change log to update a dynamodb table, and at some point I have some invalid state containing these null values:
{code:java}
+I 100 PREMATCH -->

| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |

In my Dynamodb table:
| 100 | 1 | PREMATCH |{code}
 
{code:java}
-U 100 PREMATCH -->

| -U |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                         <NULL> | 

In my Dynamodb table:
| 100 | 1 | NULL |{code}
At this moment the state in the Dynamodb table is wrong :( the status should not be set to NULL as we had one value before,
{code:java}
+U 100 CLOSE -->
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                          CLOSE |

In my Dynamodb table:
| 100 | 1 | CLOSE |

{code}
then it becomes consistent again.

 

So I'm wondering if the handling of the update_before kind of messages within this join operator is correct, I mean, should the incoming event
-U 100 PREMATCH
delete the corresponding entry (I guess this is what is done internally)?

 

My guess is that if it didn't do anything with this update_before event we would have this sequence of events:
{code:java}
+I 100 PREMATCH -->

| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |

In my Dynamodb table:
| 100 | 1 | PREMATCH |{code}
{code:java}
-U 100 PREMATCH -->

Nothing{code}
{code:java}
+U 100 CLOSE -->
| -D |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                          CLOSE |

In my Dynamodb table:
| 100 | 1 | CLOSE |{code}
which would fix the problem, but I don't know if this would bring regressions at other places.

WDYT?;;;","02/Feb/23 07:41;lincoln.86xy;[~terlenbach] I understand your question, but why not use inner join stead? It'll never output a partial result related to null match record.

For the left join, it can not distinguish either a -U or -D is the last message of the record, if there's no matched record from right table, it must output a result to follow the left join semantic.;;;","02/Feb/23 08:45;terlenbach;I just shared a part of the request that we're using but we cannot use an inner join as we're attaching a default status to matches with a NULL status and we want that these matches go through the rest of the pipeline, with an inner join, matches without statuses wouldn't go further than the join operation. The join part of the query looks more like:
{code:java}
SELECT
    agg_match.match_id,
    agg_match.bet_count,
    last_matches_state.status
FROM
    agg_match
LEFT JOIN last_matches_state
ON last_matches_state.match_id = agg_match.match_id
WHERE COALESCE(last_matches_state.status, 'PREMATCH') = 'PREMATCH'; {code}
I didn't totally get what you said regarding the -U and -I records, but aren't -U records always followed by either a +U record or a +I and a -D record? wouldn't it be ok to just skip -U records in case of left joins?;;;","02/Feb/23 12:48;lincoln.86xy;[~terlenbach] 

> but aren't -U records always followed by either a +U record or a +I and a -D record? 

not exactly, take the following changelog as example input

{code}

id status
+I 100 PREMATCH
-U 100 PREMATCH
+U 100 CLOSE

{code}

if exists a filter condition ""status = 'PREMATCH"", then in this case(not the only case) the '-U 100 PREMATCH' will be the last message sending to the downstream operator.

 

 ;;;","03/Feb/23 10:46;terlenbach;Hi [~lincoln.86xy] , thanks for all the explanations. I managed to find a workaround to my problem leveraging the LAST_VALUE aggregation function which does not emit events when the value does not change (not the case for the top1):
{code:java}
WITH known_matches AS (
  SELECT
    -- With the last value aggregation, no event is emitted when the value does not change
    LAST_VALUE(match_id) AS match_id
  FROM
    matches_state
  GROUP BY match_id
)
, last_matches_state AS (
    SELECT match_id, status
    FROM (
        SELECT
            match_id,
            status,
            ROW_NUMBER() OVER (PARTITION BY match_id ORDER BY event_time_utc DESC) AS rownum
        FROM matches_state
    )
    WHERE rownum = 1
)
, agg_match AS (
    SELECT
        match_id,
        COUNT(*) AS bet_count    FROM input_table
    GROUP BY match_id
)
SELECT
    agg_match.match_id,
    agg_match.bet_count,
    last_matches_state.status
FROM
    agg_match
LEFT JOIN last_matches_state
ON last_matches_state.match_id = agg_match.match_id
WHERE 
  last_matches_state.status IS NOT NULL OR
  (
    NOT EXISTS (SELECT * FROM known_matches WHERE known_matches.match_id = agg_match.match_id)
    AND last_matches_state.status IS NULL
  );
 {code}
Which outputs:
{code:java}
+----+--------------------------------+----------------------+--------------------------------+
| op |                       match_id |            bet_count |                         status |
+----+--------------------------------+----------------------+--------------------------------+
| +I |                            100 |                    1 |                         <NULL> |
| -D |                            100 |                    1 |                         <NULL> |
| +I |                            100 |                    1 |                       PREMATCH |
| -D |                            100 |                    1 |                       PREMATCH |
| +I |                            100 |                    1 |                          CLOSE |
| -D |                            100 |                    1 |                          CLOSE |
| +I |                            100 |                    1 |                          ENDED |
| -D |                            100 |                    1 |                          ENDED |
| +I |                            100 |                    1 |                        ENDED-1 |
| -D |                            100 |                    1 |                        ENDED-1 |
| +I |                            100 |                    1 |                        ENDED-2 |
| -D |                            100 |                    1 |                        ENDED-2 |
| +I |                            100 |                    1 |                        ENDED-3 |
+----+--------------------------------+----------------------+--------------------------------+{code}
Anyway, thank you for taking time to look at this, my conclusion is that outer joins are to be used carefully, as due to retract messages, there might be extra messages with null values. Maybe upsert messages all the way down the pipeline instead of retract messages could avoid this? Dunno if that's even possible :shrug: ;;;","03/Feb/23 12:59;lincoln.86xy;[~terlenbach] Good to hear you found a solution!

Actually the current sql planner is trying the best to optimize the whole query to use the 'upsert mode' which exactly the case you want (no -U message, only +I, +U, -D messags), but unfortunately due to the correctness requirement and current changelog implementation we can not alway gain such optimization and forcely require the corresponding operator(s) generate -U(update_before) messages, e.g., downstream operators use a difference 'key' from source key of changelog for updating.

Anyway, this is really a problem that is not easy to use in current streaming sql, and we are trying to improve it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Operator SDK RuntimeInfo in Health Probe,FLINK-30769,13520997,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,23/Jan/23 13:37,07/Feb/23 12:23,04/Jun/24 20:41,07/Feb/23 12:23,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,The operator health check endpoint currently only catches startup errors and does not react on the info now exposed by the Java operator sdk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 12:23:28 UTC 2023,,,,,,,,,,"0|z1fasw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 12:23;gyfora;Merged to main ca231195f05578947cb4e8fb591914e2dd1d4fed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-web version cleanup,FLINK-30768,13520990,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,victoryu930909,mapohl,mapohl,23/Jan/23 12:56,30/Oct/23 13:51,04/Jun/24 20:41,30/Oct/23 13:51,1.15.3,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,auto-deprioritized-major,pull-request-available,starter,,"The Flink website sources have multiple dependency versions (e.g. log4j, sl4fj in q/gradle-quickstart.sh in {{q/gradle-quickstart.sh}}) that are not referenced in Flink's parent pom file. That means that contributors might forget updating the right locations in {{flink-web}} when updating the dependencies. Additionally, {{q/gradle-quickstart.sh}} specifies a {{$defaultFlinkVersion}} variable but requires to update a hardcoded Flink version further down in the script as well (see [q/gradle-quickstart.sh:119|https://github.com/apache/flink-web/blob/dc24124816d86617991050a2e36fe70ee40ff2dc/q/gradle-quickstart.sh#L119]). 

This Jira issue is about reducing the locations where we have to update versions (through variables) and adding references that these variables have to be updated to the corresponding versions that used in the Flink source code as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33383,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 22:35:19 UTC 2023,,,,,,,,,,"0|z1farc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 12:58;mapohl;Similar issues exist in {{{}q/sbt-quickstart.sh{}}}. This issue should include doing a proper research on other code segments that can be improved in this matter. I didn't go through all the files;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to fetch com.nimbusds:nimbus-jose-jwt:jar:9.29 artifact,FLINK-30767,13520957,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,23/Jan/23 07:49,03/Feb/23 10:09,04/Jun/24 20:41,03/Feb/23 10:09,1.15.3,,,,,,,,,,,,,,,,,,,,,,Build System,,,,,0,test-stability,,,,"Failure happened in both e2e test runs:
* [e2e1] {{2023-01-21T00:32:28.2196176Z}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45136&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&l=11748
* [e2e2] {{2023-01-21T00:33:08.2487224Z}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45136&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=9baa6deb-e632-5387-d76c-cf2ba9138f2e&l=11748
* [cron_snapshot_deployment_maven] {{2023-01-21T00:39:02.2061760Z}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45136&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=12786
* [compile_cron_azure] {{2023-01-21T00:38:31.1736613Z}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45136&view=logs&j=cc2c0b9a-de49-5517-79e5-b1df1cadc1e5&t=e46d8929-052e-573c-00f4-50453f4d6369&l=11030
* [e2e_1_cron_azure] {{2023-01-21T00:41:35.9229946Z}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45136&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=e811a31d-7c99-5e74-90b0-fc9fa25fddce&l=11748
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:3.1.1:shade (shade-flink) on project flink-azure-fs-hadoop: Error creating shaded jar: Could not resolve following dependencies: [com.nimbusds:nimbus-jose-jwt:jar:9.29 (test)]: Could not resolve dependencies for project org.apache.flink:flink-azure-fs-hadoop:jar:1.15-SNAPSHOT: Could not find artifact com.nimbusds:nimbus-jose-jwt:jar:9.29 in google-maven-central (https://maven-central-eu.storage-download.googleapis.com/maven2/) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-azure-fs-hadoop
{code}

The azure build couldn't be started due to the failure in {{compile_cron_azure}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45136&view=logs&j=182c7bbd-e545-5b8a-4729-b868bef672c5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 10:09:09 UTC 2023,,,,,,,,,,"0|z1fak0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 07:57;mapohl;It looks like a temporary infrastructure issue.;;;","24/Jan/23 07:54;mapohl;Same issue appeared for the {{master}} build [20230121.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=results] which ran around the same time.;;;","03/Feb/23 10:09;mapohl;Looks like a temporary infrastructure issue that didn't appear afterwards. I'm gonna close this issue since we didn't observe it recently. Feel free to reopen the issue if it appears again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only parameterize tests with Flink 1.14 and 1.15,FLINK-30766,13520903,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,21/Jan/23 19:52,24/Jan/23 21:22,04/Jun/24 20:41,24/Jan/23 21:22,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"As we only have some specialized behaviour for Flink 1.14 vs 1.15 and only in certain cases we should not parameterize tests with all Flink versions.

Furthermore the parameterization should not be done at all where it makes no difference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 21:22:03 UTC 2023,,,,,,,,,,"0|z1fa8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 21:22;gyfora;merged to main cb30a70dc961e6f61ddd370e828435112db152a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderElectionService.stop should always call revokeLeadership,FLINK-30765,13520750,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Jan/23 13:39,28/Jun/23 08:02,04/Jun/24 20:41,12/Apr/23 17:06,1.18.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The current implementations of {{LeaderElectionService}} do not implement the {{stop()}} call consistently. Some (e.g. [StandaloneLeaderElectionService|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/StandaloneLeaderElectionService.java#L53] call revoke on the {{LeaderContender)}} whereas others don't (e.g. [DefaultLeaderElectionService|https://github.com/apache/flink/blob/6e1caa390882996bf2d602951b54e4bb2d9c90dc/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L96]). The [MultipleComponentLeaderElectionService|https://github.com/apache/flink/blob/0290715a57b8d243586ab747b0cd2416c8081012/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L166] does call revoke on the {{LeaderContender}} instances, though.

We should align this behavior and specify it in the LeaderElectionService contract before going ahead with refactoring the interfaces (FLIP-285).",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30763,,,,,,,,FLINK-32422,,FLINK-30759,FLINK-19542,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 17:06:54 UTC 2023,,,,,,,,,,"0|z1f9ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/23 13:59;mapohl;FLINK-30759 clears things up for the {{DispatcherRunnerLeaderElectionLifecycleManager}} and {{DefaultDispatcherRunner}} shutdown. 
Currently, the manager stops the {{LeaderElectionService}}. Closing the {{DefaultDispatcherRunner}} happens in parallel which causes a raise condition when {{revokeLeadership}} is called by {{LeaderElectionService.stop()}}. Both will trigger {{DefaultDispatcherRunner.stopDispatcherLeaderProcess()}} to be called. Merging both classes will make the shutdown process clearer.;;;","12/Apr/23 17:06;mapohl;master: 3e4f653ffa0e9b86d82b1c1a00a1523e0891635b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include generic delegation token params in the main config page,FLINK-30764,13520658,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,20/Jan/23 12:52,20/Jan/23 16:55,04/Jun/24 20:41,20/Jan/23 16:55,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,Runtime / Configuration,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 20 16:55:26 UTC 2023,,,,,,,,,,"0|z1f8q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/23 16:55;chesnay;master: 9e9081cbf88ec7d596fafbf0d674a50696a5ad1c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make TestingLeaderElectionService comply to how the LeaderElectionService interface is intended,FLINK-30763,13520605,13432464,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,20/Jan/23 12:26,24/Jan/23 12:54,04/Jun/24 20:41,24/Jan/23 12:54,1.15.3,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,Tests,,,,0,,,,,{{TestingLeaderElectionService}} doesn't comply to the {{LeaderElectionService}} interface (at least how it's intended): A LeaderElectionService can be created that operates without issues even if it's not properly started.,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30765,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-20 12:26:21.0,,,,,,,,,,"0|z1f8e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add nightly builds for AWS connectors repo,FLINK-30762,13520377,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,20/Jan/23 10:07,25/Jan/23 13:03,04/Jun/24 20:41,25/Jan/23 13:03,,,,,,,aws-connector-4.1.0,,,,,,,,,,,,,,,,Connectors / AWS,,,,,0,pull-request-available,,,,Add nightly snapshot build for flink-connector-aws,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 13:03:51 UTC 2023,,,,,,,,,,"0|z1f6zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/23 13:03;dannycranmer;Merged commit [{{e5f1362}}|https://github.com/apache/flink-connector-aws/commit/e5f13624d0bd78abce13677201aefb61f0acfaa7] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove JVM asserts from leader election code,FLINK-30761,13520303,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Jan/23 08:51,24/Jan/23 12:52,04/Jun/24 20:41,24/Jan/23 12:52,1.15.3,1.16.0,1.17.0,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,{{assert}} is not enabled in the test run. We should using Preconditions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 12:52:56 UTC 2023,,,,,,,,,,"0|z1f6j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 12:52;mapohl;master: b26c695b5332dd44f9c744cf7d7c0db5398928ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix @GuardedBy and @Nullable annotations in DefaultLeaderElectionService,FLINK-30760,13520300,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Jan/23 08:47,24/Jan/23 12:51,04/Jun/24 20:41,24/Jan/23 12:49,1.15.3,1.16.0,1.17.0,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 12:51:05 UTC 2023,,,,,,,,,,"0|z1f6ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 12:49;mapohl;master: bf1fc7ad206421509433346351dcd8f6f23ea9b1;;;","24/Jan/23 12:51;mapohl;I didn't backport this change despite it being a problem in older versions as well since it's only a documentation issue. Additionally, if we decide to backport FLINK-26522, we would have to backport any subtask as well. Not backporting this one keeps a consistent state among the subtasks since none of the others are backported so far.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge DispatcherRunnerLeaderElectionLifecycleManager into DefaultDispatcherRunner,FLINK-30759,13520299,13432464,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Jan/23 08:45,24/Jan/23 12:41,04/Jun/24 20:41,24/Jan/23 12:41,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"{{DispatcherRunnerLeaderElectionLifeCycleManager}} handles the lifecycle of the {{{}DefaultDispatcherRunner{}}}'s leader election. The leader election itself is then called within the {{{}DefaultDispatcherRunner{}}}.

Merging both would make the code less complex and prepare the proper refactoring of the LeaderElectionService interface(s).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30765,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 12:41:03 UTC 2023,,,,,,,,,,"0|z1f6i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 12:41;mapohl;master: 2d1d61c68d81eba0f721b10eaf2a7246aa0814c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove sql-client.display.max-column-width,FLINK-30758,13520268,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jingge,jingge,jingge,20/Jan/23 03:52,20/Jan/23 03:52,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,,,,,"sql-client.display.max-column-width is deprecated and should be removed in the release after the next release. 

 

Please consider removing PrintStyle.DEFAULT_MAX_COLUMN_WIDTH if there is no need to use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30025,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-20 03:52:03.0,,,,,,,,,,"0|z1f6bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ugrade busybox version to a pinned version for operator,FLINK-30757,13520216,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,spencerx,gaborgsomogyi,gaborgsomogyi,19/Jan/23 16:06,14/Feb/23 09:15,04/Jun/24 20:41,14/Feb/23 09:15,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,starter,,,"It has been seen that the operator e2e tests were flaky when used the latest version of the busybox so we've pinned it to a relatively old version. https://github.com/apache/flink-kubernetes-operator/commit/8e41ed5ec1adda9ae06bc0b6203abf42939fbf2b

It would be good to do 2 things
* Upgrade the busybox version to the latest in a pinned way
* Add debug logs of the busybox pod in case of failure (to see why failed)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 09:15:21 UTC 2023,,,,,,,,,,"0|z1f600:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 06:47;spencerx;Hi [~gaborgsomogyi] [~gyfora] , I am using the Flink k8s opeartor in production environment during work and want to learn and contribute to the Flink k8s operator. Can I take this task as the starter?;;;","07/Feb/23 07:07;gyfora;Of course, go ahead:);;;","07/Feb/23 07:42;gaborgsomogyi;Happy to see contributors :);;;","10/Feb/23 20:56;spencerx;Hi [~gyfora] [~gaborgsomogyi], I added the code to print init container log, tested locally and opened a draft MR to trigger Github CI. However, it is said that {{First-time contributors need a maintainer to approve running workflows}}. Could you please help approve? Thanks!;;;","11/Feb/23 00:15;spencerx;The e2e [error|https://github.com/apache/flink-kubernetes-operator/actions/runs/4147827390/jobs/7175460340#step:9:1340] in busybox init container is {_}wget: error getting response: Connection reset by peer{_}. It looks like similar error is already reported as an [issue|https://github.com/docker-library/busybox/issues/162]. 
I pushed another commit to downgrade from 1.36.0 to 1.35.0 but the workflow run still needs approval.;;;","14/Feb/23 09:15;gyfora;merged to main 02bef3d69acc03add5c8e0970aa59b85278965dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTableExplainTests.test_explain fails,FLINK-30756,13520204,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,,mapohl,mapohl,19/Jan/23 13:46,19/Jan/23 14:35,04/Jun/24 20:41,19/Jan/23 14:34,1.17.0,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"We observed StreamTableExplainTests.test_explain failing on {{{}master{}}}. I'm marking this as a blocker because it, so far, only appeared for 1.17.

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45055&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25533]

Also, other tests failed:
{code:java}
Jan 19 08:29:57 FAILED pyflink/table/tests/test_explain.py::StreamTableExplainTests::test_explain
Jan 19 08:29:57 FAILED pyflink/table/tests/test_sql.py::StreamSqlTests::test_sql_ddl - py4j.p...
Jan 19 08:29:57 FAILED pyflink/table/tests/test_table_environment_api.py::TableEnvironmentTest::test_explain
Jan 19 08:29:57 FAILED pyflink/table/tests/test_table_environment_api.py::TableEnvironmentTest::test_explain_with_extended
Jan 19 08:29:57 FAILED pyflink/table/tests/test_table_environment_api.py::TableEnvironmentTest::test_explain_with_multi_sinks
Jan 19 08:29:57 FAILED pyflink/table/tests/test_udaf.py::StreamTableAggregateTests::test_using_decorator {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30753,,,FLINK-30191,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 14:34:19 UTC 2023,,,,,,,,,,"0|z1f5xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 13:47;mapohl;[~dianfu] [~Yellow] ;;;","19/Jan/23 13:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45069&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25556;;;","19/Jan/23 13:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45072&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901;;;","19/Jan/23 13:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45044&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=26330;;;","19/Jan/23 14:02;mapohl;0bbc7b1e9fed89b8c3e8ec67b7b0dad5999c2c01 was added as the only Python-related commit between [20230119.4|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45039&view=results] (0b8a83ce) and [20230119.6|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45044&view=results] (0bbc7b1e).

I'm going to revert 0bbc7b1e9fed89b8c3e8ec67b7b0dad5999c2c01 and reopen FLINK-30191.;;;","19/Jan/23 14:34;mapohl;FLINK-30753 fixes the issue. See related CI build [20230119.15|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45081&view=results]. I'm not gonna merge the revert PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make SinkV2 support speculative execution for batch jobs,FLINK-30755,13520185,13519977,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,SleePy,SleePy,SleePy,19/Jan/23 11:20,28/Jan/23 03:50,04/Jun/24 20:41,28/Jan/23 03:48,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,31/Jan/23 00:00,0,,,,,"This is the first part of FLIP-281 implementation. In this ticket, we would introduce some base abstraction of supporting speculative sink and then make SinkV2 API work with it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 28 03:48:33 UTC 2023,,,,,,,,,,"0|z1f5t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 03:48;zhuzh;833c4d25c884a2657fdf4bbdf4127a44faede7ae
2690fb17ea380d6102e53e2985617bef8e517659
242987c75e519ae7b084b19226e598e9663de555
a9e1738a2e131426ea8fa3051a2f4cba841730eb
b3589afff6477676c4fe2201aad9d37fab59ae29
d336dad93459622be2dd619cc1d8d3709118628b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ExceptionThrowingDelegationTokenProvider/Receiver multi-threaded test issues,FLINK-30754,13520168,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,19/Jan/23 09:50,01/Feb/23 13:45,04/Jun/24 20:41,26/Jan/23 10:26,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 26 10:26:33 UTC 2023,,,,,,,,,,"0|z1f5pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/23 10:26;mbalassi;3a64648 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Py4J cannot acquire Table.explain() method,FLINK-30753,13520165,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,19/Jan/23 09:38,19/Jan/23 14:34,04/Jun/24 20:41,19/Jan/23 12:38,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45044&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 12:38:27 UTC 2023,,,,,,,,,,"0|z1f5oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 12:38;hxbks2ks;Merged into master via 9ebb331be45dc2b094aca215e1062dad9800cd76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support 'EXPLAIN PLAN_ADVICE' statement in PyFlink,FLINK-30752,13520146,13517856,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,19/Jan/23 06:37,29/Jan/23 06:09,04/Jun/24 20:41,29/Jan/23 06:09,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,For API completeness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 06:09:33 UTC 2023,,,,,,,,,,"0|z1f5kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 06:09;dianfu;Merged to master via d2710743eb55e76e0e8d1fe8a5c0d6d791147c56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove references to disableDataSync in RocksDB documentation,FLINK-30751,13520142,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dchristle,dchristle,dchristle,19/Jan/23 05:53,05/Jun/23 07:08,04/Jun/24 20:41,05/Jun/23 03:45,1.16.0,,,,,,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,"The EmbeddedRocksDBStateBackend allows configuration using some predefined options via the .setPredefinedOptions method. The documentation for PredefinedOptions ([link|https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/contrib/streaming/state/PredefinedOptions.html]) mentions disableDataSync is called for {{FLASH_SSD_OPTIMIZED}} and {{{}SPINNING_DISK_OPTIMIZED{}}}.

 

But this option was removed several years ago in RocksDB 5.3.0 ([link|https://github.com/facebook/rocksdb/blob/main/HISTORY.md#530-2017-03-08]), and according to the code [PredefinedOptions.java|https://github.com/apache/flink/blob/0bbc7b1e9fed89b8c3e8ec67b7b0dad5999c2c01/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/PredefinedOptions.java#L72], it is no longer actually set in Flink.

We should remove references to disableDataSync in PredefinedOptions.java, and in state_backend.py, so that it does not appear in the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 03:45:48 UTC 2023,,,,,,,,,,"0|z1f5jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 03:45;yunta;merged in master: 581936f340a5aa7cfbf9166f2b1d0ad5ed457ce2, b232a7ebbe3fbf0c444fd0f48eaeafa0aa13a27a

release-1.17: cc8b37af7c2c20e20ec71eb6b87a0103041dfd3b, 22696eec97e251d7b12dbbcc6530857643d322d2

release-1.16: b65b11bc9c87c489be0a514edd8c3290e090996c,
5c0108be3ae54bad5d7af8b8d4ed7ffac4bb8002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactActionITCase.testBatchCompact in table store is unstable,FLINK-30750,13520135,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,zjureel,zjureel,19/Jan/23 03:29,02/Feb/23 03:22,04/Jun/24 20:41,02/Feb/23 03:22,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"https://github.com/apache/flink-table-store/actions/runs/3954989166/jobs/6772877033


2023-01-17T11:45:17.9511390Z [INFO] Results:
2023-01-17T11:45:17.9511641Z [INFO] 
2023-01-17T11:45:17.9511838Z [ERROR] Errors: 
2023-01-17T11:45:17.9512585Z [ERROR]   CompactActionITCase.testBatchCompact » JobExecution Job execution failed.
2023-01-17T11:45:17.9512964Z [INFO] 
2023-01-17T11:45:17.9513223Z [ERROR] Tests run: 224, Failures: 0, Errors: 1, Skipped: 4




Besides above error, there's another exception as followed

https://github.com/apache/flink-table-store/actions/runs/3964547232/jobs/6793496230
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:227)
	... 37 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	... 35 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)
	... 28 more
Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 ms
	... 29 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 03:22:54 UTC 2023,,,,,,,,,,"0|z1f5i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 03:22;lzljs3620320;master: f2b3ba97cbcc46f2ffa519b174576eaa35e15c82;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delegation token provider enabled flag documentation is wrong,FLINK-30749,13520088,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,18/Jan/23 15:25,19/Jan/23 15:26,04/Jun/24 20:41,19/Jan/23 15:26,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,Runtime / Configuration,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 15:26:11 UTC 2023,,,,,,,,,,"0|z1f588:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 15:26;chesnay;master: 2698078ea47b828fb13b0ec1ce09b1534f7496de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Overview"" page of ""Querys"" into Chinese",FLINK-30748,13520086,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 15:07,14/Mar/23 04:02,04/Jun/24 20:41,14/Mar/23 04:02,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/overview]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/overview.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 04:02:33 UTC 2023,,,,,,,,,,"0|z1f57s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 15:24;chenhaiyang;Hi [~jark],
I have already translated the ""overview"" page of ""Query"", the pull request has been committed.

Please see [https://github.com/apache/flink/pull/21726] .;;;","14/Mar/23 04:02;jark;Fixed in master: 63f5555d315fdc9bef29e37021855860468bb093...5d15191bbd9d3e9c357c3d3d3d2e410361510022;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Window Deduplication"" page of ""Querys"" into Chinese",FLINK-30747,13520085,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 15:06,13/Mar/23 07:58,04/Jun/24 20:41,13/Mar/23 07:58,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-deduplication]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/window-deduplication.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 07:58:11 UTC 2023,,,,,,,,,,"0|z1f57k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 15:16;chenhaiyang;Hi [~jark] ,The pull request for this issue has been committed,Please see [https://github.com/apache/flink/pull/21722];;;","13/Mar/23 07:58;jark;Fixed in master: c5b68ebef4d37adae61186e6e75e7ffeefa9eec0 and 2bb10e5ef0c762d213c5f02f3895471332fd872a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink Table / SQL Column types of query result and sink for do not match, but should",FLINK-30746,13520084,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,colin.williams,colin.williams,18/Jan/23 15:06,27/Feb/23 15:22,04/Jun/24 20:41,27/Feb/23 15:22,1.16.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"For the branch `table-example` in the `table-example` directory [https://github.com/apple-corps/recipes/tree/table-example/table-example] we provide a test job [https://github.com/apple-corps/recipes/blob/table-example/table-example/src/test/java/TableJobTest.java]

When we run the job, for example with `mvn install` we get:

```

org.apache.flink.table.api.ValidationException: 
Column types of query result and sink for 'temporaryCatalog.default.enrichmentOut' do not match.
Cause: Incompatible types for sink column 'enrichment' at position 2.

Query schema: [id: STRING NOT NULL, status: STRING NOT NULL, ts: TIMESTAMP(3) *ROWTIME*, enrichment: STRING NOT NULL]
Sink schema:  [id: STRING, status: STRING, enrichment: STRING, ts: TIMESTAMP(3)]
    at TableJobTest.testExecuteSQL(TableJobTest.java:83)
```

However by the schemas the sink schema should be showing compatible types. [https://github.com/apple-corps/recipes/blob/table-example/table-example/src/main/java/com/immerok/cookbook/schemas/Schemas.java]

One curiousity is the *ROWTIME* which we see in the Query Schema but not in the Sink schema.

This issue has been discussed in [https://apache-flink.slack.com/archives/C03G7LJTS2G/p1673979671298779]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 15:22:41 UTC 2023,,,,,,,,,,"0|z1f57c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 15:10;martijnvisser;[~godfreyhe] [~jark] Any idea why this issue occurs? ;;;","27/Feb/23 15:21;jark;The exception is very clear, the column order is mismatched. In the query schema, the {{enrichment}} column is at position 3, but in the sink schema, the {{enrichment}} column is at position 2.;;;","27/Feb/23 15:22;jark;I closed the issue for now. Feel free to reopen it if not the reason. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check-pointing with Azure Data Lake Storage,FLINK-30745,13520080,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dheerajpanangat,dheerajpanangat,18/Jan/23 14:44,30/May/23 08:13,04/Jun/24 20:41,,1.14.6,1.15.2,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,,,,,"Hi,

While checkpointing to Azure Blob Storage using Flink, we get the following error :
{code:java}
Caused by: Configuration property <accoutnname>.dfs.core.windows.net not found.
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:372)
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1133)
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:174)
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:110) {code}
We have given the configurations in core-site.xml too for following
{code:java}
fs.hdfs.impl
fs.abfs.impl -> org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
fs.file.impl
fs.azure.account.auth.type
fs.azure.account.oauth.provider.type
fs.azure.account.oauth2.client.id
fs.azure.account.oauth2.client.secret
fs.azure.account.oauth2.client.endpoint
fs.azure.createRemoteFileSystemDuringInitialization -> true {code}
On debugging found that flink reads from core-default-shaded.xml, but even if the properties are specified there, the default configs are not loaded and we get a different exception as :
{code:java}
Caused by: Unable to load key provider class.
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getTokenProvider(AbfsConfiguration.java:540)
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1136)
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:174)
at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:110) {code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 08:13:39 UTC 2023,,,,,,,,,,"0|z1f56g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 17:01;surendralilhore;Thanks [~dheerajpanangat] for reporting this issue.

What value you configured for property ""fs.azure.account.keyprovider.<accountname>"" ?;;;","19/Jan/23 04:47;dheerajpanangat;Hi [~surendralilhore] ,
Not specified any value for this property : fs.azure.account.keyprovider.<accountname>
We are not encrypting the secrets as of now.

Just to add, we also use hudi with flink and that also uses Azure Blob Storage and works fine.

The difference in them being, for hudi to detect the file system it uses the hadoop jars/classes, but for checkpointing it uses the filesystem that is part of flink jars/classes (shaded classes).;;;","20/Jan/23 11:55;dheerajpanangat;Hi [~surendralilhore] ,
Post below changes. I don't see any errors in the logs and I can see the checkpointing folders created on Azure too.

Used the *core-default-shaded.xml* configuration
Provided the shaded classes instead of Hadoop classes :

in core-site.xml
{code:java}
<property>
    <name>fs.hdfs.impl</name>
    <value>org.apache.hadoop.hdfs.DistributedFileSystem</value>
</property>
<property>
    <name>fs.abfs.impl</name>
    <value>org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem</value>
</property>
<property>
    <name>fs.file.impl</name>
    <value>org.apache.hadoop.fs.LocalFileSystem</value>
</property>
<property>
    <name>fs.azure.account.oauth.provider.type</name>
    <value>org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider</value>
</property>{code}
in core-default-shaded.xml
{code:java}
<property>
    <name>fs.hdfs.impl</name>
    <value>org.apache.hadoop.hdfs.DistributedFileSystem</value>
</property>
<property>
    <name>fs.abfs.impl</name>
    <value>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem</value>
</property>
<property>
    <name>fs.file.impl</name>
    <value>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.LocalFileSystem</value>
</property>
<property>
    <name>fs.azure.account.oauth.provider.type</name>
    <value>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider</value>
</property>{code}
 

Thanks for checking this.
Let me know your thoughts if any on the above approach.;;;","20/Jan/23 14:21;dheerajpanangat;Hi [~surendralilhore] ,
We were able to get it working on our local machine, where we are just executing the main class.

When we deployed the same solution on Kubernetes using Flink Native Kubernetes command, it gives the same error there.
The core-default-shaded.xml is included in the docker image jar which is deployed, but still seeing the same issues there.

Can you help check what could be the possible reasons ?

Thanks;;;","22/Jan/23 05:52;surendralilhore;[~dheerajpanangat] , sorry for late reply.

As mentioned in Flink doc ([Azure blob Flink configuration |https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/azure/] ), you need to configure abfs properties in *flink-conf.yaml.*

[HadoopConfigLoader |https://github.com/apache/flink/blob/master/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/util/HadoopConfigLoader.java#L82] load this configuration from flink configuration. 
{quote}Provided the shaded classes instead of Hadoop classes
{quote}
You are correct

 

Please configure below properties in flink-conf.yaml in Kubernetes cluster and try. 
{noformat}
fs.azure.account.auth.type : OAuth

fs.azure.account.oauth.provider.type : org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider

fs.azure.account.oauth2.client.id : <application_id>

fs.azure.account.oauth2.client.secret : <secret>

fs.azure.account.oauth2.client.endpoint : https://XXXXXXXXXXXXXX.com/XXXXXXXXXXXXXXXXXXXXX/oauth2/token {noformat}
 

 ;;;","30/May/23 08:13;dheerajpanangat;Hi [~surendralilhore] ,
Sorry for reverting late on this ticket. 

But just to summarise the issue:
- Giving the shaded classes work fine when we are using Flink to connect to Azure Storage (for checkpointing or state)
- But, If along with Flink we are using any other library which uses hadoop to store data, then they would conflict as those libraries would not recognise the shaded classes. Example for this is Hudi with Flink.

Proposal :
- The Flink configs should be different for shaded hadoop, 
for eg 
instead of {*}fs.azure.account.auth.type{*}, it should be *flink.fs.azure.account.auth.type*

Work Around : 
- The only work around I was able to go with was to give separate xmls, as for shaded hadoop and hadoop had different file names 
(core-site.xml for Hadoop and core-default-shaded.xml for Shaded Hadoop)

Please let me know your thoughts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Window Top-N"" page of ""Querys"" into Chinese",FLINK-30744,13520073,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 14:17,13/Mar/23 06:52,04/Jun/24 20:41,13/Mar/23 06:52,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-topn]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/window-topn.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 06:52:33 UTC 2023,,,,,,,,,,"0|z1f54w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 14:20;chenhaiyang;Hi [~jark] ,The pull request for this issue has been committed,Please see [https://github.com/apache/flink/pull/21824];;;","13/Mar/23 06:52;jark;Fixed in master: 369e3b9feac318802eff5d9655157297d963a7c7 and 65ab8e820a3714d2134dfb4c9772a10c998bd45a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Kubernetes HA Services docs,FLINK-30743,13520068,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,wobu,wobu,18/Jan/23 13:58,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,auto-deprioritized-minor,pull-request-available,,,"i recently tried to setup a flink standalone session cluster with kubernetes HA and needed to adjust configmap RBACs settings via try & error because the documentation of:
 * [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/ha/kubernetes_ha/]
 * [https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/deployment/resource-providers/standalone/kubernetes/#high-availability-with-standalone-kubernetes]

didn't state everything which was needed. E.g. for configmap resource the verb ""watch"" is essential.

With
{color:#569cd6}apiVersion{color}{color:#d4d4d4}: {color}{color:#ce9178}v1{color}
{color:#569cd6}kind{color}{color:#d4d4d4}: {color}{color:#ce9178}ServiceAccount{color}
{color:#569cd6}metadata{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  {color}{color:#569cd6}labels{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}    {color}{color:#569cd6}app{color}{color:#d4d4d4}: {color}{color:#ce9178}flink{color}
{color:#d4d4d4}  {color}{color:#569cd6}name{color}{color:#d4d4d4}: {color}{color:#ce9178}flink-service-account{color}
{color:#d4d4d4}  {color}{color:#569cd6}namespace{color}{color:#d4d4d4}: {color}{color:#ce9178}lakehouse{color}
{color:#d4d4d4}---{color}
{color:#569cd6}apiVersion{color}{color:#d4d4d4}: {color}{color:#ce9178}rbac.authorization.k8s.io/v1{color}
{color:#569cd6}kind{color}{color:#d4d4d4}: {color}{color:#ce9178}Role{color}
{color:#569cd6}metadata{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  {color}{color:#569cd6}labels{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}    {color}{color:#569cd6}app{color}{color:#d4d4d4}: {color}{color:#ce9178}flink{color}
{color:#d4d4d4}  {color}{color:#569cd6}name{color}{color:#d4d4d4}: {color}{color:#ce9178}flink-role-binding-flink{color}
{color:#d4d4d4}  {color}{color:#569cd6}namespace{color}{color:#d4d4d4}: {color}{color:#ce9178}lakehouse{color}
{color:#569cd6}rules{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}- {color}{color:#569cd6}apiGroups{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  - {color}{color:#ce9178}""""{color}
{color:#d4d4d4}  {color}{color:#569cd6}resources{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  - {color}{color:#ce9178}configmaps{color}
{color:#d4d4d4}  {color}{color:#569cd6}verbs{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  - {color}{color:#ce9178}get{color}
{color:#d4d4d4}  - {color}{color:#ce9178}create{color}
{color:#d4d4d4}  - {color}{color:#ce9178}delete{color}
{color:#d4d4d4}  - {color}{color:#ce9178}update{color}
{color:#d4d4d4}  - {color}{color:#ce9178}list{color}
{color:#d4d4d4}  - {color}{color:#ce9178}watch{color}
{color:#d4d4d4}---{color}
{color:#569cd6}apiVersion{color}{color:#d4d4d4}: {color}{color:#ce9178}rbac.authorization.k8s.io/v1{color}
{color:#569cd6}kind{color}{color:#d4d4d4}: {color}{color:#ce9178}RoleBinding{color}
{color:#569cd6}metadata{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  {color}{color:#569cd6}labels{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}    {color}{color:#569cd6}app{color}{color:#d4d4d4}: {color}{color:#ce9178}flink{color}
{color:#d4d4d4}  {color}{color:#569cd6}name{color}{color:#d4d4d4}: {color}{color:#ce9178}flink-role-binding-default{color}
{color:#d4d4d4}  {color}{color:#569cd6}namespace{color}{color:#d4d4d4}: {color}{color:#ce9178}lakehouse{color}
{color:#569cd6}roleRef{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}  {color}{color:#569cd6}apiGroup{color}{color:#d4d4d4}: {color}{color:#ce9178}rbac.authorization.k8s.io{color}
{color:#d4d4d4}  {color}{color:#569cd6}kind{color}{color:#d4d4d4}: {color}{color:#ce9178}Role{color}
{color:#d4d4d4}  {color}{color:#569cd6}name{color}{color:#d4d4d4}: {color}{color:#ce9178}flink-role-binding-flink{color}
{color:#569cd6}subjects{color}{color:#d4d4d4}:{color}
{color:#d4d4d4}- {color}{color:#569cd6}kind{color}{color:#d4d4d4}: {color}{color:#ce9178}ServiceAccount{color}
{color:#d4d4d4}  {color}{color:#569cd6}name{color}{color:#d4d4d4}: {color}{color:#ce9178}flink-service-account{color}
{color:#d4d4d4}  {color}{color:#569cd6}namespace{color}{color:#d4d4d4}: {color}{color:#ce9178}lakehouse{color}
I was finally able to configure the k8s HA instance.

I will try to provide a PR containing some documentation improvements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 22:35:04 UTC 2023,,,,,,,,,,"0|z1f53s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create E2E Test for GlueCatalog ,FLINK-30742,13520067,13485272,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,18/Jan/23 13:57,29/Apr/23 09:02,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-18 13:57:57.0,,,,,,,,,,"0|z1f53k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPv6HostnamesITCase should create remote rpc service,FLINK-30741,13520065,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Jan/23 13:54,23/Jan/23 15:19,04/Jun/24 20:41,23/Jan/23 15:19,1.14.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,,,,"The test wants to ensure that Akka's Netty can bind to IPv6 addresses, but uses a local rpc service which doesn't use netty.
This was incorrectly migrated in FLINK-23090.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23090,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 23 15:19:11 UTC 2023,,,,,,,,,,"0|z1f534:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/23 15:19;chesnay;master: 359217381a3a5e48133614407445435c3bc97622;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT to Kafka does not work when Kafka config auto.create.topics.enabled is set to false,FLINK-30740,13520061,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,cadonna,cadonna,18/Jan/23 13:21,29/Mar/23 01:55,04/Jun/24 20:41,29/Mar/23 01:55,table-store-0.3.0,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"If I use Kafka as the log system and set {{auto.create.topics.enabled}} to false in Kafka INSERTs do not work.

Steps to reproduce:

# Start a Kafka broker and set {{auto.create.topics.enabled}} to false
# Issue the following statements
{code:sql}
CREATE CATALOG table_store_catalog WITH (
   'type'='table-store',
   'warehouse'=<path to object store>
);

USE CATALOG table_store_catalog;

CREATE TABLE word_count (
      word STRING PRIMARY KEY NOT ENFORCED,
      cnt BIGINT
 ) WITH (
   'log.system' = 'kafka',
   'kafka.bootstrap.servers' = <address to broker>,
   'kafka.topic' = 'test-topic,
   'log.consistency' = 'eventual'
 );
 
 INSERT INTO word_count VALUES ('foo', 1);
{code} 

The task manager logs show:
{code}
flink-sandbox-taskmanager-1  | 2023-01-18 12:46:17,085 WARN  org.apache.flink.table.store.shaded.org.apache.kafka.clients.NetworkClient [] - [Producer clientId=producer-1] Error while fetching metadata with correlation id 544 : {test-topic=UNKNOWN_TOPIC_OR_PARTITION}
{code}

The INSERT job on the task manager fails with
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.store.kafka.KafkaSinkFunction.lambda$open$0(KafkaSinkFunction.java:75)
	at org.apache.flink.table.store.shaded.org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:982)
	at org.apache.flink.table.store.shaded.org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:885)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.send(FlinkKafkaInternalProducer.java:142)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:926)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:101)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:245)
	at org.apache.flink.table.store.connector.sink.StoreWriteOperator.processElement(StoreWriteOperator.java:134)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Pending record count must be zero at this point: 1
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1428)
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:976)
		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
		at org.apache.flink.table.store.connector.sink.StoreWriteOperator.close(StoreWriteOperator.java:166)
		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163)
		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
		at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916)
		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930)
		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930)
		... 3 more
	Caused by: java.lang.IllegalStateException: Pending record count must be zero at this point: 1
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:1111)
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:936)
		... 15 more
{code}

Apparently, the Kafka topic is created when the first record is written to the Kafka topic, although I found code to create a Kafka topic explicitly on table creation:
https://github.com/apache/flink-table-store/blob/f201b507fef88501c4beb4c62807bef818e31be5/flink-table-store-kafka/src/main/java/org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java#L123

Topic creation should not rely on enabling auto topic creation in Kafka, because users might opt to disable auto topic creation to prevent unexpected costs when a fully-managed Kafka service is used. For example see the Confluent Cloud documentation:
https://docs.confluent.io/cloud/current/clusters/broker-config.html#enable-automatic-topic-creation  

IMO, when a table is created, the corresponding Kafka topic should be explicitly created.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:55:34 UTC 2023,,,,,,,,,,"0|z1f528:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 01:55;lzljs3620320;https://github.com/apache/incubator-paimon/issues/740;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayRestEndpointStatementITCase failed with NullPointer,FLINK-30739,13520048,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunta,yunta,18/Jan/23 12:50,24/Jan/23 04:12,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,"Instance https://myasuka.visualstudio.com/flink/_build/results?buildId=437&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f


{code:java}
Jan 18 10:54:20 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.testFlinkSqlStatements  Time elapsed: 1.37 s  <<< FAILURE!
Jan 18 10:54:20 org.opentest4j.AssertionFailedError:
Jan 18 10:54:20 
Jan 18 10:54:20 expected: 
Jan 18 10:54:20   ""# table.q - CREATE/DROP/SHOW/ALTER/DESCRIBE TABLE
Jan 18 10:54:20   #
Jan 18 10:54:20   # Licensed to the Apache Software Foundation (ASF) under one or more
Jan 18 10:54:20   # contributor license agreements.  See the NOTICE file distributed with
Jan 18 10:54:20   # this work for additional information regarding copyright ownership.
Jan 18 10:54:20   # The ASF licenses this file to you under the Apache License, Version 2.0
Jan 18 10:54:20   # (the ""License""); you may not use this file except in compliance with
Jan 18 10:54:20   # the License.  You may obtain a copy of the License at
Jan 18 10:54:20   #
Jan 18 10:54:20   # http://www.apache.org/licenses/LICENSE-2.0
Jan 18 10:54:20   #
Jan 18 10:54:20   # Unless required by applicable law or agreed to in writing, software
Jan 18 10:54:20   # distributed under the License is distributed on an ""AS IS"" BASIS,
Jan 18 10:54:20   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
Jan 18 10:54:20   # See the License for the specific language governing permissions and
Jan 18 10:54:20   # limitations under the License.
Jan 18 10:54:20   
Jan 18 10:54:20   # ==========================================================================
Jan 18 10:54:20   # validation test
Jan 18 10:54:20   # ==========================================================================
Jan 18 10:54:20   
Jan 18 10:54:20   create table tbl(a int, b as invalid_function());
Jan 18 10:54:20   !output
Jan 18 10:54:20   org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature invalid_function()
Jan 18 10:54:20   !error

{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 24 04:12:53 UTC 2023,,,,,,,,,,"0|z1f4zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/23 04:12;Wencong Liu;I seems like a existed similar issue. cc [~fsk119] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable verbose mode,FLINK-30738,13520044,13516733,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,18/Jan/23 12:34,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,,,1.20.0,,,,,,,,,,,,,,,,Build System,,,,,0,pull-request-available,,,,"The plugin has verbose mode enabled by default. we can reduce some noise in the build output by disabling that.

Specifically, this part can be omitted:

{code:java}
[INFO] CycloneDX: Parameters
[INFO] ------------------------------------------------------------------------
[INFO] schemaVersion          : 1.4
[INFO] includeBomSerialNumber : true
[INFO] includeCompileScope    : true
[INFO] includeProvidedScope   : true
[INFO] includeRuntimeScope    : true
[INFO] includeTestScope       : false
[INFO] includeSystemScope     : true
[INFO] includeLicenseText     : true
[INFO] outputReactorProjects  : true
[INFO] outputFormat           : all
[INFO] outputName             : bom
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 19 15:07:25 UTC 2023,,,,,,,,,,"0|z1f4yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 15:25;chesnay;parent_pom: 21256b1a111610df3c8e53dd9cc7dfb4c156157e;;;","19/Oct/23 15:07;jiabao.sun;Hi [~chesnay], could you help review that PR when you have time?
Many thanks for that :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup basic CI,FLINK-30737,13520038,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Jan/23 12:16,18/Jan/23 13:37,04/Jun/24 20:41,18/Jan/23 13:37,,,,,,,connector-parent-1.0.0,,,,,,,,,,,,,,,,Connectors / Parent,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 13:37:19 UTC 2023,,,,,,,,,,"0|z1f4x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 13:37;chesnay;parent_pom: 540c620a35ec1b2251f4c4ee799b1d4689078bbe ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude generated sources from checkstyle,FLINK-30736,13520034,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,18/Jan/23 12:09,18/Jan/23 13:36,04/Jun/24 20:41,18/Jan/23 13:36,,,,,,,connector-parent-1.0.0,,,,,,,,,,,,,,,,Connectors / Parent,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 13:36:20 UTC 2023,,,,,,,,,,"0|z1f4w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 13:36;chesnay;parent_pom: db590e5f3576dba26028cc4959f9ae9704463922;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Top-N"" page of ""Querys"" into Chinese",FLINK-30735,13520021,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 10:26,19/Mar/23 10:59,04/Jun/24 20:41,19/Mar/23 10:57,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/topn]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/topn.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 10:57:09 UTC 2023,,,,,,,,,,"0|z1f4tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 10:29;chenhaiyang;Hi [~jark] ,The pull request for this issue has been committed,Please see [https://github.com/apache/flink/pull/21715];;;","19/Mar/23 10:57;jark;Fixed in master: 460227a7a89852b057a74ead75fa291483921930 and 598f92f46f7773228403becd449ce422b61cdc9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KBinsDiscretizer handles Double.NaN incorrectly,FLINK-30734,13520020,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hongfanxo,hongfanxo,18/Jan/23 10:19,07/Feb/23 03:32,04/Jun/24 20:41,,ml-2.1.0,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,"When the training data contains Double.NaN values and the strategy is set to ""quantile"", the generated model data has Double.NaN as the right edge of the largest bin.

My expected behavior is to ignore Double.NaN values when training, and to support skip/error/keep strategy when transforming with generated KBinsDiscretizerModel.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 03:32:01 UTC 2023,,,,,,,,,,"0|z1f4t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 03:32;hongfanxo;Sklearn has a discussion about this feature: [1] 

SparkML already supports this feature in a similar algorithm named QuantileDiscretizer: [2]

 

[1][https://github.com/scikit-learn/scikit-learn/issues/9341]

[2]https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.QuantileDiscretizer.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The AzureCI Slack bot does not list all failed builds in the Slack channel,FLINK-30733,13520009,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,mapohl,mapohl,18/Jan/23 09:00,30/Mar/23 07:31,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,Test Infrastructure,,,,,0,test-stability,,,,"We've experienced failing builds not being reported by the AzureCI Slack bot in [Apache Flink/#builds Slack channel|https://apache-flink.slack.com/archives/C03MR1HQHK2].

Builds affected:
 * [20230118.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44988&view=results] ([""manual"" Slack thread|https://apache-flink.slack.com/archives/C03MR1HQHK2/p1674032103576869])
 * [20230113.8|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44782&view=results] ([""manual"" Slack thread|https://apache-flink.slack.com/archives/C03MR1HQHK2/p1673615705103309])
 * [20230119.6|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45044&view=results] ([""manual"" Slack thread|https://apache-flink.slack.com/archives/C03MR1HQHK2/p1674136551787109])
* [20230121.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=results] ([""manual"" Slack thread|https://apache-flink.slack.com/archives/C03MR1HQHK2/p1674546072581919])
* [20230123.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45149&view=results] ([""manual"" Slack thread|https://apache-flink.slack.com/archives/C03MR1HQHK2/p1674546051484829])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 07:31:28 UTC 2023,,,,,,,,,,"0|z1f4qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 13:33;mapohl;We couldn't find any logs so far. I filed a bug report in the Azure Dev Community: https://developercommunity.visualstudio.com/t/Slack-bot-doesnt-report-all-failed-buil/10256169;;;","24/Jan/23 13:03;mapohl;Just for documentation purposes: One request from the Azure support was to set up a 2nd channel and subscribe to the project from within that channel. That way, we could observe whether the same builds are affected for both channel or whether they differ.;;;","24/Jan/23 14:32;martijnvisser;Created #builds-debug to see if both channels get different results;;;","24/Jan/23 15:22;mapohl;Two additional builds were missed (which happened before {{#builds-debug}} was set up):
* [20230124.3|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45169&view=results]
* [20230124.4|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45170&view=results];;;","30/Jan/23 08:34;mapohl;[20230130.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=results] failed and was not reported in both channels: {{#builds}} and {{{}#builds-debug{}}}.;;;","30/Jan/23 08:41;mapohl;There's [a request from the support team|https://developercommunity.visualstudio.com/t/Slack-bot-doesnt-report-all-failed-buil/10256169#T-N10261706] to check the Service Hooks settings:
{quote}
Please kindly go to Project Settings>Service Hooks to check the history of corresponding consumer.

<img/>
If the failed build information shows well here but missed in Slack channel, we would recommend you contact Slack support for more professional help via Slack Community , as the History from Project Settings>Service Hooks shows the notifications sent successfully from Azure DevOps side, there might be some issue on receiving the notifications on Slack side.

If the History missed the builds either, please 1. try to create a new Slack channel to check if there is the same issue.
2. Try to remove Azure Pipelines app for Slack from your Slack and install the app again to check if there is the same issue. (Note: after uninstallation of app all the subscription might be automatically deleted from DB and cannot be recovered)
{quote}
Screenshots are provided in the Azure Support Request thread.;;;","30/Jan/23 13:25;mapohl;The recently missed failed build {{20230130.1}} did show up in the events logs. This indicates that it's rather a Slack issue. The Azure Dev Support proposed reaching out the Slack community to get further help in this matter. I created a [bug report in the Slack Community|https://forums.slackcommunity.com/s/question/0D53a00008yXyq4CAC/events-that-were-sent-by-a-slack-bot-seem-to-have-been-swallowed-by-slack].;;;","06/Feb/23 16:26;mapohl;* [20230206.10|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45761&view=results];;;","20/Feb/23 10:17;mapohl;[20230218.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46283&view=results];;;","20/Feb/23 17:05;mapohl;[~martijnvisser] could you create a support ticket in the Apache Flink workspace. We would link it to the existing support request that was accidentally created in my employer's Slack workspace.;;;","23/Feb/23 09:00;martijnvisser;[~mapohl] It can be found under https://apache-flink.slack.com/help/requests/4664607;;;","23/Feb/23 10:21;mapohl;Thanks, I forwarded that one to the support agent.;;;","24/Feb/23 08:01;mapohl;I've reached out to the email address {{AzureDevOpsSlackApps@microsoft.com}} listed in [1] in this matter.

[1] https://apache-flink.slack.com/apps/AFH4Y66N9-azure-pipelines?tab=more_info;;;","09/Mar/23 08:13;mapohl;* [20230308.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46927&view=results]
* [20230315.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47161&view=results];;;","29/Mar/23 11:40;mapohl;I opened a 2nd report on the Azure Dev Community which was closed right away and merged with the previous bug report. I'm getting a bit tired of following up on that one. I'm inclined to close this issue and leave it like that. 

We seem to run into this slack notification issue rather rarely. Severe Flink test stability issues should appear more often and, therefore, should be also captured with other failed builds. [~martijnvisser] any objections against that or ideas how to proceed with that issue?;;;","30/Mar/23 07:31;martijnvisser;[~mapohl] I think we downgrade the priority to Not a priority but leave it open in case we want to pick it up again in the future;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Running HA (hashmap, sync) end-to-end test"" failed with exit code 1",FLINK-30732,13520005,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,18/Jan/23 08:50,19/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,auto-deprioritized-critical,test-stability,,,"""Running HA (hashmap, sync) end-to-end test"" failed in the following build:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44988&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=13253

Some initial investigation revealed that there might have been problems with the TaskManager availability? I found a reoccuring stacktrace about a slot allocation failure:
{code}
2023-01-18 02:17:08,677 WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Slot allocation for slot 127.0.0.1:43245-83194d_1 for job 3d663afea41cc2fdec0e2c91b671f2bc failed.
org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException: Could not send message [RemoteRpcInvocation(TaskExecutorGateway.requestSlot(SlotID, JobID, AllocationID, ResourceProfile, String, ResourceManagerId, Time))] from sender [Actor[akka://flink/temp/taskmanager_0$oh]] to recipient [Actor[akka.tcp://flink@127.0.0.1:43245/user/rpc/taskmanager_0#1526007138]], because the recipient is unreachable. This can either mean that the recipient has been terminated or that the remote RpcService is currently not reachable.
        at org.apache.flink.runtime.rpc.akka.DeadLettersActor.handleDeadLetter(DeadLettersActor.java:61) ~[flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_a2e10295-0436-48e6-a879-dfe2b8967a56.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) [?:?]
        at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) [?:?]
        at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) [?:?]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) [?:?]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183) [?:?]
{code} ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23210,,,FLINK-30909,,,,,,"18/Jan/23 08:53;mapohl;FLINK-30732.tar.gz;https://issues.apache.org/jira/secure/attachment/13054661/FLINK-30732.tar.gz",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:05 UTC 2023,,,,,,,,,,"0|z1f4ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 08:54;mapohl;I attached the logs for the build mentioned in the description. The stacktrace mentioned in the description is part of the JobManager logs {{flink-logs/flink-vsts-standalonesession-2-fv-az68-971.log}}.;;;","18/Jan/23 08:56;mapohl;I'm linking FLINK-23210 as related because it covers the very same test but the issue was closed as {{Not Reproducible}}.;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Set Operations"" page of ""Querys"" into Chinese ",FLINK-30731,13520001,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 08:08,19/Mar/23 11:54,04/Jun/24 20:41,19/Mar/23 11:54,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/set-ops]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/set-ops.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 11:54:07 UTC 2023,,,,,,,,,,"0|z1f4ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 08:13;chenhaiyang;Hi [~jark] ,The pull request for this issue has been committed,Please see [https://github.com/apache/flink/pull/21713];;;","19/Mar/23 11:54;jark;Fixed in master: aec9efb811bb37b28e168435d679416044ac5d8e and 9830fd831cfc1ffa1382270a0f329c509d9021f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringIndexer cannot handle null values correctly,FLINK-30730,13520000,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,18/Jan/23 07:58,08/Feb/23 02:39,04/Jun/24 20:41,07/Feb/23 11:32,ml-2.1.0,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"When training data contains null values, StringIndexer throws a exception. The reason is this method [1]: null values are neither String type nor Number type.

In StringIndexerModel, null values are also not handled correctly when performing transformation.

 

[1] [https://github.com/apache/flink-ml/blob/966cedd7bbab4e12d8d8b37dbd582146714e68a6/flink-ml-lib/src/main/java/org/apache/flink/ml/feature/stringindexer/StringIndexer.java#L164]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 11:32:24 UTC 2023,,,,,,,,,,"0|z1f4oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:32;zhangzp;Fixed on master via 2b83d247cb5e81fb04b31399d436dbb9e809a473;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregateITCase generates stacktraces of IllegalStateExceptions in Maven output but doesn't fail,FLINK-30729,13519996,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,mapohl,mapohl,18/Jan/23 07:41,24/Apr/23 13:51,04/Jun/24 20:41,24/Apr/23 13:51,1.17.0,,,,,,1.17.1,,,,,,,,,,,,,,,,Runtime / State Backends,Table SQL / API,,,,0,pull-request-available,,,,"We're seeing some weird IllegalStateException stacktraces in CI for {{o.a.f.table.planner.runtime.stream.table.AggregateITCase}} which, interestingly, doesn't cause the test to fail. That's something we should investigate, I guess:
{code}
java.lang.IllegalStateException: LogWriter is closed
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
	at org.apache.flink.runtime.state.changelog.inmemory.InMemoryStateChangelogWriter.appendMeta(InMemoryStateChangelogWriter.java:64)
	at org.apache.flink.state.changelog.AbstractStateChangeLogger.logMetaIfNeeded(AbstractStateChangeLogger.java:156)
	at org.apache.flink.state.changelog.AbstractStateChangeLogger.log(AbstractStateChangeLogger.java:140)
	at org.apache.flink.state.changelog.AbstractStateChangeLogger.valueCleared(AbstractStateChangeLogger.java:104)
	at org.apache.flink.state.changelog.ChangelogListState.clear(ChangelogListState.java:113)
	at org.apache.flink.runtime.state.ttl.TtlListState$IteratorWithCleanup.cleanupIfEmpty(TtlListState.java:193)
	at org.apache.flink.runtime.state.ttl.TtlListState$IteratorWithCleanup.hasNext(TtlListState.java:186)
	at org.apache.flink.table.planner.runtime.utils.JavaUserDefinedAggFunctions$DataViewTestAgg.getValue(JavaUserDefinedAggFunctions.java:355)
	at GroupAggsHandler$10675.getValue(Unknown Source)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:146)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
	at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor$0(RecordProcessorUtils.java:60)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:236)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:546)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:835)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:784)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:945)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:924)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:738)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13847",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 13:51:17 UTC 2023,,,,,,,,,,"0|z1f4ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 07:43;mapohl;[~Yanfei Lei] may you take a look at this (I'm mentioning you because you added this condition in FLINK-23035)?;;;","30/Jan/23 06:52;Yanfei Lei;Sorry for the late reply, I'll take a look. ;;;","03/Feb/23 07:13;Yanfei Lei;Cause [most tests|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/table/AggregateITCase.scala#L241] in {{o.a.f.table.planner.runtime.stream.table.AggregateITCase}} use `failingDataSource` to test job failover, this checkState condition is triggered during the job failover, so it will not cause the test to fail.

 

The following is one of the test logs, we can see that the job has experienced failover in this test, maybe we can close this ticket.

 

 
Test testPrecisionForSumAggregationOnDecimal[StateBackend=ROCKSDB](org.apache.flink.table.planner.runtime.stream.table.AggregateITCase) is running.
--------------------------------------------------------------------------------
01:09:01,401 [ main] INFO org.apache.flink.runtime.testutils.PseudoRandomValueSelector [] - Randomly selected true for execution.checkpointing.unaligned.enabled
01:09:01,401 [ main] INFO org.apache.flink.runtime.testutils.PseudoRandomValueSelector [] - Randomly selected PT0S for execution.checkpointing.aligned-checkpoint-timeout
01:09:01,401 [ main] INFO org.apache.flink.runtime.testutils.PseudoRandomValueSelector [] - Randomly selected true for state.backend.changelog.enabled
01:09:01,401 [ main] INFO org.apache.flink.runtime.testutils.PseudoRandomValueSelector [] - Randomly selected PT0.1S for state.backend.changelog.periodic-materialize.interval
01:09:01,408 [ main] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
01:09:01,416 [ main] INFO org.apache.flink.api.java.typeutils.TypeExtractor [] - Field Row#fieldByName will be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance and schema evolution.
01:09:01,416 [ main] INFO org.apache.flink.api.java.typeutils.TypeExtractor [] - class java.util.LinkedHashMap does not contain a getter for field accessOrder
01:09:01,416 [ main] INFO org.apache.flink.api.java.typeutils.TypeExtractor [] - class java.util.LinkedHashMap does not contain a setter for field accessOrder
01:09:01,416 [ main] INFO org.apache.flink.api.java.typeutils.TypeExtractor [] - Class class java.util.LinkedHashMap cannot be used as a POJO type because not all fields are valid POJO fields, and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance and schema evolution.
01:09:01,416 [ main] INFO org.apache.flink.api.java.typeutils.TypeExtractor [] - Field Row#positionByName will be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance and schema evolution.
01:09:01,416 [ main] INFO org.apache.flink.api.java.typeutils.TypeExtractor [] - class org.apache.flink.types.Row is missing a default constructor so it cannot be used as a POJO type and must be processed as GenericType. Please read the Flink documentation on ""Data Types & Serialization"" for details of the effect on performance and schema evolution.
01:09:01,501 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Received JobGraph submission 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,501 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Submitting job 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,501 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: JobMasterServiceLeadershipRunner
01:09:01,501 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_715 .
01:09:01,501 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Initializing job 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,503 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0) for Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,503 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Created execution graph adcc2a646d5d2f67d85c7ff837ecdef9 for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,503 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Running initialization on master for job Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,503 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Successfully ran initialization on master in 0 ms.
01:09:01,503 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 0 ms, total 1 pipelined regions currently.
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using job/cluster config to configure application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.state.changelog.AbstractChangelogStateBackend [] - ChangelogStateBackend is used, delegating RocksDBStateBackend.
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.state.StateBackendLoader [] - State backend loader loads ChangelogStateBackend to delegate RocksDBStateBackend
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using legacy state backend RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152} as Job checkpoint storage
01:09:01,505 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - No checkpoint found during restore.
01:09:01,506 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@344df908 for Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,506 [jobmanager-io-thread-17] INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka://flink/user/rpc/jobmanager_715 , session=da0f1504-fdd5-494d-8a64-759db5b33b5e
01:09:01,506 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Starting execution of job 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49) under job master id 8a64759db5b33b5eda0f1504fdd5494d.
01:09:01,506 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
01:09:01,506 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49) switched from state CREATED to RUNNING.
01:09:01,507 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to SCHEDULED.
01:09:01,507 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from CREATED to SCHEDULED.
01:09:01,507 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Connecting to ResourceManager akka://flink/user/rpc/resourcemanager_692(9a64c20cfad53b52a522a2b00a584392)
01:09:01,507 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Resolved ResourceManager address, beginning registration
01:09:01,507 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 8a64759db5b33b5eda0f1504fdd5494d@akka://flink/user/rpc/jobmanager_715 for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,508 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 8a64759db5b33b5eda0f1504fdd5494d@akka://flink/user/rpc/jobmanager_715 for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,508 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - JobManager successfully registered at ResourceManager, leader id: 9a64c20cfad53b52a522a2b00a584392.
01:09:01,508 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 342047f7eef5e3f1ba2c9dac1ca59a49: [ResourceRequirement\{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
01:09:01,563 [ Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint for job 342047f7eef5e3f1ba2c9dac1ca59a49 since Checkpoint triggering task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) of job 342047f7eef5e3f1ba2c9dac1ca59a49 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running..
01:09:01,570 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Receive slot request 3a3d35a0d8345d765a91cd668f0b8bfa for job 342047f7eef5e3f1ba2c9dac1ca59a49 from resource manager with leader id 9a64c20cfad53b52a522a2b00a584392.
01:09:01,570 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Allocated slot for 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,570 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job 342047f7eef5e3f1ba2c9dac1ca59a49 for job leader monitoring.
01:09:01,570 [mini-cluster-io-thread-3] INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka://flink/user/rpc/jobmanager_715 with leader id da0f1504-fdd5-494d-8a64-759db5b33b5e.
01:09:01,570 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka://flink/user/rpc/jobmanager_715 for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Establish JobManager connection for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Offer reserved slots to the leader of job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from SCHEDULED to DEPLOYING.
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (attempt #0) with attempt id adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 93621b6a-2d19-4fdd-a0a4-2f2c26ecfe0a @ localhost (dataPort=-1) with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from SCHEDULED to DEPLOYING.
01:09:01,571 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (attempt #0) with attempt id adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0 and vertex id c27dcf7b54ef6bfd6cff02ca8870b681_0 to 93621b6a-2d19-4fdd-a0a4-2f2c26ecfe0a @ localhost (dataPort=-1) with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa
01:09:01,572 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,572 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory'.
01:09:01,573 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0), deploy into slot with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,573 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,573 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from CREATED to DEPLOYING.
01:09:01,573 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) [DEPLOYING].
01:09:01,574 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0), deploy into slot with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,574 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,575 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from CREATED to DEPLOYING.
01:09:01,575 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) [DEPLOYING].
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using job/cluster config to configure application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using job/cluster config to configure application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.state.changelog.AbstractChangelogStateBackend [] - ChangelogStateBackend is used, delegating RocksDBStateBackend.
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.changelog.AbstractChangelogStateBackend [] - ChangelogStateBackend is used, delegating RocksDBStateBackend.
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.state.StateBackendLoader [] - State backend loader loads ChangelogStateBackend to delegate RocksDBStateBackend
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.state.StateBackendLoader [] - State backend loader loads ChangelogStateBackend to delegate RocksDBStateBackend
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using legacy state backend RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152} as Job checkpoint storage
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using legacy state backend RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152} as Job checkpoint storage
01:09:01,576 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
01:09:01,576 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from DEPLOYING to INITIALIZING.
01:09:01,576 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from DEPLOYING to INITIALIZING.
01:09:01,576 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from DEPLOYING to INITIALIZING.
01:09:01,598 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting shared memory for RocksDB: shareScope=SLOT, managed=false
01:09:01,598 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 20971520 bytes
01:09:01,604 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
01:09:01,604 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to RUNNING.
01:09:01,614 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Finished building RocksDB keyed state-backend at /tmp/junit7725981464549332018/junit7198969602070316517/minicluster_731556ba7d1abf61a0c79e8d9425fd67/tm_0/tmp/job_342047f7eef5e3f1ba2c9dac1ca59a49_op_KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681__1_1__uuid_5a1c245f-733b-408e-9a19-05bfef176535.
01:09:01,614 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 starts periodic materialization
01:09:01,614 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 schedules the next materialization in 0 seconds
01:09:01,635 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] WARN org.apache.flink.contrib.streaming.state.RocksDBOperationUtils [] - RocksDBStateBackend performance will be poor because of the current Flink memory configuration! RocksDB will flush memtable constantly, causing high IO and CPU. Typically the easiest fix is to increase task manager managed memory size. If running locally, see the parameter taskmanager.memory.managed.size. Details: arenaBlockSize 8388608 > mutableLimit 6116692 (writeBufferSize = 67108864, arenaBlockSizeConfigured = 0, defaultArenaBlockSize = 8388608, writeBufferManagerCapacity = 6990506)
01:09:01,643 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] WARN org.apache.flink.contrib.streaming.state.RocksDBOperationUtils [] - RocksDBStateBackend performance will be poor because of the current Flink memory configuration! RocksDB will flush memtable constantly, causing high IO and CPU. Typically the easiest fix is to increase task manager managed memory size. If running locally, see the parameter taskmanager.memory.managed.size. Details: arenaBlockSize 8388608 > mutableLimit 6116692 (writeBufferSize = 67108864, arenaBlockSizeConfigured = 0, defaultArenaBlockSize = 8388608, writeBufferManagerCapacity = 6990506)
01:09:01,652 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] WARN org.apache.flink.contrib.streaming.state.RocksDBOperationUtils [] - RocksDBStateBackend performance will be poor because of the current Flink memory configuration! RocksDB will flush memtable constantly, causing high IO and CPU. Typically the easiest fix is to increase task manager managed memory size. If running locally, see the parameter taskmanager.memory.managed.size. Details: arenaBlockSize 8388608 > mutableLimit 6116692 (writeBufferSize = 67108864, arenaBlockSizeConfigured = 0, defaultArenaBlockSize = 8388608, writeBufferManagerCapacity = 6990506)
01:09:01,663 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Initialize Materialization. Current changelog writers last append to sequence number 0
01:09:01,663 [ Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint for job 342047f7eef5e3f1ba2c9dac1ca59a49 since Checkpoint triggering task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) of job 342047f7eef5e3f1ba2c9dac1ca59a49 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running..
01:09:01,663 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 schedules the next materialization in 0 seconds
01:09:01,663 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 has no state updates since last materialization, skip this one and schedule the next one in 0 seconds
01:09:01,663 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from INITIALIZING to RUNNING.
01:09:01,664 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from INITIALIZING to RUNNING.
01:09:01,763 [ Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 1 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1674004141762 for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,763 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Initialize Materialization. Current changelog writers last append to sequence number 2
01:09:01,763 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Starting materialization from 0 : 2
01:09:01,775 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - snapshot of GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 for checkpoint 1, change range: 0..2, materialization ID 0
01:09:01,777 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 finishes materialization, updates the snapshotState upTo 2 : org.apache.flink.runtime.state.SnapshotResult@c25958f
01:09:01,777 [AsyncOperations-thread-1] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 schedules the next materialization in 0 seconds
01:09:01,779 [jobmanager-io-thread-28] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Completed checkpoint 1 for job 342047f7eef5e3f1ba2c9dac1ca59a49 (4483 bytes, checkpointDuration=17 ms, finalizationTime=0 ms).
01:09:01,781 [Channel state writer Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 discarding 0 drained requests
01:09:01,782 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] WARN org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED with failure cause:
java.lang.Exception: Artificial Failure
at org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.run(FailingCollectionSource.java:172) ~[test-classes/:?]
at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
01:09:01,782 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0).
01:09:01,782 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FAILED to JobManager for task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#0 adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
01:09:01,783 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED on 93621b6a-2d19-4fdd-a0a4-2f2c26ecfe0a @ localhost (dataPort=-1).
java.lang.Exception: Artificial Failure
at org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.run(FailingCollectionSource.java:172) ~[test-classes/:?]
at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
01:09:01,784 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - 2 tasks will be restarted to recover the failed task adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_0.
01:09:01,784 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49) switched from state RUNNING to RESTARTING.
01:09:01,784 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from RUNNING to CANCELING.
01:09:01,784 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskmanager.Task [] - Attempting to cancel task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0).
01:09:01,784 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from RUNNING to CANCELING.
01:09:01,784 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskmanager.Task [] - Triggering cancellation of task code GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0).
01:09:01,785 [Channel state writer GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 discarding 0 drained requests
01:09:01,785 [Canceler for GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0).] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Shutting down PeriodicMaterializationManager.
01:09:01,787 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend [] - Closed RocksDB State Backend. Cleaning up RocksDB working directory /tmp/junit7725981464549332018/junit7198969602070316517/minicluster_731556ba7d1abf61a0c79e8d9425fd67/tm_0/tmp/job_342047f7eef5e3f1ba2c9dac1ca59a49_op_KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681__1_1__uuid_5a1c245f-733b-408e-9a19-05bfef176535.
01:09:01,791 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from CANCELING to CANCELED.
01:09:01,791 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0] INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0).
01:09:01,791 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state CANCELED to JobManager for task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#0 adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0.
01:09:01,791 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_0) switched from CANCELING to CANCELED.
01:09:01,792 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 342047f7eef5e3f1ba2c9dac1ca59a49
01:09:01,792 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49) switched from state RESTARTING to RUNNING.
01:09:01,792 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Restoring job 342047f7eef5e3f1ba2c9dac1ca59a49 from Checkpoint 1 @ 1674004141762 for 342047f7eef5e3f1ba2c9dac1ca59a49 located at file:/tmp/junit8120086463983465670/junit6880377978597470118/342047f7eef5e3f1ba2c9dac1ca59a49/chk-1.
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - No master state to restore
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from CREATED to SCHEDULED.
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from CREATED to SCHEDULED.
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from SCHEDULED to DEPLOYING.
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (attempt #1) with attempt id adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1 and vertex id cbc357ccb763df2852fee8c4fc7d55f2_0 to 93621b6a-2d19-4fdd-a0a4-2f2c26ecfe0a @ localhost (dataPort=-1) with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from SCHEDULED to DEPLOYING.
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (attempt #1) with attempt id adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1 and vertex id c27dcf7b54ef6bfd6cff02ca8870b681_0 to 93621b6a-2d19-4fdd-a0a4-2f2c26ecfe0a @ localhost (dataPort=-1) with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa
01:09:01,793 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,793 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 342047f7eef5e3f1ba2c9dac1ca59a49: [ResourceRequirement\{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
01:09:01,794 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1), deploy into slot with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,795 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,795 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from CREATED to DEPLOYING.
01:09:01,795 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) [DEPLOYING].
01:09:01,796 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1), deploy into slot with allocation id 3a3d35a0d8345d765a91cd668f0b8bfa.
01:09:01,796 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from CREATED to DEPLOYING.
01:09:01,796 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) [DEPLOYING].
01:09:01,796 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using job/cluster config to configure application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,796 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
01:09:01,796 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,797 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.state.changelog.AbstractChangelogStateBackend [] - ChangelogStateBackend is used, delegating RocksDBStateBackend.
01:09:01,797 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.state.StateBackendLoader [] - State backend loader loads ChangelogStateBackend to delegate RocksDBStateBackend
01:09:01,797 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using legacy state backend RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152} as Job checkpoint storage
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using job/cluster config to configure application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,797 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from DEPLOYING to INITIALIZING.
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Using predefined options: DEFAULT.
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using application-defined state backend: RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152}
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.changelog.AbstractChangelogStateBackend [] - ChangelogStateBackend is used, delegating RocksDBStateBackend.
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.state.StateBackendLoader [] - State backend loader loads ChangelogStateBackend to delegate RocksDBStateBackend
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.streaming.runtime.tasks.StreamTask [] - Using legacy state backend RocksDBStateBackend\{checkpointStreamBackend=File State Backend (checkpoints: 'file:/tmp/junit8120086463983465670/junit6880377978597470118', savepoints: 'null, fileStateThreshold: 20480), localRocksDbDirectories=null, enableIncrementalCheckpointing=true, numberOfTransferThreads=4, writeBatchSize=2097152} as Job checkpoint storage
01:09:01,797 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from DEPLOYING to INITIALIZING.
01:09:01,797 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from DEPLOYING to INITIALIZING.
01:09:01,801 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from DEPLOYING to INITIALIZING.
01:09:01,801 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from INITIALIZING to RUNNING.
01:09:01,801 [ Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint for job 342047f7eef5e3f1ba2c9dac1ca59a49 since Checkpoint triggering task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) of job 342047f7eef5e3f1ba2c9dac1ca59a49 is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running..
01:09:01,801 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from INITIALIZING to RUNNING.
01:09:01,802 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Getting shared memory for RocksDB: shareScope=SLOT, managed=false
01:09:01,802 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend [] - Obtained shared RocksDB cache of size 20971520 bytes
01:09:01,824 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Finished building RocksDB keyed state-backend at /tmp/junit7725981464549332018/junit7198969602070316517/minicluster_731556ba7d1abf61a0c79e8d9425fd67/tm_0/tmp/job_342047f7eef5e3f1ba2c9dac1ca59a49_op_KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681__1_1__uuid_00457371-7476-4757-9546-8bc8e92a1cec.
01:09:01,824 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.state.changelog.StateChangelogStorageLoader [] - Creating a changelog storage with name 'memory' to restore from 'InMemoryChangelogStateHandle'.
01:09:01,826 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] WARN org.apache.flink.contrib.streaming.state.RocksDBOperationUtils [] - RocksDBStateBackend performance will be poor because of the current Flink memory configuration! RocksDB will flush memtable constantly, causing high IO and CPU. Typically the easiest fix is to increase task manager managed memory size. If running locally, see the parameter taskmanager.memory.managed.size. Details: arenaBlockSize 8388608 > mutableLimit 6116692 (writeBufferSize = 67108864, arenaBlockSizeConfigured = 0, defaultArenaBlockSize = 8388608, writeBufferManagerCapacity = 6990506)
01:09:01,832 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 starts periodic materialization
01:09:01,832 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 schedules the next materialization in 0 seconds
01:09:01,833 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] WARN org.apache.flink.contrib.streaming.state.RocksDBOperationUtils [] - RocksDBStateBackend performance will be poor because of the current Flink memory configuration! RocksDB will flush memtable constantly, causing high IO and CPU. Typically the easiest fix is to increase task manager managed memory size. If running locally, see the parameter taskmanager.memory.managed.size. Details: arenaBlockSize 8388608 > mutableLimit 6116692 (writeBufferSize = 67108864, arenaBlockSizeConfigured = 0, defaultArenaBlockSize = 8388608, writeBufferManagerCapacity = 6990506)
01:09:01,844 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] WARN org.apache.flink.contrib.streaming.state.RocksDBOperationUtils [] - RocksDBStateBackend performance will be poor because of the current Flink memory configuration! RocksDB will flush memtable constantly, causing high IO and CPU. Typically the easiest fix is to increase task manager managed memory size. If running locally, see the parameter taskmanager.memory.managed.size. Details: arenaBlockSize 8388608 > mutableLimit 6116692 (writeBufferSize = 67108864, arenaBlockSizeConfigured = 0, defaultArenaBlockSize = 8388608, writeBufferManagerCapacity = 6990506)
01:09:01,851 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from INITIALIZING to RUNNING.
01:09:01,852 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from INITIALIZING to RUNNING.
01:09:01,858 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Initialize Materialization. Current changelog writers last append to sequence number 2
01:09:01,858 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Starting materialization from 0 : 2
01:09:01,873 [AsyncOperations-thread-1] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 schedules the next materialization in 0 seconds
01:09:01,873 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - Task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 finishes materialization, updates the snapshotState upTo 2 : org.apache.flink.runtime.state.SnapshotResult@35e4d010
01:09:01,901 [ Checkpoint Timer] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 2 (type=CheckpointType\{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1674004141900 for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,902 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.changelog.ChangelogKeyedStateBackend [] - snapshot of GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 for checkpoint 2, change range: 2..2, materialization ID 1
01:09:01,906 [jobmanager-io-thread-30] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Completed checkpoint 2 for job 342047f7eef5e3f1ba2c9dac1ca59a49 (22764 bytes, checkpointDuration=6 ms, finalizationTime=0 ms).
01:09:01,907 [Channel state writer Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 discarding 0 drained requests
01:09:01,907 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from RUNNING to FINISHED.
01:09:01,907 [Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1).
01:09:01,908 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1)#1 adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1.
01:09:01,908 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> SourceConversion[2291] -> Calc[2292] (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_cbc357ccb763df2852fee8c4fc7d55f2_0_1) switched from RUNNING to FINISHED.
01:09:01,909 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend [] - Closed RocksDB State Backend. Cleaning up RocksDB working directory /tmp/junit7725981464549332018/junit7198969602070316517/minicluster_731556ba7d1abf61a0c79e8d9425fd67/tm_0/tmp/job_342047f7eef5e3f1ba2c9dac1ca59a49_op_KeyedProcessOperator_c27dcf7b54ef6bfd6cff02ca8870b681__1_1__uuid_00457371-7476-4757-9546-8bc8e92a1cec.
01:09:01,910 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.state.common.PeriodicMaterializationManager [] - Shutting down PeriodicMaterializationManager.
01:09:01,911 [Channel state writer GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 discarding 0 drained requests
01:09:01,911 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from RUNNING to FINISHED.
01:09:01,911 [GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1] INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1).
01:09:01,911 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1)#1 adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1.
01:09:01,911 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - GroupAggregate[2294] -> SinkConversion[2295] -> Sink: Unnamed (1/1) (adcc2a646d5d2f67d85c7ff837ecdef9_c27dcf7b54ef6bfd6cff02ca8870b681_0_1) switched from RUNNING to FINISHED.
01:09:01,912 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Streaming Job (342047f7eef5e3f1ba2c9dac1ca59a49) switched from state RUNNING to FINISHED.
01:09:01,912 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 342047f7eef5e3f1ba2c9dac1ca59a49
01:09:01,912 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Stopping checkpoint coordinator for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,913 [flink-akka.actor.default-dispatcher-9] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job 342047f7eef5e3f1ba2c9dac1ca59a49 reached terminal state FINISHED.
01:09:01,913 [mini-cluster-io-thread-2] INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job 342047f7eef5e3f1ba2c9dac1ca59a49 has been registered for cleanup in the JobResultStore after reaching a terminal state.
01:09:01,913 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Stopping the JobMaster for job 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,913 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down
01:09:01,914 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Disconnect TaskExecutor 93621b6a-2d19-4fdd-a0a4-2f2c26ecfe0a because: Stopping JobMaster for job 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,914 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [3a3d35a0d8345d765a91cd668f0b8bfa].
01:09:01,914 [flink-akka.actor.default-dispatcher-8] INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Close ResourceManager connection 9671a7bdc2b6213f2d9c99d1018dbb85: Stopping JobMaster for job 'Flink Streaming Job' (342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,914 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:2, state:ACTIVE, resource profile: ResourceProfile\{taskHeapMemory=256.000gb (274877906944 bytes), taskOffHeapMemory=256.000gb (274877906944 bytes), managedMemory=20.000mb (20971520 bytes), networkMemory=16.000mb (16777216 bytes)}, allocationId: 3a3d35a0d8345d765a91cd668f0b8bfa, jobId: 342047f7eef5e3f1ba2c9dac1ca59a49).
01:09:01,914 [flink-akka.actor.default-dispatcher-5] INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 8a64759db5b33b5eda0f1504fdd5494d@akka://flink/user/rpc/jobmanager_715 for job 342047f7eef5e3f1ba2c9dac1ca59a49 from the resource manager.
01:09:01,914 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 342047f7eef5e3f1ba2c9dac1ca59a49 from job leader monitoring.
01:09:01,914 [flink-akka.actor.default-dispatcher-7] INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Close JobManager connection for job 342047f7eef5e3f1ba2c9dac1ca59a49.
01:09:01,915 [ main] INFO org.apache.flink.table.planner.runtime.stream.table.AggregateITCase [] -
--------------------------------------------------------------------------------
Test testPrecisionForSumAggregationOnDecimal[StateBackend=ROCKSDB](org.apache.flink.table.planner.runtime.stream.table.AggregateITCase) successfully run.
 

 

 

 ;;;","03/Feb/23 10:22;mapohl;Thanks for looking into it. Is there a way that we could clean this stacktrace up? Or is there no easy way to get around that? I just want to avoid that contributors are confused by it while going through the logs.;;;","06/Feb/23 06:35;Yanfei Lei;I open a PR to replace {{checkState()}} with {{LOG.warn()}} when StateChangelogWriter is closed, to avoid the `IllegalStateException` stacktrace, [~masteryhx] could you please help take a look?;;;","24/Apr/23 13:51;ym;merged commit [{{4ca3604}}|https://github.com/apache/flink/commit/4ca3604a6afb7dace03464fafe7c5f14ab95fc5b] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Window Join"" page of ""Querys"" into Chinese",FLINK-30728,13519995,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 07:32,19/Mar/23 12:20,04/Jun/24 20:41,19/Mar/23 12:20,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-join]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/window-join.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 12:20:10 UTC 2023,,,,,,,,,,"0|z1f4nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 07:36;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?

And the pull request has been committed.

Please see: [https://github.com/apache/flink/pull/21712];;;","19/Mar/23 12:20;jark;Fixed in master: 7c534f424dfa4185cd60b11a094c5f5dcd90e200 and 5a468f582f6d4d55b59d1f4cfe65e9dd5af7f4a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinReorderITCase.testBushyTreeJoinReorder failed due to IOException,FLINK-30727,13519992,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,337361684@qq.com,mapohl,mapohl,18/Jan/23 07:25,06/Feb/23 09:03,04/Jun/24 20:41,06/Feb/23 06:49,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Network,Table SQL / Planner,,,,0,pull-request-available,test-stability,,,"IOException due to timeout occurring while requesting exclusive NetworkBuffer caused JoinReorderITCase.testBushyTreeJoinReorder to fail:
{code}
[...]
Jan 18 01:11:27 Caused by: java.io.IOException: Timeout triggered when requesting exclusive buffers: The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max',  or you may increase the timeout which is 30000ms by setting the key 'taskmanager.network.memory.exclusive-buffers-request-timeout-ms'.
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalRequestMemorySegments(NetworkBufferPool.java:256)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestPooledMemorySegmentsBlocking(NetworkBufferPool.java:179)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.reserveSegments(LocalBufferPool.java:262)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setupChannels(SingleInputGate.java:517)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:277)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:962)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:648)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
Jan 18 01:11:27 	at java.lang.Thread.run(Thread.java:748)
{code}
Same build, 2 failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14300
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=14362",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30376,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 09:03:51 UTC 2023,,,,,,,,,,"0|z1f4mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 07:25;mapohl;[~zhengyunhong97] may you have a look at it?;;;","18/Jan/23 07:48;mapohl;This is marked as a blocker because it looks like it was newly introduced.;;;","18/Jan/23 07:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44993&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14068;;;","19/Jan/23 02:28;337361684@qq.com;Ok, [~mapohl] , I will look at it right away.;;;","20/Jan/23 00:59;lincoln.86xy;[~337361684@qq.com] I also encountered this failure in a local machine

 

{code}

[ERROR] Errors:
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:47 » Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:47 » Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testBushyTreeJoinReorder:352->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testBushyTreeJoinReorder:352->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithFullOuterJoin:174->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithFullOuterJoin:174->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndFullOuterJoin:192->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndFullOuterJoin:192->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndLeftOuterJoin:249->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndLeftOuterJoin:249->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerJoin:210->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerJoin:210->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithMixedJoinTypeAndCondition:300->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithMixedJoinTypeAndCondition:300->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithRightOuterJoin:267->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithRightOuterJoin:267->assertEquals:65 Runtime
[INFO]
[ERROR] Tests run: 5110, Failures: 0, Errors: 18, Skipped: 10

{code}

 

{code}

[ERROR] org.apache.flink.table.planner.runtime.batch.sql.join.JoinReorderITCase.testJoinReorderWithLeftOuterJoin(boolean)[1]  Time elapsed: 2.297 s  <<< ERROR!

Caused by: java.lang.IllegalArgumentException
    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
    at org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.<init>(BaseHybridHashTable.java:164)
    at org.apache.flink.table.runtime.hashtable.BinaryHashTable.<init>(BinaryHashTable.java:163)
    at org.apache.flink.table.runtime.operators.join.HashJoinOperator.open(HashJoinOperator.java:135)
    at org.apache.flink.table.runtime.operators.multipleinput.MultipleInputStreamOperatorBase.open(MultipleInputStreamOperatorBase.java:128)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:730)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:706)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:673)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:945)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:738)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
    at java.lang.Thread.run(Thread.java:877)

...

Caused by: java.io.IOException: Insufficient number of network buffers: required 97, but only 74 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys '
taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
        at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalCreateBufferPool(NetworkBufferPool.java:493)
        at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:466)
        at org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.lambda$createBufferPoolFactory$0(ResultPartitionFactory.java:337)
        at org.apache.flink.runtime.io.network.partition.ResultPartition.setup(ResultPartition.java:156)
        at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:956)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:648)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
        at java.lang.Thread.run(Thread.java:877)

{code};;;","20/Jan/23 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45082&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12221;;;","20/Jan/23 07:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45088&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12225;;;","22/Jan/23 07:55;337361684@qq.com;Hi, [~mapohl]  and [~lincoln.86xy]. Now, I cannot reproduce this problem in CI and local machine, I tried to fix it in this [pr|[https://github.com/apache/flink/pull/21724],] but it seems did not works.  It seems that this error is related to runtime, can [~wanglijie] also have a look at it.;;;","23/Jan/23 08:15;mapohl;Thanks [~337361684@qq.com]. Does it make sense to add log messages to get more information out of the CI builds?;;;","23/Jan/23 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45142&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12515;;;","23/Jan/23 08:17;mapohl;Here's another one where the same test fails but it has a different stacktrace:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45142&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12520

{code}
[...]
Jan 22 01:14:38 Caused by: org.apache.flink.runtime.io.network.partition.PartitionNotFoundException: Partition 05545fca496f58e5ce23b9216a2e6882#30@bc473b1e60621f0a2630dc1128de559d_2f556c400f5feeb5f996b57664ad8a98_30_0 not found.
Jan 22 01:14:38 	at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.createSubpartitionView(ResultPartitionManager.java:70)
Jan 22 01:14:38 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.requestSubpartition(LocalInputChannel.java:136)
Jan 22 01:14:38 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel$1.run(LocalInputChannel.java:186)
Jan 22 01:14:38 	at java.util.TimerThread.mainLoop(Timer.java:555)
Jan 22 01:14:38 	at java.util.TimerThread.run(Timer.java:505)
{code};;;","24/Jan/23 07:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45153&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12515;;;","24/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12521;;;","24/Jan/23 15:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45169&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12749;;;","24/Jan/23 15:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45170&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12231;;;","25/Jan/23 08:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12590;;;","26/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12590;;;","27/Jan/23 07:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45229&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12514;;;","27/Jan/23 09:12;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45225&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Jan/23 11:12;SleePy;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45311&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","30/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45240&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12537;;;","30/Jan/23 08:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45289&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12231;;;","30/Jan/23 08:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12306;;;","30/Jan/23 12:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45388&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12521;;;","30/Jan/23 13:28;zhuzh;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45404&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/Jan/23 02:05;godfrey;Fixed in 1.17.0: dcbb20688c0de238f65a6986f9888c5c5088e34a which adds more network memory

We will continue to follow this issue;;;","31/Jan/23 02:31;337361684@qq.com;Hi, all. The reason for the failure of this ITCase is that the network buffer are insufficient in streaming mode. Because I cannot reproduce it in local environment, I can't find the real cause of this error. So, the current solution is to allocate more network buffer by setting parameter 'taskmanager.memory.network.fraction' .

    After setting this parameter,  CI is stable.  We will continue to follow up on this error.;;;","31/Jan/23 07:39;mapohl;Both builds didn't include the aforementioned fix:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45408&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12241
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45418&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12235;;;","31/Jan/23 18:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45501&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12816;;;","01/Feb/23 01:18;lincoln.86xy;The build failure has included the fix

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45498&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","01/Feb/23 01:22;337361684@qq.com;Hi, [~mapohl] , The root cause of this error may be I didn't set a parallelism for TableEnvironment in this ITCase, so this ITCase used default parallelism which equals to CPU cores (In azure CI, CPU cores equals to 32) as the parallelism. For setting parallelism as 32 with the complex job graph in this case, network memory may be insufficient.

The solution to this error is to set the parallelism manually. I will verify it on a machine with a large number of cpu cores.;;;","01/Feb/23 06:13;fsk119;I meet mulitple times in the pipeline:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45530&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45530&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0c940707-2659-5648-cbe6-a1ad63045f0a]

Hope we can fix this soon;;;","01/Feb/23 09:19;mapohl;The fix [mentioned above|https://issues.apache.org/jira/browse/FLINK-30727?focusedCommentId=17682344&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17682344] is not included in the following failed builds:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45519&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12521
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12596
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12523
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12520;;;","01/Feb/23 09:28;mapohl;[~337361684@qq.com] [~godfrey] The following build failed but included your fix:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45524&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12237;;;","01/Feb/23 10:20;337361684@qq.com;The root cause hotfix pr: https://github.com/apache/flink/pull/21821;;;","01/Feb/23 13:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12525;;;","02/Feb/23 08:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12521;;;","02/Feb/23 14:18;mapohl;The run doesn't include the fix 3ac2c330bad mentioned below, yet:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45616&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12820];;;","02/Feb/23 14:20;mapohl;master: 3ac2c330bad3f7d41721232397581b682ee7d64a;;;","02/Feb/23 14:23;mapohl;[~337361684@qq.com] is [PR #21821|https://github.com/apache/flink/pull/21821] meant to be the final fix and this Jira issue can be closed?;;;","03/Feb/23 01:43;337361684@qq.com;Hi, [~mapohl] , I need to observe the running of CI for a day. If there is no unstable CI Tests caused by this error, it can be closed. Thanks.;;;","06/Feb/23 01:21;337361684@qq.com;Hi, [~mapohl], I think this Jira can be closed. Thanks.;;;","06/Feb/23 06:49;mapohl;Thanks for letting me know. You're free to resolve the issue yourself since you're the one who has worked on it. Anyway, I closed the issue at your request. :-);;;","06/Feb/23 09:03;337361684@qq.com;Sorry, [~mapohl] ,  I thought I didn't have the right to close it. I got it now. Thank you very much(y);;;",,,,,,,,,
"Translate ""Over Aggregation"" page of ""Querys"" into Chinese ",FLINK-30726,13519978,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 04:05,19/Mar/23 12:30,04/Jun/24 20:41,19/Mar/23 12:30,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/over-agg]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/over-agg.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 12:30:21 UTC 2023,,,,,,,,,,"0|z1f4js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 04:10;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?

And the pull request has been committed.

Please see: [https://github.com/apache/flink/pull/21711];;;","19/Mar/23 12:30;jark;Fixed in master: b111bef6a47ffdba37792a94c8c75b7751ab9042 and fb1f91d6731d46a2345cbf212c3c145015c024e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-281: Sink Supports Speculative Execution For Batch Job,FLINK-30725,13519977,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,SleePy,SleePy,SleePy,18/Jan/23 04:00,10/Feb/23 12:46,04/Jun/24 20:41,06/Feb/23 09:59,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,31/Jan/23 00:00,0,,,,,This is the umbrella issue of FLIP-281. More details can be found in https://cwiki.apache.org/confluence/display/FLINK/FLIP-281+Sink+Supports+Speculative+Execution+For+Batch+Job.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31005,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-18 04:00:03.0,,,,,,,,,,"0|z1f4jk:",9223372036854775807,"Flink now allows sinks which implement decorative interface SupportsConcurrentExecutionAttempts to do speculative execution. If a sink(including Sink V2, SinkFunction and OutputFormat) can work properly when multiple attempts are executing at the same time, it can implement SupportsConcurrentExecutionAttempts.
Some built in sinks are enabled to do speculative execution, including DiscardingSink, PrintSinkFunction, PrintSink, FileSink, FileSystemOutputFormat and HiveTableSink.

More details can be found at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/speculative_execution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update doc of kafka per-partition watermark to FLIP-27 source,FLINK-30724,13519975,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,18/Jan/23 03:42,19/Jan/23 10:07,04/Jun/24 20:41,19/Jan/23 10:07,,,,,,,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,Current documentation of kafka per-partition watermark is still described with deprecated kafka souce https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/datastream/event-time/generating_watermarks/#watermark-strategies-and-the-kafka-connector . It's better to update the doc with FLIP-27 source.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 10:07:48 UTC 2023,,,,,,,,,,"0|z1f4j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 07:45;martijnvisser;Good catch, thanks for picking it up. Feel free to ping me for a review;;;","19/Jan/23 10:07;yunta;merged
master: 2a9ceb7cb0355f07ed6469b4071ecbcb0426daf0
release-1.16: ce98d8f584a21550e79ca8d72e9ae93ab7e8f1b9
release-1.15: 52bc3e918da97fe0a2cfdf4813c0e34826e24df7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce filter pushdown for parquet format,FLINK-30723,13519965,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,18/Jan/23 01:46,29/Mar/23 02:07,04/Jun/24 20:41,29/Mar/23 02:07,table-store-0.4.0,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,Introduce filter pushdown for parquet format,,,,,,,,,,FLINK-31076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 05:50:03 UTC 2023,,,,,,,,,,"0|z1f4hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 05:50;nicholasjiang;[~zjureel], [~lzljs3620320], this is duplicated by FLINK-31076.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Group Aggregation"" page of ""Querys"" into Chinese ",FLINK-30722,13519964,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,18/Jan/23 01:42,19/Mar/23 12:48,04/Jun/24 20:41,19/Mar/23 12:48,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/group-agg]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/group-agg.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 12:48:34 UTC 2023,,,,,,,,,,"0|z1f4h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 01:48;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?

And the pull request has been committed.

Please see: [https://github.com/apache/flink/pull/21706];;;","19/Mar/23 12:48;jark;Fixed in master: 8ea03789eda2ca74946e6d4274efe40301da4485 and dad718523fa483c22e6ee38abd988e111fa164fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The confluent schema registry is not compatible with Apicurio schema registry,FLINK-30721,13519931,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,pedromazala,pedromazala,17/Jan/23 16:51,19/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,auto-deprioritized-major,pull-request-available,,,"With the format `avro-confluent`, it is impossible to use [Apicurio|https://www.apicur.io/].

[The code|https://github.com/apache/flink/blob/master/flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentSchemaRegistryCoder.java#L71] reads a 4-byte integer (see `readInt` documentation [here|https://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#readInt()]) and Apicurio stores its schema ids as a long ([8 bytes|https://docs.oracle.com/javase/7/docs/api/java/io/DataInput.html#readLong()]).

The solution could be adding the schema size to be read from the log message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26654,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:01 UTC 2023,,,,,,,,,,"0|z1f49s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 17:31;martijnvisser;[~pedromazala] Thanks for filing the ticket. I do wonder if this is a bug; Flink officially doesn't support Apicurio (see FLINK-26654). ;;;","18/Jan/23 10:03;pedromazala;Gotcha [~martijnvisser]. We can remove this one then. WDYT?

IIRC this change would be the single one to support Apicurio since Apicurio provides a compatibility layer with confluent registry. ;;;","20/Jan/23 15:09;martijnvisser;[~pedromazala] It depends on the level of compatibility Flink wants to offer with Apicurio. I have no strong preference from that regard. Perhaps it makes sense to first start with the compatibility layer and see if that would suffice. Would you be open to create a PR for this?;;;","31/Jan/23 09:55;pedromazala;Hello [~martijnvisser]!

[There we go!|https://github.com/apache/flink/pull/21805];;;","21/Feb/23 13:20;pedromazala;Hello [~martijnvisser]


Is this PR enough for this issue? Can I help with something to make it go through easier?;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource failed due to a topic already exist when creating it,FLINK-30720,13519926,13525453,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,mapohl,mapohl,17/Jan/23 16:38,16/Oct/23 07:08,04/Jun/24 20:41,16/Oct/23 07:08,1.17.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,test-stability,,,,"We experienced a build failure in {{KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource}} due to an already existing topic:
{code}
Jan 17 14:15:33 [ERROR] org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource  Time elapsed: 14.771 s  <<< ERROR!
Jan 17 14:15:33 java.lang.IllegalStateException: Fail to create topic [changelog_topic partitions: 1 replication factor: 1].
Jan 17 14:15:33 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.createTestTopic(KafkaTableTestBase.java:143)
Jan 17 14:15:33 	at org.apache.flink.streaming.connectors.kafka.table.KafkaChangelogTableITCase.testKafkaDebeziumChangelogSource(KafkaChangelogTableITCase.java:60)
Jan 17 14:15:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 17 14:15:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 17 14:15:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 17 14:15:33 	at java.lang.reflect.Method.invoke(Method.java:498)
[...]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44972&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=38188",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 11:53:02 UTC 2023,,,,,,,,,,"0|z1f48o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Feb/23 14:47;1026688210;Hi  [~mapohl] ,does this Exception occur in every  test? 
Could you tell me the steps  causing the Exception occur ,I wanna test it at my local environment.;;;","27/Feb/23 11:53;mapohl;I'm not aware of that. But we have general instabilities with the Kafka-related tests. That's why I tried to collect these instabilities under the umbrella issue FLINK-31145;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-runtime-web failed due to a corrupted nodejs dependency,FLINK-30719,13519917,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,snuyanzin,mapohl,mapohl,17/Jan/23 15:50,31/May/24 08:11,04/Jun/24 20:41,,1.16.0,1.17.0,1.18.0,,,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,Test Infrastructure,Tests,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44954&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12550

The build failed due to a corrupted nodejs dependency:
{code}
[ERROR] The archive file /__w/1/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-linux-x64.tar.gz is corrupted and will be deleted. Please try the build again.
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31484,FLINK-32538,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 08:10:46 UTC 2024,,,,,,,,,,"0|z1f46o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 08:31;pltbkd;Seems that the corrupting file has been deleted and the issue just gone. I suppose maybe we can close the issue now.;;;","03/Feb/23 10:02;mapohl;fair point, [~pltbkd]. This seems to be a temporary issue. I couldn't find any way to do a retry accept for retrying the entire mvn build which is not desriable because it might hide other issues.

I'm closing this issue because it didn't appear again. Feel free to reopen the issue if we see it happening again.;;;","02/Mar/23 09:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46665&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=13103;;;","02/Mar/23 09:04;mapohl;I'm reopening the issue to keep monitoring it. We might want to try out some retry mechanism here?;;;","16/Mar/23 15:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47149&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5;;;","16/Mar/23 15:27;mapohl;2x in the same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47163&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12957
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47163&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691&l=13158;;;","16/Mar/23 15:34;mapohl;2x in the same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47160&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12958
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47160&view=logs&j=b1e44b80-6687-5cc5-6529-292f7212c609&t=e749141c-4fcf-5663-d533-e10832b9aaf9&l=12958;;;","16/Mar/23 15:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47190&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12550;;;","17/Mar/23 02:18;leonard;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47149&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5];;;","17/Mar/23 04:20;leonard;[~junhan] Could you take a look this ticket ? ;;;","20/Mar/23 08:10;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47161&view=logs&j=b1e44b80-6687-5cc5-6529-292f7212c609&t=e749141c-4fcf-5663-d533-e10832b9aaf9&l=11013;;;","21/Mar/23 03:45;junhan;Hi, I spent a while investigating on this, and I still speculate that this is caused by the unstable network.

The frontend-mave-plugin ""install node and npm"" execution was the root of this problem. It tries to download node and npm from the default downloadRoot ---- [https://nodejs.org/dist/]

My suggestion is to set up a stable proxy downloadRoot for this download. Do we have a mirror for the default downloadRoot?;;;","31/Mar/23 14:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47775&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12999;;;","18/Apr/23 14:25;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48217&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12838;;;","26/Jun/23 22:56;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50291&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12046
;;;","26/Jun/23 22:58;Sergey Nuyanzin;it looks like a maven-frontend-plugin issue mentioned at https://github.com/eirslett/frontend-maven-plugin/issues/882;;;","27/Jun/23 09:54;junhan;Looks like there is still no any proposed fix in this open issue: [https://github.com/eirslett/frontend-maven-plugin/issues/882] . The cause seems like a recognized behavior in the maven-frontend-plugin, and all we can do is to retry the CI process.;;;","04/Jul/23 10:36;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50894&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11172;;;","05/Jul/23 04:37;Sergey Nuyanzin;i submitted a PR enabling configuration of number of retries for downloading and extraction to maven-frontend-plugin
https://github.com/eirslett/frontend-maven-plugin/pull/1098;;;","05/Jul/23 08:10;tanyuxin;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50896&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10984]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50919&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10984]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50925&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10984];;;","06/Jul/23 09:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50993&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11175;;;","06/Jul/23 09:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50994&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11927;;;","06/Jul/23 09:27;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50995&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=12774;;;","24/Jul/23 23:39;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51654&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11008;;;","02/Aug/23 15:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51909&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11200;;;","02/Aug/23 15:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51910&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11200;;;","04/Aug/23 12:52;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51978&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11227;;;","31/Aug/23 09:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52863&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9344;;;","31/Aug/23 23:03;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52897&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9344;;;","04/Oct/23 17:00;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53545&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9332;;;","18/Oct/23 20:25;Sergey Nuyanzin;also submitted a PR to flink-ci-docker 
https://github.com/zentol/flink-ci-docker/pull/1;;;","18/Oct/23 23:01;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53803&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9697;;;","24/Oct/23 07:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53874&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9697;;;","24/Oct/23 07:09;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53880&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9697;;;","30/Oct/23 08:05;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54139&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9698;;;","30/Oct/23 08:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54089&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9697;;;","30/Oct/23 08:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54038&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9697;;;","30/Oct/23 08:21;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54001&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9697;;;","29/Nov/23 07:19;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54999&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9645;;;","07/Dec/23 14:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55112&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb;;;","29/Jan/24 14:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56996&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9855;;;","05/Feb/24 10:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57284&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=11934;;;","25/Mar/24 16:16;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58502&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9714

Slightly different error but still worth it mentioning:
{code}
13:36:43.413 [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.11.0:install-node-and-npm (install node and npm) on project flink-runtime-web: Could not download Node.js: Got error code 525 from the server. -> [Help 1]
{code};;;","30/May/24 12:23;Weijie Guo;{code:java}
The archive file /__w/1/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-linux-x64.tar.gz is corrupted and will be deleted. Please try the build again.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59964&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=9778
;;;","31/May/24 08:10;pnowojski;{code:java}
14:47:52.892 [ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.11.0:install-node-and-npm (install node and npm) on project flink-runtime-web: Could not extract the Node archive: Could not extract archive: '/__w/3/.m2/repository/com/github/eirslett/node/16.13.2/node-16.13.2-linux-x64.tar.gz': EOFException -> [Help 1]
1
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59951&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb
;;;",,,,,,,,
Cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.connector.opensearch.sink.OpensearchSink.emitter ,FLINK-30718,13519916,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,reta,reta,17/Jan/23 15:33,31/Jan/23 18:25,04/Jun/24 20:41,31/Jan/23 18:25,1.16.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Opensearch,,,,,0,,,,,"When using OpenSearchSink (Apache Flink OpenSearch Connector 1.0.0) programmatically
{noformat}
 
        final StreamExecutionEnvironment env = StreamExecutionEnvironment
            .createRemoteEnvironment(""localhost"", 8081);

        final Collection<Tuple4<String, String, Long, Long>> users = new ArrayList<>();
        users.add(Tuple4.of(""u1"", ""admin"", 100L, 200L));

        final DataStream<Tuple4<String, String, Long, Long>> source = env.fromCollection(users);
        final OpensearchSink<Tuple4<String, String, Long, Long>> sink =
            new OpensearchSinkBuilder<Tuple4<String, String, Long, Long>>()
                .setHosts(new HttpHost(""localhost"", 9200, ""https""))
                .setEmitter( (element, ctx, indexer) -> {
                    indexer.add(
                        Requests
                            .indexRequest()
                            .index(""users"")
                            .id(element.f0)
                            .source(Map.ofEntries(
                                Map.entry(""user_id"", element.f0),
                                Map.entry(""user_name"", element.f1),
                                Map.entry(""uv"", element.f2),
                                Map.entry(""pv"", element.f3)
                            )));
                        })
                .setConnectionUsername(""admin"")
                .setConnectionPassword(""admin"")
                .setAllowInsecure(true)
                .setBulkFlushMaxActions(1)
                .build();

        source.sinkTo(sink);
        env.execute(""Opensearch end to end sink test example"");

{noformat}
the stream processing fails with the exception
{noformat}
Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function.
	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:399)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:162)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:60)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:681)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.connector.opensearch.sink.OpensearchSink.emitter of type org.apache.flink.connector.opensearch.sink.OpensearchEmitter in instance of org.apache.flink.connector.opensearch.sink.OpensearchSink
	at java.base/java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2076)
	at java.base/java.io.ObjectStreamClass$FieldReflector.checkObjectFieldValueTypes(ObjectStreamClass.java:2039)
	at java.base/java.io.ObjectStreamClass.checkObjFieldValueTypes(ObjectStreamClass.java:1293)
	at java.base/java.io.ObjectInputStream.defaultCheckFieldValues(ObjectInputStream.java:2512)
	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2419)
	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)
	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)
	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)
	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)
	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)
	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)
	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)
	at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543)
	at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:383)
	... 9 more


 {noformat}
Reproducer project attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/23 15:34;reta;example-opensearch.zip;https://issues.apache.org/jira/secure/attachment/13054655/example-opensearch.zip",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 18:24:56 UTC 2023,,,,,,,,,,"0|z1f46g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 18:24;reta;The issue is caused by Java lambda serialization mechanism: the classpath (on a Flink side) should contain the emitter implementation, otherwise the `SerializedLambda` would never be reified.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate Travis CI to Github Actions,FLINK-30717,13519909,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,17/Jan/23 14:35,26/Jan/23 10:59,04/Jun/24 20:41,26/Jan/23 10:59,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,Documentation / Training,,,,0,pull-request-available,,,,Infra decided to no longer support Travis in the near future so we need to migrate some repos.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 11:12:39 UTC 2023,,,,,,,,,,"0|z1f44w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 11:12;chesnay;benchmarks: 9fa36c41b055b807ff7fc25cc548ff04f9ace28d
trainings: 188f865c62599ad92d538e5fbf7b1dc9a94d8f41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[Flink SQL] SQL function TO_TIMESTAMP(string1[, string2])'s result incorrect where string2 is default: 'yyyy-MM-dd HH:mm:ss'",FLINK-30716,13519893,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,jettd22,jettd22,17/Jan/23 11:50,17/Jan/23 14:35,04/Jun/24 20:41,17/Jan/23 12:01,1.11.2,,,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"A sql statement [

select to_timestamp('2017-11-27 00:00:00')

] using funcation to_timestamp for parsing string date is executed, the result is 2017-11-27T00:00 when flink version 1.11.2, and yet it's correct where flink version 1.15.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,sql,Tue Jan 17 14:35:00 UTC 2023,,,,,,,,,,"0|z1f41c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 12:01;martijnvisser;[~jettd22] The Flink 1.11 version is no longer supported by the Flink community, this won't get fixed for older versions;;;","17/Jan/23 14:35;jettd22;So, we should not choose any Flink 1.11, avoiding this version.

Martijn Visser (Jira) <jira@apache.org> 于2023年1月17日周二 20:02写道：

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-289: Support Online Inference,FLINK-30715,13519878,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,17/Jan/23 09:51,25/Jun/23 12:20,04/Jun/24 20:41,04/Apr/23 05:32,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,The FLIP design doc can be found at [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=240881268].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:57:09 UTC 2023,,,,,,,,,,"0|z1f3y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:57;lindong;Merged to apache/flink-ml master branch:
- f0c1ce7440ff3b5cb4010f7699956e556c66e66e
- b032ebdb122f32ba04bd21ad03a2f971b471e5bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Window Aggregation"" page of ""Querys"" into Chinese ",FLINK-30714,13519874,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,17/Jan/23 09:01,19/Mar/23 13:06,04/Jun/24 20:41,19/Mar/23 13:06,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,,,,,0,,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-agg]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/window-agg.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 13:06:47 UTC 2023,,,,,,,,,,"0|z1f3x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 09:07;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?

And the pull request has been committed.

Please see: [https://github.com/apache/flink/pull/21702];;;","19/Mar/23 13:06;jark;Fixed in master: 8c5f1ee214ba7ebbced4a435c97078bf894a2123 and fa8230bd79da57e87a157aaf2856c749ab44eabc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Hadoop related k8s decorators exclude possibility,FLINK-30713,13519873,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,17/Jan/23 09:01,29/Mar/23 09:20,04/Jun/24 20:41,29/Mar/23 09:20,1.17.0,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"Hadoop related k8s decorators are making a lot of assumptions. There are some users who mount Hadoop specific things (like Hadoop config, Kerberos config/keytab) in a custom way and not depending on Flink. As a result of these assumptions the actual decorators making the workload fail and there is no workaround.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 09:20:21 UTC 2023,,,,,,,,,,"0|z1f3ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 09:20;gaborgsomogyi;409e18b1f96357de72882781a171d9e81cb8450d on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update network memory configuration docs ,FLINK-30712,13519858,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,tanyuxin,tanyuxin,tanyuxin,17/Jan/23 06:52,10/Feb/23 02:17,04/Jun/24 20:41,10/Feb/23 02:17,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,Runtime / Network,,,,0,pull-request-available,,,,"After [FLINK-30469|https://issues.apache.org/jira/browse/FLINK-30469], the network memory configuration docs for TaskManager should also be updated.
The configuration descriptions to be updated mainly include `taskmanager.network.memory.buffers-per-channel`
and `taskmanager.network.memory.floating-buffers-per-gate`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 02:17:01 UTC 2023,,,,,,,,,,"0|z1f3tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 02:17;xtsong;- master (1.18): ae89c997fa7c431a0ff2078af441c938fd66e6b9
- release-1.17: 022ab7680f0dab511b2f498acbb89edcaaad2624;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support dropping partition via Flink action,FLINK-30711,13519856,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,17/Jan/23 06:47,17/Jan/23 12:21,04/Jun/24 20:41,17/Jan/23 12:21,table-store-0.4.0,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 12:21:36 UTC 2023,,,,,,,,,,"0|z1f3t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 12:21;lzljs3620320;master: f503591b7f5ee77f876296110840c87d7672bb9b
release-0.3: f201b507fef88501c4beb4c62807bef818e31be5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix invalid field id for nested type in spark catalog,FLINK-30710,13519847,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,17/Jan/23 05:46,17/Jan/23 07:52,04/Jun/24 20:41,17/Jan/23 07:52,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Current user can create table by spark sql, but the field id will start from 0 for nested type which causes exception",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 07:52:06 UTC 2023,,,,,,,,,,"0|z1f3r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 07:52;lzljs3620320;master: 6fb528f234e6d33bc3797ec23e8a7b7206a7a063;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NetworkInput#emitNext() should push records to DataOutput in a while loop,FLINK-30709,13519843,13515876,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,17/Jan/23 05:05,27/Jan/23 11:08,04/Jun/24 20:41,27/Jan/23 01:35,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"It's similar to FLINK-30533, FLINK-30533 focus on source operator, this JIRA focus on Network input.

 

Currently, each invocation of AbstractStreamTaskNetworkInput#emitNext() push at most one record to the given DataOutput. This unnecessarily increases the average Java call stack depth needed to produce a record.

Take the following program as an example. For each element produced by this program, Flink runtime needs to include in the call stack these 3 function calls:
 * StreamTask#processInput()
 * StreamOneInputProcessor#processInput()
 * AbstractStreamTaskNetworkInput#emitNext()

This ticket proposes to update AbstractStreamTaskNetworkInput#emitNext() to push records to DataOutput in a while loop. It improves Flink performance by removing an average of 3 function from the call stack needed to produce a record.

Here are the benchmark results obtained by running the [InputBenchmark#mapSink|https://github.com/apache/flink-benchmarks/blob/0bafe0e85700c889894324aadb70302381f98e03/src/main/java/org/apache/flink/benchmark/InputBenchmark.java#L55] with env.disableOperatorChaining(). And I run it 4 times on My Mac.

 
{code:java}
Before the proposed change, the avg is 12429.0605 ops/ms, here is detailed results:

Benchmark                (sourceType)   Mode  Cnt      Score     Error   Units
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  12339.771 ± 414.649  ops/ms
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  12687.872 ± 320.084  ops/ms
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  12256.445 ± 512.219  ops/ms
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  12432.154 ± 405.083  ops/ms


After the proposed change, the avg is 13836.845 ops/ms, here is detailed results:

Benchmark                (sourceType)   Mode  Cnt      Score     Error   Units
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  13092.451 ± 490.886  ops/ms
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  13881.138 ± 370.249  ops/ms
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  13960.280 ± 389.505  ops/ms
InputBenchmark.mapSink  F27_UNBOUNDED  thrpt   30  14413.511 ± 727.844  ops/ms{code}
 

The proposed change increases throughput by 11.3%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30623,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 27 11:08:10 UTC 2023,,,,,,,,,,"0|z1f3q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 10:55;pnowojski;Thanks for following up on my comment! FYI, the existing benchmark `mapRebalanceMapSink.F27_UNBOUNDED` ( http://codespeed.dak8s.net:8000/timeline/?ben=mapRebalanceMapSink.F27_UNBOUNDED&env=2 ) should be able to showcase this improvement.;;;","27/Jan/23 09:07;pnowojski;Merged to master as f6c7c30118e^ and f6c7c30118e

[~lindong] , when closing tickets could you remember next time to mark the commit range that they were merged as?

Note, after first benchmark run the performance improvement is not visible unfortunately.;;;","27/Jan/23 10:35;lindong;[~pnowojski] Sure. Thanks for the reminder. I just realized that we have this guideline for Merging Pull Requests [1] and it is documented that we should add a comment in the JIRA listing the commit hash for each branch respectively. I will follow this guideline in the future.

BTW, I also notice that the wiki says we should add ""This closes #PR_ID"" to the message of the last commit of the PR. This is the practice I have been using when merging PR in various projects (including this commit) which also allows us to find the commit for the JIRA.

However, it seems that many commits in Flink did not add this message in the commit log. Maybe we should also remind other committers to follow this guideline?

 

[1] https://cwiki.apache.org/confluence/display/FLINK/Merging+Pull+Requests|https://cwiki.apache.org/confluence/display/FLINK/Merging+Pull+Requests;;;","27/Jan/23 10:41;lindong;Regarding the benchmark result, I guess it will be useful to understand why Rui observed the performance improvement (as described in the Jira description) but the standard benchmark does not.

I am interested to do this (and also use the chance to learn about Flink's regularly-run benchmarks) but probably won't get time to investigate this in the next 2 weeks. I am wondering if Rui or something else could help investigate this.;;;","27/Jan/23 11:08;pnowojski;[~lindong] , good reminder about ""This closes"", but also keep in mind that this is marked as ""SHOULD"", not as ""MUST"".

I would rather say that investigation why this benchmark didn't show improvement doesn't have a very high priority. If someone would like to take a look, it would be interesting to know why, but I'm personally also fine leaving it as it is.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo problem of the doc `Determinism In Continuous Queries`,FLINK-30708,13519836,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,doki,doki,17/Jan/23 03:09,17/Jan/23 03:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,"The url named `continuous query on dynamic tables` is written wrongly.

!截屏2023-01-17 11.04.12.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/23 03:07;doki;截屏2023-01-17 11.04.12.png;https://issues.apache.org/jira/secure/attachment/13054633/%E6%88%AA%E5%B1%8F2023-01-17+11.04.12.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-17 03:09:17.0,,,,,,,,,,"0|z1f3oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let speculative execution take input data amount into account when detecting slow tasks,FLINK-30707,13519835,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,17/Jan/23 03:01,08/Feb/23 03:46,04/Jun/24 20:41,08/Feb/23 03:46,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The default slowTaskDetector in speculative execution is based on execution time now, if the data amount allocated to each subtask is uneven, time-based slow task detection may not reflect the actual performance of the subtask. Therefore, it is necessary to take the input information into account when detecting slow tasks.

The major changes are:
 * Need to know the amount of data processed by each execution vertex.
 * ExecutionTimeBasedSlowTaskDetector needs to introduce the amount of input data to weight it with execution time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 14:12:35 UTC 2023,,,,,,,,,,"0|z1f3og:",9223372036854775807,"The slow task detecting is improved for speculative execution. Previously, it only considers the execution time of tasks when deciding which tasks are slow. It now takes the input data volume of tasks into account. Tasks which has a longer execution time but consumes more data may not be considered as slow. This improvement helps to eliminate the negative impacts of data skew on slow task detecting.",,,,,,,,,,,,,,,,,,,"17/Jan/23 03:07;xiasun;[~zhuzh] Could you please assign this ticket to me? Thanks! ;;;","17/Jan/23 03:23;zhuzh;Thanks for the proposal! [~xiasun]
I have assigned you the ticket.;;;","29/Jan/23 14:12;zhuzh;Done via
02b09eac6238fe059b6cb0ddbe791b761962dd14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-table-common dependency for table store core,FLINK-30706,13519833,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,17/Jan/23 02:05,17/Jan/23 09:40,04/Jun/24 20:41,17/Jan/23 09:40,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 09:40:13 UTC 2023,,,,,,,,,,"0|z1f3o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 09:40;lzljs3620320;master: 938d3323e39048b22b693716533910e66510ac9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CR reference generator does not respect JsonProperty annotation,FLINK-30705,13519818,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,gyfora,gyfora,16/Jan/23 20:00,21/Jan/23 19:27,04/Jun/24 20:41,21/Jan/23 19:27,kubernetes-operator-1.2.0,kubernetes-operator-1.3.1,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We use a generator for the CR reference page ([https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/reference/)] however it seems that the JsonProperty annotation is not respected.

This is most obvious for the enum references, which generate the name of the ENUM instead of the json property.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 21 19:27:29 UTC 2023,,,,,,,,,,"0|z1f3ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jan/23 19:27;gyfora;merged to main 410a0a327fa68e0744938d0c658caaa95d44f597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add S3 delegation token support,FLINK-30704,13519796,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,16/Jan/23 14:47,20/Jan/23 10:45,04/Jun/24 20:41,20/Jan/23 10:45,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,FileSystems,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 20 10:45:30 UTC 2023,,,,,,,,,,"0|z1f3g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/23 10:45;mbalassi;0141f13 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PulsarOrderedPartitionSplitReaderTest>PulsarPartitionSplitReaderTestBase.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition fails,FLINK-30703,13519789,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,16/Jan/23 13:53,17/Nov/23 12:02,04/Jun/24 20:41,,1.15.3,1.16.0,pulsar-3.0.0,,,,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,auto-deprioritized-major,test-stability,,,"A 1.15 build failed due to {{PulsarOrderedPartitionSplitReaderTest>PulsarPartitionSplitReaderTestBase.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition}}:
{code}
an 11 03:08:20 [ERROR]   PulsarOrderedPartitionSplitReaderTest>PulsarPartitionSplitReaderTestBase.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition:260->PulsarPartitionSplitReaderTestBase.fetchedMessages:169->PulsarPartitionSplitReaderTestBase.fetchedMessages:199 [We should fetch the expected size] 
Jan 11 03:08:20 Expected size: 20 but was: 25 in:
Jan 11 03:08:20 [PulsarMessage{id=154:0:0, value=ZHiaOiOhFT, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:1:0, value=uoOmKceWTh, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:2:0, value=oeYYWzisge, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:3:0, value=yhpOOkNLER, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:4:0, value=MIbzxkfFfp, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:0:0, value=ZHiaOiOhFT, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:1:0, value=uoOmKceWTh, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:2:0, value=oeYYWzisge, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:3:0, value=yhpOOkNLER, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:4:0, value=MIbzxkfFfp, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:5:0, value=FaEpggGBTE, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:6:0, value=IlGbzPRuvi, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:7:0, value=aqgIeSbOzo, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:8:0, value=CUScdPriyM, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:9:0, value=zLRsvDxpJG, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:10:0, value=iqsGFVkXDz, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:11:0, value=hFyHYPldqN, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:12:0, value=ZYTlJcwSst, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:13:0, value=mOWkzJQQxE, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:14:0, value=CTfcVXhfUN, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:15:0, value=wAhqCGGwXO, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:16:0, value=LOoTbXaEgG, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:17:0, value=UvfzGIRURy, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:18:0, value=vlYYGbZGAH, eventTime=0},
Jan 11 03:08:20     PulsarMessage{id=154:19:0, value=wrwPanXvql, eventTime=0}]

{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44691&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27466",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 12:02:07 UTC 2023,,,,,,,,,,"0|z1f3eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 15:43;syhily;I see, the consumed messages, {{PulsarMessage{id=154:0:0, value=ZHiaOiOhFT, eventTime=0}} in line one and line six are the same.

I have fixed a same bug for duplicated message by dropping {{Consume.seek}} method, and the 1.15 release branch should contain this fix. But this issue should be some new bugs, I need more time on it to investigate the root cause.;;;","16/Jan/23 16:55;syhily;[~mapohl] It's wired to see the log that the tests on PulsarOrderedPartitionSplitReaderTest are executed in parallel, am I miss something? I use a mocked Pulsar instance which only supports one test per time. The parallel tests executing on same mock Pulsar instance may mess the cursor which cause the duplication.

{code}
03:06:03,320 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ProducerStatsRecorderImpl      [] - Starting Pulsar producer perf with config: {""topicName"":""persistent://public/default/sXmCOYoouf-partition-0"",""producerName"":null,""sendTimeoutMs"":30000,""blockIfQueueFull"":false,""maxPendingMessages"":0,""maxPendingMessagesAcrossPartitions"":0,""messageRoutingMode"":""RoundRobinPartition"",""hashingScheme"":""JavaStringHash"",""cryptoFailureAction"":""FAIL"",""batchingMaxPublishDelayMicros"":1000,""batchingPartitionSwitchFrequencyByPublishDelay"":10,""batchingMaxMessages"":1000,""batchingMaxBytes"":131072,""batchingEnabled"":false,""chunkingEnabled"":false,""compressionType"":""NONE"",""initialSequenceId"":null,""autoUpdatePartitions"":true,""autoUpdatePartitionsIntervalSeconds"":60,""multiSchema"":true,""accessMode"":""Shared"",""lazyStartPartitionedProducers"":false,""properties"":{},""initialSubscriptionName"":null}
03:06:03,321 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ProducerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0] [null] Creating producer on cnx [id: 0xbd68a334, L:/127.0.0.1:40928 - R:localhost/127.0.0.1:42178]
03:06:03,321 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [/127.0.0.1:40928][persistent://public/default/sXmCOYoouf-partition-0] Creating producer. producerId=32
03:06:03,321 [     pulsar-io-639-2] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - Opening managed ledger public/default/persistent/sXmCOYoouf-partition-0
03:06:03,321 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Creating ledger, metadata: {component=[109, 97, 110, 97, 103, 101, 100, 45, 108, 101, 100, 103, 101, 114], pulsar/managed-ledger=[112, 117, 98, 108, 105, 99, 47, 100, 101, 102, 97, 117, 108, 116, 47, 112, 101, 114, 115, 105, 115, 116, 101, 110, 116, 47, 115, 88, 109, 67, 79, 89, 111, 111, 117, 102, 45, 112, 97, 114, 116, 105, 116, 105, 111, 110, 45, 48], application=[112, 117, 108, 115, 97, 114]} - metadata ops timeout : 60 seconds
03:06:03,322 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Created ledger 154
03:06:03,322 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerFactoryImpl  [] - [public/default/persistent/sXmCOYoouf-partition-0] Successfully initialize managed ledger
03:06:03,323 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0] There are no replicated subscriptions on the topic
03:06:03,325 [broker-topic-workers-OrderedExecutor-23-0] INFO  org.apache.pulsar.broker.service.AbstractTopic               [] - Disabling publish throttling for persistent://public/default/sXmCOYoouf-partition-0
03:06:03,325 [broker-topic-workers-OrderedExecutor-23-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Cursor pulsar.dedup recovered to position 154:-1
03:06:03,325 [broker-topic-workers-OrderedExecutor-23-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Creating ledger, metadata: {component=[109, 97, 110, 97, 103, 101, 100, 45, 108, 101, 100, 103, 101, 114], pulsar/managed-ledger=[112, 117, 98, 108, 105, 99, 47, 100, 101, 102, 97, 117, 108, 116, 47, 112, 101, 114, 115, 105, 115, 116, 101, 110, 116, 47, 115, 88, 109, 67, 79, 89, 111, 111, 117, 102, 45, 112, 97, 114, 116, 105, 116, 105, 111, 110, 45, 48], pulsar/cursor=[112, 117, 108, 115, 97, 114, 46, 100, 101, 100, 117, 112], application=[112, 117, 108, 115, 97, 114]} - metadata ops timeout : 60 seconds
03:06:03,326 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.MetaStoreImpl             [] - [public/default/persistent/sXmCOYoouf-partition-0] [pulsar.dedup] Updating cursor info ledgerId=155 mark-delete=154:-1
03:06:03,326 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Updated cursor pulsar.dedup with ledger id 155 md-position=154:-1 rd-position=154:0
03:06:03,326 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Opened new cursor: ManagedCursorImpl{ledger=public/default/persistent/sXmCOYoouf-partition-0, name=pulsar.dedup, ackPos=154:-1, readPos=154:0}
03:06:03,326 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.pulsar.broker.service.persistent.MessageDeduplication [] - [persistent://public/default/sXmCOYoouf-partition-0] Replaying 0 entries for deduplication
03:06:03,327 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.BrokerService               [] - Created topic persistent://public/default/sXmCOYoouf-partition-0 - dedup is enabled
03:06:03,327 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.persistent.MessageDeduplication [] - [persistent://public/default/sXmCOYoouf-partition-0] Enabled deduplication
03:06:03,340 [     pulsar-io-639-3] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [/127.0.0.1:40928] Created new producer: Producer{topic=PersistentTopic{topic=persistent://public/default/sXmCOYoouf-partition-0}, client=/127.0.0.1:40928, producerName=mock-pulsar-IvZ7KH-0-76, producerId=32}
03:06:03,340 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ProducerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0] [mock-pulsar-IvZ7KH-0-76] Created producer on cnx [id: 0xbd68a334, L:/127.0.0.1:40928 - R:localhost/127.0.0.1:42178]
03:06:03,368 [                main] INFO  org.apache.pulsar.client.impl.ProducerStatsRecorderImpl      [] - [persistent://public/default/sXmCOYoouf-partition-0] [mock-pulsar-IvZ7KH-0-76] Pending messages: 0 --- Publish throughput: 423.54 msg/s --- 0.03 Mbit/s --- Latency: med: 1.000 ms - 95pct: 1.000 ms - 99pct: 1.000 ms - 99.9pct: 1.000 ms - max: 1.000 ms --- BatchSize: med: 1.000 - 95pct: 1.000 - 99pct: 1.000 - 99.9pct: 1.000 - max: 1.000 --- MsgSize: med: 10.000 bytes - 95pct: 10.000 bytes - 99pct: 10.000 bytes - 99.9pct: 10.000 bytes - max: 10.000 bytes --- Ack received rate: 423.54 ack/s --- Failed messages: 0 --- Pending messages: 0
03:06:03,368 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [PersistentTopic{topic=persistent://public/default/sXmCOYoouf-partition-0}][mock-pulsar-IvZ7KH-0-76] Closing producer on cnx /127.0.0.1:40928. producerId=32
03:06:03,368 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [PersistentTopic{topic=persistent://public/default/sXmCOYoouf-partition-0}][mock-pulsar-IvZ7KH-0-76] Closed producer on cnx /127.0.0.1:40928. producerId=32
03:06:03,368 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ProducerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0] [mock-pulsar-IvZ7KH-0-76] Closed Producer
03:06:03,805 [   pulsar-web-661-36] INFO  org.eclipse.jetty.server.RequestLog                          [] - 127.0.0.1 - - [11/Jan/2023:03:06:03 +0000] ""GET /admin/v2/persistent/public/default/sXmCOYoouf-partition-0/subscriptions HTTP/1.1"" 200 2 ""-"" ""Pulsar-Java-v2.10.2"" 1
03:06:03,812 [   pulsar-web-661-54] INFO  org.apache.pulsar.broker.admin.impl.PersistentTopicsBase     [] - [null][persistent://public/default/sXmCOYoouf-partition-0] Creating subscription uikegefnOB at message id -1:-1:-1 with properties null
03:06:03,812 [   pulsar-web-661-54] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Cursor uikegefnOB recovered to position 154:19
03:06:03,813 [   pulsar-web-661-54] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Creating ledger, metadata: {component=[109, 97, 110, 97, 103, 101, 100, 45, 108, 101, 100, 103, 101, 114], pulsar/managed-ledger=[112, 117, 98, 108, 105, 99, 47, 100, 101, 102, 97, 117, 108, 116, 47, 112, 101, 114, 115, 105, 115, 116, 101, 110, 116, 47, 115, 88, 109, 67, 79, 89, 111, 111, 117, 102, 45, 112, 97, 114, 116, 105, 116, 105, 111, 110, 45, 48], pulsar/cursor=[117, 105, 107, 101, 103, 101, 102, 110, 79, 66], application=[112, 117, 108, 115, 97, 114]} - metadata ops timeout : 60 seconds
03:06:03,819 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.MetaStoreImpl             [] - [public/default/persistent/sXmCOYoouf-partition-0] [uikegefnOB] Updating cursor info ledgerId=178 mark-delete=154:19
03:06:03,819 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Updated cursor uikegefnOB with ledger id 178 md-position=154:19 rd-position=154:20
03:06:03,819 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Opened new cursor: ManagedCursorImpl{ledger=public/default/persistent/sXmCOYoouf-partition-0, name=uikegefnOB, ackPos=154:19, readPos=154:20}
03:06:03,819 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.pulsar.broker.service.persistent.PersistentSubscription [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Successfully disconnected consumers from subscription, proceeding with cursor reset
03:06:03,824 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Initiate reset position to 154:-1 on cursor uikegefnOB
03:06:03,832 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] reset position to 154:-1 before current read position 154:20 on cursor uikegefnOB
03:06:03,832 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.admin.impl.PersistentTopicsBase     [] - [null][persistent://public/default/sXmCOYoouf-partition-0] Successfully created subscription uikegefnOB at message id -1:-1:-1
03:06:03,833 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.eclipse.jetty.server.RequestLog                          [] - 127.0.0.1 - - [11/Jan/2023:03:06:03 +0000] ""PUT /admin/v2/persistent/public/default/sXmCOYoouf-partition-0/subscription/uikegefnOB?replicated=false HTTP/1.1"" 204 0 ""-"" ""Pulsar-Java-v2.10.2"" 22
03:06:03,842 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ConsumerStatsRecorderImpl      [] - Starting Pulsar consumer status recorder with config: {""topicNames"":[""persistent://public/default/sXmCOYoouf-partition-0""],""topicsPattern"":null,""subscriptionName"":""uikegefnOB"",""subscriptionType"":""Shared"",""subscriptionProperties"":null,""subscriptionMode"":""Durable"",""receiverQueueSize"":1000,""acknowledgementsGroupTimeMicros"":100000,""negativeAckRedeliveryDelayMicros"":60000000,""maxTotalReceiverQueueSizeAcrossPartitions"":50000,""consumerName"":null,""ackTimeoutMillis"":0,""tickDurationMillis"":1000,""priorityLevel"":0,""maxPendingChunkedMessage"":10,""autoAckOldestChunkedMessageOnQueueFull"":false,""expireTimeOfIncompleteChunkedMessageMillis"":60000,""cryptoFailureAction"":""FAIL"",""properties"":{},""readCompacted"":false,""subscriptionInitialPosition"":""Latest"",""patternAutoDiscoveryPeriod"":60,""regexSubscriptionMode"":""PersistentOnly"",""deadLetterPolicy"":null,""retryEnable"":false,""autoUpdatePartitions"":true,""autoUpdatePartitionsIntervalSeconds"":60,""replicateSubscriptionState"":false,""resetIncludeHead"":false,""keySharedPolicy"":null,""batchIndexAckEnabled"":false,""ackReceiptEnabled"":false,""poolMessages"":false,""startPaused"":false,""maxPendingChuckedMessage"":10}
03:06:03,843 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Subscribing to topic on cnx [id: 0xbd68a334, L:/127.0.0.1:40928 - R:localhost/127.0.0.1:42178], consumerId 8
03:06:03,843 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [/127.0.0.1:40928] Subscribing on topic persistent://public/default/sXmCOYoouf-partition-0 / uikegefnOB
03:06:03,844 [     pulsar-io-639-2] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0-uikegefnOB] Rewind from 154:-1 to 154:0
03:06:03,844 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0] There are no replicated subscriptions on the topic
03:06:03,844 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Created new subscription for 8
03:06:03,844 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [/127.0.0.1:40928] Created subscription on topic persistent://public/default/sXmCOYoouf-partition-0 / uikegefnOB
03:06:03,844 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Subscribed to topic on localhost/127.0.0.1:42178 -- consumer: 8
03:06:03,844 [                main] INFO  org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase [] - Register split PulsarPartitionSplit{partition=persistent://public/default/sXmCOYoouf-partition-0|0-65535} consumer for current reader.
03:06:08,332 [    pulsar-web-661-1] INFO  org.apache.pulsar.broker.service.BrokerService               [] - [persistent://public/default/sXmCOYoouf-partition-0] Unloading topic
03:06:08,332 [    pulsar-web-661-1] INFO  org.apache.pulsar.broker.service.Consumer                    [] - Disconnecting consumer: Consumer{subscription=PersistentSubscription{topic=persistent://public/default/sXmCOYoouf-partition-0, name=uikegefnOB}, consumerId=8, consumerName=10588, address=/127.0.0.1:40928}
03:06:08,332 [    pulsar-web-661-1] INFO  org.apache.pulsar.broker.service.persistent.PersistentDispatcherMultipleConsumers [] - Removed consumer Consumer{subscription=PersistentSubscription{topic=persistent://public/default/sXmCOYoouf-partition-0, name=uikegefnOB}, consumerId=8, consumerName=10588, address=/127.0.0.1:40928} with pending 20 acks
03:06:08,333 [    pulsar-web-661-1] INFO  org.apache.pulsar.broker.service.persistent.PersistentDispatcherMultipleConsumers [] - [persistent://public/default/sXmCOYoouf-partition-0 / uikegefnOB] All consumers removed. Subscription is disconnected
03:06:08,333 [    pulsar-web-661-1] INFO  org.apache.pulsar.broker.service.persistent.PersistentSubscription [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Successfully closed subscription [ManagedCursorImpl{ledger=public/default/persistent/sXmCOYoouf-partition-0, name=uikegefnOB, ackPos=154:-1, readPos=154:20}]
03:06:08,333 [    pulsar-web-661-1] INFO  org.apache.pulsar.broker.service.persistent.PersistentSubscription [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Successfully disconnected and closed subscription
03:06:08,333 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ConnectionHandler              [] - [persistent://public/default/sXmCOYoouf-partition-0] [uikegefnOB] Closed connection [id: 0xbd68a334, L:/127.0.0.1:40928 - R:localhost/127.0.0.1:42178] -- Will try again in 0.1 s
03:06:08,333 [    pulsar-web-661-1] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Closing managed ledger
03:06:08,333 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.MetaStoreImpl             [] - [public/default/persistent/sXmCOYoouf-partition-0] [pulsar.dedup] Updating cursor info ledgerId=-1 mark-delete=154:-1
03:06:08,333 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.MetaStoreImpl             [] - [public/default/persistent/sXmCOYoouf-partition-0] [uikegefnOB] Updating cursor info ledgerId=-1 mark-delete=154:-1
03:06:08,333 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0][pulsar.dedup] Closed cursor at md-position=154:-1
03:06:08,333 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0][pulsar.dedup] Successfully closed & deleted ledger 155 in cursor
03:06:08,333 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0][uikegefnOB] Closed cursor at md-position=154:-1
03:06:08,334 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0] Topic closed
03:06:08,334 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0][uikegefnOB] Successfully closed & deleted ledger 178 in cursor
03:06:08,434 [  pulsar-timer-675-1] INFO  org.apache.pulsar.client.impl.ConnectionHandler              [] - [persistent://public/default/sXmCOYoouf-partition-0] [uikegefnOB] Reconnecting after timeout
03:06:08,435 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Subscribing to topic on cnx [id: 0xbd68a334, L:/127.0.0.1:40928 - R:localhost/127.0.0.1:42178], consumerId 8
03:06:08,435 [     pulsar-io-639-2] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [/127.0.0.1:40928] Subscribing on topic persistent://public/default/sXmCOYoouf-partition-0 / uikegefnOB
03:06:08,435 [     pulsar-io-639-2] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - Opening managed ledger public/default/persistent/sXmCOYoouf-partition-0
03:06:08,443 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Creating ledger, metadata: {component=[109, 97, 110, 97, 103, 101, 100, 45, 108, 101, 100, 103, 101, 114], pulsar/managed-ledger=[112, 117, 98, 108, 105, 99, 47, 100, 101, 102, 97, 117, 108, 116, 47, 112, 101, 114, 115, 105, 115, 116, 101, 110, 116, 47, 115, 88, 109, 67, 79, 89, 111, 111, 117, 102, 45, 112, 97, 114, 116, 105, 116, 105, 111, 110, 45, 48], application=[112, 117, 108, 115, 97, 114]} - metadata ops timeout : 60 seconds
03:06:08,446 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Created ledger 203
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Loading cursor pulsar.dedup
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Recovering from bookkeeper ledger cursor: pulsar.dedup
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Loading cursor uikegefnOB
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Recovering from bookkeeper ledger cursor: uikegefnOB
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Cursor pulsar.dedup recovered to position 154:-1
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Recovery for cursor pulsar.dedup completed. pos=154:-1 -- todo=1
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Cursor uikegefnOB recovered to position 154:-1
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Recovery for cursor uikegefnOB completed. pos=154:-1 -- todo=0
03:06:08,474 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerFactoryImpl  [] - [public/default/persistent/sXmCOYoouf-partition-0] Successfully initialize managed ledger
03:06:08,475 [bookkeeper-ml-scheduler-OrderedScheduler-26-0] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0] There are no replicated subscriptions on the topic
03:06:08,505 [broker-topic-workers-OrderedExecutor-31-0] INFO  org.apache.pulsar.broker.service.AbstractTopic               [] - Disabling publish throttling for persistent://public/default/sXmCOYoouf-partition-0
03:06:08,505 [broker-topic-workers-OrderedExecutor-31-0] INFO  org.apache.pulsar.broker.service.persistent.MessageDeduplication [] - [persistent://public/default/sXmCOYoouf-partition-0] Replaying 20 entries for deduplication
03:06:08,507 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.BrokerService               [] - Created topic persistent://public/default/sXmCOYoouf-partition-0 - dedup is enabled
03:06:08,508 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.bookkeeper.mledger.impl.ManagedCursorImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0-uikegefnOB] Rewind from 154:0 to 154:0
03:06:08,508 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0] There are no replicated subscriptions on the topic
03:06:08,508 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.persistent.PersistentTopic  [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Created new subscription for 8
03:06:08,508 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.ServerCnx                   [] - [/127.0.0.1:40928] Created subscription on topic persistent://public/default/sXmCOYoouf-partition-0 / uikegefnOB
03:06:08,508 [mock-pulsar-bookkeeper-OrderedExecutor-0-0] INFO  org.apache.pulsar.broker.service.persistent.MessageDeduplication [] - [persistent://public/default/sXmCOYoouf-partition-0] Enabled deduplication
03:06:08,508 [pulsar-client-io-671-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl                   [] - [persistent://public/default/sXmCOYoouf-partition-0][uikegefnOB] Subscribed to topic on localhost/127.0.0.1:42178 -- consumer: 8
03:06:08,650 [pulsar-transaction-executor-613-1] INFO  org.apache.bookkeeper.mledger.impl.NonDurableCursorImpl      [] - [public/default/persistent/sXmCOYoouf-partition-0] Created non-durable cursor read-position=154:0 mark-delete-position=154:-1
03:06:08,650 [pulsar-transaction-executor-613-1] INFO  org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl         [] - [public/default/persistent/sXmCOYoouf-partition-0] Opened new cursor: NonDurableCursorImpl{ledger=public/default/persistent/sXmCOYoouf-partition-0, ackPos=154:-1, readPos=154:0}
03:06:08,651 [pulsar-transaction-executor-613-1] INFO  org.apache.pulsar.broker.transaction.buffer.impl.TopicTransactionBuffer [] - [persistent://public/default/sXmCOYoouf-partition-0]Transaction buffer snapshot recover cursor close complete.
{code};;;","16/Jan/23 17:07;syhily;I think we can change the mocked Pulsar into a TestContainers based tests like Kafka. Kafka tests in Flink use to execute on a embedded instance which also caused a lot of issues. We have removed the mocked Pulsar in latest release. I think we can removed it here and try to see if this works.;;;","16/Jan/23 17:11;syhily;FYI, The related issue on Pulsar: https://github.com/apache/pulsar/pull/17237#issuecomment-1237188920;;;","18/Jan/23 11:58;mapohl;{quote}Matthias Pohl It's wired to see the log that the tests on PulsarOrderedPartitionSplitReaderTest are executed in parallel, am I miss something?{quote}

Parallel test execution support was added in FLINK-26246 for Flink 1.16+ only. But it is opt-in, i.e. the tests would run sequentially by default. Anyway, parallel test execution shouldn't be enabled in 1.15.;;;","13/Feb/23 02:34;syhily;After debugging with Pulsar developers, we still can't reproduce this bug locally. I prefer to keep this issue opened for a while and close it latter if no such issue occur. WDYT [~mapohl]?;;;","13/Feb/23 08:34;mapohl;Sure, considering that the connectors are externalized with 1.17 and this will stop affecting Flink after 1.16 is out and this issue only appeared once so far;;;","02/Mar/23 09:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46685&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=28438;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","17/Nov/23 12:02;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54647&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37821;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ElasticDialect,FLINK-30702,13519785,,New Feature,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,grzegorz.kolakowski,grzegorz.kolakowski,grzegorz.kolakowski,16/Jan/23 13:38,03/Jan/24 10:18,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,1,pull-request-available,stale-assigned,,,"Elastic exposes SQL API ([https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-sql.html]) along with dedicated JDBC connector ([https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-jdbc.html]). 

 

SQL API does not support _write_ operations, only _read_ ones: SELECT, SHOW, DESCRIBE ([https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-commands.html]).",,,,,,,,,,,,FLINK-30780,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 22:35:22 UTC 2023,,,,,,,,,,"0|z1f3dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 13:38;grzegorz.kolakowski;I'll provide a PR soon.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SimpleTableTestHelper and use sql in SparkITCase,FLINK-30701,13519766,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,16/Jan/23 12:03,17/Jan/23 11:16,04/Jun/24 20:41,17/Jan/23 11:16,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Remove SimpleTableTestHelper in SparkITCase and use create/select/insert sql instead,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 11:16:37 UTC 2023,,,,,,,,,,"0|z1f39c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 11:16;lzljs3620320;master: 181235267919fdae0e99e43d0c08de8b8e06a275;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Deduplication"" page of ""Querys"" into Chinese ",FLINK-30700,13519752,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,16/Jan/23 10:10,19/Mar/23 14:03,04/Jun/24 20:41,19/Mar/23 14:03,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/deduplication]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/deduplication.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 14:03:58 UTC 2023,,,,,,,,,,"0|z1f368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 10:14;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?

And the pull request has been committed.

Please see: [https://github.com/apache/flink/pull/21687];;;","19/Mar/23 14:03;jark;Fixed in master: f7ee479c08cc83327a1b55ae3f1d7798a63b0a7b and a06aaa00b083955fdff01be00e78afbbd666cd57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improved getRandomString method code format in the StringUtils class,FLINK-30699,13519749,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,TaoZex,TaoZex,16/Jan/23 10:09,17/Sep/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,API / Core,,,,,0,auto-deprioritized-minor,pull-request-available,,," 

!image-2023-01-16-18-13-56-912.png|width=398,height=148!

!image-2023-01-16-18-14-12-939.png|width=398,height=114!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 10:13;TaoZex;image-2023-01-16-18-13-56-912.png;https://issues.apache.org/jira/secure/attachment/13054617/image-2023-01-16-18-13-56-912.png","16/Jan/23 10:14;TaoZex;image-2023-01-16-18-14-12-939.png;https://issues.apache.org/jira/secure/attachment/13054616/image-2023-01-16-18-14-12-939.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 17 22:35:06 UTC 2023,,,,,,,,,,"0|z1f35k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 10:10;TaoZex;I want to fix this problem.;;;","16/Jan/23 10:42;TaoZex;[https://github.com/apache/flink/pull/21688]

Fix in this pr.;;;","18/Jan/23 06:05;Wencong Liu;Thanks for your proposal! [~TaoZex] . I'm a little confused about the aim of this issue. Do you mean your change will have a better efficiency? It will be more convincing that you design some tests and put the results on this issue.;;;","18/Jan/23 06:22;TaoZex;[~Wencong Liu] Thank you for your reply. This is a mistake in the title, my purpose is to correct the format of the code, these changes will not bring efficiency improvement.;;;","09/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","17/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceStatementITCase.testFlinkSqlStatements fails,FLINK-30698,13519733,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,16/Jan/23 09:26,16/Jan/23 09:47,04/Jun/24 20:41,16/Jan/23 09:45,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,test-stability,,,,"We're seeing an assertion in {{SqlGatewayServiceStatementITCase.testFlinkSqlStatements}} in the following build:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44824&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=16651",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30677,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 09:45:10 UTC 2023,,,,,,,,,,"0|z1f320:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 09:33;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44835&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=17487]

It's a different cause: A NullPointerException is thrown.
{code:java}
Jan 15 01:54:40 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.284 s <<< FAILURE! - in org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase
Jan 15 01:54:40 [ERROR] org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.testFlinkSqlStatements(String)[5]  Time elapsed: 1.212 s  <<< ERROR!
Jan 15 01:54:40 java.lang.NullPointerException
Jan 15 01:54:40 	at org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.stringifyException(SqlGatewayRestEndpointStatementITCase.java:194)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.runStatements(AbstractSqlGatewayStatementITCase.java:144)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.runTest(AbstractSqlGatewayStatementITCase.java:269)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.testFlinkSqlStatements(AbstractSqlGatewayStatementITCase.java:118)
[...] {code};;;","16/Jan/23 09:43;Paul Lin;Duplicate of https://issues.apache.org/jira/browse/FLINK-30677;;;","16/Jan/23 09:45;mapohl;Args, thanks for the pointer. I don't know why I missed that one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scalafmt documentation,FLINK-30697,13519731,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,16/Jan/23 09:16,19/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,auto-deprioritized-major,starter,,,"The Flink documentation instructs the user to copy {{tools/maven/scalastyle-config.xml}} (see [docs|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/flinkdev/ide_setup/#checkstyle-for-scala]) to enable scala checkstyle rules. This is obsolete after the changes made FLINK-26553 with its related [ML discussion|https://lists.apache.org/thread/wkrjp6jw2fvqfr16kmh4ltsn7dk8d010].

This issue is about updating the docs accordingly. The instructions can be found in the [corresponding announcement thread|https://lists.apache.org/thread/vsmkzz3kgxtfjnrszo3cg6zznr23l93v].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26553,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:01 UTC 2023,,,,,,,,,,"0|z1f31k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""joins"" page of ""Querys"" into Chinese",FLINK-30696,13519729,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,16/Jan/23 09:15,19/Mar/23 13:48,04/Jun/24 20:41,19/Mar/23 13:48,,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is[ [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/joins|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/joins]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/joins.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 13:48:01 UTC 2023,,,,,,,,,,"0|z1f314:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 09:17;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?

And the pull request has been committed.

Please see: [https://github.com/apache/flink/pull/21685]

 ;;;","19/Mar/23 13:48;jark;Fixed in master: abed76b106d0c8cfe63e0755489ec6bc85c41012 and 4c8aa0ba7b0da1f9ae04ae0b5c1593877313fa28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to set parallelism for compact operator according to the number of files in AQE.,FLINK-30695,13519706,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,16/Jan/23 07:25,31/Jan/23 04:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,"After FLINK-29635, we introduce auto compaction for Hive sink.  But it may cause costing much time to compact in batch aqe since the paramlism inferred  by aqe of the operator to compact files may small. 

In current design for compact files in Hive sink, there's a coordinator operator that collects all files written and decide which files should be merge to a file. It will pack the infomation to a CompactUnit which contains the files path that should be merge to a file.

Then, the coordinator operator will pass CompactUnit to downstream compact operator to do actual compaction.

The volume for the data emitted by the coordinator  is small for it only send control messages, which will cause the parallelism of the  compact operator small in aqe.  But actually,  most of work(reading files and write a new file) is done by the compact operator . If the parallelism of compact operator is small,  it must cost much time to compact.

Althogh the user can set the parallelism of the  compact operator  by manually, but it requires user to configure it and user may still find it's hard to set proper parallelism.

Ideally, the parallelism of the  compact operator should be equal to the number of the final merged  files which can be decided by the the coordinator operator. I think the aqe framework can provide some mechanism to make the operator itself decide the parallelism.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 04:33:58 UTC 2023,,,,,,,,,,"0|z1f2w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 04:33;luoyuxia;Maybe [~wanglijie] [~zhuzh] can have a look about the mechanism to make operator decide parallelism by itself(exactly upstream operator in this case) in aqe.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Windowing TVF"" page of ""Querys"" into Chinese ",FLINK-30694,13519676,13285584,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chenhaiyang,chenhaiyang,chenhaiyang,16/Jan/23 02:39,14/Mar/23 04:01,04/Jun/24 20:41,14/Mar/23 04:01,1.16.0,,,,,,1.18.0,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The page url is[ [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/window-tvf/]

 

The markdown file is located in docs/content.zh/docs/dev/table/sql/queries/window-tvf.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 04:01:45 UTC 2023,,,,,,,,,,"0|z1f2pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 02:49;chenhaiyang;Hi [~jark] ,Can you assign this issue to me?
I'd like to working on this ticket.
Thank you very much!;;;","16/Jan/23 04:18;chenhaiyang;Hi [~jark], the PR has been commited, please take a look here: [https://github.com/apache/flink/pull/21823]
 
 ;;;","02/Mar/23 06:06;chenhaiyang;Hi [~jark], I have pull the pr for this ticket.

;;;","14/Mar/23 04:01;jark;Fixed in master: a94b556e6a02dcc9220a9e19acd62c230550bf27...7b03113461c36d06c0643d9bdca7055071d41429;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce RowFormat to SQL Gateway for result fetching,FLINK-30693,13519662,13478114,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,yzl,yzl,15/Jan/23 13:04,17/Jan/23 13:21,04/Jun/24 20:41,17/Jan/23 13:21,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,"Currently, the SQL gateway only sends JSON-formatted result to REST endpoint. So this ticket introduces RowFormat to control the result format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 13:21:13 UTC 2023,,,,,,,,,,"0|z1f2m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 13:21;fsk119;Merged into master: 029860f26ce8455cdd2bd1244785f42fee0440ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce embedded session manager to support REMOVE JAR,FLINK-30692,13519649,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,15/Jan/23 09:45,30/Jan/23 14:10,04/Jun/24 20:41,30/Jan/23 14:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 14:10:35 UTC 2023,,,,,,,,,,"0|z1f2jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 14:10;fsk119;Merged into master: c9e87fe410c42f7e7c19c81456d4212a58564f5e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client supports to specify endpoint address,FLINK-30691,13519648,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,15/Jan/23 09:44,29/Jan/23 13:39,04/Jun/24 20:41,29/Jan/23 13:39,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 13:39:06 UTC 2023,,,,,,,,,,"0|z1f2j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 13:39;fsk119;Merged into master:

0b783a54e8d5c1f30b735bb658bb558cae11bb6f

95df4e726f3862d3be69a7cf753acd7890bb905d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"java document, web document, word some error perfect",FLINK-30690,13519642,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zheju_he,zheju_he,zheju_he,15/Jan/23 07:04,15/Feb/23 01:56,04/Jun/24 20:41,15/Feb/23 01:56,,,,,,,1.18.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"* java document \{@link} is incorrectly used
 * The Chinese document stream kafka connect has a spelling error

 * An error spelling flink as fink occurred in FlinkSqlParserImplTest",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/23 07:08;zheju_he;image-2023-01-15-15-08-27-625.png;https://issues.apache.org/jira/secure/attachment/13054597/image-2023-01-15-15-08-27-625.png","15/Jan/23 07:09;zheju_he;image-2023-01-15-15-09-46-671.png;https://issues.apache.org/jira/secure/attachment/13054598/image-2023-01-15-15-09-46-671.png","15/Jan/23 07:10;zheju_he;image-2023-01-15-15-10-23-364.png;https://issues.apache.org/jira/secure/attachment/13054599/image-2023-01-15-15-10-23-364.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 01:56:30 UTC 2023,,,,,,,,,,"0|z1f2hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/23 07:09;zheju_he;[https://docs.oracle.com/javase/7/docs/technotes/tools/windows/javadoc.html#link]

The first problem can be found with more than 50 errors through the idea's regular match 
{code:java}
\*\s[^\n]+}[^\{\n]+} {code}
!image-2023-01-15-15-08-27-625.png|width=406,height=88!

The correct way to use it is *{{{{}@link{}}}*  package.class{{{}#{}}}member  label{*}{{}}}{*}

 ;;;","15/Jan/23 07:10;zheju_he;the last two questions

!image-2023-01-15-15-09-46-671.png|width=497,height=175!

!image-2023-01-15-15-10-23-364.png|width=454,height=205!;;;","15/Jan/23 07:12;zheju_he;I would like to revise these problems and try to submit pr for the first time, thanks;;;","15/Jan/23 08:13;zheju_he;This is my pr address https://github.com/apache/flink/pull/21675;;;","31/Jan/23 08:10;zheju_he;[~will86] Can you look at this pr for me;;;","15/Feb/23 01:56;jark;Fixed in master: 3856c49af77601cf7943a5072d8c932279ce46b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use auto produce bytes schema in Pulsar sink,FLINK-30689,13519635,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Jan/23 03:43,08/Feb/23 10:15,04/Jun/24 20:41,08/Feb/23 10:15,pulsar-4.0.0,,,,,,pulsar-4.0.0,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,Pulsar has a {{Schema.AUTO_PRODUCE_BYTES()}} for sending the byte array messages to broker with extra schema validation. This would be better than directly using {{Schema.BYTES}} which bypass the validation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 10:15:55 UTC 2023,,,,,,,,,,"0|z1f2g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 10:15;tison;master via https://github.com/apache/flink-connector-pulsar/pull/20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable Kryo fallback for tests in flink-ml-lib,FLINK-30688,13519150,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,lindong,lindong,14/Jan/23 13:43,16/Feb/23 11:48,04/Jun/24 20:41,16/Feb/23 11:48,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"Kryo serializer is typically inefficient and we expect all Flink ML algorithms to work without using the Kryo serializer. It will be useful to validate this by disabling Kryo fallback in all Flink ML tests.

This can be done by configuring StreamExecutionEnvironment using env.getConfig().disableGenericTypes().",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 11:48:35 UTC 2023,,,,,,,,,,"0|z1ezgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 11:48;zhangzp;Fixed on master via:

7e26b7e2904a9d742e0318035a19bf2687855bb0

19f0f31ca93f4967c99968b3609a11763bfc0afb

904aba25951b5df170b6979a8a98e9fbff00d539

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FILTER not effect in count(*),FLINK-30687,13518899,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanjialiang,tanjialiang,14/Jan/23 06:06,24/May/24 18:47,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When i try to using Flink SQL like this, i found 'FILTER(WHERE class_id = 1)' is not effect.
{code:java}
CREATE TABLE student
(
    id INT NOT NULL,
    name STRING,
    class_id INT NOT NULL
)
WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/test',
    'table-name' = 'student',
    'username' = 'root',
    'password' = '12345678'
);

SELECT COUNT(*) FILTER (WHERE class_id = 1) FROM student;

or

SELECT COUNT(1) FILTER (WHERE class_id = 1) FROM student;{code}
 

But when i tried Flink SQL like this, it worked.
{code:java}
SELECT COUNT(*) FROM student WHERE class_id = 1;

or 

SELECT COUNT(class_id) FILTER (WHERE class_id = 1) FROM student;{code}
 

By the way, mysql connector has a bug and fixed in https://issues.apache.org/jira/browse/FLINK-29558. Maybe you try this demo should cherry-pick this PR first.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/23 02:54;tanjialiang;image-2023-01-16-10-54-04-673.png;https://issues.apache.org/jira/secure/attachment/13054606/image-2023-01-16-10-54-04-673.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 09:35:18 UTC 2023,,,,,,,,,,"0|z1exwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 02:57;tanjialiang;I try to found out how to fix this, i found in org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator#createFilterExpression the filterArg must be >0, but ‘SELECT COUNT( * ) FILTER (WHERE cluster_id = 1) FROM ${table}’ 's filterArg is 0. I fixed the condition in 'filterArg >= 0', and it work.

!image-2023-01-16-10-54-04-673.png!

 

So i want to ask why we can not filterArg=0? Did it some scenes we can not use? Because 'inputFieldTypes' is a 'scala.collection.Seq' which index is start from 0.;;;","17/Jan/23 09:35;martijnvisser;[~TsReaper] I believe you know most about the codegen, could you have a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify the configuration of adaptive batch scheduler,FLINK-30686,13518607,13518520,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,14/Jan/23 03:33,31/Jan/23 15:30,04/Jun/24 20:41,31/Jan/23 15:30,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Based on the [FLIP-283|[https://cwiki.apache.org/confluence/display/FLINK/FLIP-283%3A+Use+adaptive+batch+scheduler+as+default+scheduler+for+batch+jobs]], this issue mainly focuses on the second issue.

This change includes three parts:

1.Introduce ""execution.batch.adaptive.auto-parallelism.enabled"" as a switch for automatic parallelism derivation
2.Modify adaptive batch scheduler configuration default values
3.Rename the configuration of adaptive batch scheduler",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 15:30:57 UTC 2023,,,,,,,,,,"0|z1ew3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 15:30;zhuzh;Done via 0915c9850d861165e283acc0f60545cd836f0567;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support mark the transformations whose parallelism is infected by the input transformation,FLINK-30685,13518593,13518520,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,14/Jan/23 03:26,06/Feb/23 03:23,04/Jun/24 20:41,31/Jan/23 15:31,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In order to chain operators together as much as possible, many downstream operators will use the parallelism of upstream input operators in the table planner.

If some operators need to have their own defined parallelism, the parallelism will be explicitly set. Therefore, the operator that takes the parallelism of the upstream operator as its own parallelism should be automatically derived by the AdaptiveBatchScheduler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30901,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 15:31:31 UTC 2023,,,,,,,,,,"0|z1ew0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 15:31;zhuzh;Done via fea9ffedecf81a97de5c31519ade3bab8228e743;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use the default parallelism as a flag for vertices that can automatically derive parallelism.,FLINK-30684,13518577,13518520,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,14/Jan/23 03:18,31/Jan/23 15:31,04/Jun/24 20:41,31/Jan/23 15:31,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"This change proposes to add a *parallelismConfigured* property as a flag to identify whether the parallelism of node is used ""parallelism.default"" or not. If the vertex's *parallelismConfigured* is true, the AdaptiveBatchScheduler will not automatically deciding parallelisms for it. Otherwise, AdaptiveBatchScheduler will automatically deciding parallelisms and use the ""parallelism.default"" as an alternative value for the ""jobmanager.adaptive-batch-scheduler.max-parallelism"".

This change will make user do not need to configure ""parallelism.default"" as ""-1"" to automatically deciding parallelisms for vertices.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 15:31:55 UTC 2023,,,,,,,,,,"0|z1evx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 15:31;zhuzh;Done via d9dd5dce00e2389a8ebadf7c15d8f4a0f99f535e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make adaptive batch scheduler as the default batch scheduler,FLINK-30683,13518541,13518520,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,14/Jan/23 02:59,17/Mar/23 02:30,04/Jun/24 20:41,30/Jan/23 08:42,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"Based on the [FLIP-283|https://cwiki.apache.org/confluence/display/FLINK/FLIP-283%3A+Use+adaptive+batch+scheduler+as+default+scheduler+for+batch+jobs], this issue mainly focuses on the first issue.

This change proposes to make AdaptiveBatchScheduler as the default batch scheduler and user can use it without explicitly configuring it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30846,,FLINK-31055,,,FLINK-30808,FLINK-31093,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 08:42:25 UTC 2023,,,,,,,,,,"0|z1evp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 08:42;zhuzh;Done via
143464d82814e342aa845f3ac976ae2854fc892f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-283: Use adaptive batch scheduler as default scheduler for batch jobs,FLINK-30682,13518520,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,14/Jan/23 02:48,09/Feb/23 06:46,04/Jun/24 20:41,09/Feb/23 06:46,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,"To further use the adaptive batch scheduler to improve flink's batch capability, in this FLIP we aim to make the adaptive batch scheduler as the default batch scheduler and optimize the current adaptive batch scheduler configuration.

More details see [FLIP-283|https://cwiki.apache.org/confluence/display/FLINK/FLIP-283%3A+Use+adaptive+batch+scheduler+as+default+scheduler+for+batch+jobs].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30889,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-14 02:48:33.0,,,,,,,,,,"0|z1evkg:",9223372036854775807,Adaptive batch scheduler are now used for batch jobs by default. It will automatically decide the parallelism of operators. The keys and values of related configuration items are improved for easy of use. More details can be found at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/elastic_scaling/#adaptive-batch-scheduler.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar-Flink connector corrupts its output topic,FLINK-30681,13518367,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,syhily,jacek_wislicki,jacek_wislicki,13/Jan/23 21:18,15/Jan/23 02:55,04/Jun/24 20:41,15/Jan/23 02:55,1.15.3,,,,,,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,1,pull-request-available,,,,"When PulsarSink writes a message to its output topic, the topic gets permanently corrupted and cannot be used anymore (even with newly created subscriptions).

We have isolated this behaviour to a minimal project demonstrating the problem available on [GitHub|https://github.com/JacekWislicki/vp-test5]:
# There are 2 topics: IN and OUT
# IN is subscribed by a Flink's InToOutJob (with PulsarSource) and writes to OUT (with PulsarSink)
# OUT is subscribed by a Pulsar's OutReadFunction
# When we write directly to OUT (e.g., with OutTopicProducer), OutReadFunction gets each message from its backlog and processes it with no issue (the ledger position updates)
# When we write to IN (e.g., with InTopicProducer), InToOutJob reads the message, processes it and writes to OUT
# OutReadFunction reads the message, the ledger position updates, but nothing happens
## Further messages written to OUT are not read as OUT is blocked on the last message from Flink
## Truncating OUT does not help, neither does unsubscribing or creating a new subscription

Reproduced with Pulsar 2.9.1, 2.9.2 and 2.10.2.

The issue does not occur when we use our custom temporary old SinkFunction implementation based on a Pulsar producer writing to OUT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Sun Jan 15 02:55:44 UTC 2023,,,,,,,,,,"0|z1eumg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 23:32;syhily;I have checked your code and understand what you have meet. You have set the delivery guarantee to {{EXACTLY_ONCE}} in [your code|https://github.com/JacekWislicki/vp-test5/blob/0f4a66100c138945435e2690da8912b14bf0354f/flink/src/main/java/com/example/test5/flink/job/BaseJob.java#L57]. Pulsar sink will write the messages into a Pulsar’s transaction in this way. And the transaction won’t be committed until the Flink job has finished the checkpoint and Pulsar sink will start a new transaction after committing it. So the written messages in your code won’t be consumed until the Flink job enabled the checkpoint.

Any newly written messages without any transaction in this topic won’t be consumed in the mean time. Because there are pending transactions. This could explain what you have called: Pulsar connector corrupts its output topic.

You can set the {{DeliveryGuarantee}} to {{AT_LEAST_ONCE}} to see if it fixes this case. And please keep this issue open. I'll add an explain in the [connector document|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/pulsar/#delivery-guarantee].;;;","15/Jan/23 02:55;tison;Explained in https://github.com/apache/flink-connector-pulsar/pull/18.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider using the autoscaler to detect slow taskmanagers,FLINK-30680,13518334,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,wangm92,gyfora,gyfora,13/Jan/23 14:22,24/Aug/23 15:49,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,1,,,,,"We could leverage logic in the autoscaler to detect slow taskmanagers by comparing the per-record processing times between them.

If we notice that all subtasks on a single TM are considerably slower than the rest (at similar input rates) we should try simply restarting the job instead of scaling it up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 15:49:33 UTC 2023,,,,,,,,,,"0|z1euf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 17:33;mxm;Is this related to unhealthy task managers which need to be recycled? I'm asking because if this is partition skew, restarting wouldn't help to resolve the skew.;;;","19/Jan/23 05:45;zjureel;Thanks [~gyfora] to create this issue. In fact out team in bytedance has developed similar function in our flink cluster, we are trying to apply it in production. Our test results show that it has very good effect on slow nodes of streaming process.

As for the difference between 'detect slow tm' and restart the job and the overall proposal, [~wangm92] and [~Zhanghao Chen] can give more input;;;","19/Jan/23 11:38;wangm92;Thanks [~gyfora] to create this issue.

In our production environment, during the peak business period, some flink streaming jobs maybe cause processing delays due to the slow task-manager:
 # In some cases, we found that processing slow tasks(busytime 100% or tasks in pool usage 100%) were clustered on the same nodes;
 # These nodes usually have some problems, like: the CPU usage is too high, some nodes are older and less powerful, the network delay of the node is relatively high, and so on.

These cases appear more frequently. And k8s can solve some problems, but cannot cover all cases. There should also be a speculative execution mechanism like the batch job for streaming job to process this case automatically.

We launched the *Slow-TaskManager Dection mechanism* within the company to automatically process this case. If there is a lag record in the job, the mechanism will determine whether it is caused by the slow-Taskmanager.It will require the job not to have data skew, and the slow-processing tasks are clustered on the same taskmanager or node. If not clustered on the same taskmanager/node, it is not judged as the slow-taskmanager.
 
After the slow taskmanager is found, Flink will release the taskmanager, and it will be added to the blacklist (internal implementation, the machines that join the blacklist will not schedule new pod), then the job will do failover recovery. This mechanism works very effectively in our internal environment.
 
We are very happy to address this jira and contribute our internal mechanism to the community, and discuss it together to improve the automatic operation and maintenance capability of Flink.;;;","19/Jan/23 13:04;gyfora;Thanks for the input [~wangm92] . I think this would be a good improvement in the operator itself.  We could combine this with the existing health check mechanisms that detect jobs stuck in a failure loop and simply restart them in those case.

Alternatively we could put this into the new autoscaler module which already collects metrics from the flink jobs for scaling purposes.;;;","20/Jan/23 03:41;wangm92;[~gyfora] When we designed it internally, we also considered that this mechanism will rely heavily on Metrics. It also needs to observe the time trend of metrics (to judge whether the latency of a job is on the rise or decline), so it is implemented in a control service rather than inside the flink engine. So I think it is a good choice to implement this in the operator, which can reuse the Metrics module of the autoscaler.;;;","24/Aug/23 03:03;wangm92;[~gyfora] [~mxm] Hi, can I take this Jira? I am interested in this. We have a lot of internal Flink jobs. Now the *Slow-TaskManager Dection mechanism* function has launched tens of thousands of jobs internally, and the effective processing can reach about 1,000 times a day which can reduce the lag of job. We want to Contribute this ferature back to the community and we'll come up with a design document to discuss with you;;;","24/Aug/23 05:43;gyfora;Sure [~wangm92] , I will assign this to you. I think the best way to start would be a FLIP to describe the design and what problem we are solving and how exactly so that other community members can also understand and participate in the review :) ;;;","24/Aug/23 08:35;wangm92;[~gyfora] Can you give me the permissions on Confluence? My Confluence account id is wangzzu;;;","24/Aug/23 15:49;gyfora;done [~wangm92] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not load the data of hive dim table when project-push-down is introduced,FLINK-30679,13518324,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,13/Jan/23 11:58,14/Feb/23 10:43,04/Jun/24 20:41,30/Jan/23 08:00,1.14.6,,,,,,1.14.7,1.15.4,1.16.2,1.17.0,,,,,,,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"维表project push down优化引入：
https://issues.apache.org/jira/browse/FLINK-29138

hive维表的两个问题：
https://issues.apache.org/jira/browse/FLINK-29992
https://issues.apache.org/jira/browse/FLINK-30679

 

 

 

Can not load the data of hive dim table when project-push-down is introduced.

 

hive-exec  version: 2.3.4

flink version: 1.14.6

flink-hive-connector: the latest code for release-1.14 branch

 

vectorize read:

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 3
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.useOrcVectorizedRead(HiveTableInputFormat.java:276) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:129) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:86) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at LookupFunction$26.flatMap(Unknown Source) ~[?:?] {code}
 

 

mapreduce read:

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 3
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.lambda$new$0(HiveMapredSplitReader.java:139) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) ~[?:1.8.0_301]
    at java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032) ~[?:1.8.0_301]
    at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) ~[?:1.8.0_301]
    at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438) ~[?:1.8.0_301]
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.<init>(HiveMapredSplitReader.java:141) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:157) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:86) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at LookupFunction$26.flatMap(Unknown Source) ~[?:?]
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:81) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6] {code}
 

 

The sql :

 
{code:java}
CREATE TABLE kafkaTableSource (
    name string,
    age int,
    sex string,
    address string,
    ptime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan1',
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.client.id' = 'test-consumer-group',
    'properties.group.id' = 'test-consumer-group',
    'format' = 'csv'
);

CREATE TABLE printsink (
    name string,
    age int,
    sex string,
    address string,
    score bigint,
    dt string
) WITH (
    'connector' = 'print'
);

CREATE CATALOG myhive
WITH (
        'type' = 'hive',
        'default-database' = 'hhy',
        'hive-version' = '2.0.0',
        'hadoop-conf-dir'='/Users/hehuiyuan/soft/hadoop/hadoop-2.7.3/etc/hadoop'
);

USE CATALOG myhive;
USE hhy;

set table.sql-dialect=hive;
CREATE TABLE IF NOT EXISTS tmp_flink_test_text (
    name STRING,
    age INT,
    score BIGINT
) PARTITIONED BY (dt STRING) STORED AS TEXTFILE TBLPROPERTIES (
    'streaming-source.enable' = 'false',
    'streaming-source.partition.include' = 'all',
    'lookup.join.cache.ttl' = '5 min'
);
set table.sql-dialect=default;

USE CATALOG default_catalog;
INSERT INTO default_catalog.default_database.printsink
SELECT s.name, s.age, s.sex, s.address, r.score, r.dt
FROM default_catalog.default_database.kafkaTableSource  as s
JOIN myhive.hhy.tmp_flink_test_text FOR SYSTEM_TIME AS OF s.ptime  AS r
ON r.name = s.name;
 {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 08:00:47 UTC 2023,,,,,,,,,,"0|z1eucw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 01:31;hehuiyuan;[~luoyuxia] ,hi

Take a look when you have time;;;","16/Jan/23 06:25;luoyuxia;[~hehuiyuan] Thanks for contribution. I will have a look.;;;","30/Jan/23 08:00;jark;Fixed in 
 - master: 7df2a12472f4bf990f86b905cdfcbf83d19e76b3
 - release-1.16: b5f50c354c47f27ca66c2d26c78c744d1b75c65a
 - release-1.15: b412bc315fc851c453266cd7f6f98ef3ca0ea747
 - release-1.14: 08dbb6d5eff46e72af412643f80ac353636821de
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyConnectionManagerTest.testManualConfiguration appears to be unstable,FLINK-30678,13518320,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,mapohl,mapohl,13/Jan/23 11:02,19/Jan/23 09:49,04/Jun/24 20:41,19/Jan/23 09:49,1.16.0,,,,,,1.16.1,1.17.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"We observe a test instability due to a ""Address already in use"" issue:
{code:java}
Jan 13 02:29:43 [ERROR] org.apache.flink.runtime.io.network.netty.NettyConnectionManagerTest.testManualConfiguration  Time elapsed: 0.132 s  <<< ERROR!
Jan 13 02:29:43 org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
Jan 13 02:29:43{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44777&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=6744",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 09:49:21 UTC 2023,,,,,,,,,,"0|z1euc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 09:49;chesnay;master: e705090c89ef9411304d54c57eac2f0080c522a7
1.16: fef1fdab86fe517df242e5a3ed4fe8f371adda7b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceStatementITCase.testFlinkSqlStatements fails,FLINK-30677,13518319,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Paul Lin,mapohl,mapohl,13/Jan/23 10:50,19/Jan/23 13:42,04/Jun/24 20:41,19/Jan/23 07:08,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,test-stability,,,"We're observing a test instability with {{SqlGatewayServiceStatementITCase.testFlinkSqlStatements}} in the following builds:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14251]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=14608
{code:java}
Jan 13 02:46:10 [ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 27.279 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase
Jan 13 02:46:10 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.testFlinkSqlStatements(String)[5]  Time elapsed: 1.573 s  <<< FAILURE!
Jan 13 02:46:10 org.opentest4j.AssertionFailedError: 
Jan 13 02:46:10 
Jan 13 02:46:10 expected: 
Jan 13 02:46:10   ""# table.q - CREATE/DROP/SHOW/ALTER/DESCRIBE TABLE
Jan 13 02:46:10   #
Jan 13 02:46:10   # Licensed to the Apache Software Foundation (ASF) under one or more
Jan 13 02:46:10   # contributor license agreements.  See the NOTICE file distributed with
Jan 13 02:46:10   # this work for additional information regarding copyright ownership.
Jan 13 02:46:10   # The ASF licenses this file to you under the Apache License, Version 2.0
Jan 13 02:46:10   # (the ""License""); you may not use this file except in compliance with
Jan 13 02:46:10   # the License.  You may obtain a copy of the License at
Jan 13 02:46:10   #
Jan 13 02:46:10   # http://www.apache.org/licenses/LICENSE-2.0
Jan 13 02:46:10   #
Jan 13 02:46:10   # Unless required by applicable law or agreed to in writing, software
Jan 13 02:46:10   # distributed under the License is distributed on an ""AS IS"" BASIS,
Jan 13 02:46:10   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
Jan 13 02:46:10   # See the License for the specific language governing permissions and
Jan 13 02:46:10   # limitations under the License.
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30698,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 13:40:46 UTC 2023,,,,,,,,,,"0|z1euc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 10:59;mapohl;It might be related to [f1770892|https://github.com/apache/flink/commit/f1770892c5b8bce408093fc4c2fa52aafd42d7c6] (FLINK-28655) or [6a15b7e9|https://github.com/apache/flink/commit/6a15b7e9b2459f2bb398c8d7cd062c8fb92da0f2] (FLINK-29950). Both commits were added before the previous (successful) nightly run. [~yzl] and [~Paul Lin] May you have a look at it?;;;","14/Jan/23 04:16;Paul Lin;I think the problem lies here:
{code:java}
2023-01-13T01:32:32.9258077Z Jan 13 01:32:32   # test explain select with CHANGELOG_MODE
2023-01-13T01:32:32.9258568Z Jan 13 01:32:32   explain changelog_mode select `user`, product from orders;
2023-01-13T01:32:32.9259004Z Jan 13 01:32:32   !output
2023-01-13T01:32:32.9259402Z Jan 13 01:32:32   java.util.NoSuchElementException: null
2023-01-13T01:32:32.9259800Z Jan 13 01:32:32   !error {code}
It seems not directly related to f1770892 (FLINK-28655), but I will take a look.

 ;;;","14/Jan/23 09:07;Paul Lin;-I couldn't reproduce the error locally. Is that fixed already?-

Managed to reproduce the problem, like a 1 in 10 chance.;;;","16/Jan/23 09:46;mapohl;Mentioned in FLINK-30698 already but quoting it here once more:
{quote}[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44835&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=17487]

It's a different cause: A NullPointerException is thrown.
{code:java}
Jan 15 01:54:40 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.284 s <<< FAILURE! - in org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase
Jan 15 01:54:40 [ERROR] org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.testFlinkSqlStatements(String)[5]  Time elapsed: 1.212 s  <<< ERROR!
Jan 15 01:54:40 java.lang.NullPointerException
Jan 15 01:54:40 	at org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.stringifyException(SqlGatewayRestEndpointStatementITCase.java:194)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.runStatements(AbstractSqlGatewayStatementITCase.java:144)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.runTest(AbstractSqlGatewayStatementITCase.java:269)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.testFlinkSqlStatements(AbstractSqlGatewayStatementITCase.java:118)
[...] {code}
{quote};;;","16/Jan/23 09:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44863&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14251;;;","17/Jan/23 08:36;martijnvisser;[~fsk119] Any thoughts on this one?;;;","17/Jan/23 10:00;fsk119;Sorry for introducing unstable test. I think the main root cause is because of the FLINK-29950. Before FLINK-29950, we always use a DUMMY_RESULT_STORE for the non-query results. With the DUMMY_RESULT_STORE, the ResultFetcher will actively put all data into the current buffer. But in the FLINK-29950, we uses a non-dummy result store that will start a thread periodically to fetch results. In this case, the rest client may get an empty results.  

So I think we should keep the origin behaviour to use the dummy result store. WDYT [~Paul Lin] 

 ;;;","17/Jan/23 11:25;Paul Lin;[~fsk119] I prefer the old approach too. It's intuitive and efficient.;;;","17/Jan/23 15:40;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44927&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14499
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44927&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14501;;;","18/Jan/23 07:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=14503;;;","19/Jan/23 07:08;fsk119;Fixed in the 4fdb5c40094cfaa5fb3b6d7ce9ec891dab3ef32a;;;","19/Jan/23 13:40;mapohl;This build didn't contain the fix for the Shengkai's previous comment, yet. I document it anyway for completeness reasons (build git sha: b35cef59).

1 build, 2 failures:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12619]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12976] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Data Structures for table store,FLINK-30676,13518310,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Jan/23 10:29,16/Jan/23 09:15,04/Jun/24 20:41,16/Jan/23 09:15,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Copy data structures to table store from Flink.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 09:15:18 UTC 2023,,,,,,,,,,"0|z1eua0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 09:15;lzljs3620320;master: 6a326fe9d001140fe99960e2c1e43c17bc7665e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decompose printing logic from Executor,FLINK-30675,13518292,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,13/Jan/23 08:24,17/Jan/23 11:18,04/Jun/24 20:41,17/Jan/23 11:18,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 11:18:15 UTC 2023,,,,,,,,,,"0|z1eu68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 11:18;fsk119;Merged into master: 128b0c4442a7c8e8efd5f7f3151523fca394f91c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enabling consume partial finished partition by default for speculative execution in hybrid shuffle mode,FLINK-30674,13518155,13510697,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,13/Jan/23 07:11,02/Feb/23 09:12,04/Jun/24 20:41,02/Feb/23 09:12,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Network,,,,,0,pull-request-available,,,,"At present, if hybrid shuffle enabled speculative execution, it will only consume all finished partition by default. It is better to change this default behavior to consume partial finished upstream partition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 09:12:11 UTC 2023,,,,,,,,,,"0|z1etbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:12;xtsong;master (1.17): 7bc18ef7c3065b8c2210f66b2329dc6bdf49ff56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add documentation for ""EXPLAIN PLAN_ADVICE"" statement",FLINK-30673,13518151,13517856,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,13/Jan/23 07:10,03/Feb/23 13:03,04/Jun/24 20:41,03/Feb/23 13:03,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 13:03:23 UTC 2023,,,,,,,,,,"0|z1etaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 13:03;lincoln.86xy;fixed in master: dfedb1eeab4697aad947aa798b26657d44a2d958;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support 'EXPLAIN PLAN_ADVICE' statement,FLINK-30672,13518112,13517856,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,13/Jan/23 06:54,19/Jan/23 14:55,04/Jun/24 20:41,19/Jan/23 14:55,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 14:55:30 UTC 2023,,,,,,,,,,"0|z1et28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 14:55;lincoln.86xy;fixed in master: 6f88d7a973731ad961a2eb7f56608807766169ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AlgoOperator for ClusteringEvaluator,FLINK-30671,13518082,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,kamalesh0420,zhangzp,zhangzp,13/Jan/23 06:41,19/Apr/23 02:18,04/Jun/24 20:41,,ml-2.2.0,,,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 02:18:32 UTC 2023,,,,,,,,,,"0|z1esvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jan/23 06:03;kamalesh0420;Hi [~zhangzp] can you assign me this issue? I am new to the library but I think I can work on this. ;;;","19/Apr/23 02:18;lindong;Hi [~kamalesh0420], sorry for late reply. I guess [~zhangzp] did not notice the message.

The ticket has been assigned to you. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore broadcast bytes when computing parallelism and input infos for adaptive batch scheduler,FLINK-30670,13518065,13486602,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,13/Jan/23 06:35,16/Jan/23 09:17,04/Jun/24 20:41,16/Jan/23 09:17,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,"Currently, we include the broadcast bytes in the ""jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task"" when calculating the parallelism (see [PR17952|https://github.com/apache/flink/pull/17952] for details), and set a cap ratio(0.5) for the broadcast bytes. Considering that the broadcast bytes are generally relatively small, we can ignore the broadcast bytes to simplify the logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 09:17:04 UTC 2023,,,,,,,,,,"0|z1esrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 09:17;wanglijie;Done via master df29023d8be8d91bed9aa2110ef4be7d66cc9d9f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update recent job status in FlinkDeployment resource object.,FLINK-30669,13517966,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kmozaid,kmozaid,13/Jan/23 04:26,31/Jan/23 08:24,04/Jun/24 20:41,31/Jan/23 08:24,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"User jar has code as  -
{code:java}
main() {
 init env
 pipelines.foreach{
  env.fromSource(pipeline.getSource())
     .map(pipeline.transform())
     .sinkTo(pipeline.getSink())
  env.execute(pipeline.getName())
 }
}{code}
and below configuration -
{code:java}
execution.runtime-mode: ""BATCH""
execution.attached: ""true""
$internal.pipeline.job-id: """" {code}
When this single jar executed in Application Mode by using flink-kubernetes-operator, multiple jobs are submitted sequentially and as per design only one of the JobStatus is always associated with FlinkDeployment k8s resource, this job status is periodically updated by operator. To update job status in k8s resource, it fetches all of the job status from job-manager rest endpoint and pick the first one and update that one. Problem is, job status list returned by job-manager rest api is not sorted on time.

!image-2023-01-13-10-04-32-891.png|width=883,height=489!
!image-2023-01-13-09-54-54-280.png|width=353,height=284!

As you can see in above example, job autoscaling-3 is first one in the rest response and same updated in FlinkDeployment resource, but FlinkDeployment should have status of job autoscaling-19 because that is the last job finished.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 04:24;kmozaid;image-2023-01-13-09-54-13-457.png;https://issues.apache.org/jira/secure/attachment/13054568/image-2023-01-13-09-54-13-457.png","13/Jan/23 04:24;kmozaid;image-2023-01-13-09-54-54-280.png;https://issues.apache.org/jira/secure/attachment/13054567/image-2023-01-13-09-54-54-280.png","13/Jan/23 04:34;kmozaid;image-2023-01-13-10-04-32-891.png;https://issues.apache.org/jira/secure/attachment/13054569/image-2023-01-13-10-04-32-891.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 08:24:10 UTC 2023,,,,,,,,,,"0|z1es5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 12:16;gyfora;The operator doesn't really work for multiple jobs in general. The Job observer and upgrade logic already depends on a single job for detecting errors taking savepoints etc.

I don't really see the value of fixing this.;;;","31/Jan/23 08:24;gyfora;merged to main 9168c98b2e52538648670f79ee37a19e95d9580a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ExplainFormat to Explainable and TableEnvironment,FLINK-30668,13517964,13517856,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,13/Jan/23 04:03,15/Jan/23 14:11,04/Jun/24 20:41,15/Jan/23 14:11,,,,,,,1.17.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,This subtask concentrates on Public API changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 15 14:11:13 UTC 2023,,,,,,,,,,"0|z1es5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/23 14:11;lincoln.86xy;fixed in master: 56b124bcfd661a295ab8772d265c12de25f690ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 remove the planner @internal dependency in flink-connector-hive,FLINK-30667,13517962,13433321,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,foxss,foxss,13/Jan/23 03:14,26/Jun/23 11:48,04/Jun/24 20:41,27/Apr/23 02:09,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"There are some classes in flink-connector-hive reply on  planner, but fortunately, not too many.

It mainly rely on ParserImpl, PlannerContext, PlannerQueryOperation and so on.  The dependency is mainly required to create RelNode.

To resolve this problem,  we need more abstraction for planner and provides public API for external dialects.",,,,,,,,,,,,,,,,,,FLINK-31409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 02:09:59 UTC 2023,,,,,,,,,,"0|z1es4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 22:43;foxss;parser should be PublicEvolving interface while both Flink and hive has own internal implementation. so hive connector maintainer less worry about Flink planner changes

PlannerQueryOperation should keep internal in both table-planner as well as hive-connector so hive connector can have full control and evolve without worry how Flink planner PlannerQueryOperation evolve

PlannerContext is simple enough util can be PublicEvolving;;;","06/Feb/23 01:34;luoyuxia;Just FYI, previously I created a  [FLIP-216|https://cwiki.apache.org/confluence/display/FLINK/FLIP-216%3A++Introduce+pluggable+dialect+and+plan+for+migrating+Hive+dialect]  for it as decoupling will involve public API.

Please take a look before you start to work on it so that we can make sure the directioin won't deviate too much.;;;","06/Feb/23 01:34;luoyuxia;Thank you very much for your efforts, please let me know if you have any question.;;;","27/Apr/23 02:09;luoyuxia;It should be fixed by FLINK-31409, if I'm wrong, feel free to open it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document for Delete/Update API,FLINK-30666,13517961,13517829,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/Jan/23 03:10,07/Feb/23 04:35,04/Jun/24 20:41,07/Feb/23 04:35,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 04:35:33 UTC 2023,,,,,,,,,,"0|z1es4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 04:35;lincoln.86xy;fixed in master: f3608f4277e47b76d59a5ec99c21947aea8cde5b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add implementation for Update,FLINK-30665,13517960,13517829,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/Jan/23 03:09,30/Jan/23 08:30,04/Jun/24 20:41,30/Jan/23 08:30,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 08:30:38 UTC 2023,,,,,,,,,,"0|z1es4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 08:30;lincoln.86xy;fixed in master: 5afc301bc6445bd5bbd9cb521dc20a808a67694e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Connector/Hive] cleanup hive/haoop package ambiguous package dependencies,FLINK-30664,13517959,13433321,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,foxss,foxss,13/Jan/23 03:08,19/May/23 09:24,04/Jun/24 20:41,19/May/23 09:24,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,"hive and hive-metastore combination introduced multiple versions of dependency packages, the goal is to ensure hive-connector has deterministic dependency ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 09:24:11 UTC 2023,,,,,,,,,,"0|z1es48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 02:22;luoyuxia;What do you mean by saying ""hive and hive-metastore combination introduced multiple versions of dependency packages""?

Doesn't the hive-connector have determinsitic dependency?;;;","19/May/23 09:24;luoyuxia;Close it now as it's invalid to me. Feel free to open it if I make mistake.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add implementation for SupportsRowLevelDelete,FLINK-30663,13517958,13517829,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,luoyuxia,luoyuxia,13/Jan/23 03:08,15/Jan/23 07:47,04/Jun/24 20:41,15/Jan/23 07:47,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-13 03:08:25.0,,,,,,,,,,"0|z1es40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Delete,FLINK-30662,13517957,13517829,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/Jan/23 03:07,29/Nov/23 21:25,04/Jun/24 20:41,20/Jan/23 02:59,,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 20 02:59:22 UTC 2023,,,,,,,,,,"0|z1es3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jan/23 02:59;lincoln.86xy;fixed in master: b7d32069c0971fba1c33950aa373624bb6018505;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce interfaces for Delete/Update,FLINK-30661,13517956,13517829,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/Jan/23 03:05,16/Jan/23 10:02,04/Jun/24 20:41,16/Jan/23 10:02,,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 10:02:44 UTC 2023,,,,,,,,,,"0|z1es3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 10:02;lincoln.86xy;fixed in master: 904c695776a6c28e67c50ce21115141a99446aa8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move SQLClientHiveITCase and TestHiveCatalogFactory to flink-connector-hive e2e,FLINK-30660,13517951,13433321,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,luoyuxia,foxss,foxss,13/Jan/23 02:19,09/Jun/23 02:55,04/Jun/24 20:41,08/Jun/23 01:46,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Hive,Tests,,,,0,pull-request-available,,,,"move SQLClientHiveITCase and TestHiveCatalogFactory to flink-connector-hive e2e

[https://github.com/apache/flink/pull/16532/files#]",,,,,,,,,,,,,,,,,,,FLINK-13400,,,,,,,,,,,,,FLINK-32291,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 02:55:01 UTC 2023,,,,,,,,,,"0|z1es2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 01:46;luoyuxia;master:

98f5500fea96213741aa300b4b9e76be588d867a

3f7e9245de5f6618c535d2b59251bb6e3adb0def;;;","08/Jun/23 15:55;chesnay;Can we revert this? The e2e tests are failing ever since the test was merged to master. 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49754&view=results;;;","09/Jun/23 02:55;luoyuxia;Sorry for that. It turns out that I miss the last one commit while suashing my commits in the

[pr|[https://github.com/apache/flink/pull/22679]]  Fixed it via [https://github.com/apache/flink/pull/22740];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
drop flink-sql-parser-hive ,FLINK-30659,13517941,13433321,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,foxss,foxss,13/Jan/23 02:13,14/Mar/23 01:51,04/Jun/24 20:41,13/Mar/23 02:12,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Connectors / Hive,Table SQL / Planner,,,,0,pull-request-available,,,,"Hive Parser should stay with hive connector and maintained together. During runtime, those package should load/unload together.",,,,,,,,,,,,,,,,,,,FLINK-30658,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 02:12:52 UTC 2023,,,,,,,,,,"0|z1es08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 02:18;lsy;I think this module should be dropped directly, hive parser doesn't use it now.;;;","06/Feb/23 01:27;luoyuxia;HI, thanks for efforts. As [~lsy] mentioned, we don't need this module any more. We can drop it directly, previously, I made a pr  [https://github.com/apache/flink/pull/19077] for that. You can take it just as a reference.  ;;;","07/Feb/23 12:07;luoyuxia;[~foxss] Hi, would you please drop the flink-sql-parser-hive as the start?  If you don't have time to finish this, please let me know.

 ;;;","22/Feb/23 02:11;luoyuxia;[~foxss] HI, all. Are you still working on this. If so, I'm very happy to help review. Btw, it won't take much time to finish it, If you're still interested in it and can find some time, apperciated that you can take it.

If not, please let me know asap and then I will take over it for it's a important part of decouping Hive.;;;","06/Mar/23 09:00;luoyuxia;I'll take it then. I rename the title to ""drop flink-hive-parser module"";;;","13/Mar/23 02:12;luoyuxia;master: 981ef8bd874ac5c28512d8abfa17adcefa44e9bb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove flink-sql-parser-hive dependency in table-planner,FLINK-30658,13517940,13433321,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,foxss,foxss,13/Jan/23 02:09,13/Mar/23 02:13,04/Jun/24 20:41,13/Mar/23 02:13,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"In order to move Flink-sql-parser-hive out of Flink-table, we need to remove Flink-sql-parser-hive package dependency in Flink-table-planner.",,,,,,,,,,,,,,,,,,FLINK-30659,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 01:31:55 UTC 2023,,,,,,,,,,"0|z1es00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 01:31;luoyuxia;Just FYI, previously I created a  [FLIP-216|https://cwiki.apache.org/confluence/display/FLINK/FLIP-216%3A++Introduce+pluggable+dialect+and+plan+for+migrating+Hive+dialect]  for it as decoupling will involve public API.

Please take a look before you start to work on it so that we can make sure the directioin won't deviate too much.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Shared and Key_Shared related tests in Pulsar connector,FLINK-30657,13517938,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,13/Jan/23 01:07,31/Jan/23 10:51,04/Jun/24 20:41,31/Jan/23 10:51,1.15.3,1.16.0,pulsar-3.0.0,,,,pulsar-3.0.1,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,test-stability,,,"As the [FLINK-30413|https://issues.apache.org/jira/browse/FLINK-30413] issue talked, we have dropped the Shared and Key_Shared supported in upcoming flink-connector-pulsar 4.0 release. The flaky tests of Shared and Key_Shared still matters the old Flink build.

Cause these tests are useless now, we can just disable them without any fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,,,FLINK-30413,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 10:51:44 UTC 2023,,,,,,,,,,"0|z1erzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 08:53;mapohl;Quoting what I commented on in FLINK-30413:
{quote}[...] I realize that this very test that popped up as documented in my previous comment, should actually be disabled already. That's something we might have to look into as part of FLINK-30657. Thanks for taking care of it.
{quote};;;","13/Jan/23 21:55;syhily;[~mapohl] Thanks for notify. I have seem that these codes are already disabled, so I change the title to remove them.;;;","13/Jan/23 23:10;syhily;[~mapohl] Can you help me review these three PR?

# https://github.com/apache/flink/pull/21670
# https://github.com/apache/flink/pull/21671
# https://github.com/apache/flink-connector-pulsar/pull/17;;;","16/Jan/23 13:55;mapohl;While investigating why a disabled test as mentioned in [my previous comment|https://issues.apache.org/jira/browse/FLINK-30657?focusedCommentId=17676524&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17676524] failed, I noticed that I overlooked the spelling of the test. Not the already disabled {{PulsarUnorderedPartitionSplitReaderTest}} is unstable but the corresponding {{PulsarOrderedPartitionSplitReaderTest}}. I created FLINK-30703 to document this.;;;","16/Jan/23 15:41;syhily;Thanks. I'll check that issue.;;;","19/Jan/23 09:19;martijnvisser;I've downgraded this ticket from a Blocker to Critical, given that I don't think this should block the Flink 1.16.1 release. ;;;","31/Jan/23 02:53;tison;flink-connector-pulsar 3.0 via https://github.com/apache/flink-connector-pulsar/pull/17

I'm not sure whether we should backport to 1.15 and 1.16.;;;","31/Jan/23 10:51;mapohl;Considering that the still open test instability is covered by FLINK-30703 and the other tests are already disabled in 1.15 and 1.16, backporting the removal might not be necessary.

I'm closing this issue

flink-connector-pulsar/v3.0: b708afd5f1b0b5fc41f2177b27a6aab0ac9136b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide more logs for schema compatibility check,FLINK-30656,13517888,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,masteryhx,masteryhx,masteryhx,12/Jan/23 16:01,19/Jan/24 01:37,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,API / Type Serialization System,,,,,0,,,,,"Currently, we have very few logs and exception info when checking schema compatibility.

It's difficult to see why the compatibility is not compatible, especially for some complicated nested serializers.

For example, for map serializer, when it's not compatible, we may only see below without other information:
{code:java}
Caused by: org.apache.flink.util.StateMigrationException: The new state serializer (org.apache.flink.api.common.typeutils.base.MapSerializer@e95e076a) must not be incompatible with the old state serializer (org.apache.flink.api.common.typeutils.base.MapSerializer@c33b100f). {code}
So I think we could add more infos when checking the compatibility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 02:15:59 UTC 2024,,,,,,,,,,"0|z1erog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 09:31;leonard;Thanks [~masteryhx]for opening this ticket, +1 to add key logs for  schema compatibility check.;;;","18/Jan/24 02:15;masteryhx;We should support to remain some messages for TypeSerializerSchemaCompatibility just like SchemaCompatibility in Avro.

Then every TypeSerializer could defined their own message about compatibility.

I have two proposals:

1. Add new method called TypeSerializerSchemaCompatibility#incompatible and #compatibleAfterMigration to support message, e.g. TypeSerializerSchemaCompatibility#incompatible(String message). And deprecated related old methods.
{code:java}
public static <T> TypeSerializerSchemaCompatibility<T> incompatible(String message) {
    return new TypeSerializerSchemaCompatibility<>(Type.INCOMPATIBLE, message, null);
} {code}
2. Add a new method called TypeSerializerSchemaCompatibility#withMessage:

 
{code:java}
private TypeSerializerSchemaCompatibility<T> withMessage(String message) {
    this.message = message;
    return this;
} {code}
Proposal 1 behaves just like SchemaCompatibility in Avro who forces caller to add message. But since TypeSerializerSchemaCompatibility is a PublicEvolving API, maybe we need a FLIP firstly?
Proposal 2 just add a new method so that we will not break change, but every callers (including some custom-defined TypeSerializers) should call it manually because it will not fail when compile.
[~leonard] [~pnowojski] [~Weijie Guo] WDYT?
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClientITCase.testMatchRecognize failed with a 404,FLINK-30655,13517881,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,mapohl,mapohl,12/Jan/23 15:25,25/May/23 07:35,04/Jun/24 20:41,25/May/23 07:35,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,test-stability,,,,"We experience a test instability with SqlClientITCase.testMatchRecognize failing due to a 404 error:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44755&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=15080]
{code:java}
Jan 12 14:51:42 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 208.308 s <<< FAILURE! - in SqlClientITCase
Jan 12 14:51:42 [ERROR] SqlClientITCase.testMatchRecognize  Time elapsed: 49.435 s  <<< ERROR!
Jan 12 14:51:42 com.github.dockerjava.api.exception.NotFoundException: 
Jan 12 14:51:42 Status 404: {""message"":""Could not find the file /flink/records-matchrecognize.out in container 403f2ed027372780e0eb2b3ea33f300b93d33eeb1ecfb72ff7b19ce28b73a0ac""}
Jan 12 14:51:42 
Jan 12 14:51:42 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:241) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30257,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-12 15:25:54.0,,,,,,,,,,"0|z1ermw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to force consumption from StartCursor every time the application starts,FLINK-30654,13517876,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,syhily,kangzai,kangzai,12/Jan/23 15:14,10/Feb/23 05:59,04/Jun/24 20:41,10/Feb/23 05:59,pulsar-4.0.0,,,,,,pulsar-4.0.0,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,Pulsar does not take effect when the subscription is set to start position consumption. It is recommended to add an option to be determined by the user or to add a lastAck strategy to adapt to the scenario of whether to start consumption from the last consumption submission position,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 02:20;kangzai;WechatIMG250.png;https://issues.apache.org/jira/secure/attachment/13054566/WechatIMG250.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 05:59:14 UTC 2023,,,,,,,,,,"0|z1erls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 05:59;tison;master via https://github.com/apache/flink-connector-pulsar/pull/23;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trigger resource Events on autoscaler actions,FLINK-30653,13517866,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,12/Jan/23 14:29,30/Jan/23 09:08,04/Jun/24 20:41,30/Jan/23 09:08,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,pull-request-available,,,,The autoscaler currently doesn't trigger any events. We should use Kube events to notify users about scaling actions on a per-vertex level or in cases where the autoscaler blocked some scaling action due to past history.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 09:08:01 UTC 2023,,,,,,,,,,"0|z1erjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 09:08;gyfora;merged to main 53d22d8a3e33f3971df43a29735471b7a8ccefaf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use max busytime instead of average to compute true processing rate,FLINK-30652,13517859,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,12/Jan/23 13:53,13/Jan/23 20:35,04/Jun/24 20:41,13/Jan/23 20:35,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,pull-request-available,,,,"Currently we use the some of busyTimes and processed records to estimate the true processing rate.

This computation however is not correct when any data skew is present as TPR is not fully additive. The first task to reach 100% utilization will set a limit to the pipeline processing rate through backpressure.

To avoid this we should use the max busyTime and compute the TPR from that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 20:35:25 UTC 2023,,,,,,,,,,"0|z1eri0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 20:35;gyfora;merged to main 92034fa912f39f5c8bd57632295c7ca85801f33a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move utility methods to CatalogTest and remove CatalogTestUtils class ,FLINK-30651,13517857,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,samrat007,samrat007,samrat007,12/Jan/23 13:50,01/May/23 16:24,04/Jun/24 20:41,21/Apr/23 13:20,,,,,,,1.18.0,,,,,,,,,,,,,,,,Table SQL / API,Tests,,,,0,pull-request-available,,,,"[CatalogTestUtils|https://github.com/apache/flink/blame/master/flink-table/flink-table-common/src/test/java/org/apache/flink/table/catalog/CatalogTestUtil.java#L43] class contains static utilities function. This functions/ methods can be moved to CatalogTest class and make code-flow easier to understand.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 13:54:05 UTC 2023,,,,,,,,,,"0|z1erhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 13:54;samrat007;i would like to work on this issue ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-280: Introduce EXPLAIN PLAN_ADVICE to provide SQL advice,FLINK-30650,13517856,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,12/Jan/23 13:47,10/Feb/23 12:48,04/Jun/24 20:41,03/Feb/23 13:03,1.17.0,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,This is the parent Jira task of FLIP-280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-12 13:47:33.0,,,,,,,,,,"0|z1erhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shutting down MiniCluster times out,FLINK-30649,13517838,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,12/Jan/23 12:40,04/Oct/23 09:59,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,Test Infrastructure,,,,0,stale-assigned,starter,test-stability,,"{{Run kubernetes session test (default input)}} failed with a timeout.
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=6317]

It appears that there was some issue with shutting down the pods of the MiniCluster:
{code:java}
2023-01-12T08:22:13.1388597Z timed out waiting for the condition on pods/flink-native-k8s-session-1-7dc9976688-gq788
2023-01-12T08:22:13.1390040Z timed out waiting for the condition on pods/flink-native-k8s-session-1-taskmanager-1-1 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 04 09:59:08 UTC 2023,,,,,,,,,,"0|z1erdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/23 11:45;mapohl;The test actually succeeds. The timeout really happens when the MiniCluster is shutdown:
{code}
[...]
Jan 12 08:21:10 Program execution finished
Jan 12 08:21:10 Job with JobID ab8a33b1f1d0c5c77de8bd8462c0c468 has finished.
Jan 12 08:21:10 Job Runtime: 16427 ms
Jan 12 08:21:10 
Jan 12 08:21:11 pass WordCount
Jan 12 08:21:11 Stopping job timeout watchdog (with pid=244982)
Jan 12 08:21:11 deployment.apps ""flink-native-k8s-session-1"" deleted
Jan 12 08:21:11 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
Jan 12 08:22:12 Stopping minikube ...
Jan 12 08:22:12 * Stopping node ""minikube""  ...
Jan 12 12:33:26 [FAIL] Test script contains errors.
{code}

I'm going to rename the title of this issue to clarify things.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Sep/23 14:56;flaviu.cicio;Hi, [~mapohl]

Can I have this one as my first ticket?;;;","20/Sep/23 21:14;mapohl;Thanks for volunteering. Do you have an idea on how to proceed here?;;;","21/Sep/23 14:06;flaviu.cicio;The issue is not reproducing anymore, thus we only have the logs

I can compare them with the ones that do not fail, maybe we find a clue about what happened

Other things that we can try:
 * Verify if it happened multiple times, check if there's a pattern
 * Access pod logs, but I'm not sure if it's possible;;;","04/Oct/23 09:59;mapohl;Yeah, essentially, it's hard to investigate if it's not reproducible. That's why I was curious whether you have an idea what's going on. It could be also a bug in Minikube which could be resolved by upgrading to a newer version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella]FLIP-282 Introduce Delete/Update API,FLINK-30648,13517829,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,12/Jan/23 12:07,07/Feb/23 04:35,04/Jun/24 20:41,07/Feb/23 04:35,,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"For more details, please refer to [FLIP-282|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=235838061]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-12 12:07:19.0,,,,,,,,,,"0|z1erbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add human readable datetime of watermark column in the web for better debug experience,FLINK-30647,13517827,13501095,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,yunta,yunta,12/Jan/23 11:43,13/Jan/23 03:03,04/Jun/24 20:41,13/Jan/23 03:03,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,1,pull-request-available,,,,"Currently, when we want to check the watermark of different tasks in the web ui, it reported as unix timestamp. However, this is not human readable and not easy to figure out the processing progress.

 !image-2023-01-12-19-39-59-585.png! 

We can introduce another column to parse the unix timestamp to offer better debug experience.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/23 11:40;yunta;image-2023-01-12-19-39-59-585.png;https://issues.apache.org/jira/secure/attachment/13054548/image-2023-01-12-19-39-59-585.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 03:03:35 UTC 2023,,,,,,,,,,"0|z1eraw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 03:03;yunta;merged in master: 28c3e1a3923ba560b559a216985c1abeb794ebaa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive catalog throws ClassNotFoundException when custom hive-site.xml is presented,FLINK-30646,13517803,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,12/Jan/23 10:35,13/Jan/23 02:15,04/Jun/24 20:41,13/Jan/23 02:15,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"For Hive 2.3.9, if a custom {{hive-site.xml}} is presented in {{$HIVE_HOME/conf}}, when creating Table Store Hive catalog in Flink, the following exception will be thrown.

{code}
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2273) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2367) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.loadFilterHooks(HiveMetaStoreClient.java:250) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:145) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_352]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_352]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_352]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_352]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:415) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:82) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:106) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:66) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:435) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1426) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1172) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16.0.jar:1.16.0]
	... 10 more
{code}

This is because {{hive-default.xml.template}} contains a property named {{hive.metastore.filter.hook}}. Its default value is {{org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl}}. However all Hive packages in Table Store are shaded, so the class loader cannot find the original class.

we need to remove relocation of Hive classes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 02:15:18 UTC 2023,,,,,,,,,,"0|z1er5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 02:15;TsReaper;master: 072640a72fe18bf3c0439cd4f0ec7602e0b0ff80
release-0.3: fcea22e114888af239e3a04e130f942df655122e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-286] The scope/stability annotation in AbstractStreamOperator are inconsistent.,FLINK-30645,13517781,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,becket_qin,becket_qin,12/Jan/23 09:11,12/Jan/23 09:12,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,,,,,"It looks that currently the {{{_}AbstractStreamOperator}}{_} API has some scope/stability annotation inconsistency. More specifically,
 * The {{AbstractStreamOperator}} class is marked as {{_@PublicEvolving_}}
 * {{AbstractStreamOperator.getInternalTimerService()}} returns a type of {{InternalTimerService}} __ which is marked as {{@Internal}}
 * {{InternalOperatorMetricGroup}} and {{InternalIOperatorIOMetricGroup}} __ are also available to the subclasses of {{AbstractStreamOperator}} but marked as {{{}@Internal{}}}.

FLIP-286 proposes to fix the above annotation inconsistency by marking the following classes \{{@PublicEvolving}}.
 * org.apache.flink.streaming.api.operators.InternalTimerService
 * org.apache.flink.runtime.metrics.groups.InternalOperatorMetricGroup
 * org.apache.flink.runtime.metrics.groups.InternalOperatorIOMetricGroup

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-12 09:11:20.0,,,,,,,,,,"0|z1er0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogCompatibilityITCase.testRestore fails due to CheckpointCoordinator being shutdown,FLINK-30644,13517777,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Jan/23 08:55,13/May/24 13:59,04/Jun/24 20:41,,1.17.0,1.19.1,,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,Runtime / State Backends,,,,0,auto-deprioritized-critical,test-stability,,,"We observe a build failure in {{ChangelogCompatibilityITCase.testRestore}} due to the {{CheckpointCoordinator}} being shut down:

{code:java}
[...]
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: CheckpointCoordinator shutdown.
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:544)
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2140)
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2127)
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoints(CheckpointCoordinator.java:2004)
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoints(CheckpointCoordinator.java:1987)
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingAndQueuedCheckpoints(CheckpointCoordinator.java:2183)
Jan 12 02:37:37 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.shutdown(CheckpointCoordinator.java:426)
Jan 12 02:37:37 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.onTerminalState(DefaultExecutionGraph.java:1329)
[...]{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44731&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9255",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 13 13:59:28 UTC 2024,,,,,,,,,,"0|z1eqzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 09:00;martijnvisser;In the logs the following stacktrace is available

{code:java}
02:37:15,039 [                main] WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] - Failed to initialize delegation token provider hadoopfs
java.lang.NoClassDefFoundError: org/apache/hadoop/hdfs/HdfsConfiguration
	at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.init(HadoopFSDelegationTokenProvider.java:72) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:106) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:91) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:48) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:431) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:237) ~[flink-runtime-1.17-SNAPSHOT-tests.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:109) ~[flink-runtime-1.17-SNAPSHOT-tests.jar:1.17-SNAPSHOT]
	at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:64) ~[flink-test-utils-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.test.state.ChangelogCompatibilityITCase.before(ChangelogCompatibilityITCase.java:304) ~[test-classes/:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
	at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
	at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.Suite.runChild(Suite.java:128) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.Suite.runChild(Suite.java:27) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.rules.RunRules.evaluate(RunRules.java:20) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115) ~[junit-4.13.2.jar:4.13.2]
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42) ~[junit-vintage-engine-5.9.1.jar:5.9.1]
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80) ~[junit-vintage-engine-5.9.1.jar:5.9.1]
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72) ~[junit-vintage-engine-5.9.1.jar:5.9.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147) ~[junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127) ~[junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90) ~[junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55) ~[junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102) [junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54) [junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) [junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) [junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) [junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53) [junit-platform-launcher-1.9.1.jar:1.9.1]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188) [surefire-junit-platform-3.0.0-M5.jar:3.0.0-M5]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154) [surefire-junit-platform-3.0.0-M5.jar:3.0.0-M5]
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124) [surefire-junit-platform-3.0.0-M5.jar:3.0.0-M5]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428) [surefire-booter-3.0.0-M5.jar:3.0.0-M5]
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162) [surefire-booter-3.0.0-M5.jar:3.0.0-M5]
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562) [surefire-booter-3.0.0-M5.jar:3.0.0-M5]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548) [surefire-booter-3.0.0-M5.jar:3.0.0-M5]
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdfs.HdfsConfiguration
	at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581) ~[?:?]
	at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) ~[?:?]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:522) ~[?:?]
	... 63 more
{code};;;","12/Jan/23 09:01;martijnvisser;The above comment is actually related to https://issues.apache.org/jira/browse/FLINK-29622?focusedCommentId=17654849&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17654849 and not to this test instability;;;","21/Apr/23 07:12;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48321&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=10691;;;","26/Apr/23 03:48;Yanfei Lei;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48321&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=10691] stack trace shows that the reason is fileNotFound:
{code:java}
Apr 21 01:35:03 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
Apr 21 01:35:03 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
Apr 21 01:35:03 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1606)
Apr 21 01:35:03 	... 3 more
Apr 21 01:35:03 Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:/tmp/junit862341719583315537/junit8040524335885911429/e0cfadc575a94b10511f5ef02629fb30/chk-1' on file system 'file'.
Apr 21 01:35:03 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
Apr 21 01:35:03 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:114)
Apr 21 01:35:03 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
Apr 21 01:35:03 	... 3 more
Apr 21 01:35:03 Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:/tmp/junit862341719583315537/junit8040524335885911429/e0cfadc575a94b10511f5ef02629fb30/chk-1' on file system 'file'.
Apr 21 01:35:03 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:275)
Apr 21 01:35:03 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136)
Apr 21 01:35:03 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1824)
Apr 21 01:35:03 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:223)
Apr 21 01:35:03 	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:198)
Apr 21 01:35:03 	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365)
Apr 21 01:35:03 	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:210)
Apr 21 01:35:03 	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:136)
Apr 21 01:35:03 	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
Apr 21 01:35:03 	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
Apr 21 01:35:03 	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)
Apr 21 01:35:03 	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:348)
Apr 21 01:35:03 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
Apr 21 01:35:03 	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
Apr 21 01:35:03 	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
Apr 21 01:35:03 	... 4 more
{code}
 

 ;;;","30/May/23 07:56;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49454&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8235;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 09:27;mapohl;It might be fixed by another issue considering that it didn't reappear for a while;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","30/Oct/23 07:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54167&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=10662;;;","24/Apr/24 07:49;Weijie Guo;1.19 test_cron_jdk11 tests:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59106&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","03/May/24 15:54;rskraba;* 1.20 Java 11 / Test (module: tests) https://github.com/apache/flink/actions/runs/8856547891/job/24323134209#step:10:7762;;;","13/May/24 13:59;rskraba;* 1.19 test_cron_jdk17 tests https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59477&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8264
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_check fails,FLINK-30643,13517776,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,12/Jan/23 08:52,30/Mar/23 10:46,04/Jun/24 20:41,12/Jan/23 08:53,1.17.0,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,test-stability,,,"We experience failures in the documentation checks:
{code:java}
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:43:22"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:44:34"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:45:35"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 00:16:38 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:82:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:42:22"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:43:34"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:44:35"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 00:16:45 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:82:20"": page not found
Total in 13590 ms
Error: Error building site: logged 28 error(s)
Error building the docs
 {code}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44731&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=108",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 09:01:03 UTC 2023,,,,,,,,,,"0|z1eqzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 09:01;chalixar;Hello This is due to migration of AWS connectors
It is tracked underFLINK-30641;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsvFileCompactionITCase>CompactionITCaseBase.testPartition timed out,FLINK-30642,13517774,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,martijnvisser,martijnvisser,12/Jan/23 08:47,19/Aug/23 22:35,04/Jun/24 20:41,,1.15.4,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,auto-deprioritized-major,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44732&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=ed165f3f-d0f6-524b-5279-86f8ee7d0e2d&l=12107

{code:java}
Jan 12 01:43:49 [ERROR] org.apache.flink.formats.csv.CsvFileCompactionITCase.testPartition  Time elapsed: 90.019 s  <<< ERROR!
Jan 12 01:43:49 org.junit.runners.model.TestTimedOutException: test timed out after 90 seconds
Jan 12 01:43:49 	at sun.misc.Unsafe.park(Native Method)
Jan 12 01:43:49 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 12 01:43:49 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 12 01:43:49 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Jan 12 01:43:49 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 12 01:43:49 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 12 01:43:49 	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
Jan 12 01:43:49 	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
Jan 12 01:43:49 	at org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase.testPartition(CompactionITCaseBase.java:124)
Jan 12 01:43:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 12 01:43:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 12 01:43:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 12 01:43:49 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 12 01:43:49 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jan 12 01:43:49 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jan 12 01:43:49 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jan 12 01:43:49 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jan 12 01:43:49 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jan 12 01:43:49 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jan 12 01:43:49 	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
Jan 12 01:43:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Jan 12 01:43:49 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
Jan 12 01:43:49 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
Jan 12 01:43:49 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 12 01:43:49 	at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:02 UTC 2023,,,,,,,,,,"0|z1eqz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_404_check fail,FLINK-30641,13517757,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dannycranmer,wanglijie,wanglijie,12/Jan/23 07:12,12/Jan/23 10:38,04/Jun/24 20:41,12/Jan/23 10:38,,,,,,,,,,,,,,,,,,,,,,,Build System,,,,,0,,,,,"{code:java}
Cloning into 'flink-connector-rabbitmq'...
Note: switching to '325b6ba8d866bb02204736229aa54ade304119a3'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

Start building sites … 
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:43:22"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:44:34"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:45:35"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:82:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:42:22"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:43:34"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:44:35"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:82:20"": page not found
Total in 16618 ms
Error: Error building site: logged 28 error(s)
Error building the docs
##[error]Bash exited with code '1'. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30643,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 08:03:19 UTC 2023,,,,,,,,,,"0|z1eqvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 07:16;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44745&view=logs&j=c5d67f7d-375d-5407-4743-f9d0c4436a81&t=38411795-40c9-51fa-10b0-bd083cf9f5a5;;;","12/Jan/23 07:17;martijnvisser;[~danny.cranmer] It looks like this is caused by the AWS repo, based on https://github.com/apache/flink/actions/runs/3897787417/jobs/6655818209#step:5:165 - Can you have a look?

[~wanglijie] Thanks for raising the ticket, I've marked it as a blocker since it blocks all documentation builds for master;;;","12/Jan/23 07:53;dannycranmer;Yes will fix asap;;;","12/Jan/23 08:03;dannycranmer;The issue is (was) that I had deleted the {{v4}} branch in favour of {{main}} in {{flink-connector-aws}} to align with other connector repos. The Flink docs build points to this branch. Have restored the branch and kicked a build to resolve this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test in CliClientITCase,FLINK-30640,13517755,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,yzl,yzl,12/Jan/23 06:44,11/Mar/23 05:08,04/Jun/24 20:41,11/Mar/23 05:08,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Hive,Table SQL / Client,,,,0,pull-request-available,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]

 

The failed test can work normally in my local environment.",,,,,,,,,,,,,,,,,,,FLINK-30508,,,,,,,,,FLINK-30508,,,,,,,,,,,,,,,"10/Mar/23 09:49;qingyue;image-2023-03-10-17-49-26-971.png;https://issues.apache.org/jira/secure/attachment/13056236/image-2023-03-10-17-49-26-971.png","10/Mar/23 09:51;qingyue;image-2023-03-10-17-51-42-192.png;https://issues.apache.org/jira/secure/attachment/13056237/image-2023-03-10-17-51-42-192.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 05:08:26 UTC 2023,,,,,,,,,,"0|z1equw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 06:51;luoyuxia;It happens 
{code:java}
CREATE TABLE foo as select 1; 

SELECT * from foo; 

// error message
java.lang.RuntimeException: Error while running command to get file permissions : ExitCodeException exitCode=2: ls: cannot access '/tmp/junit2938020000869140718/hive_warehouse/foo/.part-ac6e74fd-9996-4d9e-bdb4-c7f4028d27ca-0-0.inprogress.6437ce4c-f979-4770-8a7a-fe6a48c6aa51': No such file or directory{code}
Seems related to CTAS, may [~tartarus] have a look?;;;","13/Jan/23 08:00;tartarus;It looks like ctas didn't execute successfully.

Will retrying with run azure work?;;;","15/Jan/23 12:58;yzl;Re-run is OK. Seems it is just not stable.;;;","16/Jan/23 07:26;mapohl;I'm closing this issue in favor of FLINK-30508.;;;","31/Jan/23 10:27;fsk119;Hi. [~mapohl] There are two different problems. ;;;","31/Jan/23 10:28;fsk119;I meet the problem again:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45471&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

 ;;;","31/Jan/23 14:08;mapohl;Thanks for clarification. How can we differentiate the Jira issues? The test failure in the build reported in this Jira's description shows {{CliClientITCase.testSqlStatements}} similar to what's reported for FLINK-30508?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15651;;;","31/Jan/23 16:43;mapohl;On another note: Are we considering this a blocker for 1.17?;;;","03/Feb/23 10:31;mapohl;[~tartarus] is this considered a testing issue or indicates a more fundamental problem in the production code?;;;","09/Feb/23 02:33;fsk119;Merged into master: 550697af3df222df906aae108e409c197c178876 

Merged into release-1.17: 7cc2b4f35ea54c260365cdbc3e1ba92a06c5b112;;;","09/Feb/23 06:24;mapohl;The following build doesn't include the aforementioned fix, yet:
1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45909&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=42892;;;","10/Mar/23 07:26;mapohl;We have this issue happening again on 1.17:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47010&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=16287;;;","10/Mar/23 07:32;mapohl;I'm increasing the priority of this issue to Blocker until it's clear whether it's a only test code issue or more severe
[~fsk119] [~tartarus] [~lsy] may you have a look at it?;;;","10/Mar/23 09:51;qingyue;I compared the `table.q` file between ""expected"" v.s. ""but was"", and found that the only difference is that the table column lengths are different from L#1358~L#1360 and L#1424.

cc [~fsk119] Do you have any clue about this?

!image-2023-03-10-17-51-42-192.png|width=2634,height=613!

 

!image-2023-03-10-17-49-26-971.png|width=1733,height=214!;;;","11/Mar/23 05:02;fsk119;SQL Client infers the column width from the input size and print the border for the content. If the content are almost same but the border size is different, I think the content may contains some magic char inside. You can take a look at the [line|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/print/TableauStyle.java#L338]. I think it just a test issue, we can decrease the priority for this now.  ;;;","11/Mar/23 05:08;fsk119;Because the current problem is much different with the origin problem, I open a new ticket FLINK-31403 for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish SBOM for externalized connectors,FLINK-30639,13517645,13516733,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zch93,mbalassi,mbalassi,11/Jan/23 15:23,15/Mar/23 15:23,04/Jun/24 20:41,15/Mar/23 15:23,,,,,,,connector-parent-1.0.0,,,,,,,,,,,,,,,,,,,,,0,,,,,"Probably adding to the connector parent will be sufficient.
https://github.com/apache/flink-connector-aws/blob/main/pom.xml#L27
https://github.com/zentol/flink-connector-parent/blob/master/pom.xml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 18 12:31:27 UTC 2023,,,,,,,,,,"0|z1eq7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 10:00;zch93;Opened draft PR against flink-connector-aws repo: [https://github.com/apache/flink-connector-aws/pull/46];;;","18/Jan/23 12:31;chesnay;connector-parent: aa659f7694d2e50dfe690031d2c3d4e436830176;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator doesn't forward allowNonRestoredState flag,FLINK-30638,13517629,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,gsomogyi,asardaes,asardaes,11/Jan/23 14:03,25/Mar/23 21:47,04/Jun/24 20:41,,kubernetes-operator-1.3.1,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"My Flink image is currently based on 1.15, and I'm using last-state upgrade mode, so I have HA enabled. I have set {{allowNonRestoredState}} as true.

After an upgrade where I removed some transformations from the pipeline, I see the following exception in the deployed Flink cluster:

Job 38585f46d840b3c2e9f843e230398d35 reached terminal state FAILED.
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
...
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: There is no operator for the state d3bab17db5b56f81eb48b3e7d5cce15d

I'm attaching some of the operator's logs, but I don't see anything strange there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/23 14:03;asardaes;flink-kubernetes-operator_json.txt;https://issues.apache.org/jira/secure/attachment/13054527/flink-kubernetes-operator_json.txt",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 16:07:42 UTC 2023,,,,,,,,,,"0|z1eq40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 14:11;gyfora;I think the problem is that last-state mode uses simple checkpoint recovery which ignores the flag. I think this will work if you switch to savepoint.;;;","11/Jan/23 15:09;asardaes;When you say the flag is ignored, do you mean by Flink itself?;;;","11/Jan/23 15:13;gyfora;Yes, we set the right configuration but allowNonRestoredState config only affects the initial startup of the job when you start from savepoint. 
When using last-state, we use a different mechanism that imitates a simple fail over scenario;;;","11/Jan/23 15:36;asardaes;I see. Do you know if this would also work if I leave last-state mode?

# Delete the Kubernetes resource of the Flink Deployment
# Edit the source YAML to include the _checkpoint_ as {{initialSavepointPath}}
# Recreate the resource based on the updated YAML;;;","11/Jan/23 15:43;gyfora;Yes, you could do that and that will work. Or you can switch to savepoint upgrade mode when making incompatible changes.;;;","11/Jan/23 16:07;asardaes;All right, thanks. I understood this is not really specific to the Kubernetes operator, but it might be worth updating its documentation to point out that the flag is irrelevant for last-state upgrade mode.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In linux-aarch64 environment, using “is” judgment to match the window type of overwindow have returned incorrect matching results",FLINK-30637,13517615,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xinchen147,xinchen147,xinchen147,11/Jan/23 12:55,29/Jan/23 01:54,04/Jun/24 20:41,17/Jan/23 07:13,1.13.6,,,,,,1.15.4,1.16.1,1.17.0,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"In  linux-arch64 environment, “window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING” in  in the PandasBatchOverWindowAggregateFunctionOperation class of the pyflink source code has returned the wrong result.

For example, when window_type is 6, it represents the window type of ‘ROW_UNBOUNDED_FOLLOWING’, but “window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING” return false because the memory address of window_type has changed. It will lead to the wrong type of window, such as row sliding window, so, the wrong input data of python udf have been assembled and wrong results of that have appeared.

 

Specifically, the pyflink unit testcase is ‘test_over_window_aggregate_function’ in ‘pyflink\table\tests\test_pandas_udaf.py’. It performance incorrectly when I execute it by pytest on linux-aarch64 system. I cut this unit use case to the following code and executed it in the flink standalone mode of aarch64 system, and got the same error result:

 
{code:java}
import unittest
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings
from pyflink.table.udf import udaf, AggregateFunction


class MaxAdd(AggregateFunction, unittest.TestCase):

    def open(self, function_context):
        mg = function_context.get_metric_group()
        self.counter = mg.add_group(""key"", ""value"").counter(""my_counter"")
        self.counter_sum = 0

    def get_value(self, accumulator):
        # counter
        self.counter.inc(10)
        self.counter_sum += 10
        return accumulator[0]

    def create_accumulator(self):
        return []

    def accumulate(self, accumulator, *args):
        result = 0
        for arg in args:
            result += arg.max()
        accumulator.append(result)


@udaf(result_type=DataTypes.FLOAT(), func_type=""pandas"")
def mean_udaf(v):
    import logging
    logging.error(""debug"")
    logging.error(v)
    return v.mean()


t_env = TableEnvironment.create(
    EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())
t_env.get_config().get_configuration().set_string(""parallelism.default"", ""2"")
t_env.get_config().get_configuration().set_string(
    ""python.fn-execution.bundle.size"", ""1"")

import datetime

t = t_env.from_elements(
    [
        (1, 2, 3, datetime.datetime(2018, 3, 11, 3, 10, 0, 0)),
        (1, 3, 1, datetime.datetime(2018, 3, 11, 3, 10, 0, 0)),
        (1, 8, 5, datetime.datetime(2018, 3, 11, 4, 20, 0, 0))
    ],
    DataTypes.ROW(
        [DataTypes.FIELD(""a"", DataTypes.TINYINT()),
         DataTypes.FIELD(""b"", DataTypes.SMALLINT()),
         DataTypes.FIELD(""c"", DataTypes.INT()),
         DataTypes.FIELD(""rowtime"", DataTypes.TIMESTAMP(3))]))

# sink
t_env.execute_sql(""""""
        CREATE TABLE mySink (
          c INT,
          d FLOAT 
        ) WITH (
          'connector' = 'print'
        )
    """""")

t_env.create_temporary_system_function(""mean_udaf"", mean_udaf)
t_env.register_function(""max_add"", udaf(MaxAdd(),
                                        result_type=DataTypes.INT(),
                                        func_type=""pandas""))
t_env.register_table(""T"", t)
t_env.execute_sql(""""""
        insert into mySink
        select
         max_add(b, c)
         over (PARTITION BY a ORDER BY rowtime
         ROWS BETWEEN UNBOUNDED preceding AND 0 FOLLOWING),
         mean_udaf(b)
         over (PARTITION BY a ORDER BY rowtime
         ROWS BETWEEN 1 PRECEDING AND UNBOUNDED FOLLOWING)
        from T
    """""").wait()
'''
assert_equals(actual,
              [""5,4.3333335"",
               ""13,5.5"",
               ""6,4.3333335""])
'''{code}
The expected results are [""5,4.3333335"", ""13,5.5"", ""6,4.3333335""], but actual results are List(5,2.0, 13,5.5, 4,2.5). For ‘mean_udaf’ and ‘OverWindow.UNBOUNDED FOLLOWING’ in the code, by adding the error log, I found that when window_type is 6 and 'OverWindow.ROW_UNBOUNDED_FOLLOWING' also represents 6, the following code from pyflink source code returned false.
{code:java}
// pyflink\fn_execution\operations.py (line 273)
elif window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING:
    # row unbounded following window
    window_start = window.lower_boundary
    for j in range(input_cnt):
        start = max(j + window_start, 0)
        series_slices = [s.iloc[start: input_cnt] for s in input_series]
        result.append(func(series_slices)){code}
And it It finally chose row sliding window to assemble input data of mean_udaf:
{code:java}
// pyflink\fn_execution\operations.py (line 280) 
else:
    # row sliding window
    window_start = window.lower_boundary
    window_end = window.upper_boundary
    for j in range(input_cnt):
        start = max(j + window_start, 0)
        end = min(j + window_end + 1, input_cnt)
        series_slices = [s.iloc[start: end] for s in input_series]
        result.append(func(series_slices)){code}
Obviously, that's not right. The right choice will be made in x86 environment.

The reason is window_ type‘s memory address  is different from ‘OverWindow.ROW_ UNBOUNDED_ FOLLOWING’ in linux-aarch64 environment. On the contrary, they are the same in the linux-x86 environment. The reason why the memory address is different is unknown yet. But I observed that window_type comes from 'serialized_fn.windows':
{code:java}
def __init__(self, spec): 
super(PandasBatchOverWindowAggregateFunctionOperation, self).__init__(spec) 
self.windows = [window for window in self.spec.serialized_fn.windows]
{code}
Perhaps grpc, protobuf dependencies or serialization operations in the arrch environment have affected the memory address of the int variables, I'll explore the underlying reasons later.

 

Solution and suggestion:

Since the window selections need to compare the values of two integer variables(window_type, OverWindow.ROW_ UNBOUNDED_ FOLLOWING), I recommend replacing ‘is’ with ‘==’ at the window type matching. That can also prevents erroneous results caused by python small integer object pool failure which may also affects the memory address. And this modification has been verified to perform correctly on both x86 and aarch64 environments, either this unit test case or the case I cut.

 

 

 

 

 ","Linux version 5.10.0-60.18.0.50.oe2203.aarch64

(abuild@obs-worker-002) (gcc_old (GCC) 10.3.1, GNU ld (GNU Binutils) 2.37) #1 SMP Wed Mar 30 02:43:08 UTC 2022

 

pyflink-version：1.13.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Tue Jan 17 07:13:29 UTC 2023,,,,,,,,,,"0|z1eq0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 13:16;martijnvisser;[~xinchen147] Can you please verify this in Flink 1.16, since Flink 1.13 is no longer supported in the community?;;;","11/Jan/23 13:43;xinchen147;[~martijnvisser] Thank you for your reply. Maybe I can have a try in Flink 1.16, but the same part of Pyflink‘s source code has not changed yet in Flink 1.16. And I'll provide a detailed description of the problem and suggested solutions to this section later. [~hxbks2ks] also knows the problem because I have consulted him earlier.;;;","11/Jan/23 14:07;martijnvisser;[~xinchen147] I can imagine, but there have been a number of dependency updates in later versions which is why I thought it could have a different outcome on a later version;;;","12/Jan/23 12:52;xinchen147;Why the window_type's memory address from serialized_fn.windows is diffrent from ‘OverWindow.ROW_ UNBOUNDED_ FOLLOWING’?

I learned that python uses the small integer object pool to control. When a represents 5 and b also represents 5, they are the same object, ‘a is b’ will return ‘True‘. But this is not the case here. And I do experiments in linux-aarch64 system as follows:
{code:java}
a = 5 
logging.error(a is window_type)  # Window_type represents 5, but the result is False.
logging.error(a is OverWindow.ROW_UNBOUNDED_FOLLOWING) # OverWindow.ROW_UNBOUNDED_FOLLOWING represents 5, the result is True

##### test for small integer object pool
b = 5
c = 5 
logging.error(b is c) # The result is True.{code}
It seems that the python small integer pool is not invalid, so I think it may be necessary to trace the source of window_type. I hope someone can give some suggestions and ideas. Thank you very much.

 

 ;;;","17/Jan/23 07:13;hxbks2ks;Merged into master via d053867fb5c0fc647ea9266aab35598d7f3fc5c4
Merged into release-1.16 via eca940c5bf9e17c90dbb6f35e4ba370027137368
Merged into release-1.15 via 4035d61a2756ec16046fb687f533be0501fbbd35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Typo fix: ""to to"" -> ""to""",FLINK-30636,13517590,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gunnar.morling,gunnar.morling,gunnar.morling,11/Jan/23 10:21,03/Aug/23 10:36,04/Jun/24 20:41,03/Aug/23 10:36,,,,,,,1.18.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"There's a surprising number of occurrences of ""to to"" in JavaDoc and the like, where it actually should just be ""to"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 10:35:39 UTC 2023,,,,,,,,,,"0|z1epvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 10:21;gunnar.morling;Will send a PR for this shortly.;;;","03/Aug/23 10:35;rmetzger;Merged to master in https://github.com/apache/flink/commit/b78cd3f536331d02eff9af4702904f331d90bc07;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to recover for row-format when writing to OSS using FileSystemTable connector,FLINK-30635,13517578,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,11/Jan/23 09:36,05/Jun/23 12:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,FileSystems,,,,0,,,,,"When recover for row-format file such as csv/json, it will try to copy oss to local which depends on LocalFileSystem, but the LocalFileSystem can't be loaded by the classloader if the related LocalFileSystem haven't been included in FLINK_HOME/lib.

!image-2023-01-11-17-42-15-964.png|width=790,height=246!  

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32241,,,,,,"11/Jan/23 09:42;luoyuxia;image-2023-01-11-17-42-15-964.png;https://issues.apache.org/jira/secure/attachment/13054518/image-2023-01-11-17-42-15-964.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 12:36:21 UTC 2023,,,,,,,,,,"0|z1epso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 08:33;masteryhx;Hi, [~luoyuxia] , LocalFileSystem of hadoop should exist if we have hadoop-common.
Could you verify whether it works if setting _fs.file.impl_ to _org.apache.hadoop.fs.LocalFileSystem.class.getName()_ explicitly ? (It may be a shaded path);;;","05/Jun/23 12:24;luoyuxia;[~masteryhx] Thanks for advice. I think set fs.file.impl explicitly may well solve my problem. But IIUC, we have no way to set fs.file.impl with Flink since it's a hadoop  conf and our flink config can't recognize it and then set it to hadoop conf. 

I think our `OSSFileSystemFactory` should provide a way to set  congurations to hadoop conf not only the congurations with prefiex  `fs.oss.` 

 ;;;","05/Jun/23 12:36;luoyuxia;After some debugging, I think the finnal reason for my case is that I use the shaded hadoop.

The hadoop libs do exist in FLINK_HOME/lib, if not specified  `fs.file.impl`, it'll try to load the filesytem using SPI. The class name to be discovered is `org.apache.hadoop.fs.LocalFileSystem` as as specified in META-INF/services/org.apache.hadoop.fs.FileSystem.

But the class has been relocated to `org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.fs.LocalFileSystem`, so it can't find the class `org.apache.hadoop.fs.LocalFileSystem`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTableSourceITCase.setupSourceDatabaseAndData fails with InvalidObjectException,FLINK-30634,13517577,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,11/Jan/23 09:35,27/Feb/23 02:17,04/Jun/24 20:41,27/Feb/23 02:17,1.16.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,test-stability,,,,"We have multiple failures in HiveTableSourceITCase

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44692&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=27933",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-11 09:35:31.0,,,,,,,,,,"0|z1epsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update AWS SDKv2 to v2.19.14,FLINK-30633,13517573,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,11/Jan/23 09:10,16/Jan/23 08:58,04/Jun/24 20:41,16/Jan/23 08:58,,,,,,,1.15.4,1.16.1,1.17.0,aws-connector-3.1.0,aws-connector-4.1.0,,,,,,,,,,,,Connectors / AWS,Connectors / DynamoDB,Connectors / Firehose,Connectors / Kinesis,,0,pull-request-available,,,,"Update AWS SDK to latest version since it fixes a bug:

- https://issues.apache.org/jira/browse/FLINK-30304",,,,,,,,,,,,,,,,,,,FLINK-30304,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 14:14:29 UTC 2023,,,,,,,,,,"0|z1eprk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 14:14;dannycranmer;- Merged commit [{{eed230f}}|https://github.com/apache/flink-connector-aws/commit/eed230f65c4543395c3bdf5ba5b69b65159e9ff5] into flink-connector-aws v3.0
 - Merged commit [{{2a24e7c}}|https://github.com/apache/flink-connector-aws/commit/2a24e7c1e8d2a8f613838fd0419177d4f96991bf] into flink-connector-aws main
 - Merged commit [{{a32bf0f}}|https://github.com/apache/flink/commit/a32bf0faf0eecf8f84ad931c18624c60a0c67953] into flink master (1.17)
 - Merged commit [{{4acfbee}}|https://github.com/apache/flink/commit/4acfbee2175e68deb43fbb5a2cc001e91b3b14f9] into flink release-1.16 
 - Merged commit [{{c4b0c1e}}|https://github.com/apache/flink/commit/c4b0c1e61d503c4bb6c2fae1247162a0306b5a1e] into flink release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce DataType for table store,FLINK-30632,13517570,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Jan/23 08:54,12/Jan/23 14:58,04/Jun/24 20:41,12/Jan/23 14:58,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Introduce table store own DataType to decouple Flink SQL LogicalType.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 14:58:18 UTC 2023,,,,,,,,,,"0|z1epqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 14:58;lzljs3620320;master: f416e00eb50d9a000265a824f733da69ccc24e77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the max number of subpartitons consumed by each downstream task,FLINK-30631,13517568,13486602,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,11/Jan/23 08:46,16/Jan/23 09:17,04/Jun/24 20:41,16/Jan/23 09:17,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In the current implementation(FLINK-25035), when the upstream vertex parallelism is much greater than the downstream vertex parallelism, it may lead to a large number of channels in the downstream tasks(for example, A -> B, all to all edge, max parallelism is 1000. If parallelism of A is 1000, parallelism of B is decided to be 1, then the only subtask of B will consume 1000 * 1000 subpartitions), resulting in a large overhead for processing channels.

In this ticket, we temporarily address this issue by limiting the max number of subpartitons consumed by each downstream task. The ultimate solution should be to support single channel consume multiple subpartitons.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 09:17:26 UTC 2023,,,,,,,,,,"0|z1epqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 09:17;wanglijie;Done via master c0c7dc47905fbe80123ab0cb1ff55129acec4dab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python: BatchModeDataStreamTests.test_keyed_map failed,FLINK-30630,13517567,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,11/Jan/23 08:43,28/Aug/23 22:35,04/Jun/24 20:41,,1.15.3,,,,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,auto-deprioritized-critical,test-stability,,,"{{BatchModeDataStreamTests.test_keyed_map}} failed in 1.15:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44691&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295",,,,,,,,,,,,,,,,,,,,,,,,FLINK-31355,FLINK-30913,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 22:35:13 UTC 2023,,,,,,,,,,"0|z1epq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 08:45;mapohl;[~dianfu] [~hxbks2ks];;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/23 08:58;mapohl;Looks like that issue is caused by the Java side not sending the expected responses to Python:
{code}
2023-01-11T04:51:52.1315205Z Jan 11 04:51:52 ERROR    py4j.java_gateway:java_gateway.py:1143 An error occurred while trying to connect to the Java server (127.0.0.1:45019)
2023-01-11T04:51:52.1315757Z Jan 11 04:51:52 Traceback (most recent call last):
2023-01-11T04:51:52.1316341Z Jan 11 04:51:52   File ""/__w/1/s/flink-python/pyflink/util/exceptions.py"", line 146, in deco
2023-01-11T04:51:52.1316730Z Jan 11 04:51:52     return f(*a, **kw)
2023-01-11T04:51:52.1317400Z Jan 11 04:51:52   File ""/__w/1/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value
2023-01-11T04:51:52.1317878Z Jan 11 04:51:52     raise Py4JJavaError(
2023-01-11T04:51:52.1318254Z Jan 11 04:51:52 py4j.protocol.Py4JJavaError: <exception str() failed>
2023-01-11T04:51:52.1318591Z Jan 11 04:51:52 
2023-01-11T04:51:52.1318939Z Jan 11 04:51:52 During handling of the above exception, another exception occurred:
2023-01-11T04:51:52.1319290Z Jan 11 04:51:52 
2023-01-11T04:51:52.1319598Z Jan 11 04:51:52 Traceback (most recent call last):
2023-01-11T04:51:52.1320289Z Jan 11 04:51:52   File ""/__w/1/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1224, in send_command
2023-01-11T04:51:52.1320815Z Jan 11 04:51:52     raise Py4JNetworkError(""Answer from Java side is empty"")
2023-01-11T04:51:52.1321252Z Jan 11 04:51:52 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2023-01-11T04:51:52.1321590Z Jan 11 04:51:52 
2023-01-11T04:51:52.1321941Z Jan 11 04:51:52 During handling of the above exception, another exception occurred:
2023-01-11T04:51:52.1322646Z Jan 11 04:51:52 
2023-01-11T04:51:52.1322961Z Jan 11 04:51:52 Traceback (most recent call last):
2023-01-11T04:51:52.1323669Z Jan 11 04:51:52   File ""/__w/1/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1038, in send_command
2023-01-11T04:51:52.1324172Z Jan 11 04:51:52     response = connection.send_command(command)
2023-01-11T04:51:52.1324872Z Jan 11 04:51:52   File ""/__w/1/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1228, in send_command
2023-01-11T04:51:52.1325364Z Jan 11 04:51:52     raise Py4JNetworkError(
2023-01-11T04:51:52.1325746Z Jan 11 04:51:52 py4j.protocol.Py4JNetworkError: Error while receiving
2023-01-11T04:51:52.1326088Z Jan 11 04:51:52 
2023-01-11T04:51:52.1326441Z Jan 11 04:51:52 During handling of the above exception, another exception occurred:
2023-01-11T04:51:52.1326783Z Jan 11 04:51:52 
2023-01-11T04:51:52.1327089Z Jan 11 04:51:52 Traceback (most recent call last):
2023-01-11T04:51:52.1327777Z Jan 11 04:51:52   File ""/__w/1/s/flink-python/.tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1224, in send_command
2023-01-11T04:51:52.1328307Z Jan 11 04:51:52     raise Py4JNetworkError(""Answer from Java side is empty"")
2023-01-11T04:51:52.1328746Z Jan 11 04:51:52 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
{code}

I'm not an expert in this field. But we could consider closing this issue if the problem didn't appear again?;;;","21/Aug/23 11:05;mapohl;I closed FLINK-31355 and FLINK-30913 as duplicates of this one because they seem to have the same cause (i.e. {{Answer from Java side is empty}}) even though different tests failed ({{CommonDataStreamTests.test_execute_and_collect}}, {{PyFlinkStreamUserDefinedFunctionTests.test_chaining_scalar_function}}).;;;","28/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat is unstable,FLINK-30629,13517561,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,Jiangang,xtsong,xtsong,11/Jan/23 08:33,11/Mar/24 13:18,04/Jun/24 20:41,,1.17.0,1.18.0,,,,,1.20.0,,,,,,,,,,,,,,,,Client / Job Submission,,,,,0,pull-request-available,stale-assigned,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10819

{code:java}
Jan 11 04:32:39 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.02 s <<< FAILURE! - in org.apache.flink.client.ClientHeartbeatTest
Jan 11 04:32:39 [ERROR] org.apache.flink.client.ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat  Time elapsed: 9.157 s  <<< ERROR!
Jan 11 04:32:39 java.lang.IllegalStateException: MiniCluster is not yet running or has already been shut down.
Jan 11 04:32:39 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.getDispatcherGatewayFuture(MiniCluster.java:1044)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:917)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.getJobStatus(MiniCluster.java:841)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobStatus(MiniClusterJobClient.java:91)
Jan 11 04:32:39 	at org.apache.flink.client.ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat(ClientHeartbeatTest.java:79)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/23 10:02;Jiangang;ClientHeartbeatTestLog.txt;https://issues.apache.org/jira/secure/attachment/13054544/ClientHeartbeatTestLog.txt","31/May/23 06:02;Sergey Nuyanzin;logs-cron_azure-test_cron_azure_core-1685497478.zip;https://issues.apache.org/jira/secure/attachment/13058664/logs-cron_azure-test_cron_azure_core-1685497478.zip",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 14 09:40:27 UTC 2024,,,,,,,,,,"0|z1epow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 08:36;xtsong;[~Jiangang], could you please take a look at this？;;;","11/Jan/23 09:28;mapohl;Same build, multiple test failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10895
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10821;;;","11/Jan/23 09:47;xtsong;Offline reached out to [~Jiangang] and he said he would look into this.

I've disabled the test case in 9b5394ad8f1c935ecadcf5015b8ed90d78efd3c7.;;;","11/Jan/23 12:18;Jiangang;[~xtsong] Thanks. I would fix it as soon as possible.;;;","12/Jan/23 10:08;Jiangang;[~xtsong] I have reproduced the failure locally and upload the log. From the log, we can see that the dispatcher-5 is busy with scheduling the job and the process jobClientAlivenessCheck is delayed. The client heartbeat may be not received during this time for the same reason. 

To solve the problem, we can move the method initJobClientExpiredTime after the method runJob in Dispatcher's  method runRecoveredJob. What do you think? Thanks.;;;","13/Jan/23 02:25;xtsong;[~Jiangang], Thanks for looking into this. Sounds good to me.;;;","16/Jan/23 06:39;Jiangang;[~xtsong] Have a look at the code? Thanks.;;;","09/Feb/23 03:37;xtsong;- master (1.18): 2ad9585f47fe7fa3dcad286194bdcc4dcd131712
- release-1.17: 5c14d4a266a32c91ba6512d831f36a211907c16f;;;","09/Feb/23 06:45;renqs;The unstable case still exists on 1.17:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45921&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9864]

The failed case is ClientHeartbeatTest.testJob{*}CancelledIfClientHeartbeatTimeout{*}, which looks like related to this one, so I report the case here. 

[~Jiangang] Could you take a look when you are available? Thanks;;;","10/Feb/23 07:02;mapohl;I'm posting it here as well because [~renqs] already opened this reopened this one:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45934&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9716
{code}
Feb 09 07:53:07 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 9.939 s <<< FAILURE! - in org.apache.flink.client.ClientHeartbeatTest
Feb 09 07:53:07 [ERROR] org.apache.flink.client.ClientHeartbeatTest.testJobCancelledIfClientHeartbeatTimeout  Time elapsed: 2.708 s  <<< FAILURE!
Feb 09 07:53:07 java.lang.AssertionError: 
Feb 09 07:53:07 
Feb 09 07:53:07 Expecting actual throwable to be an instance of:
Feb 09 07:53:07   java.util.concurrent.ExecutionException
Feb 09 07:53:07 but was:
Feb 09 07:53:07   java.util.concurrent.TimeoutException
Feb 09 07:53:07 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
Feb 09 07:53:07 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Feb 09 07:53:07 	at org.assertj.core.internal.Futures.assertFailedWithin(Futures.java:118)
Feb 09 07:53:07 	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
Feb 09 07:53:07 	at org.apache.flink.client.ClientHeartbeatTest.testJobCancelledIfClientHeartbeatTimeout(ClientHeartbeatTest.java:64)
[...]
{code};;;","10/Feb/23 12:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46001&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9788;;;","10/Feb/23 12:33;mapohl;This seems to pop up more generally now, huh?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46002&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9713;;;","13/Feb/23 09:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46020&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9713;;;","13/Feb/23 09:08;mapohl;[~Jiangang] can you have a look at this once more?;;;","13/Feb/23 09:17;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9713;;;","13/Feb/23 09:35;Weijie Guo;[~mapohl] It seems that the recent failed test case is different from the previous one. The new failure case is `testJobCanceledIfClientHeartbeatTimeout` instead of `testJobRunningIfClientReportHeartbeat`. 

I looked at the code a little. It seems that the reason for the failure is that the test gave an inappropriate timeout value, which will make it very unstable. Since we should not rely on local timeout is the consensus of flink community , we can make this test rely on global timeout to avoid this problem. Of course, I will open a pr to fix this.

 ;;;","13/Feb/23 09:40;mapohl;Thanks for looking into it, [~Weijie Guo];;;","13/Feb/23 09:40;mapohl;Multiple test failures in same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9713
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46029&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9932;;;","13/Feb/23 09:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46032&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9714;;;","13/Feb/23 09:52;mapohl;2 failures, same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46033&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9792
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46033&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9714;;;","13/Feb/23 10:01;Weijie Guo;I have opened a pr([https://github.com/apache/flink/pull/21916]) to handle this, [~mapohl] Would you mind taking a look.;;;","13/Feb/23 10:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46036&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9929;;;","14/Feb/23 10:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46087&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9712;;;","14/Feb/23 14:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46124&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9792;;;","15/Feb/23 03:34;Weijie Guo;master(1.18) via bf0ad52cbcb052961c54c94c7013f5ac0110ef8a

release-1.17 via 21158c06516be4d888cd1dcf907717ea38747dfb;;;","15/Feb/23 07:47;mapohl;Thanks for taking care of it, [~Weijie Guo]. Just a reminder: The fixVersion is only set to 1.18.0 for an issue after 1.17.0 is actually released. Right now, merging to {{master}} and {{release-1.17}} doesn't mean that the fixVersion (and to some degree also the affected version) has to be set to 1.18.0. If we merge to both branches, it essentially means that it is fixed in 1.17.0. If, theoretically, we have a change that only goes into {{master}}, that would mean that the corresponding Jira issue would get a fixVersion {{1.18.0}} but not {{1.17.0}}. 

This scheme only applies in the phase where we have a release branch created but the corresponding release not finalized. It took me a bit to get my head around that one as well. And I even do it wrong sometimes now. I just wanted to clarify it once more. :-) I updated the ticket accordingly.;;;","15/Feb/23 07:51;Weijie Guo;Thanks [~mapohl] for the clarification. Your reminder is very helpful~(y);;;","15/Feb/23 08:31;mapohl;This build doesn't contain the aforementioned fix, yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46153&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9784;;;","02/May/23 05:31;Sergey Nuyanzin;Reopen since it appeared again for master
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48608&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9871;;;","02/May/23 05:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48604&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9871;;;","02/May/23 11:30;Sergey Nuyanzin;[~Weijie Guo] could you please have a look here?;;;","09/May/23 07:04;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=1310&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=9775;;;","09/May/23 09:37;Weijie Guo;It seems that the original problem fixed by [~Jiangang] has reappeared. Would you mind taking  a look at this?;;;","12/May/23 05:39;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48940&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9775;;;","15/May/23 04:00;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48972&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9703;;;","20/May/23 07:29;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49173&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9993;;;","20/May/23 07:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49175&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9704;;;","23/May/23 04:08;Jiangang;[~Weijie Guo] [~Sergey Nuyanzin]  I will have a look. ;;;","27/May/23 05:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49421&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9992;;;","29/May/23 10:11;Jiangang;[~Sergey Nuyanzin] I have tried multi times but can not reproduce it locally. I see that the time elapsed nearly 20+ second between the
ClientHeartbeatTest' start and the final error. In normal case, each case should be less than 2 second. I wonder whether the info log is printed in somewhere? It is hard to see the problem just from the code. We should more infos. Or can you give me some suggestions? Thanks.;;;","31/May/23 05:35;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49492&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9920;;;","31/May/23 06:03;Sergey Nuyanzin;[~Jiangang] I've attached logs(related ci job) for the latest failure
hope it will help
Otherwise we could think of adding more logs to have more info next time;;;","02/Jun/23 12:05;Jiangang;[~Sergey Nuyanzin] Thanks. From the log, we can see the logs in time order:
 # The dispatcher shuts down for that the client's heartbeat timeout.
 # The client begins to report its heartbeat.

The reason is that the client will report its heartbeat after calling the method waitUntilJobInitializationFinished. In this method, we try to get the job's status by waiting 

exponentially and it may take a while. There are two ways to fix the test:
 # Increase the client's timeout from 500 ms to 1 second or more.
 # In the method waitUntilJobInitializationFinished, try to get the job's status more frequently.

What do you think? cc [~xtsong] ;;;","06/Jun/23 14:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49532&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9921;;;","06/Jun/23 14:47;Sergey Nuyanzin;[~Jiangang] thanks for your update 
can we increase it till 1 sec it see if it helps?;;;","09/Jun/23 02:09;Jiangang;[~Sergey Nuyanzin] Fixed. Please review the code. Thanks.;;;","09/Jun/23 09:03;Sergey Nuyanzin;Thanks [~Jiangang]
Could you please rebase branch to the latest master since it's failed because of FLINK-32231;;;","09/Jun/23 12:16;Jiangang;Rebased. Thanks.;;;","27/Jun/23 06:21;Sergey Nuyanzin;Thanks [~Jiangang] 
the pr merged to master as 02ab5374e88780bce3c9a6991e36a7b681f88a20, a couple of weeks ago and there is no new cases so far
closing the issue;;;","03/Aug/23 14:15;mapohl;Reopening this one because it looks like it's still happening.

1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51922&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9699

Checking the state of this issue reveals that a fix was provided for master but the corresponding 1.17 backport is still open (and has a CI failure). [~Jiangang] can you finalize the work?;;;","03/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","06/Feb/24 07:31;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57324&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9747;;;","14/Feb/24 09:40;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57519&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9747;;;"
Kerberos in HiveCatalog is not work,FLINK-30628,13517528,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Jan/23 04:21,19/Mar/23 05:39,04/Jun/24 20:41,19/Mar/23 05:39,,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,We should read kerberos keytab from catalog options and doAs for hive metastore client.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-11 04:21:05.0,,,,,,,,,,"0|z1ephk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor FileSystemTableSink to use FileSink instead of StreamingFileSink,FLINK-30627,13517505,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,10/Jan/23 21:28,13/Feb/23 02:42,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,,,,,"{{FileSystemTableSink}} currently depends on most of the capabilities from {{StreamingFileSink}}, for example https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/FileSystemTableSink.java#L223-L243

This is necessary to complete FLINK-28641",,,,,,,,,,,,FLINK-28641,,,,,,,,,,,,,,,,FLINK-31019,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 02:42:49 UTC 2023,,,,,,,,,,"0|z1epcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 21:03;martijnvisser;[~tanyuxin] Could you elaborate on how you would like to solve this problem? So that some of the other maintainers can see if that's a good approach to solving this. ;;;","13/Feb/23 02:42;tanyuxin;[~martijnvisser] Sorry that I underestimated the complexity of this problem at the beginning. I don't have a good solution yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RabbitMQ connector has dependency convergence error for org.junit.jupiter:junit-jupiter,FLINK-30626,13517503,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,martijnvisser,martijnvisser,martijnvisser,10/Jan/23 21:21,03/Apr/23 11:47,04/Jun/24 20:41,03/Apr/23 11:47,rabbitmq-3.0.1,,,,,,,,,,,,,,,,,,,,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,,,,"https://github.com/apache/flink-connector-rabbitmq/actions/runs/3773487890/jobs/6414966557

{code:java}
Dependency convergence error for org.junit.jupiter:junit-jupiter:5.8.1 paths to dependency are:
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.junit.jupiter:junit-jupiter:5.8.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-test-utils-junit:1.17-20221224.001931-108
    +-org.junit.jupiter:junit-jupiter:5.9.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-architecture-tests-test:1.17-20221224.002742-107
    +-org.junit.jupiter:junit-jupiter:5.9.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-architecture-tests-test:1.17-20221224.002742-107
    +-org.apache.flink:flink-test-utils:1.17-SNAPSHOT
      +-org.junit.jupiter:junit-jupiter:5.9.1
and
+-org.apache.flink:flink-connector-rabbitmq-parent:3.0-SNAPSHOT
  +-org.apache.flink:flink-architecture-tests-production:1.17-20221224.003645-107
    +-org.junit.jupiter:junit-jupiter:5.9.1
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31701,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-10 21:21:00.0,,,,,,,,,,"0|z1epc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in tumblingWindow on 06.01.2023,FLINK-30625,13517477,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,wanglijie,martijnvisser,martijnvisser,10/Jan/23 17:06,31/Jan/23 15:46,04/Jun/24 20:41,31/Jan/23 15:46,,,,,,,1.17.0,,,,,,,,,,,,,,,,,,,,,0,,,,,"tumblingWindow median=4089.5410015 recent_median=3879.587406

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=tumblingWindow&extr=on&quarts=on&equid=off&env=2&revs=200",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30624,,,,,,,,,,FLINK-30624,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 15:46:09 UTC 2023,,,,,,,,,,"0|z1ep68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 02:24;wanglijie;This has the same root cause as FLINK-30624, which has been fixed by 21e844b6c00b1796fdfc00136ca26d90e889b149 on master.
Will leave this ticket open for a while to check the benchmark results.;;;","31/Jan/23 15:46;pnowojski;It looks like the performance regression has been fixed after merging FLINK-30624. Thanks [~wanglijie] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in stateBackends.FS / FS_ASYNC / MEMORY on 05.01.2023,FLINK-30624,13517476,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,martijnvisser,martijnvisser,10/Jan/23 17:05,01/Feb/23 06:13,04/Jun/24 20:41,01/Feb/23 06:13,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"stateBackends.FS median=4147.01197 recent_median=3957.8419495
stateBackends.FS_ASYNC median=4148.8160595 recent_median=3973.418166
stateBackends.MEMORY median=4114.406091 recent_median=3935.8805775

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.FS&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.FS_ASYNC&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.MEMORY&extr=on&quarts=on&equid=off&env=2&revs=200

",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30625,,,,,,,,,FLINK-30625,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 06:12:57 UTC 2023,,,,,,,,,,"0|z1ep60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 03:07;masteryhx;I guess the regression is caused by FLINK-30544.
In theory, FLINK-30544 trade o( n ) space which is located at heap for o(logn) time.

It will reduce heap spaces StateBackend could use so that make heap-related StateBackend like FS/MEMORY regress.

I also submitted benchmark requests to verify this:
 * [179|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/179/] – before FLINK-30544:
 ** stateBackends.MEMORY:  4177.022 (+-53.816)
 ** stateBackends.FS: 4153.435 (+-66.577)
 ** stateBackends.FS_ASYNC: 4139.056 (+-40.393)
 * [180|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/180/] - after FLINK-30544:
 ** stateBackends.MEMORY:  3936.158 (+-65.696)
 ** stateBackends.FS: 3963.934 (+-64.942)
 ** stateBackends.FS_ASYNC: 3930.955 (+-68.809)

So maybe [~wanglijie] could help to take a look ?;;;","12/Jan/23 03:56;wanglijie;[~masteryhx] I 'll take a look.;;;","12/Jan/23 07:18;martijnvisser;Thank you for picking this up!;;;","29/Jan/23 15:42;wanglijie;After an offline discussion with [~zhuzh], we believe that the regressions are caused by some statebackend-related code blocks([HeapPriorityQueue|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueue.java], [HeapPriorityQueueElement|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueElement.java]) that can no longer be inlined by jvm after FLINK-30544. Before FLINK-30544, there was only one implementation of {{HeapPriorityQueueElement}} (the [TimerHeapInternalTimer|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/TimerHeapInternalTimer.java]) while running the benchmark, which allowed the jvm to inline its methods (getInternalIndex,setInternalIndex). However, we introduced [InputChannelStatus|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/watermarkstatus/StatusWatermarkValve.java#:~:text=protected%20static%20class%20InputChannelStatus%20implements%20HeapPriorityQueueElement%20%7B] (implements {{{}HeapPriorityQueueElement{}}}) in FLINK-30544, resulting in two implementations of {{{}HeapPriorityQueueElement{}}}, the related methods can no longer be inlined, introduced the extra virtual method call overhead, resulting in the performance regression.

I have ran a [test branch|https://github.com/wanglijie95/flink/tree/after-30544-fix] to verify our thought. I created new classes {{{}HeapPriorityQueue2/HeapPriorityQueueElement2{}}}, which have exactly the same logic as {{{}HeapPriorityQueue/HeapPriorityQueueElement{}}}, and used them in {{StatusWatermarkValve}} as replacements. Then I ran the benchmarks and found that the regressions disappeared:
 * [209 - test branch|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/209/]
 ** stateBackends.MEMORY:  4138.693 (+-66.226)
 ** stateBackends.FS: 4172.001 (+-50.766)
 ** stateBackends.FS_ASYNC: 4200.842 (+-51.285)

I think we can solve this ticket by the about approach(introduce new classes with the same logic), I will open a PR to fix it soon.;;;","30/Jan/23 16:49;zhuzh;Fix 21e844b6c00b1796fdfc00136ca26d90e889b149 is merged.
Will leave this ticket open for a while to check the benchmark results.;;;","01/Feb/23 06:12;zhuzh;The performance regression is gone.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in checkpointSingleInput.UNALIGNED on 04.01.2023,FLINK-30623,13517474,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,martijnvisser,martijnvisser,10/Jan/23 17:02,31/Jan/23 14:09,04/Jun/24 20:41,31/Jan/23 14:09,,,,,,,1.17.0,,,,,,,,,,,,,,,,Benchmarks,Runtime / Checkpointing,,,,0,pull-request-available,,,,"Performance regression
checkpointSingleInput.UNALIGNED median=338.1445195 recent_median=67.6453005
checkpointSingleInput.UNALIGNED_1 median=213.230041 recent_median=39.830277
deployAllTasks.STREAMING median=168.533106 recent_median=159.8534395
stateBackends.MEMORY median=3229.0248875 recent_median=2985.782919
tupleKeyBy median=4155.684199 recent_median=3987.5812305

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED_1&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=8&ben=deployAllTasks.STREAMING&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=6&ben=stateBackends.MEMORY&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=6&ben=tupleKeyBy&extr=on&quarts=on&equid=off&env=2&revs=200",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30533,,,FLINK-30709,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 14:09:26 UTC 2023,,,,,,,,,,"0|z1ep5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 09:38;Yanfei Lei;For checkpointSingleInput.UNALIGNED and checkpointSingleInput.UNALIGNED_1, the regression occurred during 13ef498172b...fb272D2cdebf. Seems to be related to fb272D2cdebf(FLINK-30533). I submitted two benchmark requests to verify this:
 * [#177|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/177/] (before FLINK-30533)
 ** checkpointSingleInput.UNALIGNED ： 333.635178(+-8.169488)
 ** checkpointSingleInput.UNALIGNED_1：213.837107(+-7.282883)
 * [#178|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/178/] (after FLINK-30533)
 ** checkpointSingleInput.UNALIGNED ： 61.536982（+-3.581509）
 ** checkpointSingleInput.UNALIGNED_1：38.207438（+-2.937051）

[~lindong] could you please help take a look?;;;","11/Jan/23 09:49;martijnvisser;[~Yanfei Lei]  Thanks for helping out on this;;;","12/Jan/23 13:04;pnowojski;I have [some doubts|https://issues.apache.org/jira/browse/FLINK-30533?focusedCommentId=17676585&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17676585] about FLINK-30533 design. Maybe it would be best to revert it and re-open the discussion? ;;;","16/Jan/23 10:37;fanrui;Thanks [~martijnvisser] reports this issue, and [~Yanfei Lei]  [~pnowojski] discuss here.

After analysis, I think the root cause is FLINK-30533 doesn't check `recordWriter.isAvailable()` in the loop of SourceOperator.

When the recordWriter is unavailable, task shouldn't send data to downstream task, the task thread may get stuck in requestMemoryBuffer due to backpressure. The checkpoint barrier cannot be handled when task thread is stucking in requestMemoryBuffer, that's why performance regression for unaligned checkpoint. 

cc [~lindong] 

 ;;;","16/Jan/23 14:46;lindong;Thanks [~martijnvisser] for creating the issue. And thanks [~fanrui] for creating the bugfix PR!

Sorry, I didn't notice the Jira notification due to my gmail filter misconfiguration. I just fixed the configuration and should be able to notice Jira comments in the future.

I will look into this issue and the PR tomorrow morning.;;;","26/Jan/23 19:54;pnowojski;It seems like not everything has been fixed?

[http://codespeed.dak8s.net:8000/timeline/?ben=checkpointSingleInput.UNALIGNED&env=2]

Does anyone has some idea why?;;;","27/Jan/23 06:22;fanrui;FLINK-26803 was merged at 23.01.2023, I'm not sure whether it will affect the benchmark. I can check it today.;;;","27/Jan/23 08:51;pnowojski;Thanks!;;;","27/Jan/23 10:52;lindong;Merged to master branch with the following commits:

2d1510a9d559a49806a60ececfd854dd53a6591d
877511b8ea2b4ef62fc520a0d0bd9087f2f25c56
560f314703858464f5089e24e065347d00704af5;;;","27/Jan/23 12:05;lindong;It appears that the records/sec of checkpointSingleInput dropped by about 9% from 337.5 to 309.4.

My hypothesis for this regression is that most workload of this benchmark is related to control flow (e.g. checkpoint-related events) rather than data flow. Thus it can not take advantage of the optimization we put in emitNext(...). Instead, it suffers the additional overhead of the operation (e.g. checking whether mailbox is empty and whether task is available) we put in the emitNext(), which leads to the 10% performance regression.

 ;;;","27/Jan/23 12:53;fanrui;> It can not take advantage of the optimization we put in emitNext(...). Instead, it suffers the additional overhead of the operation (e.g. checking whether mailbox is empty and whether task is available) we put in the emitNext(), which leads to the 10% performance regression.

Hi [~lindong] thanks for your feedback. Sorry, I don't think it can lead to the 10% performance regression. We can take a look the benchmark of mapSink[1]. The records/sec can reach 40k, indicating that the additional overhead is very small. Shouldn't reduce 330 records/sec to 300.

I think FLINK-26803 is the root cause, the execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=5 by default. It means 5 subtasks will share the same Unaligned checkpooint file. It will reduce the number of small files, but the UC time will become larger.

I run the UNALIGNED benchmark on my Mac based on master, and check the benchmark result after setting execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=1.
{code:java}
// execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=5
Benchmark                                            (mode)   Mode  Cnt    Score    Error  Units
CheckpointingTimeBenchmark.checkpointSingleInput  UNALIGNED  thrpt   30  317.063 ± 12.762  ops/s

// execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=1
Benchmark                                            (mode)   Mode  Cnt    Score   Error  Units
CheckpointingTimeBenchmark.checkpointSingleInput  UNALIGNED  thrpt   30  351.852 ± 6.857  ops/s {code}
 

From the benchmark, I think we can ensure FLINK-26803 is the root cause. 

[~pnowojski], do you think we should set execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=1 in the flink-benchmark? Or should we add the parameters about it? We can compare the performance impact on UC under different execution.checkpointing.unaligned.max-subtasks-per-channel-state-file.

 

[1] [http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&env=2]
[2] [http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200];;;","30/Jan/23 14:37;pnowojski;Thanks for the analysis [~fanrui] !
{quote} It means 5 subtasks will share the same Unaligned checkpooint file. It will reduce the number of small files, but the UC time will become larger.
{quote}
What do you think is the actual reason behind the regression? That now we have to enqueue writes from a couple of subtasks one after another, so for example with 2 subtasks, the second has to wait until first completes it's writes? 

And what do you think is the impact of this setting in a production setups? If this is an issue related to number of shared IO threads, it might be that we only increased the checkpoint time by a small constant ({{{}0.3ms / checkpoint{}}} based on the numbers that [~fanrui]  has quoted), that simply remains a constant with more realistic setups. Increase checkpoint time by 0.3ms when checkpoints are taking a couple of seconds, doesn't matter.;;;","31/Jan/23 09:47;fanrui;{quote} What do you think is the actual reason behind the regression? That now we have to enqueue writes from a couple of subtasks one after another, so for example with 2 subtasks, the second has to wait until first completes it's writes? 
{quote} 

When two subtasks share the same file, only one subtask can write the file at a time, that is, subtask 2 must wait while subtask1 is writing to the file, so the UC time will become larger.

{quote} And what do you think is the impact of this setting in a production setups?
{quote} 

I think the impact on production is minimal. If hdfs writes quickly, the checkpoint time will not increase significantly. Of course, if hdfs writes slowly, it may have an impact, a reasonable solution at this point is: flink or hdfs sre should improve the stability and performance of hdfs.
;;;","31/Jan/23 14:09;pnowojski;Ok, let's keep it as it is. Thanks [~fanrui] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support auto consume schema in Pulsar source,FLINK-30622,13517446,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,10/Jan/23 12:17,10/Feb/23 02:05,04/Jun/24 20:41,10/Feb/23 02:05,pulsar-4.0.0,,,,,,pulsar-4.0.0,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"Pulsar client supports [auto schema|https://pulsar.apache.org/docs/2.10.x/schema-understand/#auto-schema] if you don't know the schema type of a Pulsar topic in advance.

This can make the message consuming from Pulsar topic which contains multiple schema extremely easy. We plan to add auto schema support in Pulsar source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 02:05:05 UTC 2023,,,,,,,,,,"0|z1eozk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 02:05;tison;master via https://github.com/apache/flink-connector-pulsar/pull/19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Violation of main thread constraint detected,FLINK-30621,13517440,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,10/Jan/23 11:25,16/Jan/23 11:10,04/Jun/24 20:41,,1.15.3,1.16.0,1.17.0,,,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,"When looking into FLINK-30540, I noticed the following warning being logged not only in the context of {{DataSinkTaskTest}}:
{code}
00:56:09,055 [   pool-263-thread-1] WARN  org.apache.flink.runtime.rpc.MainThreadValidatorUtil         [] - Violation of main thread constraint detected: expected <Thread[main,5,main]> but running in <Thread[pool-263-thread-1,5,main]>.
java.lang.Exception: Violation of main thread constraint detected: expected <Thread[main,5,main]> but running in <Thread[pool-263-thread-1,5,main]>.
        at org.apache.flink.runtime.rpc.MainThreadValidatorUtil.isRunningInExpectedThread(MainThreadValidatorUtil.java:73) ~[flink-rpc-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.lambda$new$0(ComponentMainThreadExecutorServiceAdapter.java:56) ~[test-classes/:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.assertRunningInMainThread(ComponentMainThreadExecutorServiceAdapter.java:86) ~[test-classes/:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.assertRunningInMainThread(DeclarativeSlotPoolBridge.java:446) ~[classes/:?]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.checkBatchSlotTimeout(DeclarativeSlotPoolBridge.java:464) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30540,,"10/Jan/23 13:59;mapohl;FLINK-30621.20230101.1.tar.gz;https://issues.apache.org/jira/secure/attachment/13054489/FLINK-30621.20230101.1.tar.gz",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 11:10:20 UTC 2023,,,,,,,,,,"0|z1eoy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 11:26;mapohl;[~chesnay] may you have a look at this?;;;","10/Jan/23 13:40;mapohl;The [related build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44366&view=results] ran on {{master}}. I attached the Maven logs for that build to this Jira issue (other artifacts like the ZK logs and the 3GB core dump were removed).;;;","10/Jan/23 14:17;mapohl;I'm wondering how this works at all in general, This warning is triggered in {{MainThreadValidatorUtil.isRunningInExpectedThread}} which returns {{false}}. But the calling method [ComponentMainThreadExecutorServiceAdapter#mainThreadCheck|https://github.com/apache/flink/blob/be75c0d82613726a7f508824730bbf2c9f6e8625/flink-runtime/src/test/java/org/apache/flink/runtime/concurrent/ComponentMainThreadExecutorServiceAdapter.java#L56] does an assert on the return value which should fail in general. Do we disable JVM assertions in our CI pipeline? Our couldn't find any evidence for that in the code.;;;","10/Jan/23 14:35;mapohl;I found logs from [build 20220816.4|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40027&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9]  which is a 1.15 build that contained this warning as well. 

I'm curious about the reason why I'm totally wrong and this is actual expected behavior :-D;;;","10/Jan/23 15:43;wanglijie;Do we enable JVM assertions via ""-ea"" in CI pipeline? I remember the JVM assertions is disabled by default.;;;","11/Jan/23 09:48;mapohl;Ah true, that's what I should have checked as well. I assumed that assertions are enabled by default. :facepalm:;;;","16/Jan/23 11:06;mapohl;After talking with [~chesnay] offline about it, one conclusion is that we use {{ComponentMainThreadExecutorServiceAdapter}} in a wrong way in various tests, e.g. completing futures in the test's main thread which might cause this error message.;;;","16/Jan/23 11:10;mapohl;I looked into some other tests. For the [test-ci/core job of 20230116.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44863&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8140] module, there are test cases that have this warning even though they are not using the {{ComponentMainThreadExecutorServiceAdapter}}. It's odd, though, that in the aforementioned build, we see the warning for a test like {{BlobClientSslTest.testNonSSLConnection}} which doesn't use the main thread-related components:
{code}
00:58:52,799 [                main] INFO  org.apache.flink.runtime.blob.BlobClientSslTest              [] - 
================================================================================
Test testNonSSLConnection(org.apache.flink.runtime.blob.BlobClientSslTest) is running.
--------------------------------------------------------------------------------
00:58:55,198 [   pool-160-thread-1] WARN  org.apache.flink.runtime.rpc.MainThreadValidatorUtil         [] - Violation of main thread constraint detected: expected <Thread[main,5,main]> but running in <Thread[pool-160-thread-1,5,main]>.
java.lang.Exception: Violation of main thread constraint detected: expected <Thread[main,5,main]> but running in <Thread[pool-160-thread-1,5,main]>.
        at org.apache.flink.runtime.rpc.MainThreadValidatorUtil.isRunningInExpectedThread(MainThreadValidatorUtil.java:73) ~[flink-rpc-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.lambda$new$0(ComponentMainThreadExecutorServiceAdapter.java:56) ~[test-classes/:?]
        at org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.assertRunningInMainThread(ComponentMainThreadExecutorServiceAdapter.java:86) ~[test-classes/:?]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.assertRunningInMainThread(DeclarativeSlotPoolBridge.java:446) ~[classes/:?]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.checkBatchSlotTimeout(DeclarativeSlotPoolBridge.java:464) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
00:58:55,205 [   pool-161-thread-1] WARN  org.apache.flink.runtime.rpc.MainThreadValidatorUtil         [] - Violation of main thread constraint detected: expected <Thread[main,5,main]> but running in <Thread[pool-161-thread-1,5,main]>.
java.lang.Exception: Violation of main thread constraint detected: expected <Thread[main,5,main]> but running in <Thread[pool-161-thread-1,5,main]>.
        at org.apache.flink.runtime.rpc.MainThreadValidatorUtil.isRunningInExpectedThread(MainThreadValidatorUtil.java:73) ~[flink-rpc-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.lambda$new$0(ComponentMainThreadExecutorServiceAdapter.java:56) ~[test-classes/:?]
        at org.apache.flink.runtime.concurrent.ComponentMainThreadExecutorServiceAdapter.assertRunningInMainThread(ComponentMainThreadExecutorServiceAdapter.java:86) ~[test-classes/:?]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.assertRunningInMainThread(DeclarativeSlotPoolBridge.java:446) ~[classes/:?]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.checkBatchSlotTimeout(DeclarativeSlotPoolBridge.java:464) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
[...]
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive catalog supports specifying custom Hive metastore client,FLINK-30620,13517439,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,10/Jan/23 11:20,13/Jan/23 06:22,04/Jun/24 20:41,13/Jan/23 06:22,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Currently Hive metastore client class is hard coded in {{HiveCatalog}}, however users may want to specify custom Hive metastore client to read and write Hive compliant storage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 06:22:34 UTC 2023,,,,,,,,,,"0|z1eoy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 06:22;TsReaper;master: 6144fa3a65356322b800d0b392099f0ac92043f3
release-0.3: 7a03fcc2b63aa8ca502c244df8da91a67588f7b5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerTest.testStatusMetrics is not stable,FLINK-30619,13517436,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,mapohl,mapohl,10/Jan/23 10:55,16/Mar/23 15:45,04/Jun/24 20:41,26/Jan/23 15:44,1.16.0,1.17.0,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,test-stability,,,"We experience a test instability in {{AdaptiveSchedulerTest.testStatusMetrics}}.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44635&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8475",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25667,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 16 15:45:14 UTC 2023,,,,,,,,,,"0|z1eoxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 10:58;mapohl;FLINK-25667 handled instabilities of this test in Flink 1.15. I created a follow-up ticket here because a fix was provided back then and there's this big time difference between the two, so the test instability might be due to some new changes.;;;","17/Jan/23 09:23;hxbks2ks;same case in master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44951&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","20/Jan/23 07:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45097&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8373;;;","24/Jan/23 07:57;mapohl;[~chesnay] may you have a look at that one?;;;","24/Jan/23 14:17;chesnay;Can't reproduce it locally. Either a clock issue on the machines causing time measurements to go backwards or a strange thread visibility issue.
I'm inclined to just add a retry to the test.;;;","26/Jan/23 15:44;chesnay;master: 38d390114e93e9f202dc5ac7fa668dd255637139
1.16: 0f52f62b1891230308282e0b819062eb13eb5644;;;","16/Mar/23 15:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47219&view=logs&j=d89de3df-4600-5585-dadc-9bbc9a5e661c&t=be5a4b15-4b23-56b1-7582-795f58a645a2&l=9329

I'm not reopening that one considering that we're going to deprecated 1.15 soon anyway.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-pulsar not retrievable from Apache's Snapshot Maven repository,FLINK-30618,13517434,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,mapohl,mapohl,10/Jan/23 10:43,26/Jan/23 08:27,04/Jun/24 20:41,26/Jan/23 08:27,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Pulsar,Test Infrastructure,,,,0,pull-request-available,test-stability,,,"The build failure was caused by {{flink-connector-pulsar}} not being retrievable from the Apache Snapshot Maven repository:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10132

{code}
Jan 10 02:03:24 [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
Jan 10 02:03:24 [ERROR] Failed to execute goal on project flink-python: Could not resolve dependencies for project org.apache.flink:flink-python:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-sql-connector-pulsar:jar:4.0-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -> [Help 1]
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 10 02:03:24 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 10 02:03:24 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] After correcting the problems, you can resume the build with the command
Jan 10 02:03:24 [ERROR]   mvn <goals> -rf :flink-python
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 26 08:27:50 UTC 2023,,,,,,,,,,"0|z1eoww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 10:49;mapohl;[~martijnvisser] may you look into this issue? I saw this before (I guess) but didn't consider it as an issue because it happened right after the pulsar connector was released (at least that was my reasoning). But this time, we should have all the artifacts already in place, shouldn't we?;;;","10/Jan/23 19:00;syhily;Since we have deploy the {{4.0-SNAPSHOT}}, the root cause may be something else?

{code}
Jan 10 02:03:10 [WARNING] Could not transfer metadata org.apache.flink:flink-sql-connector-pulsar:4.0-SNAPSHOT/maven-metadata.xml from/to apache.snapshots (https://repository.apache.org/snapshots): transfer failed for https://repository.apache.org/snapshots/org/apache/flink/flink-sql-connector-pulsar/4.0-SNAPSHOT/maven-metadata.xml, status: 502 Proxy Error
Jan 10 02:03:10 [WARNING] Failure to transfer org.apache.flink:flink-sql-connector-pulsar:4.0-SNAPSHOT/maven-metadata.xml from https://repository.apache.org/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots has elapsed or updates are forced. Original error: Could not transfer metadata org.apache.flink:flink-sql-connector-pulsar:4.0-SNAPSHOT/maven-metadata.xml from/to apache.snapshots (https://repository.apache.org/snapshots): transfer failed for https://repository.apache.org/snapshots/org/apache/flink/flink-sql-connector-pulsar/4.0-SNAPSHOT/maven-metadata.xml, status: 502 Proxy Error
{code}
;;;","11/Jan/23 09:54;mapohl;good catch, @yufan! We had issues with disk space in this build that manifested itself in the cache not being properly written:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=ef90e7e5-0acf-4005-9e23-49f8aa9cc762&l=212]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b&t=603e646d-7bc1-4fbf-9ad3-20cc53626a82&l=212];;;","11/Jan/23 16:21;mapohl;Ok, after looking into the AzureCI caching functionality and the Maven warning a bit, I'd be inclined to say that they are unrelated. It's rather a problem with the connection. I'll keep this issue open in case it pops up again.;;;","11/Jan/23 20:35;syhily;[~martijnvisser] I found that the Pulsar's {{4.0-SNAPSHOT}} build didn't get updated with the nightly build. Do we have some policy on when to update the externalized connector's snapshot build.;;;","12/Jan/23 14:23;chesnay;We haven't set up snapshot deployments for any externalized connector.;;;","14/Jan/23 00:51;syhily;[~chesnay] I found the document of the externalized connector can be updated to the nightly built of the flink website. So I want to know when we will finalize the snapshot deployment of the externalized connector?;;;","16/Jan/23 09:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44870&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10461;;;","16/Jan/23 09:56;mapohl;I'm wondering why we only observed this issue with the pulsar connector so far. As far as I remember, we also use other connectors in {{{}flink-python{}}}'s test suite.;;;","24/Jan/23 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=7757;;;","26/Jan/23 08:27;martijnvisser;Fixed in master: 173ddfcb9fd2b15cf3697f5b79406d7d5ed1ad20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong data type for null literal with cast in Hive dialect,FLINK-30617,13517432,13510724,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,10/Jan/23 10:36,16/Jan/23 09:29,04/Jun/24 20:41,16/Jan/23 09:29,1.16.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"When in hive dialect, it's found that the expression `cast null as bigint` will result in string type. Then it will cause the following sql fail:
{code:java}
create table t(a ARRAY<BIGINT>)
insert into t select array(cast(null as bigint)) {code}
The exception is:
{code:java}
org.apache.flink.table.api.ValidationException: Column types of query result and sink for 'test-catalog.default.t' do not match.
Cause: Incompatible types for sink column 'a' at position 0.Query schema: [_o__c0: ARRAY<STRING>]
Sink schema:  [a: ARRAY<BIGINT>] {code}
Introduce by FLINK-26474, after that we will fold `cast null as bigint` to `null `which will then considered as the null type. And for null type as argument, the function `array` will get a result type of array<string>. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 09:28:30 UTC 2023,,,,,,,,,,"0|z1eowg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 09:28;jingzhang;Fixed in master: 1c5247943627dbf4ad64de89fe5b936ddb638368;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't support batchMessageId when restore from checkpoint,FLINK-30616,13517411,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,songv,songv,10/Jan/23 07:53,10/Jan/23 08:57,04/Jun/24 20:41,10/Jan/23 08:45,1.16.0,pulsar-3.0.0,,,,,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,,,,,"I have a non-partition topic: 
 * the producer for the topic sends batch messages to the topic(to improve the speed of producers)
 * the flink job consumes this topic by Exclusive subscription type

When the flink task manager is restarted for some reason, an exception is thrown when restored from the checkpoint:
{code:java}
java.lang.RuntimeException: One or more fetchers have encountered the exception
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:106) ~[?:?]
at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:403) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:387) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[flink-dist-1.16.0.jar:1.16.0]
at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-files-1.16.0.jar:1.16.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
... 1 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently. This batch size is %d [83]
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:160) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:65) ~[?:?]
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:44) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:92) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:171) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:51) ~[?:?]
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-files-1.16.0.jar:1.16.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
... 1 more{code}
some important logs in the task manager:
{code:java}
2023-01-09 14:51:01,645 DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) and restoring with state from alternative (1/1).2023-01-09 14:51:01,664 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]2023-01-09 14:51:01,740 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 02023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Prepare to run AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Enqueued task AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Cleaned wakeup flag.2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Prepare to run AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,742 DEBUG org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase [] - Handle split changes SplitAddition:[[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,742 INFO  org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader [] - Reset subscription position by the checkpoint 25551:17912:-1:82023-01-09 14:51:01,743 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records {code}
I don't know if it is a feature or a bug, but this means that we can't restore from a batch message id checkpoint. I would like to know what to do better. [~Tison] 

 ","flink version: 1.16.0

flink-connector-pulsar version: 1.16.0",,,,,,,,,,,,,,,,,FLINK-30552,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 08:57:17 UTC 2023,,,,,,,,,,"0|z1eors:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 08:35;syhily;Thanks for submitting this issue. This is a known issue and has been resolved in https://github.com/apache/flink-connector-pulsar/pull/11. We just need to backport to {{pulsar-3.0}} branch. Can you close this issue for duplicated?;;;","10/Jan/23 08:39;tison;[~songv] According to [~syhily]'s comment above, you may try out the master branch code on https://github.com/apache/flink-connector-pulsar to see if it solves your case. Also, I'd like to know whether the next feature release is also acceptable for you.;;;","10/Jan/23 08:44;songv;[~tison] [~syhily] Thanks, I need it to restore tasks in the prod env, I will try to backport the [https://github.com/apache/flink-connector-pulsar/pull/11] to pulsar-3.0 and build a package.;;;","10/Jan/23 08:53;songv;I found there are many conflicts while cherry-pick [https://github.com/apache/flink-connector-pulsar/pull/11]
When could we release a fix version for pulsar-3.0-1.16? [~syhily] ;;;","10/Jan/23 08:56;syhily;[~songv] The [backport PR|https://github.com/apache/flink-connector-pulsar/pull/16] has been submitted. You can check it for local building.;;;","10/Jan/23 08:57;songv;nice job!!!! txs [~syhily] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor ReadWriteTableITCase to get rid of Managed Table,FLINK-30615,13517384,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,10/Jan/23 04:25,11/Jan/23 08:09,04/Jun/24 20:41,11/Jan/23 08:09,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"# Get rid of Managed Table
 # Split kafka log test into a new test class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 08:09:23 UTC 2023,,,,,,,,,,"0|z1eols:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 08:09;lzljs3620320;master: 7417df23f715c2879a8ca2d508dee1009ac93173;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve resolving schema compatibility -- Milestone two,FLINK-30614,13517383,13494219,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,masteryhx,masteryhx,10/Jan/23 04:25,16/Apr/23 06:04,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,API / Type Serialization System,,,,,0,,,,,"In the milestone two, we should:
 # Remove TypeSerializerSnapshot#resolveSchemaCompatibility(TypeSerializer<T> newSerializer) and related implementation.
 # Make all places where use TypeSerializerSnapshot#resolveSchemaCompatibility(TypeSerializer<T> newSerializer) to check the compatibility call Typeserializer#resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot). 
 # Remove the default implementation of the new method.

It will be done after several stable version.

See FLIP-263 for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 16 06:04:37 UTC 2023,,,,,,,,,,"0|z1eolk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/23 06:04;zxcoccer;hi, I would like to give a try on this, can  I take this ticket?I will try my best to complete it
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve resolving schema compatibility -- Milestone one,FLINK-30613,13517382,13494219,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,10/Jan/23 04:22,27/Feb/24 08:45,04/Jun/24 20:41,15/Jan/24 02:15,,,,,,,1.19.0,,,,,,,,,,,,,,,,API / Type Serialization System,,,,,0,pull-request-available,stale-assigned,,,"In the milestone one, we should:
 # Add an extra method (TypeserializeSnapshotr#resolveSchemaCompatibility(TypeSerializerSnapshot<T> oldSerializerSnapshot)) in TypeSerializerSnapshot.java as above, and return INCOMPATIBLE as default.
 # Mark the original method as deprecated and it will use new method to resolve as default.
 # Implement the new method for all built-in TypeserializerSnapshots.

See FLIP-263 for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 02:15:51 UTC 2024,,,,,,,,,,"0|z1eolc:",9223372036854775807,"In Flink 1.19, the old method of resolving schema compatibility has been deprecated and the new one is introduced. See FLIP-263 for more details.
Please migrate to the new method following below link:
https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/serialization/custom_serialization/#migrating-from-deprecated-typeserializersnapshotresolveschemacompatibilitytypeserializer-newserializer-before-flink-119",,,,,,,,,,,,,,,,,,,"13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","15/Jan/24 02:15;masteryhx;merged 13921a08...0e5de813 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ObjectArrayConverter is not thread-safe，raised by reuseArray、reuseWriter,FLINK-30612,13517379,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,guoxin,guoxin,10/Jan/23 03:30,11/May/23 11:46,04/Jun/24 20:41,,1.12.2,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,Table SQL / Runtime,,,,0,,,,,"data clutter in multi-threaded environment.

by design?

or , how do i use it in multi-thread env?
{code:java}
package org.apache.flink.table.data.util;

public class DataFormatConverters{

  ....

  public static final class ObjectArrayConverter<T> extends DataFormatConverter<ArrayData, T[]>{
    ....

    // it's not thread-safe
    private transient BinaryArrayData reuseArray;

    private transient BinaryArrayWriter reuseWriter;
    ....

    private ArrayData toBinaryArray(T[] value){
        // not thread-safe
    }

    ....

  }

  ....
}



{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 11:19:21 UTC 2023,,,,,,,,,,"0|z1eoko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 12:55;leonard;Could you update the description more precise? and 1.12 is a pretty old version, the community won't fix bug for it.;;;","11/May/23 11:19;guoxin;this variable in inner-class ObjectArrayConverter is not thread-safe, please see [https://github.com/apache/flink/blob/24685395ccf19344e9331201ae0cd600b7722a89/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/data/util/DataFormatConverters.java#L1155]

 

the same problem exists with inner-class  MapConverter and ObjectArrayConverter in DataFormatConverters.

the latest version still has this problem. obviously.

when i define  a corresponding  implementation of org.apache.flink.table.function.AsyncTableFunction.java  to query set and list data in redis, i get messy data.

example:

i want [a, b, c], but got [#$a, b, 1@$].

code is too loooong to show here. 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expire snapshot should be reentrant,FLINK-30611,13517369,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,10/Jan/23 03:22,12/Jan/23 12:49,04/Jun/24 20:41,10/Jan/23 10:40,,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"At present, if the file is incomplete, expire will throw an exception.
However, the snapshot in expire may be incomplete. It can be interrupted and killed suddenly.
Therefore, we should ensure the safety of expire, make it reentrant, and avoid throwing exceptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 10:40:18 UTC 2023,,,,,,,,,,"0|z1eokg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 10:40;TsReaper;master: 929a411f29f4fc76dfead6e716daae56c165724c
release-0.3: 1f0f28c3cf9c680ba9294362f3a4ba128236f8ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-table-runtime free for disk io in flink-core,FLINK-30610,13517360,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,10/Jan/23 03:02,10/Jan/23 15:16,04/Jun/24 20:41,10/Jan/23 15:16,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 15:16:17 UTC 2023,,,,,,,,,,"0|z1eoig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 15:16;lzljs3620320;master: 6c9f4331e1da6d4eba5b0e423f20cd9e5fcbc10a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ephemeral storage to CRD,FLINK-30609,13517279,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ZhenqiuHuang,morhidi,morhidi,09/Jan/23 18:50,21/Apr/23 12:05,04/Jun/24 20:41,21/Apr/23 12:05,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,starter,,,"We should consider adding ephemeral storage to the existing [resource specification |https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/reference/#resource]in CRD, next to {{cpu}} and {{memory}}

https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 12:05:14 UTC 2023,,,,,,,,,,"0|z1eo0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 04:44;gyfora;Are you suggesting simply adding:
```
ephemeral-storage: ""2Gi""
``` 
?;;;","11/Jan/23 04:46;gyfora;+1 from my part, this indeed seems to be a common requirement that may not be obvious to users through pod template and also there is no Flink config equivalent to set it.;;;","11/Jan/23 18:31;mason6345;+1 as well to make configuring it easier but I hope that the current method of configuring will behave the same. New way can precedence though ;;;","07/Mar/23 21:52;gyfora;[~morhidi] do you mind if [~pbharaj] takes over this ticket?;;;","31/Mar/23 15:12;gyfora;[~pbharaj] I see you did not make too much progress on this, could [~ZhenqiuHuang] take over this?;;;","03/Apr/23 15:56;nfraison.datadog;We are looking into using this feature and have some time to implement it.
If it is fine by you I can take over on it.;;;","03/Apr/23 16:27;ZhenqiuHuang;[~nfraison.datadog]
We are glad that you also have the needs. I am actively creating the PR now. Please wait for 1 or 2 days. ;;;","03/Apr/23 23:30;ZhenqiuHuang;[~nfraison.datadog]
Please review the PR according to the requirement from your org. Thank you very much!;;;","21/Apr/23 12:05;gyfora;merged to main bad872d04e324fde2b6396e18bae5a37d804f59b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support rename table ,FLINK-30608,13517262,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,09/Jan/23 15:58,06/Mar/23 06:46,04/Jun/24 20:41,10/Feb/23 10:43,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 10:43:46 UTC 2023,,,,,,,,,,"0|z1enx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 10:43;lzljs3620320;master: 706e7c16237b1947892dee23fd3445c31e5d972e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table.to_pandas doesn't support Map type,FLINK-30607,13517248,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,09/Jan/23 14:18,29/Mar/23 04:15,04/Jun/24 20:41,11/Jan/23 02:47,1.15.3,,,,,,1.17.0,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"It seems that the Table#to_pandas method in PyFlink doesn't support Map type. It throws the following exception.
{code:java}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
: java.lang.UnsupportedOperationException: Python vectorized UDF doesn't support logical type MAP<INT, INT> currently.
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:743)
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:617)
    at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:167)
    at org.apache.flink.table.types.logical.MapType.accept(MapType.java:115)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowField(ArrowUtils.java:189)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.lambda$toArrowSchema$0(ArrowUtils.java:180)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowSchema(ArrowUtils.java:181)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:483)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748) {code}
This can be reproduced with the following code.
{code:java}
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)
table = t_env.from_descriptor(
    TableDescriptor.for_connector(""datagen"")
    .schema(
        Schema.new_builder()
        .column(""val"", DataTypes.MAP(DataTypes.INT(), DataTypes.INT()))
        .build()
    )
    .option(""number-of-rows"", ""10"")
    .build()
)
df = table.to_pandas()
print(df) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 04:15:58 UTC 2023,,,,,,,,,,"0|z1enu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 02:47;dianfu;Merged to master via b781a13dd615e8d131defe37ca9e550416c10595 ~ afc8fb08ab0879537814d3c77372268eb6d6a4de;;;","29/Mar/23 04:15;xuannan;[~dianfu] Thanks for the patch! This is very useful in our use case. May I ask if we have a plan to backport this to version 1.16?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Pulsar to 2.11.0,FLINK-30606,13517243,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,syhily,syhily,09/Jan/23 13:05,15/Feb/23 08:18,04/Jun/24 20:41,15/Feb/23 08:18,pulsar-4.0.0,,,,,,pulsar-4.0.0,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"The fifth release candidate for Apache Pulsar 2.11.0 is voting and be approved. So the final 2.11.0 will be released soon. This is a huge release which fixes a lot of bugs.

We will bump the Pulsar client to this version.",,,,,,,,,,,,FLINK-30489,,,,,,,FLINK-24302,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 08:18:26 UTC 2023,,,,,,,,,,"0|z1ensw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 09:22;syhily;Pulsar 2.11.0 has been released. https://repo1.maven.org/maven2/org/apache/pulsar/pulsar-client-all/2.11.0/;;;","15/Feb/23 08:18;tison;master via 8b58e7f81058ee50f7d194319ca122cc134033dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Streaming File Sink end-to-end test'  fails with UnsupportedOperationException,FLINK-30605,13517237,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,mapohl,mapohl,09/Jan/23 12:16,09/Jan/23 13:01,04/Jun/24 20:41,09/Jan/23 12:36,1.17.0,,,,,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,test-stability,,,,"We have a test failure in {{Streaming File Sink end-to-end test}} being caused by an {{UnsupportedOperationException}} ({{{}StreamingFileSink{}}} is not supported as a type):
{code:java}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unsupported sink type: StreamingFileSink
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.UnsupportedOperationException: Unsupported sink type: StreamingFileSink
	at org.apache.flink.connector.file.sink.FileSinkProgram.main(FileSinkProgram.java:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
	... 9 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44597&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=2154",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30166,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 13:01:54 UTC 2023,,,,,,,,,,"0|z1enrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 12:36;mapohl;The changes of FLINK-30166 were reverted on {{{}master{}}}.

master: 974f884021ea0587ffa029b997a76487e9911a36;;;","09/Jan/23 13:01;martijnvisser;Figured it out: the S3 tests don't run on CI (only after merging) and I didn't change them correctly;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the limitation that parallelism decided by adaptive batch scheduler is always a power of two,FLINK-30604,13517200,13486602,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,09/Jan/23 08:19,12/Jan/23 14:00,04/Jun/24 20:41,12/Jan/23 14:00,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"In FLINK-26517, we add a limitation that the parallelism decided by adaptive batch scheduler must be 2^N(for the subpartitions can evenly distribute to downstream tasks). After FLINK-29666, we can evenly distribute data to downstream tasks, so we can remove the limitation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 14:00:39 UTC 2023,,,,,,,,,,"0|z1enjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 14:00;wanglijie;Done via master e57e5ee19af6313cdd48cdec61e2328338ebce86;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactActionITCase in table store is unstable,FLINK-30603,13517189,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,zjureel,zjureel,09/Jan/23 06:24,29/Mar/23 01:59,04/Jun/24 20:41,29/Mar/23 01:59,table-store-0.4.0,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"https://github.com/apache/flink-table-store/actions/runs/3927960511/jobs/6715071149

Error:  Failures: 
Error:    CompactActionITCase.testStreamingCompact:193 expected:<[+I 1|100|15|20221208, +I 1|100|15|20221209]> but was:<[+I 1|100|15|20221208]>
[INFO] 
Error:  Tests run: 221, Failures: 1, Errors: 0, Skipped: 4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 07:44:50 UTC 2023,,,,,,,,,,"0|z1engw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 07:32;lzljs3620320;[~TsReaper] CC;;;","10/Jan/23 10:39;TsReaper;master: b7188bcc46989c66e44f1fb04cd45972e1a6fe50
release-0.3: ad164c32fca33c365192b2025eaea61980d961eb;;;","16/Jan/23 07:44;zjureel;This test case is still unstable cc [~lzljs3620320]   [~TsReaper];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove FileStoreTableITCase in table store,FLINK-30602,13517179,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,09/Jan/23 05:11,09/Jan/23 06:41,04/Jun/24 20:41,09/Jan/23 06:41,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Remove `FileStoreTableITCase` in table store,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 06:41:41 UTC 2023,,,,,,,,,,"0|z1eneo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 06:41;lzljs3620320;master: b27fa51ed05bca3596ef1ba5131e97921ec01e33;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Omit ""setKeyContextElement"" call for non-keyed stream/operators to improve performance",FLINK-30601,13517174,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,09/Jan/23 04:18,17/Jan/23 06:30,04/Jun/24 20:41,17/Jan/23 06:30,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Currently, flink will set the correct key context(by call [setKeyContextElement|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/ChainingOutput.java#:~:text=input.setKeyContextElement(castRecord)%3B]) before processing each record, which is typically used to extract key from record and pass that key to the state backends.

However, the ""setKeyContextElement"" is obviously not need for non-keyed stream/operator, in which case we can omit the ""setKeyContextElement"" calls to improve performance. Note that ""setKeyContextElement"" is an interface method, it requires looking up the interface table when calling, which will further increase the method call overhead.
 
We run the following program as benchmark with parallelism=1 and object re-use enabled. The benchmark results are averaged across 5 runs for each setup. Before and after applying the proposed change, the average execution time changed from 88.39 s to 78.76 s, which increases throughput by 10.8%.
{code:java}
env.fromSequence(1, 1000000000L)
        .map(x -> x)
        .map(x -> x)
        .map(x -> x)
        .map(x -> x)
        .map(x -> x).addSink(new DiscardingSink<>());{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 06:30:05 UTC 2023,,,,,,,,,,"0|z1endk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 06:30;wanglijie;Done via master 4e3cd986cd4304c699d5c7d368ffd6e0ead5a096;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge flink-table-store-kafka to flink-table-store-connector,FLINK-30600,13517166,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,09/Jan/23 02:13,09/Jan/23 05:50,04/Jun/24 20:41,09/Jan/23 05:50,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"At present, Kafka heavily relies on the implementation of Flink, which is difficult to extract, so it can be directly incorporated into the Flink connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 05:50:11 UTC 2023,,,,,,,,,,"0|z1enbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 05:50;lzljs3620320;master: dd95fc47da7dad818c13bb178a7671db326088d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish SBOM for k8s operator,FLINK-30599,13517165,13516733,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,mbalassi,mbalassi,08/Jan/23 21:40,12/Jan/23 17:13,04/Jun/24 20:41,12/Jan/23 17:13,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,FLINK-30578 for the operator.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 17:13:15 UTC 2023,,,,,,,,,,"0|z1enbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 17:13;mbalassi;02f08c1 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong code generated for WatermarkGenerator because of inconsistent source type info when deserialized from exec plan,FLINK-30598,13517158,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,08/Jan/23 14:21,06/Jul/23 09:45,04/Jun/24 20:41,06/Jul/23 09:45,1.16.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When compile from an exist exec plan which contains watermark declaration and it referred the metadata column, the generated code for WatermarkGenerator maybe wrong

because currently `DynamicTableSourceSpec`.getTableSource passes the user defined schema to `SourceAbilitySpec` to perform optimization like projection/watermark pushdown, while optimization path from sql use a fixed reorder form: ""PHYSICAL COLUMNS + METADATA COLUMNS"", this may cause the problem.

a repro-case:

{code}
@Test
    public void testWatermarkPushDownWithMetadata() throws Exception {
        // to verify FLINK-: the case declares metadata field first, without fix it will get a
        // wrong code generated by WatermarkGeneratorCodeGenerator which reference the incorrect
        // varchar column as the watermark field.
        createTestValuesSourceTable(
                ""MyTable"",
                JavaScalaConversionUtil.toJava(TestData.data3WithTimestamp()),
                new String[] {
                    ""ts timestamp(3) metadata"",
                    ""a int"",
                    ""b bigint"",
                    ""c varchar"",
                    ""watermark for ts as ts - interval '5' second""
                },
                new HashMap<String, String>() {
                    {
                        put(""enable-watermark-push-down"", ""true"");
                        put(""readable-metadata"", ""ts:timestamp(3)"");
                    }
                });

        File sinkPath =
                createTestCsvSinkTable(
                        ""MySink"", ""a int"", ""b bigint"", ""c varchar"", ""ts timestamp(3)"");

        compileSqlAndExecutePlan(""insert into MySink select a, b, c, ts from MyTable where b = 3"")
                .await();

        assertResult(
                Arrays.asList(
                        ""4,3,Hello world, how are you?,"" + toLocalDateTime(4000L),
                        ""5,3,I am fine.,"" + toLocalDateTime(5000L),
                        ""6,3,Luke Skywalker,"" + toLocalDateTime(6000L)),
                sinkPath);
    }
{code}

the wrong code snippet(`row.getString(3)` should be a TimestampData):
{code}
public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
          
          org.apache.flink.table.data.binary.BinaryStringData field$19;
          boolean isNull$19;
          org.apache.flink.table.data.binary.BinaryStringData field$21;
          boolean isNull$22;
          org.apache.flink.table.data.TimestampData result$23;
          boolean isNull$24;
          org.apache.flink.table.data.TimestampData result$25;
          
          isNull$19 = row.isNullAt(3);
          field$19 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$19) {
            field$19 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(3));
          }
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:45:40 UTC 2023,,,,,,,,,,"0|z1ena0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 01:38;lincoln.86xy;This issue has several dependencies to be resolved, pending for a while.;;;","06/Jul/23 09:45;lincoln.86xy;fixed in master: d946eaac160748f39567818a221f25499fad84df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor FlinkProjectJoinTransposeRule and FlinkPruneEmptyRules,FLINK-30597,13517152,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Bo Cui,Bo Cui,08/Jan/23 10:00,19/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,pull-request-available,,,Delete the FlinkProjectJoinTransposeRule  and FlinkPruneEmptyRules that is the same as calcite in Flink.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:02 UTC 2023,,,,,,,,,,"0|z1en8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Multiple POST /jars/:jarid/run requests with the same jobId, runs duplicate jobs",FLINK-30596,13517089,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morezaei00,morezaei00,morezaei00,06/Jan/23 21:57,14/Jul/23 16:21,04/Jun/24 20:41,03/Jul/23 09:31,1.16.1,1.17.0,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,,,Runtime / REST,,,,,0,pull-request-available,,,,"Analysis from [~trohrmann]:

{quote}
The problem is the following: When submitting a job, then the {{Dispatcher}} will wait for the termination of a previous {{JobMaster}}. This is done to enable the proper cleanup of the job resources. In the initial submission case, there is no previous {{JobMaster}} with the same {{jobId}}. The problem is now that Flink schedules the [{{persistAndRunJob}}|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L571] action, which runs the newly submitted job, as [an asynchronous task|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1312-L1318]. This is done to ensure that the action is run on the {{Dispatcher}}'s main thread since the termination future can be run on a different thread. Due to this behaviour, there can be other tasks enqueued in the {{Dispatcher}}'s work queue which are executed before. Such a task could be another job submission which wouldn't see that there is already a job submitted with the same {{jobId}} since [we only do this in {{runJob}}|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L602] which is called by {{persistAndRunJob}}. This is the reason why you don't see a duplicate job submission exception for the second job submission. Even worse, this will eventually [lead to an invalid state|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L611-L615] and fail the whole cluster entrypoint.
{quote}

The following fix to the {{Dispatcher}} seems to fix the issue, but before submitting a PR, I wanted to post this for possible follow up discussions:

{code:language=java}
private CompletableFuture<Void> waitForTerminatingJob(
            JobID jobId, JobGraph jobGraph, ThrowingConsumer<JobGraph, ?> action) {
        ...
        return FutureUtils.thenAcceptAsyncIfNotDone(
                jobManagerTerminationFuture,
                getMainThreadExecutor(),
                FunctionUtils.uncheckedConsumer(
                    (ignored) -> {
                        jobManagerRunnerTerminationFutures.remove(jobId);
                        action.accept(jobGraph);
                    }));
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 08:39:59 UTC 2023,,,,,,,,,,"0|z1emuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:11;Weijie Guo;[~morezaei00] Are you still doing this work? If not, I would like to continue.;;;","02/Feb/23 18:53;morezaei00;Hey [~Weijie Guo], I'd opened a discussion on the dev listing for feedback on the change, but it appears that there are no objections/feedback.

I'll go ahead and create a PR for this today.;;;","08/Feb/23 17:39;morezaei00;I was hoping to shine some light on this issue given that it affects any application using the REST endpoints to run/submit jobs, causing confusion on the actual state of the submitted jobs in a cluster.

I've created a PR against the {{master}} branch, and I'd like to be able to port it back to {{1.16.2}} and {{1.15.4}} to close the loop on the issue for the last three major releases:
https://github.com/apache/flink/pull/21849

Could I get someone from the Runtime team to take a look at the PR?;;;","16/May/23 08:12;renqs;[~morezaei00] Any updates on this issue? Thanks;;;","28/Jun/23 15:45;chesnay;master: b528d9b81c03345c0415490fc41e27968313e5f0
1.17: 91e1314fedafc7a23fc239dcbb7907ec2b32d1bb
1.16: 9b073c97e5e758dcfda40bc064fadd5b52dd4327;;;","29/Jun/23 04:12;morezaei00;I've backported the fix for 1.17 and 1.16. Please see GitHub links.
;;;","29/Jun/23 08:39;chesnay;I've already started the backports and was just waiting for CI, closing the PRs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support create table like,FLINK-30595,13517054,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangjun,zhangjun,zhangjun,06/Jan/23 15:59,09/Jan/23 02:38,04/Jun/24 20:41,09/Jan/23 02:38,,,,,,,table-store-0.3.0,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,support CREATE TABLE LIKE ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:38:24 UTC 2023,,,,,,,,,,"0|z1emn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 02:38;lzljs3620320;master: 00efbde8a42259f76b55f3bfee96a447090e0fcd
release-0.3: ed96a7ca247e0b3bf7d02599f6bdca30bf7c31b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Java version because of JDK bug in the operator,FLINK-30594,13517046,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,06/Jan/23 15:07,06/Feb/23 12:51,04/Jun/24 20:41,06/Feb/23 12:47,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,1,pull-request-available,,,,"The following JDK bug is found during operator usage: https://bugs.openjdk.org/browse/JDK-8221218

This has been resolved in 11.0.18 which should be used in the operator base image.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 12:47:26 UTC 2023,,,,,,,,,,"0|z1emlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 15:20;gaborgsomogyi;I've double checked and the latest maven:3.8.6-openjdk-11 image contains still the problematic version.
{code:java}
openjdk version ""11.0.16"" 2022-07-19
OpenJDK Runtime Environment 18.9 (build 11.0.16+8)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.16+8, mixed mode, sharing)
{code}
;;;","06/Jan/23 18:02;gaborgsomogyi;In the meantime I've realized that the following determines the java version:
{code:java}
...
FROM openjdk:11-jre
...
{code}
On the other hand this is the latest available which is the mentioned not yet fixed version.
;;;","19/Jan/23 10:51;gaborgsomogyi;I've just double checked and seems like there is no new java version:

{code:java}
$ docker run -it openjdk:11-jre java -version
openjdk version ""11.0.16"" 2022-07-19
OpenJDK Runtime Environment 18.9 (build 11.0.16+8)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.16+8, mixed mode, sharing)
{code}
;;;","06/Feb/23 04:18;xxxinli1;[~gaborgsomogyi]  The OpenJDK images have announced deprecations.  How about turning the base image into Eclipse or other distributions?

 

[openjdk - Official Image | Docker Hub|https://hub.docker.com/_/openjdk] 

[eclipse-temurin Tags | Docker Hub|https://hub.docker.com/_/eclipse-temurin/tags?page=1&name=11];;;","06/Feb/23 11:49;gaborgsomogyi;[~xxxinli1] it makes sense, opened a PR.;;;","06/Feb/23 12:47;gyfora;merged to main f9024a7fe09492957d3708079bf28227af54f5c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Determine restart time on the fly for Autoscaler,FLINK-30593,13517039,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,gyfora,gyfora,06/Jan/23 12:16,10/Jan/24 11:58,04/Jun/24 20:41,24/Nov/23 16:50,,,,,,,kubernetes-operator-1.8.0,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,pull-request-available,,,,Currently the autoscaler uses a preconfigured restart time for the job. We should dynamically adjust this on the observered restart times for scale operations.,,,,,,,,,,,,,,,,FLINK-33789,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 06:56:48 UTC 2023,,,,,,,,,,"0|z1emjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/23 06:56;nicholasjiang;[~gyfora], does this reply on the stop time of the scale operations? JobStatus only records the start time of the scale operations and doesn't record the stop time at present, which cause that we could not get the restart times from the scaling history.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The unsupported hive version is not deleted on the hive overview document,FLINK-30592,13517031,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chrismartin,chrismartin,chrismartin,06/Jan/23 10:43,11/Jan/23 02:09,04/Jun/24 20:41,09/Jan/23 02:44,1.16.0,1.17.0,,,,,1.16.1,1.17.0,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,"flink 1.16.0 drop support for Hive versions 1.*, 2.1.* and 2.2.* which are no longer supported by the Hive community,but overview document  was not remove these versions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:44:59 UTC 2023,,,,,,,,,,"0|z1emi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 02:44;jark;Fixed in 
 - master: ca822525acadcd6c2c3ba9b49783b5cdfa738261
 - release-1.16: 997365ebc70930c423767546d90cba44ae2df769;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateful functions SDK for Flink DataStream Integration example is not available,FLINK-30591,13517025,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zentopia,Zentopia,06/Jan/23 09:51,06/Jan/23 09:57,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Stateful Functions,,,,,0,,,,," 

[https://nightlies.apache.org/flink/flink-statefun-docs-release-3.2/docs/sdk/flink-datastream/]

!image-2023-01-06-17-50-21-013.png!

 

!image-2023-01-06-17-49-04-381.png!

 

The 'apache/flink-statefun' repository doesn't contain the 'statefun-examples/statefun-flink-datastream-example/src/main/java/org/apache/flink/statefun/examples/datastream/Example.java' path in 'master'.  
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/23 09:49;Zentopia;image-2023-01-06-17-49-04-381.png;https://issues.apache.org/jira/secure/attachment/13054404/image-2023-01-06-17-49-04-381.png","06/Jan/23 09:50;Zentopia;image-2023-01-06-17-50-21-013.png;https://issues.apache.org/jira/secure/attachment/13054403/image-2023-01-06-17-50-21-013.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-06 09:51:59.0,,,,,,,,,,"0|z1emgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove set default value manually for table options,FLINK-30590,13517019,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,06/Jan/23 09:38,29/Mar/23 01:46,04/Jun/24 20:41,29/Mar/23 01:46,table-store-0.4.0,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,Remove set default value manually in `CoreOptions.setDefaultValues` which may cause wrong error information and it's not needed anymore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 08 12:07:06 UTC 2023,,,,,,,,,,"0|z1emfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 11:52;nicholasjiang;[~zjureel], IMO, it's needed for users to reduce the user-defined configurations, for example, setting `scan.timestamp-millis` means that the scan mode is FROM_TIMESTAMP, not the default value `DEFAULT` of the scan mode. The manual `CoreOptions.setDefaultValues` helps users to set the default value of the certain configuration. What default value may cause wrong error information? 

`;;;","08/Jan/23 12:07;zjureel;[~nicholasjiang] Currently `CoreOptions.startupMode` will check the special configuration and determine the scan mode. `CoreOptions.setDefault` is duplicated

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot expiration should be skipped in Table Store dedicated writer jobs,FLINK-30589,13517016,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,06/Jan/23 09:17,09/Jan/23 02:08,04/Jun/24 20:41,09/Jan/23 02:08,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.0,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Currently Table Store dedicated writer jobs will also expire snapshots. This may cause conflicts when multiple writer jobs are running.

We should expire snapshots only in dedicated compact job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:08:01 UTC 2023,,,,,,,,,,"0|z1emeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 02:08;lzljs3620320;master: 8365182ae1b715508b3a24ed3911f2dfa244b17b
release-0.3: 83732d283a215c537a5a22e4a7df4983f7630f33;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improvement in truncate code in the new ABFSOutputstream ,FLINK-30588,13517014,13316143,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ram_krish,ram_krish,06/Jan/23 08:55,06/Jan/23 08:56,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,,,,,"There are few TODOs in the new ABFSOutputstream. 

1) Currently the buffer to read and do the truncate operation from the source file is 2MB. This we can allow some configuration based on the future usage and perf results.

2) Currently while writing to temp file we write the entire data and then finally close/flush the data. Based on usages we can call intermediate flush/sync on the files. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-06 08:55:08.0,,,,,,,,,,"0|z1eme8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate primary key in an append-only table in ddl,FLINK-30587,13517008,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,06/Jan/23 08:27,09/Jan/23 02:09,04/Jun/24 20:41,09/Jan/23 02:09,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Current table store check primary key in an append-only table, it should be checked in catalog table too",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:09:37 UTC 2023,,,,,,,,,,"0|z1emcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 02:09;lzljs3620320;master: 50b52db946232f3474bfcdedc8e38c67985a0cac
release-0.3: 525866c1711a7aa1b96d60e1b3e7c6786b89f1e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix calcCodeGen failed if calc with like condition contains double quotation mark,FLINK-30586,13517006,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,06/Jan/23 08:23,30/Apr/24 02:35,04/Jun/24 20:41,10/Jan/23 06:24,1.16.0,,,,,,1.16.1,1.17.0,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"If I write a sql like ""SELECT * FROM MyTable WHERE b LIKE '%""%'"" in Flink-1.16 as

'like' condition contains double quotation mark, it will cause code gen failed because wrong code generated by codeGen. 

!code-gen-1.png!

 

!code-gen-2.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:23;337361684@qq.com;code-gen-1.png;https://issues.apache.org/jira/secure/attachment/13054398/code-gen-1.png","06/Jan/23 08:23;337361684@qq.com;code-gen-2.png;https://issues.apache.org/jira/secure/attachment/13054397/code-gen-2.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 06:24:30 UTC 2023,,,,,,,,,,"0|z1emcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:30;luoyuxia;Similar to FLINK-29651;;;","10/Jan/23 06:24;godfrey;Fixed in 1.17.0: 68b37fb867374df5a201f0b170e35c21266e5d7b

1.16.1: 54518e9e27c8cc17f27b5d9a4de48e71cd817e42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve flame graph performance at subtask level,FLINK-30585,13517001,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,fanrui,fanrui,fanrui,06/Jan/23 07:21,09/Jun/23 09:17,04/Jun/24 20:41,09/Jun/23 09:17,1.17.0,,,,,,1.18.0,,,,,,,,,,,,,,,,Runtime / Task,Runtime / Web Frontend,,,,0,pull-request-available,,,,"After FLINK-30185 , we can view the flame graph of subtask level. However, it always collects flame graphs for all subtasks.

We should collect the flame graph of single subtask instead of all subtasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 09:16:25 UTC 2023,,,,,,,,,,"0|z1embc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 09:59;pnowojski;[~fanrui] do I understand this correctly? Is this just a performance optimisation? Currently we are collecting flamegraphs from all threads and all subtasks (across all TMs?), but then somewhere we are filtering out all but one selected subtask?;;;","23/Mar/23 10:13;fanrui;Hi [~pnowojski] , you are totally right, it's just a performance optimization.

And I found a problem, when the parallelism is very high, the collection of flame graphs will fail.So, this optimization is necessary. I will do this optimization next month.;;;","09/Jun/23 09:16;fanrui;<master: 1.18> d857cfda152a5073be07890a5d63855e0fa22639;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the flame graph doc of subtask level,FLINK-30584,13517000,13516999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,fanrui,fanrui,fanrui,06/Jan/23 07:17,09/Jan/23 10:55,04/Jun/24 20:41,09/Jan/23 08:14,,,,,,,1.17.0,,,,,,,,,,,,,,,,Documentation,,,,,0,pull-request-available,,,,Update the flame graph doc of subtask level,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 08:14:49 UTC 2023,,,,,,,,,,"0|z1emb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 08:14;xtsong;master (1.17): b4810b5a0fdbafafdd089c621e9f68c2f2800c76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide the flame graph to the subtask level,FLINK-30583,13516999,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,06/Jan/23 07:11,23/Mar/23 10:00,04/Jun/24 20:41,23/Mar/23 10:00,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / REST,Runtime / Web Frontend,,,,0,,,,,This is a umbrella Jira about providing the flame graph to the subtask level.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 07:47:16 UTC 2023,,,,,,,,,,"0|z1emaw:",9223372036854775807,"Starting with Flink 1.17, Flame Graph provides ""drill down"" visualizations to the task level. Select a subtask of interest, and user can see the flame graph of the corresponding subtask.",,,,,,,,,,,,,,,,,,,"06/Jan/23 08:51;fanrui;Hi [~xtsong] , could you please assign these sub-tasks to me in your free time? thanks :)

 

And please let me know if I missed some tasks.;;;","06/Jan/23 09:03;xtsong;Thanks [~fanrui], you are assigned.

Just to managed the expectation: As we are approaching the 1.17 feature freeze date, there's an increasing code-review workload. The upcoming holidays should also be considered. Therefore, I may or may not have the capacity to help with the code-review of these sub-tasks in the 1.17 release cycle.;;;","06/Jan/23 09:08;fanrui;Hi [~xtsong] , thanks for your quick feedback.

 

The subtask2 is about official doc, it help flink users to use this feature, it should be finished in 1.17, and the subtask3 is improvement, it can be finished in 1.18. What do you think?;;;","06/Jan/23 09:10;xtsong;Sounds good to me.;;;","06/Jan/23 09:13;fanrui;Thanks, I will finish the subtask2 asap:);;;","24/Feb/23 07:47;pnowojski;Thanks for this improvement! I've just found it in the WebUI and docs :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-avro Flink-orc free for flink-table-store-format,FLINK-30582,13516992,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,06/Jan/23 05:38,06/Jan/23 09:29,04/Jun/24 20:41,06/Jan/23 09:29,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 09:29:53 UTC 2023,,,,,,,,,,"0|z1em9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 09:29;lzljs3620320;master: b3474aff36a661677e22d56e627ef227390c2689;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate FileStoreTableITCase and use CatalogITCaseBase,FLINK-30581,13516890,13516872,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengwang,lzljs3620320,lzljs3620320,06/Jan/23 03:12,09/Jan/23 02:04,04/Jun/24 20:41,09/Jan/23 02:03,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"We recommend users to use Catalog tables instead managed tables.
Managed tables should be deprecated. Now we already did not expose managed in documentation. We can remove it.
Before removing, tests should be refactored.

FileStoreTableITCase with managed tables should be changed to CatalogITCaseBase.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 02:03:50 UTC 2023,,,,,,,,,,"0|z1elmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 02:03;lzljs3620320;master: 8a20d9337ef6135730bdb63a8af902ead6855a7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[umbrella] Refactor tests for table store,FLINK-30580,13516872,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,06/Jan/23 03:02,29/Mar/23 01:59,04/Jun/24 20:41,29/Mar/23 01:59,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,This is a umbrella issue to improve tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-06 03:02:56.0,,,,,,,,,,"0|z1elio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introducing cofigurable option to enable hive native function,FLINK-30579,13516797,13488604,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,lsy,lsy,lsy,06/Jan/23 02:23,17/Jan/23 10:14,04/Jun/24 20:41,17/Jan/23 10:14,1.16.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Currently, hive native function implementation can't assign behavior with hive udaf, so we should introduce an configurable option to allow enable this optimization, the default behavior is disabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 10:14:12 UTC 2023,,,,,,,,,,"0|z1el20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 10:14;fsk119;Mereged into master: 95869e0bc99f9703807a114a4240dbc443d970cb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish SBOM artifacts,FLINK-30578,13516733,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dongjoon,dongjoon,dongjoon,06/Jan/23 00:25,14/Mar/24 07:36,04/Jun/24 20:41,,1.17.0,,,,,,1.20.0,,,,,,,,,,,,,,,,Build System,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AVRO-3700,PARQUET-2224,ORC-1342,SPARK-41893,FLINK-30252,FLINK-29866,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 22:35:23 UTC 2023,,,,,,,,,,"0|z1ekns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 21:13;mbalassi;9bb6500 in master;;;","09/Jan/23 21:14;mbalassi;Keeping it open until the sub-tasks (smaller relevant repos under the Flink umbrella) get added too.;;;","09/Jan/23 23:21;dongjoon;Thank you. Got it, [~bamrabi].;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenShift FlinkSessionJob artifact write error on non-default namespaces,FLINK-30577,13516721,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tagarr,jbusche,jbusche,05/Jan/23 20:45,30/Jan/23 09:06,04/Jun/24 20:41,30/Jan/23 09:06,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"[~tagarr] has pointed out an issue with using the /opt/flink/artifacts filesystem on OpenShift in non-default namespaces.  The OpenShift permissions don't allow write to /opt.  
```
org.apache.flink.util.FlinkRuntimeException: Failed to create the dir: /opt/flink/artifacts/jim/basic-session-deployment-only-example/basic-session-job-only-example
```
A few ways to solve the problem are:
1. Remove the comment on line 34 here in [flink-conf.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/conf/flink-conf.yaml#L34] and change it to: /tmp/flink/artifacts

2. Append this after line 143 here in [values.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/values.yaml#L142]:
kubernetes.operator.user.artifacts.base.dir: /tmp/flink/artifacts

3.  Changing it in line 142 of [KubernetesOperatorConfigOptions.java|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java#L142] like this:
.defaultValue(""/tmp/flink/artifacts"") 
and then rebuilding the operator image.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 09:06:59 UTC 2023,,,,,,,,,,"0|z1ekl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 20:57;gyfora;Or users on open shift could simply configure the users.artifact.base.dir to something other than the default:) ;;;","06/Jan/23 05:30;jbusche;So document that they should edit the configmap and restart the operator? I just tried that and it worked:

 

oc edit cm flink-operator-config -n openshift-operators
{quote}change:

    # kubernetes.operator.user.artifacts.base.dir: /opt/flink/artifacts

to

    kubernetes.operator.user.artifacts.base.dir: /tmp/flink/artifacts
{quote}
 

Restart the operator pod:

oc get pods -n openshift-operators
{quote}NAME                                         READY   STATUS    RESTARTS   AGE

flink-kubernetes-operator-5f5bb584db-t75ts   2/2     Running   0          4m45s
{quote}
oc delete pod flink-kubernetes-operator-5f5bb584db-t75ts -n openshift-operators

 

Then in a non-default namespace the session jobs work:

oc get flinksessionjobs -n jim
{quote}NAME                             JOB STATUS   RECONCILIATION STATUS

basic-session-job-example        RUNNING      DEPLOYED

basic-session-job-example2       RUNNING      DEPLOYED

basic-session-job-only-example   RUNNING      DEPLOYED
{quote};;;","06/Jan/23 08:06;gyfora;I just wanted to point out this possiblity as well, I think if this causes problems in some envs we should simply change the default config value for that.
But we should consider how this will impact existing operator deployments.;;;","16/Jan/23 20:05;gyfora;[~jbusche] are you planning to work on this before the 1.4.0 release?;;;","17/Jan/23 06:03;jbusche;I hadn't heard back from [~tagarr] to see if documentation was enough, or if he wanted to pursue the change.  What's the timeline for 1.4.0?  I'll message him again now... thanks!;;;","18/Jan/23 09:45;gyfora;Target 1.4.0 feature freeze is early February ;;;","19/Jan/23 13:26;tagarr;So the issue is not just with olm, but also with the helm operator. One simple fix would be to add an emptyDir volume mount for /opt/flink/artifacts to the helm charts in flink-operator.yaml. Another workaround on openshift would be to add the anyuid scc onto the operator install namespace i.e. run 

oc adm policy add-scc-to-group anyuid system:serviceaccounts:<NAMESPACE>

 

 ;;;","19/Jan/23 13:26;tagarr;[~gyfora] you can assign the issue to me;;;","24/Jan/23 23:40;jbusche;I've switched to your branch [~tagarr] and then done the following:
 # Created a new docker image using the Dockerfile
 # Installed helm both in default and later in the flink namespaces using your code and the new docker image.
 # Deployed a sessionjob and flinkdep, they looked good.

Next, I cleaned up and created an OLM bundle, deployed it to openshift-operators.
 # Deployed sessionjob and flinkdeps to two different namespaces, they looked good.

Last, I deployed the operator to a specific namespace: jim2
 # Deployed sessionjob and flinkdeps to two different namespaces, they looked good.

In all cases, the operator log looked clean, and had entries like this:
{quote}oc logs -f flink-kubernetes-operator-86b7655f65-pj9x7 |grep artifacts

2023-01-24 23:38:25,401 o.a.f.k.o.a.ArtifactManager    [INFO ][jim2/basic-session-job-only-example] Created dir: /opt/flink/*artifacts*/jim2/basic-session-deployment-only-example/basic-session-job-only-example
{quote};;;","30/Jan/23 09:06;gyfora;merged to main 9441b1145ba05845e15b26b517d9b9f0732c9c1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcOutputFormat refactor,FLINK-30576,13516653,13468865,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,eskabetxe,eskabetxe,05/Jan/23 14:38,08/Jan/23 12:19,04/Jun/24 20:41,08/Jan/23 12:19,,,,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"This refactor is to allow the use of JdbcOutputFormat on Sink2

Actually the JdbcOutputFormat needs the RuntimeContext to check if ObjectReuse is active or not..

The refactor is for change from RuntimeContext to ExecutionConfig (we still need that ExecutionConfig be available on Sink2.InitContext, and a FLIP will be raised)

 

[~wanglijie] this is what we talk about",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 02:22:44 UTC 2023,,,,,,,,,,"0|z1ek60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 15:37;wanglijie;[~eskabetxe] I think we don’t need to create a separate ticket/pr for the refactor. It should be a part of FLINK-28284
 ;;;","05/Jan/23 17:04;eskabetxe;Moved to sub-task..

Or you prefer all changes on same PR;;;","06/Jan/23 02:22;wanglijie;I think all changes on the same PR is better, a separate commit is enough.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler doesn't scale down on 0 load,FLINK-30575,13516637,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,gyfora,gyfora,05/Jan/23 12:29,31/Mar/23 13:01,04/Jun/24 20:41,06/Mar/23 18:07,,,,,,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,pull-request-available,,,,"The logic for computing true processing rates rely on recordsProcessed / recordProcessingTime style computations which do not really work well when everything is 0.

This leads to no scaling actions when the load suddenly drops to 0. We should handle these special cases and scale the pipeline to some minimal parallelism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-05 12:29:19.0,,,,,,,,,,"0|z1ek2g:",9223372036854775807,Ensure vertices with zero incoming records are scaled down,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not scale further up if last scaling was ineffective,FLINK-30574,13516626,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,05/Jan/23 10:00,12/Jan/23 13:51,04/Jun/24 20:41,12/Jan/23 13:51,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,pull-request-available,,,,"Related to https://issues.apache.org/jira/browse/FLINK-30571 but somewhat simpler. 

It should be easy to detect when a scale up operation did not significantly increase the parallelism (or even deteriorated it). In these cases it is very important to not try to scale further.

This can happen if the job hit some external limit (such as rate limiting on an external service) or if the performance problems are caused by other factors such as memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 13:51:22 UTC 2023,,,,,,,,,,"0|z1ek00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 13:51;gyfora;merged to main ed6acd453c6e14f7fdb3295c6f0d0567de6acbd8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store dedicated compact job may skip some records when checkpoint interval is long,FLINK-30573,13516624,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,05/Jan/23 09:53,06/Jan/23 08:27,04/Jun/24 20:41,06/Jan/23 08:27,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.0,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Currently the sink for Table Store dedicated compact job only receives records about what buckets are changed, instead of what files are changed. If the writer is kept open, new files of this bucket may be skipped.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 08:27:52 UTC 2023,,,,,,,,,,"0|z1ejzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:27;TsReaper;master: 8fdbbd4e8c22a82d79509cab0add4a6aa7331672
release-0.3: 1ba8f372a05d64e7370f7dcbb65b77e23079f8ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make parquet as default data file format,FLINK-30572,13516621,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,05/Jan/23 09:27,11/Apr/23 03:36,04/Jun/24 20:41,29/Mar/23 02:07,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"- We have done some tests. Parquet is 30% faster.
- After FLINK-30565, Parquet can support complex types and file systems such as OSS and s3 (decoupled from hadoop filesystem).
- After FLINK-30569, the table can switch formats at will.

Therefore, if detailed and comprehensive tests have been carried out here, we can use Parquet as the default format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-05 09:27:00.0,,,,,,,,,,"0|z1ejyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compute scale parallelism based on observed scalability ,FLINK-30571,13516618,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gyfora,gyfora,05/Jan/23 09:17,24/Feb/23 14:58,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,,0,,,,,"When computing target parallelism for job vertices we currently assume linear scaling with a fixed (1) coefficient.

This assumes that in order to double the capacity we simply double the parallelism.

While linearity already might be violated by many real time workloads this form of strong linearity rarely holds due to the overhead of increased network traffic, coordination etc.

As we can access past (parallelism, processingRate) information based on the scaling history we should estimate the scalability coefficient either using a simple or weighted linear regression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-05 09:17:12.0,,,,,,,,,,"0|z1ejy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RexNodeExtractor#isSupportedPartitionPredicate generates unexpected partition predicates,FLINK-30570,13516617,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,05/Jan/23 09:15,19/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,pull-request-available,,,"Currently, the condition {{where rand(1) < 0.0001}} will be recognized as a partition predicates and will be evaluated to false when compiling the SQL. It has two problem. 
First, it should not be recognized as a partition predicates, and the nondeterministic function should never pass the partition pruner ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:02 UTC 2023,,,,,,,,,,"0|z1ejy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 12:13;aitozi;I opened a PR to solve this problem, looking forward to be reviewed, thanks;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File Format can not change with data file exists,FLINK-30569,13516587,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Jan/23 06:56,06/Jan/23 01:44,04/Jun/24 20:41,06/Jan/23 01:44,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"# Set file format to orc
# Write records.
# Set file format to parquet.
# Write records
# Read -> throw exception...

We should support change file format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 01:44:17 UTC 2023,,,,,,,,,,"0|z1ejrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 01:44;lzljs3620320;master: 63a27cd7af945839f67afaf6e946bcf617ad18a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add benchmark for PolyNomialExpansion, Normalizer, Binarizer, Interaction, MaxAbsScaler, VectorSlicer, ElementWiseProduct and Featurehasher",FLINK-30568,13516573,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hotsuns,hotsuns,05/Jan/23 04:22,19/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:02 UTC 2023,,,,,,,,,,"0|z1ejo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong insert overwrite behavior when  the table contains uppercase character with Hive dialect,FLINK-30567,13516572,13510724,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,05/Jan/23 04:16,01/Feb/23 13:45,04/Jun/24 20:41,30/Jan/23 03:28,1.16.0,1.16.1,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"If use hive dialect, the data would be wrong if we rerun the following job multiple tables.
{code:java}
INSERT overwrite TABLE db_name.target_tB_name partition(p_date = '20230101')
select 
author_id
from db_name.source_tb_name
WHERE p_date = '20230101'
and author_id <> '0'{code}
The framework has a minor bug when determining whether an override is required if the target name contains uppercase character.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 10:34:19 UTC 2023,,,,,,,,,,"0|z1ejo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 04:17;luoyuxia;Introduced by FLINK-29337, we miss to convert the table name into lowercase when check the sql statement is whether for overwrite.;;;","30/Jan/23 03:28;jingzhang;Fixed in master: 06dc85aa67d7abb9e44102544588eaf1c292b93b

Fixed in release-1.16: 63bbd14d24525a94d125fe599900505e4bca74e1;;;","30/Jan/23 03:32;jingzhang;[~luoyuxia] Since the bug is introduced since 1.16. How about cp the pr to 1.16 branch?;;;","30/Jan/23 12:45;luoyuxia;[~jingzhang] Yes, the backport pr is in [https://github.com/apache/flink/pull/21781];;;","31/Jan/23 11:16;mapohl;[~luoyuxia] there's a remote branch {{remotes/origin/revert-21781-release-1.16-FLINK-30567}} created in the Apache Flink repo. I guess, that's created accidentally when creating a revert through Github? May you clean the branch up if that's the case?;;;","01/Feb/23 01:30;luoyuxia;[~mapohl] Thanks for reminder. I think it's created accidentally. But seems I have no right to clean the branch up. Seems the branch is created by [~jingzhang], may [~jingzhang] help check and clean it up?;;;","01/Feb/23 02:56;jingzhang;[~mapohl] [~luoyuxia] It's my fault. Sorry.

I've already cleaned the branch.;;;","01/Feb/23 10:34;mapohl;No worries. Thanks :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add benchmark configurations for agglomerativeclustering, hashingtf, idf, kbinsdiscretizer, linearregression, linearsvc,  logisticregression, ngram, regextokenizer, tokenizer and vectorindexer",FLINK-30566,13516570,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhangzp,zhangzp,zhangzp,05/Jan/23 04:03,19/Apr/23 01:43,04/Jun/24 20:41,19/Apr/23 01:43,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"Add benchmark configurations for the following algorithms:
 * agglomerativeclustering
 * hashingtf
 * idf
 * kbinsdiscretizer
 * linearregression
 * linearsvc
 * logisticregression
 * ngram
 * regextokenizer
 * tokenizer
 * vectorindexer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:43:10 UTC 2023,,,,,,,,,,"0|z1ejnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:43;lindong;Merged to apache/flink-ml master branch fc0b154f02d77a83e61033e30c84fd928a29216f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-parquet free for flink-table-store-format,FLINK-30565,13516564,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Jan/23 03:04,05/Jan/23 11:50,04/Jun/24 20:41,05/Jan/23 11:50,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 11:50:57 UTC 2023,,,,,,,,,,"0|z1ejm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 11:50;lzljs3620320;master: 1f7fad954383af5840d1da68992ad80a9cf82a66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select from a new table with Kafka LogStore crashes with UnknownTopicOrPartitionException,FLINK-30564,13516540,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Gerrrr,Gerrrr,04/Jan/23 23:34,29/Mar/23 01:44,04/Jun/24 20:41,29/Mar/23 01:44,,,,,,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"Selecting from newly created table that uses Kafka as a Log Store creates a job that crash-loops with {{UnknownTopicOrPartitionException: This server does not host this topic-partition}} exception. This happens because neither {{CREATE TABLE}} nor {{SELECT FROM}} create the underlying topic. 

Steps to reproduce:
{noformat}
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH (
    'connector' = 'table-store',
    'path' = 's3://my-bucket/table-store',
    'log.system' = 'kafka',
    'kafka.bootstrap.servers' = 'broker:9092',
    'kafka.topic' = 'word_count_log',
    'auto-create' = 'true',
    'log.changelog-mode' = 'all',
    'log.consistency' = 'transactional'
);

SELECT * FROM word_count; {noformat}
 

JM logs:
{noformat}
flink          | 2023-01-04 23:27:24,292 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext [] - Exception while handling result from async call in SourceCoordinator-Source: word_count[1]. Triggering job failover.
flink          | org.apache.flink.util.FlinkRuntimeException: Failed to list subscribed topic partitions due to
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.KafkaSourceEnumerator.checkPartitionChanges(KafkaSourceEnumerator.java:234) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83) ~[flink-dist-1.16.0.jar:1.16.0]
flink          |     at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-dist-1.16.0.jar:1.16.0]
flink          |     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_352]
flink          |     at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_352]
flink          |     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_352]
flink          |     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_352]
flink          |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352]
flink          |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352]
flink          |     at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352]
flink          | Caused by: java.lang.RuntimeException: Failed to get metadata for topics [word_count_log].
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.subscriber.KafkaSubscriberUtils.getTopicMetadata(KafkaSubscriberUtils.java:47) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.subscriber.TopicListSubscriber.getSubscribedTopicPartitions(TopicListSubscriber.java:52) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.KafkaSourceEnumerator.getSubscribedTopicPartitions(KafkaSourceEnumerator.java:219) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80) ~[flink-dist-1.16.0.jar:1.16.0]
flink          |     ... 7 more
flink          | Caused by: java.util.concurrent.ExecutionException: org.apache.flink.table.store.shaded.org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. {noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:43:59 UTC 2023,,,,,,,,,,"0|z1ejgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 01:43;lzljs3620320;https://github.com/apache/incubator-paimon/issues/732;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update training exercises to use Flink 1.16,FLINK-30563,13516498,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,tanyuxin,danderson,danderson,04/Jan/23 16:48,04/Feb/23 00:26,04/Jun/24 20:41,04/Feb/23 00:26,1.16.0,,,,,,,,,,,,,,,,,,,,,,Documentation / Training / Exercises,,,,,0,pull-request-available,,,,"The training exercises in the [flink-training|https://github.com/apache/flink-training] repo need to be updated to use Flink 1.16.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 04 00:26:39 UTC 2023,,,,,,,,,,"0|z1ej7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 03:13;tanyuxin;Thanks [~danderson] for reporting this. I want to take a look at this.;;;","04/Feb/23 00:26;danderson;Implemented with

412283a43682cdbef4dddf46257f18c6750787df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CEP Operator misses patterns on SideOutputs and parallelism >1 since 1.15.x+,FLINK-30562,13516482,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jamalarm,Jamalarm,04/Jan/23 14:15,05/Jan/23 17:36,04/Jun/24 20:41,,1.15.3,1.16.0,,,,,,,,,,,,,,,,,,,,,API / DataStream,Library / CEP,,,,1,,,,,"(Apologies for the speculative and somewhat vague ticket, but I wanted to raise this while I am investigating to see if anyone has suggestions to help me narrow down the problem.)

We are encountering an issue where our streaming Flink job has stopped working correctly since Flink 1.15.3. This problem is also present on Flink 1.16.0. The Keyed CEP operators that our job uses are no longer emitting Patterns reliably, but critically *this is only happening when parallelism is set to a value greater than 1*. 

Our local build tests were previously set up using in-JVM `MiniCluster` instances, or dockerised Flink clusters all set with a parallelism of 1, so this problem was not caught and it caused an outage when we upgraded the cluster version in production.

Observing the job using the Flink console in production, I can see that events are *arriving* into the Keyed CEP operators, but no Pattern events are being emitted out of any of the operators. Furthermore, all the reported Watermark values are zero, though I don't know if that is a red herring as it seems Watermark reporting seems to have changed since 1.14.x.

I am currently attempting to create a stripped down version of our streaming job to demonstrate the problem, but this is quite tricky to set up. In the meantime I would appreciate any hints that could point me in the right direction.

I have isolated the problem to the Keyed CEP operator by removing our real sinks and sources from the failing test. I am still seeing the erroneous behaviour when setting up a job as:

# Events are read from a list using `env.fromCollection( ... )`
# CEP operator processes events
# Output is captured in another list for assertions

My best guess at the moment is something to do with Watermark emission? There seems to have been changes related to watermark alignment, perhaps this has caused some kind of regression in the CEP library? To reiterate, *this problem only occurs with parallelism of 2 or more. Setting the parallelism to 1 immediately fixes the issue*","Problem observed in:

Production:
Dockerised Flink cluster running in AWS Fargate, sourced from AWS Kinesis and sink to AWS SQS

Local:
Completely local MiniCluster based test with no external sinks or sources",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/23 17:35;Jamalarm;flink-asf-30562-clean.zip;https://issues.apache.org/jira/secure/attachment/13054379/flink-asf-30562-clean.zip",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 17:36:04 UTC 2023,,,,,,,,,,"0|z1ej40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 15:21;bgeng777;hi [~Jamalarm], thanks for the report. I tried to reproduce your problem with flink 1.16.0 using standalone cluster on my computer(my demo using event time can be found [here|https://github.com/bgeng777/ververica-cep-demo/tree/FLINK-30562]). I agree there are some difference in stdout when setting parallism to 2 comparing with setting it 1. But it seems that thre result is corrent. 
In my demo, when p is 2, the stdout of the matches is:
{quote}1> 3,3,3
1> 2,2,2{quote}
when p is 1:
{quote}3,3,3
2,2,2{quote}
Is above result the same with your experiments?;;;","04/Jan/23 15:32;Jamalarm;Hi [~bgeng777], thanks for the quick response. Your demo is roughly the same as the one I'm trying to set up to reproduce the issue in a compact way. I will use it for guidance to see if I can get something useful available. 

My experiments are showing:

*Flink Versions 1.4.3, parallelism: any*
CEP operators produce expected output

*Flink Versions 1.5.x+, parallelism: 1*
CEP operators produce expected output

*Flink Versions 1.5.x+, parallelism: 2+*
CEP operators produce no output at all

It's worth noting that we did not change any code related to our CEP usage between these tests, we simply updated the library versions.

We are using more pattern constraints than exist in your test file, I'm wondering if it might be related to one of those. For example, we use "".within(...)"" and "".times(...)"" on most of our Pattern definitions.;;;","05/Jan/23 15:43;Jamalarm;Hi [~bgeng777]

I've made some progress in narrowing down the problem. I am still working on producing a reproducible code snippet I can share, but the problem is definitely related to *Side Outputs*.

For context, we use Side Outputs to route events to different CEP operators depending on a Customer ID value (different customers are interested in different CEP sequences). We previously used the {{.split()}} operator before it was deprecated.

We set up the side outputs with a call like this (I have dramatically simplified the code but the problem is still occurring with the code in this form):

{code:java}
streamWithSideOutputs = stream.process(new BrandedSideOutputFunction());

// Where the side output function ...

    public static class BrandedSideOutputFunction extends ProcessFunction<PlatformEvent, PlatformEvent> {

        private final OutputTag<PlatformEvent> outputTag = new OutputTag<>(""RED_BRAND"", TypeInformation.of(PlatformEvent.class));

        @Override
        public void processElement(PlatformEvent value, Context ctx, Collector<PlatformEvent> out) {
            ctx.output(outputTag, value);
            out.collect(value);
        }
    }
{code}

You'll note that obviously this side output function only actually outputs to one, hardcoded side output. The real code is more complex but as I say, the problem still occurs with the code as written above.

With this {{.process(...)}} call upstream of the CEP operators, and the {{parallelism}} set to a value greater than 1, the Patterns will fail to be detected roughly 1/3rd of the time. Note that this happens even if I connect the CEP operator to either the *main* {{DataStream}} or to a side output via {{.getSideOutput(tag)}}.

If the {{parallelism}} is set to 1, or if I remove the side-output generating {{.process(...)}} call and connect the CEP operator directly to the existing {{DataStream}}, the Patterns will be detected 100% of the time.

There seems to be something up with the interaction between side outputs, parallelism and the CEP operator in Flink 1.15.0+.

I will keep working on producing a project I can share reproducing this problem, but hopefully this gives you something to go on?
;;;","05/Jan/23 17:36;Jamalarm; [^flink-asf-30562-clean.zip] 

I've produced a (relatively) simple project here that reproduces the problem. Please let me know if you have any questions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogStreamHandleReaderWithCache cause FileNotFoundException,FLINK-30561,13516470,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,04/Jan/23 12:49,10/Feb/23 12:21,04/Jun/24 20:41,10/Feb/23 12:21,1.16.0,1.17.0,,,,,1.16.2,1.17.0,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"When a job with state changelog enabled continues to restart, the following exceptions may occur :
{code:java}
java.lang.RuntimeException: java.io.FileNotFoundException: /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671689962742_1333392/dstl-cache-file/dstl6215344559415829831.tmp (No such file or directory)
    at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
    at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94)
    at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671689962742_1333392/dstl-cache-file/dstl6215344559415829831.tmp (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:158)
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:95)
    at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
    ... 21 more {code}
*Problem causes：*
 # *_ChangelogStreamHandleReaderWithCache_* use RefCountedFile manager local cache file. The reference count is incremented when the input stream is opened from the cache file, and decremented by one when the input stream is closed. So the input stream must be closed and only once.
 # _*StateChangelogHandleStreamHandleReader#getChanges()*_ may cause the input stream to be closed twice. This happens when changeIterator.read(tuple2.f0, tuple2.f1) throws an exception (for example, when the task is canceled for other reasons during the restore process) the current state change iterator will be closed twice.

{code:java}
private void advance() {
    while (!current.hasNext() && handleIterator.hasNext()) {
        try {
            current.close();
            Tuple2<StreamStateHandle, Long> tuple2 = handleIterator.next();
            LOG.debug(""read at {} from {}"", tuple2.f1, tuple2.f0);
            current = changeIterator.read(tuple2.f0, tuple2.f1);
        } catch (Exception e) {
            ExceptionUtils.rethrow(e);
        }
    }
}

@Override
public void close() throws Exception {
    current.close();
}{code}
So we should make sure current state change iterator only be closed once. I suggest to make the following changes to _*StateChangelogHandleStreamHandleReader*_ :
{code:java}
private boolean currentClosed = false;

private void advance() {
    while (!current.hasNext() && handleIterator.hasNext()) {
        try {
            current.close();
            currentClosed = true;

            Tuple2<StreamStateHandle, Long> tuple2 = handleIterator.next();
            LOG.debug(""read at {} from {}"", tuple2.f1, tuple2.f0);
            current = changeIterator.read(tuple2.f0, tuple2.f1);
            currentClosed = false;
        } catch (Exception e) {
            ExceptionUtils.rethrow(e);
        }
    }
}

@Override
public void close() throws Exception {
    if (!currentClosed) {
        current.close();
    }
}{code}
 

cc [~yuanmei] , [~roman] .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,,FLINK-28898,FLINK-30107,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 12:21:16 UTC 2023,,,,,,,,,,"0|z1ej1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 05:39;masteryhx;It seems FLINK-30107 and FLINK-28898 are caused by this ?;;;","05/Jan/23 07:54;Feifan Wang;Hi [~masteryhx] ,  FLINK-30107 and FLINK-28898 are caused by origin changelog file not found , the problem described by this ticket is local cached file not found.;;;","06/Jan/23 04:23;Yanfei Lei;Hi [~Feifan Wang], could you please share what circumstances the error occurs? 

> This happens when changeIterator.read(tuple2.f0, tuple2.f1) throws an exception (for example, when the task is canceled for other reasons during the restore process) 

IIUC, when the task is canceled for other reason, the whole job is canceled, for the next restart, the `FileNotFoundException` will not affect the next run, and the refCount would be reset in the next run. Will the previous refCount still be used after the job is canceled?;;;","06/Jan/23 11:43;Feifan Wang;Hi [~Yanfei Lei] , ChangelogStreamHandleReaderWithCache live across job attempts for providing higher cache hit radio, the previous refCount still used after job restarted.;;;","09/Jan/23 04:41;Yanfei Lei;[~Feifan Wang] Thanks for the clarification. I think this makes sense in case the TM process doesn't restart when the job failover.

 ;;;","16/Jan/23 08:26;Feifan Wang;Hi [~ym]  , I submitted a pr to fix this problem, can you help me review it ?;;;","26/Jan/23 01:19;Feifan Wang;Hi [~roman]  , I submitted a pr to fix this problem, can you help me review it ?
 ;;;","10/Feb/23 12:21;roman;Thanks for the fix [~Feifan Wang] !

Merged into master as ba2b55df207fb79ad776eaf64ec8a6c1ab27bac9,

into 1.17 as 0d14c6188252fadc1408034d73f232312c2f683f,

into 1.16 as 526d6f949356d65d685bdd223cc5dd71a3997135.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more description of 'Overwriting a Partition' to doc 'Writing Tables',FLINK-30560,13516459,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,04/Jan/23 11:25,05/Jan/23 04:35,04/Jun/24 20:41,05/Jan/23 04:35,table-store-0.4.0,,,,,,table-store-0.3.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 04:35:36 UTC 2023,,,,,,,,,,"0|z1eiyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 04:35;lzljs3620320;master: ad5c0e24255c55b3fd478fb6e509b2191c068a90
release-0.3: 2000daa9baef23983d4b6f14727700d60c1eeae3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
May get wrong result for `if` expression if it's string data type,FLINK-30559,13516450,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,luoyuxia,luoyuxia,luoyuxia,04/Jan/23 10:31,28/May/24 13:56,04/Jun/24 20:41,20/Jul/23 04:21,,,,,,,1.16.3,1.17.2,1.18.0,,,,,,,,,,,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Can be reproduced by the folowing code in `org.apache.flink.table.planner.runtime.batch.sql.CalcITCase`

 
{code:java}
checkResult(""SELECT if(b > 10, 'ua', c) from Table3"", data3) {code}
The actual result is [co, He, He, ...].

Seems it will only get the first two characters.

 ",,,,,,,,,,,,,,,,,,FLINK-30966,,,,,,FLINK-35476,FLINK-31653,FLINK-31007,FLINK-30966,FLINK-31003,FLINK-30018,FLINK-30966,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 01:49:20 UTC 2023,,,,,,,,,,"0|z1eiww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 10:37;luoyuxia;I found the following generated code in Cal Node
{code:java}
// --- Start code generated by org.apache.flink.table.planner.codegen.calls.IfCallGen
          isNull$10 = false;
          if (!isNull$10) {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$6).numChars() > 2) {
          result$11 = ((org.apache.flink.table.data.binary.BinaryStringData) str$6).substring(0, 2);
          } else {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$6).numChars() < 2) {
          int padLength$12;
          padLength$12 = 2 - ((org.apache.flink.table.data.binary.BinaryStringData) str$6).numChars();
          org.apache.flink.table.data.binary.BinaryStringData padString$13;
          padString$13 = org.apache.flink.table.data.binary.BinaryStringData.blankString(padLength$12);
          result$11 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(((org.apache.flink.table.data.binary.BinaryStringData) str$6), padString$13);
          } else {
          result$11 = ((org.apache.flink.table.data.binary.BinaryStringData) str$6);
          }
{code}
I found the code contains the code of substring, it's a bit of weird Seems there's some issue in here.;;;","10/Jan/23 08:47;miamiaoxyz;Hi, I would like to fix it, can i take this issue？
[|https://issues.apache.org/jira/secure/AddComment!default.jspa?id=13486520];;;","11/Jan/23 08:51;luoyuxia;Thanks [~miamiaoxyz]  Please feel free to take it.;;;","19/May/23 08:07;yunta;[~luoyuxia] This bug has been reported by many guys, can we make it resolved?;;;","22/May/23 01:49;luoyuxia;[~yunta] Thanks for reminder. Haven't been awared of that.  I'll push the pr forward. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The metric 'numRestarts' reported in SchedulerBase will be overridden by metric 'fullRestarts',FLINK-30558,13516448,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiasun,xiasun,xiasun,04/Jan/23 10:14,05/Jan/23 06:59,04/Jun/24 20:41,05/Jan/23 06:59,1.16.0,1.17.0,,,,,1.16.1,1.17.0,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,pull-request-available,,,,"The method SchedulerBase#registerJobMetrics register metrics 'numRestarts' and 'fullRestarts' with the same metric object, as discussed in FLINK-30246, that will result in the loss of the metric 'numRestarts'.
{code:java}
metrics.gauge(MetricNames.NUM_RESTARTS, numberOfRestarts); 
metrics.gauge(MetricNames.FULL_RESTARTS, numberOfRestarts);{code}
I have verified this problem via rest api /jobs/:jobid/metrics, and the response shows below, we can find that the metric 'numRestarts' is missing.
{noformat}
[{""id"":""numberOfFailedCheckpoints""},{""id"":""cancellingTime""},{""id"":""lastCheckpointSize""},{""id"":""totalNumberOfCheckpoints""},{""id"":""lastCheckpointExternalPath""},{""id"":""lastCheckpointRestoreTimestamp""},{""id"":""failingTime""},{""id"":""runningTime""},{""id"":""uptime""},{""id"":""restartingTime""},{""id"":""initializingTime""},{""id"":""numberOfInProgressCheckpoints""},{""id"":""downtime""},{""id"":""lastCheckpointProcessedData""},{""id"":""numberOfCompletedCheckpoints""},{""id"":""deployingTime""},{""id"":""lastCheckpointFullSize""},{""id"":""fullRestarts""},{""id"":""createdTime""},{""id"":""lastCheckpointDuration""},{""id"":""lastCheckpointPersistedData""}]{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Jan 05 06:59:42 UTC 2023,,,,,,,,,,"0|z1eiwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 10:15;xiasun;I'd like to work on this issue, Could I be assigned this ticket?;;;","04/Jan/23 10:18;zhuzh;Yes this is a problem, especially considering that {{fullRestarts}} is deprecated and is supposed to be replaced with {{numRestarts}}.
Thanks for reporting this issue and volunteering to fix it. [~xiasun]
I have assigned you the ticket.;;;","05/Jan/23 06:59;zhuzh;master:
b50f42673fcd44bdd6de73f77d646df5c2ad1eed

1.16:
fe5e0c1772fc5392288a606e81b6f1dffe73cb90;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Kinesis Data Streams connectors from Flink master branch,FLINK-30557,13516441,13513212,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,04/Jan/23 08:47,06/Jan/23 08:37,04/Jun/24 20:41,06/Jan/23 08:37,,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,,,,"Remove:
 * {{flink-connector-kinesis}}
 * {{flink-connector-aws-kinesis-streams}}
 * {{flink-sql-connector-kinesis}}
 * {{flink-sql-connector-aws-kinesis-streams}}
 * Corresponding e2e tests",,,,,,,,,,,,,,,,,FLINK-30549,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 08:37:13 UTC 2023,,,,,,,,,,"0|z1eiuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:37;dannycranmer;Merged commit [{{ed73f85}}|https://github.com/apache/flink/commit/ed73f85c05df1c2e42f56f4b60beb935ffebcb48] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the logic for enumerating splits for Hive source to avoid potential OOM,FLINK-30556,13516436,13444738,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Wencong Liu,luoyuxia,luoyuxia,04/Jan/23 08:12,12/Sep/23 06:12,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,pull-request-available,stale-assigned,,,"Currently, when read hive source in batch mode, it'll first enumerate all split for the hive table. But when the table is large, the split will be too many which may well cause OOM. Some commuity users has also reported this problem. 

We need to optimize the logic for enumerating splits for hive table source to avoid potential OOM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31092,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 13 22:35:24 UTC 2023,,,,,,,,,,"0|z1eits:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 08:45;Wencong Liu;Hello [~luoyuxia] , I'm quite interested in this issue. Could you provide the code location where HiveSource enumerate all splits?;;;","04/Jan/23 10:27;luoyuxia;[~Wencong Liu] Cool, really appreciated. The code is in `HiveSourceFileEnumerator#enumerateSplits`, in where it will try to get all splits for the hive table at once.

If you have any other problem, please let me know.;;;","04/Jan/23 13:27;Wencong Liu;In my opinion, the method of enumerateSplits in interface  FileEnumerator could return the type Iterator rather than the type Collection. This allows the FileSourceSplit to be read only when it is used, WDYT? [~luoyuxia] ;;;","05/Jan/23 02:12;luoyuxia;[~Wencong Liu] Thanks for invesgation. TBH, that's what my thought. It'll not only help fix the OOM of Hive source, but also help the other source that depends on FileSystem Source. My only concern is that it'll touch  public api which require a FLIP and some complexity. But I think it's the right way, I think you can have a quick poc for it. 

BTW, the other idea which may be more simple is we can just follow what we do in ContinuousHiveSplitEnumerator which will enumerate  in a increamental way for stream mode.  Althogh it's just for Hive source, but I think it's enough.

I'm fine with any of them, but you can compare with these two ways and choose a way.;;;","08/Jan/23 12:56;Wencong Liu;Thanks [~luoyuxia]. As the ContinuousHiveSplitEnumerator can solve this problem, I think it is enough. But I will try the implementation of iterator. When the POC is completed, I can associate the PR with this issue. More discussion can be proposed later. WDYT?;;;","09/Jan/23 01:24;luoyuxia;[~Wencong Liu] Great, look forward to it.;;;","13/Mar/23 07:54;luoyuxia;[~Wencong Liu] Any progress on this ticket? I hope it can be finised before FLINK-30064 move hive connector out from Flink repo ;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive cluster can not read oss/s3 tables,FLINK-30555,13516434,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Jan/23 07:36,05/Jan/23 12:08,04/Jun/24 20:41,05/Jan/23 12:08,,,,,,,table-store-0.3.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"FLINK-29964 add oss support for Hive, but only valid in the case of standalone Hive, the distributed Hive compute engine cannot access.
We should add more FileSystems.initialize to Hive connector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 12:08:39 UTC 2023,,,,,,,,,,"0|z1eitc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 12:08;lzljs3620320;master: fc00d929b9e98693fbf8439fa50f51021b69a03a
release-0.3: 1574b224df0e88f82f5e374507938f8e38905791;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove useless session id in the Sql Client Executor,FLINK-30554,13516427,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,04/Jan/23 06:39,09/Jan/23 11:49,04/Jun/24 20:41,09/Jan/23 11:49,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"In the Gateway, we use SessionHandle to identify the session, which is created by the SessionManager automatically.  In the Sql Client, we don't expose this now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 11:49:47 UTC 2023,,,,,,,,,,"0|z1eirs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 11:49;fsk119;Merged into master: d37fca88cfa93d69389f4e7dd43f1a7cc8ae9ee7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
checkpoint always IN-PROGRESS because of hdfs,FLINK-30553,13516425,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Bug,,linqichen178,linqichen178,04/Jan/23 06:26,09/Jan/23 10:21,04/Jun/24 20:41,09/Jan/23 10:21,1.14.4,,,,,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,,"hey, I find a big problem. My flink didnot do checkpoint since 2022-12-24 (now 2023-1-4) which should do every 5 min. The last checkpoint's status is ""IN-PROGRESS"",but all taskmanager have done their own work. I make jstack on jobmanager and found that thread's status is ""TIMED_WAITING"" where executing ""DFSOutputStream.waitForAckedSeqno()"".
because my company not allow to copy things to public envirment, so i take some photos.",!微信图片_20230104140754.jpg!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/23 06:25;linqichen178;微信图片_20230104140754.jpg;https://issues.apache.org/jira/secure/attachment/13054307/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230104140754.jpg","04/Jan/23 06:26;linqichen178;微信图片_20230104140840.jpg;https://issues.apache.org/jira/secure/attachment/13054306/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230104140840.jpg","04/Jan/23 06:25;linqichen178;微信图片_20230104140848.jpg;https://issues.apache.org/jira/secure/attachment/13054305/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230104140848.jpg","04/Jan/23 06:26;linqichen178;微信图片_20230104140857.jpg;https://issues.apache.org/jira/secure/attachment/13054304/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230104140857.jpg","04/Jan/23 06:25;linqichen178;微信图片_20230104140903.jpg;https://issues.apache.org/jira/secure/attachment/13054303/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230104140903.jpg",,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 10:21:32 UTC 2023,,,,,,,,,,"0|z1eirc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 08:23;xtsong;Based on the information provided, I'm not sure whether {{DFSOutputStream.waitForAckedSeqno()}} is the cause of the problem. As shown in your screenshot, it calls `wait()` with a timeout of 1000ms. This should not cause the thread to be blocked for days.

If this is indeed the problem, then you probably should open a ticket in the hadoop project, rather than flink.;;;","09/Jan/23 09:59;linqichen178;If this is indeed the problem, then you probably should open a ticket in the hadoop project, rather than flink.—I found that hbase's user has problem like this , I will try to solve it via hadoop project. thanks a lot !!;;;","09/Jan/23 10:21;xtsong;Thanks you, [~linqichen178]. Closing this ticket for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector shouldn't assert the BatchMessageId size.,FLINK-30552,13516319,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,03/Jan/23 12:13,11/Jan/23 07:57,04/Jun/24 20:41,05/Jan/23 02:52,pulsar-4.0.0,,,,,,pulsar-3.0.1,pulsar-4.0.0,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"Pulsar will try to assert the batch message id size in {{MessageIdUtils}}, but the batch size is  determined by the producer in batch mode. So we can promise the size could be 1.

And the next message id calculation should be calculated by using BatchMessageId.",,,,,,,,,,,,,,,,,,,FLINK-30616,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 07:57:43 UTC 2023,,,,,,,,,,"0|z1ei3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 02:52;tison;master via https://github.com/apache/flink-connector-pulsar/pull/11;;;","11/Jan/23 07:57;tison;3.0.1 via https://github.com/apache/flink-connector-pulsar/pull/16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add open method to PartitionCommitPolicy,FLINK-30551,13516314,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aitozi,aitozi,03/Jan/23 11:39,19/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,auto-deprioritized-major,pull-request-available,,,"Currently, the {{PartitionCommitPolicy}} do not have the open hook. The custom partition commit policy does not have an appropriate entry point for the init work.
So I purpose to add an {{open}} method to make this work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:02 UTC 2023,,,,,,,,,,"0|z1ei2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 11:41;aitozi;cc [~gaoyunhaii] what do you think of this ?;;;","21/Feb/23 06:40;tanjialiang;I think it is necessary for custom partition commit policy to get the flink configuration from open method. I can set some custom option into flink conf, and use in my custom commit policy. WDYT? [~gaoyunhaii] [~lzljs3620320] ;;;","21/Feb/23 12:19;aitozi;In my PR, only the table's with properties will be passed to the open method. But I think it will be convenient to also pass the flink conf to the open method, because some custom metaStore's config are in job level can be set to the flink conf. What about keeping the both by adding the interface like

{code:java}
default void open(Configuration tableOptions, Configuration flinkConf) {}
{code}
;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink master to integrate KDS/KDF docs from flink-connector-aws,FLINK-30550,13516306,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,03/Jan/23 10:45,03/Jan/23 13:15,04/Jun/24 20:41,03/Jan/23 13:15,,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / AWS,Connectors / Firehose,Connectors / Kinesis,,,0,pull-request-available,,,,KDS/KDF connectors have been externalized to the flink-connector-aws repo. Update the docs to reflect this,,,,,,,,,,,,FLINK-30385,,,,,FLINK-30382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 13:15:41 UTC 2023,,,,,,,,,,"0|z1ei0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 13:15;dannycranmer;Merged commit [{{9d9ee6e}}|https://github.com/apache/flink/commit/9d9ee6eedb035e85e5d71d6278c7f08f25b6b6ec] into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-connector-aws-kinesis-firehose from Flink master branch,FLINK-30549,13516305,13513212,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,03/Jan/23 10:42,04/Jan/23 11:19,04/Jun/24 20:41,03/Jan/23 16:14,,,,,,,1.17.0,,,,,,,,,,,,,,,,Connectors / Firehose,,,,,0,pull-request-available,,,,"Remove:
 * {{flink-connector-aws-kinesis-firehose}}
 * {{flink-sql-connector-aws-kinesis-firehose}}
 * Corresponding e2e tests

 ",,,,,,,,,,,,,,,,FLINK-30557,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 16:14:05 UTC 2023,,,,,,,,,,"0|z1ei0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 16:14;dannycranmer;Merged commit [{{13ef498}}|https://github.com/apache/flink/commit/13ef498172bbc4cd0a63f8d7d24b3bbd4bba87c5] into apache:master ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In some cases AvroDeserializationSchema buffer is not reset,FLINK-30548,13516289,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,douheyu,douheyu,03/Jan/23 09:43,03/Jan/23 10:09,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,,,,,"When the avro schema changes, if the downstream uses the old schema to read the data generated by the new schema.
The buffer pos of AvroDeserializationSchema.decoder will not be reset.
This will result in misaligned reads.

Because AvroDeserializationSchema.decoder is reuse and not reset buffer pos.

The next read should start from the latest pos.

[https://github.com/apache/flink/blob/99c2a415e9eeefafacf70762b6f54070f7911ceb/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroDeserializationSchema.java#L165]

This line should be changed to :
{code:java}
this.decoder = DecoderFactory.get().directBinaryDecoder(inputStream, null); {code}
 

The following code can reproduce the problem
{code:java}
public class AvroChangeTest {
    private static AvroDeserializationSchema<AvroOld> avroSchema = AvroDeserializationSchema.forSpecific(AvroOld.class);

    @Test
    public void testWrite() throws IOException {
        DatumWriter<AvroNew> writer = new SpecificDatumWriter<AvroNew>(AvroNew.getClassSchema());

        File file = new File(""avro_test.data"");
        if (file.exists()) {
            file.delete();
        }

        for (int i = 0; i < 10; i++) { // avro serialization
            AvroNew taInfo = new AvroNew();
            taInfo.setCreateDate(""2023-01-03"");
            taInfo.setAdUserId(i);
            taInfo.setClickId(""2"" + i);

            ByteArrayOutputStream out = new ByteArrayOutputStream();
            BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);
            writer.write(taInfo, encoder);
            encoder.flush();
            out.close();
            byte[] data = out.toByteArray();

            System.out.println(data.length + ""\t"" + taInfo.toString());
            FileUtils.writeStringToFile(file, Base64.encodeBase64String(data) + ""\n"", true);
        }
    }

    @Test
    public void testRead() throws IOException { // avro deserialization
        File file = new File(""avro_test.data"");
        List<String> lines = FileUtils.readLines(file);
        for (String line : lines) {
            byte[] data = Base64.decodeBase64(line);
            AvroOld old = avroSchema.deserialize(data);

            System.out.println(old.toString());
        }
    }
}
 {code}
{code:java}
{
    ""namespace"": ""com.qihoo.dw.model.avroModel"",
    ""type"": ""record"",
    ""name"": ""AvroOld"",
    ""fields"": [
        {
            ""name"": ""create_date"",
            ""type"": [""string"",""null""],
            ""default"": """",
            ""doc"": ""事件时间(yyyy-MM-dd)""
        },
        {
            ""name"": ""ad_user_id"",
            ""type"": ""long"",
            ""default"": 0,
            ""doc"": ""广告主ID""
        }
    ]
} {code}
{code:java}
{
    ""namespace"": ""com.qihoo.dw.model.avroModel"",
    ""type"": ""record"",
    ""name"": ""AvroNew"",
    ""fields"": [
        {
            ""name"": ""create_date"",
            ""type"": [""string"",""null""],
            ""default"": """",
            ""doc"": ""事件时间(yyyy-MM-dd)""
        },
        {
            ""name"": ""ad_user_id"",
            ""type"": ""long"",
            ""default"": 0,
            ""doc"": ""广告主ID""
        },
        {
            ""name"": ""click_id"",
            ""type"": ""string"",
            ""default"": """"
        }
    ]
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/23 09:59;douheyu;AvroNew.java;https://issues.apache.org/jira/secure/attachment/13054282/AvroNew.java","03/Jan/23 09:59;douheyu;AvroOld.java;https://issues.apache.org/jira/secure/attachment/13054283/AvroOld.java",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 10:09:53 UTC 2023,,,,,,,,,,"0|z1ehx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 10:09;douheyu;If you agree, can you let me modify it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-table-runtime free for flink-table-store-common,FLINK-30547,13516285,13509216,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,03/Jan/23 09:27,05/Jan/23 02:15,04/Jun/24 20:41,05/Jan/23 02:15,,,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 02:15:31 UTC 2023,,,,,,,,,,"0|z1ehw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 02:15;lzljs3620320;master: f2314e46617d4d0dcbdc5f65a2699209072a08e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WindowDeduplicate didn't write result to sink timely,FLINK-30546,13516276,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Not A Bug,,kevinshin,kevinshin,03/Jan/23 08:38,04/Jan/23 04:28,04/Jun/24 20:41,04/Jan/23 04:28,1.15.3,,,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"StreamExecutionEnvironment checkpoint Trigger every 15s.

Datasource is kafka，build by : 

*KafkaSource<MyKafkaRecord> source = KafkaSource.<MyKafkaRecord>builder()*
 *.setBootstrapServers(kafkaServers)*
 *.setTopicPattern(topic)*
 *.setGroupId(consumerGroupId)*
 *.setStartingOffsets(OffsetsInitializer.latest())*
 *.setDeserializer(KafkaRecordDeserializationSchema.of(new MyKafkaDeserialization(true, true)))*
 *.build();* 

which will include ConsumerRecord.timestamp and set to MyKafkaRecord's timestamp field.

and build a DataStream by : 

*DataStream<MyKafkaRecord> stream2 = env.fromSource(source, WatermarkStrategy.<MyKafkaRecord>forBoundedOutOfOrderness(Duration.ofSeconds(3)).withTimestampAssigner((event, recordTimestamp)-> event.getTimestamp()), ""Kafka Source"");*

MyKafkaRecord contain the raw value of kafka record,which is a json string. ** then I will parse *MyKafkaRecord* to a POJO named *APlog ,* Aplog have a filed named eventTime  to hold MyKafkaRecord's timestamp field *:*

*DataStream<APlog> aplogStream = stream2.map(record->\{.......})*

*aplogStream.assignTimestampsAndWatermarks(WatermarkStrategy.<APlog>forBoundedOutOfOrderness(Duration.ofSeconds(3)).withTimestampAssigner((event, recordTimestamp)-> event.getEventTime()));*

Then convert  the aplogStream to table aplog : 

*Table aplog = tEnv.fromDataStream(aplogStream, $(""userName""), $(""userMAC""), $(""bssid""), $(""ssid""), $(""apName""), $(""radioID""), $(""vlanid""), $(""action""), $(""ddate""), $(""dtime""), $(""rawValue""), $(""region""), $(""eventTime""), $(""eventTime_2"").rowtime());*

Then regist as a TemporaryView : 

*tEnv.createTemporaryView(""aplog"", aplog);*

Try to WindowDeduplicate on aplog : 

*TableResult result = tEnv.executeSql(""select * from "" +*
 *""(select *, ROW_NUMBER() OVER ("" +*
 *""PARTITION BY window_start, window_end, userName ORDER BY eventTime_2 DESC"" +*
 *"") as row_num from "" +*
 *""TABLE(TUMBLE(TABLE aplog, DESCRIPTOR(eventTime_2), INTERVAL '10' SECONDS))"" +*
 *"") where row_num <= 1"");*

*result.print();*

result didn't print out to console every time after the checkpoint completed, I don't know when will the result to be print out. 

but when I try : 

*TableResult result = tEnv.executeSql(""select * from TABLE(TUMBLE(TABLE aplog, DESCRIPTOR(eventTime_2), INTERVAL '10' SECONDS))"");*

*result.print();*

I can see the result out to console every time after the checkpoint completed.",flink 1.15.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 04:27:43 UTC 2023,,,,,,,,,,"0|z1ehu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 04:27;kevinshin;I've resolved the problem. Watermark need to set idleness. thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SchemaManager doesn't check 'NOT NULL' specification when committing AddColumn change,FLINK-30545,13516269,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,03/Jan/23 07:12,04/Jan/23 01:45,04/Jun/24 20:41,03/Jan/23 08:24,,,,,,,table-store-0.3.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,"Currently, table store doesn't support adding column with 'NOT NULL' specification, but it doesn't check this condition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 08:24:30 UTC 2023,,,,,,,,,,"0|z1ehso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 08:24;lzljs3620320;master:
a47dc2b483692df2a61d2a567f1c58323e6a4fdb
3b9ce35f6d65eda8c7ee8ffbe924a903f4a91249

release-0.3:
61484f9ec335c97ab92ba624926897bbdc58a8d7
32de0c4d2b3db5ff916000dd0ba55e0570710b5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speed up finding minimum watermark across all channels by introducing heap-based algorithm,FLINK-30544,13516252,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,03/Jan/23 04:32,07/Apr/23 04:31,04/Jun/24 20:41,05/Jan/23 09:22,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Currently, every time a task receives a watermark, it tries to update the minimum watermark.Currently, we use the traversal algorithm to find the minimum watermark across all channels(see [StatusWatermarkValue#findAndOutputNewMinWatermarkAcrossAlignedChannels|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/watermarkstatus/StatusWatermarkValve.java#:~:text=private%20void-,findAndOutputNewMinWatermarkAcrossAlignedChannels,-(DataOutput%3C%3F%3E] for details), and the time complexity is O(N), where N is the number of channels.

We can optimize it by introducing a heap-based algorthim, reducing the time complexity to O(log(N))",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31628,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 09:22:37 UTC 2023,,,,,,,,,,"0|z1ehow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 07:22;apche-Px;不错的提议;;;","05/Jan/23 09:22;wanglijie;Done via master d9b8463da4181de48f62b4a551d11672ba313db2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding more examples for setting up jobs via operator.,FLINK-30543,13516251,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,sriramgr,sriramgr,03/Jan/23 04:16,02/Feb/23 08:03,04/Jun/24 20:41,02/Feb/23 08:03,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Currently, we have only basic examples which help to see how to run the job via an operator if we can add more examples for all upgrade modes that would be more helpful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 08:03:09 UTC 2023,,,,,,,,,,"0|z1ehoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 04:17;sriramgr;[~gyfora] - Please add your thoughts.;;;","02/Feb/23 08:03;gyfora;I am closing this, we can open tickets with concrete example ideas if we need;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support adaptive local hash aggregate in runtime,FLINK-30542,13516226,13500276,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,03/Jan/23 01:47,31/Jan/23 10:28,04/Jun/24 20:41,31/Jan/23 10:28,1.17.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,0,pull-request-available,,,,Introduce a new strategy to adaptively determine whether local hash aggregate is required according to the aggregation degree of local hash aggregate.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 10:28:22 UTC 2023,,,,,,,,,,"0|z1ehj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 10:28;godfrey;Fixed in 1.17.0: 122ba8f319b0d68374abba08d676e6dfa82cc114;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for OnlineStandardScaler,FLINK-30541,13516187,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,02/Jan/23 16:28,02/Feb/23 14:23,04/Jun/24 20:41,02/Feb/23 14:23,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 14:23:28 UTC 2023,,,,,,,,,,"0|z1ehag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 12:48;apche-Px;nice;;;","02/Feb/23 14:23;lindong;Merged to apache/flink-ml master branch with commit ad6d233cf52bcb7ec0beff55193f4742938c3b66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSinkTaskTest failed with a 134 exit code,FLINK-30540,13516165,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,02/Jan/23 10:57,16/Nov/23 13:32,04/Jun/24 20:41,,1.17.0,,,,,,,,,,,,,,,,,,,,,,Runtime / Task,Tests,,,,0,auto-deprioritized-critical,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44366&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8480

We experienced a 134 exit code when finalizing {{DataSinkTaskTest}}:
{code}

Jan 01 00:58:47 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
Jan 01 00:58:47 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
Jan 01 00:58:47 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
Jan 01 00:58:47 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
Jan 01 00:58:47 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
Jan 01 00:58:47 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
Jan 01 00:58:47 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
Jan 01 00:58:47 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 01 00:58:47 [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 01 00:58:47 [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jan 01 00:58:47 [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
Jan 01 00:58:47 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
Jan 01 00:58:47 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
Jan 01 00:58:47 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
Jan 01 00:58:47 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Jan 01 00:58:47 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Jan 01 00:58:47 [ERROR] Command was /bin/sh -c cd /__w/3/s/flink-runtime && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -Xmx768m -jar /__w/3/s/flink-runtime/target/surefire/surefirebooter7079560860955914030.jar /__w/3/s/flink-runtime/target/surefire 2023-01-01T00-54-17_721-jvmRun4 surefire118217553075734742tmp surefire_1430697542098749596tmp
Jan 01 00:58:47 [ERROR] Error occurred in starting fork, check output in log
Jan 01 00:58:47 [ERROR] Process Exit Code: 134
Jan 01 00:58:47 [ERROR] Crashed tests:
Jan 01 00:58:47 [ERROR] org.apache.flink.runtime.operators.DataSinkTaskTest
Jan 01 00:58:47 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Jan 01 00:58:47 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Jan 01 00:58:47 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Jan 01 00:58:47 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Jan 01 00:58:47 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 01 00:58:47 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Jan 01 00:58:47 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Jan 01 00:58:47 [ERROR] at java.lang.Thread.run(Thread.java:748)
Jan 01 00:58:47 [ERROR] -> [Help 1]
Jan 01 00:58:47 [ERROR] 
Jan 01 00:58:47 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 01 00:58:47 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 01 00:58:47 [ERROR] 
Jan 01 00:58:47 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 01 00:58:47 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30621,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 13:32:26 UTC 2023,,,,,,,,,,"0|z1eh5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 11:40;mapohl;It's caused by a segmentation fault: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44366&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=7706
{code}
Jan 01 00:56:39 [INFO] #
Jan 01 00:56:39 [INFO] # A fatal error has been detected by the Java Runtime Environment:
Jan 01 00:56:39 [INFO] #
Jan 01 00:56:39 [INFO] #  SIGSEGV (0xb) at pc=0x00007f8168722f67, pid=771, tid=0x00007f7fbcc2c700
Jan 01 00:56:39 [INFO] #
Jan 01 00:56:39 [INFO] # JRE version: OpenJDK Runtime Environment (8.0_292-b10) (build 1.8.0_292-8u292-b10-0ubuntu1~16.04.1-b10)
Jan 01 00:56:39 [INFO] # Java VM: OpenJDK 64-Bit Server VM (25.292-b10 mixed mode linux-amd64 compressed oops)
Jan 01 00:56:39 [INFO] # Problematic frame:
{code}
A core dump was provided.;;;","10/Jan/23 12:10;mapohl;I'm failing to convert the core dump into something useful. I guess, not having the exact JDK version causes the conversion to fail.
[~wangyang0918] could you connect to the Alibaba machine and try to convert the core dump using the local Java binaries?

{code}
$ jmap -dump:format=b,file=FLINK-30540.java-temurin-1.8.0_345-b01.hprof /usr/bin/java ./-__w-3-s-flink-runtime-core.771
{code};;;","10/Jan/23 16:08;mapohl;Loading it as is into VisualVM didn't work either. But that might be due to [the following log message|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44366&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8449]:
{code}
Jan 01 00:58:39 [INFO] # C  [libc.so.6+0x7ff67]# [ timer expired, abort... ]
Jan 01 00:58:46 [ERROR] Aborted (core dumped)
{code};;;","11/Jan/23 11:52;wangyang0918;[~mapohl] So do you want me to get the heap dump from core dump file on the Alibaba CI machine?;;;","11/Jan/23 11:59;wangyang0918;[~renqs] Could you please help on this? I think you should also have the login credentials for Alibaba CI machines.;;;","11/Jan/23 12:00;mapohl;I'm not optimistic that it will work because of [my previous comment|https://issues.apache.org/jira/browse/FLINK-30540?focusedCommentId=17656699&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17656699]. But it's worth a try.;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","16/Nov/23 13:32;mapohl;1.17: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54315&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8392];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase.testDetachedPerJobYarnCluster and testDetachedPerJobYarnClusterWithStreamingJob timing out,FLINK-30539,13516162,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mapohl,mapohl,02/Jan/23 10:16,19/Aug/23 22:35,04/Jun/24 20:41,,1.16.0,,,,,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,,0,auto-deprioritized-major,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44337&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=32023

In order to help investigating the issue: Both tests failed because they are running into the 60s timeout that was defined for each of them. We should get rid of the timeout to access the thread dump. It might be related to FLINK-24169",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24169,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 22:35:03 UTC 2023,,,,,,,,,,"0|z1eh4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/23 10:36;gaborgsomogyi;I've added my comment to FLINK-24169. By eliminating timeout you mean we should depend on CI timeout?
All in all I think it would make sense to re-evaluate this when https://github.com/apache/flink/pull/21128 is merged. This PR may drastically change this area.. Hopefully in a good way :);;;","02/Jan/23 11:16;mapohl;Thanks for sharing your opinion [~gaborgsomogyi].
{quote}By eliminating timeout you mean we should depend on CI timeout?{quote}
Yes, because the CI timeout (or actually the watcher process) would trigger a thread dump that would reveal where the test is hanging. That said, removing the timeouts is not a proposal for fixing the test instability but to comply to our [coding conventions|https://flink.apache.org/contributing/code-style-and-quality-common.html#avoid-timeouts-in-junit-tests] which would help us dig into what's going on.;;;","02/Jan/23 11:20;gaborgsomogyi;Makes sense. As mentioned it would be good to wait till Hadoop version bump happens. We've invested quite some time to make it happen :);;;","02/Jan/23 13:57;mapohl;-Ok, I assume you meant waiting with investigating the issue which makes sense. I went ahead and created the PRs for removing the timeout, though.-

reevaluating my comment: I missed going over [your comment in FLINK-24169|https://issues.apache.org/jira/browse/FLINK-24169?focusedCommentId=17653560&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17653560] initially. Considering that you want to tackle the relative path issue for YARN tests in [PR #21128|https://github.com/apache/flink/pull/21128] that covers the hadoop update, it makes sense to wait with investigating this issue. Thanks for that. Merging the PRs for removing the timeouts makes sense, anyway.;;;","03/Jan/23 09:42;gaborgsomogyi;I think it's not going to cause any big conflict when we remove the timeouts so approved them.;;;","03/Jan/23 10:24;mapohl;Removed timeouts:
master: 2ffe7e01919682be8248b466c59f59d39ae26fad
1.16: e7f7e6b8c174efabc3f5683047b230127972e5ed
1.15: 45c0503295fdf0874998b090dae2bb6a583d3b46;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error handling of stop job operation,FLINK-30538,13516145,13441036,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Paul Lin,Paul Lin,Paul Lin,02/Jan/23 07:54,01/Feb/23 08:00,04/Jun/24 20:41,01/Feb/23 06:35,,,,,,,,,,,,,,,,,,,,,,,Table SQL / Client,Table SQL / Gateway,,,,0,pull-request-available,,,,"Currently, the stop-job operation produces some verbose error msg and doesn't handle exceptions in stop-without-savepoint gracefully.",,,,,,,,,,,,,,,,,,,FLINK-30811,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 06:35:51 UTC 2023,,,,,,,,,,"0|z1eh14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 06:35;fsk119;Merged into master: d657395248c4cb189ee11356aa596dd84093b3d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for OpenSearch 2.3,FLINK-30537,13516117,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,martijnvisser,martijnvisser,01/Jan/23 11:53,15/May/24 06:40,04/Jun/24 20:41,15/May/24 06:40,,,,,,,,,,,,,,,,,,,,,,,Connectors / Opensearch,,,,,0,auto-deprioritized-major,pull-request-available,,,"Create a version for Flink’s Opensearch connector that supports version 2.3.

From the ASF Flink Slack: https://apache-flink.slack.com/archives/C03GV7L3G2C/p1672339157102319",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 06:40:52 UTC 2024,,,,,,,,,,"0|z1eguw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jan/23 18:29;reta;The current Opensearch connector supports both 1.3.x and 2.x (with 1.3.x being the default client), the 2.x requires JDK-11 baseline.;;;","02/Jan/23 14:11;martijnvisser;[~reta] I'll reply to the poster in Slack, thanks;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","15/May/24 06:40;Sergey Nuyanzin;Closed in favor of FLINK-33859;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove CountingOutput from per-record code path for most operators,FLINK-30536,13516022,13515876,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,30/Dec/22 13:25,10/Jan/23 04:00,04/Jun/24 20:41,10/Jan/23 04:00,,,,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"For the example program shown below, a CountingOutput will be added to the per-record code path for each map operation added in the program. This reduces the Flink performance as it increases the function call stack depth on the critical code path.
{code:java}
DataStream<Long> stream = env.fromSequence(1, 500000000L);
for (int i = 0; i < 10; i++) {
    stream = stream.map(x -> x);
}
stream.addSink(new DiscardingSink<>());
{code}
 

Instead of adding a CountingOutput that wraps around ChainingOutput, we can add a Counter in the ChainingOutput to achieve the same goal in most cases. We can do this when constructing the operator chain.

By making the change described above, we can reduce the call stack depth, increase the chance for function inline, and reduce the Flink runtime overhead.

 

Prior to the proposed change, each map() operation in the above program would add the following 3 functions on the call stack needed to produce a record:
 * CountingOutput#collect
 * ChainingOutput#collect
 * StreamMap#processElement

After the proposed change, the number of functions added for each map() operation would be reduced from 3 to 2, with ChainingOutput#collect removed from the call stack.

 

Here are the benchmark results obtained by running the above program with parallelism=1 and object re-use enabled. The results are averaged across 5 runs for each setup.
 * Prior to the proposed change, the average execution time is 56.54 sec with std=4.9 sec.
 * After the proposed change, the average execution time is 63.43 sec with std=6.3 sec.
 * The proposed change increases throughput by 12.2%.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-30 13:25:26.0,,,,,,,,,,"0|z1eg9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TTL state based benchmarks,FLINK-30535,13516001,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,yunta,yunta,30/Dec/22 08:01,19/Mar/24 13:31,04/Jun/24 20:41,19/Mar/24 13:30,,,,,,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,pull-request-available,,,,This ticket is inspired by https://issues.apache.org/jira/browse/FLINK-30088 which wants to optimize the TTL state performance. I think it would be useful to introduce state benchmarks based on TTL as Flink has some overhead to support TTL.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 19 13:31:15 UTC 2024,,,,,,,,,,"0|z1eg54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 04:16;zakelly;big +1 for this;;;","14/Dec/23 05:41;yunta;[~Zakelly] would you like to take this ticket?;;;","14/Dec/23 07:15;zakelly;[~yunta] Sure, I would like to do this.;;;","26/Dec/23 05:45;yunta;Some work done on Flink side:
master: 0c82f8af859a4f463a07f5dfb35648970c1c3425;;;","19/Mar/24 12:26;rovboyko;[~Zakelly] , thank for this functionality!

One question - why it's still in OPEN status?;;;","19/Mar/24 13:31;zakelly;[~rovboyko] Actually this has been resolved. Closing...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discuss / migrate People section to Hugo,FLINK-30534,13515923,13382695,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,martijnvisser,martijnvisser,martijnvisser,29/Dec/22 08:37,28/Apr/23 14:04,04/Jun/24 20:41,28/Apr/23 14:04,,,,,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,,,,,Let's first discuss if we want to migrate https://flink.apache.org/community.html#people to Hugo - It takes quite some time to migrate this table to the new setup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-29 08:37:42.0,,,,,,,,,,"0|z1efo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceOperator#emitNext() should push records to DataOutput in a while loop,FLINK-30533,13515915,13515876,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,29/Dec/22 07:59,17/Jan/23 06:32,04/Jun/24 20:41,05/Jan/23 07:25,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,pull-request-available,,,,"Currently, each invocation of SourceOperator#emitNext() push at most one record to the given DataOutput. This unnecessarily increases the average Java call stack depth needed to produce a record.

Take the following program as an example. For each element produced by this program, Flink runtime needs to include in the call stack these 3 function calls:
 * StreamTask#processInput()
 * StreamOneInputProcessor#processInput()
 * StreamTaskSourceInput#emitNext()

{code:java}
env.fromSequence(1, 1000000000L).map(x -> x).addSink(new DiscardingSink<>());
{code}
 

This ticket proposes to update SourceOperator#emitNext() to push records to DataOutput in a while loop. It improves Flink performance by removing an average of 3 function from the call stack needed to produce a record.

Here are the benchmark results obtained by running the above program with parallelism=1 and object re-use enabled. The results are averaged across 5 runs for each setup.
 * Prior to the proposed change, the average execution time is 46.1 sec with std=5.1 sec.
 * After the proposed change, the average execution time is 33.3 sec with std=0.9 sec.
 * The proposed change increases throughput by 38.4%.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30623,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 06:32:20 UTC 2023,,,,,,,,,,"0|z1efm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 09:17;Yanfei Lei;Hi [~lindong] , could you please help take a look at FLINK-30623? There is a large regression in checkpointSingleInput.UNALIGNED benchmark after this ticket. ;;;","13/Jan/23 11:11;pnowojski;It looks like a nice improvement:
http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&env=2
I have some questions about the way it was implemented. Apart of FLINK-30623, can someone tell me why this was only implemented for sources? There was quite a lot of effort put in the system, to make sure that sources are behaving and handled the same way as network inputs. 

The other question I have is have you considered abstracting away `Supplier mailboxHasMail` into something more general. Instead of passing `Supplier<Boolean>` via the constructor, make `DataOutput<OUT> output` to return/have a method like `bool DataOutput#canEmitBatchOfRecords()` that serves the same purpose?;;;","14/Jan/23 08:17;yunta;[~lindong] flink-benchmarks and your local benchmark are both executed with object-reuse enabled. As we all know that disabling object-reuse would introduce significant overhead to make this improvement not visible, I just be curious about the benchmark results without object-reuse, do you have some numbers here?;;;","16/Jan/23 14:48;lindong;Sorry, I didn't notice the Jira notification due to my gmail filter misconfiguration. I just fixed the configuration and should be able to notice Jira comments in the future.

I will reply to the above questions tomorrow morning.
 ;;;","17/Jan/23 01:25;lindong;Hi [~pnowojski], thanks for the comments!

Regarding the 1st question, could you tell which components should also be updated? Maybe we can change other components incrementally in addition to this JIRA.

I might have missed the context of making sure that ""sources are behaving and handled the same way as network inputs"". Could you provide links of related tickets or discussions?

 

Regarding the 2nd question, I am not sure if we should add the DataOutput#canEmitBatchOfRecords thought. Prior to this PR, DataOutput is responsible for data flow and it only provides API to emit data/watermark/latency. Classes such as SourceOperator and StreamTask are responsible for control flow (e.g. mailbox, availability, loop). Having StreamTask passes mailboxHasMail to SourceOperator preserves this separation of data flow and control flow.

It seems cleaner and simpler to still have DataOutput be responsible only for data flow. Could you help explain what is the benefit of using DataOutput#canEmitBatchOfRecords?

 ;;;","17/Jan/23 01:26;lindong;And I left comment in [https://github.com/apache/flink/pull/21690.] Maybe we can continue the discussion there?;;;","17/Jan/23 05:34;fanrui;Hi [~lindong] [~pnowojski] , thanks for your contribution and discussion.

For the 1st question, I created FLINK-30709 to make a similar improvement for NetworkInput. Welcome to discuss it there.

For the 2nd question, we can discuss it in https://github.com/apache/flink/pull/21690

 ;;;","17/Jan/23 05:55;lindong;Hi [~yunta], I run the benchmark again without and without object-reuse enabled. It appears that even with object-reuse enabled, this PR can still have considerable performance improvement for the program given in the Jira description.

Here are the average execution time for each run under different setup:

- Using Flink 1.15 with object-reuse enabled: 38.74 sec
- Using latest master with object-reuse enabled: 29.60 sec
- Using Flink 1.15 with object-reuse disabled: 46.00 sec
- Using latest master with object-reuse disabled: 37.43 sec

 ;;;","17/Jan/23 06:32;yunta;[~lindong] Thanks for the feedback and we can see the improvement is still obvious with object-reuse disabled. Looks like a nice improvement!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add benchmark for DCT, SQLTransformer and StopWordsRemover algorithm",FLINK-30532,13515880,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,29/Dec/22 01:46,06/Jan/23 06:59,04/Jun/24 20:41,06/Jan/23 06:59,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-29 01:46:11.0,,,,,,,,,,"0|z1efeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce operator chain call stack depth,FLINK-30531,13515876,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lindong,lindong,lindong,29/Dec/22 01:30,02/Feb/23 10:23,04/Jun/24 20:41,02/Feb/23 10:14,,,,,,,1.17.0,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,,,,,"Benchmark results show that Flink time to execute simple programs is more than 3X slower than Spark. For example, if we run the following program with object re-use enabled and with parallelism=1, it takes roughtly 120 sec on a macbook, whereas it takes Spark less than 40 sec to run the same logic on the same machine.
{code:java}
DataStream<Long> stream = env.fromSequence(1, 1000000000L)
.map(x -> x)
.map(x -> x)
.map(x -> x)
.map(x -> x)
.map(x -> x).addSink(new DiscardingSink<>());
{code}
 

It turns out that the operator chain overhead introduced by Flink is surprisingly high. For the above example program, Flink runtime goes through a call stack of 24 functions to produce 1 element. And each extra map(...) operation introduces 3 extra functions in the call stack.

Here are the 24 functions in the call stack:
{code:bash}
StreamTask#processInput
StreamOneInputProcessor#processInput
StreamTaskSourceInput#emitNext
SourceOperator#emitNext
IteratorSourceReaderBase#pollNext
SourceOutputWithWatermarks#collect
AsyncDataOutputToOutput#emitRecord
ChainingOutput#collect
StreamMap#processElement
CountingOutput#collect
ChainingOutput#collect
StreamMap#processElement
CountingOutput#collect
ChainingOutput#collect
StreamMap#processElement
CountingOutput#collect
ChainingOutput#collect
StreamMap#processElement
CountingOutput#collect
ChainingOutput#collect
StreamMap#processElement
CountingOutput#collect
ChainingOutput#collect
StreamSink#processElement
{code}
 

Given the observation described above, here are the explanations for why Flink is slow for programs with low computation overhead:
 * For each record produced, Flink runtime currently incurs an unnecessarily deep function call stack. It can be more than 24 for a simple program consisting of 5 map() operations.
 * Java's maximum inline level is less than 18 [2]. It is easy for operator chain call stack to exceed this limit and prevent Java from inlining function calls, which further increases the function call overhead.
 * For function calls that are not inlined, it requires looking up a virtual table since most functions are virtual functions.

Given the above explanations of the performance issue, here are the ideas to reduce Flink's runtime overhead:
 * Update SourceOperator#emitNext() to push records to DataOutput in a while loop. This can reduce the depth of the call stack needed to produce a record by 3 functions. See FLINK-30533 for more information.
 * Fuse some functions (e.g. ChainingOutput, StreamMap, CountingOutput) to reduce the call stack depth required for each extra operation (e.g. map(...)).

[1] [https://arxiv.org/pdf/1610.09166.pdf]

[2] [https://bugs.openjdk.org/browse/JDK-8234863]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 10:23:04 UTC 2023,,,,,,,,,,"0|z1efdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 08:17;xtsong;[~lindong], can this umbrella be closed?;;;","02/Feb/23 10:23;lindong;[~xtsong] Yes. I just closes this Jira since there is no more optimization I plan to make as part of this JIRA. And the comparison of performance overhead between Flink and Spark seems to be reasonable based on the benchmark results described below.

I used the benchmark specified in the JIRA description.
 * Flink 1.17-snapshot (including all the optimizations listed in this ticket) takes 64.2 sec
 * Flink 1.15.1 takes 77.3 sec
 * Spark 3.2.3 takes 40.0 sec

Now Spark's throughput is still 60% faster than Flink. This is probably reasonable given that Flink has additional control flow overhead per iteration (e.g. check the mailbox) and additional overhead for each operator on the data path (e.g. increment input/output counter metric).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink configuration from user-provided ConfigMap,FLINK-30530,13515867,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tashoyan,tashoyan,28/Dec/22 20:09,02/Sep/23 06:25,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Currently the Flink configuration can be specified in the YAML descriptor of FlinkDeployment via the _flinkConfiguration_ setting:
{code:yaml}
flinkConfiguration: 
  taskmanager.numberOfTaskSlots: ""2""
  ...
{code}
Same for the logging configuration:
{code:yaml}
logConfiguration: 
    ""log4j-console.properties"": |
      rootLogger.level = DEBUG
      ...{code}
This makes the YAML descriptor overloaded and huge. In addition, Flink and logging configuration may differ for different applications, while the Kubernetes settings maybe same for all applications. Therefore it makes sense to extract Flink and logging configurations from the YAML descriptor.

This can be done via a user-provided ConfigMap:
{code:yaml}
flinkConfigMap: basic-example-flink-config
{code}
In this example we have a Flink application {_}basic-example{_}. The _basic-example-flink-config_ ConfigMap contains all config files used by Flink: flink-conf.yaml, log4j-console.properties, possibly other files. The content of this ConfigMap gets mounted as a volume to {_}/opt/flink/conf{_}.

Therefore we can have different Flink settings for different applications and the same YAML descriptor for all of them (only the value for _flinkConfigMap_ differs).","Flink 1.15.2

Flink Kubernetes operator 1.2.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 02 06:25:51 UTC 2023,,,,,,,,,,"0|z1efbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 06:52;Wencong Liu;Hi [~tashoyan] , I'm quite interested in this issue. In which Java file should I find the problem you describe?;;;","30/Dec/22 13:18;gyfora;Because flink and log config is probably different for every Flink Job/Cluster, I don't really see the big value in decoupling it into separate configmaps.

This just increases the number of user managed resources from 1 (yaml) to 2/3 (yaml + configmaps) for a single job/cluster. The operator anyways need to read up the content of these configs as it applies some default values + config overrides before it creates the cluster.;;;","30/Dec/22 13:19;gyfora;The YAML descriptor too is very application specific so I don't see how this would help sharing them. Things like labels, jars, job arguments, resource specs are usually different for each job.;;;","30/Dec/22 16:43;tashoyan;Hi [~gyfora] 

The YAML descriptor contains settings, that can be common for all applications. For example, the *podTemplate* can be identical for all. Then, *serviceAccount* - same for all applications running in this Kubernetes cluster. {*}image{*}, *flinkVersion* and even *jarURI* - can be same for a number of applications.

Flink settings are more specific to a particular application. Flink memory settings is a matter of fine tuning per application. Same for checkpointing settings.  Same for parallelism and many other settings.

For me it would be fine to have separate user-provided ConfigMaps for Flink and Log4J settings: *flinkConfigMap* and {*}log4jConfigMap{*}. For example, we could have two possible values for {*}log4jConfigMap{*}: console logging and file logging. It would be better, than to copy-paste the same logging configuration across many YAML descriptors.

Finally, it is more convenient to group related settings in respective configuration files (in Kubernetes - ConfigMaps). That's why people have separate flink-conf.yaml, log4j.properties, etc.;;;","30/Dec/22 18:24;gyfora;At the end of the day you would still need to create separate FlinkDeployment resources for each Flink Cluster/Job.

I think Helm templating would be a great match for your usecase. You could set your default values in your FlinkDeployment helm template and simply create jobs based on that.;;;","16/Jan/23 15:16;tashoyan;Helm chart is a complementary tool, but not a solution. For a Flink deployment we have a 200-lines template file. This file still contains 3-in-1: flink-conf.yaml, log4j.properties and the YAML-descriptor.

Instead we would rather have 2 smaller templates (flink-conf.yaml, YAML-descriptor) and 1 simple file (log4j.properties).

{*}UPDATE{*}: The configuration for logging in JSON format is even longer. This makes the template file huge. ;;;","19/Jan/23 13:07;gyfora;Thanks [~tashoyan] . One thing to note here is that the operator would in any case read the contents of those configmaps and apply the values as those are needed to interact with the jobs / deploy applications etc.
Do you have a concrete suggestion on how you would change the CRD?

It would be great to keep this more or less aligned with what we have now.;;;","31/Aug/23 06:43;gyfora;We now have operator, namespace and flink version specific default config support. Do you still feel that feature would be a good addition or can we close this ticket?;;;","02/Sep/23 06:25;tashoyan;I would say that default config is a different thing. So this feature would be a good addition.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use simple cancel for savepoint upgrades on failing jobs,FLINK-30529,13515856,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ZhenqiuHuang,gyfora,gyfora,28/Dec/22 15:10,17/Nov/23 13:24,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"In some cases where savepoint upgrades are required we should consider simply cancelling the job and observing the last checkpoint/savepoint available.

We should think through what are the caveats of this approach but this could help upgrade some clusters that would be stuck now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-28 15:10:53.0,,,,,,,,,,"0|z1ef94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job may be stuck in upgrade loop when last-state fallback is disabled and deployment is missing,FLINK-30528,13515855,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,28/Dec/22 14:58,01/Jan/23 16:17,04/Jun/24 20:41,01/Jan/23 16:17,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"When last-state upgrade fallback is disabled (or we are switching flink versions) and the JM deployment is missing for some reason, we get stuck in the upgrade loop as the spec change /upgrade logic always takes precedence over the JM deployment recovery logic.

In this cases the JM deployment need to be recovered first so savepoint upgrade can be executed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 01 16:17:22 UTC 2023,,,,,,,,,,"0|z1ef8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jan/23 16:17;gyfora;release-1.3 c4e691696086358dfa981e35d9a799e7f31b1784
main a1842d4c0170feb008293963ec51c0343f42771d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Last-state suspend followed by flinkVersion change may lead to state loss,FLINK-30527,13515854,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,28/Dec/22 14:56,01/Jan/23 16:16,04/Jun/24 20:41,01/Jan/23 16:16,kubernetes-operator-1.2.0,kubernetes-operator-1.3.0,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We do not check flink version changes on recovery, so if the user suspended the job using last-state mode (or that was the only mode available) and then subsequently the flinkVersion is changed to a non-HA compatible version, the job would be restored using last-state and state would be lost.

In these cases we should set an error in the Flink resource instructing the user that changing version is not allowed at that point.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 01 16:16:52 UTC 2023,,,,,,,,,,"0|z1ef8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jan/23 16:16;gyfora;main 0dd9e9c257b1e4d7659d8c2cb367a35e7736c7a1
release-1.3 7f65de716cb7a2d6ac158cd01f1ee2923b634795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle failures in OpenSearch with ActionRequestFailureHandler being deprecated,FLINK-30526,13515852,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,28/Dec/22 14:39,28/Dec/22 15:36,04/Jun/24 20:41,,,,,,,,,,,,,,,,,,,,,,,,Connectors / Opensearch,,,,,0,,,,,"{quote} Hi everyone,
I have a streaming application that has Elasticsearch sink.
I Upgraded flink version from 1.11 to 1.16 and also moved from ES 7 to OpenSearch 2.0, and now I'm facing some deprected issues, hope you can help me.
In the previous version I created ElasticsearchSink and added a failure handler, which protected the sink to not fail on some exceptions.
 final ActionRequestFailureHandler failureHandler = (action, failure, restStatusCode, indexer) -> {
            if (ExceptionUtils.findThrowable(failure, EsRejectedExecutionException.class).isPresent()) {
                indexer.add(action);
            } else if (ExceptionUtils.findThrowable(failure, ElasticsearchParseException.class).isPresent()) {
                log.warn(""Got malformed document , action {}"", action);
                // malformed document; simply drop elasticsearchSinkFunction without failing sink
            } else if (failure instanceof IOException && failure.getCause() instanceof NullPointerException && failure.getMessage().contains(""Unable to parse response body"")) {
                //issue with ES 7 and opensearch - that does not send type - while response is waiting for it
                //at org.elasticsearch.action.DocWriteResponse.<init>(DocWriteResponse.java:127) -- this.type = Objects.requireNonNull(type);
                log.debug(""known issue format the response for ES 7.5.1 and DB OS (opensearch) :{}"", failure.getMessage());
            } else {
                // for all other failures, log and don't fail the sink
                log.error(""Got error while trying to perform ES action {}"", action, failure);
            }
        };
		

 final ElasticsearchSink.Builder<T> builder = new ElasticsearchSink.Builder<>(transportNodes, elasticsearchSinkFunction);
In the new version the class ActionRequestFailureHandler is deprecated and after investigation I can't find any way to handle failures.
For all failures the sink fails.
Is there anything I didn't see?
Thanks is advance! 
{quote}

From the Apache Flink Slack channel https://apache-flink.slack.com/archives/C03G7LJTS2G/p1672122873318899",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 28 15:36:41 UTC 2022,,,,,,,,,,"0|z1ef88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 15:36;reta;The deprecations are related to https://issues.apache.org/jira/browse/FLINK-24323 and basically hint towards using [https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API] instead, hope it helps.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot open jobmanager configuration web page,FLINK-30525,13515839,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,28/Dec/22 12:37,13/Jan/23 15:08,04/Jun/24 20:41,13/Jan/23 15:08,1.16.1,,,,,,1.16.1,1.17.0,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"we remove the environments in rest api in https://issues.apache.org/jira/browse/FLINK-30116.
The jobmanager.configuration web page will throw ""TypeError: Cannot read properties of undefined (reading 'length')"" 

the environment in jobmanager.configuration web page should be delete too.
 !image-2022-12-28-20-37-00-825.png! 
 !image-2022-12-28-20-37-05-551.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/22 12:37;huwh;image-2022-12-28-20-37-00-825.png;https://issues.apache.org/jira/secure/attachment/13054194/image-2022-12-28-20-37-00-825.png","28/Dec/22 12:37;huwh;image-2022-12-28-20-37-05-551.png;https://issues.apache.org/jira/secure/attachment/13054193/image-2022-12-28-20-37-05-551.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 15:08:26 UTC 2023,,,,,,,,,,"0|z1ef5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 12:42;huwh;[~chesnay] looking forward to your opinions on it. ;;;","09/Jan/23 07:52;huwh;[~renqs] Can you take a look at this? This will affect the release of 1.17;;;","13/Jan/23 15:08;chesnay;master: 560b4612735a2b9cd3b5db88adf5cb223e85535b
1.16: 8283dfcf69f36398db0aa1767c8a0d1ad78364a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support full screen viewing on the Logs/Stdout/Thread Dump page,FLINK-30524,13515826,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,damumu,damumu,28/Dec/22 11:34,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,,,1.20.0,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,1,pull-request-available,,,,"!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/336411/1672227240081-94627c11-852c-40a2-b1de-853a3a61c050.png|width=1153,id=u22bf640f!

Add button to support full screen viewing on the Logs/Stdout/Thread Dump page.

!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/336411/1672283805743-097d3678-5b6e-47ea-a83d-74639f136dbd.png|width=1183,id=ua821fb87!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/22 03:16;damumu;image-2022-12-29-11-16-20-204.png;https://issues.apache.org/jira/secure/attachment/13054199/image-2022-12-29-11-16-20-204.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 06:28:24 UTC 2022,,,,,,,,,,"0|z1ef2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 06:28;damumu;[~yunta] Please help me review this pr, thank you;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refine benchmark of vectorAssembler,FLINK-30523,13515825,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,28/Dec/22 11:31,06/Jan/23 06:09,04/Jun/24 20:41,06/Jan/23 06:09,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,Refine benchmark of vectorAssembler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-28 11:31:04.0,,,,,,,,,,"0|z1ef28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`SHOW TBLPROPERTIES` can't read properties of table in Spark3,FLINK-30522,13515823,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,28/Dec/22 10:58,04/Jan/23 01:45,04/Jun/24 20:41,04/Jan/23 01:45,table-store-0.4.0,,,,,,table-store-0.3.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/22 11:00;yzl;1.png;https://issues.apache.org/jira/secure/attachment/13054189/1.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 04 01:45:51 UTC 2023,,,,,,,,,,"0|z1ef1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 01:45;lzljs3620320;master: 545617a6fcf7eefd0a3f19d7adac431074891367
release-0.3: eeb09ca128553d77e9f8886b335e6048cd498179
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve `Altering Tables` of Doc,FLINK-30521,13515817,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,28/Dec/22 09:56,29/Dec/22 02:51,04/Jun/24 20:41,29/Dec/22 02:51,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.0,table-store-0.4.0,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Add more syntax description in the section `Altering Tables`.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 02:51:11 UTC 2022,,,,,,,,,,"0|z1ef0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 02:51;TsReaper;master: 659a5ce4b1b7a45c506391476b3b67396346403f
release-0.3: cfd132362b14ba66b41db3264ca70d34e496cc78;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Arguments contains with '#' will error split in loadYAMLResource,FLINK-30520,13515815,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,tanjialiang,tanjialiang,28/Dec/22 09:12,29/Dec/22 10:52,04/Jun/24 20:41,29/Dec/22 08:27,1.15.3,1.16.0,,,,,,,,,,,,,,,,,,,,,API / Core,,,,,0,pull-request-available,,,,"When i submit a flink jar job in Kubernetes Application mode which main args contains '#', it will be error split by org.apache.flink.configuration.GlobalConfiguration#loadYAMLResource

 

such as i using flink-kubernetes-operator to submit a job in kubernetes application mode

 
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: word-count
spec:
  image: apache/flink:1.16.0-scala_2.12-java8
  flinkVersion: v1_16
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""1""
  jobManager:
    resource:
      memory: ""2048m""
      cpu: 1
  taskManager:
    resource:
      memory: ""2048m""
      cpu: 1
  serviceAccount: flink
  job:
    jarURI: local:///opt/flink/examples/streaming/WordCount.jar
    args:
      - --output
      - /tmp/1#.txt
    parallelism: 2
    upgradeMode: stateless
{code}
 

 

It will be error split when loading the flink-conf.yaml

!image-2022-12-28-17-13-25-351.png!

 

And i enter to the jobmanager's pod to saw the flink-conf.yaml's program-args is right

!image-2022-12-28-17-13-38-337.png!

 

Maybe we should have a more strict validate for yaml comment? this is the yaml editor by vim, we can see comment should be start with '#' in line or comment start with ' #'

!image-2022-12-28-18-03-40-806.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15358,,,,,,,,,,,,,,,"28/Dec/22 09:13;tanjialiang;image-2022-12-28-17-13-25-351.png;https://issues.apache.org/jira/secure/attachment/13054180/image-2022-12-28-17-13-25-351.png","28/Dec/22 09:13;tanjialiang;image-2022-12-28-17-13-38-337.png;https://issues.apache.org/jira/secure/attachment/13054181/image-2022-12-28-17-13-38-337.png","28/Dec/22 10:03;tanjialiang;image-2022-12-28-18-03-40-806.png;https://issues.apache.org/jira/secure/attachment/13054186/image-2022-12-28-18-03-40-806.png",,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 29 10:52:54 UTC 2022,,,,,,,,,,"0|z1ef00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 08:27;xtsong;[~tanjialiang],

This seems to be duplicating FLINK-15358, in which we decide to introduce proper standard yaml parsing (FLINK-23620). Unfortunately, the effort is staled at the moment.

As for the solution, I'd in favor of solving the whole yaml parsing issue at once, rather than patching one after another. But that's discussable. Either way, helps are welcomed and appreciated on this.

Let's keep the discussion in one place (FLINK-15358). I'm closing this duplicated ticket.;;;","29/Dec/22 10:52;tanjialiang;[~xtsong], thanks for your reply, it is the best way to introduce proper standard yaml parsing. Hope to fix it soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add e2e tests for operator dynamic config,FLINK-30519,13515813,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gyfora,gyfora,28/Dec/22 09:05,18/Oct/23 14:39,04/Jun/24 20:41,18/Oct/23 14:39,,,,,,,kubernetes-operator-1.7.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,auto-deprioritized-critical,pull-request-available,stale-assigned,starter,"The dynamic config feature is currently not covered by e2e tests and is subject to accidental regressions, as shown in:

https://issues.apache.org/jira/browse/FLINK-30329

We should add an e2e test that covers this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 18 14:39:14 UTC 2023,,,,,,,,,,"0|z1eezk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Feb/23 13:32;haiqingchen; Hi [~gyfora]  could you assign this ticket to me? I'd like to work on this.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","31/Aug/23 06:44;gyfora;[~haiqingchen] are you still working on this by any chance or should someone take over?;;;","02/Sep/23 02:42;haiqingchen;[~gyfora] sorry, I'm not working on this right now and someone can take over.;;;","16/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","24/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","18/Oct/23 14:39;gyfora;merged to main 207d394c956a0853b40336890b324c19f1342ac0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flink-operator] Kubernetes HA Service not working with standalone mode,FLINK-30518,13515810,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,usamj,tbnguyen1407,tbnguyen1407,28/Dec/22 08:43,18/Jun/23 15:29,04/Jun/24 20:41,03/Feb/23 13:39,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"-Since flink-conf.yaml is mounted as read-only configmap, the /docker-entrypoint.sh script is not able to inject correct Pod IP to `jobmanager.rpc.address`. This leads to same address (e.g flink.ns-ext) being set for all Job Manager pods. This causes:-
Setting up FlinkDeployment in Standalone mode with Kubernetes HA Service. Problems:

(1) flink-cluster-config-map always contains wrong address for all 3 component leaders (see screenshot, should be pod IP instead of clusterIP service name)

(2) Accessing Web UI when jobmanager.replicas > 1 is not possible with error
{code:java}
{""errors"":[""Service temporarily unavailable due to an ongoing leader election. Please refresh.""]} {code}
 

~ flinkdeployment.yaml ~
{code:java}
spec:
  mode: standalone
  flinkConfiguration:
    high-availability: kubernetes
    high-availability.storageDir: ""file:///opt/flink/storage""
    ...
  jobManager:
    replicas: 3
  ... {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31187,,,,,,"28/Dec/22 08:40;tbnguyen1407;flink-configmap.png;https://issues.apache.org/jira/secure/attachment/13054176/flink-configmap.png","28/Dec/22 14:51;bgeng777;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13054196/screenshot-1.png",,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 13:39:13 UTC 2023,,,,,,,,,,"0|z1eeyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/22 09:04;gyfora;This may be related to:

https://issues.apache.org/jira/browse/FLINK-28554
https://issues.apache.org/jira/browse/FLINK-30329;;;","28/Dec/22 09:39;tbnguyen1407;[~gyfora] I checked. Both are not related. I think this is more about the design of having flink-conf mounted as configmap, which is always read-only.

 

A possible solution is for the Operator to mount flink-conf.yaml elsewhere and copy it to /opt/flink/conf/ to make it writable.;;;","28/Dec/22 14:07;gyfora;You are right [~tbnguyen1407] , those issues are not related, my mistake.

However the operator does not control how the flink-conf for jobs are mounted. We simply use the Flink clients to deploy the cluster, which mounts the configmap as part of the deployment. So this is not something that we can fix in the operator. 

cc [~bgeng777] [~yangwang166] 

I don't remember this being a problem earlier. Is it possible that the Kubernetes HA with native mode doesnt support multiple JM replicas?;;;","28/Dec/22 14:56;bgeng777;I think [~gyfora] is right that this is not a problem before.
I just tried an older version(1.1.0) of the operator with setting replicas to 2 for jobManager and it works fine on running jobs and accessing web ui. 
I checked my configmap(`basic-checkpoint-ha-example-cluster-config-map`) and it seems that leader.dispatcher is set correctly.
 !screenshot-1.png! 

I will try to reproduce this problem with the image of the operator based on the latest main branch as the [PR|https://github.com/apache/flink-kubernetes-operator/pull/494] to remove read-only limt is merged.;;;","28/Dec/22 15:00;gyfora;[~bgeng777] I think this is not related to any operator version as the other issue you are referring to is about the operators own flink-conf yaml, not about the deployed clusters. So this should work in all operator versions the same way.;;;","28/Dec/22 15:19;bgeng777;[~gyfora] I see. Thanks for the information. I misunderstood the problem somehow :(
But I just tried 1.3.0 operator and 1.16 flink to run basic-checkpoint-ha-example with setting replicas of JM to 3. It works fine as well.
[~tbnguyen1407] would you mind sharing the full deployment yaml for this problem?       
 ;;;","28/Dec/22 15:40;tbnguyen1407;[~bgeng777] Thank you. I found the problem when comparing with the basic-checkpoint-ha-example. I was using deployment mode ""standalone"". Switching to ""native"" the configmap is correctly populated.

My understanding about the difference between the 2 modes is how TaskManager pods are spawned. Not sure if using Kubernetes HA service in ""standalone"" mode is supported or not.

Notes: Our current helm chart sets up ""standalone"" cluster (JM and TM are predefined deployments) without the Operator and is using Kubernetes HA Service so I think Operator also should have support for it.;;;","28/Dec/22 15:54;gyfora;Strange, the standalone and native integration mode should work in the same way with regards to creating and mounting configmaps as they use the same decorators.;;;","28/Dec/22 16:01;tbnguyen1407;[~gyfora] I think my assumption is wrong about the flink-conf configmap as even with native mode the rpc.address is not set in flink-conf.yaml (file is read-only) but the advertised address is correct in cluster configmap. May be due to something else. You can try with the basic-checkpoint-ha-example and switch the mode to ""standalone"".;;;","28/Dec/22 16:07;gyfora;But can you clarify what doesn't work with multiple replicas? Because with 1 replica it should work.
With 2 replicas?
Does the job not start? Cannot fail over?;;;","28/Dec/22 16:10;tbnguyen1407;[~gyfora] I did not test with jobs yet. First problem is not able to access the Web UI (either ingress or clusterIP service) when there is more than 1 replica.

Also with wrong advertised addresses in cluster configmap, I think it will definitely affect leader election.;;;","28/Dec/22 16:22;gyfora;Please test this with replicas 1 and 2 and see what happens whether jobs can start etc. So we know what the exact problem is :) 
We have e2e tests set up with 1 replica so I am pretty sure that works. (also thats the most common setup);;;","12/Jan/23 09:40;wangyang0918;Really sorry for the late response. For native K8s implementation with HA enabled, we always override the jobmanager.rpc.address to pod IP. So for standalone mode with HA, we also need to do this in the operator.

You could find the similar logic in the example yaml for standalone mode.
{code:java}
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        # The following args overwrite the value of jobmanager.rpc.address configured in the configuration config map to POD_IP.
        args: [""jobmanager"", ""$(POD_IP)""] {code};;;","12/Jan/23 09:44;gyfora;[~usamj] could you please take a look?;;;","30/Jan/23 09:08;usamj;Yeah I'm looking into a fix;;;","03/Feb/23 13:39;gyfora;merged to main 87eb5c6f1855ef8fc1400039ce2517fc718f2f01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decrease log output interval while waiting for YARN JobManager be allocated,FLINK-30517,13515806,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,28/Dec/22 08:05,02/Mar/23 07:54,04/Jun/24 20:41,02/Mar/23 07:54,1.16.0,,,,,,1.17.0,,,,,,,,,,,,,,,,Deployment / YARN,,,,,0,pull-request-available,,,,"Flink Client will retrieve the application status every 250ms after submitting to YARN. 
If JobManager does not start in 60 seconds, it will log ""Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster"" every 250ms. This will lead to too many logs. 

We can keep the check interval at 250ms, but log the message every 1 minute.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/22 07:56;huwh;image-2022-12-28-15-56-56-045.png;https://issues.apache.org/jira/secure/attachment/13054174/image-2022-12-28-15-56-56-045.png",,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 07:54:17 UTC 2023,,,,,,,,,,"0|z1eey0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 07:54;Weijie Guo;master(1.18) via 1993ad0c968efa60ad5071e8472f630b247251f0.

release-1.17 via 30e32eaecf926b8a32916f0b614c1a57dc58bf22.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support files table in table store,FLINK-30516,13515799,13490538,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,28/Dec/22 07:10,03/Feb/23 05:21,04/Jun/24 20:41,03/Feb/23 05:21,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.4.0,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,,,,Add files table in Table Store and user can query row count from `mytable$files`,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 05:21:41 UTC 2023,,,,,,,,,,"0|z1eewg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 07:13;zjureel;For this issue, I think maybe we should add statistics such as total row count, file count in `Snapshot` to avoid reading all meta files when user perform query on mytable$snapshots, what do you think? [~lzljs3620320];;;","03/Feb/23 05:21;lzljs3620320;master: f30aef6ee570b719293280bc36d015132d8f68df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add benchmark for CountVectorizer, Imputer, RobustScaler, UnivariateFeatureSelector and VarianceThresholdSelector",FLINK-30515,13515787,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,28/Dec/22 04:05,06/Jan/23 06:04,04/Jun/24 20:41,06/Jan/23 06:04,,,,,,,ml-2.2.0,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2022-12-28 04:05:57.0,,,,,,,,,,"0|z1eets:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSource savepoint recovery sequence,FLINK-30514,13515755,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,WonderBeat,WonderBeat,27/Dec/22 17:20,03/Oct/23 18:55,04/Jun/24 20:41,,1.15.2,1.15.3,1.16.0,,,,,,,,,,,,,,,,,,,,Connectors / HybridSource,,,,,0,auto-deprioritized-major,pull-request-available,,,"{{org.apache.flink.connector.base.source.hybrid.HybridSourceReader}} accumulates splits during recovery in

{{{}org.apache.flink.connector.base.source.hybrid.HybridSourceReader#restoredSplits{}}}.

As a next step it creates a reader and pushes all {{recoveredSplits to}} the reader.
{{org.apache.flink.connector.base.source.hybrid.HybridSourceReader#setCurrentReader}}

Instantiation sequence of the {{setCurrentReader}} is following
 - {{reader.start()}}
 - {{reader.addSplits()}}

Seems like it doesn't work if we use {{FileSourceReader}} as an underlying reader.

{{FileSourceReader#start()}} method checks if any splits are available to read and executes {{sendSplitRequest}} if empty. Current {{HybridSourceReader}} recovery sequence is not aligned with this.

So, every time we recover we immediately jump to the next splits. 
Let me show you some logs. In this experiment job should have started with files inside the 1000000 bucket but jumped to the bucket number 2000000
Job Manager
{code:java}
2022-12-27 13:38:47.155 StaticFileSplitEnumerator  - Assigned split to subtask 1 : FileSourceSplit: s3a://bucket/2000000/part-00001-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97489087)  hosts=[localhost] ID=0000000032 position=null
2022-12-27 13:38:47.156 StaticFileSplitEnumerator  - Assigned split to subtask 9 : FileSourceSplit: s3a://bucket/2000000/part-00002-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97342071)  hosts=[localhost] ID=0000000033 position=null
2022-12-27 13:38:47.156 StaticFileSplitEnumerator  - Assigned split to subtask 6 : FileSourceSplit: s3a://bucket/2000000/part-00000-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97377047)  hosts=[localhost] ID=0000000031 position=null
2022-12-27 13:38:47.157 StaticFileSplitEnumerator  - Assigned split to subtask 5 : FileSourceSplit: s3a://bucket/2000000/part-00003-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97406878)  hosts=[localhost] ID=0000000034 position=null
2022-12-27 13:38:47.157 StaticFileSplitEnumerator  - Assigned split to subtask 2 : FileSourceSplit: s3a://bucket/2000000/part-00009-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97536205)  hosts=[localhost] ID=0000000040 position=null
2022-12-27 13:38:47.157 StaticFileSplitEnumerator  - Assigned split to subtask 4 : FileSourceSplit: s3a://bucket/2000000/part-00004-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97420601)  hosts=[localhost] ID=0000000035 position=null
2022-12-27 13:38:47.157 StaticFileSplitEnumerator  - Assigned split to subtask 8 : FileSourceSplit: s3a://bucket/2000000/part-00005-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97472256)  hosts=[localhost] ID=0000000036 position=null
2022-12-27 13:38:47.157 StaticFileSplitEnumerator  - Assigned split to subtask 3 : FileSourceSplit: s3a://bucket/2000000/part-00006-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97495880)  hosts=[localhost] ID=0000000037 position=null
2022-12-27 13:38:47.157 StaticFileSplitEnumerator  - Assigned split to subtask 0 : FileSourceSplit: s3a://bucket/2000000/part-00007-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97389425)  hosts=[localhost] ID=0000000038 position=null
2022-12-27 13:38:47.158 StaticFileSplitEnumerator  - Assigned split to subtask 7 : FileSourceSplit: s3a://bucket/2000000/part-00008-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97428709)  hosts=[localhost] ID=0000000039 position=null
{code}
Task Manager
{code:java}
2246:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00007-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79887236)  hosts=[localhost] ID=0000000018 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226029]
2247:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00000-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79987191)  hosts=[localhost] ID=0000000011 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226030]
2248:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00009-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80247830)  hosts=[localhost] ID=0000000020 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226535]
2249:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00004-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80055663)  hosts=[localhost] ID=0000000015 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226712]
2250:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00005-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80022187)  hosts=[localhost] ID=0000000016 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226346]
2251:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00006-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80109242)  hosts=[localhost] ID=0000000017 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=227284]
2252:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00001-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79980911)  hosts=[localhost] ID=0000000012 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226429]
2253:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00003-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79996693)  hosts=[localhost] ID=0000000014 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=227154]
2254:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00002-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80040476)  hosts=[localhost] ID=0000000013 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=225920]
2255:2022-12-27 13:38:47.110 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/1000000/part-00008-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79986997)  hosts=[localhost] ID=0000000019 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226278]
2265:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00006-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80109242)  hosts=[localhost] ID=0000000017 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=227284]]
2266:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00004-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80055663)  hosts=[localhost] ID=0000000015 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226712]]
2267:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00005-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80022187)  hosts=[localhost] ID=0000000016 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226346]]
2268:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00003-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79996693)  hosts=[localhost] ID=0000000014 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=227154]]
2269:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00009-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80247830)  hosts=[localhost] ID=0000000020 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226535]]
2270:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00002-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 80040476)  hosts=[localhost] ID=0000000013 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=225920]]
2271:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00007-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79887236)  hosts=[localhost] ID=0000000018 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226029]]
2272:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00001-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79980911)  hosts=[localhost] ID=0000000012 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226429]]
2273:2022-12-27 13:38:47.115 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00008-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79986997)  hosts=[localhost] ID=0000000019 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226278]]
2275:2022-12-27 13:38:47.116 FileSourceSplitReader  - Handling split change SplitAddition:[[FileSourceSplit: s3a://bucket/1000000/part-00000-db38c407-84c0-486f-9d71-c214f142b1c8-c000.zstd.parquet [0, 79987191)  hosts=[localhost] ID=0000000011 position=CheckpointedPosition: offset=NO_OFFSET, recordsToSkip=226030]]
2281:2022-12-27 13:38:47.160 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00009-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97536205)  hosts=[localhost] ID=0000000040 position=null]
2282:2022-12-27 13:38:47.160 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00001-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97489087)  hosts=[localhost] ID=0000000032 position=null]
2283:2022-12-27 13:38:47.159 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00002-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97342071)  hosts=[localhost] ID=0000000033 position=null]
2284:2022-12-27 13:38:47.160 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00000-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97377047)  hosts=[localhost] ID=0000000031 position=null]
2285:2022-12-27 13:38:47.160 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00003-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97406878)  hosts=[localhost] ID=0000000034 position=null]
2288:2022-12-27 13:38:47.161 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00005-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97472256)  hosts=[localhost] ID=0000000036 position=null]
2289:2022-12-27 13:38:47.161 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00004-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97420601)  hosts=[localhost] ID=0000000035 position=null]
2292:2022-12-27 13:38:47.162 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00006-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97495880)  hosts=[localhost] ID=0000000037 position=null]
2293:2022-12-27 13:38:47.163 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00007-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97389425)  hosts=[localhost] ID=0000000038 position=null]
2295:2022-12-27 13:38:47.163 SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3a://bucket/2000000/part-00008-00cb73ef-346b-4e1e-a86a-007223ddf275-c000.zstd.parquet [0, 97428709)  hosts=[localhost] ID=0000000039 position=null]
{code}
Same logs in github gist: [https://gist.github.com/WonderBeat/ddfdc852556997b09451d48766b54183]

This can be fixed with a simple reordering in the {{{}HybridSourceReader#createReader{}}}. {{""reader.addSplits}} -> {{reader.start""}} sounds logical, wdyt?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 10:35:02 UTC 2023,,,,,,,,,,"0|z1eemo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","20/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
