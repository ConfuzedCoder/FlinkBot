Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Container),Inward issue link (Dependent),Outward issue link (Dependent),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
java.lang.ClassNotFoundException: org.apache.flink.hbase.shaded.io.netty.channel.EventLoopGroup,FLINK-28013,13449681,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanghe,JasonLee,JasonLee,12/Jun/22 16:35,01/Sep/23 14:25,04/Jun/24 20:51,28/Aug/23 12:12,1.15.0,,,,,,,,,,,,,,hbase-3.0.0,,,,Connectors / HBase,,,,,0,pull-request-available,,,,"When using the hbase-1.4 version of the connector, the following exception will be reported. It seems that the io.netty related package is lost. I found that the io.netty package is redirected in maven. When I remove the redirection and compile a new one package, found that the task can be submitted normally, and then I tested the hbase-2.2 version of the package, and found that the task can be submitted normally, I compared the maven configuration of 1.4 and 2.2 and it seems that there is not much difference, so I am a little confused , why version 1.4 needs to remove redirection io.netty to run tasks normally
{code:java}
java.lang.RuntimeException: Cannot create connection to HBase.
    at org.apache.flink.connector.hbase.sink.HBaseSinkFunction.open(HBaseSinkFunction.java:153)
    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
    at org.apache.flink.table.runtime.operators.sink.SinkOperator.open(SinkOperator.java:58)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
    at org.apache.flink.connector.hbase.sink.HBaseSinkFunction.open(HBaseSinkFunction.java:116)
    ... 13 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.GeneratedConstructorAccessor21.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
    ... 16 more
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/hbase/shaded/io/netty/channel/EventLoopGroup
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2332)
    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2297)
    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393)
    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2419)
    at org.apache.flink.hbase.shaded.org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:714)
    ... 20 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.hbase.shaded.io.netty.channel.EventLoopGroup
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 27 more
 {code}
!error-hbase-1.4.jpg!","Flink: 1.15.0

Hbase: 1.4.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/22 16:35;JasonLee;error-hbase-1.4.jpg;https://issues.apache.org/jira/secure/attachment/13044975/error-hbase-1.4.jpg",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 28 12:12:33 UTC 2023,,,,,,,,,,"0|z1363k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/23 13:08;wanghe;I got the same exception. After comparing the dependency tree of the two modules, I found that the 2.x  connector introduced 'org.apache.hbase.thirdparty:hbase-shaded-netty' via 'hbase-client', which may be the root cause of the difference in behavior between the two packages.;;;","28/Aug/23 12:12;martijnvisser;Fixed in apache/flink-connector-hbase:main - 8b794a3ddb80f705d62e28d3b193f2d7d3521020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add built-in support for fetching jar from GCS (Google Cloud Storage),FLINK-28012,13449662,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,haoxin,haoxin,12/Jun/22 09:02,13/Jun/22 07:40,04/Jun/24 20:51,13/Jun/22 07:40,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"I think it's easy and meaningful to add built-in support in session job mode for downloading jar from GCS now.

The logic should look like the code below
{code:java}
if (""http"".equals(uri.getScheme()) || ""https"".equals(uri.getScheme())) {
    return HttpArtifactFetcher.INSTANCE.fetch(jarURI, flinkConfiguration, targetDir);
} else if (""gs"".equals(uri.getScheme())) {
    return GcsArtifactFetcher.INSTANCE.fetch(jarURI, flinkConfiguration, targetDir);
} else {
    return FileSystemBasedArtifactFetcher.INSTANCE.fetch(
            jarURI, flinkConfiguration, targetDir);
} {code}
We only need to extend the ArtifactManager and implement the new GcsArtifactFetcher.

Also, the users can set up the GCS authorization credentials by ENV and ConfigMap so that no need for any change inner the operator. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 07:40:39 UTC 2022,,,,,,,,,,"0|z135zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/22 09:40;aitozi;I think the gcs should work with the filesystem based fetcher, have you tried by adding the flink-gs-fs-hadoop dependency ?;;;","12/Jun/22 10:02;haoxin;> have you tried by adding the flink-gs-fs-hadoop dependency ?

No, will give it a try later.;;;","13/Jun/22 02:10;wangyang0918;Yes. I also agree that GCS support should be covered by Flink filesystem.;;;","13/Jun/22 05:53;haoxin;Haven't tested that but maybe it's great if we can add some instructions for the Operator just like Flink does.

 

[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/gcs/]

 ;;;","13/Jun/22 07:02;aitozi;[~haoxin] there is a link to the flink filesystem now, you could refer to it [here|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.0/docs/custom-resource/overview/#flinksessionjob-spec-overview];;;","13/Jun/22 07:40;haoxin;Thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize getAllPartitions in HiveSource,FLINK-28011,13449652,13444738,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,12/Jun/22 05:15,03/Aug/22 08:38,04/Jun/24 20:51,03/Aug/22 08:38,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 08:38:52 UTC 2022,,,,,,,,,,"0|z135x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 08:38;godfreyhe;Fixed in master: 363023d27a0fe838a2fa20422ae111da43643df2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use deleteRange to optimize the clear operation of RocksDBMapState.,FLINK-28010,13449599,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,Ming Li,Ming Li,Ming Li,11/Jun/22 09:20,29/Jan/23 03:09,04/Jun/24 20:51,21/Nov/22 02:17,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"[FLINK-21321|https://issues.apache.org/jira/browse/FLINK-21321] has introduced {{deleteRange}} for fast clipping of Incremental checkpoint, so can the {{clear}} method in {{RocksDBMapState}} be replaced with {{{}deleteRange{}}}?",,,,,,,,,,,,,,,,,,,,,,,,FLINK-30475,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 21 02:16:29 UTC 2022,,,,,,,,,,"0|z135lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 03:29;Yanfei Lei;I think this is an interesting idea. There is a bit of difference between FLINK-21321 and {{clear()}} method in {{{}RocksDBMapState{}}}. For {{{}clear{}}}() method, the keys we want to delete are all under a big {*}map key{*}.  BTW, the KeyGroupRange is a bounded interval [], while deleteRange is a left-bounded interval [), we should be careful with the intervals.;;;","01/Jul/22 06:52;Ming Li;[~Yanfei Lei] thank you for your reply, I think this can save us a lot of delete overhead. At the same time, can this also optimize the performance of seek? Because LSM-Tree is not friendly to range-query when there are a large number of tombones.;;;","01/Jul/22 07:09;Yanfei Lei;[~Ming Li]  The performance of seeking is a good point worth considering, there are benchmarks on the performance of {{{}Point Lookups{}}}, {{Range Scan}} in  [https://rocksdb.org/blog/2018/11/21/delete-range.html|https://rocksdb.org/blog/2018/11/21/delete-range.html,] ,  {{deleteRange}} would bring a little regression on reading.;;;","05/Jul/22 07:07;Ming Li;hi, [~yunta], do you think this optimization is feasible?;;;","05/Jul/22 10:29;yunta;I think this is possible if we can build the bytes with the next primary key (or the so-called map key) for the usage of deleteRange.;;;","06/Jul/22 08:08;Ming Li;[~yunta], maybe we just need to add 1 to the prefix bytes?;;;","07/Jul/22 02:47;yunta;[~Ming Li] Since we do not have a special separator between the primary key and user key, we need to use the next byte array to represent the prefix bytes. If so, I think we can make this.;;;","11/Jul/22 09:54;Ming Li;[~yunta], can you assign this to me and help review the code?;;;","11/Jul/22 12:02;yunta;[~Ming Li] Already assigned to you, please go ahead.;;;","21/Nov/22 02:16;Yanfei Lei;Because the experimental results of [iterator|https://github.com/apache/flink/pull/20405#discussion_r951119238] are not as expected, as [~Ming Li] said, ""maybe this is not a good optimization, we moved the cost of delete with Iterator to read, and this cost will persist unless a compaction occurs"".  I am going to close this ticket, please reopen this if there are some other optimizations.;;;",,,,,,,,,,,,,,,,,,,,,,
Optimize data split,FLINK-28009,13449597,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,complone,complone,11/Jun/22 09:06,19/Mar/23 05:48,04/Jun/24 20:51,19/Mar/23 05:48,table-store-0.1.0,,,,,,,,,,,,,,,,,,Table Store,,,,,0,auto-deprioritized-major,pull-request-available,,,"Optimizing split data logic for large data volumes using parallel streams

https://github.com/apache/flink-table-store/pull/151",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 28 22:38:05 UTC 2022,,,,,,,,,,"0|z135l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 02:18;lzljs3620320;Hi [~complone]  Back to jira, can you prove what this jira is trying to optimize? For example, is there any benchmark that proves where it's slow, we need a performance boost, how much input data is there, and in what scenarios is it causing a bottleneck?;;;","20/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","28/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not get secondary resource from context after operator restart,FLINK-28008,13449577,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,morhidi,aitozi,aitozi,11/Jun/22 01:52,24/Nov/22 01:03,04/Jun/24 20:51,27/Jun/22 09:01,kubernetes-operator-1.1.0,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"how to reproduce:
 * create session and submit session job
 * delete session job
 * restart operator
 * submit session job again

Then it will print log 
{noformat}
2022-06-11 01:51:15,645 o.a.f.k.o.r.s.FlinkSessionJobReconciler [WARN ][default/basic-session-job-example] Session cluster deployment is not found
2022-06-11 01:51:15,645 o.a.f.k.o.r.s.FlinkSessionJobReconciler [WARN ][default/basic-session-job-example2] Session cluster deployment is not found{noformat}
But the session cluster is still there
{noformat}
basic-session-cluster-547655d95c-888mm    1/1     Running   0          6m34s{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 27 09:01:26 UTC 2022,,,,,,,,,,"0|z135go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/22 03:01;aitozi;I test with the last release version with 
{code:java}
helm install flink-kubernetes-operator flink-kubernetes-operator-1.0.0/flink-kubernetes-operator {code}
It do not reproduce. I will take a closer look how it happen;;;","11/Jun/22 03:28;aitozi;I don't know if there any trick with the new sdk version. cc [~morhidi], maybe you have some more insights for this;;;","13/Jun/22 02:12;wangyang0918;Does it mean the shared informer does not work properly?;;;","13/Jun/22 07:05;aitozi;I think so, but I have not make sure;;;","22/Jun/22 13:00;morhidi;I'll have a look at it.;;;","23/Jun/22 07:05;morhidi;Reported this under https://github.com/java-operator-sdk/java-operator-sdk/issues/1299 and driving it till resolution.;;;","24/Jun/22 08:53;morhidi;quick update: we have a PR open in JOSDK with the [fix|[https://github.com/java-operator-sdk/java-operator-sdk/pull/1300]] we need to address this issue. ETA for JOSDK v3.0.3 is today or Monday. I'll open a PR once the patch release is out.;;;","27/Jun/22 09:01;gyfora;merged to main 6a4e6a5edb386675fede2790b81e31cdbcb2952c;;;",,,,,,,,,,,,,,,,,,,,,,,,
Tests for AWS Connectors Using SDK v2 to use Synchronous Clients,FLINK-28007,13449545,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,CrynetLogistics,CrynetLogistics,CrynetLogistics,10/Jun/22 16:59,17/Jun/22 08:44,04/Jun/24 20:51,17/Jun/22 08:44,1.15.0,,,,,,,,,,,,,,1.16.0,,,,Connectors / Common,Connectors / Kinesis,,,,0,pull-request-available,,,,"h3. Background

The unit & integration tests for the aws connectors in the Flink repository create clients using static helper methods in flink-connector-aws-base, in the AWSServicesTestUtils class.

These static helper methods create the asynchronous flavour of the clients required by aws connectors.

*Task*

* Change these to the synchronous version for each aws client.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26256,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 08:44:11 UTC 2022,,,,,,,,,,"0|z1359k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:44;dannycranmer;Merged to master https://github.com/apache/flink/commit/403cd3b86b9131161d6380bfd2c5200dcbe6989c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run architecture tests in single JVM,FLINK-28006,13449471,13449467,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Jun/22 13:26,15/Jun/22 10:41,04/Jun/24 20:51,15/Jun/22 10:41,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,"The architecture tests, in particular the production ones, load a lot of classes into memory (~700mb). To prevent this from failing in the future I propose to run these tests in a single JVM that gets the entire memory budget.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 10:41:48 UTC 2022,,,,,,,,,,"0|z134t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 10:41;chesnay;master: 6b71e44ff929ae8af89234b105f15aeb33029a8e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce network memory in UnalignedCheckpointITCase,FLINK-28005,13449468,13449467,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Jun/22 13:24,15/Jun/22 08:28,04/Jun/24 20:51,15/Jun/22 08:28,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Checkpointing,Tests,,,,0,pull-request-available,,,,"This test has some cases where it spawns 25 task managers, each allocation 64mb for the network buffer pool.

This is likely way more than necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 08:28:16 UTC 2022,,,,,,,,,,"0|z134sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 08:28;chesnay;master: f2e25d797e7ba7b2c76941e137b3d77a3e1a6e4b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce CI heap space by 25%,FLINK-28004,13449467,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,10/Jun/22 13:22,16/Jun/22 12:47,04/Jun/24 20:51,16/Jun/22 12:47,,,,,,,,,,,,,,,1.16.0,,,,Build System / CI,,,,,0,pull-request-available,,,,"I've recently seen several builds where docker crashes again with out of memory errors.

Our current memory configuration is a bit optimistic and relies on all forks _not_ hitting the upper limit.

If we'd reduce the heap space by 25% everything could run at maximum without memory running out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 16 12:47:10 UTC 2022,,,,,,,,,,"0|z134s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 12:47;chesnay;master: bac9943b344005b9beb142adf84138fb1773fca4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive sql is wrongly modified by SqlCompleter in SQL client when using -f {file},FLINK-28003,13449447,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiangang,jingzhang,jingzhang,10/Jun/22 12:40,16/Aug/22 04:10,04/Jun/24 20:51,16/Aug/22 04:10,1.15.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Client,,,,,0,pull-request-available,,,,"When I run the following sql in SqlClient using 'sql-client.sh -f zj_test.sql'
{code:java}
create table if not exists db.zj_test(
pos                   int,
rank_cmd              string
)
partitioned by (
`p_date` string,
`p_hourmin` string);

INSERT OVERWRITE TABLE db.zj_test PARTITION (p_date='20220605', p_hourmin = '0100')
SELECT
pos ,
rank_cmd
FROM db.sourceT
where p_date = '20220605' and p_hourmin = '0100'; {code}
An error would be thrown out because the 'pos' field is changed to 'POSITION'. I guess `SqlCompleter` in sqlClient module might do something wrong.

The error could be reproduced using the attached file.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/22 12:38;jingzhang;zj_test.sql;https://issues.apache.org/jira/secure/attachment/13044938/zj_test.sql",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 04:10:41 UTC 2022,,,,,,,,,,"0|z134ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 10:10;martijnvisser;If you want to use a reserved keyword in your SQL syntax, you'll need to use backticks for these columns. ;;;","14/Jun/22 02:51;jingzhang;[~martijnvisser] Thanks for response, however you might misunderstand here.

I don't use reserved keyword, I use 'pos', but SqlClient replaces it to

'position' which leads to an error.

This replacement causes a lot of problems.;;;","14/Jun/22 03:14;jingzhang;As discussed with [~fsk119] offline, we would add a configure to disable sql complete in SqlClient. Besides, for '-f sqlFile', it makes sense to disable sql complete by default.;;;","14/Jun/22 07:29;martijnvisser;[~jingzhang] I was under the impression that pos was a reserved keyword, but I misread that. Apologies. ;;;","16/Jun/22 10:32;jingzhang;[~martijnvisser] Never mind, thanks for your attention.;;;","06/Jul/22 10:06;Jiangang;I have discussed with [~jingzhang] and solve it in our inner flink version. Could you review the code, [~fsk119] . Thanks.;;;","05/Aug/22 05:14;Jiangang;[~shengkai] Do you have time to review it? Thanks.;;;","16/Aug/22 04:10;jark;Fixed in master: 226f1602c047092cc1997f6e861aa37858df21f7;;;",,,,,,,,,,,,,,,,,,,,,,,,
"test_connectors.py failed with ""object is not an instance of declaring class"" in JDK11 ",FLINK-28002,13449435,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,10/Jun/22 11:51,22/Jun/22 05:49,04/Jun/24 20:51,20/Jun/22 12:24,1.16.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-10T02:43:20.7206790Z Jun 10 02:43:20 E                   : java.lang.IllegalArgumentException: object is not an instance of declaring class
2022-06-10T02:43:20.7207481Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-10T02:43:20.7208200Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-10T02:43:20.7209003Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-10T02:43:20.7209720Z Jun 10 02:43:20 E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-06-10T02:43:20.7210572Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-10T02:43:20.7211291Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-10T02:43:20.7212101Z Jun 10 02:43:20 E                   	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-10T02:43:20.7212796Z Jun 10 02:43:20 E                   	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-06-10T02:43:20.7213500Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2022-06-10T02:43:20.7214327Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2022-06-10T02:43:20.7215097Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2022-06-10T02:43:20.7215885Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2022-06-10T02:43:20.7216700Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2022-06-10T02:43:20.7217558Z Jun 10 02:43:20 E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2022-06-10T02:43:20.7218225Z Jun 10 02:43:20 E                   	at java.base/java.lang.Thread.run(Thread.java:829)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36508&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 12:24:03 UTC 2022,,,,,,,,,,"0|z134l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 12:24;dianfu;Merged to master via 314e276f6c6bff990e82515d6ce90fd6a7c9561d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PlannerScalaFreeITCase.testImperativeUdaf failed with Permission denied,FLINK-28001,13449431,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,hxbks2ks,hxbks2ks,10/Jun/22 11:30,10/Jun/22 11:46,04/Jun/24 20:51,10/Jun/22 11:44,1.15.1,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,test-stability,,,,"{code:java}
2022-06-10T04:07:42.0596591Z Jun 10 04:07:42 [ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 19.247 s <<< FAILURE! - in org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase
2022-06-10T04:07:42.0597622Z Jun 10 04:07:42 [ERROR] PlannerScalaFreeITCase.testImperativeUdaf  Time elapsed: 9.599 s  <<< ERROR!
2022-06-10T04:07:42.0598648Z Jun 10 04:07:42 java.io.IOException: Cannot run program ""/tmp/junit8835683122079875817/junit8979658544832432488/bin/sql-client.sh"": error=13, Permission denied
2022-06-10T04:07:42.0599451Z Jun 10 04:07:42 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
2022-06-10T04:07:42.0600138Z Jun 10 04:07:42 	at org.apache.flink.tests.util.AutoClosableProcess.createProcess(AutoClosableProcess.java:193)
2022-06-10T04:07:42.0602153Z Jun 10 04:07:42 	at org.apache.flink.tests.util.AutoClosableProcess.access$100(AutoClosableProcess.java:51)
2022-06-10T04:07:42.0603571Z Jun 10 04:07:42 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:122)
2022-06-10T04:07:42.0604501Z Jun 10 04:07:42 	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:221)
2022-06-10T04:07:42.0605770Z Jun 10 04:07:42 	at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$StandaloneClusterController.submitSQLJob(LocalStandaloneFlinkResource.java:207)
2022-06-10T04:07:42.0607154Z Jun 10 04:07:42 	at org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.executeSqlStatements(PlannerScalaFreeITCase.java:137)
2022-06-10T04:07:42.0608344Z Jun 10 04:07:42 	at org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.testImperativeUdaf(PlannerScalaFreeITCase.java:125)
2022-06-10T04:07:42.0609106Z Jun 10 04:07:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-10T04:07:42.0609770Z Jun 10 04:07:42 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-10T04:07:42.0611043Z Jun 10 04:07:42 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-10T04:07:42.0612044Z Jun 10 04:07:42 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-10T04:07:42.0612984Z Jun 10 04:07:42 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-10T04:07:42.0614128Z Jun 10 04:07:42 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-10T04:07:42.0615502Z Jun 10 04:07:42 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-10T04:07:42.0617022Z Jun 10 04:07:42 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-10T04:07:42.0618088Z Jun 10 04:07:42 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-06-10T04:07:42.0619146Z Jun 10 04:07:42 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-06-10T04:07:42.0620094Z Jun 10 04:07:42 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-06-10T04:07:42.0620828Z Jun 10 04:07:42 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-06-10T04:07:42.0621660Z Jun 10 04:07:42 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-06-10T04:07:42.0622739Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-10T04:07:42.0623484Z Jun 10 04:07:42 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-10T04:07:42.0624211Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-10T04:07:42.0624951Z Jun 10 04:07:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-10T04:07:42.0625734Z Jun 10 04:07:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-10T04:07:42.0626455Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-10T04:07:42.0627130Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-10T04:07:42.0627963Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-10T04:07:42.0629060Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-10T04:07:42.0629763Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-10T04:07:42.0630429Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-10T04:07:42.0631065Z Jun 10 04:07:42 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-06-10T04:07:42.0631772Z Jun 10 04:07:42 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-06-10T04:07:42.0632477Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-10T04:07:42.0633143Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-10T04:07:42.0634053Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-10T04:07:42.0634749Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-10T04:07:42.0635427Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-10T04:07:42.0636222Z Jun 10 04:07:42 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-06-10T04:07:42.0636907Z Jun 10 04:07:42 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-06-10T04:07:42.0637547Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-10T04:07:42.0638373Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-10T04:07:42.0638981Z Jun 10 04:07:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-10T04:07:42.0639592Z Jun 10 04:07:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-10T04:07:42.0640275Z Jun 10 04:07:42 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-10T04:07:42.0641069Z Jun 10 04:07:42 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-10T04:07:42.0642013Z Jun 10 04:07:42 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-10T04:07:42.0642847Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-10T04:07:42.0643748Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-10T04:07:42.0644748Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-10T04:07:42.0645698Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-10T04:07:42.0646758Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-10T04:07:42.0647752Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-10T04:07:42.0648535Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-10T04:07:42.0649395Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-10T04:07:42.0650313Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-10T04:07:42.0651393Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-06-10T04:07:42.0652324Z Jun 10 04:07:42 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-06-10T04:07:42.0653098Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-06-10T04:07:42.0653994Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-10T04:07:42.0655103Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-06-10T04:07:42.0655993Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-10T04:07:42.0656766Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-10T04:07:42.0657494Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-10T04:07:42.0658217Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-10T04:07:42.0658869Z Jun 10 04:07:42 Caused by: java.io.IOException: error=13, Permission denied
2022-06-10T04:07:42.0659431Z Jun 10 04:07:42 	at java.lang.UNIXProcess.forkAndExec(Native Method)
2022-06-10T04:07:42.0660019Z Jun 10 04:07:42 	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
2022-06-10T04:07:42.0660627Z Jun 10 04:07:42 	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
2022-06-10T04:07:42.0661236Z Jun 10 04:07:42 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
2022-06-10T04:07:42.0661909Z Jun 10 04:07:42 	... 65 more
2022-06-10T04:07:42.0662265Z Jun 10 04:07:42 
2022-06-10T04:07:42.0662783Z Jun 10 04:07:42 [ERROR] PlannerScalaFreeITCase.testImperativeUdaf  Time elapsed: 9.628 s  <<< ERROR!
2022-06-10T04:07:42.0664311Z Jun 10 04:07:42 java.io.IOException: Cannot run program ""/tmp/junit7530886358682985343/junit3495912346521521167/bin/sql-client.sh"": error=13, Permission denied
2022-06-10T04:07:42.0665111Z Jun 10 04:07:42 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
2022-06-10T04:07:42.0665836Z Jun 10 04:07:42 	at org.apache.flink.tests.util.AutoClosableProcess.createProcess(AutoClosableProcess.java:193)
2022-06-10T04:07:42.0666758Z Jun 10 04:07:42 	at org.apache.flink.tests.util.AutoClosableProcess.access$100(AutoClosableProcess.java:51)
2022-06-10T04:07:42.0667649Z Jun 10 04:07:42 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:122)
2022-06-10T04:07:42.0668542Z Jun 10 04:07:42 	at org.apache.flink.tests.util.flink.FlinkDistribution.submitSQLJob(FlinkDistribution.java:221)
2022-06-10T04:07:42.0669497Z Jun 10 04:07:42 	at org.apache.flink.tests.util.flink.LocalStandaloneFlinkResource$StandaloneClusterController.submitSQLJob(LocalStandaloneFlinkResource.java:207)
2022-06-10T04:07:42.0670611Z Jun 10 04:07:42 	at org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.executeSqlStatements(PlannerScalaFreeITCase.java:137)
2022-06-10T04:07:42.0671600Z Jun 10 04:07:42 	at org.apache.flink.table.sql.codegen.PlannerScalaFreeITCase.testImperativeUdaf(PlannerScalaFreeITCase.java:125)
2022-06-10T04:07:42.0672439Z Jun 10 04:07:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-10T04:07:42.0673125Z Jun 10 04:07:42 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-10T04:07:42.0673902Z Jun 10 04:07:42 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-10T04:07:42.0674607Z Jun 10 04:07:42 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-10T04:07:42.0675299Z Jun 10 04:07:42 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-10T04:07:42.0676088Z Jun 10 04:07:42 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-10T04:07:42.0676878Z Jun 10 04:07:42 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-10T04:07:42.0677630Z Jun 10 04:07:42 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-10T04:07:42.0678380Z Jun 10 04:07:42 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-06-10T04:07:42.0679128Z Jun 10 04:07:42 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-06-10T04:07:42.0679853Z Jun 10 04:07:42 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-06-10T04:07:42.0680571Z Jun 10 04:07:42 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-06-10T04:07:42.0681265Z Jun 10 04:07:42 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-06-10T04:07:42.0682035Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-10T04:07:42.0682781Z Jun 10 04:07:42 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-10T04:07:42.0683506Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-10T04:07:42.0684224Z Jun 10 04:07:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-10T04:07:42.0684989Z Jun 10 04:07:42 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-10T04:07:42.0685696Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-10T04:07:42.0686344Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-10T04:07:42.0687019Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-10T04:07:42.0687703Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-10T04:07:42.0688468Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-10T04:07:42.0689135Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-10T04:07:42.0689757Z Jun 10 04:07:42 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-06-10T04:07:42.0690332Z Jun 10 04:07:42 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-06-10T04:07:42.0690958Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-10T04:07:42.0691681Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-10T04:07:42.0692427Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-10T04:07:42.0693112Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-10T04:07:42.0693789Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-10T04:07:42.0694485Z Jun 10 04:07:42 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-06-10T04:07:42.0695240Z Jun 10 04:07:42 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-06-10T04:07:42.0695889Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-10T04:07:42.0696550Z Jun 10 04:07:42 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-10T04:07:42.0697176Z Jun 10 04:07:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-10T04:07:42.0697784Z Jun 10 04:07:42 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-10T04:07:42.0698452Z Jun 10 04:07:42 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-10T04:07:42.0699303Z Jun 10 04:07:42 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-10T04:07:42.0700349Z Jun 10 04:07:42 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-10T04:07:42.0701200Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-10T04:07:42.0702274Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-10T04:07:42.0703197Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-10T04:07:42.0704148Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-10T04:07:42.0705083Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-10T04:07:42.0705996Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-10T04:07:42.0706742Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-10T04:07:42.0707576Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-10T04:07:42.0708442Z Jun 10 04:07:42 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-10T04:07:42.0709305Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-06-10T04:07:42.0710054Z Jun 10 04:07:42 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-06-10T04:07:42.0710776Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-06-10T04:07:42.0711700Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-10T04:07:42.0712967Z Jun 10 04:07:42 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-06-10T04:07:42.0713907Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-10T04:07:42.0714653Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-10T04:07:42.0715356Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-10T04:07:42.0716056Z Jun 10 04:07:42 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-10T04:07:42.0716664Z Jun 10 04:07:42 Caused by: java.io.IOException: error=13, Permission denied
2022-06-10T04:07:42.0717213Z Jun 10 04:07:42 	at java.lang.UNIXProcess.forkAndExec(Native Method)
2022-06-10T04:07:42.0717784Z Jun 10 04:07:42 	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
2022-06-10T04:07:42.0718372Z Jun 10 04:07:42 	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
2022-06-10T04:07:42.0718969Z Jun 10 04:07:42 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
2022-06-10T04:07:42.0719531Z Jun 10 04:07:42 	... 65 more
2022-06-10T04:07:42.0719853Z Jun 10 04:07:42 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36510&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27968,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 11:41:49 UTC 2022,,,,,,,,,,"0|z134k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 11:31;hxbks2ks; [~lsy] Could you help take a look?;;;","10/Jun/22 11:31;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36510&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199;;;","10/Jun/22 11:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36510&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","10/Jun/22 11:41;lsy;[~hxbks2ks] This is duplicated with https://issues.apache.org/jira/browse/FLINK-27968;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
security.kerberos.login.principal can be defined wihtout keytab which is wrong configuration,FLINK-28000,13449416,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,10/Jun/22 10:32,13/Jun/22 08:20,04/Jun/24 20:51,13/Jun/22 08:20,1.16.0,,,,,,,,,,,,,,,,,,Connectors / Common,Deployment / Kubernetes,Deployment / YARN,,,0,pull-request-available,security,,,"Please see the parameter description [here|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#security-kerberos-login-principal].
{code:java}
Kerberos principal name associated with the keytab.
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 08:20:48 UTC 2022,,,,,,,,,,"0|z134gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 08:20;mbalassi;66a9c49 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchMethodError when using Hive 3 dialect,FLINK-27999,13449413,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,10/Jun/22 10:08,20/Jun/22 12:34,04/Jun/24 20:51,14/Jun/22 08:32,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"VirtualColumn.VIRTUAL_COLUMN_NAMES uses different types in Hive2 / Hive3, causing NoSuchMethodErrors when the Hive dialect is used.

When use 'mvn test -PHive3.1.1'  Hive connector, it will throw the following exception ""Caused by: MetaException(message:Required table missing : ""DBS"" in Catalog """" Schema """". DataNucleus requires this table to perform its persistence operations. Either your MetaData is incorrect, or you need to enable ""datanucleus.schema.autoCreateTables"")"".

From the error message, we can know the reason is the configuration ""datanucleus.schema.autoCreateTables"" is not true. But when create HiveCatalog, we do really set the configuration ""datanucleus.schema.autoCreateTables"" to true in the hive-site.xml.

After some debuging, I found the reason is that some test modify a static variable, and then boil the other test.

When running HiveCatalogFactoryTest, it will call  HiveCatalog#createHiveConf(@Nullable String hiveConfDir, @Nullable String hadoopConfDir) to create a HiveConf for HiveCatalog. In this method, it will set  the static variable ""hiveSiteURL"" to null.

Then, if we run ""HiveCatalogHiveMetadataTest"", it will call HiveTestUtils#createHiveConf() to create HiveConf. The following code will create a empty HiveConf;

 
{code:java}
HiveConf hiveConf = new HiveConf(); {code}
But in the initialize function of HiveConf, it first will apply all default variables, so ""datanucleus.schema.autoCreateTables"" will be set to false. And the ""hiveSiteURL"" is null, so skip add the resource. Then, it will check """"hive.metastore.schema.verification"" is true or false. If it's true, it will set ""datanucleus.schema.autoCreateTables"" to false. This have a higher priority and thus overwrite the value we configure in hive-site.xml.

To fix it, we only need to reset the static variable ""hiveSiteURL"" with our hive-site.xml.

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28042,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 08:32:46 UTC 2022,,,,,,,,,,"0|z134g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 10:15;martijnvisser;[~luoyuxia] Can you make the title more descriptive / add a description? This ticket is now rather vague;;;","10/Jun/22 10:35;martijnvisser;Thank you so much [~luoyuxia] :);;;","10/Jun/22 10:35;luoyuxia;[~martijnvisser] Thanks for your reminder. I will update it later.;;;","14/Jun/22 08:32;chesnay;master: 69390b71be731557397c4523e818c3fae1af278a
1.15: 90b8e82ee8a868864e65e63076fd723bdc9796d9 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Calcite version to 1.30,FLINK-27998,13449408,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,10/Jun/22 09:40,19/Apr/23 06:59,04/Jun/24 20:51,19/Apr/23 06:46,,,,,,,,,,,,,,,1.18.0,,,,Table SQL / API,Table SQL / Runtime,,,,0,pull-request-available,,,,"The latest available version of Calcite is currently 1.30. We already need to execute the rework that was planned when upgrading to Calcite 1.27 FLINK-20873 and when upgrading to Calcite 1.28 FLINK-21239

When doing this upgrade, we should do this to the last available version. This is needed to resolve multiple bugs. 

Additional note: Flink currently uses Calcite 1.26.0 which has this note on the release page:

{{Warning: Calcite 1.26.0 has severe issues with RexNode simplification caused by SEARCH operator ( wrong data from query optimization like in CALCITE-4325, CALCITE-4352, NullPointerException), so use 1.26.0 for development only, and beware that Calcite 1.26.0 might corrupt your data.}}
https://calcite.apache.org/news/2020/10/06/release-1.26.0/

The following files should be removed from the Flink code base when upgrading calcite to 1.30.0

in `org.apahce.calcite.rel.core`:
 * Correlate
 * Filter
 * Intersect
 * Minus
 * SetOp
 * Sort
 * Union
 * Values
 * Window

in `org.apahce.calcite.rel.hint`:
 * HintPredicates
 * NodeTypeHintPredicate

in `org.apahce.calcite.rel.logical`:
 * LogicalCorrelate
 * LogicalFilter
 * LogicalIntersect
 * LogicalMinus
 * LogicalSort
 * LogicalUnion
 * LogicalValues
 * LogicalWindow",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28518,FLINK-29319,FLINK-31362,FLINK-29932,FLINK-21239,FLINK-20873,FLINK-28678,FLINK-28744,FLINK-31349,FLINK-29540,FLINK-31350,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 06:46:47 UTC 2023,,,,,,,,,,"0|z134f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 15:02;Sergey Nuyanzin;Hi [~martijnvisser] 
could you please assign it to me?
I would volunteer to contribute here. Currently I'm  playing with 1.27. Once it starts passing tests I could go further
Here it is my sandbox
https://github.com/snuyanzin/flink/tree/calcite_update;;;","24/Jun/22 07:31;martijnvisser;[~Sergey Nuyanzin] Perhaps it makes sense to team-up with [~jingzhang] since she also volunteered via the mailing list and knows Calcite quite well. Updating Calcite for Flink is a rather large thing (see https://github.com/apache/flink/pull/13577 as the last one) which ideally is covered by multiple people;;;","24/Jun/22 09:46;Sergey Nuyanzin;thanks for clarification
[~jingzhang] do you mind if I help you with Calcite update;;;","04/Jul/22 02:57;jingzhang;[~Sergey Nuyanzin] Sorry for late response.

Of course, you are welcome.

You could complete the POC and splits into several subtasks.

I could take some subtasks and review the PRs.

What do you think?;;;","20/Jul/22 08:51;martijnvisser;[~Sergey Nuyanzin] Are you OK with that? ;;;","25/Jul/22 21:11;Sergey Nuyanzin;Hi [~martijnvisser] yes, thanks
Sorry still on vacation this week;;;","27/Jul/22 09:25;martijnvisser;[~Sergey Nuyanzin] No worries.
I've talked offline with [~jingzhang] and we agreed that we should directly upgrade to Calcite 1.31 (since that's containing some new functionality that [~jingzhang] brought forward) for Flink 1.17, so there's no immediate rush. ;;;","30/Aug/22 06:41;Sergey Nuyanzin;[~martijnvisser] could you please elaborate about description: it is stated that need to update to 1.30.0 and in last comment you say 1.31.0

so may be 2 questions to clarify
1. should the description be changed to update to 1.31.0?
2. A set of classes mentioned to removed after upgrade to 1.30.0. I had a look at the issue they are referring to [1] and it is [2] which was fixed in 1.31.0. Is it ok to remove them after upgrade to 1.30.0? (may the question is not actual if finally the task it upgrade to 1.31.0)

[1] https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/java/org/apache/calcite/rel/core/Correlate.java#L46
[2] https://issues.apache.org/jira/browse/CALCITE-5107;;;","30/Aug/22 10:56;martijnvisser;[~Sergey Nuyanzin] We usually create different tickets for the update to Calcite, because that makes it possible to track what can be changed in Flink for that specific Calcite version. See FLINK-28744 for the 1.31 ticket. You can either split them in multiple PRs / commits, whatever is easier to review I guess. 

I don't know the answer to the second one, I hope that [~jingzhang] can help there. ;;;","30/Aug/22 11:52;twalthr;[~Sergey Nuyanzin] [~jingzhang] please ping me in the Flink Slack channel if you need a review or otherwise help on this. Upgrading Calcite is not an easy task and we need to avoid to introduce bugs as much as possible. I'm happy to help where I can.;;;","30/Aug/22 11:55;twalthr;Btw maybe it makes sense to apply Calcite version by Calcite version until 1.30. We can also have a feature branch for that. I'm not a big fan of huge PRs with 200+ files changed as we did in the last upgrade. So maybe one PR per Calcite version?;;;","06/Sep/22 06:42;Sergey Nuyanzin;Thanks for the suggestion [~twalthr] and for the proposed help with review, will definitely let you know 
I will try to see if with update to 1.27 would be easier... and if it works then continue version by version
Then all the issues I find within 1.27 I will put under 1.27 update jira issue  FLINK-20873 ;;;","18/Oct/22 02:48;godfreyhe;Can this ticket be closed since ""Upgrade Calcite version to 1.32"" has been created (FLINK-29319) ?

;;;","18/Oct/22 06:54;Sergey Nuyanzin;Since it was suggested moving forward version by version may be it still makes sense to have it open?

P.S. by the way I have a draft PR to update to 1.32.0 passing Flink ci tests however probably it's easier especially for review to go step by step
https://github.com/apache/flink/pull/20922;;;","18/Oct/22 07:18;martijnvisser;[~godfreyhe] We deliberately kept them open because most of the individual tickets have notes on what's relevant for Flink to change when performing the upgrade to that version. ;;;","14/Feb/23 21:40;Sergey Nuyanzin;A comment about issue description

{quote}in `org.apahce.calcite.rel.core`:

    Correlate
    Filter
    Intersect
    Minus
    SetOp
    Sort
    Union
    Values
    Window

in `org.apahce.calcite.rel.hint`:

    HintPredicates
    NodeTypeHintPredicate

in `org.apahce.calcite.rel.logical`:

    LogicalCorrelate
    LogicalFilter
    LogicalIntersect
    LogicalMinus
    LogicalSort
    LogicalUnion
    LogicalValues
    LogicalWindow
{quote}
In fact it could be done after https://issues.apache.org/jira/browse/CALCITE-5107.
Since it happened only at 1.31.0 then most of this removal stuff should be done while upgrading to 1.31.0;;;","19/Apr/23 06:46;Sergey Nuyanzin;Merged to master [423cdcb99c3f66be435ad2e70d6a15f10a69e252|https://github.com/apache/flink/commit/423cdcb99c3f66be435ad2e70d6a15f10a69e252];;;",,,,,,,,,,,,,,,
How to unregister custom metrics at runtime?,FLINK-27997,13449406,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,henvealf,henvealf,10/Jun/22 09:21,15/Mar/23 11:56,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,,,,,"How to custom unregister metrics?

Worry about memory overflow due to too many metrics.

Any suggestions for adding a feature to unregister metrics?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 15 11:56:16 UTC 2023,,,,,,,,,,"0|z134eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 11:56;Wencong Liu;Hello [~henvealf] . Thanks for your proposal. What metrics do you think will cause memory overflow?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect support INTERVAL type,FLINK-27996,13449389,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,10/Jun/22 07:25,21/Jul/22 13:03,04/Jun/24 20:51,21/Jul/22 13:02,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 13:02:57 UTC 2022,,,,,,,,,,"0|z134aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 13:02;jark;Fixed in master: 8cae2e70f2c07a8cfb3d3496312a652e9afba492;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Janino version ,FLINK-27995,13449377,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,fsk119,fsk119,10/Jun/22 06:49,27/Feb/23 13:42,04/Jun/24 20:51,27/Feb/23 13:41,1.16.0,,,,,,,,,,,,,,1.18.0,,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"Currently, the Janino version doesn't support JDK11 well. 

 
[https://lists.apache.org/thread/q052xdn1mnhjm9k4ojjjz22dk4r1xwfz]",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29135,,,,,,,,,,FLINK-20873,FLINK-21239,FLINK-30984,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 27 13:42:40 UTC 2023,,,,,,,,,,"0|z13488:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 07:15;Benenson;Also suggest to add UT (it should fail now), so we can detect similar issues in advance. ;;;","10/Jun/22 08:01;martijnvisser;I'm actually not sure that we can bump Janino without also bumping Calcite, which is a bigger challenge:

{{<!-- Keep Janino in sync with calcite. -->}} 
https://github.com/apache/flink/blob/master/flink-table/pom.xml#L78;;;","10/Jun/22 11:59;jark;I think it's worth trying just bump Janino. Upgrading Calcite is a super large work, and we don't have a plan to do it in 1.16. ;;;","10/Jun/22 13:06;lincoln.86xy;Agree with Jark that try to bump up Janino separately.
Another exception which we've encountered on current Janino is the ""StackOverflowError"" (temporarily solved by adding up stack memory)
this is a knowns issue and fixed in [3.1.1|https://janino-compiler.github.io/janino/changelog.html]
{code}
Got rid of ""CodeContext.flowAnalysis()"" (approx. 500 LOC!), who's sole purpose was to compute the .class files' ""maxStack"" attribute - we now get that through the computed stack maps.
{code};;;","10/Jun/22 13:14;martijnvisser;Thanks for the input Jark and Lincoln!;;;","28/Sep/22 06:12;lsy;[~martijnvisser] Does this ticket have any progress?;;;","28/Sep/22 12:10;martijnvisser;[~lsy] No, but the upgrade work for Calcite has started ;;;","03/Nov/22 04:28;lincoln.86xy;[~fsk119] I'm investigating the problem in the thread[1], but from the exception messages posted, it looks more like an issue with codegen in 1.15 itself, rather than an exception with the janino version on jdk11,  while the other thread[2] appears to be the janino version issue, could you take a look for this again? 

[1] https://lists.apache.org/thread/q052xdn1mnhjm9k4ojjjz22dk4r1xwfz
[2] https://lists.apache.org/thread/9tw165cgpdqz4ron76b1ckmwm9hy4qfd;;;","08/Nov/22 11:25;Sergey Nuyanzin;I will put this comment since working with update of Calcite to 1.28.0 I had to think about update of Janino since Calcite 1.28.0 depends on 3.1.6.
There are several issues on Janino side which should be kept in mind

# There are several issues with code compiliations introduced in 3.1.3 leading to failure of Flink tests in {{AggregateITCase}} and {{OverAggregateITCase}}. They are fixed in 3.1.7 and 3.1.8.
# Since 3.1.3 Janino is unable to compile code with variable initialized in {{while}} condition (required by some Flink tests), link to the issue https://github.com/janino-compiler/janino/issues/185 (fixed in master)
# Since 3.1.3 Janino is unable to compile code with variables initialized in one {{if/else}} branches (required by lots of Flink tests), link to the issue https://github.com/janino-compiler/janino/issues/187 (fixed in master)
# Since 3.1.3 Janino is unable to compile code with ternary operator where the first branch is {{null}} without casting (required by some Flink tests), link to the issue https://github.com/janino-compiler/janino/issues/188 (not fixed yet)

Meanwhile each of these issues could be workarounded like declaration variables with initialisation to some default value and adding explicit cast. In that case Flink tests are passing (still discussable what is better wait for Janino's release with all the fixes or not).



;;;","08/Nov/22 11:33;Sergey Nuyanzin;[~martijnvisser], [~fsk119] could you please assign it to me 
since anyway I monitor all these Janino's issues I mentioned and once Arno (Janino's maintainer) has fixed something I'm going to check this fix against all the Flink-table tests.;;;","08/Nov/22 11:53;Sergey Nuyanzin;About 
[2] https://lists.apache.org/thread/9tw165cgpdqz4ron76b1ckmwm9hy4qfd
mentioned by [~lincoln.86xy]
This is exactly one of the issues I faced during update https://github.com/janino-compiler/janino/issues/186
and it was fixed together with  https://github.com/janino-compiler/janino/issues/187 ;;;","08/Nov/22 13:43;lincoln.86xy;[~Sergey Nuyanzin] Thank you for helping to confirm this issue and pushing for a solution to janino's bug, also the large upgrading work for calcite, great job!;;;","08/Feb/23 09:44;Sergey Nuyanzin;[~martijnvisser] , [~lincoln.86xy] , [~fsk119] 
Right now 2 (which require the majority of workarounds to make it working with 3.1.x) of 3 issues are fixed and release with 3.1.9.

The last one [https://github.com/janino-compiler/janino/issues/188]  is not fixed yet, however it could be easily workarouded by adding explicit cast.

Unfortunately Arno is not responding for long period of time... 
Do you think we can go with 3.1.9 and a WA for the third issue? PR is ready and linked to this Jira issue, existing tests are green.;;;","08/Feb/23 10:29;martijnvisser;[~Sergey Nuyanzin] I would vote for doing the upgrade and applying the workaround, while creating a follow-up ticket that mentions the workaround and the Janino issue number as tech debt.;;;","09/Feb/23 08:05;Sergey Nuyanzin;thanks for the response, i created a follow up issue for that https://issues.apache.org/jira/browse/FLINK-30984
would be really grateful if someone have a look at current issue's PR and give feedback;;;","27/Feb/23 13:42;Sergey Nuyanzin;Merged to master: [455b449731d9b860be5793ab6070c36bebdedae6|https://github.com/apache/flink/commit/455b449731d9b860be5793ab6070c36bebdedae6];;;",,,,,,,,,,,,,,,,
 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinator,FLINK-27994,13449376,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,igniz,igniz,10/Jun/22 06:46,10/Jun/22 07:47,04/Jun/24 20:51,10/Jun/22 07:47,1.14.2,1.14.3,1.14.4,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,,,,,"使用flink cdc 2.2.0报错

2661 [jobmanager-io-thread-1] WARN  org.apache.flink.runtime.util.HadoopUtils  - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).
6604 [SourceCoordinator-Source: MySQL Source] ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinator  - Uncaught exception in the SplitEnumerator for Source Source: MySQL Source while starting the SplitEnumerator.. Triggering job failover.
java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava18/com/google/common/util/concurrent/ThreadFactoryBuilder
    at com.ververica.cdc.connectors.mysql.source.assigners.MySqlSnapshotSplitAssigner.startAsynchronouslySplit(MySqlSnapshotSplitAssigner.java:195)
    at com.ververica.cdc.connectors.mysql.source.assigners.MySqlSnapshotSplitAssigner.open(MySqlSnapshotSplitAssigner.java:163)
    at com.ververica.cdc.connectors.mysql.source.assigners.MySqlHybridSplitAssigner.open(MySqlHybridSplitAssigner.java:95)
    at com.ververica.cdc.connectors.mysql.source.enumerator.MySqlSourceEnumerator.start(MySqlSourceEnumerator.java:101)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$start$0(SourceCoordinator.java:136)
    at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$runInEventLoop$8(SourceCoordinator.java:329)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.ThreadFactoryBuilder
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 13 more 

!image-2022-06-10-14-50-05-003.png!

 ","{color:#0e4a8e}<{color}{color:#458588}properties{color}{color:#0e4a8e}>
{color}{color:#0e4a8e} {color}{color:#bcbf01}<{color}{color:#458588}flink.version{color}{color:#bcbf01}>{color}1.14.4{color:#bcbf01}</{color}{color:#458588}flink.version{color}{color:#bcbf01}>
{color}{color:#bcbf01} <{color}{color:#458588}scala.binary.version{color}{color:#bcbf01}>{color}2.12{color:#bcbf01}</{color}{color:#458588}scala.binary.version{color}{color:#bcbf01}>
{color}{color:#bcbf01} <{color}{color:#458588}hadoop.version{color}{color:#bcbf01}>{color}3.2.1{color:#bcbf01}</{color}{color:#458588}hadoop.version{color}{color:#bcbf01}>
{color}{color:#bcbf01} <{color}{color:#458588}slf4j.version{color}{color:#bcbf01}>{color}1.7.36{color:#bcbf01}</{color}{color:#458588}slf4j.version{color}{color:#bcbf01}>
{color}{color:#bcbf01} <{color}{color:#458588}project.build.sourceEncoding{color}{color:#bcbf01}>{color}UTF-8{color:#bcbf01}</{color}{color:#458588}project.build.sourceEncoding{color}{color:#bcbf01}>
{color}{color:#0e4a8e}</{color}{color:#458588}properties{color}{color:#0e4a8e}>{color}

{color:#bcbf01}<{color}{color:#458588}dependency{color}{color:#bcbf01}>
{color}{color:#bcbf01} {color}{color:#bc0ba2}<{color}{color:#458588}groupId{color}{color:#bc0ba2}>{color}com.alibaba{color:#bc0ba2}</{color}{color:#458588}groupId{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}artifactId{color}{color:#bc0ba2}>{color}fastjson{color:#bc0ba2}</{color}{color:#458588}artifactId{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}version{color}{color:#bc0ba2}>{color}1.2.83{color:#bc0ba2}</{color}{color:#458588}version{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}scope{color}{color:#bc0ba2}>{color}provided{color:#bc0ba2}</{color}{color:#458588}scope{color}{color:#bc0ba2}>
{color}{color:#bcbf01}</{color}{color:#458588}dependency{color}{color:#bcbf01}>
{color}{color:#bcbf01}
{color}{color:#bcbf01}<{color}{color:#458588}dependency{color}{color:#bcbf01}>
{color}{color:#bcbf01} {color}{color:#bc0ba2}<{color}{color:#458588}groupId{color}{color:#bc0ba2}>{color}mysql{color:#bc0ba2}</{color}{color:#458588}groupId{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}artifactId{color}{color:#bc0ba2}>{color}mysql-connector-java{color:#bc0ba2}</{color}{color:#458588}artifactId{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}version{color}{color:#bc0ba2}>{color}8.0.21{color:#bc0ba2}</{color}{color:#458588}version{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}scope{color}{color:#bc0ba2}>{color}provided{color:#bc0ba2}</{color}{color:#458588}scope{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}exclusions{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} {color}{color:#61aa0d}<{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}protobuf-java{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}com.google.protobuf{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#bc0ba2}</{color}{color:#458588}exclusions{color}{color:#bc0ba2}>
{color}{color:#bcbf01}</{color}{color:#458588}dependency{color}{color:#bcbf01}>{color}

{color:#bcbf01}<{color}{color:#458588}dependency{color}{color:#bcbf01}>
{color}{color:#bcbf01} {color}{color:#bc0ba2}<{color}{color:#458588}groupId{color}{color:#bc0ba2}>{color}com.ververica{color:#bc0ba2}</{color}{color:#458588}groupId{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}artifactId{color}{color:#bc0ba2}>{color}flink-connector-mysql-cdc{color:#bc0ba2}</{color}{color:#458588}artifactId{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}version{color}{color:#bc0ba2}>{color}2.2.1{color:#bc0ba2}</{color}{color:#458588}version{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}scope{color}{color:#bc0ba2}>{color}provided{color:#bc0ba2}</{color}{color:#458588}scope{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} <{color}{color:#458588}exclusions{color}{color:#bc0ba2}>
{color}{color:#bc0ba2} {color}{color:#61aa0d}<{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}guava{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}com.google.guava{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}jackson-core{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}com.fasterxml.jackson.core{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}jackson-databind{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}com.fasterxml.jackson.core{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}slf4j-api{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.slf4j{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}kafka-clients{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.apache.kafka{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}jackson-jaxrs-json-provider{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}com.fasterxml.jackson.jaxrs{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}jaxb-api{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}javax.xml.bind{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}javassist{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.javassist{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}jetty-util{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.eclipse.jetty{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}jetty-servlet{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.eclipse.jetty{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}commons-lang3{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.apache.commons{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} <{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#3f9101}<{color}{color:#458588}artifactId{color}{color:#3f9101}>{color}flink-shaded-guava{color:#3f9101}</{color}{color:#458588}artifactId{color}{color:#3f9101}>
{color}{color:#3f9101} <{color}{color:#458588}groupId{color}{color:#3f9101}>{color}org.apache.flink{color:#3f9101}</{color}{color:#458588}groupId{color}{color:#3f9101}>
{color}{color:#3f9101} {color}{color:#61aa0d}</{color}{color:#458588}exclusion{color}{color:#61aa0d}>
{color}{color:#61aa0d} {color}{color:#bc0ba2}</{color}{color:#458588}exclusions{color}{color:#bc0ba2}>
{color}{color:#bcbf01}</{color}{color:#458588}dependency{color}{color:#bcbf01}>
{color}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/22 06:50;igniz;image-2022-06-10-14-50-05-003.png;https://issues.apache.org/jira/secure/attachment/13044906/image-2022-06-10-14-50-05-003.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Fri Jun 10 07:47:56 UTC 2022,,,,,,,,,,"0|z13480:",9223372036854775807,1.13.x暂时不影响,,,,,,,,,,,,,,,,,,,"10/Jun/22 07:47;martijnvisser;[~igniz] Please open ticket related to CDC connectors at https://github.com/ververica/flink-cdc-connectors;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When only use 'Flink SQL' to develop programs, `watermark` can be defined when creating view tables",FLINK-27993,13449370,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,HunterHunter,HunterHunter,10/Jun/22 05:49,10/Jun/22 09:38,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"When I only use 'flex SQL' to develop programs, I cannot define watermarks from view table，I have a data source table a, which has only one field `messge`. I need to parse it into others fields(like `time` field), and then build a view table, but I have no place to define watermark.

Or in other scenarios, I need to perform various transformations from a table to get the `eventtime` field. Or merge and convert from multiple tables to the final table

 

eg:

create table A(

messge string

) with( kafka )

create view table B select udf(messge) as `eventtime`  from A.

I can define `watermark` , as i know, I can only define it when creating a table.

 

My current practice is to mix Table APIs.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-10 05:49:37.0,,,,,,,,,,"0|z1346o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cep StreamExecMatch need check the parallelism and maxParallelism of the two transformation in it,FLINK-27992,13449369,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dwysakowicz,jackylau,jackylau,10/Jun/22 05:47,19/Jan/24 09:28,04/Jun/24 20:51,18/Jan/24 14:32,1.16.0,,,,,,,,,,,,,,1.19.0,,,,Library / CEP,,,,,0,pull-request-available,,,,"StreamExecMatch node has two transformation (StreamRecordTimestampInserter -> Match), the upstream of StreamExecMatch is hash edge when use set different parallelism and maxParallelism it will cause problem.

because the window operator using downstream node's max parallelism compute keygroup and cep operator  using max parallelism of itself and it may not equal

such as:

window - --(hash edge)>  StreamRecordTimestampInserter --(forward edge)–> Cep ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 18 15:08:32 UTC 2024,,,,,,,,,,"0|z1346g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 14:30;martijnvisser;[~dwysakowicz] I've upgraded this ticket to a Blocker because I think it should be fixed for the next 1.18 patch release, if you disagree feel free to lower it. ;;;","18/Jan/24 14:17;dwysakowicz;Both operators use the same parallelism. Match uses the parallelism of the input. There is a different issue though that Match uses `ChainingStrategy.HEAD` which puts `StreamRecordInserter` and `Match` into separate chains adding unwanted `FORWARD` exchange.;;;","18/Jan/24 14:18;dwysakowicz;Fixed in 201571b486f405358a31e077247241892d537198;;;","18/Jan/24 15:08;martijnvisser;[~dwysakowicz] Will we also backport to 1.18? Or just 1.19.0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC format supports reporting statistics,FLINK-27991,13449367,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,10/Jun/22 05:42,15/Jul/22 06:38,04/Jun/24 20:51,15/Jul/22 06:38,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 15 06:38:18 UTC 2022,,,,,,,,,,"0|z13460:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 06:38;godfreyhe;Fixed in master: a521e82a47384ad88e2424f9f6734f0c6d1f9b14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet format supports reporting statistics,FLINK-27990,13449366,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,10/Jun/22 05:42,14/Jul/22 13:17,04/Jun/24 20:51,14/Jul/22 13:17,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 13:17:48 UTC 2022,,,,,,,,,,"0|z1345s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 13:17;godfreyhe;Fixed in master: 4bafbe250602f90e7a5c9692bddc3e14c3be8911;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV format supports reporting statistics,FLINK-27989,13449365,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,10/Jun/22 05:42,13/Jul/22 12:39,04/Jun/24 20:51,13/Jul/22 12:39,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 12:39:37 UTC 2022,,,,,,,,,,"0|z1345k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 12:39;godfreyhe;Fixed in master: dbc7ff712a1694262629b9a349f7a1fa46240b6b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let HiveTableSource extend from SupportsStatisticReport,FLINK-27988,13449364,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,10/Jun/22 05:41,22/Aug/22 02:01,04/Jun/24 20:51,03/Aug/22 08:24,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29046,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 08:24:56 UTC 2022,,,,,,,,,,"0|z1345c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 08:24;godfreyhe;Fixed in master: 5f2d088a2713ced5c6ce072db92f4378f73bc739;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let FileSystemTableSource extend from SupportsStatisticReport,FLINK-27987,13449363,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Jun/22 05:40,17/Jun/22 05:34,04/Jun/24 20:51,17/Jun/22 05:34,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 05:34:44 UTC 2022,,,,,,,,,,"0|z13454:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 05:34;godfreyhe;Fixed in master: a34d0a8b3fb26473c61c63a786fffe63c651c08c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor the name of finish method for JdbcOutputFormatBuilder,FLINK-27986,13449362,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,leo65535,leo65535,10/Jun/22 05:38,18/Jan/24 07:55,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-minor,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:03 UTC 2023,,,,,,,,,,"0|z1344w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FlinkRecomputeStatisticsProgram to compute statistics after filter push and partition pruning,FLINK-27985,13449361,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Jun/22 05:38,17/Jun/22 05:35,04/Jun/24 20:51,17/Jun/22 05:35,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 05:35:08 UTC 2022,,,,,,,,,,"0|z1344o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 05:35;godfreyhe;Fixed in master: a251888dce5aab3f76d066241d84061b87658bb3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce FileBasedStatisticsReportableInputFormat interface,FLINK-27984,13449360,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Jun/22 05:36,17/Jun/22 05:34,04/Jun/24 20:51,17/Jun/22 05:34,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 05:34:18 UTC 2022,,,,,,,,,,"0|z1344g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 05:34;godfreyhe;Fixed in master: 2011a4a9bab947f3819e139faa3341e765a9505d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SupportsStatisticsReport interface,FLINK-27983,13449359,13449358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Jun/22 05:35,17/Jun/22 05:33,04/Jun/24 20:51,17/Jun/22 05:33,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 05:33:54 UTC 2022,,,,,,,,,,"0|z13448:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 05:33;godfreyhe;Fixed in master: a2e7416e132e3f980c844f3fe4007a4ad306e324;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-231: Introduce SupportsStatisticReport to support reporting statistics from source connectors,FLINK-27982,13449358,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,10/Jun/22 05:33,03/Aug/22 08:25,04/Jun/24 20:51,03/Aug/22 08:25,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,,,,,https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=211883860&draftShareId=eda17eaa-43f9-4dc1-9a7d-3a9b5a4bae00&,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 08:25:29 UTC 2022,,,,,,,,,,"0|z13440:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 08:25;godfreyhe;all fixed in 1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add python examples to document of ""Time Attributes""",FLINK-27981,13449345,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pemide,pemide,pemide,10/Jun/22 04:00,29/Aug/22 02:32,04/Jun/24 20:51,29/Aug/22 02:32,,,,,,,,,,,,,,,1.16.0,,,,API / Python,Documentation,,,,0,,,,,"Add python examples to document of Time Attributes""

document url: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/time_attributes/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 05:25:25 UTC 2022,,,,,,,,,,"0|z13414:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 04:00;pemide; [~dianfu]  Could i take this ticket?;;;","10/Jun/22 05:25;dianfu;[~pemide] Done~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add python examples to document of ""DataStream API Integration"" ",FLINK-27980,13449343,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pemide,pemide,pemide,10/Jun/22 03:58,29/Aug/22 02:31,04/Jun/24 20:51,29/Aug/22 02:31,,,,,,,,,,,,,,,1.16.0,,,,API / Python,Documentation,,,,0,,,,,"Add python examples to document of ""DataStream API Integration"" 

document urls: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/data_stream_api/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 05:25:03 UTC 2022,,,,,,,,,,"0|z1340o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 03:59;pemide;[~dianfu] Could i take this ticket?;;;","10/Jun/22 05:25;dianfu;[~pemide] Done ~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to upgrade session cluster in a more fine grained way,FLINK-27979,13449335,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,10/Jun/22 02:53,27/Jun/22 13:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Currently, we upgrade the session cluster by delete the session cluster directly, which do not respect to the upgrade mode of the session job. I think we could improve it by performing the upgrade in two phase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 27 13:35:40 UTC 2022,,,,,,,,,,"0|z133yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/22 09:22;ConradJam;I think we can define a cluster template and then create a basic cluster based on the template and allow users to submit jobs to the cluster once it is up and running;;;","27/Jun/22 13:35;aitozi;This issue is mean to support the upgrade session cluster in a more refined way;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update spotless and add add-exports to support jdk17,FLINK-27978,13449299,13281165,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,Sergey Nuyanzin,Sergey Nuyanzin,09/Jun/22 19:01,09/Jun/22 19:04,04/Jun/24 20:51,09/Jun/22 19:04,,,,,,,,,,,,,,,,,,,Build System,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 09 19:04:47 UTC 2022,,,,,,,,,,"0|z133r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 19:04;Sergey Nuyanzin;created by mistake;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SavePoints housekeeping API in Flink Cli, Rest API, SQL client",FLINK-27977,13449295,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,09/Jun/22 18:25,09/Jun/22 20:03,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Command Line Client,Runtime / REST,Table SQL / Client,,,0,,,,,"We ran into this issue that a lot of savepoints have been created by customers (via their apps). It will take extra (hacking) effort to clean it. 

We should support Savepoints housekeeping to delete all savepoints:
 # REST API - /savepoints-disposal{*}{*}
 # Flink CLI - {{$ ./bin/flink savepoint --disposeAll}}
 # SQL client - DROP SAVEPOINTS (alternative option could be DROP SAVEPOINT ALL, but ALL is a SQL keyword) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-09 18:25:22.0,,,,,,,,,,"0|z133q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[WebUi] Allow order by jobname,FLINK-27976,13449261,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eskabetxe,eskabetxe,eskabetxe,09/Jun/22 14:59,13/Sep/22 13:36,04/Jun/24 20:51,13/Sep/22 13:36,,,,,,,,,,,,,,,1.17.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,Allow to order jobs (running and canceled) by job name,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 13 13:36:23 UTC 2022,,,,,,,,,,"0|z133io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 13:36;martijnvisser;Fixed in master: a91501f37262b461f7c645defe805686f7e274dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary RBAC rules from operator,FLINK-27975,13449252,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeesmon,mbalassi,mbalassi,09/Jun/22 14:15,02/Jul/22 08:14,04/Jun/24 20:51,02/Jul/22 08:14,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"[~jeesmon] reported the following RBAC rules obsolete:

{code}
 - apiGroups:
      - flink-operator
    resources:
      - ""*""
    verbs:
      - ""*""
{code}

https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/templates/rbac.yaml#L24-L29

Also * on nodes was flagged in his security review, rightfully. The rule seems too permissive in my opinion too. As far as I remember it was needed for our services potentially using NodePort (we use ClusterIp by default). This should be properly verified and tidied up. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jul 02 08:14:24 UTC 2022,,,,,,,,,,"0|z133go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 12:12;jeesmon;[~mbalassi] Can you please assign this to me if you haven't started working on it? I will try to work on this next few days. Thanks.;;;","10/Jun/22 13:24;mbalassi;Sure, done.;;;","30/Jun/22 10:46;mbalassi;Hi [~jeesmon] are you still planning to pick this up? Otherwise I will do it next week.;;;","30/Jun/22 12:04;jeesmon;[~mbalassi] Thanks for checking. I was waiting on internal approval for contribution and it is approved now. I will try to work on it either today or tomorrow. I will add a comment if I cannot get to it and you can take it next week.;;;","02/Jul/22 08:14;mbalassi;0997fe4 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Potentially wrong classloader being used to create dynamic table sources,FLINK-27974,13449250,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,09/Jun/22 14:11,13/Jun/22 07:57,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"A user reported an issue on slack where a job fails in the CLI because of {{ClassNotFoundException: org.apache.flink.table.planner.delegation.ParserFactory}} in {{FileSystemTableFactory#formatFactoryExists}} when trying to load the {{Factory}} service.

While looking through the call stack I noticed that the classloader passed via the context is a thread's context classloader, set in {{CatalogSourceTable#createDynamicTableSource}}.

This seems a bit fishy; since this runs in the context of the CLI this CL is likely the user CL, but the planner classes are loaded in a separate classloader (not in the parent). As a result the planner classes cannot be looked up via the service loader mechanism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 07:57:42 UTC 2022,,,,,,,,,,"0|z133g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/22 20:32;djiangxu;This can be replicated by pyflink-shell from 1.15.0. Using 1.14.4 pyflink-shell would not produce the error
{code}
./pyflink-shell.sh local
import tempfile
import os
import shutil

sink_path = tempfile.gettempdir() + '/streaming.csv'
if os.path.exists(sink_path):
    if os.path.isfile(sink_path):
        os.remove(sink_path)
    else:
        shutil.rmtree(sink_path)

s_env.set_parallelism(2)
t = st_env.from_elements([(1, 'hi', 'hello'), (2, 'hi', 'hello')], ['a', 'b', 'c'])
st_env.execute_sql(f""create temporary table stream_sink (a bigint, b string, c string) with ('connector' = 'filesystem', 'path' = '{sink_path}', 'format' = 'csv')"")
t.select(""a + 1, b, c"").execute_insert(""stream_sink"").wait()
{code}
{code}
...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/djiang/flink/flink-1.15.0/opt/python/pyflink.zip/pyflink/table/table.py"", line 1107, in execute_insert
  File ""/Users/djiang/flink/flink-1.15.0/opt/python/py4j-0.10.9.3-src.zip/py4j/java_gateway.py"", line 1321, in __call__
  File ""/Users/djiang/flink/flink-1.15.0/opt/python/pyflink.zip/pyflink/util/exceptions.py"", line 146, in deco
  File ""/Users/djiang/flink/flink-1.15.0/opt/python/py4j-0.10.9.3-src.zip/py4j/protocol.py"", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o74.executeInsert.
: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.stream_sink'.

Table options are:

'connector'='filesystem'
'format'='csv'
'path'='/var/folders/47/d25xsmfd0_ldv7cx2lfywgvh0000gs/T/streaming.csv'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262)
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222)
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861)
	at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56)
	at org.apache.flink.table.api.Table.executeInsert(Table.java:1470)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/table/planner/delegation/ParserFactory
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at java.util.Iterator.forEachRemaining(Iterator.java:116)
	at org.apache.flink.connector.file.table.FileSystemTableFactory.formatFactoryExists(FileSystemTableFactory.java:205)
	at org.apache.flink.connector.file.table.FileSystemTableFactory.discoverDecodingFormat(FileSystemTableFactory.java:171)
	at org.apache.flink.connector.file.table.FileSystemTableFactory.createDynamicTableSink(FileSystemTableFactory.java:90)
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259)
	... 30 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.planner.delegation.ParserFactory
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 52 more
{code};;;","13/Jun/22 07:57;chesnay;[~djiangxu] were you using the hive connector?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableWrite and TableCommit as an abstraction layer above FileStore for writing RowData,FLINK-27973,13449212,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,09/Jun/22 11:29,10/Jun/22 02:57,04/Jun/24 20:51,10/Jun/22 02:57,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"In this step we introduce {{TableWrite}} and {{TableCommit}}. They are an abstraction layer above {{FileStoreWrite}}, {{FileStoreCommit}} and {{FileStoreExpire}} to provide RowData writing and committing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 02:57:43 UTC 2022,,,,,,,,,,"0|z1338g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 02:57;lzljs3620320;master: 2b9c186e771db18af08bac2a6a2f8cd6f1a084ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition between task/savepoint notification failure,FLINK-27972,13449200,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,09/Jun/22 10:35,22/Jun/22 06:55,04/Jun/24 20:51,22/Jun/22 06:55,1.15.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"When a task throws an exception in notifyCheckpointComplete we send 2 messages to the JobManager:
1) we inform the CheckpointCoordinator about the failed savepoint
2) we inform the scheduler about the failed task.

Depending on how these arrive the adaptive scheduler exhibits different behaviors. If 1) arrives first it properly informs the user about the created savepoint which might contain uncommitted transactions; if 2) arrives first it just restarts the job.

I'm not sure how big of an issue the latter case is, but it does invalidate FLINK-26923.

In any case we might want to consider having the StopWithSavepoint state wait until the savepoint future has failed before doing anything else.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27869,,,,,,,FLINK-26923,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 22 06:55:37 UTC 2022,,,,,,,,,,"0|z13368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/22 06:55;chesnay;master: d13cb056912d9011df96671c3bd60299a59a1117;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-orc and flink-orc-nohive,FLINK-27971,13449186,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,09/Jun/22 09:15,27/Oct/22 14:40,04/Jun/24 20:51,27/Oct/22 14:40,,,,,,,,,,,,,,,1.17.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 14:40:34 UTC 2022,,,,,,,,,,"0|z13334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 09:16;rskraba;Hello!  Can I be assigned this JIRA?;;;","14/Jun/22 08:51;Zsigner;Hi [~rskraba]  ，I have already upgraded the flink-json module before. May I ask if there are any problems that need to be revised?

FLINK-27352;;;","15/Jun/22 07:26;rskraba;Hello [~Zsigner] , please excuse the mistake – I was going through the flink-format modules in alphabetical order, and forgot to skip flink-json (already done).  I took the liberty of renaming the subtask to flink-orc instead of closing it.;;;","27/Oct/22 14:40;mapohl;master: 1b857a287a036cbd784a6ec7d404639629fa8f0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-hadoop-buik,FLINK-27970,13449185,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,rskraba,rskraba,rskraba,09/Jun/22 09:15,21/Feb/24 08:54,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 10:35:14 UTC 2023,,,,,,,,,,"0|z1332w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 09:16;rskraba;Hello!  Can you assign this Jira to me?;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamPhysicalOverAggregate doesn't support consuming update and delete changes,FLINK-27969,13449181,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Delivered,,SpongebobZ,SpongebobZ,09/Jun/22 08:43,29/Apr/24 16:04,04/Jun/24 20:51,29/Apr/24 15:58,1.14.3,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"Exception trace:
{code:java}
// exception
StreamPhysicalOverAggregate doesn't support consuming update and delete changes which is produced by node Join(joinType=[LeftOuterJoin], where=[(COL2 = COL4)], select=[...], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey]) {code}
FlinkSQL that scheduled as streaming table like this:
{code:java}
// dml
SELECT RANK() OVER (PARTITION BY A.COL1 ORDER BY A.COL2) AS ODER_ONUM
FROM A
INNER JOIN B ON A.COL1 = B.COL1
LEFT JOIN C ON C.COL3 = 1 AND CAST(A.COL2 AS STRING) = C.COL4{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19059,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 10 16:00:48 UTC 2023,,,,,,,,,,"0|z13320:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 16:00;rmetzger;I think this is a known limitation, tracked here: https://issues.apache.org/jira/browse/FLINK-19059;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
end-to-end-tests-sql CI test failed,FLINK-27968,13449173,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,ana4,ana4,09/Jun/22 08:07,10/Jun/22 13:09,04/Jun/24 20:51,10/Jun/22 13:09,1.16.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Build System / CI,Table SQL / Client,Table SQL / Planner,Tests,,0,pull-request-available,,,," 
{code:java}
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Jun 09 03:15:01 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Jun 09 03:15:01 Caused by: java.io.IOException: error=13, Permission denied
Jun 09 03:15:01 	at java.lang.UNIXProcess.forkAndExec(Native Method)
Jun 09 03:15:01 	at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
Jun 09 03:15:01 	at java.lang.ProcessImpl.start(ProcessImpl.java:134)
Jun 09 03:15:01 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
Jun 09 03:15:01 	... 65 more
Jun 09 03:15:01 
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [INFO] Results:
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [ERROR] Errors: 
Jun 09 03:15:02 [ERROR] PlannerScalaFreeITCase.testImperativeUdaf
Jun 09 03:15:02 [ERROR]   Run 1: Cannot run program ""/tmp/junit915579470100095315/junit4815507674620015662/bin/sql-client.sh"": error=13, Permission denied
Jun 09 03:15:02 [ERROR]   Run 2: Cannot run program ""/tmp/junit5631176215080579455/junit3588658300175738616/bin/sql-client.sh"": error=13, Permission denied
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [INFO] 
Jun 09 03:15:02 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0
 {code}
 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36468&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461]",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28001,,,,,FLINK-27606,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 13:09:24 UTC 2022,,,,,,,,,,"0|z13308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 08:37;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36472&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","09/Jun/22 08:39;chesnay;yeah let's just disable that thing because it also fails for me.;;;","09/Jun/22 08:40;chesnay;Disabled in b77582f0bb234196a0d6e17da753d4567a0642c3.;;;","09/Jun/22 08:41;chesnay;It's weird that it runs in the misc profile anyway; it should run in e2e_2.;;;","10/Jun/22 13:09;chesnay;master: ca47e88a91ac28e962c9ac586bdcf983a36027d1
1.15.1: 52b3569e01607e2eda598511eeb0951e7f9238dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to inject entropy on s3 prefix when doing a save point in Flink,FLINK-27967,13449161,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,vinay._.devadiga,vinay._.devadiga,09/Jun/22 07:10,17/Jun/22 11:38,04/Jun/24 20:51,17/Jun/22 11:38,1.14.0,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,,"Hi while using flink 1.14.0,I ran the example job /examples/streaming/StateMachineExample.jar and the job submitted successfully and then I tried to save point it to s3 with entropy enabled but the entropy key was not respected here are my configurations.Can anyone please guide me through issue, I an trying to go through the code but could not find anything sustainable.
|flink-conf|fs.allowed-fallback-filesystems|s3|Cluster configuration| |
|flink-conf|execution.checkpointing.unaligned|true|Cluster configuration| |
|flink-conf|state.backend.incremental|true|Cluster configuration| |
|flink-conf|execution.checkpointing.timeout|600min|Cluster configuration| |
|flink-conf|execution.checkpointing.externalized-checkpoint-retention|RETAIN_ON_CANCELLATION|Cluster configuration| |
|flink-conf|state.backend|rocksdb|Cluster configuration| |
|flink-conf|s3.entropy.key|_entropy_|Cluster configuration| |
|flink-conf|state.checkpoints.dir|s3://vinaydevuswest2/_entropy_/flink/checkpoint-data/|Cluster configuration| |
|flink-conf|execution.checkpointing.max-concurrent-checkpoints|1|Cluster configuration| |
|flink-conf|execution.checkpointing.min-pause|5000|Cluster configuration| |
|flink-conf|execution.checkpointing.checkpoints-after-tasks-finish.enabled|true|Cluster configuration| |
|flink-conf|state.savepoints.dir|s3://vinaydevuswest2/_entropy_/flink/savepoint-data/|Cluster configuration| |
|flink-conf|state.storage.fs.memory-threshold|0|Cluster configuration| |
|flink-conf|s3.entropy.length|4|Cluster configuration| |
|flink-conf|execution.checkpointing.tolerable-failed-checkpoints|30|Cluster configuration| |
|flink-conf|execution.checkpointing.mode|EXACTLY_ONCE|Cluster configuration|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/22 06:50;vinay._.devadiga;Screenshot 2022-06-09 at 12.10.51 PM.png;https://issues.apache.org/jira/secure/attachment/13044821/Screenshot+2022-06-09+at+12.10.51+PM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 10:45:28 UTC 2022,,,,,,,,,,"0|z132xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 07:15;martijnvisser;[~vinay._.devadiga] I'm wondering if you're suffering from the same problem as FLINK-24878;;;","09/Jun/22 15:34;vinay._.devadiga;Hi [~martijnvisser] the problem is same but I tried the solution there you can see in my configuration 
|conf|state.storage.fs.memory-threshold|0|

but still entropy key is not respected.;;;","09/Jun/22 16:26;vinay._.devadiga;[~martijnvisser]  Savepoint completed. Path: s3://vinaydevuswest2/{_}entropy{_}/flink/savepoint-data/savepoint-238579-bc904c407d82 this is the path where it  got saved .

 

 

According to the documentation - f entropy injection is activated, a configured substring in the path is replaced with random characters. For example, path {{s3://my-bucket/checkpoints/{_}entropy{_}/dashboard-job/}} would be replaced by something like {{s3://my-bucket/checkpoints/gf36ikvg/dashboard-job/}}

{{}};;;","10/Jun/22 11:35;martijnvisser;[~alpinegizmo] Any ideas on this? ;;;","10/Jun/22 13:21;danderson;[~vinay._.devadiga] You should expect to the find the checkpoint metadata at the path you mentioned (the one where entropy injection has _not_ occurred), but in addition to this you should also find that the checkpoint data has been written to paths that have had entropy added in.

If that's not the case, I don't have an explanation.;;;","17/Jun/22 10:45;prabhujoseph;We identified this happening due to a configuration issue. Flink supports entropy injection only when FlinkS3FileSystem is used and not when Hadoop S3FileSystem is used. Adding below in flink-conf.yaml has fixed the issue

{code}
yarn.ship-files: /usr/lib/flink/opt/flink-s3-fs-hadoop-1.14.0.jar
#fs.allowed-fallback-filesystems: s3
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
All the classes move to the connector specific files,FLINK-27966,13449120,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,ana4,ana4,09/Jun/22 03:40,08/Aug/22 08:01,04/Jun/24 20:51,08/Aug/22 07:59,,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,," If all the classes are placed `connectors/__init__.py`, conflicts may happen that two classes belonging to two different connectors having the same name.

https://github.com/apache/flink/pull/19732#discussion_r883361009",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 08:01:20 UTC 2022,,,,,,,,,,"0|z132og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 04:08;ana4;I would like to take this.;;;","08/Aug/22 07:59;dianfu;Merged to master via 9dd2b19995beb766f0cd0986b079f2d210c1e836;;;","08/Aug/22 08:01;dianfu;[~ana4] Since it's closing to the code freeze of 1.16 and this is very important to clean up the code structure, I have taken over this issue and addressed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXPLAIN PLAN produces wrong query schema for insert clause with static partition,FLINK-27965,13449115,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,09/Jun/22 03:22,15/Jun/22 06:50,04/Jun/24 20:51,15/Jun/22 06:50,1.15.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"h3. How to reproduce

Add the following test under TableEnvironmentITCase
{code:java}
@Test
def testExplainInsertOverwrite(): Unit = {
  val sinkPath = tempFolder.newFolder().toString
  tEnv.executeSql(
    s""""""
       |create table MySink (
       |  first string,
       |  part string
       |) partitioned by (part)
       |with (
       |  'connector' = 'filesystem',
       |  'path' = '$sinkPath',
       |  'format' = 'testcsv'
       |)
     """""".stripMargin)

    tEnv.executeSql(
      ""explain plan for "" +
        ""insert overwrite MySink partition (part = '123') "" +
        ""select first from MySink where part = '123'"")
}
{code}
h3. Stacktrace
{code:java}
org.apache.flink.table.api.ValidationException: Column types of query result and sink for 'default_catalog.default_database.MySink' do not match.
Cause: Different number of columns.Query schema: [first: STRING, EXPR$1: STRING NOT NULL, EXPR$2: STRING NOT NULL]
Sink schema:  [first: STRING, part: STRING]    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.createSchemaMismatchException(DynamicSinkUtils.java:453)
    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.validateSchemaAndApplyImplicitCast(DynamicSinkUtils.java:256)
    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.convertSinkToRel(DynamicSinkUtils.java:208)
    at org.apache.flink.table.planner.connectors.DynamicSinkUtils.convertSinkToRel(DynamicSinkUtils.java:170)
    at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translateToRel$1(PlannerBase.scala:258)
    at scala.Option.map(Option.scala:146)
    at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:228)
    at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$getExplainGraphs$2(PlannerBase.scala:508)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike.map(TraversableLike.scala:233)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:487)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:93)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:50)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:664)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1298)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:701)
    at org.apache.flink.table.api.TableEnvironmentITCase.testExplainInsertOverwrite(TableEnvironmentITCase.scala:179)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22155,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 06:50:34 UTC 2022,,,,,,,,,,"0|z132nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/22 15:54;qingyue;Another case

Add ""explain plan for"" under TableSinkJsonPlanTest
{code:java}
@Test
public void testPartitioning() {
    String sinkTableDdl =
            ""CREATE TABLE MySink (\n""
                    + ""  a bigint,\n""
                    + ""  b int,\n""
                    + ""  c varchar\n""
                    + "") partitioned by (c) with (\n""
                    + ""  'connector' = 'filesystem',\n""
                    + ""  'format' = 'testcsv',\n""
                    + ""  'path' = '/tmp')"";
    tEnv.executeSql(sinkTableDdl);
    util.verifyJsonPlan(""insert into MySink partition (c='A') select a, b from MyTable""); // ok
    tEnv.explainSql(""insert into MySink partition (c='A') select a, b from MyTable""); // ok
    tEnv.executeSql(
                    ""explain plan for ""
                            + ""insert into MySink partition (c='A') select a, b from MyTable"")
            .print(); // throw exception
} {code};;;","15/Jun/22 06:50;lzljs3620320;master: 10ee79ac8c3cd8698d1696d4689d31292f42d8aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Cassandra connector in Python DataStream API,FLINK-27964,13449112,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pemide,pemide,pemide,09/Jun/22 03:09,07/Jul/22 07:31,04/Jun/24 20:51,29/Jun/22 07:19,,,,,,,,,,,,,,,1.16.0,,,,API / Python,Connectors / Cassandra,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 29 07:19:11 UTC 2022,,,,,,,,,,"0|z132mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 03:13;pemide;[~dianfu] I am very interested in this issue. Could you please assign this issue to me?;;;","09/Jun/22 03:20;dianfu;[~pemide] Done~;;;","29/Jun/22 07:19;dianfu;Merged to master via 78c30cad69584625718640792c8102a4af3e1e45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkRuntimeException in KafkaSink causes a Flink job to hang,FLINK-27963,13449084,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,igaevd,igaevd,08/Jun/22 22:22,19/Aug/23 10:35,04/Jun/24 20:51,,1.14.4,1.15.0,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,FlinkRuntimeException,KafkaSink,"If FlinkRuntimeException occurs in the [KafkaSink|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/kafka/#kafka-sink] then the Flink job tries to re-send failed data  again and gets into endless loop ""exception->send again""

*Code sample which throws the FlinkRuntimeException:*

{code:java}
int numberOfRows = 1;
    int rowsPerSecond = 1;

    DataStream<String> stream = environment.addSource(
                    new DataGeneratorSource<>(
                            RandomGenerator.stringGenerator(1050000), // max.message.bytes=1048588
                            rowsPerSecond,
                            (long) numberOfRows),
                    TypeInformation.of(String.class))
            .setParallelism(1)
            .name(""string-generator"");


    KafkaSinkBuilder<String> builder = KafkaSink.<String>builder()
            .setBootstrapServers(""localhost:9092"")
            .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
            .setRecordSerializer(
                    KafkaRecordSerializationSchema.builder().setTopic(""test.output"")
                            .setValueSerializationSchema(new SimpleStringSchema())
                            .build());


    KafkaSink<String> sink = builder.build();

    stream.sinkTo(sink).setParallelism(1).name(""output-producer""); {code}
*Exception Stack Trace:*
{code:java}
2022-06-02/14:01:45.066/PDT [flink-akka.actor.default-dispatcher-4] INFO output-producer: Writer -> output-producer: Committer (1/1) (a66beca5a05c1c27691f7b94ca6ac025) switched from RUNNING to FAILED on 271b1b90-7d6b-4a34-8116-3de6faa8a9bf @ 127.0.0.1 (dataPort=-1). org.apache.flink.util.FlinkRuntimeException: Failed to send data to Kafka null with FlinkKafkaInternalProducer{transactionalId='null', inTransaction=false, closed=false} at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.throwException(KafkaWriter.java:440) ~[flink-connector-kafka-1.15.0.jar:1.15.0] at org.apache.flink.connector.kafka.sink.KafkaWriter$WriterCallback.lambda$onCompletion$0(KafkaWriter.java:421) ~[flink-connector-kafka-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:353) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:317) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753) ~[flink-streaming-java-1.15.0.jar:1.15.0] at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) ~[flink-runtime-1.15.0.jar:1.15.0] at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-runtime-1.15.0.jar:1.15.0] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) ~[flink-runtime-1.15.0.jar:1.15.0] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) ~[flink-runtime-1.15.0.jar:1.15.0] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292] Caused by: org.apache.kafka.common.errors.RecordTooLargeException: The message is 1050088 bytes when serialized which is larger than 1048576, which is the value of the max.request.size configuration. {code}
**",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Sat Aug 19 10:35:04 UTC 2023,,,,,,,,,,"0|z132gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 07:04;martijnvisser;I don't think this is a bug on Flink's end. Kafka is specifically returning an exception that the message is larger than the max.request.size. Flink is unaware of why Kafka won't produce it, so it will retry indefinitely. The fix is also not on Flink's end, but on Kafka's end: you'll need to increase the max.request.size. ;;;","09/Jun/22 16:50;igaevd;[~martijnvisser], thank you for answering!  The RecordTooLargeException is just an example. The problem is that the Flink job stops processing all other messages/data and gets stuck in a Restarting-Failed-Restarting loop after the only one message is failed to be produced due to any runtime exception. For example, one of our Flink jobs processes ~1k messages per minute through ~10 Flink operators (Functions, Mappers etc.) while their parallelism is set to value from 32 to 134. If any runtime exception occurs then whole message processing stops. It looks like a great candidate for DDOS attackers. It doesn't look like the failover system, it requires some strategy to keep processing data and manage any exception wherever they come from, otherwise this is the bug. By the way, the Kafka server doesn't stop processing requests after the RecordTooLargeException has happened, it is why I think this is an issue on Flink's end.  ;;;","09/Jun/22 19:37;igaevd;[~martijnvisser], I have an idea how any exception which comes from Kafka side could be managed properly by developers who use Flink Kafka Connectors in their projects.

*The idea:*  
To allow to overwrite the [KafkaWriter::deliveryCallback|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L90] field (which is private final field in the 1.15.0 and 1.14.4 versions as well as in the future 1.16 version) by a custom delivery callback. The custom callback may suppress an [ApiException|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/errors/ApiException.java], if it is required by a project architecture. Also, if the deliveryCallback is set to null then it is not going to be used in the [KafkaProducer::doSend|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L966] method at all. In this ""deliveryCallback =null"" case, all exceptions could be managed in a custom [producer interceptor|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/ProducerInterceptor.java] because the ""[this.interceptors.onSendError(record, tp, e);|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1062]"" is called in all catches in the [KafkaProducer::doSend|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L966]

From my prospective of view,  ""to manage exceptions"" means to skip failed messages without the a Restarting-Failed-Restarting loop, just log errors and do not stop the whole data processing.

 

*Possible solution:*

The new ""Callback deliveryCallback"" input parameter in the [KafkaWriter constructor|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L132]
{code:java}
KafkaWriter(
        DeliveryGuarantee deliveryGuarantee,
        Properties kafkaProducerConfig,
        String transactionalIdPrefix,
        Sink.InitContext sinkInitContext,
        KafkaRecordSerializationSchema<IN> recordSerializer,
        SerializationSchema.InitializationContext schemaContext,
        Collection<KafkaWriterState> recoveredStates,
        Callback deliveryCallback) {
    this.deliveryGuarantee = checkNotNull(deliveryGuarantee, ""deliveryGuarantee"");
    this.kafkaProducerConfig = checkNotNull(kafkaProducerConfig, ""kafkaProducerConfig"");
    this.transactionalIdPrefix = checkNotNull(transactionalIdPrefix, ""transactionalIdPrefix"");
    this.recordSerializer = checkNotNull(recordSerializer, ""recordSerializer"");
    this.deliveryCallback = deliveryCallback != null ? deliveryCallback : new WriterCallback(
                    sinkInitContext.getMailboxExecutor(),
                    sinkInitContext.<RecordMetadata>metadataConsumer().orElse(null));
 {code}
 

*Some details:*

To produce/sink a message there is the following call stack (flink-connector-kafka v1.15.0)  

-> [KafkaSink|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaSink.java]

-> [KafkaWriter|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java]

     creates a delivery callback (private final Callback deliveryCallback) and passes it to the [doSend|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L966] method in the [KafkaProducer|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java] below 

-> [KafkaProducer|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java]
     the [doSend|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L966] method calls [KafkaWriter::deliveryCallback::onCompletion(null, exception)|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1056]  if the [ApiException|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/errors/ApiException.java] occurs 

 
{code:java}
private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
// .......

} catch (ApiException e) {
    log.debug(""Exception occurred during message send:"", e);
    if (callback != null)
        callback.onCompletion(null, e);
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    return new FutureFailure(e);
} catch (InterruptedException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw new InterruptException(e);
} catch (KafkaException e) {
    this.errors.record();
    this.interceptors.onSendError(record, tp, e);
    throw e;
} catch (Exception e) {
    // we notify interceptor about all exceptions, since onSend is called before anything else in this method
    this.interceptors.onSendError(record, tp, e);
    throw e;
}{code}
-> [KafkaWriter::onCompletion|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L413] throws an exception if the exception input parameter is not null
{code:java}
@Override
public void onCompletion(RecordMetadata metadata, Exception exception) {
    if (exception != null) {
        FlinkKafkaInternalProducer<byte[], byte[]> producer =
                KafkaWriter.this.currentProducer;
        mailboxExecutor.execute(
                () -> {
                    numRecordsOutErrorsCounter.inc();
                    numRecordsSendErrorsCounter.inc();
                    throwException(metadata, exception, producer);
                },
                ""Failed to send data to Kafka"");
    }

    if (metadataConsumer != null) {
        metadataConsumer.accept(metadata);
    }
} {code}
 

 ;;;","10/Jun/22 07:12;martijnvisser;[~igaevd] With regards to ""The problem is that the Flink job stops processing all other messages/data and gets stuck in a Restarting-Failed-Restarting loop after the only one message is failed to be produced due to any runtime exception"", I would argue that this is by design since Flink has certain guarantees that it can not fulfil if there's a problem on one of the sinks. 

Kafka doesn't stop processing, but it does actively reject the message. It's not Flink who's complaining, it's Kafka. 

With regards to your proposal, I can understand that there are use cases where it's interesting to to keep Flink running and not fail on certain conditions. However, I would expect those use cases to be earlier in the processing chain, like when executing the business logic. I don't see much value solving that in the connector (and especially not on a per connector basis). When something is being sent to a sink, my expectation is that Flink should produce it without exceptions. Else, it shouldn't have been sent to the sink.

Also looping in [~renqs] for his opinion on this topic :);;;","10/Jun/22 20:55;igaevd;Hi [~martijnvisser], thank you for being involved! 

I absolutely agree that a Flink message should be produced without exceptions otherwise it shouldn't be produced at all. 

My concern is still the same, it must be a robust fault-tolerant system. Not all potential issues could be checked/covered before a message is going to be produced. Exceptions may (even should) happen. Now they are re-processed (almost) indefinitely stopping the whole message processing. It should be delegated to system design architectures, at higher lever than Kafka connectors.

From my prospective of view, I would like to analyze failed messages, skip them if they cannot be re-processed, save non producible messages somewhere for the future analysis,  log errors, post alerts if it is required. I don't see how and where runtime exceptions can be suppressed and managed properly when Kafka connectors throw them categorically. It is why I still think that the best place to manage exceptions is the place where they are coming from. I believe that a possibility to overwrite the call back will be really useful for developers who use Kafka connectors.

Could you please advise how this  Restarting-Failed-Restarting loop could be avoided if an exception is happens in the Kafka connector?  

Thank you!

 ;;;","13/Jun/22 07:30;renqs;Thanks for raising this up [~igaevd] [~martijnvisser] ! It looks like a reasonable feature request to me, not just for Kafka but also for all connector sinks. I think we can introduce a pluggable handler in the sink API that catches exception when writing data to the external system and let user to decide how to deal with the error (side output / ignore / fail the job). WDYT? ;;;","13/Jun/22 21:28;igaevd;Hi [~renqs], thank you for reaching out!

I believe this exception catcher in the sink API would be a very useful and necessary feature that would help to handle exceptions more flexibly without risky to get in the ""fail-restart-fail"" loop. For example, we still suffer from periodic stops of message processing as we cannot suppress +un-recoverable+ exceptions properly, though we could analyze them  and process correctly.;;;","22/Jun/22 23:06;igaevd;[~renqs], do know by any change when you will be able to implement this feature/fix? The exception handling in Kafka connectors is still a problem on our side. Thank you!;;;","23/Jun/22 04:08;renqs;It will take some time to get this feature released. As this touches the public API it requires discussion in the mailling list, and FLIP if we need to implement this as a common feature in Sink API. [~igaevd] Do you have any interest to work on this? ;;;","22/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","30/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
KafkaSourceReader fails to commit consumer offsets for checkpoints,FLINK-27962,13449081,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,igaevd,igaevd,08/Jun/22 22:08,14/Nov/22 14:49,04/Jun/24 20:51,11/Oct/22 09:28,1.14.4,1.15.0,,,,,,,,,,,,,1.16.0,,,,Connectors / Kafka,,,,,3,,,,,"The KafkaSourceReader works well for many hours, then fails and re-connects successfully, then continues to work some time. After the first three failures it hangs on ""Offset commit failed"" and never connected again. Restarting the Flink job does help and it works until the next ""3 times fail"".

I am aware about [the note|https://nightlies.apache.org/flink/flink-docs-stable/docs/connectors/datastream/kafka/#consumer-offset-committing] that Kafka source does NOT rely on committed offsets for fault tolerance. Committing offset is only for exposing the progress of consumer and consuming group for monitoring.

I agree if the failures are only periodic, but I would argue complete failures are unacceptable



*Failed to commit consumer offsets for checkpoint:*
{code:java}
Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
2022-06-06 14:19:52,297 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 464521
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.

Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
2022-06-06 14:20:02,297 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 464522
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.

Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
2022-06-06 14:20:02,297 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 464523
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets

..... fails permanently until the job restart
 {code}
*Consumer Config:*
{code:java}
allow.auto.create.topics = true
auto.commit.interval.ms = 5000
auto.offset.reset = none
bootstrap.servers = [test.host.net:9093]
check.crcs = true
client.dns.lookup = use_all_dns_ips
client.id = test-client-id
client.rack =
connections.max.idle.ms = 180000
default.api.timeout.ms = 60000
enable.auto.commit = false
exclude.internal.topics = true
fetch.max.bytes = 52428800
fetch.max.wait.ms = 500
fetch.min.bytes = 1
group.id = test-group-id
group.instance.id = null
heartbeat.interval.ms = 3000
interceptor.classes = []
internal.leave.group.on.close = true
internal.throw.on.fetch.stable.offset.unsupported = false
isolation.level = read_uncommitted
key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
max.partition.fetch.bytes = 1048576
max.poll.interval.ms = 300000
max.poll.records = 500
metadata.max.age.ms = 180000
metric.reporters = []
metrics.num.samples = 2
metrics.recording.level = INFO
metrics.sample.window.ms = 30000
partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
receive.buffer.bytes = 65536
reconnect.backoff.max.ms = 1000
reconnect.backoff.ms = 50
request.timeout.ms = 60000
retry.backoff.ms = 100
sasl.client.callback.handler.class = null
sasl.jaas.config = [hidden]
sasl.kerberos.kinit.cmd = /usr/bin/kinit
sasl.kerberos.min.time.before.relogin = 60000
sasl.kerberos.service.name = null
sasl.kerberos.ticket.renew.jitter = 0.05
sasl.kerberos.ticket.renew.window.factor = 0.8
sasl.login.callback.handler.class = class com.test.kafka.security.AzureAuthenticateCallbackHandler
sasl.login.class = null
sasl.login.refresh.buffer.seconds = 300
sasl.login.refresh.min.period.seconds = 60
sasl.login.refresh.window.factor = 0.8
sasl.login.refresh.window.jitter = 0.05
sasl.mechanism = OAUTHBEARER
security.protocol = SASL_SSL
security.providers = null
send.buffer.bytes = 131072
session.timeout.ms = 30000
socket.connection.setup.timeout.max.ms = 30000
socket.connection.setup.timeout.ms = 10000
ssl.cipher.suites = null
ssl.enabled.protocols = [TLSv1.2]
ssl.endpoint.identification.algorithm = https
ssl.engine.factory.class = null
ssl.key.password = null
ssl.keymanager.algorithm = SunX509
ssl.keystore.certificate.chain = null
ssl.keystore.key = null
ssl.keystore.location = null
ssl.keystore.password = null
ssl.keystore.type = JKS
ssl.protocol = TLSv1.2
ssl.provider = null
ssl.secure.random.implementation = null
ssl.trustmanager.algorithm = PKIX
ssl.truststore.certificates = null
ssl.truststore.location = null
ssl.truststore.password = null
ssl.truststore.type = JKS
value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28060,,,,,,,FLINK-25293,,,FLINK-28060,,,,,,,,,,,,"20/Sep/22 21:21;tommyschnabel;Screen Shot 2022-09-20 at 5.18.04 PM.png;https://issues.apache.org/jira/secure/attachment/13049533/Screen+Shot+2022-09-20+at+5.18.04+PM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Mon Nov 14 14:49:04 UTC 2022,,,,,,,,,,"0|z132fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 07:08;martijnvisser;Flink is trying to commit the offset, but Kafka is reporting that it's currently not possible. From my experience, this is because the Kafka broker is currently not available (either it has crashed or something like an upgrade is happening). This is not a Flink problem, the problem is that there's currently no Kafka broker available where Flink can commit its offset too. 

[~renqs] Please correct me if I'm wrong

[~igaevd] I've downgraded this from a Blocker to a Major since this is not a blocker according to Flink's Jira Process https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process;;;","09/Jun/22 17:04;igaevd;[~martijnvisser], thank you for answering again!

The thing is the broker is available, {+}otherwise{+}:

Why does a Flink job restart *always* fixes the committing issue {*}immediately{*}?

Why are all other Flink jobs continuing to commit offsets successfully (using the same brokers) at the same time?

Why Storm Spouts can consume messages (using the same logic, the same brokers, and the same topics) without any broker problems for months?  

The issue appeared after we migrated from Strom to Flink. If we stop a Flink job and roll to the corresponding Strom topology back then the issue disappears. Meanwhile nothing is changed on the Kafka side. It is weird and doesn't look like general issue with brokers on the Kafka side.;;;","10/Jun/22 07:43;martijnvisser;[~igaevd] More than happy to (try to) help :)

In most situations that I've seen, these issues are related to setup. It could be Kafka issues (given FLINK-27963, perhaps Kafka is actively blocking the Flink job at that moment), the network issues, JVM issues, pod issues, there are so many things that can cause these type of errors. Before making the conclusion that there's a bug on a Flink side, it requires more investigation. It's understandable that you're pointing to Flink, my counter argument would be that if this was a real bug in Flink, we would have had a lot more bug reports from users who are using this. 

Do you have the Jobmanager and Taskmanager logs? Perhaps we can see something else that helps us understand what's going on and pinpoint the issue (it could still be a bug of course);;;","13/Jun/22 07:22;renqs;Thanks for the discussion [~igaevd] [~martijnvisser] ! At first glance it looks like a problem on broker side because {{CoordinatorNotAvailableException}} should be an error responded by broker, but it's weird that the consumer hangs after this error. It'll be great to have your JM and TM logs as suggested by [~martijnvisser]. Also you can try to enable the trace level logging of Kafka consumer to reveal more details about the root cause.

The reason other applications like Storm doesn't fail at the same time might be that they are using different group IDs so their group coordinator could be on different brokers. Just an assumption in my mind and we still need some logs to investigate the issue. ;;;","13/Jun/22 22:19;igaevd;Hi [~renqs] , [~martijnvisser] ! 

I got these logs and still cleaning them to remove extra records (~1 million of records in total) and hide sensitive information, will upload as soon as I prepare them, thank you!

A few more comments on the issue:

- the offset commit fails once the load on the job increases at any time. For example,  once it got a batch of 500 requests
- the pods have never crashed or restarted for any of the task manager or job manager pods in the cluster
- here are links to others commenting on the issue: [Flink 1.14.4 -> 1.15.0 Upgrade Problem-Apache Mail Archives|https://lists.apache.org/thread/hojo9xr9bg0chk7xz2rg3jqj8obhrv2p], and [java - Flink kafka source stops committing offset after transient failure - Stack Overflow|https://stackoverflow.com/questions/57054929/flink-kafka-source-stops-committing-offset-after-transient-failure]

 

 ;;;","14/Jun/22 07:32;martijnvisser;[~igaevd] Thanks, looking forward to the logs. The more info there is, the better. For context, the 2nd link you've included is talking about a connector that was removed a long time ago in Flink. The 1st one is definitely interesting but without more info it's hard to figure out where the issue might be. ;;;","14/Jun/22 18:27;igaevd;Hi [~martijnvisser] , [~renqs]! Just FYI, the logs have been attached.;;;","20/Sep/22 21:14;tommyschnabel;
Hi there, we're also seeing this at Twilio Segment on versions 1.15.1 and 1.15.2, but _not_ on 1.14.4. I didn't test 1.15.0 but I imagine it's present there. Here's what we're seeing in one of our task manager's logs:
{code}
2022-09-20 15:58:07,978 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 363507
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
2022-09-20 15:58:07,981 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 363507
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
2022-09-20 15:58:08,029 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 363507
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
2022-09-20 15:58:08,055 WARN  org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 363507
org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
{code}

Is there any ETA on getting this fixed?

Updating to attaching one of our graphs to show how we're seeing 2/3rds of our partitions not committing offsets while 1/3rd does go down. Inspecting further I've discovered that we _are_ processing all those supposedly lagged messages, the offsets are just not being committed back to kafka.;;;","20/Sep/22 21:21;tommyschnabel; !Screen Shot 2022-09-20 at 5.18.04 PM.png! ;;;","21/Sep/22 07:46;martijnvisser;It will be interesting to see if this will be resolved with Flink 1.16, since that version will be shipped with a newer version of the Kafka Clients (Going from 2.8.1 in 1.15 to 3.2.1 in 1.16). Flink 1.14 is using 2.4.1. Especially since the stacktrace points to the Kafka Client not being able to commit, my hypothesis would be that this is where the problem resides. ;;;","21/Sep/22 07:46;martijnvisser;[~renqs] What do you think? ;;;","21/Sep/22 09:26;renqs;From the log provided I think this is the same as FLINK-28060. [~tommyschnabel] Does this issue happen after a broker failure? ;;;","21/Sep/22 13:19;tommyschnabel;Hi [~renqs], no we didn't have any broker failures around this time. We actually had another cluster running 1.14.4 reading from the same topics which saw no issues during this time;;;","22/Sep/22 07:58;martijnvisser;[~tommyschnabel] Would it be possible to patch your 1.14.4 Flink with the different version from the Kafka Client? ;;;","22/Sep/22 13:23;tommyschnabel;Hi [~martijnvisser], yes I tested that successfully yesterday. We're upgrading to 1.15.2 to be able to use NO_CLAIM restore mode when we found this issue. I successfully cherry-picked bc9b401ed1f2e7257c7b44c9838e34ede9c52ed5 onto fork's release branch and confirmed this fixes our issue. Thanks again for all your help everyone!;;;","11/Nov/22 11:53;elanv;Hi, [~renqs]. 

I've had this problem for a long time with 1.14.4 and updated my Flink version to 1.16.0 three days ago to fix the problem. However, since yesterday, two days after starting the job, this problem is occurring again.

The Kafka cluster is AWS MSK and there were no brokers restarted.

The job with the problem is using 3 kafka sources, but only one source has a problem. Also, several Flink jobs are using the same Kafka cluster, and the other jobs are currently fine.

 

Share logs related to the problem.

---------
{code:java}
    
2022-11-10 05:13:21,789 INFO org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.internals.AbstractCoordinator [] - [Consumer clientId=xxxxxxxxxx, groupId=xxxxxxxxxx] Group coordinator b-2.msk-cluster-xxx.xxxx.xx.kafka.ap-northeast-2.amazonaws.com:9098 (id: 2147483645 rack: null) is unavailable or invalid due to cause: null.isDisconnected: true. Rediscovery will be attempted.
2022-11-10 05:13:21,790 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 157075
org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.DisconnectException
2022-11-10 05:13:21,790 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 157076
org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.DisconnectException
2022-11-10 05:13:21,790 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 157077
org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets

....


Caused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.DisconnectException
2022-11-10 05:13:21,911 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 157105
org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available

....

2022-11-10 05:13:23,447 WARN org.apache.flink.connector.kafka.source.reader.KafkaSourceReader [] - Failed to commit consumer offsets for checkpoint 157106
org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.RetriableCommitFailedException: Offset commit failed with a retriable exception. You should retry committing the latest consumed offsets.
Caused by: org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
 {code}
 

 ;;;","14/Nov/22 14:49;tommyschnabel;[~elanv], I've had luck setting metadata.max.age.ms (kafka producer config) to something less than whatever your kafka timeouts are set to. We've been able to get through momentary kafka outages seamlessly that way;;;",,,,,,,,,,,,,,,
The EventUtils generate event name should take the resource's uid into account,FLINK-27961,13449032,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aitozi,aitozi,08/Jun/22 14:49,09/Jun/22 06:08,04/Jun/24 20:51,09/Jun/22 06:08,,,,,,,,,,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Currently the event name do not include the uid of the target resource. If a resource is recreated, it will be associated with the former object's events. It's not expected and will be confusing with the empty events when describe the resource.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 09 06:08:12 UTC 2022,,,,,,,,,,"0|z13254:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 06:08;gyfora;main: 9f46163e7c4490b006a2f25824158eb45eeee7ee
release-1.0: d8e0928a5a272604e9135015455910b6b2fcb66c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the apt-get updating optional ,FLINK-27960,13449024,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,aitozi,aitozi,08/Jun/22 14:06,14/Jun/22 12:38,04/Jun/24 20:51,14/Jun/22 12:31,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"I notice that it cost much time to do the apt-get updating, it's not necessary during development, So I think it will be convenient to add an option to control the image building to skip some unnecessary stage 

  !screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 14:08;aitozi;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13044787/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 12:31:22 UTC 2022,,,,,,,,,,"0|z1323c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 08:06;gyfora;For me apt-get update generally runs in less than 5 seconds while the maven build itself takes more than 40 seconds. Based on this I would not remove it. 

How long does it take for you?;;;","09/Jun/22 13:42;aitozi;From my log it don't finish in 8min, I do not know the root cause. If I skip that, it will be fast.;;;","09/Jun/22 13:42;aitozi;Maybe it's due to the network limit;;;","09/Jun/22 14:07;mbalassi;We have also made the equivalent optional in our internal build at Apple. :-) ;;;","09/Jun/22 14:34;aitozi;For convenient, I think we could include it in the main branch like the {{-Dfast}} in flink;;;","14/Jun/22 12:31;gyfora;merged to main 42f289a4e56f2acf569cc8da595fc673bb5dcaa0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Use ResolvedSchema in flink-avro instead of TableSchema,FLINK-27959,13449008,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Jun/22 12:55,09/Jun/22 09:47,04/Jun/24 20:51,09/Jun/22 09:47,,,,,,,,,,,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / API,,,,0,pull-request-available,,,,"{{TableSchema}} is deprecated 
It is recommended to use {{ResolvedSchema}} and {{Schema}} in {{TableSchema}} javadoc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 09 09:47:23 UTC 2022,,,,,,,,,,"0|z131zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 13:06;martijnvisser;[~Sergey Nuyanzin] Do you want to pick this one up?;;;","08/Jun/22 13:16;Sergey Nuyanzin;yes, however I do not have enough grants to assign it to me;;;","09/Jun/22 09:47;danderson;Fixed in master with 6017f996a71d68707643917572f91f6b0523a2a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compare batch maxKey to reduce comparisons in SortMergeReader,FLINK-27958,13449007,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,lzljs3620320,lzljs3620320,08/Jun/22 12:49,29/Mar/23 03:10,04/Jun/24 20:51,29/Mar/23 03:10,,,,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,,0,pull-request-available,,,,"In SortMergeReader, each sub reader is batched reader.

When adding a new batch to the priority queue, we can look at the maximum key of the batch, and if its maximum key is smaller than the minimum key of other batches, then we can just output the whole batch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-08 12:49:18.0,,,,,,,,,,"0|z131zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extract AppendOnlyFileStore out of KeyValueFileStore,FLINK-27957,13448992,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,08/Jun/22 11:42,15/Jun/22 10:44,04/Jun/24 20:51,15/Jun/22 10:44,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Currently {{FileStore}} for append only records and key-values are mixed in one {{FileStoreImpl}} class. This makes the code base messy and also introduce bugs (for example, {{AppendOnlyFileStore}} should rely on a special reader implementation but it is not, causing failures when using avro format).

We need to extract {{AppendOnlyFileStore}} out of {{KeyValueFileStore}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 10:44:47 UTC 2022,,,,,,,,,,"0|z131w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 10:44;lzljs3620320;master: a3e52f85900758f6c6f4a751c1ac14d012e833a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes Operator Deployment strategy type should be Recreate,FLINK-27956,13448991,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mbalassi,gyfora,gyfora,08/Jun/22 11:38,09/Jun/22 06:07,04/Jun/24 20:51,09/Jun/22 06:07,kubernetes-operator-1.0.0,,,,,,,,,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We should change the Deployment strategy.type from the default (RollingUpdate) to Recreate to avoid potential problems when a new operator pod is deployed during upgrade.

[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy]

 

Only one operator pod is supposed to run at any given time to avoid any errors/inconsistencies, and without HA/leader election, this setting is necessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 09 06:07:18 UTC 2022,,,,,,,,,,"0|z131w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 06:07;gyfora;main: 1b470f77bde76861e80b57f0b261b5131d0e723d

release-1.0: cc8207cf8d283b112ef0bd1a82415addab32036a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink installation failure on Windows OS,FLINK-27955,13448986,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,08/Jun/22 11:22,17/Jun/22 13:10,04/Jun/24 20:51,08/Jun/22 12:26,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,API / Python,,,,,0,pull-request-available,,,,"Because pemja doesn't support windows os, it makes installation failed in windows os in release-1.15. We need to fix it asap.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 12:26:54 UTC 2022,,,,,,,,,,"0|z131uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 11:27;chesnay;Why should we fix it? We don't support Windows.;;;","08/Jun/22 11:32;hxbks2ks;Many pyflink users are used to developing pyflink jobs locally in windows os
;;;","08/Jun/22 12:26;hxbks2ks;Merged into master via 9184bdf7ae8cc269a0104b2216b8e1c24bd5ef14
Merged into release-1.15 via 499ca049840141c12a929a3d29da31e1a164292d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobVertexFlameGraphHandler does not work on standby Dispatcher,FLINK-27954,13448982,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Weijie Guo,chesnay,chesnay,08/Jun/22 10:55,11/Mar/24 12:44,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,1.20.0,,,,Runtime / Coordination,Runtime / REST,,,,0,pull-request-available,stale-assigned,,,"The {{JobVertexFlameGraphHandler}} relies internally on the {{JobVertexThreadInfoTracker}} which calls 
{{ResourceManagerGateway#requestTaskExecutorThreadInfoGateway}} to get a gateway for requesting the thread info from the task executors. Since this gateway is not serializable it would categorically fail if called from a standby dispatcher.
Instead this should follow the logic of the {{MetricFetcherImpl}}, which requests addresses instead and manually connects to the task executors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27933,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 10:35:14 UTC 2023,,,,,,,,,,"0|z131u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 17:20;Weijie Guo;Thanks [~chesnay] for reporting this and giving fix suggestion, I'd like to do this.;;;","15/Mar/23 11:58;Wencong Liu;cc [~Weijie Guo] ;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
using the original order to add the primary key in PushProjectIntoTableSourceScanRule,FLINK-27953,13448955,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zoucao,zoucao,08/Jun/22 08:44,21/Jun/22 05:09,04/Jun/24 20:51,,1.14.4,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"In PushProjectIntoTableSourceScanRule, if the source produces a changelog stream, the primary key will be added to the end of projected fields, see the following SQL:
{code:java}
StreamTableTestUtil util = streamTestUtil(TableConfig.getDefault());
        TableEnvironment tEnv = util.getTableEnv();
        String srcTableDdl =
                ""CREATE TABLE fs (\n""
                        + ""  a bigint,\n""
                        + ""  b int,\n""
                        + ""  c varchar,\n""
                        + ""  d int,\n""
                        + ""  e int,\n ""
                        + ""  primary key (a,b) not enforced \n""
                        + "") with (\n""
                        + "" 'connector' = 'values',\n""
                        + "" 'disable-lookup'='true',\n""
                        + "" 'changelog-mode' = 'I,UB,UA,D')"";
        tEnv.executeSql(srcTableDdl);
        tEnv.getConfig().set(""table.exec.source.cdc-events-duplicate"", ""true"");
{code}
{code:java}
 System.out.println(tEnv.explainSql(""select a, c from fs where c > 0 and b = 0""));

projected list:
[[0],[1],[2]]

== Optimized Execution Plan ==
Calc(select=[a, c], where=[(CAST(c AS BIGINT) > 0)])
+- ChangelogNormalize(key=[a, b])
   +- Exchange(distribution=[hash[a, b]])
      +- Calc(select=[a, b, c], where=[(b = 0)])
         +- DropUpdateBefore
            +- TableSourceScan(table=[[default_catalog, default_database, fs, filter=[], project=[a, b, c], metadata=[]]], fields=[a, b, c])
{code}
{code:java}
 System.out.println(tEnv.explainSql(""select a, c from fs where c > 0"")); 

projected list:
[[0],[2],[1]]

 == Optimized Execution Plan ==
Calc(select=[a, c], where=[(CAST(c AS BIGINT) > 0)])
+- ChangelogNormalize(key=[a, b])
   +- Exchange(distribution=[hash[a, b]])
      +- DropUpdateBefore
         +- TableSourceScan(table=[[default_catalog, default_database, fs, filter=[], project=[a, c, b], metadata=[]]], fields=[a, c, b])
{code}
Field b is not involved in
{code:sql}
select a, c from fs where c > 0{code}
, but it is a primary key, so we add it to the end of projected list, If 'table.exec.source.cdc-events-duplicate' is enabled. The condition about field b will change output type, that says the duplicate node will get the different input type, and the state serializer will also be changed, leading to state incompatibility.

I think we can use the original order from the source table to add the primary key to projected list.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 21 05:09:12 UTC 2022,,,,,,,,,,"0|z131o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 03:55;zoucao;Hi [~godfreyhe], could you have time to take a look?;;;","20/Jun/22 16:04;xuyangzhong;It seems it's a good way to keep the origin order about projections to archive state compatibility in Flink SQL. I agree with you and I'm glad to take this issue if need.;;;","21/Jun/22 05:09;zoucao;Hi [~xuyangzhong], thanks for your feedback, I have just started to work on it and will open a PR recently.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table UDF fails when using Double.POSITIVE_INFINITY as parameters,FLINK-27952,13448932,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhangzp,zhangzp,08/Jun/22 07:40,21/Aug/23 22:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,pull-request-available,,," 

The following code fails and throws NumberFormatException when casting Double.POSITIVE_INFINITY to BigDecimal.
{code:java}
@Test
public void testTableUdf() {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    DataStream <Row> data = env.fromElements(Row.of(1.), Row.of(2.));
    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
    Table table = tEnv.fromDataStream(data).as(""f0"");
    Double[][] d = new Double[][]{new Double[]{1.0, Double.POSITIVE_INFINITY}};
    Expression[] expressions = new Expression[2];
    expressions[0] = org.apache.flink.table.api.Expressions.call(MyUDF.class, $(""f0""), d);
    expressions[1] = org.apache.flink.table.api.Expressions.call(MyUDF.class, $(""f0""), d);
    table.addColumns(expressions)
        .as(""f0"", ""output"", ""output2"")
        .execute().print();
}
public static class MyUDF extends ScalarFunction {
    public Integer eval(Integer num, Double[][] add) {
        return (int)(num + add[0][0]);
    }
} {code}
 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/22 16:19;xuyangzhong;image-2022-06-21-00-19-12-151.png;https://issues.apache.org/jira/secure/attachment/13045322/image-2022-06-21-00-19-12-151.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:22 UTC 2023,,,,,,,,,,"0|z131iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate the ""Debugging Classloading"" page into Chinese",FLINK-27951,13448924,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,murkynight,murkynight,murkynight,08/Jun/22 06:35,03/Aug/22 11:58,04/Jun/24 20:51,03/Aug/22 11:58,,,,,,,,,,,,,,,1.16.0,,,,chinese-translation,,,,,0,pull-request-available,,,,"The page ""Debugging Classloading"" needs to be translated into Chinese.

I'm willing to work on this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 11:58:44 UTC 2022,,,,,,,,,,"0|z131h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 07:21;martijnvisser;[~murkynight] I've assigned it to you;;;","14/Jun/22 02:46;murkynight;[~martijnvisser] I have done the work and have created a pr, can you help to review plz, thanks!;;;","14/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","29/Jul/22 01:39;murkynight;[~martijnvisser] I have updated pr, can you help to review it, thanks!;;;","03/Aug/22 10:49;murkynight;[~martijnvisser] I have recreated pr, can you help to review it, thanks a lot;;;","03/Aug/22 11:58;martijnvisser;Fixed in master: e2a5c86e38a6a65cbf7b1a26f6ab056ef346c4a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"flink streaming writes to the hive table, the set file suffix parameter is invalid",FLINK-27950,13448912,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,SOD_DOB,SOD_DOB,08/Jun/22 04:05,09/Jun/22 03:51,04/Jun/24 20:51,09/Jun/22 03:51,1.13.2,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,flink-connector-hive,,,,"when I use flink streaming and write into hive，I set the following parameters：
{code:java}
TBLPROPERTIES ('hive.output.file.extension'='.parquet') {code}
I can't find the suffix "".parquet"" file when I check in hdfs, but it fires normally when I use Hive SQL.

Am I using it correctly? or other reasons?

this is my demo：

 
{code:java}
val tableEnvSettings: EnvironmentSettings = EnvironmentSettings.newInstance()
  .useBlinkPlanner()
  .inStreamingMode()
  .build()

val tEnv: StreamTableEnvironment = StreamTableEnvironment.create(env, tableEnvSettings)

val catalog = new HiveCatalog(""myHive"", ""xx"", ""/usr/local/xx/conf"")

tEnv.registerCatalog(""myHive"", catalog)
tEnv.useCatalog(""myHive"")
tEnv.useDatabase(""xx"")

tEnv.getConfig.setSqlDialect(SqlDialect.HIVE)
val createTSql: String =
  s""""""
    |create table if not exists $hiveTable (
    | ...
    |)
    |...
    |TBLPROPERTIES (
    |  'sink.parallelism'='1',
    |  'partition.time-extractor.timestamp-pattern'='$$dt',
    |  'sink.shuffle-by-partition.enable'='true',
    |  'sink.partition-commit.policy.kind'='metastore,success-file',
    |  'hive.output.file.extension'='.parquet'
    |)
    |"""""".stripMargin
tEnv.executeSql(createTSql)
tEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)

tEnv.createTemporaryView(""t_xx"", DataStream)

val insertSql: String =
  s""""""
     |  insert into $hiveTable
     |  select *
     |  from t_xx
    |"""""".stripMargin

tEnv.executeSql(insertSql)
tEnv.dropTemporaryView(""t_userAction"") {code}
 

 

 ","flink：flink-1.13.2

hive： 1.1.0-cdh5.14.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,scala,Thu Jun 09 03:43:45 UTC 2022,,,,,,,,,,"0|z131eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 07:23;martijnvisser;[~SOD_DOB] I've closed the ticket since it's not in English. If you translate this ticket to English, I can re-open it;;;","08/Jun/22 08:01;martijnvisser;[~luoyuxia] Any thoughts on this?;;;","08/Jun/22 13:09;luoyuxia;[~SOD_DOB] Thanks for reporting it. But I think it's not a bug. You can set the following property in hive-site.xml. 
{code:java}
<property>
   <name>hive.output.file.extension</name>
   <value>.parquet</value>
 </property> {code}
And create the hive catalog using the updated file.

For Hive's behavior, I believe it's not you set the property in table's properties  that make difference, it's you set the property in Hive's hive-site.xml or using command `SET hive.output.file.extension=.parquet` in a session.;;;","09/Jun/22 02:17;SOD_DOB;[~luoyuxia] thanks! I repair it in flink streaming.

But in Hive Sql, I'm sure this parameter is set to take effect,

 
{code:java}
TBLPROPERTIES ('hive.output.file.extension'='.parquet')  {code}
 

and the dependent hive-site.xml does not have this parameter.

I can't find the relevant documentation about flink-connector-hive.

And finally thank you very much;;;","09/Jun/22 03:43;luoyuxia;[~SOD_DOB] Thanks for your information. Maybe I miss some thing when reading Hive's source code.

I think it's a hive configuration [OUTPUT_FILE_EXTENSION|[https://github.com/apache/hive/blob/master/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L789]] , so that Flink doc won't need to refer it.

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka create topic  error,FLINK-27949,13448901,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,sandyfog,sandyfog,08/Jun/22 02:55,15/Jul/22 02:46,04/Jun/24 20:51,15/Jul/22 02:46,table-store-0.1.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,,,,,"{{kafka version:2.1.1}}
 
{{org.apache.kafka.common.errors.UnsupportedVersionException: Creating topics with default partitions/replication factor are only supported in CreateTopicRequest version 4+. The following topics need values for partitions and replicas:}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 19 09:48:47 UTC 2022,,,,,,,,,,"0|z131c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 03:03;lzljs3620320;[~sandyfog] Thanks for your reporting.;;;","19/Jun/22 09:48;lzljs3620320;See the latest documentation, the usage of catalog does not automatically create a topic anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unifying how result partition should be released,FLINK-27948,13448900,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,08/Jun/22 02:49,08/Jun/22 02:49,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"For pipelined result partitions, intermediate data will be released with downstream consumption. For the blocking result partition, the intermediate data will be released by JM by sending RPC after all downstream tasks are finished. Please let me know if I got something wrong.
Can we have a unified way to decide the release logic of a partition.In the long run, I think that's something to consider.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-08 02:49:28.0,,,,,,,,,,"0|z131bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Spark Reader for table store,FLINK-27947,13448896,13449930,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,08/Jun/22 02:30,17/Jun/22 08:32,04/Jun/24 20:51,17/Jun/22 08:32,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Now that we have a more stable connector interface, we can develop a bit more ecology.
Apache Spark is a common batch computing engine, and the more common scenarios are: Flink Streaming writes storage, Spark reads storage.


So we can support Spark's reader.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 08:32:12 UTC 2022,,,,,,,,,,"0|z131aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 08:32;lzljs3620320;master: c58576eb3bd3d860c5ba5a940b4d0e0b3cb5f55a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extending SQL syntax in a way similar to 'spark.sql.extensions',FLINK-27946,13448888,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,liliwei,liliwei,08/Jun/22 01:05,08/Jun/22 07:26,04/Jun/24 20:51,08/Jun/22 07:26,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Does Flink consider extending SQL syntax in a similar way to 'spark.sql.extensions'?

This approach provides greater flexibility in adding many new SQL syntax and rules without modifying the source code, especially in the case of custom syntax scenarios.

 

!image-2022-06-08-09-06-54-935.png|width=561,height=328!

 

FYI：

[https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SparkSessionExtensions.html]

[https://hudi.apache.org/docs/procedures]

[https://iceberg.apache.org/docs/latest/spark-procedures/]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 01:06;liliwei;image-2022-06-08-09-06-54-935.png;https://issues.apache.org/jira/secure/attachment/13044738/image-2022-06-08-09-06-54-935.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 07:26:16 UTC 2022,,,,,,,,,,"0|z13194:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 07:26;martijnvisser;[~liliwei] Flink currently doesn't support this. Before we create a Jira ticket, such a feature request would definitely require a discussion on the Dev mailing list including a corresponding FLIP and a vote. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support HA for flink operator,FLINK-27945,13448876,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,jeesmon,jeesmon,07/Jun/22 20:57,08/Jun/22 11:40,04/Jun/24 20:51,08/Jun/22 11:40,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,Support running multiple replicas of operator pod with leader election for high availability.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 11:40:29 UTC 2022,,,,,,,,,,"0|z1316g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 20:58;jeesmon;Reference: https://docs.openshift.com/container-platform/4.7/operators/operator_sdk/osdk-leader-election.html

Leader Election Example: https://github.com/fabric8io/kubernetes-client/blob/master/kubernetes-examples/src/main/java/io/fabric8/kubernetes/examples/LeaderElectionExamples.java

Issue in JOSDK: https://github.com/java-operator-sdk/java-operator-sdk/issues/411;;;","08/Jun/22 05:55;gyfora;cc [~matyas] ;;;","08/Jun/22 11:40;gyfora;Based on discussions with the JOSDK developers this feature is not on their short term roadmap as almost no production user requests it.

The simple deployment based recovery (combined with https://issues.apache.org/jira/browse/FLINK-27956) should be enough for us until this feature becomes available, and even then it would only provide limited real value.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IO metrics collision happens if a task has union inputs,FLINK-27944,13448847,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,zhuzh,zhuzh,07/Jun/22 17:03,01/Dec/22 16:01,04/Jun/24 20:51,01/Dec/22 16:01,1.15.0,,,,,,,,,,,,,,1.15.4,1.16.1,1.17.0,,Runtime / Metrics,,,,,0,pull-request-available,stale-assigned,,,"When a task has union inputs, some IO metrics(numBytesIn* and numBuffersIn*) of the different inputs may collide and failed to be registered.

 

The problem can be reproduced with a simple job like:
{code:java}
DataStream<String> source1 = env.fromElements(""abc"");
DataStream<String> source2 = env.fromElements(""123"");
source1.union(source2).print();{code}
 

Logs of collisions:
{code:java}
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,629 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBytesInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocal'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInLocalPerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0, Shuffle, Netty, Input]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemote'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
2022-06-08 00:59:01,630 WARN  org.apache.flink.metrics.MetricGroup                         [] - Name collision: Group already contains a Metric with the name 'numBuffersInRemotePerSecond'. Metric will not be reported.[, taskmanager, fa9f270e-e904-4f69-8227-8d6e26e1be62, WordCount, Sink: Print to Std. Out, 0]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 01 16:01:44 UTC 2022,,,,,,,,,,"0|z13100:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 06:33;zhoujira86;this issue is introduced by register the metrics twice, I have created a PR to avoid the warning. ;;;","21/Jun/22 07:15;zhoujira86;[~chesnay] hi Chesnay, would you please help review the PR;;;","04/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","30/Nov/22 12:26;Weijie Guo;[~zhoujira86] Is there any new progress on this PR? We have encountered the same problem, if you don't have time, I would like to add some tests and continue to complete the work.;;;","30/Nov/22 12:58;pnowojski;I think this PR has been abandoned (I asked to rebase it over a month ago without any response), so feel free to take the code over and create a new one! ;;;","30/Nov/22 13:42;Weijie Guo;[~pnowojski] Thanks for re-assigning this to me,  I will take over and go head~;;;","01/Dec/22 13:20;pnowojski;merged to master as 20808fd^ and 20808fd 
merged to release 1.16 as c589cd5f7c8^ and c589cd5f7c8

[~Weijie Guo], could you prepare a backport to release-1.15 branch? I've backported your changes to release-1.16 already, but while doing that for release-1.15 I stumbled across some conflicts.;;;","01/Dec/22 14:06;Weijie Guo;[~pnowojski] Thanks for the merge and backport, I have created the pull request to backport this to release-1.15(https://github.com/apache/flink/pull/21439), PTAL~;;;","01/Dec/22 16:01;pnowojski;Thanks! 

Merged to release 1.15 as dd3c3473a85^ and dd3c3473a85;;;",,,,,,,,,,,,,,,,,,,,,,,
Link Kubernetes Operator RoadMap Page To Flink RoadMap Page Web Site,FLINK-27943,13448822,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ConradJam,ConradJam,ConradJam,07/Jun/22 14:14,23/Jun/22 10:56,04/Jun/24 20:51,23/Jun/22 09:14,kubernetes-operator-1.0.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,Project Website,,,,0,pull-request-available,,,,"Link [K8S Operator RoadMap|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/development/roadmap/] To [Flink RoadMap|https://flink.apache.org/roadmap.html] WebSite",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 23 10:56:11 UTC 2022,,,,,,,,,,"0|z130ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 14:17;ConradJam;Hi [~gyfora] , I want to try this ticket , can you assign to me this issuse and build k8s operator roadmap page?;;;","23/Jun/22 09:14;martijnvisser;Fixed in asf-site: 1ea65037b09ebc653c212921b45b9c8a5c3cf66c;;;","23/Jun/22 10:56;martijnvisser;Thnx for correcting the fixversion [~gyfora];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-rabbitmq,FLINK-27942,13448809,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,alexanderpreuss,alexanderpreuss,07/Jun/22 13:41,05/Oct/22 12:23,04/Jun/24 20:51,05/Oct/22 12:23,1.16.0,,,,,,,,,,,,,,1.17.0,,,,Connectors/ RabbitMQ,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 05 12:23:17 UTC 2022,,,,,,,,,,"0|z130rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 12:24;Sergey Nuyanzin;Hi [~alexanderpreuss] 
Do you mind if I pick this or are you going to implement it?;;;","08/Jun/22 12:59;alexanderpreuss;Hi [~Sergey Nuyanzin], feel free to assign yourself on any of these :);;;","09/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","05/Oct/22 12:23;mapohl;master: d191bda7e63a2c12416cba56090e5cd75426079b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-kinesis,FLINK-27941,13448808,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,chalixar,alexanderpreuss,alexanderpreuss,07/Jun/22 13:41,09/Jan/23 10:13,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Connectors / Kinesis,,,,,0,pull-request-available,stale-assigned,starter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 09 10:12:47 UTC 2023,,,,,,,,,,"0|z130rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","07/Oct/22 07:48;mapohl;The corresponding PR seems to be stalled for some time with open review comments. I'm going to leave this issue assigned for a bit to wait for some response from [~Jun He].;;;","09/Jan/23 09:14;chalixar;[~mapohl] 

Given the connectors being moved now to [https://github.com/apache/flink-connector-aws] and that the PR seems to have been stalled for long time, would it make sense to reassign the ticket.

Please let me know if we can move this forward, happy to take it then.;;;","09/Jan/23 10:12;mapohl;Thanks for offering your help. I can assign the issue to you since there was no response since October on that one. (y) Feel free to create a PR against [apache:flink-connector-aws|https://github.com/apache/flink-connector-aws];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-jdbc,FLINK-27940,13448807,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Done,eskabetxe,alexanderpreuss,alexanderpreuss,07/Jun/22 13:40,23/Dec/22 14:12,04/Jun/24 20:51,23/Dec/22 03:10,1.16.0,,,,,,,,,,,,,,jdbc-3.1.0,,,,Connectors / JDBC,,,,,0,pull-request-available,starter,,,,,,,,,,,,,,,FLINK-29541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 23 14:12:34 UTC 2022,,,,,,,,,,"0|z130r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jun/22 14:44;wanglijie;Hi [~alexanderpreuss], if you don't mind, I'd like to take on this.;;;","11/Jun/22 22:38;jingge;[~wanglijie95] just assigned this task to you. Thanks for your contribution!;;;","12/Jun/22 03:49;wanglijie;thanks, [~jingge] :);;;","07/Oct/22 07:44;mapohl;[~wanglijie] any update on the progress on this ticket? I couldn't find any PR being connected to this Jira issue.;;;","08/Oct/22 02:47;wanglijie;[~mapohl]  It was blocked by the migration of {{JdbcTablePlanTest}}, because it extends {{TableTestBase}} in {{flink-table-planner}} module. So the {{TableTestBase}} needs to be migrated first, then I'll continue this work.;;;","27/Oct/22 12:56;rskraba;I've run into this situation in other modules.  My recommendation is to leave those tests that depend on a large hierarchy of abstract {{XxxxTestBase}} classes for now, and migrate everything else -- a lot of these widely reused tests should probably be migrated together in a group and probably shouldn't block your work here!

;;;","28/Oct/22 06:14;wanglijie;[~rskraba] good idea, I will do so :);;;","23/Dec/22 03:09;wanglijie;Done via 5ec38735e5ac2b12620e91ce4977b805fb04c5b8 (https://github.com/apache/flink-connector-jdbc);;;","23/Dec/22 11:20;mapohl;[~wanglijie] keep in mind that we have dedicated fixVersions for the externalized connectors. I changed {{1.17.0}} to {{jdbc-3.1.0}}. Please correct me if I misinterpreted the situation.;;;","23/Dec/22 14:12;wanglijie;[~mapohl] You are right, thanks for the correction :);;;",,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-hive,FLINK-27939,13448806,13417682,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,hk__lrzy,alexanderpreuss,alexanderpreuss,07/Jun/22 13:39,17/Aug/23 10:35,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,stale-assigned,starter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 10:35:14 UTC 2023,,,,,,,,,,"0|z130qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/22 07:43;mapohl;[~catyee] thanks for working on this topic. I'm wondering what the state is. I couldn't find any PR being related to that one.;;;","16/Feb/23 11:50;hk__lrzy;[~mapohl]  I can work on this jira for migrate `hive-connector` to junit 5, plz assign it to me if Yuan Kui have no reply. thanks.;;;","16/Feb/23 12:16;mapohl;Thanks, [~hk__lrzy]. I assigned the ticket to you because there was no response from [~catyee] in the meantime.;;;","26/Feb/23 07:39;luoyuxia;[~hk__lrzy] Hi, is there any progress for this jira? Personally, I hope we can migrate it to junit5 in release 1.18;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-hbase-base,FLINK-27938,13448805,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,snuyanzin,alexanderpreuss,alexanderpreuss,07/Jun/22 13:39,05/Oct/22 12:30,04/Jun/24 20:51,05/Oct/22 12:30,1.16.0,,,,,,,,,,,,,,1.17.0,,,,Connectors / HBase,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 05 12:30:50 UTC 2022,,,,,,,,,,"0|z130qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/22 12:30;mapohl;master: adc476a0df2355226df19c37c0834ee0d9c19023;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-gcp-pubsub,FLINK-27937,13448804,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,alexanderpreuss,alexanderpreuss,07/Jun/22 13:38,12/Sep/22 12:27,04/Jun/24 20:51,12/Sep/22 12:27,1.16.0,,,,,,,,,,,,,,1.17.0,,,,Connectors / Google Cloud PubSub,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 12 12:27:40 UTC 2022,,,,,,,,,,"0|z130qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/22 12:27;mapohl;master: ea3ff8e4f518c1c8a8ad8d142eb7a04b0644db15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-cassandra,FLINK-27936,13448802,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,alexanderpreuss,alexanderpreuss,07/Jun/22 13:36,12/Sep/22 12:30,04/Jun/24 20:51,12/Sep/22 12:30,1.16.0,,,,,,,,,,,,,,1.17.0,,,,Connectors / Cassandra,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 12 12:30:39 UTC 2022,,,,,,,,,,"0|z130q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/22 12:30;mapohl;master: a4841408aa90b8530ba00cf8017f81de204dd744;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Pyflink example of create temporary view document,FLINK-27935,13448770,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pemide,pemide,pemide,07/Jun/22 10:43,10/Jun/22 11:00,04/Jun/24 20:51,10/Jun/22 11:00,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,API / Python,Documentation,,,,0,pull-request-available,,,,"Doc url: [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/common/]

Doc part: Expanding Table identifiers",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 11:00:02 UTC 2022,,,,,,,,,,"0|z130iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 10:56;pemide;[~dianfu] This Doc urls missing the part of Expanding Table identifiers in Pyflink example, Could i take this ticket and fix it?;;;","08/Jun/22 02:08;dianfu;[~pemide] Thanks. Have assigned it to you~;;;","10/Jun/22 11:00;dianfu;Merged to:
- master via e50e66fe9fd848cccada278936937614136d9911
- release-1.15 via 4b3121c81d538dcdd380183e1efe439a8e9f737c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python API- Inefficient deserialization/serialization of state variables within a batch,FLINK-27934,13448758,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,fransking,fransking,07/Jun/22 09:44,19/Aug/23 10:35,04/Jun/24 20:51,,statefun-3.2.0,,,,,,,,,,,,,,,,,,Stateful Functions,,,,,0,auto-deprioritized-minor,pull-request-available,,,"In the Python API state variables can be accessed via the UserFacingContext:

variable = context.storage.variable

This calls into the Cell instance for that state variable which has get() & set() methods.  The get() method always deserializes from the typed_value and the set() always re-serializes and marks the cell dirty.

 

This has two side effects

1:

var1 = context.storage.variable

var2 = context.storage.variable

id(var2) != id(var1) - they are different instances

 

2:

In a large batch (say 1000 calls to the same function type and id) this can result in deserializing and re-serializing the same same state variable 1000 times when really it only needs to be deserialized in the first invocation in the batch, held in memory until the last invocation and then re-serialized prior to collecting the mutations.  

 

I think this can be improved by having a lazily initialized backing field in the Cell class but I don't know if this was a conscious design decision to have the behavior described in 1. 

 

Any feedback would be welcome. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:04 UTC 2023,,,,,,,,,,"0|z130g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 16:08;fransking;Pull request here with my changes - https://github.com/apache/flink-statefun/pull/315;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint status cannot be queried from standby jobmanager,FLINK-27933,13448742,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,07/Jun/22 08:49,23/Jun/22 19:23,04/Jun/24 20:51,23/Jun/22 19:22,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"The savepoint status handler currently doesn't work on standby dispatchers because the OperationResult isn't serializable.

This wasn't caught by the recently added serialization safeguards, as those only covered the caller side (i.e., arguments passed to callee), but not the return value. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27954,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 14:16:52 UTC 2022,,,,,,,,,,"0|z130co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 14:16;chesnay;OperationResult made serializable:
master: 72cc925a88ad054449f0ade9a476ead7a287cb6f
1.15: 0cd291ddae6581784e8d57e34fe3346607cb4f26

Additional safeguards:
master: f2ef60c4fd5470995c4acd6e6f96c6cd6bde84c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align noWatermarks and withWatemarkAlignment API in PyFlink,FLINK-27932,13448735,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,07/Jun/22 08:19,17/Jun/22 01:23,04/Jun/24 20:51,17/Jun/22 01:23,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,Add no_watermarks and with_watermark_alignment to align with Java API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 17 01:23:11 UTC 2022,,,,,,,,,,"0|z130b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jun/22 01:23;dianfu;Merged to master via d0f0071b767b8dbe2e811e9b6b5e1e9027a20e2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the SqlGateway to assemble all components,FLINK-27931,13448724,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,07/Jun/22 07:25,22/Jul/22 10:01,04/Jun/24 20:51,22/Jul/22 10:01,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 10:01:32 UTC 2022,,,,,,,,,,"0|z1308o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 10:01;fsk119;Implemented in the master: 81379a56495283020a5919e8115936c163b251ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Format the timestamp in status to make it readable ,FLINK-27930,13448721,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,aitozi,aitozi,07/Jun/22 07:11,07/Jun/22 08:29,04/Jun/24 20:51,07/Jun/22 08:29,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Currently, the timestamp in status show the unix timestamp, I think we could improve it to an explicit date time 

current: 
{code:java}
startTime: ""1654584619845""
updateTime: ""1654585571858"" {code}
expect: 
{code:java}
startTime: ""2022-06-07T06:10:04Z"" 
updateTime: ""2022-06-07T06:50:04Z""  {code}
It will follow the k8s's object metadata manner

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 08:29:48 UTC 2022,,,,,,,,,,"0|z13080:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 07:16;gyfora;Changing status fields from long to String would be a breaking change, I don’t think we can do this;;;","07/Jun/22 07:24;aitozi;It's in string type now, but store the unix timestamp value. Will this break the compatibility ?;;;","07/Jun/22 08:02;gyfora;It might be in String type in the JobStatus but in other places like SavepointInfo, ReconciliationStatus the timestamp is in a long field.
I think it only makes everything confusing if we mix different formats within the status. Because of this I would prefer to keep it as is.;;;","07/Jun/22 08:29;aitozi;Get it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop support for python 3.6,FLINK-27929,13448715,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,bzhaoop,ana4,ana4,07/Jun/22 06:35,27/Feb/23 05:55,04/Jun/24 20:51,06/Feb/23 05:21,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,pull-request-available,,,,"Python 3.6 lifecycle is ended on 23 Dec 2021. We can drop this support in the 1.17 release.

[https://endoflife.date/python]
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 06 05:21:43 UTC 2023,,,,,,,,,,"0|z1306o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 08:15;martijnvisser;+1 for getting rid of tech debt;;;","05/Sep/22 12:39;bzhaoop;I think I can help to do this, but need your kind review. I will file a PR for this before the assignment. Please allow me to hold some common community works. Thank you;;;","07/Sep/22 07:14;hxb;Thanks [~bzhaoop] . I have assigned it to you?;;;","06/Feb/23 05:21;dianfu;Closing this ticket as Python 3.6 should have been dropped during supporting Python 3.10 in FLINK-29421.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
External Resource Framework: 'external-resources' name delimitation not working as expected,FLINK-27928,13448707,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,jcho7022,jcho7022,07/Jun/22 05:54,19/Aug/23 10:35,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Runtime / Configuration,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,external-resources,,"For flink version 1.15.0, the configuration `external-resources` delimiter separation("";"") functionality between external resource names is not working as expected. 

 

Reproducing the bug setting up a flink cluster running using docker with two external resources named `abc` and `efg`:

docker-compose.yml:

 
{code:java}
services:
  jobmanager:
    image: flink:1.15.0-scala_2.12
    ports:
      - ""8081:8081""
      - ""6123:6123""
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager

  taskmanager:
    image: flink:1.15.0-scala_2.12
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 8
        external-resources: abc;efg
        external-resource.abc.amount: 1
        external-resource.efg.amount: 1{code}
 

Related task manager logs:

 
{code:java}
root@2873c5ae72c2:/opt/flink# cat log/flink* | grep ExternalResourceUtils
2022-06-07 05:42:34,232 WARN  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - The amount of the abc;efg should be configured. Will ignore that resource.
2022-06-07 05:42:34,232 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
2022-06-07 05:42:34,236 WARN  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Could not find driver class name for abc;efg. Please make sure external-resource.abc;efg.driver-factory.class is configured.
2022-06-07 05:42:34,240 WARN  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - The amount of the abc;efg should be configured. Will ignore that resource.
{code}
Notice flink recognizes `abc;efg` as a single external resource rather than as `abc` and `efg`

 

 

The same issue does not exist in flink version 1.14.4

docker-compose.yml:
{code:java}
services:
  jobmanager:
    image: flink:1.14.4-scala_2.12
    ports:
      - ""8081:8081""
      - ""6123:6123""
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager

  taskmanager:
    image: flink:1.14.4-scala_2.12
    depends_on:
      - jobmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 8
        external-resources: abc;efg
        external-resource.abc.amount: 1
        external-resource.efg.amount: 1 {code}
related tm logs
{code:java}
docker logs ... | grep ExternalResourceUtils
2022-06-07 05:48:13,531 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: [abc, efg]
2022-06-07 05:48:13,534 WARN  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Could not find driver class name for abc. Please make sure external-resource.abc.driver-factory.class is configured.
2022-06-07 05:48:13,534 WARN  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Could not find driver class name for efg. Please make sure external-resource.efg.driver-factory.class is configured.
 {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:04 UTC 2023,,,,,,,,,,"0|z1304w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Aug/22 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve table store connector common interfaces,FLINK-27927,13448704,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,07/Jun/22 05:11,07/Jun/22 10:42,04/Jun/24 20:51,07/Jun/22 10:42,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"We currently have the initial FileStoreTable-related interface, but something is missing to satisfy our four approaches:
1. Type conversion
2. Data structure conversion
3. Filter conversion
4. Scan and Read

In this jira, more easy-to-use features will be added.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 10:42:13 UTC 2022,,,,,,,,,,"0|z13048:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 10:42;lzljs3620320;master:

f904b88924c8648d9becbd41d62ebf66191da60e

4471bff060620eb4c446493f79406d3d40a15923

adf9aeb5fd2ef24689754892c590bbb2f4d0f409

679d3d0a6d06e2067d4748b4f147648f3d82e200

a5579a1a981998cf4181540c36841f54942ddb81

844ee24a6cd315f923150f8e8a3c92e415b39050;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate ```upgrading.md``` into chinese,FLINK-27926,13448699,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,billrao,billrao,billrao,07/Jun/22 04:04,23/Jun/22 08:31,04/Jun/24 20:51,23/Jun/22 08:31,,,,,,,,,,,,,,,1.16.0,,,,chinese-translation,,,,,0,pull-request-available,,,,A pull request is submitted as this issue is created. ,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 23 08:31:03 UTC 2022,,,,,,,,,,"0|z13034:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 04:14;billrao;A pull request #19890 is submitted. [~dianfu] Could you help to merge it? Thanks ;;;","23/Jun/22 08:31;martijnvisser;Fixed in master: 4904727b81e8349dfd02a75f8916718fb38e7f9b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid to create watcher without the resourceVersion,FLINK-27925,13448680,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ouyangwulin,aitozi,aitozi,07/Jun/22 02:10,12/Jun/23 07:11,04/Jun/24 20:51,12/Jun/23 07:11,1.18.0,,,,,,,,,,,,,,1.18.0,,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"Currently, we create the watcher in KubernetesResourceManager. But it do not pass the resourceVersion parameter, it will trigger a request to etcd. It will bring the burden to the etcd in large scale cluster (which have been seen in our internal k8s cluster). More detail can be found [here|https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter] 

I think we could use the informer to improve it (which will spawn a list-watch and maintain the resourceVersion internally)",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31589,,,,,,,,,,,,,,,,,,,"19/Dec/22 12:19;ouyangwuli;image-2022-12-19-20-19-41-303.png;https://issues.apache.org/jira/secure/attachment/13053980/image-2022-12-19-20-19-41-303.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 12 07:11:09 UTC 2023,,,,,,,,,,"0|z12zyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 02:15;aitozi;cc [~wangyang0918] ;;;","10/Jun/22 03:31;wangyang0918;I agree that we could use a shared informer for the pod watching. It should simply the error handling especially when creating watch failed or too old resource version exception.

The drawback is the informer will consume more memory due to local cache.

 

Are we sure that triggering a request to ETCD is caused by pods watcher without resource version?;;;","10/Jun/22 05:07;aitozi;I think it will only cache the current job's pod list, Right ? I think the memory consume is acceptable.

From the doc, it will trigger a request to ETCD if we create a watcher with the resource version unset. From our internal case, it's also observed.;;;","13/Jun/22 02:19;wangyang0918;I am not whether triggering a request to ETCD is caused by {{{}getPodsWithLabels{}}}.;;;","13/Jun/22 07:22;aitozi;Get your meaning, I will try to test for it;;;","19/Dec/22 09:44;ouyangwuli;In the case of large-scale start and stop jobs, constantly reading data from etcd can cause a bottleneck in etcd performance. I agree with [~wangyang0918]  using informer will increase memory pressure. We can increase resourceversion=0 in watcher to reduce data read from etcd.

As [https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter] describe and the screenshots of code，

   1 .""resourceVersion unset"" is means ""Most Recent"" ，The returned data must be consistent (in detail: served from etcd via a quorum read).

   2. ""resourceVersion=""0"" is means ""Any"".  Return data at any resource version. The newest available resource version is preferred, but strong consistency is not required; data at any resource version may be served. It is possible for the request to return data at a much older resource version that the client has previously observed, particularly in high availability configurations, due to partitions or stale caches. 

!image-2022-12-19-20-19-41-303.png!;;;","19/Dec/22 13:03;wangyang0918;Thanks [~ouyangwuli] for sharing the investigation. +1 for adding the resourceVersion=0 when doing the list pods.;;;","07/Apr/23 03:33;huwh;Hi [~ouyangwulin] . Are you still working on this?;;;","07/Apr/23 04:15;Weijie Guo;Hi [~huwh].

AFAIK, This PR is waiting for reviewer's feedback. 

Through offline communication with [~wangyang0918] , I will take over and help continue review. If you want, you are welcome to take a look together.;;;","11/May/23 08:41;huwh;[~Weijie Guo] This issue has not been updated for a long time. I'd like to take it over if it is possible;;;","11/May/23 11:42;ouyangwuli;[~huwh]  tks, I will update.;;;","11/May/23 12:46;huwh;Thanks [~ouyangwulin] ,  glad to hear that.;;;","12/Jun/23 07:11;Weijie Guo;master(1.18) via 025a95b627faf8ec8b725a7784d1279b41e10ba7.;;;",,,,,,,,,,,,,,,,,,,
Add pulsar comments in the datastream package,FLINK-27924,13448679,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,07/Jun/22 02:08,08/Jun/22 09:23,04/Jun/24 20:51,08/Jun/22 09:23,,,,,,,,,,,,,,,1.16.0,,,,API / Python,Documentation,,,,0,pull-request-available,,,,https://github.com/apache/flink/pull/19732#discussion_r883356468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 09:23:44 UTC 2022,,,,,,,,,,"0|z12zyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 09:23;dianfu;Merged to master via 8cd136bf86ff5c35819c22712add1557cc352092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo fix for release-1.0.0 quick-start.md,FLINK-27923,13448623,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jbusche,jbusche,jbusche,06/Jun/22 17:01,06/Jun/22 19:41,04/Jun/24 20:51,06/Jun/22 19:41,kubernetes-operator-1.0.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Noticed a typo while deploying the example.

Currently:
kubectl create -f https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-0.1/examples/basic.yaml


Where it should be:

kubectl create -f https://raw.githubusercontent.com/apache/flink-kubernetes-operator/release-1.0.0/examples/basic.yaml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-06 17:01:22.0,,,,,,,,,,"0|z12zm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL unique key lost when set table.exec.mini-batch.enabled=true,FLINK-27922,13448590,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhangbinzaifendou,zhangbinzaifendou,06/Jun/22 13:49,10/Aug/23 22:35,04/Jun/24 20:51,,1.12.2,1.15.0,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,pull-request-available,,,"Flink SQL table has primary keys, but when set table.exec.mini-batch.enabled =true, the unique key is lost.
{code:java}
@Test
public void testJoinUniqueKey() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);    Configuration configuration = tableEnv.getConfig().getConfiguration();
    configuration.setString(""table.exec.mini-batch.enabled"", ""true"");
    configuration.setString(""table.exec.mini-batch.allow-latency"", ""5 s"");
    configuration.setString(""table.exec.mini-batch.size"", ""5000"");    StatementSet statementSet = tableEnv.createStatementSet();
    tableEnv.executeSql(""CREATE TABLE `t_apply_sku_test`(`dt` BIGINT,`refund_apply_id` BIGINT,`base_sku_id` BIGINT,`order_id` BIGINT,`user_id` BIGINT,`poi_id` BIGINT,`refund_type` BIGINT,`apply_refund_reason_code` BIGINT,`apply_refund_reason_desc` VARCHAR,`apply_refund_review_status` BIGINT,`apply_refund_review_status_desc` VARCHAR,`apply_refund_reject_reason` VARCHAR,`apply_is_refunded` INTEGER,`apply_pic_url` VARCHAR,`remark` VARCHAR,`refund_apply_originator` BIGINT,`second_reason_code` BIGINT,`second_reason` VARCHAR,`refund_target_account` BIGINT,`after_service_id` BIGINT,`receipt_status` BIGINT,`group_header_goods_status` INTEGER,`apply_operator_mis_name` VARCHAR,`refund_apply_time` BIGINT,`update_time` BIGINT,`base_sku_name` VARCHAR,`apply_refund_num` BIGINT,`view_qty` DECIMAL(38,18),`refund_scale_type` BIGINT,`refund_scale_type_desc` VARCHAR,`refund_scale` DECIMAL(38,18),`apply_refund_amt` DECIMAL(38,18),`refund_scale_user_real_pay` DECIMAL(38,18),`refund_red_packet_price` DECIMAL(38,18),`load_time` VARCHAR,`take_rate_type` BIGINT,`platform_rate` DECIMAL(38,18),`order_sku_type` INTEGER,`second_reason_aggregated_code` INTEGER,`second_reason_aggregated` VARCHAR,`compensation_amount` DECIMAL(38,18),`aftersale_type` INTEGER,`group_header_parallel_status` INTEGER,`grid_parallel_status` INTEGER) WITH ('connector'='blackhole')"");
    tableEnv.executeSql(""CREATE TABLE `t_name`(`id` BIGINT,`after_service_id` BIGINT PRIMARY KEY NOT ENFORCED,`order_id` BIGINT,`user_id` BIGINT,`poi_id` BIGINT,`city_id` BIGINT,`refund_type` INTEGER,`first_reason_code` INTEGER,`first_reason` VARCHAR,`second_reason_code` INTEGER,`second_reason` VARCHAR,`pic_url` VARCHAR,`remark` VARCHAR,`refund_price` INTEGER,`refund_red_packet_price` INTEGER,`refund_total_price` INTEGER,`refund_promotion_price` INTEGER,`refund_coupon_price` INTEGER,`refund_other_price` INTEGER,`user_receipt_status` INTEGER,`collect_status` INTEGER,`refund_target_account` INTEGER,`status` INTEGER,`flow_instance_id` BIGINT,`create_time` BIGINT,`modify_time` BIGINT) WITH ('connector'='datagen')"");
    tableEnv.executeSql(""CREATE TABLE `t_item_name`(`id` BIGINT,`refund_fwd_item_id` BIGINT,`after_service_id` BIGINT,`order_id` BIGINT,`order_item_id` BIGINT,`stack_id` BIGINT,`sku_id` BIGINT,`sku_name` VARCHAR,`supplier_id` BIGINT,`refund_quantity` INTEGER,`item_sku_type` INTEGER,`refund_scale_type` INTEGER,`refund_scale` INTEGER,`accurate_refund` INTEGER,`refund_price` INTEGER,`refund_red_packet_price` INTEGER,`refund_price_info` VARCHAR,`refund_total_price` INTEGER,`refund_promotion_price` INTEGER,`refund_coupon_price` INTEGER,`refund_other_price` INTEGER,`extend_info` VARCHAR,`create_time` BIGINT,`modify_time` BIGINT) WITH ('connector'='datagen')"");
    tableEnv.executeSql(""CREATE TABLE `t_progress_name`(`id` BIGINT,`after_service_id` BIGINT PRIMARY KEY NOT ENFORCED,`order_id` BIGINT,`progress_node` VARCHAR,`progress_node_status` INTEGER,`operator` VARCHAR,`parallel` INTEGER,`flow_element_id` VARCHAR,`extend_info` VARCHAR,`create_time` BIGINT,`modify_time` BIGINT) WITH ('connector'='datagen')"");
    tableEnv.executeSql(""CREATE TABLE `t_attr_name`(`id` BIGINT,`after_service_id` BIGINT PRIMARY KEY NOT ENFORCED,`order_id` BIGINT,`name` VARCHAR,`value` VARCHAR,`create_time` BIGINT,`modify_time` BIGINT) WITH ('connector'='datagen')"");
    tableEnv.executeSql(""CREATE TABLE `t_apply_status_name`(`id` BIGINT,`after_service_id` BIGINT PRIMARY KEY NOT ENFORCED,`order_id` BIGINT,`apply_status` INTEGER,`ai_audit_status` INTEGER,`group_header_confirm_status` INTEGER,`group_header_retrieve_status` INTEGER,`driver_retrieve_status` INTEGER,`group_header_parallel_status` INTEGER,`grid_parallel_status` INTEGER,`create_time` BIGINT,`modify_time` BIGINT) WITH ('connector'='datagen')"");
    tableEnv.executeSql(""CREATE VIEW `v_refund_fwd_item_sku` AS SELECT `after_service_id` AS `refund_apply_id`, `order_id`, `sku_id` AS `base_sku_id`, SUM(`if`(`refund_quantity` IS NULL, 0, `refund_quantity`)) AS `apply_refund_num`, SUM(`if`(`refund_quantity` IS NULL, 0, CAST(`refund_quantity` AS DOUBLE))) AS `view_qty`, SUM(`if`(`refund_price` IS NULL, 0, `refund_price`)) AS `apply_refund_amt`, SUM(`if`(`refund_price` IS NULL, 0, `refund_price`)) AS `refund_scale_user_real_pay`, SUM(`if`(`refund_red_packet_price` IS NULL, 0, `refund_red_packet_price`)) AS `refund_red_packet_price`\n""
            + ""FROM `t_item_name`\n""
            + ""GROUP BY `after_service_id`, `order_id`, `sku_id`"");    statementSet.addInsertSql(""INSERT INTO `t_apply_sku_test` SELECT CAST(`FROM_UNIXTIME`(`ord`.`modify_time` / 1000, 'yyyyMMdd') AS BIGINT) AS `dt`, `sku`.`refund_apply_id`,`sku`.`base_sku_id`, `sku`.`order_id`, `ord`.`user_id`, `ord`.`poi_id`, CAST(`ord`.`refund_type` AS BIGINT) AS `refund_type`, CAST(`ord`.`first_reason_code` AS BIGINT) AS `apply_refund_reason_code`, `ord`.`first_reason` AS `apply_refund_reason_desc`, CAST(`stat`.`apply_status` AS BIGINT) AS `apply_refund_review_status`, CASE WHEN `stat`.`apply_status` = 0 OR `stat`.`apply_status` = 13 THEN 'a' WHEN `stat`.`apply_status` = 1 THEN 'b' WHEN `stat`.`apply_status` = 2 THEN 'c' WHEN `stat`.`apply_status` = 3 THEN 'd' WHEN `stat`.`apply_status` = 4 THEN 'e' WHEN `stat`.`apply_status` = 5 THEN 'f' WHEN `stat`.`apply_status` = 6 THEN 'g' ELSE 'x' END AS `apply_refund_review_status_desc`, `if`(`progress`.`progress_node_status` = 30, `extend_info`, '') AS `apply_refund_reject_reason`, CASE WHEN `progress`.`progress_node` = 'refund.node' THEN 1 ELSE 0 END AS `apply_is_refunded`, `ord`.`pic_url` AS `apply_pic_url`, `ord`.`remark`, CAST(NULL AS BIGINT) AS `refund_apply_originator`, CAST(`ord`.`second_reason_code` AS BIGINT) AS `second_reason_code`, `ord`.`second_reason`, CAST(`ord`.`refund_target_account` AS BIGINT) AS `refund_target_account`, `ord`.`after_service_id`, CAST(`ord`.`user_receipt_status` AS BIGINT) AS `receipt_status`, CASE WHEN `attr`.`name` = 'fwd_ext' AND `attr`.`value` = '1' THEN 1 WHEN `attr`.`name` = 'fwd_ext' AND `attr`.`value` = '2' THEN 2 ELSE 0 END AS `group_header_goods_status`, `progress`.`operator` AS `apply_operator_mis_name`, `ord`.`create_time` AS `refund_apply_time`, `ord`.`modify_time` AS `update_time`, CAST(NULL AS VARCHAR) AS `base_sku_name`, CAST(`sku`.`apply_refund_num` AS BIGINT) AS `apply_refund_num`, `sku`.`view_qty` AS `view_qty`, CAST(NULL AS BIGINT) AS `refund_scale_type`, CAST(NULL AS VARCHAR) AS `refund_scale_type_desc`, CAST(NULL AS DECIMAL) AS `refund_scale`, `sku`.`apply_refund_amt` AS `apply_refund_amt`, `sku`.`refund_scale_user_real_pay` AS `refund_scale_user_real_pay`, `sku`.`refund_red_packet_price` AS `refund_red_packet_price`, CAST(LOCALTIMESTAMP AS VARCHAR) AS `load_time`, CAST(NULL AS BIGINT) AS `take_rate_type`, CAST(NULL AS DECIMAL) AS `platform_rate`, 1 AS `order_sku_type`, CAST(NULL AS INTEGER) AS `second_reason_aggregated_code`, CAST(NULL AS VARCHAR) AS `second_reason_aggregated`, CAST(NULL AS DECIMAL) AS `compensation_amount`, 1 AS `aftersale_type`, CASE WHEN `progress`.`progress_node` = 'group.header.audit.node' AND `parallel` = 1 AND `progress_node_status` = 10 THEN 1 WHEN `progress`.`progress_node` = 'group.header.audit.node' AND `parallel` = 1 AND `progress_node_status` = 20 THEN 2 WHEN `progress`.`progress_node` = 'group.header.audit.node' AND `parallel` = 1 AND `progress_node_status` = 30 THEN 3 WHEN `progress`.`progress_node` = 'group.header.audit.node' AND `parallel` = 1 AND `progress_node_status` = 40 THEN 4 WHEN `progress`.`progress_node` = 'group.header.audit.node' AND `parallel` = 1 AND `progress_node_status` = 50 THEN 5 ELSE 0 END AS `group_header_parallel_status`, CASE WHEN `progress`.`progress_node` = 'grid.audit.node' AND `parallel` = 1 AND `progress_node_status` = 10 THEN 1 WHEN `progress`.`progress_node` = 'grid.audit.node' AND `parallel` = 1 AND `progress_node_status` = 20 THEN 2 WHEN `progress`.`progress_node` = 'grid.audit.node' AND `parallel` = 1 AND `progress_node_status` = 30 THEN 3 WHEN `progress`.`progress_node` = 'grid.audit.node' AND `parallel` = 1 AND `progress_node_status` = 40 THEN 4 WHEN `progress`.`progress_node` = 'grid.audit.node' AND `parallel` = 1 AND `progress_node_status` = 50 THEN 5 ELSE 0 END AS `grid_parallel_status`\n""
            + ""FROM `t_name` AS `ord`\n""
            + ""LEFT JOIN `t_progress_name` AS `progress` ON `ord`.`after_service_id` = `progress`.`after_service_id`\n""
            + ""LEFT JOIN `t_attr_name` AS `attr` ON `ord`.`after_service_id` = `attr`.`after_service_id`\n""
            + ""LEFT JOIN `t_apply_status_name` AS `stat` ON `ord`.`after_service_id` = `stat`.`after_service_id`\n""
            + ""LEFT JOIN `v_refund_fwd_item_sku` AS `sku` ON `stat`.`after_service_id` = `sku`.`refund_apply_id`"");    System.out.println(statementSet.explain());
}{code}
== Optimized Logical Plan ==
{code:java}
Sink(table=[default_catalog.default_database.t_apply_sku_test], fields=[dt, refund_apply_id, base_sku_id, order_id, user_id, poi_id, refund_type, apply_refund_reason_code, apply_refund_reason_desc, apply_refund_review_status, apply_refund_review_status_desc, apply_refund_reject_reason, apply_is_refunded, apply_pic_url, remark, refund_apply_originator, second_reason_code, second_reason, refund_target_account, after_service_id, receipt_status, group_header_goods_status, apply_operator_mis_name, refund_apply_time, update_time, base_sku_name, apply_refund_num, view_qty, refund_scale_type, refund_scale_type_desc, refund_scale, apply_refund_amt, refund_scale_user_real_pay, refund_red_packet_price, load_time, take_rate_type, platform_rate, order_sku_type, second_reason_aggregated_code, second_reason_aggregated, compensation_amount, aftersale_type, group_header_parallel_status, grid_parallel_status])
+- Calc(select=[CAST(((modify_time / 1000) FROM_UNIXTIME _UTF-16LE'yyyyMMdd')) AS dt, refund_apply_id, base_sku_id, order_id, user_id, poi_id, CAST(refund_type) AS refund_type, CAST(first_reason_code) AS apply_refund_reason_code, first_reason AS apply_refund_reason_desc, CAST(apply_status) AS apply_refund_review_status, CAST(((apply_status SEARCH Sarg[0, 13]) CASE _UTF-16LE'a' CASE (apply_status = 1) CASE _UTF-16LE'b' CASE (apply_status = 2) CASE _UTF-16LE'c' CASE (apply_status = 3) CASE _UTF-16LE'd' CASE (apply_status = 4) CASE _UTF-16LE'e' CASE (apply_status = 5) CASE _UTF-16LE'f' CASE (apply_status = 6) CASE _UTF-16LE'g' CASE _UTF-16LE'x')) AS apply_refund_review_status_desc, ((progress_node_status = 30) IF extend_info IF _UTF-16LE'') AS apply_refund_reject_reason, CAST(((progress_node = _UTF-16LE'refund.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") CASE 1 CASE 0)) AS apply_is_refunded, pic_url AS apply_pic_url, remark, null:BIGINT AS refund_apply_originator, CAST(second_reason_code) AS second_reason_code, second_reason, CAST(refund_target_account) AS refund_target_account, CAST(after_service_id) AS after_service_id, CAST(user_receipt_status) AS receipt_status, CAST((((name = _UTF-16LE'fwd_ext':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (value = _UTF-16LE'1':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")) CASE 1 CASE ((name = _UTF-16LE'fwd_ext':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (value = _UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")) CASE 2 CASE 0)) AS group_header_goods_status, operator AS apply_operator_mis_name, create_time AS refund_apply_time, modify_time AS update_time, null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS base_sku_name, CAST(apply_refund_num) AS apply_refund_num, CAST(view_qty) AS view_qty, null:BIGINT AS refund_scale_type, null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS refund_scale_type_desc, null:DECIMAL(38, 18) AS refund_scale, CAST(apply_refund_amt) AS apply_refund_amt, CAST(refund_scale_user_real_pay) AS refund_scale_user_real_pay, CAST(refund_red_packet_price) AS refund_red_packet_price, CAST(CAST(())) AS load_time, null:BIGINT AS take_rate_type, null:DECIMAL(38, 18) AS platform_rate, 1 AS order_sku_type, null:INTEGER AS second_reason_aggregated_code, null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS second_reason_aggregated, null:DECIMAL(38, 18) AS compensation_amount, 1 AS aftersale_type, CAST((((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 10)) CASE 1 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 20)) CASE 2 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 30)) CASE 3 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 40)) CASE 4 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 50)) CASE 5 CASE 0)) AS group_header_parallel_status, CAST((((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 10)) CASE 1 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 20)) CASE 2 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 30)) CASE 3 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 40)) CASE 4 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 50)) CASE 5 CASE 0)) AS grid_parallel_status])
   +- Join(joinType=[LeftOuterJoin], where=[(after_service_id0 = refund_apply_id)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, name, value, after_service_id0, apply_status, refund_apply_id, order_id, base_sku_id, apply_refund_num, view_qty, apply_refund_amt, refund_scale_user_real_pay, refund_red_packet_price], leftInputSpec=[NoUniqueKey], rightInputSpec=[HasUniqueKey])
      :- Exchange(distribution=[hash[after_service_id0]])
      :  +- Join(joinType=[LeftOuterJoin], where=[(after_service_id = after_service_id0)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, name, value, after_service_id0, apply_status], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
      :     :- Exchange(distribution=[hash[after_service_id]])
      :     :  +- Calc(select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, name, value])
      :     :     +- Join(joinType=[LeftOuterJoin], where=[(after_service_id = after_service_id0)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, after_service_id0, name, value], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
      :     :        :- Exchange(distribution=[hash[after_service_id]])
      :     :        :  +- Calc(select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info])
      :     :        :     +- Join(joinType=[LeftOuterJoin], where=[(after_service_id = after_service_id0)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, after_service_id0, progress_node, progress_node_status, operator, parallel, extend_info], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
      :     :        :        :- Exchange(distribution=[hash[after_service_id]])
      :     :        :        :  +- Calc(select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time])
      :     :        :        :     +- DropUpdateBefore
      :     :        :        :        +- MiniBatchAssigner(interval=[5000ms], mode=[ProcTime])
      :     :        :        :           +- TableSourceScan(table=[[default_catalog, default_database, t_name]], fields=[id, after_service_id, order_id, user_id, poi_id, city_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, refund_price, refund_red_packet_price, refund_total_price, refund_promotion_price, refund_coupon_price, refund_other_price, user_receipt_status, collect_status, refund_target_account, status, flow_instance_id, create_time, modify_time])
      :     :        :        +- Exchange(distribution=[hash[after_service_id]])
      :     :        :           +- Calc(select=[after_service_id, progress_node, progress_node_status, operator, parallel, extend_info])
      :     :        :              +- DropUpdateBefore
      :     :        :                 +- MiniBatchAssigner(interval=[5000ms], mode=[ProcTime])
      :     :        :                    +- TableSourceScan(table=[[default_catalog, default_database, t_progress_name]], fields=[id, after_service_id, order_id, progress_node, progress_node_status, operator, parallel, flow_element_id, extend_info, create_time, modify_time])
      :     :        +- Exchange(distribution=[hash[after_service_id]])
      :     :           +- Calc(select=[after_service_id, name, value])
      :     :              +- DropUpdateBefore
      :     :                 +- MiniBatchAssigner(interval=[5000ms], mode=[ProcTime])
      :     :                    +- TableSourceScan(table=[[default_catalog, default_database, t_attr_name]], fields=[id, after_service_id, order_id, name, value, create_time, modify_time])
      :     +- Exchange(distribution=[hash[after_service_id]])
      :        +- Calc(select=[after_service_id, apply_status])
      :           +- DropUpdateBefore
      :              +- MiniBatchAssigner(interval=[5000ms], mode=[ProcTime])
      :                 +- TableSourceScan(table=[[default_catalog, default_database, t_apply_status_name]], fields=[id, after_service_id, order_id, apply_status, ai_audit_status, group_header_confirm_status, group_header_retrieve_status, driver_retrieve_status, group_header_parallel_status, grid_parallel_status, create_time, modify_time])
      +- Exchange(distribution=[hash[refund_apply_id]])
         +- Calc(select=[refund_apply_id, order_id, base_sku_id, apply_refund_num, view_qty, refund_scale_user_real_pay AS apply_refund_amt, refund_scale_user_real_pay, refund_red_packet_price])
            +- GlobalGroupAggregate(groupBy=[refund_apply_id, order_id, base_sku_id], select=[refund_apply_id, order_id, base_sku_id, SUM_RETRACT((sum$0, count$1)) AS apply_refund_num, SUM_RETRACT((sum$2, count$3)) AS view_qty, SUM_RETRACT((sum$4, count$5)) AS refund_scale_user_real_pay, SUM_RETRACT((sum$6, count$7)) AS refund_red_packet_price])
               +- Exchange(distribution=[hash[refund_apply_id, order_id, base_sku_id]])
                  +- LocalGroupAggregate(groupBy=[refund_apply_id, order_id, base_sku_id], select=[refund_apply_id, order_id, base_sku_id, SUM_RETRACT($f3) AS (sum$0, count$1), SUM_RETRACT($f4) AS (sum$2, count$3), SUM_RETRACT($f5) AS (sum$4, count$5), SUM_RETRACT($f6) AS (sum$6, count$7), COUNT_RETRACT(*) AS count1$8])
                     +- Calc(select=[after_service_id AS refund_apply_id, order_id, sku_id AS base_sku_id, (refund_quantity IS NULL IF 0 IF refund_quantity) AS $f3, (refund_quantity IS NULL IF 0 IF CAST(refund_quantity)) AS $f4, (refund_price IS NULL IF 0 IF refund_price) AS $f5, (refund_red_packet_price IS NULL IF 0 IF refund_red_packet_price) AS $f6])
                        +- MiniBatchAssigner(interval=[5000ms], mode=[ProcTime])
                           +- TableSourceScan(table=[[default_catalog, default_database, t_item_name]], fields=[id, refund_fwd_item_id, after_service_id, order_id, order_item_id, stack_id, sku_id, sku_name, supplier_id, refund_quantity, item_sku_type, refund_scale_type, refund_scale, accurate_refund, refund_price, refund_red_packet_price, refund_price_info, refund_total_price, refund_promotion_price, refund_coupon_price, refund_other_price, extend_info, create_time, modify_time]) {code}
when set table.exec.mini-batch.enabled=false

== Optimized Logical Plan ==
{code:java}
Sink(table=[default_catalog.default_database.t_apply_sku_test], fields=[dt, refund_apply_id, base_sku_id, order_id, user_id, poi_id, refund_type, apply_refund_reason_code, apply_refund_reason_desc, apply_refund_review_status, apply_refund_review_status_desc, apply_refund_reject_reason, apply_is_refunded, apply_pic_url, remark, refund_apply_originator, second_reason_code, second_reason, refund_target_account, after_service_id, receipt_status, group_header_goods_status, apply_operator_mis_name, refund_apply_time, update_time, base_sku_name, apply_refund_num, view_qty, refund_scale_type, refund_scale_type_desc, refund_scale, apply_refund_amt, refund_scale_user_real_pay, refund_red_packet_price, load_time, take_rate_type, platform_rate, order_sku_type, second_reason_aggregated_code, second_reason_aggregated, compensation_amount, aftersale_type, group_header_parallel_status, grid_parallel_status])
+- Calc(select=[CAST(((modify_time / 1000) FROM_UNIXTIME _UTF-16LE'yyyyMMdd')) AS dt, refund_apply_id, base_sku_id, order_id, user_id, poi_id, CAST(refund_type) AS refund_type, CAST(first_reason_code) AS apply_refund_reason_code, first_reason AS apply_refund_reason_desc, CAST(apply_status) AS apply_refund_review_status, CAST(((apply_status SEARCH Sarg[0, 13]) CASE _UTF-16LE'a' CASE (apply_status = 1) CASE _UTF-16LE'b' CASE (apply_status = 2) CASE _UTF-16LE'c' CASE (apply_status = 3) CASE _UTF-16LE'd' CASE (apply_status = 4) CASE _UTF-16LE'e' CASE (apply_status = 5) CASE _UTF-16LE'f' CASE (apply_status = 6) CASE _UTF-16LE'g' CASE _UTF-16LE'x')) AS apply_refund_review_status_desc, ((progress_node_status = 30) IF extend_info IF _UTF-16LE'') AS apply_refund_reject_reason, CAST(((progress_node = _UTF-16LE'refund.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") CASE 1 CASE 0)) AS apply_is_refunded, pic_url AS apply_pic_url, remark, null:BIGINT AS refund_apply_originator, CAST(second_reason_code) AS second_reason_code, second_reason, CAST(refund_target_account) AS refund_target_account, CAST(after_service_id) AS after_service_id, CAST(user_receipt_status) AS receipt_status, CAST((((name = _UTF-16LE'fwd_ext':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (value = _UTF-16LE'1':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")) CASE 1 CASE ((name = _UTF-16LE'fwd_ext':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (value = _UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"")) CASE 2 CASE 0)) AS group_header_goods_status, operator AS apply_operator_mis_name, create_time AS refund_apply_time, modify_time AS update_time, null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS base_sku_name, CAST(apply_refund_num) AS apply_refund_num, CAST(view_qty) AS view_qty, null:BIGINT AS refund_scale_type, null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS refund_scale_type_desc, null:DECIMAL(38, 18) AS refund_scale, CAST(apply_refund_amt) AS apply_refund_amt, CAST(refund_scale_user_real_pay) AS refund_scale_user_real_pay, CAST(refund_red_packet_price) AS refund_red_packet_price, CAST(CAST(())) AS load_time, null:BIGINT AS take_rate_type, null:DECIMAL(38, 18) AS platform_rate, 1 AS order_sku_type, null:INTEGER AS second_reason_aggregated_code, null:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS second_reason_aggregated, null:DECIMAL(38, 18) AS compensation_amount, 1 AS aftersale_type, CAST((((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 10)) CASE 1 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 20)) CASE 2 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 30)) CASE 3 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 40)) CASE 4 CASE ((progress_node = _UTF-16LE'group.header.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 50)) CASE 5 CASE 0)) AS group_header_parallel_status, CAST((((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 10)) CASE 1 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 20)) CASE 2 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 30)) CASE 3 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 40)) CASE 4 CASE ((progress_node = _UTF-16LE'grid.audit.node':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AND (parallel = 1) AND (progress_node_status = 50)) CASE 5 CASE 0)) AS grid_parallel_status])
   +- Join(joinType=[LeftOuterJoin], where=[(after_service_id0 = refund_apply_id)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, name, value, after_service_id0, apply_status, refund_apply_id, order_id, base_sku_id, apply_refund_num, view_qty, apply_refund_amt, refund_scale_user_real_pay, refund_red_packet_price], leftInputSpec=[HasUniqueKey], rightInputSpec=[HasUniqueKey])
      :- Exchange(distribution=[hash[after_service_id0]])
      :  +- Join(joinType=[LeftOuterJoin], where=[(after_service_id = after_service_id0)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, name, value, after_service_id0, apply_status], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
      :     :- Exchange(distribution=[hash[after_service_id]])
      :     :  +- Calc(select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, name, value])
      :     :     +- Join(joinType=[LeftOuterJoin], where=[(after_service_id = after_service_id0)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info, after_service_id0, name, value], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
      :     :        :- Exchange(distribution=[hash[after_service_id]])
      :     :        :  +- Calc(select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, progress_node, progress_node_status, operator, parallel, extend_info])
      :     :        :     +- Join(joinType=[LeftOuterJoin], where=[(after_service_id = after_service_id0)], select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time, after_service_id0, progress_node, progress_node_status, operator, parallel, extend_info], leftInputSpec=[JoinKeyContainsUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])
      :     :        :        :- Exchange(distribution=[hash[after_service_id]])
      :     :        :        :  +- Calc(select=[after_service_id, user_id, poi_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, user_receipt_status, refund_target_account, create_time, modify_time])
      :     :        :        :     +- DropUpdateBefore
      :     :        :        :        +- TableSourceScan(table=[[default_catalog, default_database, t_name]], fields=[id, after_service_id, order_id, user_id, poi_id, city_id, refund_type, first_reason_code, first_reason, second_reason_code, second_reason, pic_url, remark, refund_price, refund_red_packet_price, refund_total_price, refund_promotion_price, refund_coupon_price, refund_other_price, user_receipt_status, collect_status, refund_target_account, status, flow_instance_id, create_time, modify_time])
      :     :        :        +- Exchange(distribution=[hash[after_service_id]])
      :     :        :           +- Calc(select=[after_service_id, progress_node, progress_node_status, operator, parallel, extend_info])
      :     :        :              +- DropUpdateBefore
      :     :        :                 +- TableSourceScan(table=[[default_catalog, default_database, t_progress_name]], fields=[id, after_service_id, order_id, progress_node, progress_node_status, operator, parallel, flow_element_id, extend_info, create_time, modify_time])
      :     :        +- Exchange(distribution=[hash[after_service_id]])
      :     :           +- Calc(select=[after_service_id, name, value])
      :     :              +- DropUpdateBefore
      :     :                 +- TableSourceScan(table=[[default_catalog, default_database, t_attr_name]], fields=[id, after_service_id, order_id, name, value, create_time, modify_time])
      :     +- Exchange(distribution=[hash[after_service_id]])
      :        +- Calc(select=[after_service_id, apply_status])
      :           +- DropUpdateBefore
      :              +- TableSourceScan(table=[[default_catalog, default_database, t_apply_status_name]], fields=[id, after_service_id, order_id, apply_status, ai_audit_status, group_header_confirm_status, group_header_retrieve_status, driver_retrieve_status, group_header_parallel_status, grid_parallel_status, create_time, modify_time])
      +- Exchange(distribution=[hash[refund_apply_id]])
         +- Calc(select=[refund_apply_id, order_id, base_sku_id, apply_refund_num, view_qty, refund_scale_user_real_pay AS apply_refund_amt, refund_scale_user_real_pay, refund_red_packet_price])
            +- GroupAggregate(groupBy=[refund_apply_id, order_id, base_sku_id], select=[refund_apply_id, order_id, base_sku_id, SUM_RETRACT($f3) AS apply_refund_num, SUM_RETRACT($f4) AS view_qty, SUM_RETRACT($f5) AS refund_scale_user_real_pay, SUM_RETRACT($f6) AS refund_red_packet_price])
               +- Exchange(distribution=[hash[refund_apply_id, order_id, base_sku_id]])
                  +- Calc(select=[after_service_id AS refund_apply_id, order_id, sku_id AS base_sku_id, (refund_quantity IS NULL IF 0 IF refund_quantity) AS $f3, (refund_quantity IS NULL IF 0 IF CAST(refund_quantity)) AS $f4, (refund_price IS NULL IF 0 IF refund_price) AS $f5, (refund_red_packet_price IS NULL IF 0 IF refund_red_packet_price) AS $f6])
                     +- TableSourceScan(table=[[default_catalog, default_database, t_item_name]], fields=[id, refund_fwd_item_id, after_service_id, order_id, order_item_id, stack_id, sku_id, sku_name, supplier_id, refund_quantity, item_sku_type, refund_scale_type, refund_scale, accurate_refund, refund_price, refund_red_packet_price, refund_price_info, refund_total_price, refund_promotion_price, refund_coupon_price, refund_other_price, extend_info, create_time, modify_time]) {code}
 ",Flink1.12.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 10 22:35:17 UTC 2023,,,,,,,,,,"0|z12zew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 08:26;martijnvisser;[~zhangbinzaifendou] Can you please verify this ticket also with Flink 1.15, since Flink 1.12 is no longer supported by the Flink community? ;;;","08/Jun/22 03:18;zhangbinzaifendou;[~martijnvisser] Flink 1.15 still has this problem but it is different from Flink 1.12.2

Flink1.15

[https://github.com/apache/flink/pull/19901]

Flink 1.12

https://github.com/apache/flink/pull/19903;;;","10/Jun/22 08:36;xuyangzhong;Hi, I think this is duplicated with FLINK-27851  . What do you think, [~zhangbinzaifendou] ?  ;;;","27/Jun/22 13:33;zhangbinzaifendou;Maybe we have the same bug.But something is different

In Flink 1.12, FlinkRelMdColumnUniqueness also needs to have the judgment of StreamExecMiniBatchAssigner, please test the example I provided

Flink1.15 need only in FlinkRelMdUpsertKeys increase StreamExecMiniBatchAssigner judgment.;;;","27/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the checkResourceRequirementsWithDelay in DeclarativeSlotManager,FLINK-27921,13448577,13409366,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,06/Jun/22 12:51,24/Jun/22 10:42,04/Jun/24 20:51,13/Jun/22 02:34,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"As discussed in [https://github.com/apache/flink/pull/19840#discussion_r884242067] .This 
ticket is meant to introduce the same mechanism to wait for a slight delay before process the resource check with {{{}FineGrainedSlotManager{}}}. It will reduce the frequency of unnecessary re-allocations",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28241,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 24 10:42:39 UTC 2022,,,,,,,,,,"0|z12zc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 02:34;guoyangze;master: 65fda6e15a18eff8d81fbd60e8b4708527597c91;;;","23/Jun/22 09:03;akalashnikov;[~aitozi], [~guoyangze], This task caused a huge performance drop in many benchmarks(at least I have checked that this PR is the cause of asyncWait* and sorted* benchmarks but I assume that other benchmarks' performance drop also relates to this task). You can check all problematic benchmarks here - http://codespeed.dak8s.net:8000/changes/
I think this task should be reverted or fixed ASAP since there are too many benchmarks affected and the drop size is too big(up to 50%).

CC: [~roman], [~smattheis];;;","23/Jun/22 09:29;guoyangze;Hi, [~akalashnikov]. IIUC, this change will only take effect during the scheduling phase. Could you help me to understand why the benchmarks for operators were influenced by it?

Regarding the fix, we can simply disable this feature by default (by setting the default delay to zero). [~aitozi];;;","23/Jun/22 10:16;aitozi;I think the reason behind here may be the affected benchmark will run the minicluster which will use {{DeclarativeSlotManager}} and the schedule time will be counted in. Then each schedule will be run with slight delayed ;;;","23/Jun/22 10:17;guoyangze;I don't think it makes sense to take the scheduling time into account in operator benchmarks, though.;;;","23/Jun/22 10:33;akalashnikov;As I see at least in asyncWait benchmark, it recreates the job every iteration so the scheduling phase is present on each iteration as well. And the benchmark includes time for scheduling to the total time of the benchmark. Perhaps, it is not good for the benchmark since the ideal benchmark should measure the only time of targeted operation rather than all lifecycle(setup, shutdown) but it is what we have right now.
First of all, we should be sure that it is the problem only for our benchmark but it won't be visible to the user. If so, perhaps we can change the benchmark configuration in FlinkEnvironmentContext to disable this delay for all benchmarks. It looks not so good since we will benchmark non-default configuration but at the same time, changing the default configuration, only because our benchmark is not well written, is not a good idea as well.

[~roman], [~pnowojski], Maybe you have idea(or know), of how to make deal with situations when our benchmark takes into account too many operations instead of targeted ones. for example, instead of measuring only the time for processing events it also takes into account the time of initial phase of the job. As result, if the initial phase has an expected delay(exactly as this ticket), then benchmark shows the degradation but in fact, there is no degradation in the targeted operation.;;;","23/Jun/22 11:32;roman;There were some discussions in the past (e.g. in FLINK-25750), but nothing specific AFAIK.
I think eventually we do need to solve this problem by somehow excluding job setup time (the latter is or should already be measured by scheduler benchmarks).

If we agree on that, I'd propose:
1. Create a separate ticket for the current regression
2. Update (configure) the affected or all benchmarks so they don't use the new featur (and the results return back to normal)
3. Create a ticket to exclude job setup time
4. (gradually) update all the benchmarks (and remove the special config added in (2))

WDYT?;;;","23/Jun/22 12:49;pnowojski;Benchmarks maybe could be rewritten in such way that the measuring starts once for example all source subtasks are running, using some custom source. But if we can disable this feature for benchmarks, that would be simpler.

Separate thing to consider is whether we accept the increased start up cost introduced by this ticket.;;;","23/Jun/22 13:16;guoyangze;This feature targets to mitigate the redundant calling of the `checkResourceRequirements`, which is the hot path of SlotManager.  Although it introduces 50ms cost to the job setup time, we can avoid thousands of redundant resource matching in large-scale jobs. Besides, the default delay, 50ms, is trivial compared to resource allocating time and process setup time.;;;","24/Jun/22 10:42;akalashnikov;I have created the following tickets:
https://issues.apache.org/jira/browse/FLINK-28241
https://issues.apache.org/jira/browse/FLINK-28243

I already have this PR https://github.com/apache/flink-benchmarks/pull/56, right now, I wait for benchmarks to be sure that this fix works.;;;",,,,,,,,,,,,,,,,,,,,,,
Documented enums constant support ExcludeFromDocumentation annotation,FLINK-27920,13448576,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 12:40,18/Jan/23 09:07,04/Jun/24 20:51,10/Jun/22 02:07,,,,,,,,,,,,,,,1.16.0,,,,Documentation,,,,,0,pull-request-available,,,,"if a config option has @ExcludeFromDocumentation annotation, it will not appear in the document. But for an enumeration type, sometimes we only want some of it's constant values not to appear in the document, this ticket solves this problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 02:07:44 UTC 2022,,,,,,,,,,"0|z12zbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 08:38;chesnay;Do you have an example for such an enum?;;;","07/Jun/22 10:02;xtsong;[~chesnay],
This comes from the Hybrid Shuffle effort. We want to add new valid value for `execution.batch-shuffle-mode`, but not to expose it to users until the feature is completed. Just in case the feature is half-cooked by the feature freeze time.

[~Weijie Guo],
Please make sure you're assigned by a committer before start working on a ticket. That ensures at least one committer validates the ticket and is committed to help review and merge the PR. See the guidelines for more details. https://flink.apache.org/contributing/contribute-code.html;;;","08/Jun/22 08:36;chesnay;OK that makes sense. Are we concerned that users could accidentally stumble upon it when programmatically using config options?;;;","08/Jun/22 09:59;xtsong;Well, that could happen. Unfortunately, I don't find a good way to prevent it except for putting a large ""DO NOT USE"" in JavaDoc.;;;","08/Jun/22 10:44;chesnay;Just to throw some ideas around:

Depending on how reliant the code is on the actual enum, this could maybe also be solved with a system property.
We could also prefix the new enum value with INTERNAL_/EXPERIMENTAL_ and rename it once the effort is complete.;;;","09/Jun/22 01:44;xtsong;I think prefixing the enum value is really a good idea. Thanks a lot.

For system property, I'm afraid there would be many test cases that we need to modify once the feature is complete.;;;","10/Jun/22 02:07;xtsong;master: 608e40ac42bacda2aacc6be57d2072ece7f0cf31;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add FLIP-27-based Data Generator Source,FLINK-27919,13448572,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,06/Jun/22 12:15,14/Dec/23 15:15,04/Jun/24 20:51,23/Nov/22 15:33,,,,,,,,,,,,,,,1.17.0,,,,Connectors / Common,,,,,0,pull-request-available,,,,"It is frequently required to generate arbitrary events at the ""mock"" source. Such requirement arises both for Flink users, in the scope of demo/PoC projects, and for Flink developers when writing tests. The go-to solution for these purposes so far was implementing data generators as {{{}SourceFunctions{}}}. Since {{SourceFunction}} is superseded by FLIP-27 {{Source}} API and is due to be deprecated, it is required to provide users with a convenient alternative based on the new API.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31167,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 23 15:33:22 UTC 2022,,,,,,,,,,"0|z12zaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 15:33;chesnay;master: a5667e82e25cb87dc5523b82b08aec3e1408e9c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavepointITCase.testStopWithSavepointFailingAfterSnapshotCreation failed with Expected RuntimeException after snapshot creation,FLINK-27918,13448570,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,hxbks2ks,hxbks2ks,06/Jun/22 12:11,15/Aug/23 13:45,04/Jun/24 20:51,15/Aug/23 13:45,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,auto-deprioritized-critical,stale-major,test-stability,,"{code:java}
2022-06-06T03:13:54.0165829Z Jun 06 03:13:54 [ERROR] org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointFailingAfterSnapshotCreation  Time elapsed: 0.242 s  <<< ERROR!
2022-06-06T03:13:54.0167256Z Jun 06 03:13:54 java.util.concurrent.ExecutionException: org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.
2022-06-06T03:13:54.0173825Z Jun 06 03:13:54 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-06-06T03:13:54.0174662Z Jun 06 03:13:54 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-06T03:13:54.0180645Z Jun 06 03:13:54 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithFailingSourceInOnePipeline(SavepointITCase.java:1175)
2022-06-06T03:13:54.0181702Z Jun 06 03:13:54 	at org.apache.flink.test.checkpointing.SavepointITCase.testStopWithSavepointFailingAfterSnapshotCreation(SavepointITCase.java:1020)
2022-06-06T03:13:54.0182472Z Jun 06 03:13:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-06T03:13:54.0184012Z Jun 06 03:13:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-06T03:13:54.0185109Z Jun 06 03:13:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-06T03:13:54.0185907Z Jun 06 03:13:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-06T03:13:54.0187049Z Jun 06 03:13:54 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-06T03:13:54.0188081Z Jun 06 03:13:54 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-06T03:13:54.0189241Z Jun 06 03:13:54 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-06T03:13:54.0190002Z Jun 06 03:13:54 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-06T03:13:54.0190704Z Jun 06 03:13:54 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-06-06T03:13:54.0191400Z Jun 06 03:13:54 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-06-06T03:13:54.0192051Z Jun 06 03:13:54 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-06-06T03:13:54.0192883Z Jun 06 03:13:54 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-06-06T03:13:54.0194154Z Jun 06 03:13:54 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-06-06T03:13:54.0195096Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-06T03:13:54.0196226Z Jun 06 03:13:54 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-06T03:13:54.0197088Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-06T03:13:54.0198037Z Jun 06 03:13:54 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-06T03:13:54.0199186Z Jun 06 03:13:54 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-06T03:13:54.0200147Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-06T03:13:54.0200956Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-06T03:13:54.0201666Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-06T03:13:54.0202304Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-06T03:13:54.0202932Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-06T03:13:54.0203971Z Jun 06 03:13:54 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-06-06T03:13:54.0204588Z Jun 06 03:13:54 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-06-06T03:13:54.0205173Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-06T03:13:54.0205779Z Jun 06 03:13:54 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-06T03:13:54.0206368Z Jun 06 03:13:54 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-06T03:13:54.0206934Z Jun 06 03:13:54 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-06T03:13:54.0207699Z Jun 06 03:13:54 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-06T03:13:54.0208536Z Jun 06 03:13:54 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-06T03:13:54.0209241Z Jun 06 03:13:54 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-06T03:13:54.0210007Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-06T03:13:54.0210835Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-06T03:13:54.0211679Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-06T03:13:54.0212726Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-06T03:13:54.0213736Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-06T03:13:54.0214617Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-06T03:13:54.0215315Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-06T03:13:54.0216105Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-06T03:13:54.0216951Z Jun 06 03:13:54 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-06T03:13:54.0217761Z Jun 06 03:13:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-06-06T03:13:54.0218594Z Jun 06 03:13:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-06T03:13:54.0219420Z Jun 06 03:13:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-06-06T03:13:54.0220193Z Jun 06 03:13:54 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-06T03:13:54.0220916Z Jun 06 03:13:54 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-06T03:13:54.0221574Z Jun 06 03:13:54 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-06T03:13:54.0222244Z Jun 06 03:13:54 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-06T03:13:54.0223194Z Jun 06 03:13:54 Caused by: org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.
2022-06-06T03:13:54.0224066Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onLeave(StopWithSavepoint.java:125)
2022-06-06T03:13:54.0224882Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1171)
2022-06-06T03:13:54.0225711Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToFailing(AdaptiveScheduler.java:869)
2022-06-06T03:13:54.0226504Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.FailureResultUtil.restartOrFail(FailureResultUtil.java:36)
2022-06-06T03:13:54.0227303Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onFailure(StopWithSavepoint.java:151)
2022-06-06T03:13:54.0228166Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.updateTaskExecutionState(StateWithExecutionGraph.java:363)
2022-06-06T03:13:54.0229095Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$updateTaskExecutionState$4(AdaptiveScheduler.java:496)
2022-06-06T03:13:54.0229897Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.State.tryCall(State.java:137)
2022-06-06T03:13:54.0230665Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.updateTaskExecutionState(AdaptiveScheduler.java:493)
2022-06-06T03:13:54.0231486Z Jun 06 03:13:54 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-06-06T03:13:54.0232247Z Jun 06 03:13:54 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-06-06T03:13:54.0232891Z Jun 06 03:13:54 	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
2022-06-06T03:13:54.0233781Z Jun 06 03:13:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-06T03:13:54.0234471Z Jun 06 03:13:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-06T03:13:54.0235258Z Jun 06 03:13:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-06-06T03:13:54.0236097Z Jun 06 03:13:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-06T03:13:54.0236985Z Jun 06 03:13:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-06-06T03:13:54.0237732Z Jun 06 03:13:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-06-06T03:13:54.0238489Z Jun 06 03:13:54 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-06-06T03:13:54.0239247Z Jun 06 03:13:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-06-06T03:13:54.0239920Z Jun 06 03:13:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-06T03:13:54.0240535Z Jun 06 03:13:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-06T03:13:54.0241155Z Jun 06 03:13:54 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-06T03:13:54.0241773Z Jun 06 03:13:54 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-06T03:13:54.0242392Z Jun 06 03:13:54 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-06T03:13:54.0243033Z Jun 06 03:13:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-06T03:13:54.0243931Z Jun 06 03:13:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-06T03:13:54.0244595Z Jun 06 03:13:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-06T03:13:54.0245195Z Jun 06 03:13:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-06T03:13:54.0245760Z Jun 06 03:13:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-06T03:13:54.0246341Z Jun 06 03:13:54 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-06T03:13:54.0246964Z Jun 06 03:13:54 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-06T03:13:54.0247540Z Jun 06 03:13:54 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-06T03:13:54.0248125Z Jun 06 03:13:54 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-06T03:13:54.0248700Z Jun 06 03:13:54 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-06T03:13:54.0249246Z Jun 06 03:13:54 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-06T03:13:54.0249813Z Jun 06 03:13:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-06T03:13:54.0250476Z Jun 06 03:13:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-06T03:13:54.0251146Z Jun 06 03:13:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-06T03:13:54.0251814Z Jun 06 03:13:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-06T03:13:54.0252491Z Jun 06 03:13:54 Caused by: java.lang.RuntimeException: Expected RuntimeException after snapshot creation.
2022-06-06T03:13:54.0253379Z Jun 06 03:13:54 	at org.apache.flink.test.checkpointing.SavepointITCase$CancelFailingInfiniteTestSource.notifyCheckpointComplete(SavepointITCase.java:1447)
2022-06-06T03:13:54.0254629Z Jun 06 03:13:54 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:126)
2022-06-06T03:13:54.0255558Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104)
2022-06-06T03:13:54.0256448Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145)
2022-06-06T03:13:54.0257374Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:479)
2022-06-06T03:13:54.0258466Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:413)
2022-06-06T03:13:54.0259364Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1407)
2022-06-06T03:13:54.0260280Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$15(StreamTask.java:1348)
2022-06-06T03:13:54.0261141Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$18(StreamTask.java:1387)
2022-06-06T03:13:54.0262054Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
2022-06-06T03:13:54.0262913Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
2022-06-06T03:13:54.0263802Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
2022-06-06T03:13:54.0264717Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
2022-06-06T03:13:54.0265605Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
2022-06-06T03:13:54.0266440Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
2022-06-06T03:13:54.0267219Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:856)
2022-06-06T03:13:54.0267931Z Jun 06 03:13:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:781)
2022-06-06T03:13:54.0268623Z Jun 06 03:13:54 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-06-06T03:13:54.0269318Z Jun 06 03:13:54 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
2022-06-06T03:13:54.0269969Z Jun 06 03:13:54 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-06-06T03:13:54.0270580Z Jun 06 03:13:54 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-06-06T03:13:54.0271135Z Jun 06 03:13:54 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36316&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26977,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 15 13:45:49 UTC 2023,,,,,,,,,,"0|z12zag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 11:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","28/Jun/22 07:34;godfreyhe;[~masteryhx] could you help to resolve it?;;;","12/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","14/Jul/22 11:11;masteryhx;I see it's related to the pr of FLINK-26977.
So [~dmvk] may have more backgroud about it.
[~dmvk] could you take a look at it ?;;;","21/Jul/22 22:38;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","15/Mar/23 12:11;Wencong Liu;cc @[~dmvk] ;;;","15/Mar/23 12:14;dmvk;I think this should go to the other David :)

cc [~dwysakowicz];;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","15/Aug/23 13:45;masteryhx;Since the issue was not reproduced beyond one year, I just closed it.
Please reopen it if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,
PulsarUnorderedPartitionSplitReaderTest.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition failed with AssertionError,FLINK-27917,13448569,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,syhily,hxbks2ks,hxbks2ks,06/Jun/22 12:06,04/Jan/23 02:53,04/Jun/24 20:51,04/Jan/23 02:53,1.14.5,1.15.1,,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-06-06T06:34:46.7906026Z Jun 06 06:34:46 [ERROR] org.apache.flink.connector.pulsar.source.reader.split.PulsarUnorderedPartitionSplitReaderTest.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition(PulsarPartitionSplitReaderBase)[1]  Time elapsed: 9.774 s  <<< FAILURE!
2022-06-06T06:34:46.7919217Z Jun 06 06:34:46 java.lang.AssertionError: 
2022-06-06T06:34:46.7920918Z Jun 06 06:34:46 [We should fetch the expected size] 
2022-06-06T06:34:46.7921479Z Jun 06 06:34:46 Expected size: 20 but was: 3 in:
2022-06-06T06:34:46.7922019Z Jun 06 06:34:46 [PulsarMessage{id=58:0:0:0, value=ElpTDLGvKz, eventTime=0},
2022-06-06T06:34:46.7922757Z Jun 06 06:34:46     PulsarMessage{id=58:1:0:0, value=cDGEGcCZnP, eventTime=0},
2022-06-06T06:34:46.7924900Z Jun 06 06:34:46     PulsarMessage{id=58:2:0:0, value=rZmaCxrhZF, eventTime=0}]
2022-06-06T06:34:46.7926359Z Jun 06 06:34:46 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.fetchedMessages(PulsarPartitionSplitReaderTestBase.java:186)
2022-06-06T06:34:46.7928019Z Jun 06 06:34:46 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.fetchedMessages(PulsarPartitionSplitReaderTestBase.java:156)
2022-06-06T06:34:46.7930207Z Jun 06 06:34:46 	at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderTestBase.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition(PulsarPartitionSplitReaderTestBase.java:247)
2022-06-06T06:34:46.7931943Z Jun 06 06:34:46 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-06T06:34:46.7933282Z Jun 06 06:34:46 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-06T06:34:46.7934885Z Jun 06 06:34:46 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-06T06:34:46.7936182Z Jun 06 06:34:46 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-06-06T06:34:46.7937301Z Jun 06 06:34:46 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-06-06T06:34:46.7938744Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-06-06T06:34:46.7939650Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-06-06T06:34:46.7940516Z Jun 06 06:34:46 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-06-06T06:34:46.7941737Z Jun 06 06:34:46 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-06-06T06:34:46.7942588Z Jun 06 06:34:46 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-06-06T06:34:46.7943874Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-06-06T06:34:46.7945291Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-06-06T06:34:46.7946812Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-06-06T06:34:46.7948852Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-06-06T06:34:46.7950462Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-06-06T06:34:46.7951929Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-06-06T06:34:46.7953814Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-06-06T06:34:46.7955421Z Jun 06 06:34:46 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-06-06T06:34:46.7956366Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-06-06T06:34:46.7957864Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.7959784Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-06-06T06:34:46.7961360Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-06-06T06:34:46.7962379Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-06-06T06:34:46.7964049Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-06T06:34:46.7965451Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.7966938Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-06T06:34:46.7968376Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-06T06:34:46.7969244Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-06T06:34:46.7970101Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.7970936Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-06T06:34:46.7971747Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-06T06:34:46.7972706Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-06T06:34:46.7974307Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-06-06T06:34:46.7975915Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-06-06T06:34:46.7977376Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-06-06T06:34:46.7978867Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-06-06T06:34:46.7980594Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-06-06T06:34:46.7981560Z Jun 06 06:34:46 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-06-06T06:34:46.7982574Z Jun 06 06:34:46 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
2022-06-06T06:34:46.7983541Z Jun 06 06:34:46 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
2022-06-06T06:34:46.7984586Z Jun 06 06:34:46 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
2022-06-06T06:34:46.7985725Z Jun 06 06:34:46 	at java.base/java.util.stream.Streams$StreamBuilderImpl.forEachRemaining(Streams.java:411)
2022-06-06T06:34:46.7986875Z Jun 06 06:34:46 	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658)
2022-06-06T06:34:46.7988041Z Jun 06 06:34:46 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:274)
2022-06-06T06:34:46.7988789Z Jun 06 06:34:46 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
2022-06-06T06:34:46.7989637Z Jun 06 06:34:46 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
2022-06-06T06:34:46.7990652Z Jun 06 06:34:46 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
2022-06-06T06:34:46.7991402Z Jun 06 06:34:46 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-06-06T06:34:46.7992140Z Jun 06 06:34:46 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-06-06T06:34:46.7992882Z Jun 06 06:34:46 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-06-06T06:34:46.7993957Z Jun 06 06:34:46 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
2022-06-06T06:34:46.7995208Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-06-06T06:34:46.7996080Z Jun 06 06:34:46 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-06-06T06:34:46.7996945Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-06-06T06:34:46.7997809Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.7998733Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-06T06:34:46.7999552Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-06T06:34:46.8000332Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-06T06:34:46.8001415Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.8002331Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-06T06:34:46.8003215Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-06T06:34:46.8004405Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-06T06:34:46.8006351Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-06-06T06:34:46.8007998Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-06-06T06:34:46.8009591Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-06T06:34:46.8010934Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.8012417Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-06T06:34:46.8013427Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-06T06:34:46.8014793Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-06T06:34:46.8015651Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.8016657Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-06T06:34:46.8017452Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-06T06:34:46.8018479Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-06T06:34:46.8020026Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-06-06T06:34:46.8021190Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-06-06T06:34:46.8022045Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.8022897Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-06-06T06:34:46.8023935Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-06-06T06:34:46.8025212Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-06-06T06:34:46.8026078Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-06-06T06:34:46.8026902Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-06-06T06:34:46.8027676Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-06-06T06:34:46.8028669Z Jun 06 06:34:46 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-06-06T06:34:46.8029819Z Jun 06 06:34:46 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-06-06T06:34:46.8030512Z Jun 06 06:34:46 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2022-06-06T06:34:46.8031508Z Jun 06 06:34:46 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2022-06-06T06:34:46.8032648Z Jun 06 06:34:46 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2022-06-06T06:34:46.8034238Z Jun 06 06:34:46 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2022-06-06T06:34:46.8035284Z Jun 06 06:34:46 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-06-06T06:34:46.8035834Z Jun 06 06:34:46 
2022-06-06T06:35:51.0359908Z Jun 06 06:35:51 [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 144.364 s - in org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReaderTest
2022-06-06T06:35:51.5089719Z Jun 06 06:35:51 [INFO] 
2022-06-06T06:35:51.5091416Z Jun 06 06:35:51 [INFO] Results:
2022-06-06T06:35:51.5092235Z Jun 06 06:35:51 [INFO] 
2022-06-06T06:35:51.5092685Z Jun 06 06:35:51 [ERROR] Failures: 
2022-06-06T06:35:51.5094990Z Jun 06 06:35:51 [ERROR]   PulsarUnorderedPartitionSplitReaderTest>PulsarPartitionSplitReaderTestBase.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition:247->PulsarPartitionSplitReaderTestBase.fetchedMessages:156->PulsarPartitionSplitReaderTestBase.fetchedMessages:186 [We should fetch the expected size] 
2022-06-06T06:35:51.5096131Z Jun 06 06:35:51 Expected size: 20 but was: 3 in:
2022-06-06T06:35:51.5096647Z Jun 06 06:35:51 [PulsarMessage{id=58:0:0:0, value=ElpTDLGvKz, eventTime=0},
2022-06-06T06:35:51.5097260Z Jun 06 06:35:51     PulsarMessage{id=58:1:0:0, value=cDGEGcCZnP, eventTime=0},
2022-06-06T06:35:51.5098057Z Jun 06 06:35:51     PulsarMessage{id=58:2:0:0, value=rZmaCxrhZF, eventTime=0}]
2022-06-06T06:35:51.5098512Z Jun 06 06:35:51 [INFO] 
2022-06-06T06:35:51.5098942Z Jun 06 06:35:51 [ERROR] Tests run: 133, Failures: 1, Errors: 0, Skipped: 0
2022-06-06T06:35:51.5099370Z 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36316&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 04 02:53:40 UTC 2023,,,,,,,,,,"0|z12za8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 12:07;hxbks2ks;cc [~affe];;;","29/Jun/22 17:14;rmetzger;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37363&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","13/Aug/22 18:25;syhily;This is the same issue caused by https://github.com/apache/pulsar/pull/16171. I will push a hotfix PR for this test issue.;;;","14/Aug/22 14:32;syhily;[~tison] Can you assign this issue to me? And the PR is ready for this issue.;;;","16/Aug/22 23:04;tison;master via f7af9f6462be9cac80d6c433a4abdcc8db8c84aa

[~syhily] since affected versions contains only master, I think we don't have to cherry-pick the fix?;;;","23/Aug/22 01:55;tison;1.14 via 6ce74dd8e671688b79770dacf88fd31924cc2e33
1.15 via a526391a3c6ea6dc88fd408c5a17aeb573811b5f;;;","05/Sep/22 11:12;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40665&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203] It seems to have happened again.
{code:java}
2022-09-05T05:19:03.9470055Z Sep 05 05:19:03 [ERROR] Failures: 
2022-09-05T05:19:03.9472058Z Sep 05 05:19:03 [ERROR]   PulsarOrderedPartitionSplitReaderTest>PulsarPartitionSplitReaderTestBase.consumeMessageCreatedBeforeHandleSplitsChangesAndResetToEarliestPosition:260->PulsarPartitionSplitReaderTestBase.fetchedMessages:169->PulsarPartitionSplitReaderTestBase.fetchedMessages:199 [We should fetch the expected size] 
2022-09-05T05:19:03.9473157Z Sep 05 05:19:03 Expected size: 20 but was: 23 in:
2022-09-05T05:19:03.9473660Z Sep 05 05:19:03 [PulsarMessage{id=148:0:0, value=xtvCoENDyD, eventTime=0},
2022-09-05T05:19:03.9474343Z Sep 05 05:19:03     PulsarMessage{id=148:1:0, value=LsfmAdOvPi, eventTime=0},
2022-09-05T05:19:03.9474926Z Sep 05 05:19:03     PulsarMessage{id=148:2:0, value=SADrPXpnsp, eventTime=0},
2022-09-05T05:19:03.9475652Z Sep 05 05:19:03     PulsarMessage{id=148:0:0, value=xtvCoENDyD, eventTime=0},
2022-09-05T05:19:03.9476632Z Sep 05 05:19:03     PulsarMessage{id=148:1:0, value=LsfmAdOvPi, eventTime=0},
2022-09-05T05:19:03.9477616Z Sep 05 05:19:03     PulsarMessage{id=148:2:0, value=SADrPXpnsp, eventTime=0},
2022-09-05T05:19:03.9478396Z Sep 05 05:19:03     PulsarMessage{id=148:3:0, value=BsEGjUZSJN, eventTime=0},
2022-09-05T05:19:03.9478942Z Sep 05 05:19:03     PulsarMessage{id=148:4:0, value=wvteCkUkjX, eventTime=0},
2022-09-05T05:19:03.9479500Z Sep 05 05:19:03     PulsarMessage{id=148:5:0, value=HUEpGPmjYy, eventTime=0},
2022-09-05T05:19:03.9480060Z Sep 05 05:19:03     PulsarMessage{id=148:6:0, value=kndeIFjLLK, eventTime=0},
2022-09-05T05:19:03.9480613Z Sep 05 05:19:03     PulsarMessage{id=148:7:0, value=gOmuNzCqbL, eventTime=0},
2022-09-05T05:19:03.9481348Z Sep 05 05:19:03     PulsarMessage{id=148:8:0, value=fvtNHyyqqj, eventTime=0},
2022-09-05T05:19:03.9482091Z Sep 05 05:19:03     PulsarMessage{id=148:9:0, value=iKrmISirqy, eventTime=0},
2022-09-05T05:19:03.9482628Z Sep 05 05:19:03     PulsarMessage{id=148:10:0, value=lQUCdQizRw, eventTime=0},
2022-09-05T05:19:03.9483196Z Sep 05 05:19:03     PulsarMessage{id=148:11:0, value=RkGRkfTAcS, eventTime=0},
2022-09-05T05:19:03.9483808Z Sep 05 05:19:03     PulsarMessage{id=148:12:0, value=mBcAGPkmpY, eventTime=0},
2022-09-05T05:19:03.9484447Z Sep 05 05:19:03     PulsarMessage{id=148:13:0, value=AMUWkVFpwU, eventTime=0},
2022-09-05T05:19:03.9485002Z Sep 05 05:19:03     PulsarMessage{id=148:14:0, value=zCtbxzylrl, eventTime=0},
2022-09-05T05:19:03.9485565Z Sep 05 05:19:03     PulsarMessage{id=148:15:0, value=OQNrSEYkFo, eventTime=0},
2022-09-05T05:19:03.9486121Z Sep 05 05:19:03     PulsarMessage{id=148:16:0, value=oLuIyaJZrI, eventTime=0},
2022-09-05T05:19:03.9486660Z Sep 05 05:19:03     PulsarMessage{id=148:17:0, value=PfbbxVuQKh, eventTime=0},
2022-09-05T05:19:03.9487384Z Sep 05 05:19:03     PulsarMessage{id=148:18:0, value=KeIgSeYDxM, eventTime=0},
2022-09-05T05:19:03.9487953Z Sep 05 05:19:03     PulsarMessage{id=148:19:0, value=KZkeATFOln, eventTime=0}]
2022-09-05T05:19:03.9488393Z Sep 05 05:19:03 [INFO] 
2022-09-05T05:19:03.9488819Z Sep 05 05:19:03 [ERROR] Tests run: 142, Failures: 1, Errors: 0, Skipped: 0 {code};;;","05/Sep/22 14:54;syhily;[~hxb] I'll check this.;;;","05/Sep/22 15:09;syhily;It seems like a bug on Pulsar broker: https://github.com/apache/pulsar/pull/17237. We can keep it open until we bumped the pulsar-client to 2.10.3

[~hxb] Can you help me reopen this issue?;;;","07/Sep/22 10:57;hxb;[~syhily] Thanks a lot for the investigation.;;;","28/Nov/22 09:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43513&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=28726;;;","01/Dec/22 09:16;martijnvisser;1.15: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43637&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347&l=28411;;;","04/Jan/23 02:53;tison;After FLINK-30413 dropped the related support, this test was also dropped. Invalid now.;;;",,,,,,,,,,,,,,,,,,,
HybridSourceReaderTest.testReader failed with AssertionError,FLINK-27916,13448565,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hxbks2ks,hxbks2ks,06/Jun/22 11:47,01/Mar/24 10:24,04/Jun/24 20:51,,1.16.0,1.19.0,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,auto-deprioritized-critical,test-stability,,,"
{code:java}
2022-06-05T07:47:33.3332158Z Jun 05 07:47:33 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.03 s <<< FAILURE! - in org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest
2022-06-05T07:47:33.3334366Z Jun 05 07:47:33 [ERROR] org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.testReader  Time elapsed: 0.108 s  <<< FAILURE!
2022-06-05T07:47:33.3335385Z Jun 05 07:47:33 java.lang.AssertionError: 
2022-06-05T07:47:33.3336049Z Jun 05 07:47:33 
2022-06-05T07:47:33.3336682Z Jun 05 07:47:33 Expected size: 1 but was: 0 in:
2022-06-05T07:47:33.3337316Z Jun 05 07:47:33 []
2022-06-05T07:47:33.3338437Z Jun 05 07:47:33 	at org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.assertAndClearSourceReaderFinishedEvent(HybridSourceReaderTest.java:199)
2022-06-05T07:47:33.3340082Z Jun 05 07:47:33 	at org.apache.flink.connector.base.source.hybrid.HybridSourceReaderTest.testReader(HybridSourceReaderTest.java:96)
2022-06-05T07:47:33.3341373Z Jun 05 07:47:33 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-05T07:47:33.3342540Z Jun 05 07:47:33 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-05T07:47:33.3344124Z Jun 05 07:47:33 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-05T07:47:33.3345283Z Jun 05 07:47:33 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-06-05T07:47:33.3346804Z Jun 05 07:47:33 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-05T07:47:33.3348218Z Jun 05 07:47:33 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-05T07:47:33.3349495Z Jun 05 07:47:33 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-05T07:47:33.3350779Z Jun 05 07:47:33 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-06-05T07:47:33.3351956Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-05T07:47:33.3357032Z Jun 05 07:47:33 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-06-05T07:47:33.3358633Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-06-05T07:47:33.3360003Z Jun 05 07:47:33 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-06-05T07:47:33.3361924Z Jun 05 07:47:33 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-06-05T07:47:33.3363427Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-06-05T07:47:33.3364793Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-06-05T07:47:33.3365619Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-06-05T07:47:33.3366254Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-06-05T07:47:33.3366939Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-06-05T07:47:33.3367556Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-05T07:47:33.3368268Z Jun 05 07:47:33 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-05T07:47:33.3369166Z Jun 05 07:47:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-05T07:47:33.3369993Z Jun 05 07:47:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-05T07:47:33.3371021Z Jun 05 07:47:33 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-05T07:47:33.3372128Z Jun 05 07:47:33 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-05T07:47:33.3373622Z Jun 05 07:47:33 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-05T07:47:33.3374886Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-05T07:47:33.3376377Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-05T07:47:33.3377847Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-05T07:47:33.3379349Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-05T07:47:33.3380627Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-05T07:47:33.3381508Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-05T07:47:33.3382602Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-05T07:47:33.3383792Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-05T07:47:33.3384633Z Jun 05 07:47:33 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-05T07:47:33.3385731Z Jun 05 07:47:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-06-05T07:47:33.3386886Z Jun 05 07:47:33 	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
2022-06-05T07:47:33.3388010Z Jun 05 07:47:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-06-05T07:47:33.3389382Z Jun 05 07:47:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-05T07:47:33.3390727Z Jun 05 07:47:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-06-05T07:47:33.3392040Z Jun 05 07:47:33 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-05T07:47:33.3393642Z Jun 05 07:47:33 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-05T07:47:33.3394810Z Jun 05 07:47:33 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-05T07:47:33.3395907Z Jun 05 07:47:33 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36309&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/22 00:51;binh;Screen Shot 2022-07-21 at 5.51.40 PM.png;https://issues.apache.org/jira/secure/attachment/13047101/Screen+Shot+2022-07-21+at+5.51.40+PM.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 01 10:24:36 UTC 2024,,,,,,,,,,"0|z12z9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 07:08;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37382&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=8045;;;","05/Jul/22 08:13;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8398;;;","11/Jul/22 02:31;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf;;;","11/Jul/22 06:29;martijnvisser;[~thw] Could you have a look? ;;;","21/Jul/22 23:26;binh;[~martijnvisser] I can help look into this, as I was working with the hybrid source code recently;;;","22/Jul/22 00:53;binh;[~martijnvisser] I've run the test several times locally and they all succeeded. I also tried to use debugger to inspect the logic flow, they all look correct to me. So I think this's really rare to happen.

I think we can close this?

 

 ;;;","09/Aug/22 01:45;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39636&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203 new instance;;;","23/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","31/Aug/22 22:39;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","10/Sep/22 07:45;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40850&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8072;;;","19/Sep/22 02:27;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41106&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8633;;;","01/Nov/22 09:21;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42682&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=8906;;;","05/Nov/22 05:40;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42837&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8251;;;","11/Nov/22 09:12;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43043&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8530;;;","21/Nov/22 09:54;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43252&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a;;;","29/Nov/22 08:37;mapohl;[~thomasWeise]/[~binh] may you have a look at it considering that it happened a few times recently again?;;;","01/Dec/22 09:15;martijnvisser;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43638&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=8564;;;","09/Dec/22 16:27;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43841&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8194;;;","16/Dec/22 08:23;martijnvisser;release-1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43956&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=8509;;;","17/Jan/23 15:43;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44929&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=8518;;;","13/Feb/23 10:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46035&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8553;;;","06/Mar/23 09:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46766&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=8519;;;","15/Jun/23 06:46;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49918&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=8659;;;","29/Jun/23 11:28;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50532&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8609;;;","28/Aug/23 09:04;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52693&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=8255;;;","30/Oct/23 08:00;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54144&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=4242;;;","16/Nov/23 08:27;mapohl;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54603&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=8873;;;","08/Jan/24 00:11;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55952&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=7165;;;","21/Feb/24 16:14;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57700&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=7429;;;","01/Mar/24 10:24;mapohl;master (1.20): https://github.com/apache/flink/actions/runs/8105496391/job/22154148580#step:10:10457;;;",,
WindowJoinITCase.testInnerJoinOnWTFWithOffset failed with InterruptedException,FLINK-27915,13448564,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hxbks2ks,hxbks2ks,06/Jun/22 11:34,08/Jun/22 05:32,04/Jun/24 20:51,08/Jun/22 05:32,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,"
{code:java}
2022-06-04T03:55:42.6403835Z Jun 04 03:55:42 [ERROR] Tests run: 80, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 90.085 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.WindowJoinITCase
2022-06-04T03:55:42.6405383Z Jun 04 03:55:42 [ERROR] WindowJoinITCase.testInnerJoinOnWTFWithOffset  Time elapsed: 1.228 s  <<< ERROR!
2022-06-04T03:55:42.6406676Z Jun 04 03:55:42 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-04T03:55:42.6407398Z Jun 04 03:55:42 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-06-04T03:55:42.6433580Z Jun 04 03:55:42 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-06-04T03:55:42.6435011Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-06-04T03:55:42.6436197Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-06-04T03:55:42.6437458Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-04T03:55:42.6439108Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-04T03:55:42.6440559Z Jun 04 03:55:42 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-06-04T03:55:42.6442232Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-04T03:55:42.6443814Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-04T03:55:42.6445174Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-04T03:55:42.6446427Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-04T03:55:42.6447547Z Jun 04 03:55:42 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-06-04T03:55:42.6448854Z Jun 04 03:55:42 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-06-04T03:55:42.6450237Z Jun 04 03:55:42 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-06-04T03:55:42.6451912Z Jun 04 03:55:42 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-06-04T03:55:42.6453755Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-04T03:55:42.6454979Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-04T03:55:42.6456447Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-04T03:55:42.6457742Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-04T03:55:42.6459136Z Jun 04 03:55:42 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-06-04T03:55:42.6460363Z Jun 04 03:55:42 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-06-04T03:55:42.6461398Z Jun 04 03:55:42 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-06-04T03:55:42.6462509Z Jun 04 03:55:42 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-06-04T03:55:42.6464606Z Jun 04 03:55:42 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-06-04T03:55:42.6465889Z Jun 04 03:55:42 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-04T03:55:42.6467232Z Jun 04 03:55:42 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-06-04T03:55:42.6468402Z Jun 04 03:55:42 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-06-04T03:55:42.6469590Z Jun 04 03:55:42 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-06-04T03:55:42.6471024Z Jun 04 03:55:42 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-06-04T03:55:42.6472343Z Jun 04 03:55:42 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-06-04T03:55:42.6473784Z Jun 04 03:55:42 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-06-04T03:55:42.6475031Z Jun 04 03:55:42 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-06-04T03:55:42.6476470Z Jun 04 03:55:42 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-06-04T03:55:42.6477643Z Jun 04 03:55:42 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-06-04T03:55:42.6478657Z Jun 04 03:55:42 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-06-04T03:55:42.6479701Z Jun 04 03:55:42 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-06-04T03:55:42.6481049Z Jun 04 03:55:42 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-04T03:55:42.6482125Z Jun 04 03:55:42 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-06-04T03:55:42.6483906Z Jun 04 03:55:42 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-06-04T03:55:42.6485102Z Jun 04 03:55:42 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-06-04T03:55:42.6486234Z Jun 04 03:55:42 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-06-04T03:55:42.6487304Z Jun 04 03:55:42 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-06-04T03:55:42.6488468Z Jun 04 03:55:42 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-06-04T03:55:42.6489732Z Jun 04 03:55:42 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-06-04T03:55:42.6491079Z Jun 04 03:55:42 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-04T03:55:42.6492305Z Jun 04 03:55:42 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-04T03:55:42.6493625Z Jun 04 03:55:42 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-04T03:55:42.6494749Z Jun 04 03:55:42 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-04T03:55:42.6495665Z Jun 04 03:55:42 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2022-06-04T03:55:42.6496537Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.howToHandleFailure(AdaptiveScheduler.java:1098)
2022-06-04T03:55:42.6497704Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.adaptive.Executing.onFailure(Executing.java:93)
2022-06-04T03:55:42.6499063Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.updateTaskExecutionState(StateWithExecutionGraph.java:363)
2022-06-04T03:55:42.6500566Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$updateTaskExecutionState$4(AdaptiveScheduler.java:496)
2022-06-04T03:55:42.6501955Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.adaptive.State.tryCall(State.java:137)
2022-06-04T03:55:42.6503359Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.updateTaskExecutionState(AdaptiveScheduler.java:493)
2022-06-04T03:55:42.6505053Z Jun 04 03:55:42 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-06-04T03:55:42.6506468Z Jun 04 03:55:42 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-06-04T03:55:42.6507609Z Jun 04 03:55:42 	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
2022-06-04T03:55:42.6508833Z Jun 04 03:55:42 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-04T03:55:42.6510061Z Jun 04 03:55:42 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-04T03:55:42.6511278Z Jun 04 03:55:42 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-06-04T03:55:42.6512823Z Jun 04 03:55:42 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-04T03:55:42.6514580Z Jun 04 03:55:42 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-06-04T03:55:42.6516045Z Jun 04 03:55:42 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-06-04T03:55:42.6517482Z Jun 04 03:55:42 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-06-04T03:55:42.6518724Z Jun 04 03:55:42 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-06-04T03:55:42.6520127Z Jun 04 03:55:42 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-04T03:55:42.6521314Z Jun 04 03:55:42 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-04T03:55:42.6522568Z Jun 04 03:55:42 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-04T03:55:42.6524042Z Jun 04 03:55:42 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-04T03:55:42.6525250Z Jun 04 03:55:42 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-04T03:55:42.6526297Z Jun 04 03:55:42 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-04T03:55:42.6527545Z Jun 04 03:55:42 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-04T03:55:42.6528604Z Jun 04 03:55:42 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-04T03:55:42.6529577Z Jun 04 03:55:42 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-04T03:55:42.6530600Z Jun 04 03:55:42 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-04T03:55:42.6531636Z Jun 04 03:55:42 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-04T03:55:42.6532753Z Jun 04 03:55:42 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-04T03:55:42.6534065Z Jun 04 03:55:42 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-04T03:55:42.6535006Z Jun 04 03:55:42 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-04T03:55:42.6536025Z Jun 04 03:55:42 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-04T03:55:42.6537041Z Jun 04 03:55:42 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-04T03:55:42.6537841Z Jun 04 03:55:42 	... 4 more
2022-06-04T03:55:42.6538579Z Jun 04 03:55:42 Caused by: java.io.IOException: java.lang.InterruptedException
2022-06-04T03:55:42.6539865Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
2022-06-04T03:55:42.6541437Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
2022-06-04T03:55:42.6542346Z Jun 04 03:55:42 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
2022-06-04T03:55:42.6543391Z Jun 04 03:55:42 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
2022-06-04T03:55:42.6544264Z Jun 04 03:55:42 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-04T03:55:42.6544944Z Jun 04 03:55:42 	at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
2022-06-04T03:55:42.6545716Z Jun 04 03:55:42 	at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
2022-06-04T03:55:42.6546474Z Jun 04 03:55:42 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
2022-06-04T03:55:42.6547185Z Jun 04 03:55:42 	at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
2022-06-04T03:55:42.6547911Z Jun 04 03:55:42 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-06-04T03:55:42.6548623Z Jun 04 03:55:42 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
2022-06-04T03:55:42.6549274Z Jun 04 03:55:42 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-06-04T03:55:42.6549907Z Jun 04 03:55:42 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-06-04T03:55:42.6550468Z Jun 04 03:55:42 	at java.lang.Thread.run(Thread.java:748)
2022-06-04T03:55:42.6550943Z Jun 04 03:55:42 Caused by: java.lang.InterruptedException
2022-06-04T03:55:42.6551520Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
2022-06-04T03:55:42.6552200Z Jun 04 03:55:42 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-04T03:55:42.6553308Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildFutureWriteRequest$4(ChannelStateWriteRequest.java:113)
2022-06-04T03:55:42.6554458Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.cancel(ChannelStateWriteRequest.java:253)
2022-06-04T03:55:42.6555409Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.lambda$null$1(ChannelStateWriteRequestExecutorImpl.java:117)
2022-06-04T03:55:42.6556225Z Jun 04 03:55:42 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-04T03:55:42.6556817Z Jun 04 03:55:42 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-06-04T03:55:42.6557629Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.cleanupRequests(ChannelStateWriteRequestExecutorImpl.java:115)
2022-06-04T03:55:42.6558443Z Jun 04 03:55:42 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-04T03:55:42.6559148Z Jun 04 03:55:42 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-06-04T03:55:42.6559727Z Jun 04 03:55:42 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
2022-06-04T03:55:42.6560480Z Jun 04 03:55:42 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:80)
2022-06-04T03:55:42.6561119Z Jun 04 03:55:42 	... 1 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36302&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-06 11:34:13.0,,,,,,,,,,"0|z12z94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate JOSDK metrics with Flink Metrics reporter,FLINK-27914,13448561,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gyfora,gyfora,gyfora,06/Jun/22 10:37,18/Jul/22 16:22,04/Jun/24 20:51,08/Jul/22 12:41,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,Starter,,,"The Java Operator SDK comes with an internal metric interface that could be implemented to forward metrics/measurements to the Flink metric registries. 

We should investigate and implement this if possible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 08 12:41:37 UTC 2022,,,,,,,,,,"0|z12z8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 08:49;nicholasjiang;[~gyfora], I'm working for the implementation of the 'io.javaoperatorsdk.operator.api.monitoring.Metrics' interface and the integration with the operator. Could you please assign this ticket to me?;;;","08/Jul/22 12:41;gyfora;merged to main 308f8add74165febaa2f80c61a2a9d242d8b0629;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove savepointHistoryMaxCount and savepointHistoryMaxAge from FlinkOperatorConfiguration,FLINK-27913,13448559,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,nicholasjiang,gyfora,gyfora,06/Jun/22 10:33,10/Jun/22 06:47,04/Jun/24 20:51,10/Jun/22 06:47,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,Starter,,,"Currently savepointHistoryMaxCount and savepointHistoryMaxAge is part of the FlinkOperatorConfiguration class which means that users cannot override this from their user deployment.

We should remove it and get it directly from the effective config. 

We should however introduce a max allowed value for these configurations that is configured on the FlinkOperatorConfiguration level so that users cannot infinitely grow the status size and cause problems for the k8s api.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 06:47:31 UTC 2022,,,,,,,,,,"0|z12z80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 11:58;nicholasjiang;[~gyfora], I will work for this ticket. Please help to assign this ticket to me. Thanks.;;;","10/Jun/22 06:47;gyfora;merged to main 9d1be9cadd2b292b2a9ea37852f1b3cda65d1422;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve operator config names,FLINK-27912,13448558,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ConradJam,gyfora,gyfora,06/Jun/22 10:30,23/Jun/22 17:04,04/Jun/24 20:51,23/Jun/22 17:04,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We should remove observer/reconciler and other internal concepts from most config names especially the ones that are mainly user facing.

We should also unify/simplify different config names to make this maintainable in the future.
||From||To||
|kubernetes.operator.reconciler.flink.cancel.job.timeout|kubernetes.operator.flink.client.cancel.timeout|
|kubernetes.operator.reconciler.flink.cluster.shutdown.timeout|kubernetes.operator.resource.cleanup.timeout|
|kubernetes.operator.observer.flink.client.timeout|kubernetes.operator.flink.client.timeout|
|kubernetes.operator.reconciler.jm-deployment-recovery.enabled|kubernetes.operator.jm-deployment-recovery.enabled|
|kubernetes.operator.reconciler.reschedule.interval|kubernetes.operator.reconcile.interval|
|kubernetes.operator.reconciler.max.parallelism|kubernetes.operator.reconcile.parallelism|

 

The old config key should be kept as deprecatedkey to maintain compatibility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 23 17:04:29 UTC 2022,,,,,,,,,,"0|z12z7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 11:20;ConradJam;Hi [~gyfora] , I want to get this ticket, can you assign to me :P ?;;;","23/Jun/22 17:04;gyfora;merged to main 794df5844b158aab0cbe8ef6d9a44277aba639c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set HDFS LEASE_TIMEOUT as user-configurable,FLINK-27911,13448555,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zoucao,zoucao,06/Jun/22 10:20,07/Jun/22 08:50,04/Jun/24 20:51,,1.12.2,1.14.4,,,,,,,,,,,,,,,,,FileSystems,,,,,0,,,,,"The HDFS LEASE_TIMEOUT in *HadoopRecoverableFsDataOutputStream* is set to 100_000 millis, In some cases, 100 seconds is not enough. In our company, when using StreamingFileSink to write records to HDFS and the parallelism is set to 512 or larger, the following exceptions occur frequently. I think we can update the parameter LEASE_TIMEOUT as user-configurable.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/22 10:19;zoucao;exceptions;https://issues.apache.org/jira/secure/attachment/13044685/exceptions",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-06 10:20:16.0,,,,,,,,,,"0|z12z74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSink not enforcing rolling policy if started from scratch,FLINK-27910,13448551,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pltbkd,gaoyunhaii,gaoyunhaii,06/Jun/22 09:53,17/Jun/22 13:08,04/Jun/24 20:51,08/Jun/22 07:53,1.15.0,1.16.0,,,,,,,,,,,,,1.15.1,1.16.0,,,Connectors / FileSystem,,,,,1,pull-request-available,,,,"The current FileWriter only register the timer in initializeState, which is now only called on restoring. Thus if the job is started from scratch, the timer would fail to be registered and cause the rolling policy not work. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 07:53:13 UTC 2022,,,,,,,,,,"0|z12z68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 09:54;gaoyunhaii;Also cc [~pltbkd] ;;;","06/Jun/22 17:00;danderson;This is super confusing, and there's no reasonable workaround – so I've bumped up the priority to Critical.;;;","07/Jun/22 06:27;pltbkd;This is a mistake while migrating the FileSink to the new sink API. In the new sink API createWriter and restoreWriter have been separated into two methods, while originally creating a writer is by calling restoreWriter with an empty state collection. We mistook the the meaning of createWriter and only created a writer in it. 
A PR has been provided to fix this bug, which changes the createWriter as the original behavior.;;;","08/Jun/22 07:53;gaoyunhaii;Fix on master via d036c23c0e5c079eaafef250a5a14b7f3eead8f1

Fix on release-1.15 via a9905e2a14b54a10c429af8b75f414c8ac3b7638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add document and tests for hybrid shuffle mode,FLINK-27909,13448527,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:41,05/Aug/22 01:59,04/Jun/24 20:51,05/Aug/22 01:59,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 01:59:11 UTC 2022,,,,,,,,,,"0|z12z0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 01:59;xtsong;master (1.16): 004d31ae208d206702d7772e0319450d7d978900;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce HsResultPartition and HsSubpartitionView,FLINK-27908,13448526,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:41,05/Aug/22 01:56,04/Jun/24 20:51,05/Aug/22 01:56,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,"Introduce HsResultPartition and HsSubpartitionView to supports record write and read. The HsResultPartition will append data to HsMemoryDataManager and spill data to disk according to HsSpillingStrategy. Consumer task will register HsSubpartitionView to upstream, poll data from memory or disk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 01:56:17 UTC 2022,,,,,,,,,,"0|z12z0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 01:57;xtsong;master (1.16): 4a2f3a15903ca365c14368b34b30a6234a51aa5e;;;","03/Aug/22 08:53;Weijie Guo; I accidentally lost several changes when rebase the code. Reopen this ticket to fix this.;;;","05/Aug/22 01:56;xtsong;Fixed in master (1.16) 2dd51f644da14a88914cc7aeeb021f824a28ee48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement disk read and write logic for hybrid shuffle,FLINK-27907,13448525,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:40,08/Jul/22 08:29,04/Jun/24 20:51,08/Jul/22 08:29,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,"Implement disk read and write logic for hybrid shuffle. In order to access the disk as sequentially as possible, Introduce HsMemoryDataSpiller, HsResultPartitionReadScheduler and HsSubpartitionFileReader.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 08 08:29:36 UTC 2022,,,,,,,,,,"0|z12z0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jul/22 08:29;xtsong;master (1.16): 53d95af00a4a69f05cad0c849641ca22f5f7579a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce HsDataIndex,FLINK-27906,13448524,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:40,30/Jun/22 09:03,04/Jun/24 20:51,30/Jun/22 09:03,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,Introduce HsDataIndex to provide index management for hybrid shuffle mode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 30 09:03:10 UTC 2022,,,,,,,,,,"0|z12z08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 09:03;xtsong;master (1.16): cb9718c310e8eea93ae69300ae1e526fb78b3cad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce HsSpillingStrategy and Implement SelectiveSpillingStrategy、FullSpillingStrategy,FLINK-27905,13448523,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:39,18/Jul/22 08:32,04/Jun/24 20:51,18/Jul/22 08:32,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,Introduce {{HsSpillingStrategy}} and Implement {{HsSelectiveSpillingStrategy}} to support make a spilling decision for hybrid shuffle.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 08:32:32 UTC 2022,,,,,,,,,,"0|z12z00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 08:32;xtsong;master (1.16): 06ef00b3efc1fb015f3afbb7349be5e6266306dd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce HsMemoryDataManager,FLINK-27904,13448522,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:38,26/Jul/22 01:50,04/Jun/24 20:51,26/Jul/22 01:50,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,Introduce HsDataBuffer to manage memory data of hybrid shuffle mode.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 01:50:37 UTC 2022,,,,,,,,,,"0|z12yzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jul/22 07:43;Weijie Guo;Through offline communication with [~aitozi] , because he has been busy recently and has no time to do this, I will take over and continue to do it, many thanks for his interest and participation.

[~xtsong], Could you please assign this ticket to me, thanks a lot!;;;","26/Jul/22 01:50;xtsong;master (1.16): 0d466603a981acfee8d7136220461f63849fbe0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce and support HYBRID resultPartitionType,FLINK-27903,13448521,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:37,14/Jun/22 06:02,04/Jun/24 20:51,13/Jun/22 06:23,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 06:23:55 UTC 2022,,,,,,,,,,"0|z12yzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 06:23;xtsong;master: aa693b5076f5461bb193946cd75cbe6d8e101a90;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor ResultPartitionType to decouple scheduling and partition release logic,FLINK-27902,13448520,13447855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,06/Jun/22 07:36,08/Jun/22 07:57,04/Jun/24 20:51,08/Jun/22 07:57,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Network,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 07:57:00 UTC 2022,,,,,,,,,,"0|z12yzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 07:57;xtsong;master: 98d4cd659bc33313de3888e5e1e29bdf4b182f2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support TableEnvironment.create(configuration) in PyFlink,FLINK-27901,13448517,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,06/Jun/22 07:17,08/Jun/22 02:29,04/Jun/24 20:51,08/Jun/22 02:29,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,Align TableEnvironment.create(configuration) API with Java.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 02:29:24 UTC 2022,,,,,,,,,,"0|z12yyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 02:29;dianfu;Merged to master via f011e52ac9943639c62f68b44bddcbacfa96e51c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decouple the advertisedAddress and rest.bind-address,FLINK-27900,13448510,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,LucentWong,LucentWong,06/Jun/22 06:53,16/Mar/23 09:09,04/Jun/24 20:51,,1.10.3,1.11.6,1.12.0,1.13.6,1.14.4,,,,,,,,,,,,,,Runtime / REST,,,,,0,,,,,"Currently the Flink Rest api does not have authentication, according to the doc [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/security/security-ssl/#external--rest-connectivity]
 # We set up the Flink cluster in k8s
 # We set up a nginx sidecar to enable auth for Flink Rest api.
 # We set *rest.bind-address* to localhost to hide the original Flink address and port
 # We enabled the ssl for the Flink Rest api

It works fine wen the client tried to call the Flink Rest api with *https* scheme.

But if the client using *http* scheme, the *RedirectingSslHandler* will try to redirect the address to the advertised url. According to {*}RestServerEndpoint{*}, Flink will use the value of *rest.bind-address* as the {*}advertisedAddress{*}. So the client will be redirected to *127.0.0.1* and failed to connect the url.

So we hope the advertisedAddress can be decoupled with rest.bind-addres, to provide more flexibility to the Flink deployment.","Flink 1.13, 1.12, 1.11, 1.10 with ssl

Deploy Flink in Kubernetes pod with a nginx sidecar for auth",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 16 09:09:52 UTC 2023,,,,,,,,,,"0|z12yx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 12:42;Wencong Liu;Hello [~LucentWong] , I think if advertisedAddress is decoupled from the bind-address, it means the advertisedAddress can be set by user itself. It's conflicted with it's meaning of ""advertised"".;;;","16/Mar/23 09:09;LucentWong;[~Wencong Liu] Thanks for the reply.

But I have some different view about the advertisedAddress. By checking some other open source products, I think the advertise listener(host) is able to change. Just to list two of what I know, Kafka([https://kafka.apache.org/documentation/#brokerconfigs_advertised.listeners]) and Pulsar ([https://pulsar.apache.org/docs/2.11.x/concepts-multiple-advertised-listeners/#advertised-listeners])

And for the aspect of *RedirectingSslHandler.* If we need to put Flink service in the internal network and expose the service by a proxy, as the advertised address must be same as bind-address, the redirecting handler will always return the address that cannot be accessed from the external.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deactivate the shade plugin doesn't take effect,FLINK-27899,13448508,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jackylau,jackylau,06/Jun/22 06:36,18/Jul/22 12:01,04/Jun/24 20:51,08/Jun/22 11:15,1.16.0,,,,,,,,,,,,,,,,,,Quickstarts,,,,,0,pull-request-available,,,,"{code:java}
We need to specify id 
add this <id>shade-flink</id>

  <!-- deactivate the shade plugin for the walkthrough archetypes -->
<plugin>
   <groupId>org.apache.maven.plugins</groupId>
   <artifactId>maven-shade-plugin</artifactId>
   <executions>
      <execution>
         <phase/>
      </execution>
   </executions>
</plugin> {code}
logs here:

 

!image-2022-06-06-14-35-00-438.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27886,,,,,,,,,,,,,,,,,,,"06/Jun/22 06:35;jackylau;image-2022-06-06-14-35-00-438.png;https://issues.apache.org/jira/secure/attachment/13044661/image-2022-06-06-14-35-00-438.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-06 06:36:34.0,,,,,,,,,,"0|z12ywo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix PartitionPushDown in streaming mode for hive source,FLINK-27898,13448492,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zoucao,zoucao,06/Jun/22 03:35,20/Jun/22 07:06,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,"In hive source, the PartitionPushDown will cause some problems in streaming-mode, we can add the following test in {*}HiveTableSourceITCase{*}

{code:java}
@Test
public void testPushDown() throws Exception {
final String catalogName = ""hive"";
final String dbName = ""source_db"";
final String tblName = ""stream_test"";
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.enableCheckpointing(10 * 1000);
StreamTableEnvironment tEnv =
HiveTestUtils.createTableEnvInStreamingMode(env, SqlDialect.HIVE);
tEnv.registerCatalog(catalogName, hiveCatalog);
tEnv.useCatalog(catalogName);
tEnv.executeSql(
""CREATE TABLE source_db.stream_test (""
+ "" a INT,""
+ "" b STRING""
+ "") PARTITIONED BY (ts int) TBLPROPERTIES (""
+ ""'streaming-source.enable'='true',""
+ ""'streaming-source.monitor-interval'='10s',""
+ ""'streaming-source.consume-order'='partition-name',""
+ ""'streaming-source.consume-start-offset'='ts=1'""
+ "")"");

HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)
.addRow(new Object[]{0, ""a0""})
.addRow(new Object[]{1, ""a0""})
.commit(""ts=0"");
HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)
.addRow(new Object[]{1, ""a1""})
.addRow(new Object[]{2, ""a1""})
.commit(""ts=1"");

HiveTestUtils.createTextTableInserter(hiveCatalog, dbName, tblName)
.addRow(new Object[]{1, ""a2""})
.addRow(new Object[]{2, ""a2""})
.commit(""ts=2"");
System.out.println(tEnv.explainSql(""select * from hive.source_db.stream_test where ts > 1""));
TableResult result = tEnv.executeSql(""select * from hive.source_db.stream_test where ts > 1"");
result.print();
}
{code}

{code:java}
+----+-------------+--------------------------------+-------------+
| op |           a |                              b |          ts |
+----+-------------+--------------------------------+-------------+
| +I |           1 |                             a2 |           2 |
| +I |           2 |                             a2 |           2 |
| +I |           1 |                             a1 |           1 |
| +I |           2 |                             a1 |           1 |
{code}

{code:java}
== Abstract Syntax Tree ==
LogicalProject(a=[$0], b=[$1], ts=[$2])
+- LogicalFilter(condition=[>($2, 1)])
   +- LogicalTableScan(table=[[hive, source_db, stream_test]])

== Optimized Physical Plan ==
TableSourceScan(table=[[hive, source_db, stream_test, partitions=[{ts=2}]]], fields=[a, b, ts])

== Optimized Execution Plan ==
TableSourceScan(table=[[hive, source_db, stream_test, partitions=[{ts=2}]]], fields=[a, b, ts])
{code}

The PartitionPushDown rule can generate the correct partitions that need to consume by using the existing partition. If the partitions are pushed to the hive source, the filter node will be removed. But hive source will not use the partition info which is pushed down in streaming mode, I think it causes some problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 07:06:23 UTC 2022,,,,,,,,,,"0|z12yt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 03:39;zoucao;cc [~luoyuxia], [~lzljs3620320], plz help me to confirm it, and if the problem exists, I am willing to fix it.;;;","07/Jun/22 12:14;luoyuxia;[~zoucao] From my side, it does a problem. And I'm afriad of there's same problem in filesystem source. Feel free to take it~;;;","08/Jun/22 03:23;zoucao;Hi [~luoyuxia], I think this problem only exists in the hive source for streaming reading, filesystem source does not support streaming reading, such that it can get the right partitions to consume.

Why does it exist in the hive source for streaming reading?
See *HiveSource*, in the Constructor, parameter *fileEnumerator* holds the partition info which is pushed down. In the method *HiveSource#createEnumerator*, if under the batch reading, `super.createEnumerator(enumContext)` will be invoked, so the fileEnumerator can be used, but under the streaming reading, the fileEnumerator will not be used.

How to solve?
1. For a simple resolving, we can remove the partitionPushdown supports under the streaming reading for hive source.
2. Otherwise, considering the partitions comitted in the future, we need to push down a boolean expression, not only the partition value, all partitions which should be consumed need to be decided by the expression. It's a little bit more complicated, and we need to change or add a public method, now the following method can not meet the condition.
{code:java}
applyPartitions(List<Map<String, String>> remainingPartitions); 
{code}

From my side, I think the second way is better, and more flexibility.

 

;;;","10/Jun/22 03:03;luoyuxia;[~zoucao] Thanks for detail explaination. Just go head to open a pr.;;;","10/Jun/22 03:21;zoucao;Hi, [~luoyuxia], before opening a pr, I think we should discuss how to solve it, and I am willing to know your thoughts about this problem. The table-planner module will also be involved, such that I think we should ask others for advice, gentle ping [~jark], what do you think about it?;;;","18/Jun/22 04:15;luoyuxia;Sorry for late reply. About my thoughts, I think the second way is better as our SupportFilterPushDown also pass  a expression. The only concern for me is it involves with change public interfaces, thus need a flip, which is likely to spent more effort.;;;","20/Jun/22 07:06;zoucao;Hi [~luoyuxia], I think you're right, if we choose the second way, a Flip is necessary, I will do some preparation, and then start a discussion about this to collect the opinions from others, WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
SearchArgumentToPredicateConverter can not handle partial filters,FLINK-27897,13448416,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,lzljs3620320,lzljs3620320,04/Jun/22 11:52,24/Jun/22 11:42,04/Jun/24 20:51,24/Jun/22 11:42,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"For example: SLECT * FROM T WHERE a = 1 and ${unsupported_filters};

According to the reasonable path, the condition a=1 should be able to be pushed down to the table store, but currently the whole will throw UnsupportedOperationException, resulting in a=1 not being pushed down.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 24 11:42:32 UTC 2022,,,,,,,,,,"0|z12yc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 11:42;lzljs3620320;master: a415eb7c910e9851db0a7e4efa40f815c62df8f4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up FlinkService#cancelJob implementation,FLINK-27896,13448407,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,gyfora,gyfora,04/Jun/22 08:48,28/Jun/22 14:01,04/Jun/24 20:51,28/Jun/22 14:01,kubernetes-operator-1.1.0,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"The current cancelJob implementation feels a bit overly complex and not very clean. We should probably rethink the flow a little to make it easier to read.

cc [~Miuler] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-04 08:48:22.0,,,,,,,,,,"0|z12ya8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable the CI test for Hive's 3.x ,FLINK-27895,13448398,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Jun/22 02:46,14/Jun/22 08:34,04/Jun/24 20:51,14/Jun/22 08:34,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Build System / CI,Connectors / Hive,,,,0,pull-request-available,,,,"We only enable Ci test for Hive's 2.3.9,  we also need enable the test for Hive's 3.x to guarantee the compatibility for Hive 3.x .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 08:34:55 UTC 2022,,,,,,,,,,"0|z12y88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/22 04:28;jark;[~luoyuxia] could you support this as a high priority? I think this is important before supporting other Hive dialects. ;;;","04/Jun/22 05:12;luoyuxia;[~jark] Yes, it'll be my first priority.;;;","14/Jun/22 08:34;chesnay;master: 39c7958f2ba0219272c8c11118ce4d6ee23a84d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build flink-connector-hive failed using Maven@3.8.5,FLINK-27894,13448364,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,bgeng777,bgeng777,03/Jun/22 16:15,07/Jun/22 07:31,04/Jun/24 20:51,07/Jun/22 07:31,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"When I tried to build flink project locally with Java8 and Maven3.8.5, I met such error:


{code:java}
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.16-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [repository.jboss.org (http://repository.jboss.org/nexus/content/groups/public/, default, disabled), conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> [Help 1]

{code}

After some investigation, the reason may be that Maven 3.8.1 disables support for repositories using ""http"" protocol. Due to [NIFI-8398], one possible solution is adding 
{code:xml}
    <repositories>
        <!-- HTTP Repository for pentaho-aggdesigner-algorithm 5.1.5-jhyde required by calcite-1.2.0-incubating -->
        <repository>
            <id>conjars</id>
            <url>https://conjars.org/repo</url>
        </repository>
    </repositories>
{code}
 in the pom.xml of flink-connector-hive module.
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27640,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jun 04 02:48:13 UTC 2022,,,,,,,,,,"0|z12y0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 16:16;bgeng777;cc [~luoyuxia]  have you ever met such error?;;;","04/Jun/22 02:48;luoyuxia;Simliar issue to [FLINK-27640|https://issues.apache.org/jira/browse/FLINK-27640];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additionalPrinterColumns for Job Status and Reconciliation Status in CRDs,FLINK-27893,13448335,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,jeesmon,jeesmon,jeesmon,03/Jun/22 14:09,23/Sep/22 13:46,04/Jun/24 20:51,23/Sep/22 13:46,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Right now only columns available when you run ""kubectl get flinkdeployment"" is NAME and AGE. It is beneficial to see the .status.jobStatus.State and .status.ReconciliationStatus.State as additionalPrintColumns in CRD. It will be beneficial for automation to use those columns for status reporting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 23 13:46:23 UTC 2022,,,,,,,,,,"0|z12xu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/22 08:54;gyfora;We have to be careful what we expose as not everything is completely accurate from the user's perspective. Job state for instance is a periodic glimpse of the actual job state and is not really mapped reliably by the operator;;;","04/Jun/22 08:55;gyfora;I think ReconciliationStatus.State is better because that is controlled by the operator but it can be confusing because we use DEPLOYED everytime when its RUNNING/SUSPENDED

 ;;;","08/Jul/22 17:00;maddisondavid;I do agree that the JobState reflects a point in time value and therefore may miss state changes that happen in between the reconcile schedule (i.e. a Job changing RUNNING -> RESTARTING -> RUNNING) however, it's still potentially useful information that is not reflected elsewhere. 

Other useful information would be the Flink Version, Upgrade Mode and Savepoint the job last started from, i.e.
{code:java}
NAME                          STATE     FLINK    MODE        SAVEPOINT
basic-checkpoint-ha-example   RUNNING   1.15.0   savepoint   file:/mnt/flink/savepoints/savepoint-000000-081fd2030040
basic-example                 RUNNING   1.15.0   stateless{code}
 ;;;","08/Jul/22 18:55;gyfora;Thank you for the input [~maddisondavid] , I see the value but I am going to play the devils advocate here a little :) 

What you expect to see when you list FlinkDeployments is not necessarily the state of the job, but the state of the resource. Arguable the job state is part of it so I am not against showing it but I suggest we show something like: 


{noformat}
enum ResourceLifecycleState {
    SUSPENDED, UPGRADING, DEPLOYED, STABLE, ROLLING_BACK, ROLLED_BACK, FAILED
}{noformat}
I am working on something like this for tracking detailed metrics about the operator (measure time in applicaiton lifecycle state transictions).

Not sure UpgradeMode/Savepoint should be shown here, I think thats not really important as long as it works correctly. Flink version makes sense ;;;","23/Sep/22 13:44;jeesmon;[~gyfora] Can we mark this as closed based on FLINK-29383. Thanks.;;;","23/Sep/22 13:45;gyfora;Ah I did not realize that this was a duplicate. Closing;;;","23/Sep/22 13:46;gyfora;duplicate of FLINK-29383;;;",,,,,,,,,,,,,,,,,,,,,,,,,
More than 1 secondary resource related to primary,FLINK-27892,13448304,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morhidi,morhidi,,03/Jun/22 09:32,24/Nov/22 01:03,04/Jun/24 20:51,14/Jun/22 14:54,kubernetes-operator-1.1.0,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"When submitting the `the basic-session-job.yaml' in multiple namespaces:

{{flink-kubernetes-operator java.lang.IllegalStateException: More than 1 secondary resource related to primary flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.source.ResourceEventSource.getSecondaryResource(ResourceEventSource.java:19) flink-kubernetes-operator at io.javaoperatorsdk.operator.api.reconciler.DefaultContext.getSecondaryResource(DefaultContext.java:47) flink-kubernetes-operator at io.javaoperatorsdk.operator.api.reconciler.Context.getSecondaryResource(Context.java:15) flink-kubernetes-operator at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.validateSessionJob(FlinkSessionJobController.java:135) flink-kubernetes-operator at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:91) flink-kubernetes-operator at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:51) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:201) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:153) flink-kubernetes-operator at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:152) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:135) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:115) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:86) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:59) flink-kubernetes-operator at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:390) flink-kubernetes-operator at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) flink-kubernetes-operator at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) flink-kubernetes-operator at java.base/java.lang.Thread.run(Unknown Source)}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 14:54:11 UTC 2022,,,,,,,,,,"0|z12xnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 10:46;morhidi;Can be reproduced by submitting the same session jobs into multiplenamespaces:

 

{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkDeployment}}
{{metadata:}}
{{  name: basic-session-cluster}}
{{  namespace: default}}
{{spec:}}
{{  image: flink:1.15}}
{{  flinkVersion: v1_15}}
{{  jobManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  taskManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  serviceAccount: flink}}
{{—}}
{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkSessionJob}}
{{metadata:}}
{{  name: basic-session-job-example}}
{{  namespace: default}}
{{spec:}}
{{  deploymentName: basic-session-cluster}}
{{  job:}}
{{    jarURI: [https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar]}}
{{    parallelism: 4}}
{{    upgradeMode: stateless}}
{{—}}
{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkDeployment}}
{{metadata:}}
{{  name: basic-session-cluster}}
{{  namespace: flink}}
{{spec:}}
{{  image: flink:1.15}}
{{  flinkVersion: v1_15}}
{{  jobManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  taskManager:}}
{{    resource:}}
{{      memory: ""2048m""}}
{{      cpu: 1}}
{{  serviceAccount: flink}}
{{—}}
{{apiVersion: flink.apache.org/v1beta1}}
{{kind: FlinkSessionJob}}
{{metadata:}}
{{  name: basic-session-job-example}}
{{  namespace: flink}}
{{spec:}}
{{  deploymentName: basic-session-cluster}}
{{  job:}}
{{    jarURI: [https://repo1.maven.org/maven2/org/apache/flink/flink-examples-streaming_2.12/1.15.0/flink-examples-streaming_2.12-1.15.0-TopSpeedWindowing.jar]}}
{{    parallelism: 4}}
{{    upgradeMode: stateless}};;;","14/Jun/22 14:54;gyfora;merged to main 33ca85ea4b7ec45a35c554cab0fba562160672b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ARRAY_APPEND and ARRAY_PREPEND supported in SQL & Table API,FLINK-27891,13448298,13076759,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,03/Jun/22 09:21,19/Feb/24 22:07,04/Jun/24 20:51,19/Feb/24 22:07,,,,,,,,,,,,,,,1.20.0,,,,Table SQL / API,,,,,0,pull-request-available,stale-assigned,,,"{{ARRAY_APPEND}} - adds element to the end of the array and returns the resulting array
{{ARRAY_PREPEND}} - adds element to the beginning of the array and returns the resulting array

Syntax:
{code:sql}
ARRAY_APPEND( <array> , <new_element> );
ARRAY_PREPEND( <new_element> , <array> );
{code}
Arguments:

array: An ARRAY to to add a new element.

new_element: A new element.

Returns:

An array. If array is NULL, the result is NULL.

Examples:
{code:sql}
SELECT array_append(array[1, 2, 3], 4);
-- array[1, 2, 3, 4]
select array_append(cast(null as int array), 2);
-- null
SELECT array_prepend(4, array[1, 2, 3]);
-- array[4, 1, 2, 3]
SELECT array_prepend(null, array[1, 2, 3]);
-- array[null, 1, 2, 3]
{code}
See more:
{{ARRAY_APPEND}}
Snowflake [https://docs.snowflake.com/en/sql-reference/functions/array_append.html]
PostgreSQL [https://www.postgresql.org/docs/14/functions-array.html#ARRAY-FUNCTIONS-TABLE]
{{ARRAY_PREPEND}}
Snowflake [https://docs.snowflake.com/en/sql-reference/functions/array_prepend.html]
PostgreSQL [https://www.postgresql.org/docs/14/functions-array.html#ARRAY-FUNCTIONS-TABLE]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 19 22:06:58 UTC 2024,,,,,,,,,,"0|z12xm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","19/Feb/24 22:06;Sergey Nuyanzin;Merged as [e644beac8e5ffe71d9b6185c06ed31050e7c5268|https://github.com/apache/flink/commit/e644beac8e5ffe71d9b6185c06ed31050e7c5268];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SideOutputExample.java fails,FLINK-27890,13448272,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,coderap,wxwmd,wxwmd,03/Jun/22 07:16,17/Jun/22 13:08,04/Jun/24 20:51,13/Jun/22 08:25,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Examples,,,,,0,pull-request-available,,,,"The bug appears on line 87 of flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/sideoutput/SideOutputExample.java：
{code:java}
text.assignTimestampsAndWatermarks(IngestionTimeWatermarkStrategy.create()); {code}
This line forgets to set the return value, so the timestamp is not assigned to the element in {_}text{_}. As a result, running this code throws an error.","# flink version 1.15
 # jdk 1.8
 # scala 2.12",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-19317,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 08:25:26 UTC 2022,,,,,,,,,,"0|z12xg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/22 03:55;coderap;Bug introduced by FLINK-19317 [~aljoscha]

It assigns the TimestampsAndWatermarks but not uses the return result for the next step

Please assign this ticket to me cc [~aljoscha] 

 

 

 ;;;","07/Jun/22 08:45;martijnvisser;CC [~danderson];;;","09/Jun/22 10:09;danderson;[~coderap] Good catch! You're right, this doesn't work correctly. I'm assigning this to you.;;;","13/Jun/22 08:25;danderson;Merged in master with [{{a70e704}}|https://github.com/apache/flink/commit/a70e7045a3eabacb50f54a204bcde4fe554d8e8b]
Merged in release-1.15 with [{{6fcec2c}}|https://github.com/apache/flink/commit/6fcec2cf464f0467c3bb5ca3d249ac3ac754820a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when the LastReconciledSpec is null,FLINK-27889,13448258,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Miuler,Miuler,Miuler,03/Jun/22 05:35,04/Jun/22 17:32,04/Jun/24 20:51,04/Jun/22 17:25,,,,,,,,,,,,,,,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"My FlinkDeployment was left with erros, when he can not start correctly, the following message:

 

 
{code:java}
2022-06-01 04:36:10,070 o.a.f.k.o.r.ReconciliationUtils [WARN ][flink-02/cosmosdb] Attempt count: 5, last attempt: true
2022-06-01 04:36:10,072 i.j.o.p.e.ReconciliationDispatcher [ERROR][flink-02/cosmosdb] Error during event processing ExecutionScope{ resource id: CustomResourceID
{name='cosmosdb', namespace='flink-02'}, version: null} failed.
org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.IllegalArgumentException: Only ""local"" is supported as schema for application mode. This assumes that the jar is located in the image, not the Flink client. An example of such path is: local:///opt/flink/examples/streaming/WindowJoin.jar
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:130)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:59)
        at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:101)
        at io.javaoperatorsdk.operator.processing.Controller$2.execute(Controller.java:76)
        at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
        at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:75)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:143)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:109)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:74)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
        at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Only ""local"" is supported as schema for application mode. This assumes that the jar is located in the image, not the Flink client. An example of such path is: local:///opt/flink/examples/streaming/WindowJoin.jar
        at org.apache.flink.kubernetes.utils.KubernetesUtils.lambda$checkJarFileForApplicationMode$2(KubernetesUtils.java:407)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedFunction$2(FunctionUtils.java:73)
        at java.base/java.util.stream.ReferencePipeline$3$1.accept(Unknown Source)
        at java.base/java.util.Collections$2.tryAdvance(Unknown Source)
        at java.base/java.util.Collections$2.forEachRemaining(Unknown Source)
        at java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)
        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)
        at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source)
        at java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)
        at java.base/java.util.stream.ReferencePipeline.collect(Unknown Source)
        at org.apache.flink.kubernetes.utils.KubernetesUtils.checkJarFileForApplicationMode(KubernetesUtils.java:412)
        at org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:206)
        at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)
        at org.apache.flink.kubernetes.operator.service.FlinkService.submitApplicationCluster(FlinkService.java:163)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.deployFlinkJob(ApplicationReconciler.java:283)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.reconcile(ApplicationReconciler.java:83)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.reconcile(ApplicationReconciler.java:58)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:126)
        ... 13 more
2022-06-01 04:36:10,073 i.j.o.p.e.EventProcessor       [ERROR][flink-02/cosmosdb] Exhausted retries for ExecutionScope{ resource id: CustomResourceID{name='cosmosdb', namespace='flink-02'}
, version: null}
2022-06-01 04:37:27,344 o.a.f.k.o.c.FlinkDeploymentController [INFO ][flink-02/cosmosdb] Deleting FlinkDeployment
2022-06-01 04:37:27,345 i.j.o.p.e.ReconciliationDispatcher [ERROR][flink-02/cosmosdb] Error during event processing ExecutionScope{ resource id: CustomResourceID
{name='cosmosdb', namespace='flink-02'}, version: 5206202} failed.
java.lang.RuntimeException: Cannot create observe config before first deployment, this indicates a bug.
        at org.apache.flink.kubernetes.operator.config.FlinkConfigManager.getObserveConfig(FlinkConfigManager.java:137)
        at org.apache.flink.kubernetes.operator.service.FlinkService.cancelJob(FlinkService.java:357)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.shutdown(ApplicationReconciler.java:327)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractDeploymentReconciler.cleanup(AbstractDeploymentReconciler.java:56)
        at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractDeploymentReconciler.cleanup(AbstractDeploymentReconciler.java:37)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:107)
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:59)
        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:68)
        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:50)
        at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)
        at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:49)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleCleanup(ReconciliationDispatcher.java:252)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:72)
        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)
        at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
{{2022-06-01 04:37:27,345 i.j.o.p.e.EventProcessor       [ERROR][flink-02/cosmosdb] Exhausted retries for ExecutionScope{ resource id: CustomResourceID{name='cosmosdb', namespace='flink-02'}
, version: 5206202}}}
2022-06-01 04:40:44,540 o.a.f.k.o.c.FlinkConfigManager [INFO ] Default configuration did not change, nothing to do...
2022-06-01 04:40:44,848 o.a.f.m.s.Slf4jReporter        [INFO ]
{code}
 

 

The FlinkDeployment of the example is the fallowing: [^scratch_7.json]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/22 05:42;Miuler;scratch_7.json;https://issues.apache.org/jira/secure/attachment/13044599/scratch_7.json",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jun 04 17:31:18 UTC 2022,,,,,,,,,,"0|z12xd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jun/22 17:25;gyfora;merged to main cd5107d7ace9d2896efde91ac0ca3c7a9226f3bc;;;","04/Jun/22 17:31;gyfora;release-1.0 : 6ce1c6b51241ca2b866816232acb1612e2b18c40;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Error in Flink1.15,FLINK-27888,13448242,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,songwenbin,songwenbin,03/Jun/22 01:38,08/Jun/22 14:01,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"In flink1.15 ,  if sql include UDF(AggregateFunction) and HOP windown , it execute error : Cannot determine simple type name 'org' .

But in Flink1.14 or flink1.15 ,it 's ok.

 ","mac os , linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 13:45;songwenbin;flink-demo-1.0-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/13044781/flink-demo-1.0-SNAPSHOT.jar","08/Jun/22 13:57;songwenbin;flink_demo_jar_content_list.jpg;https://issues.apache.org/jira/secure/attachment/13044785/flink_demo_jar_content_list.jpg","03/Jun/22 09:35;coderap;flink_not_contain_jar.png;https://issues.apache.org/jira/secure/attachment/13044619/flink_not_contain_jar.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,"In Flink1.15 
1. put attachment jar to directory:  ./flink-release-1.15/build-target/lib/ , 
eg: ./flink-release-1.15/build-target/lib/flink-demo-1.0-SNAPSHOT.jar
2. start flink local cluster:  ./flink-release-1.15/build-target/start-cluster.sh
3. start sql-client：./flink-release-1.15/build-target/sql-client.sh
4. run below sql script in sql-client: 

CREATE  FUNCTION ddd AS 'org.example.SumHopWindowUpsertVal';


CREATE TABLE monthly_source_5 (
        item            STRING,
        val             DOUBLE,
        ts              BIGINT,
        isEnergy        BIGINT,
        shiftId         INTEGER,
        eventTime       BIGINT,
        event_time      AS TO_TIMESTAMP_LTZ(eventTime, 3),
        process_time    AS PROCTIME(),
        projectId       AS 1,        
        WATERMARK FOR event_time AS event_time)
    WITH ('connector' = 'kafka','topic' = 'daily-consumption','properties.bootstrap.servers' = 'KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS', 'properties.group.id' = 'poem-consumergroup-flink-v2',  'value.format' = 'json','scan.startup.mode' = 'latest-offset');

CREATE VIEW daily_last_value_with_monthly_frame5 AS
    SELECT
        item AS tag,
        val,
        ts,        
        process_time,
        isEnergy,
        eventTime,
        event_time,
        1  AS frame
    FROM monthly_source_5
    WHERE shiftId =0;

CREATE VIEW energy_monthly_consumptions5 AS
    SELECT
        tag,
        ts                               AS ts,
        1  AS monthFrameTs,
        val                              AS val,
        eventTime,
        event_time,
        isEnergy
    FROM daily_last_value_with_monthly_frame5;

CREATE VIEW monthly_consumption_result_10 AS    
    SELECT
        tag                         AS tag,
        monthFrameTs                AS ts,
        ddd(
            ts,
            val,
            monthFrameTs)           AS val,
        MAX(eventTime)              AS eventTime,
        1                           AS isEnergy                
    FROM TABLE(
        HOP(TABLE energy_monthly_consumptions5, DESCRIPTOR(event_time), INTERVAL '1' DAYS, INTERVAL '31' DAYS))
    WHERE isEnergy = 1
    GROUP BY tag, monthFrameTs, window_start, window_end;

select * from monthly_consumption_result_10;",false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Wed Jun 08 13:53:18 UTC 2022,,,,,,,,,,"0|z12x9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 09:35;coderap;Your function or class does not packaged into the flink-sql-demo.jar

!flink_not_contain_jar.png!;;;","08/Jun/22 13:53;songwenbin;I updated new flink-demo-1.0-SNAPSHOT.jar to attachment. Thank you . 

 

!flink_demo_jar_content_list.jpg!  

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to access https://nightlies.apache.org/flink/,FLINK-27887,13448206,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,carlton.ibm,carlton.ibm,02/Jun/22 19:40,03/Jun/22 12:03,04/Jun/24 20:51,03/Jun/22 11:34,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,important,,,,"My organization is unable to access nightlies.apache.org/flink

We believe we may have been mistakenly placed on a deny list. Need someone to contact me for resolution at: [Benjamin.carlton@ibm.com|mailto:Benjamin.carlton@ibm.com]

If there is another preferred method of resolution, please let me know how to resolve. I have emailed the webmaster already but we did not receive a response. Thank you.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 03 12:02:52 UTC 2022,,,,,,,,,,"0|z12x1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 21:20;martijnvisser;[~carlton.ibm] Do you have any error message or more details when you're trying to access this page? ;;;","02/Jun/22 22:03;carlton.ibm;[~martijnvisser] No error messages, just never connects from any AP within IBM as a whole. VPN or hotspots work fine. After internal investigations, the network team has asked me to inquire here to see if IBM's addresses were accidentally placed on a deny list. If so, we would like them to be placed on allow instead.;;;","03/Jun/22 11:34;martijnvisser;[~carlton.ibm] That's weird. As mentioned in the Slack channel, we unfortunately can't really help here since we're using the services provided by ASF Infra https://infra.apache.org/ which are shared by all Apache projects. You could create a new Jira ticket towards the project {{Infrastructure}} here. Unfortunately there's nothing else that we can help out with on this. ;;;","03/Jun/22 12:02;carlton.ibm;Thank you Martijn! I've created an infrastructure issue as you advised.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error way for skipping shade plugin in sub-modules,FLINK-27886,13448164,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,taoran,taoran,02/Jun/22 15:07,19/Aug/23 10:35,04/Jun/24 20:51,,1.13.6,1.14.4,1.15.0,,,,,,,,,,,,,,,,BuildSystem / Shaded,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,"Currently in some sub-modules, for example, flink-quickstart-java  flink-quickstart-scala, flink-walkthrough, we want to skip shade but use error way like this

{code:java}
<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<executions>
					<execution>
						</phase>
					</execution>
				</executions>
			</plugin>
{code}

It just set none phase for itself, can't forbid the inherited parent shade.

 !screenshot-1.png! 
 !screenshot-2.png! 

the correct way is 

{code:java}
<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<executions>
					<execution>
						<id>shade-flink</id>
						<phase>none</phase>
					</execution>
				</executions>
			</plugin>
{code}

it can forbid parent shade-flink and also it's own shade. because there are no extra own shade executions.
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27899,,,,,,,,,,,,,,,,,,,,,,"02/Jun/22 15:09;taoran;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13044585/screenshot-1.png","02/Jun/22 15:09;taoran;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13044586/screenshot-2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:04 UTC 2023,,,,,,,,,,"0|z12ws8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 15:24;taoran;[~gaoyunhaii], [~jark] Hi, gaoyun, jark, can u help me to review this issue?;;;","02/Jun/22 15:35;martijnvisser;[~chesnay] any thoughts on this?;;;","08/Jun/22 06:48;taoran;cc [~chesnay]  PTAL, thanks;;;","15/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","23/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-csv,FLINK-27885,13448161,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,rskraba,rskraba,rskraba,02/Jun/22 14:56,17/Aug/23 10:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31674,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 10:35:15 UTC 2023,,,,,,,,,,"0|z12wrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 14:57;rskraba;Can you assign this to me please?;;;","30/Mar/23 14:35;rskraba;There are some test cases that need to be migrated at the same time as base classes in flink-table-planner, and will be updated as part of that PR.;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce OutputFormatBase to provide flushing/maxConcurrentRequests functionality,FLINK-27884,13448150,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,02/Jun/22 13:53,13/Jul/22 14:01,04/Jun/24 20:51,13/Jul/22 14:00,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Common,,,,,0,pull-request-available,,,,"This ticket is a generalization of [FLINK-27457|https://issues.apache.org/jira/browse/FLINK-27457]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27457,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 14:00:30 UTC 2022,,,,,,,,,,"0|z12wp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 13:57;echauchot;[~chesnay] As discussed in [this PR|https://github.com/apache/flink/pull/19680] I thought that flush mechanism could be interesting for the other output formats even if output formats are being deprecated. Can you assign me this ticket if you agree or close it otherwise ?;;;","03/Jun/22 08:25;echauchot;First difficulty that I can see is with guava dep and shading: cassandra uses _com.google.guava:guava:jar:18.0_ but flink-core for common io uses {_}org.apache.flink:flink-shaded-guava:jar:30.1.1-jre-15.0{_}. The problem is that if we promote the flush mechanism to common io there will be in the core an abstract _send()_ method that returns a guava _ListenableFuture_ and that is implemented in the connector modules such as cassandra. As a consequence the two incompatible guava _ListenableFuture_ will conflict.;;;","07/Jun/22 09:30;chesnay;I think in general this makes sense.

Regarding the future issue, let's just not expose guava in the API. We can use a CompletionStage instead, and sub-classes will have to adapt that somehow to work with whatever future solution they are using.;;;","08/Jun/22 08:43;echauchot;Yes, this is the easiest solution. Can you assign me the ticket? I'll do the PR right ahead

 ;;;","13/Jul/22 14:00;chesnay;master: 85a5e1d199e5529bc657e4797399e43a800194e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSubscriberTest failed with NPC,FLINK-27883,13448117,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,hxbks2ks,hxbks2ks,02/Jun/22 12:13,16/Oct/23 12:14,04/Jun/24 20:51,16/Oct/23 12:14,1.16.0,1.17.0,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,auto-deprioritized-major,test-stability,,,"
{code:java}
2022-06-02T01:42:45.0924799Z Jun 02 01:42:45 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 66.787 s <<< FAILURE! - in org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest
2022-06-02T01:42:45.0926067Z Jun 02 01:42:45 [ERROR] org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest  Time elapsed: 66.787 s  <<< FAILURE!
2022-06-02T01:42:45.0926867Z Jun 02 01:42:45 org.opentest4j.MultipleFailuresError: 
2022-06-02T01:42:45.0927608Z Jun 02 01:42:45 Multiple Failures (2 failures)
2022-06-02T01:42:45.0928626Z Jun 02 01:42:45 	java.lang.AssertionError: Create test topic : topic1 failed, org.apache.kafka.common.errors.TimeoutException: The request timed out.
2022-06-02T01:42:45.0929717Z Jun 02 01:42:45 	java.lang.NullPointerException: <no message>
2022-06-02T01:42:45.0930482Z Jun 02 01:42:45 	at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:196)
2022-06-02T01:42:45.0931579Z Jun 02 01:42:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:226)
2022-06-02T01:42:45.0932685Z Jun 02 01:42:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.testRunFinished(RunListenerAdapter.java:93)
2022-06-02T01:42:45.0933736Z Jun 02 01:42:45 	at org.junit.runner.notification.SynchronizedRunListener.testRunFinished(SynchronizedRunListener.java:42)
2022-06-02T01:42:45.0934600Z Jun 02 01:42:45 	at org.junit.runner.notification.RunNotifier$2.notifyListener(RunNotifier.java:103)
2022-06-02T01:42:45.0935437Z Jun 02 01:42:45 	at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)
2022-06-02T01:42:45.0936147Z Jun 02 01:42:45 	at org.junit.runner.notification.RunNotifier.fireTestRunFinished(RunNotifier.java:100)
2022-06-02T01:42:45.0936807Z Jun 02 01:42:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:138)
2022-06-02T01:42:45.0937370Z Jun 02 01:42:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-06-02T01:42:45.0938011Z Jun 02 01:42:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-06-02T01:42:45.0938756Z Jun 02 01:42:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-06-02T01:42:45.0939480Z Jun 02 01:42:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-06-02T01:42:45.0940304Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-06-02T01:42:45.0941136Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-06-02T01:42:45.0942000Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-06-02T01:42:45.0943056Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-06-02T01:42:45.0944171Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-06-02T01:42:45.0944945Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-06-02T01:42:45.0945756Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-06-02T01:42:45.0946607Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-06-02T01:42:45.0947618Z Jun 02 01:42:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-06-02T01:42:45.0948525Z Jun 02 01:42:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-06-02T01:42:45.0949401Z Jun 02 01:42:45 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-06-02T01:42:45.0950119Z Jun 02 01:42:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-06-02T01:42:45.0950950Z Jun 02 01:42:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-06-02T01:42:45.0951783Z Jun 02 01:42:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-06-02T01:42:45.0952551Z Jun 02 01:42:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-06-02T01:42:45.0953352Z Jun 02 01:42:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-06-02T01:42:45.0954164Z Jun 02 01:42:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-06-02T01:42:45.0954852Z Jun 02 01:42:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-06-02T01:42:45.0955778Z Jun 02 01:42:45 	Suppressed: java.lang.AssertionError: Create test topic : topic1 failed, org.apache.kafka.common.errors.TimeoutException: The request timed out.
2022-06-02T01:42:45.0956453Z Jun 02 01:42:45 		at org.junit.Assert.fail(Assert.java:89)
2022-06-02T01:42:45.0957167Z Jun 02 01:42:45 		at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.createTestTopic(KafkaTestEnvironmentImpl.java:207)
2022-06-02T01:42:45.0958045Z Jun 02 01:42:45 		at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironment.createTestTopic(KafkaTestEnvironment.java:97)
2022-06-02T01:42:45.0958881Z Jun 02 01:42:45 		at org.apache.flink.streaming.connectors.kafka.KafkaTestBase.createTestTopic(KafkaTestBase.java:217)
2022-06-02T01:42:45.0959687Z Jun 02 01:42:45 		at org.apache.flink.connector.kafka.testutils.KafkaSourceTestEnv.createTestTopic(KafkaSourceTestEnv.java:217)
2022-06-02T01:42:45.0960700Z Jun 02 01:42:45 		at org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest.setup(KafkaSubscriberTest.java:52)
2022-06-02T01:42:45.0961431Z Jun 02 01:42:45 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-02T01:42:45.0962064Z Jun 02 01:42:45 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-02T01:42:45.0962789Z Jun 02 01:42:45 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-02T01:42:45.0963625Z Jun 02 01:42:45 		at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-02T01:42:45.0964306Z Jun 02 01:42:45 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-02T01:42:45.0965203Z Jun 02 01:42:45 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-02T01:42:45.0965926Z Jun 02 01:42:45 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-02T01:42:45.0966643Z Jun 02 01:42:45 		at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
2022-06-02T01:42:45.0967348Z Jun 02 01:42:45 		at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2022-06-02T01:42:45.0968153Z Jun 02 01:42:45 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-06-02T01:42:45.0968987Z Jun 02 01:42:45 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-02T01:42:45.0969597Z Jun 02 01:42:45 		at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-02T01:42:45.0970179Z Jun 02 01:42:45 		at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-02T01:42:45.0970740Z Jun 02 01:42:45 		... 22 more
2022-06-02T01:42:45.0971160Z Jun 02 01:42:45 	Suppressed: java.lang.NullPointerException
2022-06-02T01:42:45.0971858Z Jun 02 01:42:45 		at org.apache.flink.connector.kafka.source.enumerator.subscriber.KafkaSubscriberTest.tearDown(KafkaSubscriberTest.java:59)
2022-06-02T01:42:45.0972663Z Jun 02 01:42:45 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-06-02T01:42:45.0973360Z Jun 02 01:42:45 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-06-02T01:42:45.0974361Z Jun 02 01:42:45 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-02T01:42:45.0975026Z Jun 02 01:42:45 		at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-02T01:42:45.0975672Z Jun 02 01:42:45 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-06-02T01:42:45.0976400Z Jun 02 01:42:45 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-06-02T01:42:45.0977129Z Jun 02 01:42:45 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-06-02T01:42:45.0977976Z Jun 02 01:42:45 		at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-06-02T01:42:45.0978673Z Jun 02 01:42:45 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-06-02T01:42:45.0979332Z Jun 02 01:42:45 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-06-02T01:42:45.0979951Z Jun 02 01:42:45 		at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-06-02T01:42:45.0980547Z Jun 02 01:42:45 		at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-06-02T01:42:45.0981006Z Jun 02 01:42:45 		... 22 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36270&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:22 UTC 2023,,,,,,,,,,"0|z12whs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 12:14;hxbks2ks;cc [~renqs];;;","01/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","09/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","02/Jan/23 10:19;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44336&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=8432;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-scala,FLINK-27882,13448111,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,02/Jun/22 11:51,20/Nov/23 23:21,04/Jun/24 20:51,20/Nov/23 23:21,,,,,,,,,,,,,,,1.19.0,,,,Tests,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 20 23:21:07 UTC 2023,,,,,,,,,,"0|z12wgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","20/Nov/23 23:21;Sergey Nuyanzin;Merged as [278504a2787a154faf6f6401028d4bbadafbba0a|https://github.com/apache/flink/commit/278504a2787a154faf6f6401028d4bbadafbba0a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The key(String) in PulsarMessageBuilder returns null,FLINK-27881,13448098,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,csq,csq,02/Jun/22 10:56,20/Jun/22 12:30,04/Jun/24 20:51,16/Jun/22 14:58,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"The PulsarMessageBuild.key(String) always return null, which might cause NPE in later call.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 16 14:58:01 UTC 2022,,,,,,,,,,"0|z12wdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 13:24;martijnvisser;[~syhily] Any thoughts on this? ;;;","15/Jun/22 22:24;syhily;I'll submit a PR today. This is a known bug.;;;","16/Jun/22 14:58;chesnay;master: 6b2a3fe3afce52b0f8bf8969f95f1f5a4d94ccb1
1.15: b6d5e3dd11f3071f6c99ee86bcb86ac184d654ca ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI runs keep getting stuck,FLINK-27880,13448077,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,alexanderpreuss,alexanderpreuss,02/Jun/22 08:50,15/Jun/22 09:18,04/Jun/24 20:51,15/Jun/22 09:18,1.16.0,,,,,,,,,,,,,,,,,,,,,,,0,test-stability,,,,"I am observing multiple fails where the CI seems to be getting stuck and fails because of running into the timeout. 

[Build #36259|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36259&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=12054] got stuck in a {{StreamFaultToleranceTestBase}} subclass

[Build #36209|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36209&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9370] got stuck in {{KeyedStateCheckpointingITCase.testWithMemoryBackendSync}}

[Build #36189|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36189&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10455] got stuck in {{ZooKeeperMultipleComponentLeaderElectionDriverTest.testLeaderElectionWithMultipleDrivers}}

 ",,,,,,,,,,,,,,,FLINK-28076,FLINK-28077,FLINK-28078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 09:18:18 UTC 2022,,,,,,,,,,"0|z12w8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 09:18;mapohl;I split this issue up into separate issues since it appears to have different reasons;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate the ""Documentation Style Guide"" page into Chinese",FLINK-27879,13448076,13213873,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zyxing,zyxing,zyxing,02/Jun/22 08:49,20/Jun/22 08:51,04/Jun/24 20:51,20/Jun/22 08:51,,,,,,,,,,,,,,,,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,"The ""Documentation Style Guide"" need to be translated in Chinese.

Related files:

        modified:   _data/i18n.yml (  add nav-bar translate)
        modified:   contributing/contribute-documentation.zh.md( main page)
        modified:   contributing/docs-style.zh.md ( add missing paragraph）

I will be pleasure to take this PR.

 

Further more:

Currently,the website and document are in two implementations(Jekyll for website and Hugo for documents)

, which may need to be clarified in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 08:51:16 UTC 2022,,,,,,,,,,"0|z12w8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 08:53;martijnvisser;[~zyxing] I've assigned to the ticket to you. There is already FLINK-22922 for the migration of the Flink project website to Hugo;;;","20/Jun/22 08:51;yunta;merged in flink-web: 17ea48f8883f72d9323c8e05776cd292e90d2611;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-232] Add Retry Support For Async I/O In DataStream API,FLINK-27878,13448060,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,02/Jun/22 08:04,23/Sep/22 18:27,04/Jun/24 20:51,16/Jul/22 00:52,,,,,,,,,,,,,,,1.16.0,,,,API / DataStream,,,,,0,pull-request-available,,,,"FLIP-232: Add Retry Support For Async I/O In DataStream API
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=211883963",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29038,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,,Sat Jul 16 00:52:50 UTC 2022,,,,,,,,,,"0|z12w54:",9223372036854775807,"This feature introduces a built-in retry mechanism for async operator which is transparently to user's existing code, it is flexibly to satisfy the retry and exception handling needs for users.",,,,,,,,,,,,,,,,,,,"16/Jul/22 00:52;gaoyunhaii;Merged on master via  e85cf8c4cdf417b47f8d53bf3bb202f79e92b205.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve StringIndexer performance and optimize flink-ml-bench,FLINK-27877,13448042,13438170,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,02/Jun/22 06:08,24/Jun/22 08:40,04/Jun/24 20:51,24/Jun/22 07:48,,,,,,,,,,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-02 06:08:42.0,,,,,,,,,,"0|z12w14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error side is chosen in BatchPhysicalHashJoinRule when the join is Semi or Anti,FLINK-27876,13448024,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,02/Jun/22 02:46,18/Dec/23 11:03,04/Jun/24 20:51,17/Dec/23 06:29,1.16.0,,,,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,0,auto-deprioritized-minor,pull-request-available,,,"This bug is introduced with the https://issues.apache.org/jira/browse/FLINK-12371. If leftSize or rightSize is unknown or equal in Semi/Anti Join, the right side of the join should be chosen as build side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 17 06:28:38 UTC 2023,,,,,,,,,,"0|z12vx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","17/Dec/23 06:28;lsy;Merge to master: f7ababa8f0bab87267b101efe2ecc77b700c2222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce TableScan and TableRead as an abstraction layer above FileStore for reading RowData,FLINK-27875,13448023,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,02/Jun/22 02:42,03/Jun/22 00:53,04/Jun/24 20:51,03/Jun/22 00:53,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,In this step we introduce {{TableScan}} and {{TableRead}} They are an abstraction layer above {{FileStoreScan}} and {{FileStoreRead}} to provide {{RowData}} reading.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 03 00:53:49 UTC 2022,,,,,,,,,,"0|z12vww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/22 00:53;lzljs3620320;master:

1012ea20cecbd6237acefc6427769bec164d5076

3ec19cd53e7db287028c142fb5754d73353ff1af;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyedStateCheckpointingITCase Timeout,FLINK-27874,13448021,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,fsk119,fsk119,02/Jun/22 02:11,21/Nov/22 02:20,04/Jun/24 20:51,21/Nov/22 02:20,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,,"The test fails in the CI.

 
{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007f216400b800 nid=0x21fb waiting on condition [0x00007f216c07f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000082d016d8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1989)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1951)
	at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testProgramWithBackend(KeyedStateCheckpointingITCase.java:175)
	at org.apache.flink.test.checkpointing.KeyedStateCheckpointingITCase.testWithMemoryBackendSync(KeyedStateCheckpointingITCase.java:104)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36200&view=logs&j=38d6b56a-d502-56fb-7b73-c09f8fe7becd&t=6e6509fa-8a5d-5a6c-e17e-64f5ecc17842&l=13690",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 21 02:19:50 UTC 2022,,,,,,,,,,"0|z12vwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/22 10:00;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36478&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=14190;;;","21/Nov/22 02:19;Yanfei Lei;Since this hasn't been reproduced in 5 months, I'm going to close this ticket, please reopen it if the bug reproduces again. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Primary key is lost after calling addColumns,FLINK-27873,13447991,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sap1ens,sap1ens,01/Jun/22 17:52,07/Jun/22 16:18,04/Jun/24 20:51,,1.14.4,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"I've noticed that when adding new columns via *addColumns* method in the Table API the information about the primary key is lost.

I was able to track it down to the following files:
 * [ProjectionOperationFactory.create|https://github.com/apache/flink/blob/fbc086f2fd63dac4af3e104ba4011e864781139a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/operations/utils/ProjectionOperationFactory.java#L91-L92] creates a new QueryOperation for the updated table and it calls *ResolvedSchema.physical* method in the end.
 * [ResolvedSchema.physical|https://github.com/apache/flink/blob/94328f90c5a3d13acff6dfbf2e22d1c7aa59c080/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ResolvedSchema.java#L97] method always creates ResolvedSchema with null as a primary key.

This is not a behaviour I'd expect when _adding_ new columns.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 16:18:21 UTC 2022,,,,,,,,,,"0|z12vps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 09:08;martijnvisser;[~jark] Any thoughts on this? 
[~sap1ens] Have you also checked this on Flink 1.15? ;;;","07/Jun/22 16:18;sap1ens;Re: 1.15, I haven't actually tried reproducing it on 1.15, but the files that I linked haven't changed between 1.14.4 and now, so I'm fairly confident it's not fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow KafkaBuilder to set arbitrary subscribers,FLINK-27872,13447980,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Alcántara,Alcántara,01/Jun/22 16:14,07/Jun/22 04:04,04/Jun/24 20:51,07/Jun/22 04:04,,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,,,,,"Currently, [KafkaSourceBuilder|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.java] has two setters:
 * [setTopics|https://github.com/apache/flink/blob/586715f23ef49939ab74e4736c58d71c643a64ba/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.java#L157]
 * [setTopicPattern|https://github.com/apache/flink/blob/586715f23ef49939ab74e4736c58d71c643a64ba/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/KafkaSourceBuilder.java#L168]

which under the hood instantiate the corresponding (concrete) subscribers. This covers the most common needs, I agree, but it might fall short in some cases. Why not add a more generic setter:
 * {{setSubscriber (???)}}

Otherwise, how can users read from kafka in combination with custom subscribing logic? Without looking much into it, it seems that they basically cannot, at least without having to replicate some parts of the connector, which seems rather inconvenient.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24660,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 04:03:49 UTC 2022,,,,,,,,,,"0|z12vnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 10:00;Alcántara;Based on a quick experiment, it seems that using a custom subscriber requires to create your own fork for these two classes:
 - `KafkaSourceBuilder`
 - `KafkaSource`

The rest of the connector seems reusable, but that is still far from ideal.;;;","02/Jun/22 17:29;martijnvisser;[~renqs] What do you think?;;;","07/Jun/22 04:03;renqs;I think this duplicates FLINK-24660.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration change is undedected on config removal,FLINK-27871,13447964,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,01/Jun/22 15:38,24/Nov/22 01:02,04/Jun/24 20:51,22/Jun/22 13:07,kubernetes-operator-1.0.0,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,,,,,"The Operator does not detect when a configuration entry is removed from the configmap. The equals check in *FlinkConfigManager.updateDefaultConfig* returns *true* incorrectly:

 

{{if (newConf.equals(defaultConfig)) {}}
{{LOG.info(""Default configuration did not change, nothing to do..."");}}
{{return;}}
{{}}}",,,,,,,,,,,,,,,,,,,FLINK-28141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 21 08:23:02 UTC 2022,,,,,,,,,,"0|z12vjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 08:23;gyfora;This is fixed already right?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SemiAntiJoinStreamITCase. testAntiJoinWithTwoSidesRetraction failed with InterruptedException,FLINK-27870,13447921,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hxbks2ks,hxbks2ks,01/Jun/22 12:32,08/Jun/22 05:32,04/Jun/24 20:51,08/Jun/22 05:32,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,"
{code:java}
2022-05-31T01:09:40.9789604Z May 31 01:09:40 [ERROR] SemiAntiJoinStreamITCase.testAntiJoinWithTwoSidesRetraction  Time elapsed: 0.933 s  <<< ERROR!
2022-05-31T01:09:40.9790850Z May 31 01:09:40 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-31T01:09:40.9792055Z May 31 01:09:40 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-31T01:09:40.9793516Z May 31 01:09:40 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-05-31T01:09:40.9795208Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
2022-05-31T01:09:40.9796473Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2022-05-31T01:09:40.9797690Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2022-05-31T01:09:40.9798996Z May 31 01:09:40 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-05-31T01:09:40.9800435Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
2022-05-31T01:09:40.9826143Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
2022-05-31T01:09:40.9831553Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2022-05-31T01:09:40.9833036Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2022-05-31T01:09:40.9834496Z May 31 01:09:40 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-05-31T01:09:40.9835998Z May 31 01:09:40 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)
2022-05-31T01:09:40.9837976Z May 31 01:09:40 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-31T01:09:40.9839683Z May 31 01:09:40 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-31T01:09:40.9841296Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
2022-05-31T01:09:40.9842665Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
2022-05-31T01:09:40.9844019Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
2022-05-31T01:09:40.9845433Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
2022-05-31T01:09:40.9846782Z May 31 01:09:40 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-31T01:09:40.9847986Z May 31 01:09:40 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-31T01:09:40.9849006Z May 31 01:09:40 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-31T01:09:40.9850121Z May 31 01:09:40 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-31T01:09:40.9851177Z May 31 01:09:40 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-31T01:09:40.9884180Z May 31 01:09:40 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-31T01:09:40.9885811Z May 31 01:09:40 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-31T01:09:40.9887174Z May 31 01:09:40 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-31T01:09:40.9888338Z May 31 01:09:40 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-31T01:09:40.9889683Z May 31 01:09:40 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-31T01:09:40.9891114Z May 31 01:09:40 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-31T01:09:40.9892280Z May 31 01:09:40 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-31T01:09:40.9893486Z May 31 01:09:40 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-31T01:09:40.9895060Z May 31 01:09:40 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-31T01:09:40.9896248Z May 31 01:09:40 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-31T01:09:40.9897305Z May 31 01:09:40 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-31T01:09:40.9898386Z May 31 01:09:40 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-31T01:09:40.9899491Z May 31 01:09:40 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-31T01:09:40.9900857Z May 31 01:09:40 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-31T01:09:40.9902126Z May 31 01:09:40 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-31T01:09:40.9903383Z May 31 01:09:40 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-31T01:09:40.9904712Z May 31 01:09:40 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-31T01:09:40.9905894Z May 31 01:09:40 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-31T01:09:40.9907066Z May 31 01:09:40 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-31T01:09:40.9908362Z May 31 01:09:40 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-31T01:09:40.9909657Z May 31 01:09:40 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2022-05-31T01:09:40.9928471Z May 31 01:09:40 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2022-05-31T01:09:40.9929941Z May 31 01:09:40 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2022-05-31T01:09:40.9931124Z May 31 01:09:40 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2022-05-31T01:09:40.9932363Z May 31 01:09:40 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-05-31T01:09:40.9933928Z May 31 01:09:40 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2022-05-31T01:09:40.9935719Z May 31 01:09:40 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-31T01:09:40.9937392Z May 31 01:09:40 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-31T01:09:40.9939028Z May 31 01:09:40 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:300)
2022-05-31T01:09:40.9940474Z May 31 01:09:40 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:290)
2022-05-31T01:09:40.9941920Z May 31 01:09:40 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:281)
2022-05-31T01:09:40.9943583Z May 31 01:09:40 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-05-31T01:09:40.9963720Z May 31 01:09:40 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-05-31T01:09:40.9965234Z May 31 01:09:40 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-05-31T01:09:40.9966573Z May 31 01:09:40 	at jdk.internal.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
2022-05-31T01:09:40.9967827Z May 31 01:09:40 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-31T01:09:40.9969032Z May 31 01:09:40 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2022-05-31T01:09:40.9970317Z May 31 01:09:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-05-31T01:09:40.9971736Z May 31 01:09:40 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-31T01:09:40.9973107Z May 31 01:09:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-05-31T01:09:40.9984726Z May 31 01:09:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-31T01:09:40.9986220Z May 31 01:09:40 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-31T01:09:40.9987568Z May 31 01:09:40 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-31T01:09:40.9988735Z May 31 01:09:40 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-31T01:09:40.9989905Z May 31 01:09:40 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-31T01:09:40.9990950Z May 31 01:09:40 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-31T01:09:40.9992022Z May 31 01:09:40 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-31T01:09:40.9993115Z May 31 01:09:40 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-31T01:09:40.9994231Z May 31 01:09:40 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-31T01:09:40.9995518Z May 31 01:09:40 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-31T01:09:40.9996634Z May 31 01:09:40 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-31T01:09:40.9997845Z May 31 01:09:40 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-31T01:09:40.9998811Z May 31 01:09:40 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-31T01:09:40.9999944Z May 31 01:09:40 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-31T01:09:41.0001009Z May 31 01:09:40 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-31T01:09:41.0002002Z May 31 01:09:40 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-31T01:09:41.0002978Z May 31 01:09:40 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-31T01:09:41.0004130Z May 31 01:09:40 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-31T01:09:41.0005197Z May 31 01:09:40 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-31T01:09:41.0005985Z May 31 01:09:40 	... 5 more
2022-05-31T01:09:41.0006789Z May 31 01:09:40 Caused by: java.io.IOException: java.lang.InterruptedException
2022-05-31T01:09:41.0008215Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
2022-05-31T01:09:41.0009867Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
2022-05-31T01:09:41.0011438Z May 31 01:09:40 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
2022-05-31T01:09:41.0013334Z May 31 01:09:40 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
2022-05-31T01:09:41.0042927Z May 31 01:09:40 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-05-31T01:09:41.0044231Z May 31 01:09:40 	at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
2022-05-31T01:09:41.0046049Z May 31 01:09:40 	at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
2022-05-31T01:09:41.0047785Z May 31 01:09:40 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
2022-05-31T01:09:41.0049057Z May 31 01:09:40 	at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
2022-05-31T01:09:41.0050464Z May 31 01:09:40 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-05-31T01:09:41.0051734Z May 31 01:09:40 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
2022-05-31T01:09:41.0052993Z May 31 01:09:40 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-05-31T01:09:41.0054107Z May 31 01:09:40 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-05-31T01:09:41.0055223Z May 31 01:09:40 	at java.base/java.lang.Thread.run(Thread.java:829)
2022-05-31T01:09:41.0056132Z May 31 01:09:40 Caused by: java.lang.InterruptedException
2022-05-31T01:09:41.0057176Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:385)
2022-05-31T01:09:41.0058442Z May 31 01:09:40 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
2022-05-31T01:09:41.0060030Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildFutureWriteRequest$4(ChannelStateWriteRequest.java:113)
2022-05-31T01:09:41.0061709Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.cancel(ChannelStateWriteRequest.java:253)
2022-05-31T01:09:41.0063457Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.lambda$cleanupRequests$1(ChannelStateWriteRequestExecutorImpl.java:117)
2022-05-31T01:09:41.0065173Z May 31 01:09:40 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-05-31T01:09:41.0066224Z May 31 01:09:40 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-05-31T01:09:41.0067624Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.cleanupRequests(ChannelStateWriteRequestExecutorImpl.java:115)
2022-05-31T01:09:41.0069264Z May 31 01:09:40 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-05-31T01:09:41.0070386Z May 31 01:09:40 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-05-31T01:09:41.0071423Z May 31 01:09:40 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
2022-05-31T01:09:41.0072762Z May 31 01:09:40 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:80)
2022-05-31T01:09:41.0073884Z May 31 01:09:40 	... 1 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36190&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-01 12:32:03.0,,,,,,,,,,"0|z12va8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerITCase. testStopWithSavepointFailOnStop failed with FAIL_ON_CHECKPOINT_COMPLETE ,FLINK-27869,13447919,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,hxbks2ks,hxbks2ks,01/Jun/22 12:25,14/Jun/22 12:07,04/Jun/24 20:51,14/Jun/22 12:07,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,test-stability,,,,"
{code:java}
8.6667579Z May 31 01:18:28 [ERROR] org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testStopWithSavepointFailOnStop  Time elapsed: 0.235 s  <<< ERROR!
2022-05-31T01:18:28.6668521Z May 31 01:18:28 org.apache.flink.util.FlinkException: Stop with savepoint operation could not be completed.
2022-05-31T01:18:28.6669435Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onLeave(StopWithSavepoint.java:125)
2022-05-31T01:18:28.6670470Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1171)
2022-05-31T01:18:28.6671487Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToRestarting(AdaptiveScheduler.java:849)
2022-05-31T01:18:28.6672481Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.FailureResultUtil.restartOrFail(FailureResultUtil.java:28)
2022-05-31T01:18:28.6673459Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.StopWithSavepoint.onFailure(StopWithSavepoint.java:151)
2022-05-31T01:18:28.6674502Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.updateTaskExecutionState(StateWithExecutionGraph.java:363)
2022-05-31T01:18:28.6675603Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$updateTaskExecutionState$4(AdaptiveScheduler.java:496)
2022-05-31T01:18:28.6677238Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.State.tryCall(State.java:137)
2022-05-31T01:18:28.6678573Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.updateTaskExecutionState(AdaptiveScheduler.java:493)
2022-05-31T01:18:28.6679517Z May 31 01:18:28 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-05-31T01:18:28.6680538Z May 31 01:18:28 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-05-31T01:18:28.6681304Z May 31 01:18:28 	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
2022-05-31T01:18:28.6682058Z May 31 01:18:28 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-31T01:18:28.6682800Z May 31 01:18:28 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-31T01:18:28.6683611Z May 31 01:18:28 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-05-31T01:18:28.6684559Z May 31 01:18:28 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-31T01:18:28.6685483Z May 31 01:18:28 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-05-31T01:18:28.6686343Z May 31 01:18:28 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-31T01:18:28.6687224Z May 31 01:18:28 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-31T01:18:28.6688093Z May 31 01:18:28 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-31T01:18:28.6688877Z May 31 01:18:28 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-31T01:18:28.6689602Z May 31 01:18:28 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-31T01:18:28.6690313Z May 31 01:18:28 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-31T01:18:28.6691045Z May 31 01:18:28 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-31T01:18:28.6691782Z May 31 01:18:28 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-31T01:18:28.6692535Z May 31 01:18:28 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-31T01:18:28.6693283Z May 31 01:18:28 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-31T01:18:28.6694031Z May 31 01:18:28 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-31T01:18:28.6694733Z May 31 01:18:28 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-31T01:18:28.6695400Z May 31 01:18:28 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-31T01:18:28.6696100Z May 31 01:18:28 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-31T01:18:28.6696807Z May 31 01:18:28 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-31T01:18:28.6697495Z May 31 01:18:28 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-31T01:18:28.6698183Z May 31 01:18:28 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-31T01:18:28.6698850Z May 31 01:18:28 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-31T01:18:28.6699493Z May 31 01:18:28 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-31T01:18:28.6700184Z May 31 01:18:28 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-31T01:18:28.6701039Z May 31 01:18:28 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-31T01:18:28.6702033Z May 31 01:18:28 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-31T01:18:28.6718393Z May 31 01:18:28 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-31T01:18:28.6719334Z May 31 01:18:28 Caused by: java.lang.RuntimeException: FAIL_ON_CHECKPOINT_COMPLETE
2022-05-31T01:18:28.6720311Z May 31 01:18:28 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase$DummySource.notifyCheckpointComplete(AdaptiveSchedulerITCase.java:363)
2022-05-31T01:18:28.6722132Z May 31 01:18:28 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:126)
2022-05-31T01:18:28.6724882Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104)
2022-05-31T01:18:28.6725781Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145)
2022-05-31T01:18:28.6761885Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:479)
2022-05-31T01:18:28.6764133Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:413)
2022-05-31T01:18:28.6765072Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1407)
2022-05-31T01:18:28.6771019Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$15(StreamTask.java:1348)
2022-05-31T01:18:28.6771910Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$18(StreamTask.java:1387)
2022-05-31T01:18:28.6772850Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
2022-05-31T01:18:28.6773693Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
2022-05-31T01:18:28.6774423Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
2022-05-31T01:18:28.6775292Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
2022-05-31T01:18:28.6776191Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
2022-05-31T01:18:28.6776992Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
2022-05-31T01:18:28.6777759Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:856)
2022-05-31T01:18:28.6778461Z May 31 01:18:28 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:781)
2022-05-31T01:18:28.6779163Z May 31 01:18:28 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-05-31T01:18:28.6779852Z May 31 01:18:28 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
2022-05-31T01:18:28.6780484Z May 31 01:18:28 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-05-31T01:18:28.6781093Z May 31 01:18:28 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-05-31T01:18:28.6781626Z May 31 01:18:28 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36190&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27972,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 12:07:41 UTC 2022,,,,,,,,,,"0|z12v9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 12:05;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36262&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","14/Jun/22 12:07;chesnay;Pretty sure that FLINK-27972 is the root cause.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden running job check before triggering savepoints or savepoint upgrades,FLINK-27868,13447914,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,01/Jun/22 11:41,24/Nov/22 01:03,04/Jun/24 20:51,04/Jul/22 07:09,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Even if the job is in RUNNING state, often not all subtasks are yet running which leads to savepoint upgrade / savepoint trigger failures. We should harden the isRunning check we use to include subtask states as well.

This suggestion is desribed more in detail by [~matyas] here: https://github.com/apache/flink-kubernetes-operator/pull/237#issuecomment-1137054088",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 04 07:09:17 UTC 2022,,,,,,,,,,"0|z12v8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 10:17;gyfora;I think the best would be to improve the JobStatus observer with the following logic:

if RUNNING status was observer we should apply a second check to verify that all tasks are indeed running.
If Yes, keep the job in RUNNING otherwise set the state to CREATED. This way we can leverage the improved running observation throughout the operator code where we already use it instead of having to inject custom logic all over the place.;;;","07/Jun/22 00:04;aitozi;I think it will be a good improvement to savepoint with the work of [https://github.com/apache/flink-kubernetes-operator/pull/255]

One question here: How about the FINISH status will be handled? IMO, the finish task will not affect the jobs RUNNING status I think (do not have to be reset to CREATED) ;;;","07/Jun/22 18:38;morhidi;This is exactly what I was thinking too


;;;","04/Jul/22 07:09;morhidi;merged to main via 015b929912479458adfad52e378778f4e6f14163;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitAggregateITCase fails on CI,FLINK-27867,13447902,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,alexanderpreuss,alexanderpreuss,01/Jun/22 10:58,21/Jun/22 12:59,04/Jun/24 20:51,21/Jun/22 12:59,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,"{code:java}
May 30 14:00:41 [ERROR] Failures: 
May 30 14:00:41 [ERROR] SplitAggregateITCase.testAggFilterClauseBothWithAvgAndCount
May 30 14:00:41 [INFO]   Run 1: PASS
May 30 14:00:41 [INFO]   Run 2: PASS
May 30 14:00:41 [INFO]   Run 3: PASS
May 30 14:00:41 [ERROR]   Run 4: Multiple Failures (2 failures)
May 30 14:00:41 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 30 14:00:41 	java.lang.AssertionError: <no message>
May 30 14:00:41 [INFO] 
May 30 14:00:41 [ERROR] SplitAggregateITCase.testAggWithFilterClause
May 30 14:00:41 [INFO]   Run 1: PASS
May 30 14:00:41 [INFO]   Run 2: PASS
May 30 14:00:41 [ERROR]   Run 3: Multiple Failures (2 failures)
May 30 14:00:41 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 30 14:00:41 	java.lang.AssertionError: <no message>
May 30 14:00:41 [ERROR]   Run 4: Multiple Failures (2 failures)
May 30 14:00:41 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 30 14:00:41 	java.lang.AssertionError: <no message>
May 30 14:00:41 [INFO] 
May 30 14:00:41 [ERROR] SplitAggregateITCase.testCountDistinct
May 30 14:00:41 [INFO]   Run 1: PASS
May 30 14:00:41 [INFO]   Run 2: PASS
May 30 14:00:41 [ERROR]   Run 3: Multiple Failures (2 failures)
May 30 14:00:41 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 30 14:00:41 	java.lang.AssertionError: <no message>
May 30 14:00:41 [ERROR]   Run 4: Multiple Failures (2 failures)
May 30 14:00:41 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 30 14:00:41 	java.lang.AssertionError: <no message>
May 30 14:00:41 [INFO] {code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36178&view=logs&j=086353db-23b2-5446-2315-18e660618ef2&t=6cd785f3-2a2e-58a8-8e69-b4a03be28843&l=15593",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27832,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 21 12:59:03 UTC 2022,,,,,,,,,,"0|z12v60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 08:32;xuyangzhong;Hi, is this duplicated with FLINK-27832?;;;","21/Jun/22 12:59;roman;Looks like it is - same resource acquisition exception and a place where a thread is stuck:

{code}
 13:52:20,922 [Cancellation Watchdog for GroupAggregate[3811] -> Calc[3812] -> Expand[3813] -> Calc[3814] (2/4)#0 (523629e4e37ae891846b7121366f6df2
  java.lang.Object.wait(Native Method)
 java.lang.Thread.join(Thread.java:1252)
 java.lang.Thread.join(Thread.java:1326)
 org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:166)
 org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
 org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
 org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
 org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$2172/1368503189.close(Unknown Source)
 org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
 org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
 org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
 org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
 org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
 org.apache.flink.runtime.taskmanager.Task$$Lambda$3693/2107042345.run(Unknown Source)
 org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
 org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
 org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
 org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
 java.lang.Thread.run(Thread.java:748)
{code}
Closing as duplicate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraOutputFormat and CassandraSink will timeout twice if the number of maximum requests is reached,FLINK-27866,13447897,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,echauchot,echauchot,01/Jun/22 10:49,01/Jun/22 10:49,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Connectors / Cassandra,,,,,0,,,,,"Both Sink and Outputformat have the same logic: they support _maxConcurrentRequests_ with a semaphore by counting the number of in-flight write requests and acquire/release semaphore permits with each new request begin/end. At close time, during the flush they wait for all the requests to be processed by acquiring all the permits set in the semaphore.

If a given write fails because there are still _maxConcurrentRequests_ in flight after the given wait period, a _TimeoutException_ will be thrown (while trying to acquire a permit from the semaphore) . Then Flink will catch this exception and close the sink/format. During the flush it will try to acquire permits from the semaphore once again and this will lead to another wait period and another {_}TimeoutException{_}. 

 

Ideally, we should wait only once and get only one TimeoutException.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-01 10:49:02.0,,,,,,,,,,"0|z12v4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add guide and example for configuring SASL and SSL in Kafka SQL connector document,FLINK-27865,13447883,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,01/Jun/22 09:35,24/Jul/22 18:35,04/Jun/24 20:51,24/Jul/22 18:35,1.14.4,1.15.0,1.16.0,,,,,,,,,,,,1.14.6,1.15.2,1.16.0,,Connectors / Kafka,Documentation,,,,0,pull-request-available,stale-assigned,,,"Using SASL and SSL in Kafka connector is a common case and usually quite complex for new users that not quite familiar with the design of Kafka connector, so it would be helpful to add a guidance of how to enable these security options in Kafka connector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 24 18:35:07 UTC 2022,,,,,,,,,,"0|z12v1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 09:10;martijnvisser;+1 let me know if you need a reviewer!;;;","23/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","24/Jul/22 18:35;martijnvisser;Merged in

master: 5d564b1fe9dfaf9a07cc9af2726f71ed31d582ed
release-1.15: 5cc879676e8092defc750b98b4592fec10890ccf
release-1.14: d7162394ebd95bc70c987c6d9d23a7a50288f104;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated classes (jar hell) on flink-table-runtime,FLINK-27864,13447882,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gehel,gehel,01/Jun/22 09:24,01/Jun/22 09:24,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"Our build uses the [maven-duplicate-finder plugin|https://github.com/basepom/duplicate-finder-maven-plugin] to detect jar hell issues. This is finding issues with Janino being included in the flink-table-runtime jar as well as a maven dependency declared in the pom of that same artifact (see [build log|https://integration.wikimedia.org/ci/job/wikimedia-event-utilities-maven-java8-docker/219/console]). We are avoiding that issue by adding an exclusion (see [current work in progess, L66-80|https://gerrit.wikimedia.org/r/c/wikimedia-event-utilities/+/791646/34/eventutilities-flink/pom.xml#66]).

I'm not comfortable with how Flink is packaged, but I suspect a missing relocation or exclusion in the [configuration of the shade plugin|https://github.com/apache/flink/blob/release-1.15.0/flink-table/flink-table-planner/pom.xml#L311-L399] of that module.

The impact is minimal (excluding the Janino dependencies is a reasonable workaround), but this creates some level of confusion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-01 09:24:58.0,,,,,,,,,,"0|z12v1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor RecordReader and RecordIterator in table store into generic types,FLINK-27863,13447860,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,01/Jun/22 07:32,01/Jun/22 08:00,04/Jun/24 20:51,01/Jun/22 08:00,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,We'd like to introduce a `TableRead` class between file store and the users. This class should provide an iterator for `RowData` instead of `KeyValue`s. So we'll need an iterator not only for `KeyValue` but also for `RowData`.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 08:00:25 UTC 2022,,,,,,,,,,"0|z12uwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 08:00;lzljs3620320;master: a19b9e68654c885d9d44ac4c19b479dc148ec9ba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-235: Hybrid Shuffle Mode,FLINK-27862,13447855,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Weijie Guo,Weijie Guo,reswqa,01/Jun/22 06:56,02/Nov/22 13:42,04/Jun/24 20:51,05/Aug/22 02:56,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Network,,,,,0,Umbrella,,,,"Introduce a new shuffle mode can overcome some of the problems of Pipelined Shuffle and Blocking Shuffle in batch scenarios, it can make best use of available resources and minimize disk IO load.

More details see [FLIP-235|https://cwiki.apache.org/confluence/display/FLINK/FLIP-235%3A+Hybrid+Shuffle+Mode]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 06:52:10 UTC 2022,,,,,,,,,,"0|z12uvk:",9223372036854775807,"We have introduced a new Hybrid Shuffle Mode for batch executions. It combines the advantages of blocking shuffle and pipelined shuffle (in streaming mode).
- Like blocking shuffle, it does not require upstream and downstream tasks to run simultaneously, which allows executing a job with little resources.
- Like pipelined shuffle, it does not require downstream tasks to be executed after upstream tasks finish, which reduces the overall execution time of the job when given sufficient resources.
- It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies.

For more details: https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/batch/batch_shuffle",,,,,,,,,,,,,,,,,,,"06/Jun/22 14:39;aitozi;Hi [~Weijie Guo] Thanks for starting this work. I'm interested in this flip. Can I join this work and take some simple work as start;;;","07/Jun/22 06:15;Weijie Guo;Hi [~aitozi], Thank you very much for your attention, welcome to participate in it.

Let me share the current situation of this FLIP：

1、We already have a POC version in-house with some level of testing.

2、The implementation of this POC version is not exactly the same as the design in FLIP-235. For example, the spill strategy adopts all data written to disk strategy instead of selective spill strategy, etc.

3、In order to verify if there is a conflict in merging the code into the open source Flink code-base, I pushed the code to a branch on my own ([github repository|https://github.com/reswqa/flink/tree/hs-merge-from-vvr]). Since part of the code is going to be discarded in the new design, it is not pick into the test branch, so this branch cannot actually run. But it already contains the core implementation of our POC version.

4、If you have any other questions, you are very welcome to communicate with me offline.;;;","07/Jun/22 06:27;xtsong;Hi [~aitozi],
Thanks for offering. I see several ways that you may help.
* Reviewing the PRs will be definitely appreciated.
* You may also help with transforming the PoC implementation into PRs, which involves some design changes w.r.t. the FLIP as well as improving the code quality and adding test cases. For this part if you want, you may first take a look at the PoC codes, and we can set up a call discussing how the workload can be split. I believe there are some tasks that can be worked on in parallel.;;;","07/Jun/22 06:57;aitozi;Thanks [~Weijie Guo] , [~xtsong] for your kindness guide, I will take a look at the PoC work first and will reach you out for further discussion.;;;","13/Jun/22 06:52;aitozi;Hi [~xtsong] , I have an offline discussion with [~Weijie Guo]. And I will try to start parallel work from ticket3: Introduce HsDataBuffer. Can you help assign the ticket ? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce UserResourceManager to manage user defined resource,FLINK-27861,13447852,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,01/Jun/22 06:50,20/Jun/22 05:50,04/Jun/24 20:51,20/Jun/22 05:50,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 05:50:06 UTC 2022,,,,,,,,,,"0|z12uuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 05:50;jark;Fixed in master: 7032ce386d7e765ad9a295732f2f909db97dce1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
List the CSS/docs dependencies in the NOTICE,FLINK-27860,13447847,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,wangyang0918,wangyang0918,01/Jun/22 05:57,01/Jun/22 07:07,04/Jun/24 20:51,01/Jun/22 07:07,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,We should list the CSS/docs dependencies in the NOTICE file.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 07:07:05 UTC 2022,,,,,,,,,,"0|z12uts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 07:07;wangyang0918;Fixed via:

main: a393a1c2d232a61d7e45c734d6d3643c70465423

release-1.0: d5d7b664a09ae5d717742a879f6bc2a5d6f0f0a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive dialect should support create table whose name begins with digits ,FLINK-27859,13447820,13510724,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,01/Jun/22 02:12,19/Dec/22 07:33,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,"In hive, it support to create table whose name begins with digits. But in Flink, when using  Hive dialect to create such table like `default.3t`, it'll throw exception
{code:java}
Caused by: org.apache.flink.sql.parser.hive.impl.ParseException: Encountered "".3"" at line 1, column 8.
Was expecting one of:
    <EOF> 
    ""."" ... {code}
We should make the behavior compatible with Hive instead of throw an exception when using Hive dialect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 21 22:37:57 UTC 2022,,,,,,,,,,"0|z12uns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive source support filter push down for parquet format,FLINK-27858,13447814,13444738,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,01/Jun/22 01:21,11/Mar/24 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,,,,,,0,,,,,"Can move on after finish [FLINK-16952|https://issues.apache.org/jira/browse/FLINK-16952]

 ",,,,,,,,,,,,FLINK-16952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-06-01 01:21:34.0,,,,,,,,,,"0|z12umg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive source support filter push down for orc format,FLINK-27857,13447812,13444738,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,01/Jun/22 01:12,08/Dec/22 13:06,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 21 22:37:57 UTC 2022,,,,,,,,,,"0|z12um0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding pod template without spec crashes job manager,FLINK-27856,13447763,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,kevin_123,jeesmon,jeesmon,31/May/22 18:39,12/Aug/22 08:32,04/Jun/24 20:51,12/Aug/22 08:32,,,,,,,,,,,,,,,1.15.2,1.16.0,,,Deployment / Kubernetes,,,,,0,pull-request-available,,,,"While trying to add Pod annotation through pod template in FlinkDeployment, taskmanager was keep crashing.

Pod template that I used:
{code:java}
  taskManager:
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          iam.amazonaws.com/role: fake-role-arn
{code}
It created below ConfigMap and mounted to the deployment:
{code:java}
apiVersion: v1
data:
  taskmanager-pod-template.yaml: |
    ---
    apiVersion: ""v1""
    kind: ""Pod""
    metadata:
      annotations:
        iam.amazonaws.com/role: ""fake-role-arn""
kind: ConfigMap
{code}
Looks like missing ""spec"" stanza in pod template resulted in the crash and I couldn't find any documentation that ""spec"" is required for pod template even for just adding metadata annotations.

Adding below worked fine
{code:java}
  taskManager:
    podTemplate:
      apiVersion: v1
      kind: Pod
      metadata:
        annotations:
          iam.amazonaws.com/role: fake-role-arn
      spec: {}
{code}
Corresponding ConfigMap
{code:java}
apiVersion: v1
data:
  taskmanager-pod-template.yaml: |
    ---
    apiVersion: ""v1""
    kind: ""Pod""
    metadata:
      annotations:
        iam.amazonaws.com/role: ""fake-role-arn""
    spec:
      containers: []
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 08:32:13 UTC 2022,,,,,,,,,,"0|z12ub4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 02:28;wangyang0918;We will have a NPE in the JobManager if the {{spec}} is not configured in the pod template. This ticket needs to be fixed in the Flink project.

 
{code:java}
2022-06-02 02:26:11,864 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Could not start the ResourceManager akka.tcp://flink@basic-example.default:6123/user/rpc/resourcemanager_1
        at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:223) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:612) ~[flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:611) ~[flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_ee3c2e1e-3cab-4d23-8b5d-cdf80a18084e.jar:1.15.0]
        at java.util.concurrent.ForkJoinTask.doExec(Unknown Source) [?:?]
        at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source) [?:?]
        at java.util.concurrent.ForkJoinPool.scan(Unknown Source) [?:?]
        at java.util.concurrent.ForkJoinPool.runWorker(Unknown Source) [?:?]
        at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
Caused by: org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException: Cannot initialize resource provider.
        at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.initialize(ActiveResourceManager.java:158) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:241) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:218) ~[flink-dist-1.15.0.jar:1.15.0]
        ... 25 more
Caused by: java.lang.NullPointerException
        at org.apache.flink.kubernetes.utils.KubernetesUtils.loadPodFromTemplateFile(KubernetesUtils.java:421) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.initializeInternal(KubernetesResourceManagerDriver.java:115) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.resourcemanager.active.AbstractResourceManagerDriver.initialize(AbstractResourceManagerDriver.java:81) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager.initialize(ActiveResourceManager.java:156) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.resourcemanager.ResourceManager.startResourceManagerServices(ResourceManager.java:241) ~[flink-dist-1.15.0.jar:1.15.0]
        at org.apache.flink.runtime.resourcemanager.ResourceManager.onStart(ResourceManager.java:218) ~[flink-dist-1.15.0.jar:1.15.0] {code};;;","09/Aug/22 11:17;kevin_123;I have made a PR to fix this error. Please assign this ticket to me.[~wangyang0918] ;;;","12/Aug/22 08:32;wangyang0918;Fixed via:

master: 5e554c17742e1809188aa702daca3f435dcab778

release-1.15: 3d910dc8381c6965bfce6d948c7cac27cdde1a3c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job Manager fails to recover with S3 storage and HA enabled,FLINK-27855,13447741,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,dataguysc,dataguysc,31/May/22 16:54,01/Jun/22 12:48,04/Jun/24 20:51,01/Jun/22 12:48,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Flink version: 1.15  with Native Integration K8s operator mode: https://github.com/apache/flink-kubernetes-operator

Steps to replicate
1. Enable HA and mention S3 recovery path in flink configuration property: high-availability.storageDir
2. Create the flink application deployment and let it run for some time to generate checkpoints
3. Delete the flink application deployment
4. Recreate once again and the job manager pod doesn’t come up complaining about S3 recovery cleanup, error is described below

Note that the above steps go through fine if AWS EFS is being used instead of S3 for HA

Error Traceback:
{code:java}
2022-05-31 16:39:44,332 WARN  org.apache.flink.runtime.dispatcher.cleanup.DefaultResourceCleaner [] - Cleanup of BlobServer failed for job 00000000000000000000000000000000 due to a CompletionException: java.io.IOException: java.io.IOException: Error while cleaning up the BlobStore for job 00000000000000000000000000000000

2022-05-31 16:42:56,955 WARN  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Ignoring JobGraph submission (00000000000000000000000000000000) because the job already reached a globally-terminal state (i.e. FAILED, CANCELED, FINISHED) in a previous execution.
2022-05-31 16:42:57,026 ERROR              [] - Error while processing events :
org.apache.flink.util.FlinkException: Failed to execute job
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2108) ~[flink-dist-1.15.0.jar:1.15.0]
Caused by: org.apache.flink.runtime.client.DuplicateJobSubmissionException: Job has already been submitted.
	at org.apache.flink.runtime.client.DuplicateJobSubmissionException.ofGloballyTerminated(DuplicateJobSubmissionException.java:35) ~[flink-dist-1.15.0.jar:1.15.0]
2022-05-31 16:42:57,130 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application CANCELED:
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED
	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$unwrapJobResultException$6(ApplicationDispatcherBootstrap.java:389) ~[flink-dist-1.15.0.jar:1.15.0]
	at java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source) [?:?]
Caused by: org.apache.flink.client.deployment.application.UnsuccessfulExecutionException: Application Status: CANCELED
	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:71) ~[flink-dist-1.15.0.jar:1.15.0]
	... 56 more
Caused by: org.apache.flink.runtime.client.JobCancellationException: Job was cancelled.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.client.deployment.application.UnsuccessfulExecutionException.fromJobResult(UnsuccessfulExecutionException.java:60) ~[flink-dist-1.15.0.jar:1.15.0]
	... 56 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 12:47:35 UTC 2022,,,,,,,,,,"0|z12u68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 12:47;dataguysc;Recovery files weren't deleted during creation/deletion of flink deployment due to s3 permission issues , can't replicate the issue after giving required permissions to S3 from AWS EKS cluster pods using IRSA method;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor FlinkImageBuilder and FlinkContainerBuilder,FLINK-27854,13447723,13437797,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,31/May/22 15:45,05/Jul/22 02:34,04/Jun/24 20:51,05/Jul/22 02:34,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,"{{FlinkImageBuilder}} and {{FlinkContainerBuilder}} are tightly coupled with using flink-dist to create Flink images on the flight. Some configuration is currently ""hardcoded"" in those classes and cannot be controlled by the user creating the {{{}FlinkContainerTestEnvironment{}}}.  For externalized connectors, we need the setup to be more flexible and to also work with the existing images published to GHCR, without relying on {{{}flink-dist{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 05 02:34:30 UTC 2022,,,,,,,,,,"0|z12u28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 02:34;renqs;Merged in master 45ad242beb7a6d87df239a9069728c1594c905c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-229: Introduces Join Hint for Flink SQL Batch Job,FLINK-27853,13447716,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,xuyangzhong,xuyangzhong,31/May/22 15:10,30/Sep/22 08:57,04/Jun/24 20:51,30/Sep/22 08:57,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,,,,,"Introduce join hint for Flink SQL batch job. Join hint is provided for users to intervene in optimizing the plan, to avoid the optimizer use the wrong join strategy.

More details can be found here: https://cwiki.apache.org/confluence/display/FLINK/FLIP-229%3A+Introduces+Join+Hint+for+Flink+SQL+Batch+Job",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-31 15:10:13.0,,,,,,,,,,"0|z12u0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Contribute k8s operator to OperatorHub,FLINK-27852,13447705,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tedhtchang,mbalassi,mbalassi,31/May/22 13:11,13/Dec/22 01:36,04/Jun/24 20:51,08/Dec/22 23:16,,,,,,,,,,,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,Let us contribute the operator to https://operatorhub.io/ for visibility within the k8s community after the 1.0 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 08 23:16:16 UTC 2022,,,,,,,,,,"0|z12ty8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jun/22 22:56;jbusche;i [~mbalassi], [~tedhtchang] and I would be glad to try to help on this - Looking at it over the weekend, hoping to touch base early next week on it.;;;","28/Jun/22 17:22;mbalassi;Thanks, [~jbusche] and [~tedhtchang] assigned the ticket accordingly.;;;","09/Jul/22 08:24;mbalassi;[~tedchang] and [~jbusche] opened a PR to contribute this:

https://github.com/k8s-operatorhub/community-operators/pull/1431;;;","08/Dec/22 23:16;morhidi;fixed via:
1cfe67e0f9067d70d53936c13a341a2600ea2960;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join can't access the pk from source table,FLINK-27851,13447643,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,31/May/22 08:07,27/Dec/23 14:55,04/Jun/24 20:51,27/Dec/23 14:55,1.16.0,,,,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,0,pull-request-available,stale-assigned,,,"If the source table contains a pk and enable mini-batch, the join can't get the pk information in source table. The root cause is that the `FlinkRelMdUniqueKeys` is not override the function with arg MiniBatchAssigner.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-20036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 27 14:54:47 UTC 2023,,,,,,,,,,"0|z12tkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","27/Dec/23 14:54;lsy;Merged to master branch: dd16a4c07b2f8c96740fb522cb54cfd1d5a5e835;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive dialect supports ""add file xxx""",FLINK-27850,13447634,13510724,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,31/May/22 07:19,19/Dec/22 07:32,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,stale-assigned,,,"""ADD \{ FILE | FILES } resource_name [ ... ]"" allow user to add resources to Flink cluster , and thus user can run their job with the added resources. 

Such syntax has been supported by Hive/Spark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 21 22:37:58 UTC 2022,,,,,,,,,,"0|z12tio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harden correctness for non-deterministic updates present in the changelog pipeline,FLINK-27849,13447630,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lincoln.86xy,lincoln.86xy,lincoln.86xy,31/May/22 06:54,11/Mar/24 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,Table SQL / Runtime,,,,,0,,,,,"There commonly exists updates(which means not only RowKind.INSERT messages) in a streaming pipeline, then wrong results or error may occurs when use some non-deterministic functions or operations.

It is a long lived issue since the first day that flink sql was available in streaming, but it still not totally be eliminated though some efforts have been taken.

We should detect all the non-deterministic operations in the changelog pipelines, raise an error to tell users the risk and also add an mechanism that can process such a issue if a user is willing to pay some cost(probably introduce the state).

All non-deterministic operations include builtin temporal functions(now, current_timestamp...), UUID, RAND... 
or user defined non-deterministic functions (override isDeterministic return false)
or a lookup join on a lookup source which data may change over time

or a cdc-source with meta data field (described in FLINK-28242)

 

 

====== Solution  ======

Will introduce a physical plan checker to validate if there's any non-deterministic updates which may cause wrong result, and also a physical plan rewriter to eliminate the non determinism generated by lookup join node (which we think is commonly used in sql, and hard to solve by users themselves).

For implementation steps, the main changes may include 4 parts:
 # [preparing work] Adds an internal postOptimize method for physical dag processing
 # Introduces a `StreamNonDeterministicPlanResolver` to validate if there's any non-deterministic updates which may cause wrong result and rewrite lookup join node with materialization (to eliminate the non determinism generated by lookup join node)
 # Implements a new lookup join operator (sync mode only) with state to eliminate the non determinism
 # [optimization] SinkUpsertMaterializer should be aware of the input upsertKey if it is not empty

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,FLINK-27639,,,,,,,FLINK-28242,,,,,,,FLINK-27639,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,,Fri Jul 29 06:42:12 UTC 2022,,,,,,,,,,"0|z12ths:",9223372036854775807,"For complex streaming jobs, now it's possible to detect and resolve potential correctness issues before running.",,,,,,,,,,,,,,,,,,,"29/Jul/22 06:42;lincoln.86xy;Thanks to [~jark]  [~godfrey]  [~jinsong] for all your contributions and valuable suggestions during the long discussion offline, I have written a simple doc of the final solution in:  https://docs.google.com/document/d/1uFBYqyXJxuNhhB37ydGniQMF2JkS3DtGZMU4oBTJ03U

Before releasing 1.16,  we also need to prepare a formal user documentation FLINK-28738.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ZooKeeperLeaderElectionDriver keeps writing leader information, using up zxid",FLINK-27848,13447626,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,xtsong,xtsong,31/May/22 06:46,03/Feb/23 08:29,04/Jun/24 20:51,03/Feb/23 08:29,1.15.0,1.16.1,1.17.0,,,,,,,,,,,,1.15.1,1.16.2,1.17.0,,Runtime / Coordination,,,,,0,pull-request-available,,,,"After a leadership change, the new leader may keeps writing its information (which is identical) to ZK, causing the zxid on ZK quickly used up.

The problem is that, in {{ZooKeeperLeaderElectionDriver#retrieveLeaderInformationFromZooKeeper}}, {{leaderElectionEventHandler.onLeaderInformationChange(LeaderInformation.empty())}} is called no matter {{childData}} is {{null}} or not. In case of non-null, this will cause the driver keeps re-writing the leader information to ZK.

The problem was introduced in FLINK-24038, and only affects the legacy {{ZooKeeperHaServices}}. Thus, only 1.15 are affected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 03 08:29:01 UTC 2023,,,,,,,,,,"0|z12tgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 02:23;reswqa;Hi [~xtsong] , I have prepared a pull request for this ticket, can you help to review it?;;;","02/Jun/22 03:00;xtsong;Fixed in:
- release-1.15: 73a33ab5f25dd5ac7e8adb1521c092a8aedcc736;;;","01/Feb/23 17:04;mapohl;I'm reopening this issue to provide forward(?)ports for 1.16 and 1.17.

Refactoring the leader election for FLIP-285/FLINK-26522 is kind of tricky. I'm trying to slice the code changes into meaningful commits (and ideally dedicated PRs) to make the review process easier.

I ran into this issue when refactoring the code and merging classes into one which also required adapting tests. This revealed the inconsistency/bug in the ZooKeeperLeaderElectionDriver implementation. Merging the bugfixes into 1.17 and 1.16 makes the other changes more reasonable/consistent.

More specifically, this bug was revealed in \{{ZooKeeperLeaderElectionTest.testLeaderShouldBeCorrectedWhenOverwritten}} when changing from the deprecated {{NodeCache}} to {{{}CuratorCache{}}}. The new {{CuratorCacheListener}} allows to be more selective on whether we expect a node creation or change which causes a test failure. The previous test implementation worked because we sent the 2nd write operation after writing the leaderinformation which caused a node-change event and, after all, made the test pass.;;;","03/Feb/23 08:29;mapohl;master: d76214de330fea485f2971b51c172420dfdf500b
1.16: 5441cef7b0310965822f4bca0f12bf63790c4d69
1.15: 73a33ab5f25dd5ac7e8adb1521c092a8aedcc736 (copied over from Xintong's comment above);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SchemaManager supports schema evolution,FLINK-27847,13447594,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,31/May/22 02:13,02/Dec/22 05:40,04/Jun/24 20:51,02/Dec/22 05:40,,,,,,,,,,,,,,,table-store-0.3.0,,,,Table Store,,,,,0,pull-request-available,,,,"Supports:
 * Add column
 * Modify column type
 * Drop column
 * Rename column",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 02 05:40:47 UTC 2022,,,,,,,,,,"0|z12t9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/22 05:40;lzljs3620320;master: 36ed2c37201cabea3779dcb95b9a425f03a80d8e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema evolution for data file reading,FLINK-27846,13447593,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,31/May/22 02:12,29/Nov/22 06:12,04/Jun/24 20:51,28/Nov/22 12:17,,,,,,,,,,,,,,,table-store-0.3.0,,,,Table Store,,,,,0,pull-request-available,,,,"For file reads, we need to.
- Adjust the correspondence of specific fields
- If there is a type evolution, we need to upgrade the corresponding data",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 28 12:17:45 UTC 2022,,,,,,,,,,"0|z12t9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/22 08:28;zjureel;Hi [~lzljs3620320] [~TsReaper]

Currently flink table store writes and reads data according to the column name in avro/orc/parquet. When performing column modification ddl, such as renaming column, deleting column, and adding new column with old name, it will cause data reading errors. 

One current idea is to store the column id as the name to avro/orc/parquet. The problem with this modification is that it will cause incompatibility between the new version and the previous version of the data file. What do you think of this solution? Hope to hear from you, THX;;;","08/Nov/22 12:31;lzljs3620320;Hi [~zjureel], there is schemaId in DataFileMeta, even if there is no column id in orc, we can do schema evolution currently.;;;","09/Nov/22 01:43;zjureel;Thanks [~lzljs3620320] , then we can transform different data columns by schema id, I will try it;;;","09/Nov/22 02:36;lzljs3620320;[~zjureel] (y) Assigned to u.;;;","28/Nov/22 12:17;lzljs3620320;master: 
615473f0645c547aeb7b2922cbc7ad985946315d
9a3885cfd9033c3e2e8e2837acde958848b814d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Column Type Schema evolution,FLINK-27845,13447591,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,31/May/22 02:10,03/Feb/23 08:52,04/Jun/24 20:51,03/Feb/23 08:52,,,,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,,0,pull-request-available,,,,"After schema changes, for example, a column with int type to bigint type. We should convert the data from the old type to the new type.
 * What types are safe to convert? According to the implicitCastingRules in LogicalTypeCasts.
 * Where to convert? DataFileReader
 * How to convert? In RecordIterator.next, we can convert old RowData to new RowData if there is a schema change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 03 08:52:40 UTC 2023,,,,,,,,,,"0|z12t94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 08:23;complone;Can you tell me some details? Whether to convert the LogicalType in the DataType from the old value to the new value;;;","13/Jun/22 09:32;complone;Please assign me;;;","21/Jun/22 02:14;lzljs3620320;Hi [~complone], Before submitting a PR, it is best to discuss clearly in JIRA what it is that needs to be done and what the general idea is.;;;","03/Feb/23 08:52;lzljs3620320;master: 277b544478c7b922bb9b466a385d86f8eb07c8a0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support column type evolution in file meta,FLINK-27844,13447590,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,31/May/22 02:09,05/Jan/23 11:39,04/Jun/24 20:51,05/Jan/23 10:01,,,,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,,0,pull-request-available,,,,"In DataFileMeta, the valueStats need to be column type evolution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 05 10:01:45 UTC 2023,,,,,,,,,,"0|z12t8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 10:01;lzljs3620320;master: d031fd8e977ac754d78d256dd276b6dae595e723;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema evolution for data file meta,FLINK-27843,13447589,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,31/May/22 02:07,18/Nov/22 03:59,04/Jun/24 20:51,18/Nov/22 03:59,,,,,,,,,,,,,,,table-store-0.3.0,,,,Table Store,,,,,0,pull-request-available,,,,"There are quite a few metadata operations on DataFileMeta, such as getting the statistics of each column and the partition of the file.
We need to evolution to the latest schema based on schemaId when we get this information",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 18 03:59:11 UTC 2022,,,,,,,,,,"0|z12t8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/22 12:25;liliwei;May i have a try?;;;","18/Nov/22 03:59;lzljs3620320;master: 157966f696522f3c7e939c4cd3713b01c9081017;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename ndv to granularityNumber,FLINK-27842,13447561,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jingge,jingge,jingge,30/May/22 18:06,01/Jun/22 16:43,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"Currently ndv, which stands for ""number of distinct values"", is used in ColumnStats. First of all, it is difficult to understand the meaning. Second, it might be good to use a professional naming instead. 

 

Suggestion:

replace ndv with granularityNumber:

 

Afaik, the good news is that the method getNdv() hasn't been used within Flink which means the renaming will have very limited impact.

 

ColumnStats {

/** number of distinct values. */

@Deprecated
private final Long ndv;

 

/**Granularity refers to the level of details used to sort and separate data at column level. Highly granular data is categorized or separated very precisely. For example, the granularity number of gender columns should normally be 2. The granularity number of the month column will be 12. In the SQL world, it means the number of distinct values. */ 

private final Long granularityNumber;

 

@Deprecated
public Long getNdv()

{ return ndv; }

 

public Long getGranularityNumber()

{ return granularityNumber; }

}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-30 18:06:31.0,,,,,,,,,,"0|z12t2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB cache miss increase in 1.15,FLINK-27841,13447554,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,Feifan Wang,Feifan Wang,30/May/22 16:29,02/Jun/22 03:03,04/Jun/24 20:51,31/May/22 16:16,1.15.0,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,,,,,"I run same job with 1.12.2 and 1.15.0 , find that cpu busy much higher than 1.12. After careful comparison, I find higher cache miss in 1.15. But block cache size is same. Is this as expected ?

I know RocksDB version in 1.15 is 6.20.3 and 1.12 is 5.17.2, is this just caused by different versions of RocksDB ?

!image-2022-05-31-00-21-49-338.png|width=599,height=146!

!image-2022-05-31-00-22-45-123.png|width=594,height=175!

test information:

job type : regular join

per record size : 1000 bytes

parallelism : 160

TaskManager memory : 32G

num of TaskManager : 10",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/22 16:21;Feifan Wang;image-2022-05-31-00-21-49-338.png;https://issues.apache.org/jira/secure/attachment/13044374/image-2022-05-31-00-21-49-338.png","30/May/22 16:22;Feifan Wang;image-2022-05-31-00-22-45-123.png;https://issues.apache.org/jira/secure/attachment/13044373/image-2022-05-31-00-22-45-123.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 02 03:03:20 UTC 2022,,,,,,,,,,"0|z12t0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 16:16;martijnvisser;[~Feifan Wang] Thanks for opening the ticket. As you can see in FLINK-19710 and the relevant mailing list discussion https://lists.apache.org/thread/v9w4dm6wdqgn5b6jqjdrzfycxsdh38vj this was a deliberate decision. Hopefully this answers your question. ;;;","01/Jun/22 14:25;yunta;[~Feifan Wang] The cache miss should be related with the frequency of data flush and compaction, could you check whether the behavior is the same as before after bumping Flink version.;;;","02/Jun/22 03:03;Feifan Wang;Thanks [~martijnvisser] , I will read the ticket and discussion you mentioned later.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.DisconnectException,FLINK-27840,13447546,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijixiang,lijixiang,30/May/22 15:09,31/May/22 16:18,04/Jun/24 20:51,,1.13.0,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"|2022-05-30 21:12:05,430 INFO org.apache.flink.kafka.shaded.org.apache.kafka.clients.FetchSessionHandler [] - [Consumer clientId=consumer-scene-new-abt2-39, groupId=scene-new-abt2] Error sending fetch request (sessionId=1409434688, epoch=6) to node 8: {}. org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.DisconnectException: null|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 16:18:59 UTC 2022,,,,,,,,,,"0|z12sz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 16:18;martijnvisser;[~lijixiang] These type of errors usually imply some type of network issues between Flink and your Kafka cluster. I don't think this is a Flink bug. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.InterruptException,FLINK-27839,13447545,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijixiang,lijixiang,30/May/22 15:09,13/Sep/22 07:33,04/Jun/24 20:51,,1.13.0,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"2022-05-30 21:06:53,995 WARN  org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer [] - Error closing producer.
org.apache.flink.kafka.shaded.org.apache.kafka.common.errors.InterruptException: java.lang.InterruptedException
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:1217) ~[flink-sql-connector-kafka_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:1176) ~[flink-sql-connector-kafka_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.close(FlinkKafkaInternalProducer.java:172) ~[flink-sql-connector-kafka_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:949) ~[flink-sql-connector-kafka_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:861) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:840) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:753) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:659) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:620) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: java.lang.InterruptedException
    at java.lang.Object.wait(Native Method) ~[?:1.8.0_262]
    at java.lang.Thread.join(Thread.java:1252) ~[?:1.8.0_262]
    at java.lang.Thread.join(Thread.java:1326) ~[?:1.8.0_262]
    at org.apache.flink.kafka.shaded.org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:1215) ~[flink-sql-connector-kafka_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
    ... 13 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 16:19:03 UTC 2022,,,,,,,,,,"0|z12syw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 16:19;martijnvisser;[~lijixiang] These type of errors usually imply some type of network issues between Flink and your Kafka cluster. I don't think this is a Flink bug. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support timeout mechansim for executeStatement API,FLINK-27838,13447520,13478114,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,30/May/22 12:48,14/Dec/22 11:50,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-30 12:48:28.0,,,,,,,,,,"0|z12stc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support statement set in the SQL Gateway,FLINK-27837,13447519,13478114,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,fsk119,fsk119,30/May/22 12:45,21/Dec/22 08:19,04/Jun/24 20:51,21/Dec/22 08:19,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 21 08:19:24 UTC 2022,,,,,,,,,,"0|z12st4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/22 08:19;fsk119;Merged into master: c5d2534e49f165db862aa0b949355781689dc7ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDBMapState iteration may stop too early for var-length prefixes,FLINK-27836,13447477,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nkruber,nkruber,30/May/22 08:59,01/Dec/22 15:29,04/Jun/24 20:51,,1.13.6,1.14.4,1.15.0,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,,,,,"A similar, yet orthogonal, issue to https://issues.apache.org/jira/browse/FLINK-11141 is that the iterators used in RocksDBMapState iterate over everything with a matching prefix of flink-key and namespace. With var-length serializers for either of them, however, it may return data for unrelated keys and/or namespaces.
It looks like the built-in serializers of Flink are not affected though since they use a var-length encoding that is prefixed with the object's length and thus different lengths will not have the same prefix. More exotic serializers, e.g. relying on a terminating NUL character, may expose the above-mentioned behaviour, though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-11141,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 01 15:29:38 UTC 2022,,,,,,,,,,"0|z12sjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 03:04;Yanfei Lei;IIUC, this issue doesn't actually exist because ""flink use a var-length encoding that is prefixed with the object's length and thus different lengths will not have the same prefix"", I plan to close this ticket, we can reopen it if the issue does arise.;;;","01/Dec/22 15:29;nkruber;Unfortunately, it's not that simple because Flink doesn't have control over the serializer that is used here. Just because Flink's own serializers are (by chance) correct here, doesn't mean that an arbitrary serializer for user data is. And serializer choice is at the user's convenience...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce LeafPredicate interface and children method in Predicate,FLINK-27835,13447461,13447187,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,30/May/22 07:46,01/Jun/22 07:04,04/Jun/24 20:51,01/Jun/22 07:04,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"We'd like to expose a more simple interface in the row data abstraction layer. Instead of the {{withPartitionFilter}}, {{withKeyFIlter}} and {{withValueFilter}} methods in {{FileStoreScan}}, we'd like to introduce {{RowDataScan}} containing only one {{withFilter}} method. We'll enclose the logic to extract partition predicate or key predicate in this method.

To extract partition predicate from a big predicate object we'll need to look into this predicate tree. That's why we'll need {{children}} method and {{LeafPredicate}} interface to help us represent this tree structure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 07:04:26 UTC 2022,,,,,,,,,,"0|z12sg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 07:04;lzljs3620320;master: ff81dcab14c2e1345c35b08ed443c1eeb465a298;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator dockerfile could not work with podman,FLINK-27834,13447431,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,wangyang0918,wangyang0918,30/May/22 02:38,24/Nov/22 01:03,04/Jun/24 20:51,30/May/22 14:15,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"[1/2] STEP 16/19: COPY *.git ./.git

Error: error building at STEP ""COPY *.git ./.git"": checking on sources under ""/root/FLINK/release-1.0-rc2/flink-kubernetes-operator-1.0.0"": Rel: can't make  relative to /root/FLINK/release-1.0-rc2/flink-kubernetes-operator-1.0.0; copier: stat: [""/*.git""]: no such file or directory

 

podman version
Client:       Podman Engine
Version:      4.0.2
API Version:  4.0.2

 

 

I think the root cause is ""*.git"" is not respected by podman. Maybe we could simply copy the whole directory when building the image.

 
{code:java}
WORKDIR /app

COPY . .

RUN --mount=type=cache,target=/root/.m2 mvn -ntp clean install -pl !flink-kubernetes-docs -DskipTests=$SKIP_TESTS {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 14:15:27 UTC 2022,,,,,,,,,,"0|z12s9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 02:40;wangyang0918;[~gyfora] Given that the copied directory is only used for ephemeral maven build, do you have any concern on this?;;;","30/May/22 02:51;gyfora;I have bad feelings about this, it might lead to accidentally leaking credentials from local working directories.;;;","30/May/22 03:03;wangyang0918;Do you mean we might accidentally bundle the credentials into the image?;;;","30/May/22 03:05;gyfora;yes, the user can have anything in their local directory by accident;;;","30/May/22 03:06;gyfora;Maybe this is not a big concern, would love to hear what [~matyas] or [~jbusche] think;;;","30/May/22 03:08;wangyang0918;cc [~Tison] I remember you also have the same idea when fixing FLINK-27746.;;;","30/May/22 03:42;tison;Thanks for your notification [~wangyang0918]. I'll take a look today.;;;","30/May/22 07:02;morhidi;[~wangyang0918] Does our release / CI automatism rely on podman atm? If not I'm leaning towards documenting a workaround for local podman builds instead. Actually leaking with git history is also possible, but that's another topic I guess.;;;","30/May/22 07:44;tison;I second to [~matyas]'s opinions about git history and willing to know the release automatism.

Actually, it's saner to me that do not COPY .git to docker image at all. That means we don't build the image barely via ""docker build ."", but running a scripting to optionally generate git properties and the docker build process rely on that. It would be a variant of https://github.com/apache/flink-kubernetes-operator/pull/241 (running the shell script aside from {{mvn package}}).

Although, I'll try to prepare a patch based on .dockerignore for preview.

For ""credentials"" arguments, even in current approaches, those files won't go into the final image but only in intermediate build image, so I think it won't be a critical problem. cc [~gyfora];;;","30/May/22 08:44;wangyang0918;Since our release/CI does not rely on the podman, it makes sense to me that we document a workaround for local podman build now. And bundle the generated {{.flink-kubernetes-operator.version.properties}} in the release source, which will be done in FLINK-27759.;;;","30/May/22 09:04;wangyang0918;Compared with documenting the workaround for podman, I prefer [~tison]'s solution of {{"".dockerignore""}} + ""{{{}COPY . .{}}}"" for this PR.;;;","30/May/22 09:21;morhidi;I don't see yet how .dockerignore would prevent copying unintentional files, but seemingly it's not a concern, since they would end up in the build layer only.;;;","30/May/22 09:45;wangyang0918;Yes. We could not avoid the unintentional files unless we could configure the pattern in the {{{}.dockerignore{}}}. I also agree that it is not a concern.;;;","30/May/22 14:15;wangyang0918;Fixed via:

main: db13018975361e3271e26be64a3940a5710097e7

release-1.0: c8d2a71a61a943fb9b5cf31575a40c6988665740;;;",,,,,,,,,,,,,,,,,,
PulsarSourceITCase.testTaskManagerFailure failed with AssertionError,FLINK-27833,13447427,13469647,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,hxbks2ks,hxbks2ks,30/May/22 02:15,14/Aug/22 14:53,04/Jun/24 20:51,14/Aug/22 14:53,1.14.4,,,,,,,,,,,,,,1.14.6,1.15.2,1.16.0,,Connectors / Pulsar,,,,,0,test-stability,,,,"
{code:java}
2022-05-28T02:40:24.3137097Z May 28 02:40:24 [ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 123.914 s <<< FAILURE! - in org.apache.flink.connector.pulsar.source.PulsarSourceITCase
2022-05-28T02:40:24.3139274Z May 28 02:40:24 [ERROR] testTaskManagerFailure{TestEnvironment, ExternalContext, ClusterControllable}[2]  Time elapsed: 25.823 s  <<< FAILURE!
2022-05-28T02:40:24.3140450Z May 28 02:40:24 java.lang.AssertionError: 
2022-05-28T02:40:24.3141062Z May 28 02:40:24 
2022-05-28T02:40:24.3141875Z May 28 02:40:24 Expected: Records consumed by Flink should be identical to test data and preserve the order in split
2022-05-28T02:40:24.3143710Z May 28 02:40:24      but: Mismatched record at position 18: Expected '0-iuKyiUwhhuO3NoDPvpLxIUw3' but was '0-98Y'
2022-05-28T02:40:24.3145019Z May 28 02:40:24 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
2022-05-28T02:40:24.3146064Z May 28 02:40:24 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
2022-05-28T02:40:24.3147334Z May 28 02:40:24 	at org.apache.flink.connectors.test.common.testsuites.SourceTestSuiteBase.testTaskManagerFailure(SourceTestSuiteBase.java:289)
2022-05-28T02:40:24.3148548Z May 28 02:40:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-28T02:40:24.3149690Z May 28 02:40:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-28T02:40:24.3150910Z May 28 02:40:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-28T02:40:24.3158249Z May 28 02:40:24 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-28T02:40:24.3159819Z May 28 02:40:24 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688)
2022-05-28T02:40:24.3161222Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-05-28T02:40:24.3162609Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-05-28T02:40:24.3164030Z May 28 02:40:24 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-05-28T02:40:24.3165548Z May 28 02:40:24 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-05-28T02:40:24.3167224Z May 28 02:40:24 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-05-28T02:40:24.3169087Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-05-28T02:40:24.3170540Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-05-28T02:40:24.3171970Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-05-28T02:40:24.3173499Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-05-28T02:40:24.3175023Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-05-28T02:40:24.3176435Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-05-28T02:40:24.3177715Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-05-28T02:40:24.3178985Z May 28 02:40:24 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-05-28T02:40:24.3180563Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:210)
2022-05-28T02:40:24.3182004Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3183400Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:206)
2022-05-28T02:40:24.3184865Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:131)
2022-05-28T02:40:24.3185720Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:65)
2022-05-28T02:40:24.3186545Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2022-05-28T02:40:24.3187411Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3188239Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2022-05-28T02:40:24.3189060Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-05-28T02:40:24.3189814Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2022-05-28T02:40:24.3190641Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3191449Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2022-05-28T02:40:24.3192225Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2022-05-28T02:40:24.3193099Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2022-05-28T02:40:24.3194049Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:212)
2022-05-28T02:40:24.3195149Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:192)
2022-05-28T02:40:24.3196014Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-05-28T02:40:24.3197002Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-05-28T02:40:24.3197765Z May 28 02:40:24 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-05-28T02:40:24.3198433Z May 28 02:40:24 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-05-28T02:40:24.3199174Z May 28 02:40:24 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-05-28T02:40:24.3199822Z May 28 02:40:24 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-05-28T02:40:24.3200477Z May 28 02:40:24 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-05-28T02:40:24.3201136Z May 28 02:40:24 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-05-28T02:40:24.3201821Z May 28 02:40:24 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-05-28T02:40:24.3202501Z May 28 02:40:24 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-05-28T02:40:24.3203167Z May 28 02:40:24 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-05-28T02:40:24.3203832Z May 28 02:40:24 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-05-28T02:40:24.3204687Z May 28 02:40:24 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-05-28T02:40:24.3205372Z May 28 02:40:24 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-05-28T02:40:24.3206024Z May 28 02:40:24 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-05-28T02:40:24.3206687Z May 28 02:40:24 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-05-28T02:40:24.3207365Z May 28 02:40:24 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-05-28T02:40:24.3208036Z May 28 02:40:24 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-05-28T02:40:24.3208737Z May 28 02:40:24 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-05-28T02:40:24.3209433Z May 28 02:40:24 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-05-28T02:40:24.3210132Z May 28 02:40:24 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-05-28T02:40:24.3210811Z May 28 02:40:24 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-05-28T02:40:24.3211465Z May 28 02:40:24 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-05-28T02:40:24.3212200Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-05-28T02:40:24.3213007Z May 28 02:40:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-05-28T02:40:24.3213844Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:139)
2022-05-28T02:40:24.3214728Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3215591Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2022-05-28T02:40:24.3216357Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-05-28T02:40:24.3217117Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2022-05-28T02:40:24.3217932Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3218774Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2022-05-28T02:40:24.3219626Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2022-05-28T02:40:24.3220273Z May 28 02:40:24 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2022-05-28T02:40:24.3221065Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2022-05-28T02:40:24.3222000Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2022-05-28T02:40:24.3222815Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3223620Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2022-05-28T02:40:24.3224446Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-05-28T02:40:24.3225193Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2022-05-28T02:40:24.3225996Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3226872Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2022-05-28T02:40:24.3227632Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2022-05-28T02:40:24.3228281Z May 28 02:40:24 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2022-05-28T02:40:24.3229160Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
2022-05-28T02:40:24.3230090Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:143)
2022-05-28T02:40:24.3230893Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3231707Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$7(NodeTestTask.java:129)
2022-05-28T02:40:24.3232462Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-05-28T02:40:24.3233226Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:127)
2022-05-28T02:40:24.3234051Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-05-28T02:40:24.3234981Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:126)
2022-05-28T02:40:24.3235754Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:84)
2022-05-28T02:40:24.3236609Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
2022-05-28T02:40:24.3237540Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2022-05-28T02:40:24.3238393Z May 28 02:40:24 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
2022-05-28T02:40:24.3239229Z May 28 02:40:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2022-05-28T02:40:24.3239960Z May 28 02:40:24 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2022-05-28T02:40:24.3240727Z May 28 02:40:24 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2022-05-28T02:40:24.3241545Z May 28 02:40:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2022-05-28T02:40:24.3242256Z May 28 02:40:24 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2022-05-28T02:40:24.3243015Z May 28 02:40:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2022-05-28T02:40:24.3243840Z May 28 02:40:24 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-05-28T02:40:24.3244720Z May 28 02:40:24 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2022-05-28T02:40:24.3245482Z May 28 02:40:24 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2022-05-28T02:40:24.3246199Z May 28 02:40:24 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2022-05-28T02:40:24.3246883Z May 28 02:40:24 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2022-05-28T02:40:24.3247368Z May 28 02:40:24 
2022-05-28T02:40:24.8576345Z May 28 02:40:24 [INFO] 
2022-05-28T02:40:24.8585402Z May 28 02:40:24 [INFO] Results:
2022-05-28T02:40:24.8586151Z May 28 02:40:24 [INFO] 
2022-05-28T02:40:24.8586789Z May 28 02:40:24 [ERROR] Failures: 
2022-05-28T02:40:24.8588065Z May 28 02:40:24 [ERROR]   PulsarSourceITCase>SourceTestSuiteBase.testTaskManagerFailure:289 
2022-05-28T02:40:24.8589448Z May 28 02:40:24 Expected: Records consumed by Flink should be identical to test data and preserve the order in split
2022-05-28T02:40:24.8591601Z May 28 02:40:24      but: Mismatched record at position 18: Expected '0-iuKyiUwhhuO3NoDPvpLxIUw3' but was '0-98Y'
2022-05-28T02:40:24.8592500Z May 28 02:40:24 [INFO] 
2022-05-28T02:40:24.8593254Z May 28 02:40:24 [ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36141&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b
",,,,,,,,,,,,FLINK-27399,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 14 14:53:37 UTC 2022,,,,,,,,,,"0|z12s8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/22 14:53;tison;master via 18d21a0618f2195d4279828f610094ffccd052b3
1.15 via 5f842cb8665e831d6acb978a57b896accd1c0928
1.14 via 95d14edbc8dc16fe420ec12fbd5d7f61dc873699;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplitAggregateITCase tests failed with Could not acquire the minimum required resources,FLINK-27832,13447424,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,godfreyhe,hxbks2ks,hxbks2ks,30/May/22 02:00,23/Jun/22 12:40,04/Jun/24 20:51,23/Jun/22 12:40,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,"
{code:java}
2022-05-27T17:42:59.4814999Z May 27 17:42:59 [ERROR] Tests run: 64, Failures: 23, Errors: 1, Skipped: 0, Time elapsed: 305.5 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase
2022-05-27T17:42:59.4815983Z May 27 17:42:59 [ERROR] SplitAggregateITCase.testAggWithJoin  Time elapsed: 278.742 s  <<< ERROR!
2022-05-27T17:42:59.4816608Z May 27 17:42:59 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-27T17:42:59.4819182Z May 27 17:42:59 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-27T17:42:59.4820363Z May 27 17:42:59 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-05-27T17:42:59.4821463Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-27T17:42:59.4822292Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-27T17:42:59.4823317Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-27T17:42:59.4824210Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-27T17:42:59.4825081Z May 27 17:42:59 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-05-27T17:42:59.4825927Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-27T17:42:59.4826748Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-27T17:42:59.4827596Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-27T17:42:59.4828416Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-27T17:42:59.4829284Z May 27 17:42:59 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-05-27T17:42:59.4830111Z May 27 17:42:59 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-27T17:42:59.4831015Z May 27 17:42:59 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-27T17:42:59.4832222Z May 27 17:42:59 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-27T17:42:59.4833162Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-27T17:42:59.4834250Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-27T17:42:59.4835236Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-27T17:42:59.4836035Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-27T17:42:59.4836872Z May 27 17:42:59 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-27T17:42:59.4837630Z May 27 17:42:59 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-27T17:42:59.4838394Z May 27 17:42:59 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-27T17:42:59.4839044Z May 27 17:42:59 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-27T17:42:59.4839748Z May 27 17:42:59 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-27T17:42:59.4840463Z May 27 17:42:59 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-27T17:42:59.4841313Z May 27 17:42:59 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-27T17:42:59.4842335Z May 27 17:42:59 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-27T17:42:59.4843300Z May 27 17:42:59 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-27T17:42:59.4844146Z May 27 17:42:59 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-27T17:42:59.4844882Z May 27 17:42:59 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-27T17:42:59.4845587Z May 27 17:42:59 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-27T17:42:59.4846464Z May 27 17:42:59 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-27T17:42:59.4847407Z May 27 17:42:59 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-27T17:42:59.4848154Z May 27 17:42:59 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-27T17:42:59.4848904Z May 27 17:42:59 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-27T17:42:59.4849738Z May 27 17:42:59 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-27T17:42:59.4850442Z May 27 17:42:59 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-27T17:42:59.4851309Z May 27 17:42:59 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-27T17:42:59.4852242Z May 27 17:42:59 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-27T17:42:59.4853209Z May 27 17:42:59 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-27T17:42:59.4853990Z May 27 17:42:59 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-27T17:42:59.4854768Z May 27 17:42:59 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-27T17:42:59.4855597Z May 27 17:42:59 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-27T17:42:59.4856373Z May 27 17:42:59 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-27T17:42:59.4857206Z May 27 17:42:59 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-27T17:42:59.4857961Z May 27 17:42:59 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-27T17:42:59.4858708Z May 27 17:42:59 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-27T17:42:59.4859546Z May 27 17:42:59 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-27T17:42:59.4860469Z May 27 17:42:59 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2022-05-27T17:42:59.4861804Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-27T17:42:59.4863007Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-27T17:42:59.4864036Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:300)
2022-05-27T17:42:59.4864920Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:290)
2022-05-27T17:42:59.4865941Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:281)
2022-05-27T17:42:59.4866951Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-05-27T17:42:59.4868045Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-05-27T17:42:59.4869224Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1546)
2022-05-27T17:42:59.4870193Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1113)
2022-05-27T17:42:59.4870983Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1053)
2022-05-27T17:42:59.4871995Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:892)
2022-05-27T17:42:59.4872933Z May 27 17:42:59 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:463)
2022-05-27T17:42:59.4873886Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42)
2022-05-27T17:42:59.4874915Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:617)
2022-05-27T17:42:59.4875950Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:493)
2022-05-27T17:42:59.4876899Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-05-27T17:42:59.4877762Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-05-27T17:42:59.4878575Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-27T17:42:59.4879381Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-05-27T17:42:59.4880332Z May 27 17:42:59 	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
2022-05-27T17:42:59.4881426Z May 27 17:42:59 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:186)
2022-05-27T17:42:59.4882487Z May 27 17:42:59 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408)
2022-05-27T17:42:59.4883670Z May 27 17:42:59 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:397)
2022-05-27T17:42:59.4884730Z May 27 17:42:59 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:853)
2022-05-27T17:42:59.4885502Z May 27 17:42:59 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-27T17:42:59.4886291Z May 27 17:42:59 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-27T17:42:59.4887249Z May 27 17:42:59 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-27T17:42:59.4888074Z May 27 17:42:59 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-27T17:42:59.4888943Z May 27 17:42:59 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:296)
2022-05-27T17:42:59.4889952Z May 27 17:42:59 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-27T17:42:59.4890846Z May 27 17:42:59 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:295)
2022-05-27T17:42:59.4891757Z May 27 17:42:59 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-27T17:42:59.4892722Z May 27 17:42:59 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-27T17:42:59.4893763Z May 27 17:42:59 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-27T17:42:59.4894518Z May 27 17:42:59 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-27T17:42:59.4895254Z May 27 17:42:59 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-27T17:42:59.4896054Z May 27 17:42:59 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-27T17:42:59.4896756Z May 27 17:42:59 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-27T17:42:59.4897468Z May 27 17:42:59 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-27T17:42:59.4898203Z May 27 17:42:59 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-27T17:42:59.4898926Z May 27 17:42:59 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-27T17:42:59.4899753Z May 27 17:42:59 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-27T17:42:59.4900454Z May 27 17:42:59 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-27T17:42:59.4901094Z May 27 17:42:59 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-27T17:42:59.4901867Z May 27 17:42:59 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-27T17:42:59.4902572Z May 27 17:42:59 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-27T17:42:59.4903357Z May 27 17:42:59 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-27T17:42:59.4903965Z May 27 17:42:59 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-27T17:42:59.4904701Z May 27 17:42:59 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-27T17:42:59.4905227Z May 27 17:42:59 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-27T17:42:59.4905773Z May 27 17:42:59 	... 4 more
2022-05-27T17:42:59.4906616Z May 27 17:42:59 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-05-27T17:42:59.4907811Z May 27 17:42:59 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:551)
2022-05-27T17:42:59.4908485Z May 27 17:42:59 	... 39 more
2022-05-27T17:42:59.4909342Z May 27 17:42:59 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-05-27T17:42:59.4910622Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-05-27T17:42:59.4911542Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-05-27T17:42:59.4912324Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-05-27T17:42:59.4913248Z May 27 17:42:59 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-27T17:42:59.4913959Z May 27 17:42:59 	... 37 more
2022-05-27T17:42:59.4914693Z May 27 17:42:59 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36139&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15404
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27867,,,FLINK-28077,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 23 12:40:11 UTC 2022,,,,,,,,,,"0|z12s80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 03:21;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36187&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/May/22 09:01;godfreyhe;I will have a look;;;","14/Jun/22 08:40;qingyue;I've also encountered this. [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36604&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4];;;","17/Jun/22 02:07;lsy;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36789&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","17/Jun/22 11:55;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36877&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","21/Jun/22 12:53;roman;From the logs, I see that the job fails because it couldn't acquire the required resources.
That happens after
{code:java}
17:41:15,906 [Cancellation Watchdog for GroupAggregate[4046] -> Calc[4047] -> Expand[4048] -> Calc[4049] (4/4)#0 (193dc00b3a30bbbebf2b14f615878624
    java.lang.Object.wait(Native Method)
    java.lang.Thread.join(Thread.java:1252)
    java.lang.Thread.join(Thread.java:1326)
    org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:166)
    org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
    org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
    org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
    org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$2184/1901041209.close(Unknown Source)
    org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
    org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
    org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
    org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
    org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
    org.apache.flink.runtime.taskmanager.Task$$Lambda$3780/817756958.run(Unknown Source)
    org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
    org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    java.lang.Thread.run(Thread.java:748)
{code}
 
So it looks like a duplicate of FLINK-28077 / FLINK-27792.
So I'd close it as such.

;;;","22/Jun/22 03:19;zhuzh;https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/37010/logs/104;;;","22/Jun/22 10:08;roman;[~zhuzh] could you check whether 0912765acc25f179169f8e371683acaf3e9133a2 from FLINK-28077 was included into the latest build?
Or could you share a link to the full build? (the posted link only shows one phase);;;","22/Jun/22 10:49;chesnay;This is the build from zhu zhu: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37010&view=results
It does not contain the fix for FLINK-28077.;;;","22/Jun/22 10:57;chesnay;I agree with closing this as a duplicate. The logs show again an interrupted exception in the channel writer, as seen in FLINK-27792.;;;","23/Jun/22 12:40;roman;Thanks!
Closing as duplicate of FLINK-28077.;;;",,,,,,,,,,,,,,,,,,,,,
Provide example of Beam on the k8s operator,FLINK-27831,13447420,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ZhenqiuHuang,mbalassi,mbalassi,30/May/22 01:20,02/Feb/23 07:59,04/Jun/24 20:51,02/Feb/23 07:59,,,,,,,,,,,,,,,kubernetes-operator-1.4.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Multiple users have asked for whether the operator supports Beam jobs in different shapes. I assume that running a Beam job ultimately with the current operator ultimately comes down to having the right jars on the classpath / packaged into the user's fatjar.

At this stage I suggest adding one such example, providing it might attract new users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 02 07:59:36 UTC 2023,,,,,,,,,,"0|z12s74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 03:29;mbalassi;I managed to get the Beam [WordCount|https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java] example based on the [quickstart|https://beam.apache.org/get-started/quickstart-java/#get-the-example-code] running with Flink 1.14 on the operator.

It needs a bit of tweaking to make it pretty, will create a PR soonish.;;;","30/May/22 03:58;mbalassi;On second thought the example has heavy dependencies on the Beam side, might make more sense for it to live in Beam instead with only some of the docs in the flink-kubernetes-operator side. [~mxm] what do you think?;;;","08/Jul/22 14:51;mbalassi;I am creating an example within the flink-kubernetes-operator project following the structure of this example for now:

https://github.com/apache/flink-kubernetes-operator/tree/main/examples/flink-sql-runner-example;;;","07/Dec/22 00:24;ZhenqiuHuang;[~mbalassi] I am interested with task. Would you please assign it to me?;;;","02/Feb/23 07:59;gyfora;merged to main 10670984e6319d01dbf55ce8a98aed0ab592ee6c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
My Pyflink job could not submit to Flink cluster,FLINK-27830,13447389,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,paranoia,paranoia,29/May/22 16:40,30/May/22 12:23,04/Jun/24 20:51,30/May/22 11:33,1.13.0,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"I use commd
{code:java}
./flink run --python /home/ubuntu/pyflink/main.py /home/ubuntu/pyflink/xmltest.xml /home/ubuntu/pyflink/task.xml --pyFliles /home/ubuntu/pyflink/logtest.py /home/ubuntu/pyflink/KafkaSource.py /home/ubuntu/pyflink/KafkaSink.py /home/ubuntu/pyflink/pyFlinkState.py /home/ubuntu/pyflink/projectInit.py /home/ubuntu/pyflink/taskInit.py /home/ubuntu/pyflink/UDF1.py {code}
to submit my pyflink job.

The error happened on:
{code:java}
st_env.create_statement_set().add_insert_sql(f""insert into algorithmsink select {taskInfo}(line, calculationStatusMap, gathertime, storetime, rawData, terminal, deviceID, recievetime, car, sendtime, baseInfoMap, workStatusMap, `timestamp`) from mysource"").execute().wait()
{code}
My appendix error.txt contains the exceptions. It seems like there is something wrong with Apache Beam.

When I use python command to run my job (in standalone mode instead of submitting to Flink cluster), it works well.

 

 

 ","python 3.6.9

Flink 1.13.0

PyFlink 1.13.0

zookeeper 3.4.14

hadoop 2.10.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/22 16:29;paranoia;error.txt;https://issues.apache.org/jira/secure/attachment/13044320/error.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 12:23:56 UTC 2022,,,,,,,,,,"0|z12s08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 11:31;dianfu;Please check if you have manually added some Beam jars in the classpath? There is a similar issue in the [https://stackoverflow.com/questions/61620454/apache-flink-python-table-api-udf-dependencies-problem] which turns out be caused by adding extra Beam jars in the classpath.;;;","30/May/22 11:33;dianfu;I'm closing this ticket as I don't think this is a bug.

BTW: You can ask this kind of issues in the user mailing list~;;;","30/May/22 12:23;paranoia;[~dianfu] OK, the problem is solved. I use two parameters -pyarch and -pyexec to appoint python environment and submit to Flink cluster successfully.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Calcite usages in flink-python,FLINK-27829,13447358,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/May/22 06:53,13/Jun/22 08:14,04/Jun/24 20:51,06/Jun/22 01:23,1.16.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 06 01:23:57 UTC 2022,,,,,,,,,,"0|z12rtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/22 01:23;dianfu;Merged to master via 80cd04add11f200f5bb649c4233b3a6b518e0986;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducer VS KafkaSink,FLINK-27828,13447347,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiangfei Liu,Jiangfei Liu,29/May/22 01:42,14/Jun/22 04:32,04/Jun/24 20:51,,1.14.3,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,,,,,"sorry,my english is bad.

in flink1.14.3,write 10000 data to kafka.

when use FlinkKafkaProducer,completed 7s

when use KafkaSink,completed 1m40s

why KafkaSink is low speed?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/22 01:43;Jiangfei Liu;Snipaste_2022-05-25_19-52-11.png;https://issues.apache.org/jira/secure/attachment/13044310/Snipaste_2022-05-25_19-52-11.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 04:32:24 UTC 2022,,,,,,,,,,"0|z12rqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:37;martijnvisser;[~renqs] Any thoughts on this on how we could help this user?;;;","07/Jun/22 02:20;renqs;[~Jiangfei Liu] Could you share some context of your benchmark, like the topology of the job, parallelism, checkpoint configuration and so forth?;;;","14/Jun/22 04:32;Jiangfei Liu;checkpoint config:

        CheckpointConfig checkpointConfig = env.getCheckpointConfig();
        env.enableCheckpointing(CHECKPOINT_INTERVAL);
        checkpointConfig.setCheckpointingMode(CHECKPOINT_MODE);
        checkpointConfig.setCheckpointTimeout(CHECKPOINT_TIMEOUT);
        checkpointConfig.setTolerableCheckpointFailureNumber(CHECKPOINT_FAILURE_NUMBER);
        env.setRestartStrategy(RESTART_STRATEGY_CONFIGURATION);
        checkpointConfig.setMaxConcurrentCheckpoints(CHECKPOINT_MAX_CONCURRENT);
        checkpointConfig.setMinPauseBetweenCheckpoints(CHECKPOINT_MIN_PAUSE_BETWEEN);
        checkpointConfig.setExternalizedCheckpointCleanup(CHECKPOINT_EXTERNALIZED_CLEANUP);
        checkpointConfig.setCheckpointStorage(new FileSystemCheckpointStorage(HDFS_BASE + CHECKPOINT_BASE_PATH + path));
        System.setProperty(""HADOOP_USER_NAME"", HADOOP_USER_NAME);

parallelism config: 3

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecutionEnvironment method supporting explicit Boundedness,FLINK-27827,13447336,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Do,,ahailu,ahailu,28/May/22 15:11,31/May/22 11:34,04/Jun/24 20:51,31/May/22 11:34,,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,,,,,"When creating a {{{}DataStreamSource{}}}, an explicitly bounded input is only returned if the {{InputFormat}} provided implements {{{}FileInputFormat{}}}. This is results in runtime exceptions when trying to run applications in Batch execution mode while using non {{{}FileInputFormat{}}}s e.g. Apache Iceberg [1], Flink's Hadoop MapReduce compatibility API's [2] inputs, etc...

I understand there is a {{DataSource}} API [3] that supports the specification of the boundedness of an input, but that would require all connectors to revise their APIs to leverage it which would take some time.

My organization is in the middle of migrating from the {{DataSet}} API to the {{{}DataStream API{}}}, and we've found this to be a challenge as nearly all of our inputs have been determined to be unbounded as we use {{InputFormats}} that are not {{{}FileInputFormat{}}}s.

Our work-around was to provide a local patch in {{StreamExecutionEnvironment}} with a method supporting explicitly bounded inputs.

As this helped us implement a Batch {{DataStream}} solution, perhaps this is something that may be helpful for others?

 

[1] [https://iceberg.apache.org/docs/latest/flink/#reading-with-datastream]

[2] [https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/dataset/hadoop_map_reduce/] 

[3] [https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/sources/#the-data-source-api] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 11:34:07 UTC 2022,,,,,,,,,,"0|z12rog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/22 15:13;ahailu;If this is something that the community finds useful, I'm happy to be the one to pick this up.;;;","30/May/22 07:26;gaoyunhaii;Hi [~ahailu]  sorry I'm also not quite sure there is concerns initially for also supporting explicit boundness on legacy sources, but for one thing, alternatively the bounded legacy source might be create like [https://github.com/apache/flink/blob/e7834d8b56a7b4b1c2674ab399032e21e76d9e63/flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/sink/BatchExecutionFileSinkITCase.java#L60]

without maintaining the patch. For input format, the InputFormatSourceFunction could be used. 

For the long run, the community would still hope to migrate to the new source api and deprecated the legacy ones. ;;;","30/May/22 14:40;martijnvisser;With regards to:

> I understand there is a DataSource API [3] that supports the specification of the boundedness of an input, but that would require all connectors to revise their APIs to leverage it which would take some time.

This has been a deliberate choice; the DataSet has been deprecated for quite some time and FLIP-27 has been created for both bounded and unbounded sources. Most Apache maintained Flink connectors have been migrated already or in the process of being migrated. ;;;","31/May/22 11:34;ahailu;Hi [~gaoyunhaii] & [~martijnvisser], very well. Thanks for your input!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support machine learning training for very high dimesional models,FLINK-27826,13447329,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhangzp,zhangzp,zhangzp,28/May/22 12:41,30/May/22 01:45,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,,,,,"There is limited support for training high dimensional machine learning models in FlinkML though it is often useful, especially in industrial cases. When the size of the model parameter can not be hold in the memory of a single machine, FlinkML crashes now.

So it would be nice if we support high dimensional model training in FlinkML. To achieve this, we probably need to do the following things:
 # Do a survey on how to training large machine learning models of existing machine learning systems (e.g. data paralllel, model parallel).
 # Define/Implement the infra of supporting large model training in FlinkML.
 # Implement a machine learning model (e.g., logisitic regression, factorzation machine, etc) that can train models with more than ten billion parameters.
 # Benchmark the implementation and further improve it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-28 12:41:24.0,,,,,,,,,,"0|z12rmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the doc of aligned checkpoint timeout,FLINK-27825,13447320,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,28/May/22 10:00,30/May/22 10:17,04/Jun/24 20:51,30/May/22 10:03,1.16.0,,,,,,,,,,,,,,1.16.0,,,,chinese-translation,Documentation,Runtime / Checkpointing,,,0,pull-request-available,,,,"We improved the mechanism of aligned checkpoint timeout in FLINK-27251.

The doc of aligned checkpoint timeout should be updated. 

Doc link: [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpointing_under_backpressure/#aligned-checkpoint-timeout]

 

!image-2022-05-28-18-00-13-276.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/22 10:00;fanrui;image-2022-05-28-18-00-13-276.png;https://issues.apache.org/jira/secure/attachment/13044301/image-2022-05-28-18-00-13-276.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 10:09:07 UTC 2022,,,,,,,,,,"0|z12rkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 10:03;pnowojski;Thanks for the update [~fanrui]. 

merged commit 317b230 into apache:master ;;;","30/May/22 10:09;fanrui;Hi [~pnowojski] , thanks for your review.(y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Pyflink KeyedSchema,FLINK-27824,13447316,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,billrao,billrao,28/May/22 08:28,30/May/22 18:46,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"This issue is to implement KeyedSchema for pyflink. Currently, only keyless schema is supported. This prevents me from emitting keyed kafka entries to datastream kafka connector. I plan to implement this on my own. ",,72000,72000,,0%,72000,72000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 18:46:34 UTC 2022,,,,,,,,,,"0|z12rk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 11:34;dianfu;[~billrao] Would you like to take this issue?;;;","30/May/22 18:46;billrao;Yep. Although I’m on a tight schedule recently. I might not be able to fully get to this until late July. I managed to temporarily work around this issue by calling Kafka in the MapFunction (with no delivery guarantee whatsoever). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone Job continously restart by illegal checkpointId check on PartitionTimeCommitTrigger when use  FilesystemTableSink,FLINK-27823,13447313,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,suheng.cloud,suheng.cloud,28/May/22 07:21,07/Sep/22 02:01,04/Jun/24 20:51,,1.13.6,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"Hi, community

When I build up a standalone job to read from kafka topic and sink to hdfs, I found the job continously restart after normal running 4 hours.
When the first restart show up, the logs are like

 
{noformat}
2022-05-28 00:24:04,861 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 26 (type=CHECKPOINT) @ 1653668644856 for job 00000000000000000000000000000000.
2022-05-28 00:34:04,861 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Checkpoint 26 of job 00000000000000000000000000000000 expired before completing.
2022-05-28 00:34:04,866 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Triggering checkpoint 27 (type=CHECKPOINT) @ 1653669244862 for job 00000000000000000000000000000000.
2022-05-28 00:41:02,208 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Completed checkpoint 27 for job 00000000000000000000000000000000 (117373 bytes in 417284 ms).
2022-05-28 00:41:18,517 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - PartitionCommitter -> Sink: end (1/1) (7e16853a4d16a80f96a3e26e17f9d677) switched from RUNNING to FAILED on 192.168.1.142:6122-0b54e0 @ 192.168.1.142 (dataPort=43131).
java.lang.IllegalArgumentException: Checkpoint(26) has not been snapshot. The watermark information is:
{27=1653668944610}
.
at org.apache.flink.table.filesystem.stream.PartitionTimeCommitTrigger.committablePartitions(PartitionTimeCommitTrigger.java:122) ~[flink-table-blink_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.table.filesystem.stream.PartitionCommitter.commitPartitions(PartitionCommitter.java:151) ~[flink-table-blink_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.table.filesystem.stream.PartitionCommitter.processElement(PartitionCommitter.java:143) ~[flink-table-blink_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:205) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:684) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:639) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:650) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:623) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) ~[hamal-driver-1.13.6-v1.jar:?]
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) ~[hamal-driver-1.13.6-v1.jar:?]
at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-05-28 00:41:18,524 INFO org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task cc0206f9bd17ee99dc4565713cd749d7_0.
2022-05-28 00:41:18,525 INFO org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 50 tasks should be restarted to recover the failed task cc0206f9bd17ee99dc4565713cd749d7_0. 
2022-05-28 00:41:18,525 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job xxxxxx (00000000000000000000000000000000) switched from state RUNNING to RESTARTING.
2022-05-28 00:41:18,526 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - compact-operator (24/24) (8d5ae185e722482d8b1ff4bc3ba60e86) switched from RUNNING to CANCELING.
2022-05-28 00:41:18,526 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - compact-operator (23/24) (369af96456d991046eb10cfee44df415) switched from RUNNING to CANCELING.
...
...
{noformat}
 

after that, the job restart and successfully restore state form cp(using state.checkpoint-storage=jobmanager), and the following checkpoint (27/28/29/...) can also be sucessfully finished. But it seems the recovered state try to report commit msg of old checkpoint 26 to the PartitionCommitter which continously cause failures.
Finally the job restart again and again, and the same error log likes

 
{noformat}
2022-05-28 08:36:23,718 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - PartitionCommitter -> Sink: end (1/1) (669dfe28f49ec9b08cb1f605b7e1af86) switched from RUNNING to FAILED on 192.168.1.226:6122-ac5e98 @ 192.168.1.226 (dataPort=41827).
java.lang.IllegalArgumentException: Checkpoint(26) has not been snapshot. The watermark information is:
{27=1653668944610, 28=1653669762385, 29=1653669973437, 30=1653670045517, 31=1653670584329, 32=1653671198834, 33=1653671316604, 34=1653671595057, 35=1653671632382, 36=1653671940262, 37=1653672247793, 38=1653672513421, 39=1653672626251, 40=1653672872425, 41=1653673029517, 42=1653673662173, 43=1653673843265, 44=1653674382981, 45=1653674739299, 46=1653674890522, 47=1653675402372, 48=1653675767340, 49=1653676205712, 50=1653676376692, 51=1653676762574, 52=1653677105303, 53=1653677254604, 54=1653677458683, 55=1653677651603, 57=1653678458691, 58=1653678931845, 59=1653679306742, 60=1653679845020, 61=1653680406114, 62=1653680981416, 63=1653681545056, 64=1653681584696, 65=1653681622029, 66=1653682017861, 67=1653682319529, 68=1653682404672, 69=1653682559904, 70=1653682804993, 71=1653682907991, 72=1653683279780, 73=1653683905573, 74=1653684156034, 75=1653684659397, 76=1653684975030, 77=1653685329183, 78=1653685862724, 79=1653686499090, 80=1653686636903, 81=1653686780782, 82=1653687053096, 83=1653687541953, 84=1653688012617, 85=1653688337464, 86=1653688832762, 87=1653689195316, 88=1653689330027, 89=1653689545859, 90=1653689957313, 91=1653690069643, 92=1653690689424, 93=1653690963316, 94=1653691164532, 95=1653691687307, 96=1653691885408, 97=1653692235231, 98=1653692428716, 99=1653692849146, 100=1653693274253, 101=1653693438601, 102=1653694097925, 103=1653694716179, 104=1653694770858, 105=1653695305421, 106=1653695464923, 107=1653695959050, 108=1653696465917, 109=1653696825723, 110=1653696841452, 111=1653697238699, 112=1653697882510}
.
at org.apache.flink.table.filesystem.stream.PartitionTimeCommitTrigger.committablePartitions(PartitionTimeCommitTrigger.java:122) ~[flink-table-blink_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.table.filesystem.stream.PartitionCommitter.commitPartitions(PartitionCommitter.java:151) ~[flink-table-blink_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.table.filesystem.stream.PartitionCommitter.processElement(PartitionCommitter.java:143) ~[flink-table-blink_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:205) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:684) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:639) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:650) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:623) ~[flink-dist_2.12-1.13.6.jar:1.13.6]
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) ~[hamal-driver-1.13.6-v1.jar:?]
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) ~[hamal-driver-1.13.6-v1.jar:?]
at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_221]
2022-05-28 08:36:23,718 INFO org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task cc0206f9bd17ee99dc4565713cd749d7_0.
2022-05-28 08:36:23,719 INFO org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 50 tasks should be restarted to recover the failed task cc0206f9bd17ee99dc4565713cd749d7_0. 
2022-05-28 08:36:23,719 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job xxxxxx (00000000000000000000000000000000) switched from state RUNNING to RESTARTING.
2022-05-28 08:36:23,719 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - compact-operator (24/24) (66177c2069b9aeef21376d7a780ceadb) switched from RUNNING to CANCELING.
2022-05-28 08:36:23,719 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - compact-operator (23/24) (7028f34c1756cd1fff5cf25dd12fd550) switched from RUNNING to CANCELING.
{noformat}
The job logic is very simple, which flink sql like
{code:java}
CREATE TEMPORAY TABLE filesystem_sink_table(....)
PARTITIONED BY(`dt`,`hour`,`topic`) WITH(
'connector'='filesystem',
'format'='textfile',
'sink.partition-commit.trigger'='partition-time',
'sink.partition-commit.delay'='1 hour',
'sink.partition-commit.policy.kind'='success-file',
'auto-compaction' = 'true'
...
);
CREATE TEMPORARY TABLE kafka_source_table ...
streamTableEnv.executeSql(""INSERT INTO filesystem_sink_table SELECT ... FROM kafka_source_table"");
{code}
 

 

I have check the source at PartitionTimeCommitTrigger, and what puzzle me is that it seems the watermarks should only remove the committed checkpointId after pass the valiation

 
{code:java}
...
if (!watermarks.containsKey(checkpointId)) { 
   throw new IllegalArgumentException( String.format( ""Checkpoint(%d) has not been snapshot. The watermark information is: %s."", checkpointId, watermarks)); 
} 
long watermark = watermarks.get(checkpointId); 
watermarks.headMap(checkpointId, true).clear();
...
{code}
I have no idea in which scene should this exception encountered, do I mistake some config or there some inconsistent state? 

Thanks for any help.

 

 

some runtime snapshot:

!image-2022-05-28-15-35-23-708.png!

!image-2022-05-28-15-35-09-874.png!

!image-2022-05-28-15-35-02-186.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/22 07:33;suheng.cloud;image-2022-05-28-15-33-04-632.png;https://issues.apache.org/jira/secure/attachment/13044295/image-2022-05-28-15-33-04-632.png","28/May/22 07:34;suheng.cloud;image-2022-05-28-15-34-23-397.png;https://issues.apache.org/jira/secure/attachment/13044296/image-2022-05-28-15-34-23-397.png","28/May/22 07:34;suheng.cloud;image-2022-05-28-15-34-46-698.png;https://issues.apache.org/jira/secure/attachment/13044297/image-2022-05-28-15-34-46-698.png","28/May/22 07:35;suheng.cloud;image-2022-05-28-15-35-02-186.png;https://issues.apache.org/jira/secure/attachment/13044298/image-2022-05-28-15-35-02-186.png","28/May/22 07:35;suheng.cloud;image-2022-05-28-15-35-09-874.png;https://issues.apache.org/jira/secure/attachment/13044299/image-2022-05-28-15-35-09-874.png","28/May/22 07:35;suheng.cloud;image-2022-05-28-15-35-23-708.png;https://issues.apache.org/jira/secure/attachment/13044300/image-2022-05-28-15-35-23-708.png",,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 07 01:58:53 UTC 2022,,,,,,,,,,"0|z12rjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/22 01:58;yuchuanchen;Hi [~suheng.cloud].We met this problem, too.  Have you resolved it? Could you share your solution? 

We currently just ignore the illegalArgument Exception and continue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate the doc of checkpoint/savepoint guarantees,FLINK-27822,13447302,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,fanrui,fanrui,28/May/22 02:40,05/Jul/22 05:31,04/Jun/24 20:51,04/Jul/22 11:05,1.15.0,1.16.0,,,,,,,,,,,,,1.16.0,,,,chinese-translation,Documentation,,,,0,pull-request-available,,,,Translate the change of FLINK-26134 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 04 11:05:46 UTC 2022,,,,,,,,,,"0|z12rgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/22 17:49;Feifan Wang;Hi [~fanrui] , I am a Chinese speaker and be glad to do this work. Can you assign this ticket to me ?;;;","28/May/22 22:50;jingge;Thanks [~Feifan Wang], ticket has been assigned to you. You might find more guideline for [documentation contribution|https://flink.apache.org/contributing/contribute-documentation.html];;;","17/Jun/22 02:47;Feifan Wang;Thanks [~jingge] and sorry for take so long time, I hive submitted a [pr|https://github.com/apache/flink/pull/19998] for this ticket.;;;","20/Jun/22 06:26;Feifan Wang;Hi [~jingge] , can you help me find a Chinese committer to review this pr ?;;;","22/Jun/22 14:46;Feifan Wang;Hi [~yunta] , can you help me review this pr ?;;;","25/Jun/22 18:08;jingge;Hi [~Feifan Wang] , sorry for the late reply. I just reviewed it and left one comment. Thanks!;;;","26/Jun/22 15:43;Feifan Wang;Hi [~jingge] , I have committed your suggestion , thanks very much.;;;","04/Jul/22 11:05;yunta;merged in master: 16109a31468949f09c2a7bba9003761726e3d61c;;;",,,,,,,,,,,,,,,,,,,,,,,,
Cannot delete flinkdeployment when the pod and deployment deleted manually,FLINK-27821,13447233,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,Miuler,Miuler,27/May/22 13:49,28/Jun/22 09:36,04/Jun/24 20:51,28/Jun/22 09:36,kubernetes-operator-1.1.0,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"My operator was installed with following command:

 

{{git clone git@github.com:apache/flink-kubernetes-operator.git}}
{{git checkout 207b17b}}
{{cd flink-kubernetes-operator}}
{{helm -debug upgrade -i  flink-kubernetes-operator helm/flink-kubernetes-operator --set image.repository=ghcr.io/apache/flink-kubernetes-operator -set image.tag=207b17b}}

 

Then I create a flinkDeployment and flinkSessionJob, then I deleted the deployment that generated the flinkDeployment, then reinstall operator and finally I wanted to delete the flinkdeployment

 

kubectl logs -f pod/flink-kubernetes-operator-5cf66cbbcb-bpl9p

 

{{2022-05-27 13:40:22,027 o.a.f.k.o.c.FlinkDeploymentController [INFO ][flink-system/migration] Deleting FlinkDeployment}}
{{2022-05-27 13:40:34,047 o.a.f.s.n.i.n.c.AbstractChannel [WARN ] Force-closing a channel whose registration task was not accepted by an event loop: [id: 0xb2062900]}}
{{java.util.concurrent.RejectedExecutionException: event executor terminated}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:923)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:350)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:343)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:825)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:815)}}
{{        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe.register(AbstractChannel.java:483)}}
{{        at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:87)}}
{{        at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:81)}}
{{        at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.register(MultithreadEventLoopGroup.java:86)}}
{{        at org.apache.flink.shaded.netty4.io.netty.bootstrap.AbstractBootstrap.initAndRegister(AbstractBootstrap.java:323)}}
{{        at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.doResolveAndConnect(Bootstrap.java:155)}}
{{        at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:139)}}
{{        at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:123)}}
{{        at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:467)}}
{{        at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:390)}}
{{        at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:304)}}
{{        at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$32(RestClusterClient.java:863)}}
{{        at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture.postFire(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture$Completion.run(Unknown Source)}}
{{        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)}}
{{        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)}}
{{        at java.base/java.lang.Thread.run(Unknown Source)}}
{{2022-05-27 13:40:34,047 o.a.f.s.n.i.n.u.c.D.rejectedExecution [ERROR] Failed to submit a listener notification task. Event loop shut down?}}
{{java.util.concurrent.RejectedExecutionException: event executor terminated}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:923)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:350)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:343)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:825)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:815)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:499)}}
{{        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:184)}}
{{        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95)}}
{{        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30)}}
{{        at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:471)}}
{{        at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:390)}}
{{        at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:304)}}
{{        at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$32(RestClusterClient.java:863)}}
{{        at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture.postFire(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture$Completion.run(Unknown Source)}}
{{        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)}}
{{        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)}}
{{        at java.base/java.lang.Thread.run(Unknown Source)}}
{{2022-05-27 13:40:34,048 o.a.f.k.o.o.d.SessionObserver  [ERROR][flink-system/migration-cosmosdb] REST service in session cluster is bad now}}
{{java.util.concurrent.TimeoutException}}
{{        at java.base/java.util.concurrent.CompletableFuture.timedGet(Unknown Source)}}
{{        at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)}}
{{        at org.apache.flink.kubernetes.operator.service.FlinkService.listJobs(FlinkService.java:331)}}
{{        at org.apache.flink.kubernetes.operator.observer.deployment.SessionObserver.observeFlinkCluster(SessionObserver.java:45)}}
{{        at org.apache.flink.kubernetes.operator.observer.deployment.AbstractDeploymentObserver.observe(AbstractDeploymentObserver.java:92)}}
{{        at org.apache.flink.kubernetes.operator.observer.deployment.AbstractDeploymentObserver.observe(AbstractDeploymentObserver.java:56)}}
{{        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:101)}}
{{        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.cleanup(FlinkDeploymentController.java:59)}}
{{        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:68)}}
{{        at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:50)}}
{{        at io.javaoperatorsdk.operator.api.monitoring.Metrics.timeControllerExecution(Metrics.java:34)}}
{{        at io.javaoperatorsdk.operator.processing.Controller.cleanup(Controller.java:49)}}
{{        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleCleanup(ReconciliationDispatcher.java:252)}}
{{        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:72)}}
{{        at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:50)}}
{{        at io.javaoperatorsdk.operator.processing.event.EventProcessor$ControllerExecution.run(EventProcessor.java:349)}}
{{        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)}}
{{        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)}}
{{        at java.base/java.lang.Thread.run(Unknown Source)}}
{{2022-05-27 13:40:34,048 o.a.f.k.o.o.d.SessionObserver  [INFO ][flink-system/migration-cosmosdb] Observing JobManager deployment. Previous status: READY}}
{{2022-05-27 13:40:34,049 o.a.f.k.o.o.d.SessionObserver  [ERROR][flink-system/migration-cosmosdb] Missing JobManager deployment}}

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 27 15:05:58 UTC 2022,,,,,,,,,,"0|z12r1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 14:17;Miuler;I delete the flinksessionjob and then forced the deletion of the flinkdeployment and at some point it could be deleted, how strange;;;","27/May/22 14:18;gyfora;cc [~aitozi] , maybe you have seen something similar;;;","27/May/22 15:05;aitozi;[~Miuler] The deletion of the session cluster's flinkdeployment object will be postpone after the cleanup of the flinksessionjob. 

But from your log, it seems the session cluster already be shutdown when performing the cleanup. Do you manually delete the flink session cluster's Deployment ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle Upgrade/Deployment errors gracefully,FLINK-27820,13447227,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,27/May/22 13:11,20/Jun/22 13:22,04/Jun/24 20:51,20/Jun/22 13:22,kubernetes-operator-1.0.0,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The operator currently cannot gracefully handle the cases when there is a failure during (or directly after & and before updating the status) job submission.

This applies to both initial cluster submissions when a Flink CR was created but more importantly during upgrades.

This is slightly related to https://issues.apache.org/jira/browse/FLINK-27804 where mid-upgrade observe was disabled to workaround some issues, this logic should also be improved to only skip observing last-state info for already finished jobs (that were observed before).

During upgrades, the observer should be able to recognize when the job/cluster was actually submitted already even if the status update subsequently failed and move the status into a healthy DEPLOYED state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 13:22:46 UTC 2022,,,,,,,,,,"0|z12r0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/22 13:22;gyfora;merged to main ab59d6eb980512775590d0d01e697fe0c28d1b3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate better operationIds for OpenAPI spec,FLINK-27819,13447222,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/22 12:40,05/Oct/22 08:44,04/Jun/24 20:51,15/Jun/22 06:35,,,,,,,,,,,,,,,1.16.0,,,,Documentation,Runtime / REST,,,,0,pull-request-available,,,,There is an easy way to generate operation ids that are significantly better than the defaults.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29505,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 06:35:23 UTC 2022,,,,,,,,,,"0|z12qzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/22 06:35;chesnay;master: 012c1d497221c9d5348a0508aadf4c0486a21bda;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Model enums as references in OpenAPI spec,FLINK-27818,13447221,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/22 12:35,27/May/22 16:52,04/Jun/24 20:51,27/May/22 16:52,,,,,,,,,,,,,,,1.16.0,,,,Documentation,Runtime / REST,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 27 16:52:25 UTC 2022,,,,,,,,,,"0|z12qz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 16:52;chesnay;master: e7834d8b56a7b4b1c2674ab399032e21e76d9e63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManager metaspace OOM for session cluster,FLINK-27817,13447207,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,godfreyhe,godfreyhe,27/May/22 10:39,14/Aug/22 16:36,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,,1,,,,,"From user ML: https://www.mail-archive.com/user-zh@flink.apache.org/msg15224.html

For SQL jobs, the most operators are code generated with *unique class name*, this will cause the TM metaspace space continued growth until OOM in a session cluster.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 14 16:36:58 UTC 2022,,,,,,,,,,"0|z12qwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/22 16:36;Yu.an;There are many duplicate codes generated for different SQL jobs. Since they are actually representing the same logic, I guess if we make several jobs share the generated class, the generated class will reduce much so the metaspace memory can be saved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
file source reader has Some requirements,FLINK-27816,13447198,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jinshuangxian,jinshuangxian,27/May/22 10:07,30/May/22 14:42,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,0,,,,,"I use the flink sql file-system connector to consume data, write it to object storage, and use the file-system consumption to process the data in the object storage. The whole process works fine. I have 2 new requirements:
1. can I specify a timestamp to consume files within a specified time range
2. It is hoped that the data written to the object storage can be ordered in the partition (for example, partitioned according to the device id), and the file source reader can consume the files in an orderly manner similar to kafka when consuming files.
Can some enhancements be made to the file source reader?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 14:42:45 UTC 2022,,,,,,,,,,"0|z12qvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:42;martijnvisser;[~jinshuangxian] Thanks for opening the ticket. With regards to improvements, those are always dependent on someone in the community willing to pick up a ticket and create the improvement. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the join reorder strategy for batch sql job ,FLINK-27815,13447189,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,godfreyhe,godfreyhe,godfreyhe,27/May/22 09:39,17/Aug/23 10:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,stale-assigned,,,"Join is heavy operation in the execution, the join order in a query can have a significant impact on the query’s performance. 
Currently, the planner has one  join reorder strategy which is provided by Apache Calcite, and it strongly depends on the statistics.
It's better we can provide different join reorder strategies for different situations, such as:
1. provide a join reorder strategy without statistics, e.g. eliminate cross joins
2. improve current join reorders strategy with statistics
3. provide hints to allow users to choose join order strategy
4. ...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 10:35:15 UTC 2023,,,,,,,,,,"0|z12qts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 02:10;complone;Hello [~godfreyhe] , I have two questions about question 1
1. Do we need to implement the

Program#heuristicJoinOrder similar to Calcite to solve the order problem of multi-table joins

2. The JoinAssociationRule  and JoinPushThroughJoinRule of Calcite will match the left deep tree and take effect on the inner join. Do we also need support?;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an abstraction layer for connectors to read and write row data instead of key-values,FLINK-27814,13447187,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,27/May/22 09:29,19/Mar/23 05:50,04/Jun/24 20:51,19/Mar/23 05:50,table-store-0.2.0,,,,,,,,,,,,,,,,,,Table Store,,,,,0,pull-request-available,stale-assigned,,,"Currently {{FileStore}} exposes an interface for reading and writing {{KeyValue}}. However connectors may have different ways to change a {{RowData}} to {{KeyValue}} under different {{WriteMode}}. This results in lots of {{if...else...}} branches and duplicated code.

We need to add an abstraction layer for connectors to read and write row data instead of key-values.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 15 22:37:54 UTC 2022,,,,,,,,,,"0|z12qtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalStateException: after migration from statefun-3.1.1 to 3.2.0,FLINK-27813,13447185,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Won't Fix,,Kazimirov,Kazimirov,27/May/22 09:17,29/May/22 13:10,04/Jun/24 20:51,29/May/22 11:33,statefun-3.2.0,,,,,,,,,,,,,,,,,,API / State Processor,,,,,0,,,,,"Issue was met after migration from 

flink-statefun:3.1.1-java11

to

flink-statefun:3.2.0-java11

 
{code:java}
ts.execution-paymentManualValidationRequestEgress-egress, Sink: ua.aval.payments.execution-norkomExecutionRequested-egress, Sink: ua.aval.payments.execution-shareStatusToManualValidation-egress) (1/1)#15863 (98a1f6bef9a435ef9d0292fefeb64022) switched from RUNNING to FAILED with failure cause: java.lang.IllegalStateException: Unable to parse Netty transport spec.\n\tat org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplyClientFactory.parseTransportSpec(NettyRequestReplyClientFactory.java:56)\n\tat org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplyClientFactory.createTransportClient(NettyRequestReplyClientFactory.java:39)\n\tat org.apache.flink.statefun.flink.core.httpfn.HttpFunctionProvider.functionOfType(HttpFunctionProvider.java:49)\n\tat org.apache.flink.statefun.flink.core.functions.PredefinedFunctionLoader.load(PredefinedFunctionLoader.java:70)\n\tat org.apache.flink.statefun.flink.core.functions.PredefinedFunctionLoader.load(PredefinedFunctionLoader.java:47)\n\tat org.apache.flink.statefun.flink.core.functions.StatefulFunctionRepository.load(StatefulFunctionRepository.java:71)\n\tat org.apache.flink.statefun.flink.core.functions.StatefulFunctionRepository.get(StatefulFunctionRepository.java:59)\n\tat org.apache.flink.statefun.flink.core.functions.LocalFunctionGroup.enqueue(LocalFunctionGroup.java:48)\n\tat org.apache.flink.statefun.flink.core.functions.Reductions.enqueue(Reductions.java:153)\n\tat org.apache.flink.statefun.flink.core.functions.Reductions.apply(Reductions.java:148)\n\tat org.apache.flink.statefun.flink.core.functions.FunctionGroupOperator.processElement(FunctionGroupOperator.java:90)\n\tat org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)\n\tat org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)\n\tat org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)\n\tat org.apache.flink.statefun.flink.core.feedback.FeedbackUnionOperator.sendDownstream(FeedbackUnionOperator.java:180)\n\tat org.apache.flink.statefun.flink.core.feedback.FeedbackUnionOperator.processElement(FeedbackUnionOperator.java:86)\n\tat org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)\n\tat org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)\n\tat org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds) (through reference chain: org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplySpec[\""timeouts\""]->org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplySpec$Timeouts[\""call\""])\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:390)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:349)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1822)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:326)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:542)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:565)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:449)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1405)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:362)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:195)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4569)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2798)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.treeToValue(ObjectMapper.java:3261)\n\tat org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplyClientFactory.parseTransportSpec(NettyRequestReplyClientFactory.java:54)\n\t... 30 more\nCaused by: java.lang.IllegalArgumentException: Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds)\n\tat org.apache.flink.util.TimeUtils.parseDuration(TimeUtils.java:105)\n\tat org.apache.flink.statefun.flink.common.json.StateFunObjectMapper$DurationJsonDeserializer.deserialize(StateFunObjectMapper.java:49)\n\tat org.apache.flink.statefun.flink.common.json.StateFunObjectMapper$DurationJsonDeserializer.deserialize(StateFunObjectMapper.java:45)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:324)\n\t... 42 more\n”}{code}
{code:java}
25T09:36:12.963Z"",""level"":""INFO"",""logger"":""org.apache.flink.runtime.executiongraph.ExecutionGraph"",""class&lt;span class=""code-quote"">"":""org.apache.flink.runtime.executiongraph.Execution"",""method"":""transitionState"",""file"":""Execution.java"",""line"":1438,""thread"":""flink-akka.actor.default-dispatcher-23"",""stack"":""java.lang.IllegalStateException: Unable to parse Netty transport spec.\n\tat org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplyClientFactory.parseTransportSpec(NettyRequestReplyClientFactory.java:56)\n\tat org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplyClientFactory.createTransportClient(NettyRequestReplyClientFactory.java:39)\n\tat org.apache.flink.statefun.flink.core.httpfn.HttpFunctionProvider.functionOfType(HttpFunctionProvider.java:49)\n\tat org.apache.flink.statefun.flink.core.functions.PredefinedFunctionLoader.load(PredefinedFunctionLoader.java:70)\n\tat org.apache.flink.statefun.flink.core.functions.PredefinedFunctionLoader.load(PredefinedFunctionLoader.java:47)\n\tat org.apache.flink.statefun.flink.core.functions.StatefulFunctionRepository.load(StatefulFunctionRepository.java:71)\n\tat org.apache.flink.statefun.flink.core.functions.StatefulFunctionRepository.get(StatefulFunctionRepository.java:59)\n\tat org.apache.flink.statefun.flink.core.functions.LocalFunctionGroup.enqueue(LocalFunctionGroup.java:48)\n\tat org.apache.flink.statefun.flink.core.functions.Reductions.enqueue(Reductions.java:153)\n\tat org.apache.flink.statefun.flink.core.functions.Reductions.apply(Reductions.java:148)\n\tat org.apache.flink.statefun.flink.core.functions.FunctionGroupOperator.processElement(FunctionGroupOperator.java:90)\n\tat org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:99)\n\tat org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:80)\n\tat org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)\n\tat org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)\n\tat org.apache.flink.statefun.flink.core.feedback.FeedbackUnionOperator.sendDownstream(FeedbackUnionOperator.java:180)\n\tat org.apache.flink.statefun.flink.core.feedback.FeedbackUnionOperator.processElement(FeedbackUnionOperator.java:86)\n\tat org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)\n\tat org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)\n\tat org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)\n\tat org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)\n\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)\n\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)\n\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)\n\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)\n\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds) (through reference chain: org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplySpec[\""timeouts\""]->org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplySpec$Timeouts[\""call\""])\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:390)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:349)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1822)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:326)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:542)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:565)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:449)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1405)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:362)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:195)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4569)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2798)\n\tat org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.treeToValue(ObjectMapper.java:3261)\n\tat org.apache.flink.statefun.flink.core.nettyclient.NettyRequestReplyClientFactory.parseTransportSpec(NettyRequestReplyClientFactory.java:54)\n\t... 30 common frames omitted\nCaused by: java.lang.IllegalArgumentException: Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds)\n\tat org.apache.flink.util.TimeUtils.parseDuration(TimeUtils.java:105)\n\tat org.apache.flink.statefun.flink.common.json.StateFunObjectMapper$DurationJsonDeserializer.deserialize(StateFunObjectMapper.java:49)\n\tat org.apache.flink.statefun.flink.common.json.StateFunObjectMapper$DurationJsonDeserializer.des{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/22 10:39;Kazimirov;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13044269/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 29 13:10:41 UTC 2022,,,,,,,,,,"0|z12qsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 14:15;galenwarren;Hmm, not sure. The relevant part of the error message seems to be this:
Time interval unit label 'm' does not match any of the recognized units: DAYS: (d | day | days), HOURS: (h | hour | hours), MINUTES: (min | minute | minutes), SECONDS: (s | sec | secs | second | seconds), MILLISECONDS: (ms | milli | millis | millisecond | milliseconds), MICROSECONDS: (µs | micro | micros | microsecond | microseconds), NANOSECONDS: (ns | nano | nanos | nanosecond | nanoseconds) 
Do you have a time unit in your spec labeled with an 'm'?;;;","27/May/22 19:29;Kazimirov;[~galenwarren]  we have such specs for example:
{code:java}
public static final ValueSpec<String> ORIGINAL_CALLER = ValueSpec.named(""originalCaller"")
        .thatExpireAfterWrite(ChronoUnit.MONTHS.getDuration())
        .withUtf8StringType();
public static final ValueSpec<String> CALLER_ID = ValueSpec.named(""callerId"")
        .thatExpireAfterWrite(ChronoUnit.MONTHS.getDuration())
        .withUtf8StringType();
public static final ValueSpec<ArcSightValidationResponse> RESPONSE_STORAGE = ValueSpec.named(""SightResponse"")
        .thatExpireAfterWrite(ChronoUnit.MONTHS.getDuration())
        .withCustomType(SightValidationResponse.TYPE);
public static final ValueSpec<Ack> ACT_RESPONSE_STORAGE = ValueSpec.named(""ActResponse"")
        .thatExpireAfterWrite(ChronoUnit.MONTHS.getDuration())
        .withCustomType(ProtobufTypes.sepAckType());
public static final ValueSpec<Integer> REQUEST_ATTEMPT = ValueSpec.named(""attempt"")
        .thatExpireAfterWrite(ChronoUnit.HOURS.getDuration())
        .withIntType();
public static final ValueSpec<RetryableRunner> RETRYABLE_RUNNER_VALUE_SPEC = ValueSpec
        .named(""retryableRunner"")
        .withCustomType(RetryableRunner.TYPE);
public static final ValueSpec<Boolean> VALIDATION_IN_PROGRESS = ValueSpec.named(""MonitoringValidationInProgress"")
        .thatExpireAfterWrite(ChronoUnit.WEEKS.getDuration())
        .withBooleanType();{code};;;","27/May/22 19:45;galenwarren;Are you using RequestReplyFunctionBuilder to build your statefun job?;;;","29/May/22 11:32;Kazimirov;[~galenwarren]  I found the issue in the configuration in module.yaml

we used such a config in 3.1.0:
{code:java}
kind: io.statefun.endpoints.v2/http
spec:
  functions: ua.test.execution/*
  urlPathTemplate: http://test-{{ $.Values.global.platformEnvironment }}.svc.cluster.local:8080/v1/functions
  transport:
    type: io.statefun.transports.v1/async
    timeouts:
      call: 6m{code}
now in 3.2.0 -  *6m* should be *6min*;;;","29/May/22 13:10;galenwarren;Cool! Glad it was something simple.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Dynamic Change of Watched Namespaces,FLINK-27812,13447182,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,27/May/22 09:12,24/Nov/22 01:02,04/Jun/24 20:51,19/Jun/22 08:36,kubernetes-operator-1.1.0,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The new feature [Dynamically Changing Target Namespaces |https://javaoperatorsdk.io/docs/features#dynamically-changing-target-namespaces] introduced in JOSDK v3 enables to listen to namespace changes without restarting the operator. The watched namespaces are currently determined by environment variables, and can be moved into the ConfigMap next to other operator config parameters that are already handled dynamically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 19 08:36:02 UTC 2022,,,,,,,,,,"0|z12qs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/22 08:36;gyfora;merged to main 11777032e3c3011c843631471c1b52aecb52dabf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove netty dependency in flink-test-utils,FLINK-27811,13447180,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/22 09:09,13/Jun/22 08:14,04/Jun/24 20:51,07/Jun/22 07:36,,,,,,,,,,,,,,,1.16.0,,,,Build System,Tests,,,,0,pull-request-available,,,,For some reason we bundle a relocated version of netty in flink-test-utils. AFAICT this should be unnecessary because nothing makes use of the relocated version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 07:36:11 UTC 2022,,,,,,,,,,"0|z12qrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 07:36;chesnay;master: 1bec4e20b83b7bcf49304c8944b12a895a86b413;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch e2e jars bundle way more than they should,FLINK-27810,13447177,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/22 08:55,13/Jun/22 08:15,04/Jun/24 20:51,07/Jun/22 08:02,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Build System,Connectors / ElasticSearch,Tests,,,0,pull-request-available,,,,"The jars bundle flink-end-to-end-tests-common-elasticsearch and all of it's transitive dependencies, like junit or flink-rpc-core.

All of these are unnecessary for the test to work and really shouldn't be bundled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 08:02:08 UTC 2022,,,,,,,,,,"0|z12qr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 08:02;chesnay;master: a3a329998d7d1c5625c1f08b00eebe8616a72c89;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify that cluster-id is mandatory for Kubernetes HA in standalone mode,FLINK-27809,13447165,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,27/May/22 08:11,11/Mar/24 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,Deployment / Kubernetes,Runtime / Configuration,,,,0,,,,,"The description for KubernetesConfigOptions#CLUSTER_ID states that the client generates this automatically if it isn't set. This is technically correct, because the client is not involved in the deployment for standalone clusters, but to users this sounds like it is optional in general, while it must be set (to an ideally unique value) in standalone mode.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 12 13:49:20 UTC 2022,,,,,,,,,,"0|z12qog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/22 13:49;ConradJam;Should we require users to fill it out? I want to take this ticket :) [~chesnay] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Allow ""kubernetes"" as setting for HA_MODE",FLINK-27808,13447163,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/May/22 07:45,14/Jun/22 11:38,04/Jun/24 20:51,14/Jun/22 11:38,,,,,,,,,,,,,,,1.16.0,,,,Deployment / Kubernetes,Runtime / Configuration,,,,0,pull-request-available,,,,"Make it easier to enable kubernetes HA by allowing ""kubernetes"" as a setting for HA_MODE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 11:38:57 UTC 2022,,,,,,,,,,"0|z12qo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 11:38;chesnay;master: fb05a7be9b828b7e582e75e4832443806fa4ff17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The improvement of addBatch is empty when jdbc batch submit,FLINK-27807,13447124,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,amenhub@163.com,amenhub@163.com,27/May/22 02:59,19/Aug/23 10:35,04/Jun/24 20:51,,1.14.4,1.15.0,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,auto-deprioritized-minor,pull-request-available,,,"Extending the DM database dialect through JDBC, when executing Upsert semantics, a ""parameter not bound"" exception will be thrown, and it is found that the following code can be improved: 
{code:java}
for (Map.Entry<RowData, Tuple2<Boolean, RowData>> entry : reduceBuffer.entrySet()) {
    if (entry.getValue().f0) {
        upsertExecutor.addToBatch(entry.getValue().f1);
    } else {
    // delete by key
    deleteExecutor.addToBatch(entry.getKey());
    }
}
upsertExecutor.executeBatch();
deleteExecutor.executeBatch();
reduceBuffer.clear();{code}
That is to say, when the size of reduceBuffer is 1, only one of the if-else statement blocks is executed, which will cause
{code:java}
 upsertExecutor().executeBatch() {code}
or 
{code:java}
deleteExecutor.executeBatch(){code}
 to have an empty batch executed, however, this will cause some jdbc driver implementations to throw exceptions, as described above",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:05 UTC 2023,,,,,,,,,,"0|z12qfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datagen add binary & varbinary  type support,FLINK-27806,13447120,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,taoran,taoran,taoran,27/May/22 02:29,06/Jul/22 01:58,04/Jun/24 20:51,06/Jul/22 01:58,1.14.4,1.15.0,,,,,,,,,,,,,1.16.0,,,,Table SQL / Ecosystem,,,,,0,pull-request-available,,,,"Datagen connector currently not support BYTES type. e.g. BINARY & VARBINARY. It will cause exception when use

CREATE TABLE t3 (
    f0 BIGINT,
    f1 VARBINARY
) WITH (
  'connector' = 'datagen',
  ....
);


StackTrace:
Caused by: org.apache.flink.table.api.ValidationException: Unsupported type: BYTES at org.apache.flink.connector.datagen.table.RandomGeneratorVisitor.defaultMethod(RandomGeneratorVisitor.java:317) at org.apache.flink.connector.datagen.table.RandomGeneratorVisitor.defaultMethod(RandomGeneratorVisitor.java:60) at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:82) at org.apache.flink.table.types.logical.VarBinaryType.accept(VarBinaryType.java:151) at org.apache.flink.connector.datagen.table.DataGenTableSourceFactory.createContainer(DataGenTableSourceFactory.java:128) at org.apache.flink.connector.datagen.table.DataGenTableSourceFactory.createDynamicTableSource(DataGenTableSourceFactory.java:98) at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:147)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/22 02:50;taoran;random.png;https://issues.apache.org/jira/secure/attachment/13044256/random.png","27/May/22 02:53;taoran;sequence.png;https://issues.apache.org/jira/secure/attachment/13044257/sequence.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 06 01:58:18 UTC 2022,,,,,,,,,,"0|z12qeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 02:53;taoran;random:
CREATE TABLE t1 (
    f0 BIGINT,
    f1 BINARY,
    f2 VARBINARY
) WITH (
  'connector' = 'datagen',
  'rows-per-second'='5',
  'number-of-rows'='10',
  'fields.f1.kind'='random',
  'fields.f2.kind'='random'
); 
!random.png! 

sequence:
CREATE TABLE t2 (
    f0 BIGINT,
    f1 BINARY,
    f2 VARBINARY
) WITH (
  'connector' = 'datagen',
  'rows-per-second'='5',
  'number-of-rows'='10',
  'fields.f1.kind'='sequence',
  'fields.f1.start'='100',
  'fields.f1.end'='10000',
  'fields.f2.kind'='sequence',
  'fields.f2.start'='500',
  'fields.f2.end'='50000'
);
 !sequence.png! ;;;","08/Jun/22 08:26;taoran;[~Leonard] can u help me to review this pr? thanks.  [~martijnvisser] btw, can u assign this ticket to me?;;;","06/Jul/22 01:58;leonard;Implemented in master(1.16): ef9ce854a3169014001f39e0d5908c703453f2b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump ORC version to 1.7.8,FLINK-27805,13447117,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,pgaref,sonice_lj,sonice_lj,27/May/22 02:23,17/Aug/23 10:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,stale-assigned,,,"The current ORC dependency version of flink is 1.5.6, but the latest ORC version 1.7.x has been released for a long time.

In order to use these new features (zstd compression, column encryption etc.), we should upgrade the orc version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 10:35:16 UTC 2023,,,,,,,,,,"0|z12qds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 02:39;sonice_lj;[~dongjoon] Do you still work on this? If you don't have time, I'm glad to take it.;;;","27/May/22 15:13;dongjoon;Please take over this, [~sonice_lj]. :);;;","20/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","17/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not observe cluster/job mid upgrade,FLINK-27804,13447033,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,26/May/22 15:27,27/May/22 15:39,04/Jun/24 20:51,27/May/22 13:04,kubernetes-operator-1.0.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Seems like in some weird cornercases when we observe the FINISHED job (stopped with savepoint) during an upgrade the recorded last snapshot is incorrect (still need to investigate if this is due to a Flink problem or what) This can lead to upgrade errors.

This can be avoided by simply skipping the observe step when the reconciliation status is UPGRADING because at that point we actually know that the job was already shut down and state recorded correctly in the savepoint info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 27 15:39:54 UTC 2022,,,,,,,,,,"0|z12pvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 03:07;wangyang0918;I am afraid observing the FINISHED job during an upgrade has no difference with observing an already suspended FlinkDeployment. ;;;","27/May/22 03:17;gyfora;[~wangyang0918] could you please clarify what you mean?;;;","27/May/22 03:46;wangyang0918;After we suspended a FlinkDeployment with 1.15, the JobManager is still serving for the REST API so that we could fetch the savepoint.

 

What I mean is it is just in the same situation with observing the savepoint with UPGRADING state. Right?

I do not find any potential issues we could get an incorrect savepoint/checkpoint.;;;","27/May/22 12:55;gyfora;Yes I completely agree that it should work, but we once out of every few hundred upgrades we hit these weird cases that I suspect might be caused by something like this. So with this change we can at least eliminate this root cause and see if it occurs again;;;","27/May/22 13:04;gyfora;Merged

main: aa4f1d64d223d2dfa434edcd4c2ae8a9b54d0fdf
release-1.0: fbaad0f48cb5bf3a4ca2b685846dab9c072083e0;;;","27/May/22 15:16;wangyang0918;If this issue occurs every few hundred upgrades, then I am afraid we need to cancel the current release candidate #2. WDYT?;;;","27/May/22 15:39;gyfora;The problem is that I cannot reliably say what the root cause was and whether this will solve it 100%

We will keep testing this today and if we don't hit this again during another day of functional testing I think it would be better to create an RC3 including this fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Unable to write to s3 with Hudi format via Flink - Scala,FLINK-27803,13447013,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,satnair,satnair,26/May/22 14:01,30/May/22 14:48,04/Jun/24 20:51,30/May/22 14:48,1.14.4,,,,,,,,,,,,,,,,,,Connectors / Hadoop Compatibility,,,,,1,HUDI-bug,,,,"Getting this error , when writing to S3 as Hudi file format via  Flink:

{{java.nio.file.AccessDeniedException: data: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Failed to connect to service endpoint: }}

 
 * Tried updating the core-site.xml in the class path in the libs
 * All the Hadoop and awssdk jars are in the lib folder

 

Stacktrace:

 

{{13:45:27.026 [main] ERROR org.apache.flink.core.fs.FileSystem - Failed to load a file system via services}}

{{java.util.ServiceConfigurationError: org.apache.flink.core.fs.FileSystemFactory: Provider org.apache.flink.fs.s3presto.S3FileSystemFactory could not be instantiated}}

{{at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_332]}}

{{at org.apache.flink.core.plugin.PluginLoader$ContextClassLoaderSettingIterator.next(PluginLoader.java:136) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.shaded.guava30.com.google.common.collect.Iterators$ConcatenatedIterator.next(Iterators.java:1364) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.shaded.guava30.com.google.common.collect.TransformedIterator.next(TransformedIterator.java:47) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.core.fs.FileSystem.addAllFactoriesToList(FileSystem.java:1079) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.core.fs.FileSystem.loadFileSystemFactories(FileSystem.java:1060) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.core.fs.FileSystem.initialize(FileSystem.java:340) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.configureFileSystems(ClusterEntrypoint.java:229) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:185) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:617) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:59) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{Caused by: java.lang.LinkageError: loader constraint violation: when resolving overridden method ""org.apache.flink.fs.s3presto.S3FileSystemFactory.createHadoopFileSystem()Lorg/apache/hadoop/fs/FileSystem;"" the class loader (instance of org/apache/flink/core/plugin/PluginLoader$PluginClassLoader) of the current class, org/apache/flink/fs/s3presto/S3FileSystemFactory, and its superclass loader (instance of sun/misc/Launcher$AppClassLoader), have different Class objects for the type org/apache/hadoop/fs/FileSystem used in the signature}}

{{at java.lang.Class.getDeclaredConstructors0(Native Method) ~[?:1.8.0_332]}}

{{at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) ~[?:1.8.0_332]}}

{{at java.lang.Class.getConstructor0(Class.java:3075) ~[?:1.8.0_332]}}

{{at java.lang.Class.newInstance(Class.java:412) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_332]}}

{{... 12 more}}

{{13:45:27.032 [main] ERROR org.apache.flink.core.fs.FileSystem - Failed to load a file system via services}}

{{java.util.ServiceConfigurationError: org.apache.flink.core.fs.FileSystemFactory: Provider org.apache.flink.fs.s3presto.S3PFileSystemFactory could not be instantiated}}

{{at java.util.ServiceLoader.fail(ServiceLoader.java:232) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader.access$100(ServiceLoader.java:185) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$1.next(ServiceLoader.java:480) ~[?:1.8.0_332]}}

{{at org.apache.flink.core.plugin.PluginLoader$ContextClassLoaderSettingIterator.next(PluginLoader.java:136) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.shaded.guava30.com.google.common.collect.Iterators$ConcatenatedIterator.next(Iterators.java:1364) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.shaded.guava30.com.google.common.collect.TransformedIterator.next(TransformedIterator.java:47) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.core.fs.FileSystem.addAllFactoriesToList(FileSystem.java:1079) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.core.fs.FileSystem.loadFileSystemFactories(FileSystem.java:1060) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.core.fs.FileSystem.initialize(FileSystem.java:340) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.configureFileSystems(ClusterEntrypoint.java:229) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:185) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:617) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint.main(StandaloneSessionClusterEntrypoint.java:59) [flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{Caused by: java.lang.LinkageError: loader constraint violation: when resolving overridden method ""org.apache.flink.fs.s3presto.S3FileSystemFactory.createHadoopFileSystem()Lorg/apache/hadoop/fs/FileSystem;"" the class loader (instance of org/apache/flink/core/plugin/PluginLoader$PluginClassLoader) of the current class, org/apache/flink/fs/s3presto/S3FileSystemFactory, and its superclass loader (instance of sun/misc/Launcher$AppClassLoader), have different Class objects for the type org/apache/hadoop/fs/FileSystem used in the signature}}

{{at java.lang.Class.getDeclaredConstructors0(Native Method) ~[?:1.8.0_332]}}

{{at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) ~[?:1.8.0_332]}}

{{at java.lang.Class.getConstructor0(Class.java:3075) ~[?:1.8.0_332]}}

{{at java.lang.Class.newInstance(Class.java:412) ~[?:1.8.0_332]}}

{{at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ~[?:1.8.0_332]}}

{{... 12 more}}

{{exception ignored}}

{{13:50:18.015 [flink-akka.actor.default-dispatcher-19] ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Fatal error occurred in the cluster entrypoint.}}

{{org.apache.flink.util.FlinkException: JobMaster for job da27b015abb68c146dd4306c1ed619a7 failed.}}

{{at org.apache.flink.runtime.dispatcher.Dispatcher.jobMasterFailed(Dispatcher.java:913) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.dispatcher.Dispatcher.jobManagerRunnerFailed(Dispatcher.java:473) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$runJob$3(Dispatcher.java:430) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_332]}}

{{at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_332]}}

{{at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_332]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:455) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:455) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_332]}}

{{at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_332]}}

{{at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_332]}}

{{at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_332]}}

{{Caused by: org.apache.flink.runtime.jobmaster.JobMasterException: Could not start the JobMaster.}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:391) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}

{{Caused by: org.apache.flink.util.FlinkRuntimeException: Failed to start the operator coordinators}}

{{at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:90) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:585) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:965) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:882) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:389) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}

{{Caused by: org.apache.hudi.exception.HoodieIOException: Failed to get instance of org.apache.hadoop.fs.FileSystem}}

{{at org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:104) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.tableExists(StreamerUtil.java:288) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.initTableIfNotExists(StreamerUtil.java:258) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.sink.StreamWriteOperatorCoordinator.start(StreamWriteOperatorCoordinator.java:164) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:194) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:85) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:585) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:965) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:882) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:389) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}

{{Caused by: java.nio.file.AccessDeniedException: data: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Failed to connect to service endpoint: }}

{{at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:187) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3375) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:125) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3424) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:485) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:102) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.tableExists(StreamerUtil.java:288) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.initTableIfNotExists(StreamerUtil.java:258) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.sink.StreamWriteOperatorCoordinator.start(StreamWriteOperatorCoordinator.java:164) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:194) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:85) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:585) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:965) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:882) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:389) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}

{{Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Failed to connect to service endpoint: }}

{{at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:159) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3375) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:125) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3424) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:485) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:102) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.tableExists(StreamerUtil.java:288) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.initTableIfNotExists(StreamerUtil.java:258) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.sink.StreamWriteOperatorCoordinator.start(StreamWriteOperatorCoordinator.java:164) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:194) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:85) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:585) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:965) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:882) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:389) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}

{{Caused by: com.amazonaws.SdkClientException: Failed to connect to service endpoint: }}

{{at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsEndpoint(InstanceMetadataServiceCredentialsFetcher.java:58) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsResponse(InstanceMetadataServiceCredentialsFetcher.java:46) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.BaseCredentialsFetcher.fetchCredentials(BaseCredentialsFetcher.java:112) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.BaseCredentialsFetcher.getCredentials(BaseCredentialsFetcher.java:68) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:165) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3375) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:125) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3424) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:485) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:102) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.tableExists(StreamerUtil.java:288) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.initTableIfNotExists(StreamerUtil.java:258) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.sink.StreamWriteOperatorCoordinator.start(StreamWriteOperatorCoordinator.java:164) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:194) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:85) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:585) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:965) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:882) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:389) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}

{{Caused by: java.net.ConnectException: Connection refused (Connection refused)}}

{{at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_332]}}

{{at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_332]}}

{{at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_332]}}

{{at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_332]}}

{{at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_332]}}

{{at java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_332]}}

{{at sun.net.NetworkClient.doConnect(NetworkClient.java:175) ~[?:1.8.0_332]}}

{{at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) ~[?:1.8.0_332]}}

{{at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) ~[?:1.8.0_332]}}

{{at sun.net.www.http.HttpClient.<init>(HttpClient.java:242) ~[?:1.8.0_332]}}

{{at sun.net.www.http.HttpClient.New(HttpClient.java:339) ~[?:1.8.0_332]}}

{{at sun.net.www.http.HttpClient.New(HttpClient.java:357) ~[?:1.8.0_332]}}

{{at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1228) ~[?:1.8.0_332]}}

{{at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1207) ~[?:1.8.0_332]}}

{{at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1056) ~[?:1.8.0_332]}}

{{at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:990) ~[?:1.8.0_332]}}

{{at com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:52) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsEndpoint(InstanceMetadataServiceCredentialsFetcher.java:58) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsResponse(InstanceMetadataServiceCredentialsFetcher.java:46) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.BaseCredentialsFetcher.fetchCredentials(BaseCredentialsFetcher.java:112) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.BaseCredentialsFetcher.getCredentials(BaseCredentialsFetcher.java:68) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:165) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:137) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1257) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:833) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:783) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5700) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5673) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4904) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1394) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1333) ~[aws-java-sdk-s3-1.11.563.jar:?]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3375) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:125) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3424) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3392) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:485) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365) ~[flink-s3-fs-hadoop-1.14.4.jar:1.14.4]}}

{{at org.apache.hudi.common.fs.FSUtils.getFs(FSUtils.java:102) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.tableExists(StreamerUtil.java:288) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.util.StreamerUtil.initTableIfNotExists(StreamerUtil.java:258) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.hudi.sink.StreamWriteOperatorCoordinator.start(StreamWriteOperatorCoordinator.java:164) ~[hudi-flink-bundle_2.12-0.10.1.jar:0.10.1]}}

{{at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.start(OperatorCoordinatorHolder.java:194) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.DefaultOperatorCoordinatorHandler.startAllOperatorCoordinators(DefaultOperatorCoordinatorHandler.java:85) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.scheduler.SchedulerBase.startScheduling(SchedulerBase.java:585) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startScheduling(JobMaster.java:965) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.startJobExecution(JobMaster.java:882) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.jobmaster.JobMaster.onStart(JobMaster.java:389) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStart(RpcEndpoint.java:181) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.lambda$start$0(AkkaRpcActor.java:624) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StoppedState.start(AkkaRpcActor.java:623) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:185) ~[flink-rpc-akka_480230f4-b23b-49e6-a9c3-249df9c40dc7.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]}}

{{at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]}}

{{at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.12-1.14.4.jar:1.14.4]}}

{{... 13 more}}","SBT:
===
 


libraryDependencies ++= Seq(
""org.apache.flink"" %% ""flink-scala"" % ""1.10.0"" % ""provided"",
""org.apache.flink"" %% ""flink-streaming-scala"" % ""1.10.0"" % ""provided"",
""com.amazonaws"" % ""aws-java-sdk-s3"" % ""1.11.563"" % ""provided"",
""org.apache.flink"" %% ""flink-parquet"" % ""1.14.4"" % ""provided"",
""org.apache.hadoop"" % ""hadoop-aws"" % ""3.2.2"" % ""provided"",
""org.apache.parquet"" % ""parquet-avro"" % ""1.10.1"",
""com.typesafe.scala-logging"" %% ""scala-logging"" % ""3.8.0"",
 
""org.apache.flink"" %% ""flink-table-api-scala"" % ""1.14.4"",
// ""org.apache.flink"" % ""flink-table-api-java"" % ""1.14.4"",
""org.apache.flink"" %% ""flink-table-api-scala-bridge"" % ""1.14.4"" % ""provided"",
""org.apache.flink"" %% ""flink-table-planner-blink"" % ""1.13.5"" % ""provided"",
""org.apache.flink"" %% ""flink-table-planner"" % ""1.14.4"" % ""provided"",
""org.apache.flink"" % ""flink-table-common"" % ""1.14.4"" % ""provided"",
""org.apache.flink"" %% ""flink-clients"" % ""1.14.4"" % ""provided"",
""org.apache.hudi"" % ""hudi-flink-bundle_2.11"" % ""0.10.1"" % ""provided"",

""org.apache.hudi"" % ""hudi-common"" % ""0.11.0"" % ""provided"",
""org.apache.hudi"" % ""hudi-client-common"" % ""0.11.0"" % ""provided"",
""org.apache.hudi"" % ""hudi-flink-client"" % ""0.11.0"" % ""provided"",
""org.apache.hudi"" % ""hudi-sync-common"" % ""0.11.0"" % ""provided"",
// ""com.amazonaws"" % ""aws-java-sdk-bundle"" % ""1.11.563"" % ""provided"",
""org.apache.hadoop"" % ""hadoop-common"" % ""3.2.2"" % ""provided"",
""org.apache.hadoop"" % ""hadoop-client"" % ""3.2.2"" % ""provided"",
""org.apache.parquet"" % ""parquet-hadoop"" % ""1.12.2"" % ""provided"",
""org.apache.parquet"" % ""parquet-hadoop-bundle"" % ""1.12.2"" % ""provided"",
""org.apache.flink"" % ""flink-s3-fs-hadoop"" % ""1.14.4"" % ""provided""
)

assemblyMergeStrategy in assembly := {
casePathList(""META-INF"", xs @ _*) => MergeStrategy.discard
case x => MergeStrategy.first
}
 
 
 
Docker file:
=========
FROM flink:1.14.4-scala_2.12
 
ENV FLINK_HADOOP_CONF /hadoop/conf
 
RUN curl https://archive.apache.org/dist/flink/flink-1.14.4/flink-1.14.4-bin-scala_2.12.tgz -o /opt/flink-1.14.4-bin-scala_2.12.tgz
RUN tar -xzf /opt/flink-1.14.4-bin-scala_2.12.tgz
RUN mv flink-1.14.4 /opt
 
RUN rm -rf /opt/flink/*
RUN rm -rf /opt/flink/plugins/*
RUN rm -rf /opt/flink/lib/*
 
RUN cp -R /opt/flink-1.14.4/* /opt/flink
 
RUN ls -ltra /opt/flink
 
RUN mv -v /opt/flink/opt/flink-python*.jar /opt/flink/lib/
RUN mkdir -p /opt/flink/plugins/s3-fs-presto /opt/flink/plugins/s3-fs-hadoop
RUN mv -v /opt/flink/opt/flink-s3-fs-presto-*.jar /opt/flink/plugins/s3-fs-presto
RUN mv -v /opt/flink/opt/flink-s3-fs-hadoop-*.jar /opt/flink/plugins/s3-fs-hadoop
RUN mv -v /opt/flink/opt/flink-*.jar /opt/flink/plugins/
 
RUN wget https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-10.0/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar -P /opt/flink/lib/
RUN wget https://repo1.maven.org/maven2/org/apache/hudi/hudi-flink-bundle_2.12/0.10.1/hudi-flink-bundle_2.12-0.10.1.jar -P /opt/flink/lib/
RUN wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.11.563/aws-java-sdk-s3-1.11.563.jar -P /opt/flink/lib/
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/hadoop-aws-3.2.2.jar -P /opt/flink/lib/
# RUN wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/3.2.2/hadoop-client-3.2.2.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.2.2/hadoop-common-3.2.2.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/flink/flink-table-api-scala_2.12/1.14.4/flink-table-api-scala_2.12-1.14.4.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/com/google/guava/guava/31.0-jre/guava-31.0-jre.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/flink/flink-s3-fs-hadoop/1.14.4/flink-s3-fs-hadoop-1.14.4.jar -P /opt/flink/lib/
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs-client/3.2.2/hadoop-hdfs-client-3.2.2.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.2/hadoop-mapreduce-client-core-3.3.2.jar -P /opt/flink/lib
# For Parquet
RUN wget https://repo1.maven.org/maven2/org/apache/flink/flink-parquet_2.12/1.14.4/flink-parquet_2.12-1.14.4.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.12.2/parquet-hadoop-1.12.2.jar -P /opt/flink/lib
RUN wget https://repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop-bundle/1.12.2/parquet-hadoop-bundle-1.12.2.jar -P /opt/flink/lib
 
RUN chown -R flink:flink /opt/flink/plugins/
RUN chown -R flink:flink /opt/flink/lib/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/22 13:59;satnair;Dockerfile;https://issues.apache.org/jira/secure/attachment/13044239/Dockerfile","26/May/22 14:00;satnair;core-site.xml;https://issues.apache.org/jira/secure/attachment/13044238/core-site.xml",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,scala,,Mon May 30 14:48:20 UTC 2022,,,,,,,,,,"0|z12pqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:48;martijnvisser;[~satnair] Thanks for opening this ticket, but the Flink community is not involved with the Hudi connector. I would recommend raising this issue at https://github.com/apache/hudi;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint restore errors are swallowed for Flink 1.15,FLINK-27802,13446999,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Won't Fix,,gyfora,gyfora,26/May/22 12:41,06/Jul/22 13:05,04/Jun/24 20:51,06/Jul/22 13:05,kubernetes-operator-1.0.0,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"When a job is submitted with an incorrect savepoint path the error is swallowed by Flink due to the result store:

2022-05-26 12:34:43,497 WARN org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Ignoring JobGraph submission 'State machine job' (00000000000000000000000000000000) because the job already reached a globally-terminal state (i.e. FAILED, CANCELED, FINISHED) in a previous execution.
2022-05-26 12:34:43,552 INFO org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application completed SUCCESSFULLY

The easiest way to reproduce this is to create a new deployment and set initialSavepointPath to a random missing path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 27 13:12:36 UTC 2022,,,,,,,,,,"0|z12pns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/22 12:52;gyfora;cc [~wangyang0918] ;;;","27/May/22 03:31;wangyang0918;I believe you could find the root cause in the JobManager logs of first attempt. This need to be fixed in the upstream Flink because we expect the JobManager not exit when {{execution.shutdown-on-application-finish}} is false.

 

kubectl logs flink-example-statemachine-6d596bfdcd-jtmn9 --previous | less

 
{code:java}
2022-05-26 11:52:18,085 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-05-26 11:52:18,087 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Starting job 00000000000000000000000000000000 from savepoint file:///non/exist/sp ()
2022-05-26 11:52:18,174 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 00000000000000000000000000000000 reached terminal state FAILED.
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
        at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)
        at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.CompletionException: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:///non/exist/sp' on file system 'file'.
        at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source)
        at java.base/java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source)
        ... 4 more
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:///non/exist/sp' on file system 'file'.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:319)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:114)
        ... 4 more
Caused by: java.io.FileNotFoundException: Cannot find checkpoint or savepoint file/directory 'file:///non/exist/sp' on file system 'file'.
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:275)
        at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136)
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1746)
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:206)
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:181)
        at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:363)
        at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:208)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:191)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:139)
        at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:135)
        at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:115)
        at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:345)
        at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:322)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:106)
        at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:94)
        at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
        ... 4 more {code}
 ;;;","27/May/22 13:12;gyfora;that's true, but people generally look at the logs of the last attempt first which at this point will have no information.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable flushing stream when call Jackson's writeValue() method for CsvBulkWriter,FLINK-27801,13446996,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,luoyuxia,luoyuxia,26/May/22 12:17,19/Aug/23 10:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,"Currently, in `CsvBulkWriter`,  for every single element,  it's call Jackson's writeValue to write the element and then flush.  It's time consuming to flush for every single element. we should also diasable flushing when call Jackson's writeValue method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:05 UTC 2023,,,,,,,,,,"0|z12pn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","02/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
addInEdge check state error,FLINK-27800,13446986,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Licho,Licho,26/May/22 10:14,16/Feb/23 08:06,04/Jun/24 20:51,16/Feb/23 08:06,1.15.3,1.16.1,1.17.0,,,,,,,,,,,,1.15.4,1.16.2,1.17.0,,API / Core,,,,,0,pull-request-available,,,,"when add InEdge, the checkState fucntion check the edge whether is in outEdges list, this should check whether in inEdges list.

 
{code:java}
public void addInEdge(StreamEdge inEdge) {
    checkState(
            outEdges.stream().noneMatch(inEdge::equals),
            ""Adding not unique edge = %s to existing outEdges = %s"",
            inEdge,
            inEdges);
    if (inEdge.getTargetId() != getId()) {
        throw new IllegalArgumentException(""Destination id doesn't match the StreamNode id"");
    } else {
        inEdges.add(inEdge);
    }
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 16 08:03:39 UTC 2023,,,,,,,,,,"0|z12pkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 08:03;wanglijie;Fixed via
master: 756eeee8628ee7e81ca7b1b4ae80aa6cddd7284d
release-1.17: b64739a5ef976e003bf87250b41ae1142e541497
release-1.16: cca9ee1ac60810098bcf3dbf3013f4d6c91a10a8
release-1.15: c6b649bf937976038dbfcd00e59c51b8d886ad96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Version 1.13.5 is not compatible with version 1.10 UDF,FLINK-27799,13446983,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,yangyichao,yangyichao,26/May/22 09:34,30/May/22 14:50,04/Jun/24 20:51,30/May/22 14:50,1.13.5,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"Flink 1.10 Version，The following code will work

 
{code:java}
// UDF
public class SetStringUDF extends ScalarFunction {

//    @DataTypeHint(""RAW"")
    public Set<String> eval(String input) {
        return Sets.newHashSet(input, input + ""_1"", input + ""_2"");
    }

    @Override
    public TypeInformation<?> getResultType(Class<?>[] signature) {
        return TypeInformation.of(new TypeHint<Set<String>>() {
        });
    }

}

public class GetSetValue extends ScalarFunction {

    public String eval(Set<String> set) {
        return set.iterator().next();
    }
}


StreamTableEnvironment.createFunction(""set_string"", SetStringUDF.class); StreamTableEnvironment.createFunction(""get_set_value"", GetSetValue.class);

CREATE TABLE Orders (
    order_id BIGINT NOT NULL,
    name STRING,
    row_time AS cast(CURRENT_TIMESTAMP as timestamp(3)),
    WATERMARK FOR row_time AS row_time - INTERVAL '5' SECOND
) WITH (
  'connector' = 'datagen',
  'rows-per-second' = '10',
  'fields.name.length' = '1',
  'fields.order_id.min' = '1',
  'fields.order_id.max' = '10'
);CREATE TABLE target_table (
    order_id BIGINT NOT NULL,
    name STRING,
    row_time timestamp(3),
    i STRING
) WITH (
  'connector' = 'print'
);
INSERT INTO target_table
SELECT *, cast(get_set_value(set_string(name)) as string) as i
FROM Orders{code}
but in Flink 1.13.5，it will throw exception like:

 

 
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Could not extract a data type from 'java.util.Set<java.lang.String>'. Interpreting it as a structured type was also not successful.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:361)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrError(DataTypeExtractor.java:291)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrRawWithTemplate(DataTypeExtractor.java:233)
    ... 36 more
Caused by: org.apache.flink.table.api.ValidationException: Class 'java.util.Set' must not be abstract.
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:361)
    at org.apache.flink.table.types.extraction.ExtractionUtils.extractionError(ExtractionUtils.java:356)
    at org.apache.flink.table.types.extraction.ExtractionUtils.validateStructuredClass(ExtractionUtils.java:164)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractStructuredType(DataTypeExtractor.java:479)
    at org.apache.flink.table.types.extraction.DataTypeExtractor.extractDataTypeOrError(DataTypeExtractor.java:289)
    ... 37 more {code}
 

 

I have to change my UDF to fix this problem.

 
{code:java}
public class GetSetValue extends ScalarFunction {

    public String eval(@DataTypeHint(""RAW"") Object set) {
        
        Set<String> s = (Set<String>) set;
        
        return s.iterator().next();
    }
}

public class SetStringUDF extends ScalarFunction {

    @DataTypeHint(""RAW"")
    public Object eval(String input) {
        return Sets.newHashSet(input, input + ""_1"", input + ""_2"");
    }

}
 {code}
 

 

I have two questions：
 # At present, is there a way to be compatible with this problem without changing the code？
 # If 1 is not。We need fix all of the UDFs，it will be a lot work to do. Can there be a plan to complete compatibility in the future",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 14:50:20 UTC 2022,,,,,,,,,,"0|z12pk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:50;martijnvisser;[~yangyichao] Unfortunately there's currently no way to work around this. As with any new Flink version, changes can be made to interfaces that are annotated as Experimental or PublicEvolving. Only interfaces annotated with Public won't change unless Flink is bumped to a new major version. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate Flink ML to Flink 1.15.0,FLINK-27798,13446963,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,26/May/22 07:40,24/Jun/22 07:50,04/Jun/24 20:51,02/Jun/22 01:25,ml-2.1.0,,,,,,,,,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,Update Flink ML's Flink dependency to 1.15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-26 07:40:49.0,,,,,,,,,,"0|z12pfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonTableUtils.getCollectionInputFormat cannot correctly handle None values,FLINK-27797,13446960,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zsigner,yunfengzhou,yunfengzhou,26/May/22 07:31,01/Jun/22 12:50,04/Jun/24 20:51,01/Jun/22 12:50,1.15.0,,,,,,,,,,,,,,1.15.1,,,,API / Python,,,,,0,pull-request-available,,,,"In `PythonTableUtils.getCollectionInputFormat` there are implementations like follows.
This code can be found at [https://github.com/apache/flink/blob/8488368b86a99a064446ca74e775b67ffff0b94a/flink-python/src/main/java/org/apache/flink/table/utils/python/PythonTableUtils.java#L515]

```
c -> {
            if (c.getClass() != byte[].class || dataType instanceof PickledByteArrayTypeInfo) {
                return c;
            }
```

Here, the generated function did not check `c != null` before doing `c.getClass()`. which might cause that tables created through pyflink cannot parse it when values are `None`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 12:50:08 UTC 2022,,,,,,,,,,"0|z12pf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 11:21;Zsigner;can I get this ticket？;;;","01/Jun/22 12:50;hxbks2ks;Merged into master via 586715f23ef49939ab74e4736c58d71c643a64ba
Merged into release-1.15 via d5edd4c346985483179504e23d98182941b1657a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document approaches to updating existing instances of stateful functions,FLINK-27796,13446939,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,billrao,billrao,26/May/22 05:03,26/May/22 05:03,04/Jun/24 20:51,,statefun-3.2.0,,,,,,,,,,,,,,,,,,Stateful Functions,,,,,1,,,,,"As a startup company utilizing stateful functions, we often meet the needs to modify our applications after it is deployed. Some documentation on the best practice / methods of upgrading the application would be helpful.

 

For version 3.2, it seems to me that merely changing an ingress configuration in `module.yaml` and restart deployment related to statefun would break the running application. After I recovered the ingress definition to previoius version, the checkpoint restore was successful. I think that it is necessary to have a Quick Reference / Guide on how to upgrade, and which configurations should be considered immutable. 

 

A documentation from the maintainer would obviously be preferred. I could also write a skeleton that includes some of my tried-and-true approaches, so that the community can build on top of it by correcting some of the mistakes in my approach.

 

Let me know how I can be of help, or if there are other documents I can refer to.

 

Thank you.",,18000,18000,,0%,18000,18000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-26 05:03:19.0,,,,,,,,,,"0|z12pag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlValidatorException when insert null value,FLINK-27795,13446932,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,linqichen178,linqichen178,26/May/22 04:00,16/Jun/23 06:48,04/Jun/24 20:51,16/Jun/23 06:48,1.14.4,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"Hello , I found a problem， when i try to insert null value. 

i think it is very common to insert null to string type value. We should solve this instead of using  cast (xx as string)

-----------------------------------------------------------------

Flink SQL> create table print_1(
>   name string
> ) with(
> 'connector'='print'
> );
[INFO] Execute statement succeed.

Flink SQL> insert into print_1 (name) values (null);
{color:#de350b}*[ERROR] Could not execute SQL statement. Reason:*{color}
{color:#de350b}*org.apache.calcite.sql.validate.SqlValidatorException: Illegal use of 'NULL'*{color}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17484,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 27 06:49:22 UTC 2022,,,,,,,,,,"0|z12p8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 04:30;fsk119;Hi. I think it's by design. In the SQL world, we can insert value into the table only if the type is same. So we need to derive the type from the value in your SQL. Here the type of the value NULL is NullType that is not equal to the type in the sink(Varchar type). Therefore, you should cast explictly. ;;;","27/May/22 04:52;linqichen178;Thanks shengkai fang!

I donot think so. Because most of time we cannot know the field‘s value is null or not。 It is very  unfriendly to add cast(xx as xx) on all field。 

I have an idea about this. that is : 

we use one configation on flink-conf.yml . Such as : cast_null_to_field_type

when we set it true,  flink engine should do something like cast(xx as xx)。 ;;;","27/May/22 06:49;fsk119;The question is whether Flink should supports implicitly type coercion here, which is a feature that Flink doesn't support yet. 

Actually Flink proposes the discussion about type coercion in the [FLIP-154|https://cwiki.apache.org/confluence/display/FLINK/FLIP-154%3A+SQL+Implicit+Type+Coercion]. But it has been slience for a long time. 

The correct way to do this feature it to enable the config option in the FlinkPlannerImpl[1]. But it's about the implementation and we can discuss after the FLIP-154 is accepted.

[1] https://github.com/apache/flink/blob/bf342d2f67a46e5266c3595734574db270f1b48c/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/calcite/FlinkPlannerImpl.scala#L102;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The primary key obtained from MySQL is incorrect by using MysqlCatalog,FLINK-27794,13446929,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dusukang,dusukang,dusukang,26/May/22 03:36,05/Jul/22 14:09,04/Jun/24 20:51,27/Jun/22 09:27,1.15.0,,,,,,,,,,,,,,1.15.2,1.16.0,,,Connectors / JDBC,,,,,0,pull-request-available,,,,"I want to use MysqlCatalog to get the primary key of the database table `user`. The database table creation statement is as follows
{code:java}
CREATE TABLE flinksql_test.`user` (
  `uid` bigint(20) NOT NULL,
  `uname` varchar(36) DEFAULT NULL,
  `others` varchar(128) DEFAULT NULL,
  PRIMARY KEY (`uid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8; {code}
 

This is my test code:
{code:java}
import org.apache.flink.connector.jdbc.catalog.MySqlCatalog;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.catalog.CatalogBaseTable;
import org.apache.flink.table.catalog.ObjectPath;
import org.apache.flink.table.catalog.exceptions.TableNotExistException;import java.util.Optional;public class Demo02 {
    public static void main(String[] args) throws TableNotExistException {
        MySqlCatalog mySqlCatalog = new MySqlCatalog(""mysql-catalog"",
                ""flinksql_test"",
                ""root"",
                ""123456789"",
                String.format(""jdbc:mysql://127.0.0.1:3306""));
        CatalogBaseTable table = mySqlCatalog.getTable(new ObjectPath(""flinksql_test"", ""user""));
        Optional<Schema.UnresolvedPrimaryKey> primaryKey = table
                .getUnresolvedSchema()
                .getPrimaryKey();
        System.out.println(primaryKey);
    }
} {code}
 

The obtained primary key is (Host,User), but the primary key from Database is (uid)

!167908239-c6f3f0ad-af06-436f-87e2-85c60428b400.png!

 

I see, the value of the incoming catalog and schema is null, and the SQL splicing of the database to obtain the primary key does not add "" TABLE_SCHEMA LIKE ? AND""

!https://user-images.githubusercontent.com/68139929/167910188-6df6f3ec-cb33-49cc-91d5-61dcb1167c98.png!

!https://user-images.githubusercontent.com/68139929/167908960-cf873c66-a227-41fa-99ea-5cff8a181f29.png!

!https://user-images.githubusercontent.com/68139929/167909024-22b15192-0755-416e-8421-dab0fdfc0d15.png!

Later, it was found that there was also a user table in the self-contained MySQL database with the following structure:

 
{code:java}
CREATE TABLE mysql.`user` (
  `Host` char(60) COLLATE utf8_bin NOT NULL DEFAULT '',
  `User` char(32) COLLATE utf8_bin NOT NULL DEFAULT '',
  `Select_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Insert_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Update_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Delete_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Drop_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Reload_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Shutdown_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Process_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `File_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Grant_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `References_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Index_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Alter_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Show_db_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Super_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_tmp_table_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Lock_tables_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Execute_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Repl_slave_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Repl_client_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_view_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Show_view_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_routine_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Alter_routine_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_user_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Event_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Trigger_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_tablespace_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `ssl_type` enum('','ANY','X509','SPECIFIED') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT '',
  `ssl_cipher` blob NOT NULL,
  `x509_issuer` blob NOT NULL,
  `x509_subject` blob NOT NULL,
  `max_questions` int(11) unsigned NOT NULL DEFAULT '0',
  `max_updates` int(11) unsigned NOT NULL DEFAULT '0',
  `max_connections` int(11) unsigned NOT NULL DEFAULT '0',
  `max_user_connections` int(11) unsigned NOT NULL DEFAULT '0',
  `plugin` char(64) COLLATE utf8_bin NOT NULL DEFAULT 'caching_sha2_password',
  `authentication_string` text COLLATE utf8_bin,
  `password_expired` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `password_last_changed` timestamp NULL DEFAULT NULL,
  `password_lifetime` smallint(5) unsigned DEFAULT NULL,
  `account_locked` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Create_role_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Drop_role_priv` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL DEFAULT 'N',
  `Password_reuse_history` smallint(5) unsigned DEFAULT NULL,
  `Password_reuse_time` smallint(5) unsigned DEFAULT NULL,
  `Password_require_current` enum('N','Y') CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL,
  `User_attributes` json DEFAULT NULL,
  PRIMARY KEY (`Host`,`User`)
) /*!50100 TABLESPACE `mysql` */ ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0 COMMENT='Users and global privileges'; {code}
 

I think it while happen when there are multiple tables which have same table name, so we can pass the table schema to get primary key?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/22 03:44;dusukang;167908239-c6f3f0ad-af06-436f-87e2-85c60428b400.png;https://issues.apache.org/jira/secure/attachment/13044213/167908239-c6f3f0ad-af06-436f-87e2-85c60428b400.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 24 12:04:44 UTC 2022,,,,,,,,,,"0|z12p88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:51;martijnvisser;[~RocMarshal] Can you have a look? ;;;","31/May/22 12:46;RocMarshal;Thanks [~dusukang] for the reporting it and [~martijnvisser] ping. Please let me have a check.;;;","24/Jun/22 12:04;leonard;Fixed in:
 master(1.16) d0cee16e6db5f6279697924a2a18c685e576955b
 release-1.15: 1d0dac0e00739a4cf176cba6dc091662e48170c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After upgrading to flink-1.15.0, flink jobmanager cannot shutdown",FLINK-27793,13446927,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jinshuangxian,jinshuangxian,26/May/22 03:33,30/May/22 14:52,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"Under normal circumstances, after the taskmanager fails, it will retry multiple times. After reaching the number of failures, the task will exit.

After upgrading to flink-1.15.0, the test found that when the taskmanager was abnormal, the job was completed, but the task jobmanager was still running normally, and the yarn application -list commmand found that the task was still running.

Another problem, when the command ""flink stop"" is executed, the task stop success, i found that the task is still running normally on yarn.","Flink 1.15.0

Hadoop 2.7.3.2.6.5.0-292

Start the task with yarn per job",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/22 12:43;jinshuangxian;image-2022-05-26-20-43-49-784.png;https://issues.apache.org/jira/secure/attachment/13044232/image-2022-05-26-20-43-49-784.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 27 02:17:16 UTC 2022,,,,,,,,,,"0|z12p7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/22 12:52;jinshuangxian;!image-2022-05-26-20-43-49-784.png!;;;","26/May/22 13:14;JasonLee;hi maybe You can upgrade hadoop to above 2.8.5 and try it out Because the lowest hadoop version supported by Flink version 1.15.0 is 2.8.5 [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/#upgrade-the-minimal-supported-hadoop-version-to-285]  My Hadoop version is 2.9.0 and I did not experience this problem when I upgraded flink to 1.15.0.;;;","27/May/22 02:17;jinshuangxian;{color:#0747a6}JasonLee{color} thank you, i will try;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InterruptedException thrown by ChannelStateWriterImpl,FLINK-27792,13446918,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,atri,hxbks2ks,hxbks2ks,26/May/22 02:27,22/Jun/22 10:51,04/Jun/24 20:51,22/Jun/22 10:51,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,test-stability,,,,"
{code:java}
2022-05-25T15:45:17.7584795Z May 25 15:45:17 [ERROR] WindowDistinctAggregateITCase.testTumbleWindow_Rollup  Time elapsed: 1.522 s  <<< ERROR!
2022-05-25T15:45:17.7586025Z May 25 15:45:17 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T15:45:17.7587205Z May 25 15:45:17 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-25T15:45:17.7588649Z May 25 15:45:17 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-05-25T15:45:17.7589984Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-25T15:45:17.7603647Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-25T15:45:17.7605042Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T15:45:17.7605750Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-25T15:45:17.7606751Z May 25 15:45:17 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-05-25T15:45:17.7607513Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-25T15:45:17.7608232Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-25T15:45:17.7608953Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T15:45:17.7614259Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-25T15:45:17.7615777Z May 25 15:45:17 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-05-25T15:45:17.7617284Z May 25 15:45:17 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-25T15:45:17.7618847Z May 25 15:45:17 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-25T15:45:17.7620579Z May 25 15:45:17 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-25T15:45:17.7622674Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-25T15:45:17.7624066Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-25T15:45:17.7625352Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T15:45:17.7626524Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-25T15:45:17.7627743Z May 25 15:45:17 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-25T15:45:17.7628913Z May 25 15:45:17 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-25T15:45:17.7629902Z May 25 15:45:17 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-25T15:45:17.7630891Z May 25 15:45:17 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-25T15:45:17.7632074Z May 25 15:45:17 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-25T15:45:17.7654202Z May 25 15:45:17 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-25T15:45:17.7655764Z May 25 15:45:17 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-25T15:45:17.7657231Z May 25 15:45:17 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-25T15:45:17.7658586Z May 25 15:45:17 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-25T15:45:17.7660014Z May 25 15:45:17 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-25T15:45:17.7661422Z May 25 15:45:17 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-25T15:45:17.7663302Z May 25 15:45:17 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-25T15:45:17.7664594Z May 25 15:45:17 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-25T15:45:17.7665956Z May 25 15:45:17 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-25T15:45:17.7667111Z May 25 15:45:17 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-25T15:45:17.7668159Z May 25 15:45:17 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-25T15:45:17.7669229Z May 25 15:45:17 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-25T15:45:17.7670271Z May 25 15:45:17 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-25T15:45:17.7671379Z May 25 15:45:17 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-25T15:45:17.7673048Z May 25 15:45:17 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-25T15:45:17.7725189Z May 25 15:45:17 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-25T15:45:17.7753834Z May 25 15:45:17 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-25T15:45:17.7805369Z May 25 15:45:17 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-25T15:45:17.7806838Z May 25 15:45:17 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-25T15:45:17.7808159Z May 25 15:45:17 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-25T15:45:17.7809497Z May 25 15:45:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-25T15:45:17.7810702Z May 25 15:45:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-25T15:45:17.7812155Z May 25 15:45:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-25T15:45:17.7813377Z May 25 15:45:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-25T15:45:17.7814839Z May 25 15:45:17 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2022-05-25T15:45:17.7816569Z May 25 15:45:17 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-25T15:45:17.7818320Z May 25 15:45:17 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-25T15:45:17.7819932Z May 25 15:45:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:300)
2022-05-25T15:45:17.7821396Z May 25 15:45:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:290)
2022-05-25T15:45:17.7822960Z May 25 15:45:17 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:281)
2022-05-25T15:45:17.7824471Z May 25 15:45:17 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-05-25T15:45:17.7826204Z May 25 15:45:17 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-05-25T15:45:17.7827583Z May 25 15:45:17 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-05-25T15:45:17.7828764Z May 25 15:45:17 	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
2022-05-25T15:45:17.7829926Z May 25 15:45:17 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-25T15:45:17.7831105Z May 25 15:45:17 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-25T15:45:17.7832521Z May 25 15:45:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-05-25T15:45:17.7834045Z May 25 15:45:17 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-25T15:45:17.7835535Z May 25 15:45:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-05-25T15:45:17.7836936Z May 25 15:45:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-25T15:45:17.7838289Z May 25 15:45:17 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-25T15:45:17.7839679Z May 25 15:45:17 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-25T15:45:17.7840897Z May 25 15:45:17 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-25T15:45:17.7842452Z May 25 15:45:17 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-25T15:45:17.7843596Z May 25 15:45:17 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-25T15:45:17.7844725Z May 25 15:45:17 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-25T15:45:17.7845896Z May 25 15:45:17 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-25T15:45:17.7847076Z May 25 15:45:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-25T15:45:17.7848258Z May 25 15:45:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-25T15:45:17.7849401Z May 25 15:45:17 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-25T15:45:17.7850510Z May 25 15:45:17 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-25T15:45:17.7851532Z May 25 15:45:17 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-25T15:45:17.7852808Z May 25 15:45:17 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-25T15:45:17.7853919Z May 25 15:45:17 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-25T15:45:17.7855126Z May 25 15:45:17 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-25T15:45:17.7856172Z May 25 15:45:17 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-25T15:45:17.7857219Z May 25 15:45:17 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-25T15:45:17.7858209Z May 25 15:45:17 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-25T15:45:17.7859009Z May 25 15:45:17 	... 4 more
2022-05-25T15:45:17.7859790Z May 25 15:45:17 Caused by: java.io.IOException: java.lang.InterruptedException
2022-05-25T15:45:17.7861180Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
2022-05-25T15:45:17.7862855Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
2022-05-25T15:45:17.7864447Z May 25 15:45:17 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
2022-05-25T15:45:17.7866077Z May 25 15:45:17 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
2022-05-25T15:45:17.7867645Z May 25 15:45:17 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-05-25T15:45:17.7868821Z May 25 15:45:17 	at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
2022-05-25T15:45:17.7870194Z May 25 15:45:17 	at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
2022-05-25T15:45:17.7871500Z May 25 15:45:17 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
2022-05-25T15:45:17.7872941Z May 25 15:45:17 	at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
2022-05-25T15:45:17.7874230Z May 25 15:45:17 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-05-25T15:45:17.7875430Z May 25 15:45:17 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
2022-05-25T15:45:17.7876630Z May 25 15:45:17 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-05-25T15:45:17.7877772Z May 25 15:45:17 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-05-25T15:45:17.7878769Z May 25 15:45:17 	at java.lang.Thread.run(Thread.java:748)
2022-05-25T15:45:17.7879657Z May 25 15:45:17 Caused by: java.lang.InterruptedException
2022-05-25T15:45:17.7880692Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
2022-05-25T15:45:17.7882088Z May 25 15:45:17 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-05-25T15:45:17.7883596Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildFutureWriteRequest$4(ChannelStateWriteRequest.java:113)
2022-05-25T15:45:17.7885476Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.cancel(ChannelStateWriteRequest.java:253)
2022-05-25T15:45:17.7887201Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.lambda$null$1(ChannelStateWriteRequestExecutorImpl.java:117)
2022-05-25T15:45:17.7888654Z May 25 15:45:17 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-05-25T15:45:17.7889760Z May 25 15:45:17 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-05-25T15:45:17.7891233Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.cleanupRequests(ChannelStateWriteRequestExecutorImpl.java:115)
2022-05-25T15:45:17.7892866Z May 25 15:45:17 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-05-25T15:45:17.7893951Z May 25 15:45:17 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-05-25T15:45:17.7895045Z May 25 15:45:17 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
2022-05-25T15:45:17.7896427Z May 25 15:45:17 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:80)
2022-05-25T15:45:17.7897602Z May 25 15:45:17 	... 1 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36066&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27870,FLINK-27915,,,FLINK-28077,,,,,,,,,,FLINK-28077,,,,,"18/Jun/22 08:39;fanrui;image-2022-06-18-16-39-54-402.png;https://issues.apache.org/jira/secure/attachment/13045242/image-2022-06-18-16-39-54-402.png","18/Jun/22 08:42;fanrui;image-2022-06-18-16-42-13-669.png;https://issues.apache.org/jira/secure/attachment/13045243/image-2022-06-18-16-42-13-669.png","18/Jun/22 08:42;fanrui;image-2022-06-18-16-42-46-283.png;https://issues.apache.org/jira/secure/attachment/13045244/image-2022-06-18-16-42-46-283.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 10:21:52 UTC 2022,,,,,,,,,,"0|z12p5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 05:33;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36393&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

{code}
2022-06-07T21:28:31.8887838Z Jun 07 21:28:31 [ERROR] SplitAggregateITCase.testMinMaxWithRetraction  Time elapsed: 0.859 s  <<< ERROR!
2022-06-07T21:28:31.8889036Z Jun 07 21:28:31 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-07T21:28:31.8890508Z Jun 07 21:28:31 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-06-07T21:28:31.8892167Z Jun 07 21:28:31 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-06-07T21:28:31.8895974Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-06-07T21:28:31.8899142Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-06-07T21:28:31.8900476Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-07T21:28:31.8901463Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-07T21:28:31.8902561Z Jun 07 21:28:31 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-06-07T21:28:31.8903305Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-07T21:28:31.8904025Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-07T21:28:31.8904992Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-07T21:28:31.8905818Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-07T21:28:31.8906511Z Jun 07 21:28:31 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-06-07T21:28:31.8909228Z Jun 07 21:28:31 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-06-07T21:28:31.8911217Z Jun 07 21:28:31 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-06-07T21:28:31.8914114Z Jun 07 21:28:31 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-06-07T21:28:31.8915794Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-07T21:28:31.8916687Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-07T21:28:31.8917388Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-07T21:28:31.8918382Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-07T21:28:31.8919414Z Jun 07 21:28:31 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-06-07T21:28:31.8920600Z Jun 07 21:28:31 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-06-07T21:28:31.8921168Z Jun 07 21:28:31 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-06-07T21:28:31.8921957Z Jun 07 21:28:31 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-06-07T21:28:31.8922799Z Jun 07 21:28:31 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-06-07T21:28:31.8923653Z Jun 07 21:28:31 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-07T21:28:31.8924819Z Jun 07 21:28:31 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-06-07T21:28:31.8926033Z Jun 07 21:28:31 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-06-07T21:28:31.8927067Z Jun 07 21:28:31 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-06-07T21:28:31.8928341Z Jun 07 21:28:31 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-06-07T21:28:31.8929646Z Jun 07 21:28:31 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-06-07T21:28:31.8930848Z Jun 07 21:28:31 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-06-07T21:28:31.8932294Z Jun 07 21:28:31 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-06-07T21:28:31.8933612Z Jun 07 21:28:31 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-06-07T21:28:31.8934751Z Jun 07 21:28:31 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-06-07T21:28:31.8935782Z Jun 07 21:28:31 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-06-07T21:28:31.8936760Z Jun 07 21:28:31 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-06-07T21:28:31.8937754Z Jun 07 21:28:31 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-07T21:28:31.8938912Z Jun 07 21:28:31 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-06-07T21:28:31.8940174Z Jun 07 21:28:31 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-06-07T21:28:31.8941371Z Jun 07 21:28:31 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-06-07T21:28:31.8942752Z Jun 07 21:28:31 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-06-07T21:28:31.8943895Z Jun 07 21:28:31 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-06-07T21:28:31.8945024Z Jun 07 21:28:31 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-06-07T21:28:31.8946197Z Jun 07 21:28:31 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-06-07T21:28:31.8947434Z Jun 07 21:28:31 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-07T21:28:31.8948698Z Jun 07 21:28:31 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-07T21:28:31.8949715Z Jun 07 21:28:31 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-07T21:28:31.8950840Z Jun 07 21:28:31 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-07T21:28:31.8952983Z Jun 07 21:28:31 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2022-06-07T21:28:31.8954597Z Jun 07 21:28:31 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-06-07T21:28:31.8956390Z Jun 07 21:28:31 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-06-07T21:28:31.8957817Z Jun 07 21:28:31 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:300)
2022-06-07T21:28:31.8959209Z Jun 07 21:28:31 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:290)
2022-06-07T21:28:31.8960649Z Jun 07 21:28:31 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:281)
2022-06-07T21:28:31.8962353Z Jun 07 21:28:31 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-06-07T21:28:31.8963695Z Jun 07 21:28:31 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-06-07T21:28:31.8965091Z Jun 07 21:28:31 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-06-07T21:28:31.8966076Z Jun 07 21:28:31 	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
2022-06-07T21:28:31.8967069Z Jun 07 21:28:31 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-07T21:28:31.8968144Z Jun 07 21:28:31 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-07T21:28:31.8969269Z Jun 07 21:28:31 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-06-07T21:28:31.8979571Z Jun 07 21:28:31 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-07T21:28:31.8980864Z Jun 07 21:28:31 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-06-07T21:28:31.8982069Z Jun 07 21:28:31 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-06-07T21:28:31.8982865Z Jun 07 21:28:31 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-06-07T21:28:31.8983602Z Jun 07 21:28:31 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-06-07T21:28:31.8984249Z Jun 07 21:28:31 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-07T21:28:31.8984825Z Jun 07 21:28:31 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-07T21:28:31.8985413Z Jun 07 21:28:31 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-07T21:28:31.8986032Z Jun 07 21:28:31 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-07T21:28:31.8986928Z Jun 07 21:28:31 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-07T21:28:31.8987568Z Jun 07 21:28:31 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-07T21:28:31.8988194Z Jun 07 21:28:31 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-07T21:28:31.8988794Z Jun 07 21:28:31 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-07T21:28:31.8989519Z Jun 07 21:28:31 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-07T21:28:31.8990357Z Jun 07 21:28:31 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-07T21:28:31.8990956Z Jun 07 21:28:31 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-07T21:28:31.8991716Z Jun 07 21:28:31 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-07T21:28:31.8992446Z Jun 07 21:28:31 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-07T21:28:31.8993150Z Jun 07 21:28:31 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-07T21:28:31.8993866Z Jun 07 21:28:31 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-07T21:28:31.8994484Z Jun 07 21:28:31 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-07T21:28:31.8995158Z Jun 07 21:28:31 	... 4 more
2022-06-07T21:28:31.8995605Z Jun 07 21:28:31 Caused by: java.io.IOException: java.lang.InterruptedException
2022-06-07T21:28:31.8996345Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
2022-06-07T21:28:31.8997335Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
2022-06-07T21:28:31.8998654Z Jun 07 21:28:31 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
2022-06-07T21:28:31.9000135Z Jun 07 21:28:31 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
2022-06-07T21:28:31.9001430Z Jun 07 21:28:31 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-07T21:28:31.9002946Z Jun 07 21:28:31 	at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
2022-06-07T21:28:31.9004139Z Jun 07 21:28:31 	at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
2022-06-07T21:28:31.9005256Z Jun 07 21:28:31 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
2022-06-07T21:28:31.9006414Z Jun 07 21:28:31 	at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
2022-06-07T21:28:31.9007589Z Jun 07 21:28:31 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-06-07T21:28:31.9008923Z Jun 07 21:28:31 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
2022-06-07T21:28:31.9009938Z Jun 07 21:28:31 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-06-07T21:28:31.9010922Z Jun 07 21:28:31 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-06-07T21:28:31.9012092Z Jun 07 21:28:31 	at java.lang.Thread.run(Thread.java:748)
2022-06-07T21:28:31.9012842Z Jun 07 21:28:31 Caused by: java.lang.InterruptedException
2022-06-07T21:28:31.9013660Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
2022-06-07T21:28:31.9014696Z Jun 07 21:28:31 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-07T21:28:31.9015909Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildFutureWriteRequest$4(ChannelStateWriteRequest.java:113)
2022-06-07T21:28:31.9017397Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.cancel(ChannelStateWriteRequest.java:253)
2022-06-07T21:28:31.9018866Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.lambda$null$1(ChannelStateWriteRequestExecutorImpl.java:117)
2022-06-07T21:28:31.9020107Z Jun 07 21:28:31 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-07T21:28:31.9020926Z Jun 07 21:28:31 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-06-07T21:28:31.9022360Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.cleanupRequests(ChannelStateWriteRequestExecutorImpl.java:115)
2022-06-07T21:28:31.9023573Z Jun 07 21:28:31 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-07T21:28:31.9024467Z Jun 07 21:28:31 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-06-07T21:28:31.9025216Z Jun 07 21:28:31 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
2022-06-07T21:28:31.9026132Z Jun 07 21:28:31 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:80)
2022-06-07T21:28:31.9035196Z Jun 07 21:28:31 	... 1 more
2022-06-07T21:28:31.9035721Z Jun 07 21:28:31 
{code};;;","11/Jun/22 08:50;gaborgsomogyi;It would be good to be solved because it blocks other PRs to be merged: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36558&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","15/Jun/22 12:18;jark;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36734&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

{code}
2022-06-15T11:23:12.7310886Z Jun 15 11:23:12 [ERROR] Tests run: 64, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 39.052 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.SplitAggregateITCase
2022-06-15T11:23:12.7312162Z Jun 15 11:23:12 [ERROR] SplitAggregateITCase.testAggWithJoin  Time elapsed: 1.021 s  <<< ERROR!
2022-06-15T11:23:12.7312969Z Jun 15 11:23:12 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-06-15T11:23:12.7314214Z Jun 15 11:23:12 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-06-15T11:23:12.7315346Z Jun 15 11:23:12 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-06-15T11:23:12.7316235Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-06-15T11:23:12.7317279Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-06-15T11:23:12.7317996Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-15T11:23:12.7318896Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-15T11:23:12.7319664Z Jun 15 11:23:12 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:268)
2022-06-15T11:23:12.7320450Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-15T11:23:12.7321358Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-15T11:23:12.7322097Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-15T11:23:12.7323184Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-15T11:23:12.7324238Z Jun 15 11:23:12 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1277)
2022-06-15T11:23:12.7325325Z Jun 15 11:23:12 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-06-15T11:23:12.7326392Z Jun 15 11:23:12 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-06-15T11:23:12.7327492Z Jun 15 11:23:12 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-06-15T11:23:12.7328555Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-06-15T11:23:12.7329419Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-06-15T11:23:12.7330131Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-06-15T11:23:12.7330813Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-06-15T11:23:12.7331665Z Jun 15 11:23:12 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-06-15T11:23:12.7332464Z Jun 15 11:23:12 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-06-15T11:23:12.7333399Z Jun 15 11:23:12 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-06-15T11:23:12.7334456Z Jun 15 11:23:12 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-06-15T11:23:12.7335138Z Jun 15 11:23:12 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-06-15T11:23:12.7335900Z Jun 15 11:23:12 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-15T11:23:12.7336646Z Jun 15 11:23:12 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-06-15T11:23:12.7337592Z Jun 15 11:23:12 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-06-15T11:23:12.7338280Z Jun 15 11:23:12 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-06-15T11:23:12.7339029Z Jun 15 11:23:12 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-06-15T11:23:12.7339751Z Jun 15 11:23:12 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-06-15T11:23:12.7340364Z Jun 15 11:23:12 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-06-15T11:23:12.7341044Z Jun 15 11:23:12 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-06-15T11:23:12.7341816Z Jun 15 11:23:12 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-06-15T11:23:12.7342498Z Jun 15 11:23:12 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-06-15T11:23:12.7343161Z Jun 15 11:23:12 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-06-15T11:23:12.7351204Z Jun 15 11:23:12 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-06-15T11:23:12.7351840Z Jun 15 11:23:12 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-06-15T11:23:12.7352492Z Jun 15 11:23:12 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-06-15T11:23:12.7353299Z Jun 15 11:23:12 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-06-15T11:23:12.7354181Z Jun 15 11:23:12 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-06-15T11:23:12.7354824Z Jun 15 11:23:12 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-06-15T11:23:12.7355494Z Jun 15 11:23:12 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-06-15T11:23:12.7356154Z Jun 15 11:23:12 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-06-15T11:23:12.7356854Z Jun 15 11:23:12 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-06-15T11:23:12.7357572Z Jun 15 11:23:12 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-06-15T11:23:12.7652358Z Jun 15 11:23:12 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-06-15T11:23:12.7653689Z Jun 15 11:23:12 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-06-15T11:23:12.7654429Z Jun 15 11:23:12 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-06-15T11:23:12.7655297Z Jun 15 11:23:12 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
2022-06-15T11:23:12.7656211Z Jun 15 11:23:12 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-06-15T11:23:12.7657267Z Jun 15 11:23:12 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-06-15T11:23:12.7658216Z Jun 15 11:23:12 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:300)
2022-06-15T11:23:12.7659213Z Jun 15 11:23:12 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:290)
2022-06-15T11:23:12.7660250Z Jun 15 11:23:12 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:281)
2022-06-15T11:23:12.7661132Z Jun 15 11:23:12 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-06-15T11:23:12.7661949Z Jun 15 11:23:12 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-06-15T11:23:12.7664944Z Jun 15 11:23:12 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
2022-06-15T11:23:12.7665732Z Jun 15 11:23:12 	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
2022-06-15T11:23:12.7666428Z Jun 15 11:23:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-06-15T11:23:12.7667267Z Jun 15 11:23:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-06-15T11:23:12.7668005Z Jun 15 11:23:12 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-06-15T11:23:12.7668888Z Jun 15 11:23:12 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-06-15T11:23:12.7669670Z Jun 15 11:23:12 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-06-15T11:23:12.7670625Z Jun 15 11:23:12 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-06-15T11:23:12.7671653Z Jun 15 11:23:12 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2022-06-15T11:23:12.7672457Z Jun 15 11:23:12 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-06-15T11:23:12.7673218Z Jun 15 11:23:12 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-06-15T11:23:12.7674020Z Jun 15 11:23:12 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-06-15T11:23:12.7674892Z Jun 15 11:23:12 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-06-15T11:23:12.7675606Z Jun 15 11:23:12 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-06-15T11:23:12.7676281Z Jun 15 11:23:12 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-06-15T11:23:12.7676962Z Jun 15 11:23:12 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-06-15T11:23:12.7677583Z Jun 15 11:23:12 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-15T11:23:12.7678476Z Jun 15 11:23:12 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-06-15T11:23:12.7679090Z Jun 15 11:23:12 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-06-15T11:23:12.7679608Z Jun 15 11:23:12 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-06-15T11:23:12.7680247Z Jun 15 11:23:12 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-06-15T11:23:12.7680908Z Jun 15 11:23:12 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-06-15T11:23:12.7681465Z Jun 15 11:23:12 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-06-15T11:23:12.7682212Z Jun 15 11:23:12 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-06-15T11:23:12.7682820Z Jun 15 11:23:12 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-06-15T11:23:12.7683409Z Jun 15 11:23:12 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-06-15T11:23:12.7684026Z Jun 15 11:23:12 	... 4 more
2022-06-15T11:23:12.7684532Z Jun 15 11:23:12 Caused by: java.io.IOException: java.lang.InterruptedException
2022-06-15T11:23:12.7685277Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
2022-06-15T11:23:12.7686228Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:234)
2022-06-15T11:23:12.7687304Z Jun 15 11:23:12 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:560)
2022-06-15T11:23:12.7688336Z Jun 15 11:23:12 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:547)
2022-06-15T11:23:12.7689144Z Jun 15 11:23:12 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-15T11:23:12.7689855Z Jun 15 11:23:12 	at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
2022-06-15T11:23:12.7691029Z Jun 15 11:23:12 	at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
2022-06-15T11:23:12.7691772Z Jun 15 11:23:12 	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:938)
2022-06-15T11:23:12.7692539Z Jun 15 11:23:12 	at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$1(Task.java:923)
2022-06-15T11:23:12.7693396Z Jun 15 11:23:12 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-06-15T11:23:12.7694185Z Jun 15 11:23:12 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
2022-06-15T11:23:12.7694913Z Jun 15 11:23:12 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-06-15T11:23:12.7695585Z Jun 15 11:23:12 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-06-15T11:23:12.7696143Z Jun 15 11:23:12 	at java.lang.Thread.run(Thread.java:748)
2022-06-15T11:23:12.7696687Z Jun 15 11:23:12 Caused by: java.lang.InterruptedException
2022-06-15T11:23:12.7697549Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
2022-06-15T11:23:12.7698216Z Jun 15 11:23:12 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-06-15T11:23:12.7699094Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildFutureWriteRequest$4(ChannelStateWriteRequest.java:113)
2022-06-15T11:23:12.7700184Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.cancel(ChannelStateWriteRequest.java:253)
2022-06-15T11:23:12.7701125Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.lambda$null$1(ChannelStateWriteRequestExecutorImpl.java:117)
2022-06-15T11:23:12.7701973Z Jun 15 11:23:12 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-15T11:23:12.7702830Z Jun 15 11:23:12 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-06-15T11:23:12.7703916Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.cleanupRequests(ChannelStateWriteRequestExecutorImpl.java:115)
2022-06-15T11:23:12.7704790Z Jun 15 11:23:12 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
2022-06-15T11:23:12.7705444Z Jun 15 11:23:12 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
2022-06-15T11:23:12.7706286Z Jun 15 11:23:12 	at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
2022-06-15T11:23:12.7707051Z Jun 15 11:23:12 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:80)
2022-06-15T11:23:12.7707755Z Jun 15 11:23:12 	... 1 more
2022-06-15T11:23:12.7708119Z Jun 15 11:23:12 
{code};;;","17/Jun/22 08:27;pnowojski;What is causing this `InterruptedException`? Where does it originate from?;;;","17/Jun/22 09:42;chesnay;A potential source {{ChannelStateWriteRequestExecutorImpl#close}}, which interrupts the internal thread.;;;","17/Jun/22 10:12;pnowojski;Thanks for pointing this out. So what's happening in this issue? Streaming job completes while a checkpoint is in progress. Job closing sends the interrupt, that is mishandled? CC [~fanrui];;;","18/Jun/22 08:46;fanrui;Hi [~chesnay]  [~pnowojski] . Sorry for introducing this issue, I'm late because I'm busy on Friday. 

 

After I detailed analysis, the reason for this issue is as follows:

When streaming job completed, task thread will call :

 
{code:java}
Task#restoreAndInvoke -> StreamTask#cleanUp -> SubtaskCheckpointCoordinatorImpl#close -> cancel -> 
ChannelStateWriterImpl.close -> ChannelStateWriteRequestExecutorImpl.close -> 

# this is ChannelStateWriteRequestExecutorImpl.close.
public void close() throws IOException {
    wasClosed = true;
    while (thread.isAlive()) {
        thread.interrupt();
        try {
            thread.join();
        } catch (InterruptedException e) {
            if (!thread.isAlive()) {
                Thread.currentThread().interrupt();
            }
            LOG.debug(taskName + "" interrupted while waiting for the writer thread to die"", e);
        }
    }
    if (thrown != null) {
        throw new IOException(thrown);
    }
}{code}
 

 

Detailed timeline and reason:

!image-2022-06-18-16-39-54-402.png|width=950,height=528!

 

T4 code:

!image-2022-06-18-16-42-13-669.png|width=890,height=672!

 

T5 code:

!image-2022-06-18-16-42-46-283.png|width=876,height=507!

 

Note: you can check this from [~hxbks2ks]  and [~jark] 's jstack, thanks for your information.

 

I think the root cause is that we don't have a reasonable cleanup dataFuture and we should improve it. It's the same root cause as FLINK-28077, I think once FLINK-28077 is done, this issue will be resolved as well.;;;","20/Jun/22 10:21;pnowojski;Thanks for the analysis [~fanrui]. It makes sense to me. Let's close this one FLINK-28077 will be merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,
KerberosDelegationTokenManagerTest static UGI mock leaks into other tests,FLINK-27791,13446917,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,hxbks2ks,hxbks2ks,26/May/22 02:22,01/Sep/22 11:18,04/Jun/24 20:51,09/Jun/22 09:24,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / Coordination,Tests,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-05-25T12:16:09.2562348Z May 25 12:16:09 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T12:16:09.2563741Z May 25 12:16:09 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-25T12:16:09.2565457Z May 25 12:16:09 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:982)
2022-05-25T12:16:09.2567245Z May 25 12:16:09 	at org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.submitJobGraphAndWait(SlotCountExceedingParallelismTest.java:101)
2022-05-25T12:16:09.2569329Z May 25 12:16:09 	at org.apache.flink.runtime.jobmanager.SlotCountExceedingParallelismTest.testNoSlotSharingAndBlockingResultBoth(SlotCountExceedingParallelismTest.java:94)
2022-05-25T12:16:09.2571889Z May 25 12:16:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-25T12:16:09.2573109Z May 25 12:16:09 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-25T12:16:09.2574528Z May 25 12:16:09 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-25T12:16:09.2575657Z May 25 12:16:09 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-25T12:16:09.2581380Z May 25 12:16:09 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-05-25T12:16:09.2582747Z May 25 12:16:09 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-05-25T12:16:09.2583600Z May 25 12:16:09 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-05-25T12:16:09.2584455Z May 25 12:16:09 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-05-25T12:16:09.2585172Z May 25 12:16:09 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-05-25T12:16:09.2585792Z May 25 12:16:09 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-05-25T12:16:09.2586376Z May 25 12:16:09 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-25T12:16:09.2587035Z May 25 12:16:09 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-05-25T12:16:09.2587682Z May 25 12:16:09 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-05-25T12:16:09.2588589Z May 25 12:16:09 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-05-25T12:16:09.2589623Z May 25 12:16:09 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-05-25T12:16:09.2590262Z May 25 12:16:09 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-05-25T12:16:09.2590856Z May 25 12:16:09 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-05-25T12:16:09.2591453Z May 25 12:16:09 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-05-25T12:16:09.2592063Z May 25 12:16:09 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-05-25T12:16:09.2592673Z May 25 12:16:09 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-05-25T12:16:09.2593288Z May 25 12:16:09 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-05-25T12:16:09.2595864Z May 25 12:16:09 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-25T12:16:09.2596521Z May 25 12:16:09 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-25T12:16:09.2597144Z May 25 12:16:09 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-25T12:16:09.2597703Z May 25 12:16:09 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-25T12:16:09.2598247Z May 25 12:16:09 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-25T12:16:09.2599004Z May 25 12:16:09 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-05-25T12:16:09.2599696Z May 25 12:16:09 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-05-25T12:16:09.2600397Z May 25 12:16:09 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-05-25T12:16:09.2601282Z May 25 12:16:09 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-05-25T12:16:09.2602529Z May 25 12:16:09 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-05-25T12:16:09.2603747Z May 25 12:16:09 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-05-25T12:16:09.2604776Z May 25 12:16:09 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-05-25T12:16:09.2605611Z May 25 12:16:09 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-05-25T12:16:09.2606370Z May 25 12:16:09 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-05-25T12:16:09.2607054Z May 25 12:16:09 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-05-25T12:16:09.2607796Z May 25 12:16:09 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-05-25T12:16:09.2608922Z May 25 12:16:09 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-05-25T12:16:09.2609723Z May 25 12:16:09 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-05-25T12:16:09.2610414Z May 25 12:16:09 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-05-25T12:16:09.2611062Z May 25 12:16:09 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-05-25T12:16:09.2611845Z May 25 12:16:09 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-05-25T12:16:09.2612633Z May 25 12:16:09 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-05-25T12:16:09.2613364Z May 25 12:16:09 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-05-25T12:16:09.2614471Z May 25 12:16:09 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-05-25T12:16:09.2615277Z May 25 12:16:09 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-05-25T12:16:09.2615920Z May 25 12:16:09 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-05-25T12:16:09.2616568Z May 25 12:16:09 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-25T12:16:09.2617347Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-25T12:16:09.2618418Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-25T12:16:09.2619771Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:300)
2022-05-25T12:16:09.2620681Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:290)
2022-05-25T12:16:09.2621485Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:281)
2022-05-25T12:16:09.2622270Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:716)
2022-05-25T12:16:09.2623125Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-05-25T12:16:09.2624085Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1546)
2022-05-25T12:16:09.2625042Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1113)
2022-05-25T12:16:09.2625741Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1053)
2022-05-25T12:16:09.2626437Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:892)
2022-05-25T12:16:09.2627140Z May 25 12:16:09 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:463)
2022-05-25T12:16:09.2627931Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42)
2022-05-25T12:16:09.2628817Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:617)
2022-05-25T12:16:09.2629669Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:493)
2022-05-25T12:16:09.2630461Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-05-25T12:16:09.2631135Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-05-25T12:16:09.2631823Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T12:16:09.2632521Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-05-25T12:16:09.2633238Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.SharedSlot.cancelLogicalSlotRequest(SharedSlot.java:222)
2022-05-25T12:16:09.2634066Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.SlotSharingExecutionSlotAllocator.cancelLogicalSlotRequest(SlotSharingExecutionSlotAllocator.java:164)
2022-05-25T12:16:09.2635023Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.SharingPhysicalSlotRequestBulk.cancel(SharingPhysicalSlotRequestBulk.java:86)
2022-05-25T12:16:09.2635897Z May 25 12:16:09 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkWithTimestamp.cancel(PhysicalSlotRequestBulkWithTimestamp.java:66)
2022-05-25T12:16:09.2637677Z May 25 12:16:09 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:91)
2022-05-25T12:16:09.2639068Z May 25 12:16:09 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-05-25T12:16:09.2639679Z May 25 12:16:09 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-05-25T12:16:09.2640340Z May 25 12:16:09 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:443)
2022-05-25T12:16:09.2641124Z May 25 12:16:09 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-25T12:16:09.2641888Z May 25 12:16:09 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:443)
2022-05-25T12:16:09.2642593Z May 25 12:16:09 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:213)
2022-05-25T12:16:09.2643320Z May 25 12:16:09 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-25T12:16:09.2644048Z May 25 12:16:09 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-25T12:16:09.2644951Z May 25 12:16:09 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-25T12:16:09.2645549Z May 25 12:16:09 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-25T12:16:09.2646138Z May 25 12:16:09 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-25T12:16:09.2646729Z May 25 12:16:09 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-25T12:16:09.2647315Z May 25 12:16:09 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-25T12:16:09.2647930Z May 25 12:16:09 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-25T12:16:09.2648543Z May 25 12:16:09 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-25T12:16:09.2649259Z May 25 12:16:09 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-25T12:16:09.2649838Z May 25 12:16:09 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-25T12:16:09.2650370Z May 25 12:16:09 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-25T12:16:09.2650923Z May 25 12:16:09 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-25T12:16:09.2651505Z May 25 12:16:09 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-25T12:16:09.2652066Z May 25 12:16:09 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-25T12:16:09.2652617Z May 25 12:16:09 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-25T12:16:09.2653167Z May 25 12:16:09 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-25T12:16:09.2653680Z May 25 12:16:09 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-25T12:16:09.2654223Z May 25 12:16:09 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-25T12:16:09.2655018Z May 25 12:16:09 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-25T12:16:09.2655657Z May 25 12:16:09 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-25T12:16:09.2656290Z May 25 12:16:09 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-25T12:16:09.2657251Z May 25 12:16:09 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
2022-05-25T12:16:09.2658276Z May 25 12:16:09 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:551)
2022-05-25T12:16:09.2658918Z May 25 12:16:09 	... 37 more
2022-05-25T12:16:09.2659648Z May 25 12:16:09 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
2022-05-25T12:16:09.2660734Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-05-25T12:16:09.2661412Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-05-25T12:16:09.2662091Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-05-25T12:16:09.2662756Z May 25 12:16:09 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-25T12:16:09.2663267Z May 25 12:16:09 	... 35 more
2022-05-25T12:16:09.2663902Z May 25 12:16:09 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
2022-05-25T12:16:09.2665137Z May 25 12:16:09 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)
2022-05-25T12:16:09.2665881Z May 25 12:16:09 	... 28 more
2022-05-25T12:16:09.2666328Z May 25 12:16:09 Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 ms
2022-05-25T12:16:09.2666794Z May 25 12:16:09 	... 29 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36053&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27745,,,,,,,,,,FLINK-28379,,,,,,,,,,,,"02/Jun/22 03:47;zhuzh;error.log;https://issues.apache.org/jira/secure/attachment/13044540/error.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 09 09:24:26 UTC 2022,,,,,,,,,,"0|z12p5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 08:53;godfreyhe;[~zhuzh] could you have a look ?;;;","31/May/22 09:01;zhuzh;I will take a look.;;;","02/Jun/22 04:05;zhuzh;The slot allocation timeout happened because the resource manager failed to get started. And the root cause happens in {{org.apache.hadoop.security.UserGroupInformation.getCurrentUser}} (see the attached error.log).

I also tried to run the test thousands of times locally and did not re-produce the problem. So I suspect this is due to a temporary environment problem. I will close this ticket for now.;;;","02/Jun/22 12:07;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36270&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","06/Jun/22 11:27;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36302&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef;;;","06/Jun/22 11:59;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36316&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef;;;","06/Jun/22 12:00;hxbks2ks;cc [~zhuzh], do we need to reopen this issue?;;;","07/Jun/22 03:14;zhuzh;[~wangyang0918] do you have any idea of this problem?

Flink is trying to get the login user via \{{org.apache.hadoop.security.UserGroupInformation.getCurrentUser}} but it failed due to \{{IllegalArgumentException}}

.;;;","08/Jun/22 12:40;chesnay;I can't figure this out. Looking at the [JDK|https://github.com/JetBrains/jdk8u_jdk/blob/master/src/share/classes/javax/security/auth/login/AppConfigurationEntry.java] an IAE is thrown if
a) the options are null
b) the login name is null/empty

However, looking at the [Hadoop source|https://github.com/apache/hadoop/blob/branch-2.8.5/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L505], we know that
a) can't happen because BASIC_JAAS_OPTIONS is never null
b) can't happen because the login name is provided by [getOSLoginModuleName|https://github.com/apache/hadoop/blob/0b8464d75227fcee2c6e7f2410377b3d53d3d5f8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L396] and cannot be null or empty.;;;","08/Jun/22 12:41;chesnay;That said we do have _some_ tests that interact with the UserGroupInformation class and also some that set up mocks. Maybe those are breaking things...;;;","08/Jun/22 12:45;chesnay;Nice I can reproduce it locally with {{KerberosDelegationTokenManagerTest#testStartTGTRenewalShouldScheduleRenewal}}; calling {{UserGroupInformation.getLoginUser()}} after the test fails consistently.;;;","08/Jun/22 14:50;zhuzh;Good catch!;;;","09/Jun/22 09:24;chesnay;master: 94117859d5d916b120d0458f24d4f73dfb4f0524;;;",,,,,,,,,,,,,,,,,,,
Port ADD JAR /SHOW JARS syntax implementation from SqlClient to TableEnvironment side,FLINK-27790,13446912,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiangang,lsy,lsy,26/May/22 01:58,17/Aug/22 08:00,04/Jun/24 20:51,08/Aug/22 02:29,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29009,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 02:29:23 UTC 2022,,,,,,,,,,"0|z12p4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 02:09;Jiangang;[~lsy] I'd like to take this ticket. Thanks.;;;","08/Aug/22 02:29;jark;Fixed in master: c5c66af680eaf3aae6fbcb6dc48e8d6b66ade8c9 to fe9c726ac1ac09c719b2769e7b737f46aba2bc30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacySource compatible with overdraft buffer,FLINK-27789,13446911,13443562,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,26/May/22 01:58,29/Jun/22 14:36,04/Jun/24 20:51,29/Jun/22 14:04,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Common,Runtime / Checkpointing,Runtime / Network,,,0,pull-request-available,,,,"Since LegacySource does not have checkAvailable, LegacySource will use all overdraft buffers by default, this is not what we expected.

So we'll set overdraft=0 for the SourceStreamTask.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 29 14:04:11 UTC 2022,,,,,,,,,,"0|z12p48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 14:04;pnowojski;merged commit a63b7dd into apache:master now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding annotation to k8 operator Pod,FLINK-27788,13446905,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zeus1ammon,zeus1ammon,zeus1ammon,25/May/22 23:53,25/Jun/22 05:52,04/Jun/24 20:51,21/Jun/22 15:25,kubernetes-operator-0.1.0,kubernetes-operator-1.1.0,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Currently we lack the option to natively add annotations on flink operator pods. Providing this feature directly on our existing helm chart, could be useful. One potential use-case for allowing annotations on Pod is to enable scrapping of opertor metrics by monitoring Infrastructure like Prometheus , Datadog etc.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 21 16:36:46 UTC 2022,,,,,,,,,,"0|z12p2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 15:25;mbalassi;91753ec in main;;;","21/Jun/22 16:36;zeus1ammon;[~mbalassi] do we see any issue in merging this in to 1.0 release ? We internally using the stable 1.0 release, and it will help us to have this change in 1.0 branch as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New tasks think they've been restored from savepoint (even when they weren't present in that savepoint),FLINK-27787,13446903,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stephenpatel,stephenpatel,25/May/22 23:03,29/Nov/22 04:03,04/Jun/24 20:51,,1.14.0,1.14.2,1.14.3,1.14.4,1.15.0,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,,,,,"I think I've found a bug with new task restoration from savepoints.

I have some beam-on-flink pipelines that I restore from savepoints with new source types (sources not present in the dag that generated the savepoint).

On flink 1.10.1 (beam 2.29) this works fine, the dag spins up with the new sources and starts emitting data.
On flink 1.14.4 (beam 2.38) this no longer works.   The dag spins up, but the sources never emit anything.

I checked the beam source wrapper (it maps the beam source to the underlying runner source: https://github.com/apache/beam/blob/v2.38.0/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedSourceWrapper.java#L427), but it hasn't changed in several years.

By inserting some logging statements, I was able to determine that on flink 1.10, the source is told that it is NOT restored (FunctionInitializationContext.isRestored() returns false).  With flink 1.14, it is told that it IS restored.

By traversing the flink code changes, I think I've determined that the changes introduced for FLINK-23854 causes the logic in org.apache.flink.runtime.checkpoint.StateAssignmentOperation.java to not behave the way it used to.

In 1.13.6, we see [here|https://github.com/apache/flink/blob/release-1.13.6/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L221-L240] that if a subtask does not have state, it will not have a JobManagerTaskRestore instance set on it, so the FunctionInitializationContext.isRestored() method will return false.
In 1.14.0 (and all released versions after that), we see [here|https://github.com/apache/flink/blob/release-1.14.0/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/StateAssignmentOperation.java#L236-L256] that the chunk of logic that used to be present for dealing with subtasks which have no state is no longer present.  Thus, when restoring from a savepoint, the task will think it's been restored, even when it didn't exist (and thus couldn't have state) in the savepoint.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23854,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 29 04:02:53 UTC 2022,,,,,,,,,,"0|z12p2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/22 04:02;Yanfei Lei;[~arvid] could you please take a look?  I found that [PR17019|https://github.com/apache/flink/pull/17019] has discussed [related possibilities|#r697712620]];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-hive should not depend on `flink-table-planner`,FLINK-27786,13446854,13446846,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,alexanderpreuss,alexanderpreuss,25/May/22 15:00,01/Jun/22 12:45,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 12:45:30 UTC 2022,,,,,,,,,,"0|z12ork:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 12:45;alexanderpreuss;Following is a list of classes that prevent removing the `flink-table-planner` test dependency in favor or `flink-table-test-utils` right now:
 * org.apache.flink.table.planner.runtime.utils.BatchAbstractTestBase
 * org.apache.flink.table.planner.utils.TableTestUtil
 * org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory
 * org.apache.flink.table.planner.runtime.utils.BatchTestBase
 * org.apache.flink.table.planner.runtime.utils.TestingRetractSink
 * org.apache.flink.table.planner.utils.TableTestBase
 * org.apache.flink.table.planner.factories.TestValuesTableFactory
 * org.apache.flink.table.planner.runtime.stream.sql.CompactionITCaseBase
 *  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-hbase should not depend on `flink-table-planner`,FLINK-27785,13446853,13446846,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,alexanderpreuss,alexanderpreuss,25/May/22 15:00,01/Jun/22 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Connectors / HBase,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 12:44:58 UTC 2022,,,,,,,,,,"0|z12orc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 12:44;alexanderpreuss;Following is a list of classes that prevent removing the `flink-table-planner` test dependency in favor or `flink-table-test-utils` right now:


 * org.apache.flink.table.planner.utils.StreamTableTestUtil
 * org.apache.flink.table.planner.utils.TableTestBase

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-jdbc should not depend on `flink-table-planner`,FLINK-27784,13446852,13446846,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,alexanderpreuss,alexanderpreuss,25/May/22 14:59,01/Jun/22 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 12:44:35 UTC 2022,,,,,,,,,,"0|z12or4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/22 12:44;alexanderpreuss;Following is a list of classes that prevent removing the `flink-table-planner` test dependency in favor or `flink-table-test-utils` right now:


 * org.apache.flink.table.planner.factories.TestValuesTableFactory
 * org.apache.flink.table.planner.runtime.utils.TestData
 * org.apache.flink.table.planner.utils.StreamTableTestUtil
 * org.apache.flink.table.planner.utils.TableTestBase
 * org.apache.flink.table.planner.runtime.utils.StreamTestSink;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-aws-kinesis should not depend on `flink-table-planner`,FLINK-27783,13446850,13446846,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,25/May/22 14:58,30/May/22 14:01,04/Jun/24 20:51,30/May/22 14:01,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Kinesis,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 14:01:04 UTC 2022,,,,,,,,,,"0|z12oqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:01;martijnvisser;Fixed in master: 308152883899a4c0fd56fe70a97547c8f735ae26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-kinesis should not depend on `flink-table-planner`,FLINK-27782,13446849,13446846,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,25/May/22 14:58,30/May/22 14:00,04/Jun/24 20:51,30/May/22 14:00,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Kinesis,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 14:00:32 UTC 2022,,,,,,,,,,"0|z12oqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 14:00;martijnvisser;Fixed in master: c924b29ee6a75766f0fc477f17e1a4b3ac606f3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-kafka should not depend on `flink-table-planner`,FLINK-27781,13446848,13446846,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,25/May/22 14:57,30/May/22 13:59,04/Jun/24 20:51,30/May/22 13:59,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Kafka,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 13:59:39 UTC 2022,,,,,,,,,,"0|z12oq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 13:59;martijnvisser;Fixed in master: a65941d289e8a19ecaf6935bdba191e6869c0295;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector-cassandra should not depend on `flink-table-planner`,FLINK-27780,13446847,13446846,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,25/May/22 14:57,30/May/22 13:59,04/Jun/24 20:51,30/May/22 13:59,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Cassandra,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 13:59:11 UTC 2022,,,,,,,,,,"0|z12oq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 13:59;martijnvisser;Fixed in master: ac90927aa1d359af4db60a1d89decfb4ef1fc69a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connectors should not depend on `flink-table-planner`,FLINK-27779,13446846,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,alexanderpreuss,alexanderpreuss,25/May/22 14:52,19/Aug/23 10:35,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,auto-deprioritized-minor,Connector,pull-request-available,,"Connector modules currently rely heavily on `flink-table-planner` as a test dependency for testing the ITCases with 'DynamicTableX' using the TableFactory to load the respective connector. 

There is now a better way that only requires to have `flink-table-test-utils` as a test dependency. Therefore all connectors should be migrated to using the new way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:05 UTC 2023,,,,,,,,,,"0|z12ops:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table-planner should not use CodeSplitUtils#newName,FLINK-27778,13446832,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,25/May/22 13:28,13/Jun/22 08:13,04/Jun/24 20:51,13/Jun/22 08:13,,,,,,,,,,,,,,,1.16.0,,,,Build System,Table SQL / Planner,,,,0,pull-request-available,,,,"The {{table-planner}} has a direct dependency on the {{table-code-splitter}}, as several CastRules use {{CodeSplitUtil.newName}}.

This dependency is a bit hidden. In the IDE it is pulled in transitively via {{table-runtime}}, and in maven it uses the {{table-code-splitter}} dependency bundled by {{table-runtime}}.

It would be nice if we could add a {{provided}} dependency to the {{table-code-splitter}} to properly document this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 08:13:15 UTC 2022,,,,,,,,,,"0|z12omo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/22 09:29;chesnay;Actually, it is not clear to me why the table-runtime bundles table-code-splitter in the first place.;;;","27/May/22 09:42;chesnay;Reached out to [~tiwalter]; the planner is using the wrong method; it should use {{CodeGenUtils#newName}} instead.;;;","13/Jun/22 08:13;chesnay;master: 57bd1ddb5e17893d00e15034cad1f15c758b6318;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not get the parquet.compression when using native parquet&orc writer to sink hive,FLINK-27777,13446822,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Small Wong,Small Wong,25/May/22 12:56,30/May/22 14:53,04/Jun/24 20:51,,1.13.6,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"flink.version: 1.13

After set {*}table.exec.hive.fallback-mapred-writer=false{*}, then sink hive by {*}native parquet&orc writer{*}, but can not get the *`parquet.compression`* by  `{*}formatConf{*}` in class `{*}HiveTableSink{*}`.

There is no field `{*}parquet.compression{*}` in `jobConf` or `{*}sd.getSerdeInfo().getParameters(){*}`. And `parquet.compression` just  exists in `{*}hive table properties{*}` as follows. 

 
{code:java}
// code placeholder

CREATE TABLE `hive_table`(
  `user_id` int,
  `order_amount` double)
PARTITIONED BY (
  `dt` string,
  `hr` string)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  'hdfs://xxxx'
TBLPROPERTIES (
  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',
  'sink.partition-commit.delay'='1 h',
  'sink.partition-commit.policy.kind'='metastore,success-file',
  'sink.partition-commit.trigger'='partition-time',
  'transient_lastDdlTime'='1614740641',
  'parquet.compression'='snappy') {code}
 

!image-2022-05-25-20-53-20-412.png!

!image-2022-05-25-20-57-13-241.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/22 12:53;Small Wong;image-2022-05-25-20-53-20-412.png;https://issues.apache.org/jira/secure/attachment/13044193/image-2022-05-25-20-53-20-412.png","25/May/22 12:57;Small Wong;image-2022-05-25-20-57-13-241.png;https://issues.apache.org/jira/secure/attachment/13044194/image-2022-05-25-20-57-13-241.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 13:04:14 UTC 2022,,,,,,,,,,"0|z12okg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 13:04;Small Wong;[~ruili] hi, could you help me to check?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception when UDAF used in sliding window does not implement merge method in PyFlink,FLINK-27776,13446790,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,25/May/22 12:09,17/Jun/22 13:13,04/Jun/24 20:51,26/May/22 08:38,1.13.6,1.14.4,1.15.0,,,,,,,,,,,,1.14.5,1.15.1,1.16.0,,API / Python,,,,,0,pull-request-available,,,,"We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:

{code:python}
class SumAggregateFunction(AggregateFunction):

    def get_value(self, accumulator):
        return accumulator[0]

    def create_accumulator(self):
        return [0]

    def accumulate(self, accumulator, *args):
        accumulator[0] = accumulator[0] + args[0]

    def retract(self, accumulator, *args):
        accumulator[0] = accumulator[0] - args[0]

    def merge(self, accumulator, accumulators):
        for other_acc in accumulators:
            accumulator[0] = accumulator[0] + other_acc[0]

    def get_accumulator_type(self):
        return DataTypes.ARRAY(DataTypes.BIGINT())

    def get_result_type(self):
        return DataTypes.BIGINT()
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 26 08:38:15 UTC 2022,,,,,,,,,,"0|z12odc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/22 08:38;dianfu;Merged to:
- master via 51eb02e92287eff3493d4c999390d1d525f3f9d7
- release-1.15 via 9708f5767cd3983527efc26d7752a8e3cda5465d
- release-1.14 via 07fc8c40f18ed86b6adfe420bcef83da5fd022e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducer VS KafkaSink,FLINK-27775,13446786,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,Jiangfei Liu,Jiangfei Liu,25/May/22 11:52,19/Aug/23 10:35,04/Jun/24 20:51,,1.14.3,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,features,,"sorry，my english is bad

in Flink1.14.3，write 10000 data to kafka

when use FlinkKafkaProducer，comleted  7s

when use KafkaSink，comleted 1m40s

why KafkaSink is low speed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/22 11:52;Jiangfei Liu;Snipaste_2022-05-25_19-52-11.png;https://issues.apache.org/jira/secure/attachment/13044192/Snipaste_2022-05-25_19-52-11.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 10:35:05 UTC 2023,,,,,,,,,,"0|z12ocg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","03/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window Agg doesn't work when used in Batch Table API,FLINK-27774,13446757,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,hxbks2ks,hxbks2ks,25/May/22 09:44,02/Nov/23 09:35,04/Jun/24 20:51,,1.14.4,1.15.0,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"In new type system, the type of WindowReference is an instance of Types.LocalDateTime.
 !image-2022-05-25-17-44-05-690.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/22 09:44;hxbks2ks;image-2022-05-25-17-44-05-690.png;https://issues.apache.org/jira/secure/attachment/13044184/image-2022-05-25-17-44-05-690.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 09:35:00 UTC 2023,,,,,,,,,,"0|z12o60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/22 18:19;shubham.bansal;I will take a look at this and return back with the RCA of the issue.
 
 
 ;;;","17/Jun/22 03:50;shubham.bansal;[~hxbks2ks] 
I looked at it and from the documentation [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/concepts/time_attributes/] I see that

""During the conversion, Flink always derives rowtime attribute as TIMESTAMP WITHOUT TIME ZONE, because DataStream doesn’t have time zone notion, and treats all event time values as in UTC.""

I don't see it explicitly defined as LocalDateTime. Can you point me to the documentation? I can make the change if that's the case.
 
 
 ;;;","02/Nov/23 09:35;twalthr;Table API uses the deprecated windows right now. We should update it to use Window TVFs instead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the E2E tests for SQL Gateway ,FLINK-27773,13446722,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,fsk119,fsk119,25/May/22 07:29,25/Oct/22 01:43,04/Jun/24 20:51,26/Aug/22 03:36,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28914,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 25 01:42:29 UTC 2022,,,,,,,,,,"0|z12ny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 03:35;fsk119;Merged into master:

0fd718c03d891e5e235c22eb591f289d979e9f30

221fb98daa80d69aff8f366f8c867bc0ced1b8e6

ad68495bb64144fd6f1b538cc3b8b814599ed3da

b1db372c1e9292425dd7a7cf73c43da432d1150d;;;","24/Oct/22 07:23;martijnvisser;[~fsk119] What is the fixVersion for this?;;;","25/Oct/22 01:42;fsk119;[~martijnvisser] It has been implemented in the 1.16.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose Operation-level logs in the SqlGatewayService,FLINK-27772,13446720,13478114,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,25/May/22 07:28,23/Aug/22 03:33,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-25 07:28:33.0,,,,,,,,,,"0|z12nxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the listAPIs in the SqlGatewayService,FLINK-27771,13446719,13277370,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,,fsk119,fsk119,25/May/22 07:28,23/Aug/22 03:26,04/Jun/24 20:51,23/Aug/22 03:26,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-25 07:28:17.0,,,,,,,,,,"0|z12nxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the script to start/stop/stop-all gateway,FLINK-27770,13446718,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ana4,fsk119,fsk119,25/May/22 07:27,03/Aug/22 11:37,04/Jun/24 20:51,03/Aug/22 07:09,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28154,,,,,,,,,FLINK-28791,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 07:09:49 UTC 2022,,,,,,,,,,"0|z12nxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 07:29;ana4;I would like to take this.;;;","27/Jun/22 11:42;fsk119;[~ana4] I have assigned it to you.;;;","03/Aug/22 07:09;fsk119;Merged into master: 
c90d99fe2dd2a78214c1f16f6052ee17e30afd84
31bb1f49bf4ccc657ad6879bdf5d954cc24062d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the REST endpoint framework,FLINK-27769,13446717,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,fsk119,fsk119,25/May/22 07:27,02/Aug/22 05:50,04/Jun/24 20:51,02/Aug/22 05:50,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,Introduce the REST Endpoint Factory and its implementation. Make sure the SQL Gateway is avaliable to load the REST endpoint with config.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 05:50:53 UTC 2022,,,,,,,,,,"0|z12nx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 05:50;xtsong;master (1.16): 70e8d7e251c9e74910a61c02028c498fba6a35ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to execute sql for the SqlGatewayService,FLINK-27768,13446716,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,25/May/22 07:27,13/Jul/22 11:46,04/Jun/24 20:51,13/Jul/22 11:46,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 11:46:26 UTC 2022,,,,,,,,,,"0|z12nww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 11:46;fsk119;Merged in the master: 7e494c91d1e340a1f9438785e2a56242fb5894c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce Endpoint API and utils,FLINK-27767,13446715,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,25/May/22 07:26,07/Jul/22 01:57,04/Jun/24 20:51,07/Jul/22 01:57,,,,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 07 01:57:24 UTC 2022,,,,,,,,,,"0|z12nwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/22 01:57;fsk119;Closed in master: eb395309e046418128363a5401693be3987a3bca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the framework of the SqlGatewayService,FLINK-27766,13446713,13277370,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,25/May/22 07:26,23/Jun/22 01:41,04/Jun/24 20:51,23/Jun/22 01:41,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Gateway,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 23 01:40:58 UTC 2022,,,,,,,,,,"0|z12nw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jun/22 01:40;fsk119;master:da08267c2cb29d98e626a867f8e07cb5b8e7f29a
master:626bdacdbe303d9e9c346e7ba7f3d8c7b42b1d8e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerProcessFailure failed with FlinkJobNotFoundException,FLINK-27765,13446708,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,hxbks2ks,hxbks2ks,25/May/22 07:09,29/Aug/22 10:38,04/Jun/24 20:51,29/Aug/22 10:38,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,stale-assigned,test-stability,,,"
{code:java}
2022-05-25T02:05:27.1873752Z May 25 02:05:27 [ERROR] Failures: 
2022-05-25T02:05:27.1874854Z May 25 02:05:27 [ERROR] TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerProcessFailure
2022-05-25T02:05:27.1875640Z May 25 02:05:27 [INFO]   Run 1: PASS
2022-05-25T02:05:27.1876978Z May 25 02:05:27 [ERROR]   Run 2: The program encountered a RestClientException : [org.apache.flink.runtime.rest.handler.RestHandlerException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (9e72abfc6f3a9fd82424bf039ea18795)
2022-05-25T02:05:27.1878652Z May 25 02:05:27 	at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.propagateException(JobExecutionResultHandler.java:94)
2022-05-25T02:05:27.1880072Z May 25 02:05:27 	at org.apache.flink.runtime.rest.handler.job.JobExecutionResultHandler.lambda$handleRequest$1(JobExecutionResultHandler.java:84)
2022-05-25T02:05:27.1881330Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
2022-05-25T02:05:27.1882473Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
2022-05-25T02:05:27.1883303Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T02:05:27.1884199Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-05-25T02:05:27.1885040Z May 25 02:05:27 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:261)
2022-05-25T02:05:27.1886100Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-25T02:05:27.1887037Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-25T02:05:27.1887797Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T02:05:27.1888542Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-05-25T02:05:27.1889390Z May 25 02:05:27 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1275)
2022-05-25T02:05:27.1890158Z May 25 02:05:27 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-25T02:05:27.1891040Z May 25 02:05:27 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-25T02:05:27.1892037Z May 25 02:05:27 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-25T02:05:27.1892931Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-25T02:05:27.1893706Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-25T02:05:27.1894530Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-25T02:05:27.1895269Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-05-25T02:05:27.1896413Z May 25 02:05:27 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)
2022-05-25T02:05:27.1897415Z May 25 02:05:27 	at akka.dispatch.OnComplete.internal(Future.scala:299)
2022-05-25T02:05:27.1898030Z May 25 02:05:27 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-25T02:05:27.1898639Z May 25 02:05:27 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-25T02:05:27.1899254Z May 25 02:05:27 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-25T02:05:27.1899852Z May 25 02:05:27 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-25T02:05:27.1900699Z May 25 02:05:27 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-25T02:05:27.1901847Z May 25 02:05:27 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-25T02:05:27.1902979Z May 25 02:05:27 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-25T02:05:27.1904342Z May 25 02:05:27 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-25T02:05:27.1905213Z May 25 02:05:27 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-25T02:05:27.1905882Z May 25 02:05:27 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-25T02:05:27.1906592Z May 25 02:05:27 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25)
2022-05-25T02:05:27.1907622Z May 25 02:05:27 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-25T02:05:27.1908354Z May 25 02:05:27 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-25T02:05:27.1908991Z May 25 02:05:27 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-25T02:05:27.1909744Z May 25 02:05:27 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-25T02:05:27.1910395Z May 25 02:05:27 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-25T02:05:27.1911088Z May 25 02:05:27 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-25T02:05:27.1911983Z May 25 02:05:27 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-25T02:05:27.1912723Z May 25 02:05:27 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-25T02:05:27.1913408Z May 25 02:05:27 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-25T02:05:27.1914190Z May 25 02:05:27 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-25T02:05:27.1914866Z May 25 02:05:27 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-25T02:05:27.1915713Z May 25 02:05:27 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-25T02:05:27.1916473Z May 25 02:05:27 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-25T02:05:27.1917558Z May 25 02:05:27 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-25T02:05:27.1918616Z May 25 02:05:27 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-25T02:05:27.1919638Z May 25 02:05:27 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-25T02:05:27.1921063Z May 25 02:05:27 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (9e72abfc6f3a9fd82424bf039ea18795)
2022-05-25T02:05:27.1922069Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-05-25T02:05:27.1922825Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-05-25T02:05:27.1923560Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:957)
2022-05-25T02:05:27.1924339Z May 25 02:05:27 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
2022-05-25T02:05:27.1924891Z May 25 02:05:27 	... 44 more
2022-05-25T02:05:27.1925491Z May 25 02:05:27 Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (9e72abfc6f3a9fd82424bf039ea18795)
2022-05-25T02:05:27.1926308Z May 25 02:05:27 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$requestJobStatus$17(Dispatcher.java:816)
2022-05-25T02:05:27.1927084Z May 25 02:05:27 	at java.util.Optional.orElseGet(Optional.java:267)
2022-05-25T02:05:27.1927734Z May 25 02:05:27 	at org.apache.flink.runtime.dispatcher.Dispatcher.requestJobStatus(Dispatcher.java:810)
2022-05-25T02:05:27.1928377Z May 25 02:05:27 	at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
2022-05-25T02:05:27.1929043Z May 25 02:05:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-25T02:05:27.1929705Z May 25 02:05:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-25T02:05:27.1930390Z May 25 02:05:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
2022-05-25T02:05:27.1931247Z May 25 02:05:27 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-25T02:05:27.1932077Z May 25 02:05:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
2022-05-25T02:05:27.1932847Z May 25 02:05:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-25T02:05:27.1933614Z May 25 02:05:27 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-25T02:05:27.1934447Z May 25 02:05:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-25T02:05:27.1935139Z May 25 02:05:27 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-25T02:05:27.1935764Z May 25 02:05:27 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-25T02:05:27.1936589Z May 25 02:05:27 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-25T02:05:27.1937475Z May 25 02:05:27 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-25T02:05:27.1938099Z May 25 02:05:27 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-25T02:05:27.1938758Z May 25 02:05:27 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-25T02:05:27.1939410Z May 25 02:05:27 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-25T02:05:27.1940058Z May 25 02:05:27 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-25T02:05:27.1940806Z May 25 02:05:27 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-25T02:05:27.1941370Z May 25 02:05:27 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-25T02:05:27.1941954Z May 25 02:05:27 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-25T02:05:27.1942580Z May 25 02:05:27 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-25T02:05:27.1943171Z May 25 02:05:27 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-25T02:05:27.1943758Z May 25 02:05:27 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-25T02:05:27.1944392Z May 25 02:05:27 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-25T02:05:27.1944936Z May 25 02:05:27 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-25T02:05:27.1945368Z May 25 02:05:27 	... 4 more
2022-05-25T02:05:27.1945702Z May 25 02:05:27 ]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36024&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12661",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 05 10:39:14 UTC 2022,,,,,,,,,,"0|z12nv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SetOperatorsITCase tests failed,FLINK-27764,13446705,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,hxbks2ks,hxbks2ks,25/May/22 06:58,19/Aug/23 22:34,04/Jun/24 20:51,,1.14.4,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,test-stability,,"
{code:java}
2022-05-25T04:41:37.6067577Z May 25 04:41:37 [ERROR] Failures: 
2022-05-25T04:41:37.6070682Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testExcept Multiple Failures (2 failures)
2022-05-25T04:41:37.6072001Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6072612Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6073175Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testExcept Multiple Failures (2 failures)
2022-05-25T04:41:37.6073824Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6074391Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6074966Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testIntersectAll Multiple Failures (2 failures)
2022-05-25T04:41:37.6075935Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6076662Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6077254Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testIntersectAll Multiple Failures (2 failures)
2022-05-25T04:41:37.6077909Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6078541Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6079332Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testIntersect Multiple Failures (2 failures)
2022-05-25T04:41:37.6079983Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6080690Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6081261Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testIntersect Multiple Failures (2 failures)
2022-05-25T04:41:37.6081910Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6082492Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6083399Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testMinusAll Multiple Failures (2 failures)
2022-05-25T04:41:37.6084048Z May 25 04:41:37 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-25T04:41:37.6084632Z May 25 04:41:37 	java.lang.AssertionError: <no message>
2022-05-25T04:41:37.6085172Z May 25 04:41:37 [ERROR] Errors: 
2022-05-25T04:41:37.6086447Z May 25 04:41:37 [ERROR]   SetOperatorsITCase.testMinusAll » JobExecution Job execution failed.
2022-05-25T04:41:37.6087018Z May 25 04:41:37 [INFO] 
2022-05-25T04:41:37.6087395Z May 25 04:41:37 [ERROR] Tests run: 3503, Failures: 7, Errors: 1, Skipped: 26
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36025&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10355
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 22:34:57 UTC 2023,,,,,,,,,,"0|z12nug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:34;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove netty bundling&relocation in flink-streaming-kinesis-tests,FLINK-27763,13446702,13449741,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,25/May/22 06:53,13/Jun/22 08:15,04/Jun/24 20:51,25/May/22 09:22,,,,,,,,,,,,,,,1.16.0,,,,Build System,Tests,,,,0,pull-request-available,,,,"flink-streaming-kinesis-tests bundles and relocates netty (and no other dependency). We can safely remove this because no class within this module references netty, nor any other module relies on the relocated versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 09:22:28 UTC 2022,,,,,,,,,,"0|z12nts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 09:22;chesnay;master: ffca99d5f369c21182b729ce5abe082ffa40d579;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka WakeupException during handling splits changes,FLINK-27762,13446690,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,zoucan,zoucan,25/May/22 06:00,17/Jun/22 02:45,04/Jun/24 20:51,17/Jun/22 02:45,1.14.3,,,,,,,,,,,,,,1.14.6,1.15.1,,,Connectors / Kafka,,,,,0,pull-request-available,,,," 

We enable dynamic partition discovery in our flink job, but job failed when kafka partition is changed. 

Exception detail is shown as follows:
{code:java}
java.lang.RuntimeException: One or more fetchers have encountered exception
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
	at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130)
	at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:350)
	at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.kafka.common.errors.WakeupException
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.maybeTriggerWakeup(ConsumerNetworkClient.java:511)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:275)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:233)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)
	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1726)
	at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1684)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.removeEmptySplits(KafkaPartitionSplitReader.java:315)
	at org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(KafkaPartitionSplitReader.java:200)
	at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51)
	at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
	... 6 more {code}
 

After preliminary investigation, according to source code of KafkaSource,

At first: 

method *org.apache.kafka.clients.consumer.KafkaConsumer.wakeup()* will be called if consumer is polling data.

Later: 

method *org.apache.kafka.clients.consumer.KafkaConsumer.position()* will be called during handle splits changes.

Since consumer has been waken up, it will throw WakeUpException.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 15 12:38:59 UTC 2022,,,,,,,,,,"0|z12nr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 07:07;martijnvisser;Can you please try this with the latest Flink 1.14 and/or Flink 1.15 version? ;;;","25/May/22 08:42;zoucan;[~martijnvisser] 

Thanks for your reply.

I have checked the source code of latest version, but there's no difference for this part.

It's difficult to test since this exception doesn't happen every time of partition change.

In my opinion, it happend only if method *org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.wakeUp()* is called while consumer is polling data. 

In this situation, above method will eventually call {*}org.apache.kafka.clients.consumer.KafkaConsumer.wakeup(){*}. 
{code:java}
// org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.wakeUp()

public void wakeUp() {
    wakeup = true;
    if (lastRecords == null) {
        // 1. consumer is polling data, execute org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.wakeUp() 
        splitReader.wakeUp()
    } else {
        elementsQueue.wakeUpPuttingThread(fetcherIndex);
    }
} {code}
 
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.wakeUp()

public void wakeUp() {
    // 2. execute org.apache.kafka.clients.consumer.KafkaConsumer.wakeup()
    consumer.wakeup();
} {code}
 

And in method {*}org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges(){*}, 

*org.apache.kafka.clients.consumer.KafkaConsumer.position()* will be called.
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.handleSplitsChanges()

public void handleSplitsChanges(SplitsChange<KafkaPartitionSplit> splitsChange) {
    // ignore irrelevant code...

    // 3.execute this.removeEmptySplits()
   removeEmptySplits();

    maybeLogSplitChangesHandlingResult(splitsChange);
} {code}
 
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.removeEmptySplits()

private void removeEmptySplits() {
    List<TopicPartition> emptyPartitions = new ArrayList<>();
   
    for (TopicPartition tp : consumer.assignment()) {
        // 4. execute org.apache.kafka.clients.consumer.KafkaConsumer.position()
        // since kafka consumer is waken up before, it will throw WakeUpException.
        if (consumer.position(tp) >= getStoppingOffset(tp)) {
            emptyPartitions.add(tp);
        }
    }

   // ignore irrelevant code...     
} {code}
 

Since our application is already online, we must evaluate the risk for upgrading version.

Anyway, I'll try to test with latest version and share the result.  And If you have any suggestion, please share with me.;;;","25/May/22 08:55;martijnvisser;[~renqs] WDYT?;;;","25/May/22 09:30;renqs;Thanks for reporting the issue [~zoucan]! I think it is indeed a bug that hard to reproduce. It could only happen if the wakeup operation is triggered after KafkaConsumer#poll returns and the flow is still in KafkaPartitionSplitReader#fetch. 

[~martijnvisser] could you assign the ticket to me? Thanks;;;","25/May/22 09:32;martijnvisser;Thank you [~renqs] I've assigned it to you!;;;","30/May/22 02:44;zoucan;[~renqs] 

Thanks for your effort for this issue.

I‘ve reviewed your pr and i have a question. The exception i met is in method {*}KafkaPartitionSplitReader#removeEmptySplits{*}. But i can't find any action for handling this exception in that method.
{code:java}
// org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader#removeEmptySplits

private void removeEmptySplits() {
    List<TopicPartition> emptyPartitions = new ArrayList<>();
   
    for (TopicPartition tp : consumer.assignment()) {
        // WakeUpException is thrown here.
        // since KafkaConsumer#wakeUp is called before,if execute KafkaConsumer#postion() in 'if statement' above, it will throw WakeUpException.   
        if (consumer.position(tp) >= getStoppingOffset(tp)) {
            emptyPartitions.add(tp);
        }
    }

   // ignore irrelevant code...     
} {code}
Maybe we should check whether *KafkaConsumer#wakeUp* is called before, or catch WakeUpException in *KafkaPartitionSplitReader#removeEmptySplits.*;;;","30/May/22 03:21;renqs;Thanks a lot for the review [~zoucan] ! What about moving the discussion about code and PRs to Github?;;;","02/Jun/22 21:43;mason6345;Hi [~renqs], I also recently encountered a similar issue internally

> It could only happen if the wakeup operation is triggered after KafkaConsumer#poll returns and the flow is still in KafkaPartitionSplitReader#fetch. 

I don't think so, because the exception is thrown from handling the split changes.

 ;;;","15/Jun/22 12:38;leonard;fixed in 
master(1.16):50c19d9f534f86e228f3e0937d92baf766a57165
release-1.15:a5c0c8776cb06507f12a00c93bd92a7ac6d18375
release-1.14: 4c9d99bbb5f3f250643b3a057866ec98cce04359;;;",,,,,,,,,,,,,,,,,,,,,,,
Add database and table field in canal-json sink,FLINK-27761,13446680,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pensz,pensz,25/May/22 03:53,25/May/22 03:53,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Ecosystem,,,,0,,,,,"When sink with canal-json format, only two fields in json root : {{data}} and {{{}type{}}}.

for example :
{code:java}
{""data"":[{""order_status"":""OK"",""order_amount"":120,""dt"":""2022-05-25""}],""type"":""INSERT""}{code}
If we want to output the data of multiple tables to the same topic, this will become inconvenient.

If the output contains database and table , we can continue to use the {{canal-json.database.include }}and {{canal-json.table.include}} to distinguish.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-25 03:53:15.0,,,,,,,,,,"0|z12now:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE is thrown when executing PyFlink jobs in batch mode,FLINK-27760,13446679,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,25/May/22 03:51,26/May/22 01:10,04/Jun/24 20:51,26/May/22 01:10,1.13.0,,,,,,,,,,,,,,1.13.7,1.14.5,1.15.1,1.16.0,API / Python,,,,,0,pull-request-available,,,,"This is the exception stack reported by one user:
{code}
022-05-25 11:39:32,792 [MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[I... -> PythonCal ... with job vertex id 71d9b8e1b249eaa7e67ef93fb483177f (63/100)#123] WARN org.apache.flink.runtime.taskmanager.Task [] - MultipleInput(readOrder=[2,1,0], members=[\nHashJoin(joinType=[I... -> PythonCal ... with job vertex id 71d9b8e1b249eaa7e67ef93fb483177f (63/100)#123 (6fa78755ff19ac9d0d57aba21840e834) switched from INITIALIZING to FAILED with failure cause: java.lang.NullPointerException
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.getKeyedStateBackend(AbstractStreamOperator.java:466)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processElementsOfCurrentKeyIfNeeded(AbstractPythonFunctionOperator.java:238)
at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processWatermark(AbstractPythonFunctionOperator.java:208)
at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.emitWatermark(OneInputStreamOperatorOutput.java:45)
at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:39)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:632)
at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:74)
at org.apache.flink.table.runtime.operators.multipleinput.output.OneInputStreamOperatorOutput.emitWatermark(OneInputStreamOperatorOutput.java:45)
at org.apache.flink.streaming.api.operators.CountingOutput.emitWatermark(CountingOutput.java:39)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark(AbstractStreamOperator.java:632)
at org.apache.flink.table.runtime.operators.TableStreamOperator.processWatermark(TableStreamOperator.java:74)
at org.apache.flink.streaming.api.operators.AbstractStreamOperator.processWatermark2(AbstractStreamOperator.java:649)
at org.apache.flink.table.runtime.operators.multipleinput.input.SecondInputOfTwoInput.processWatermark(SecondInputOfTwoInput.java:44)
at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessorFactory$StreamTaskNetworkOutput.emitWatermark(StreamMultipleInputProcessorFactory.java:318)
at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:196)
at org.apache.flink.streaming.runtime.streamstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:137)
at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:106)
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:87)
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:424)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:569)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:651)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:541)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:586)
at java.lang.Thread.run(Thread.java:877)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 26 01:10:55 UTC 2022,,,,,,,,,,"0|z12noo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/22 01:10;dianfu;Fixed in:
- master via 1348dee741b5af89279a55cb26669e72940911b5
- release-1.15 via 6490cd52825eeeeaf4325e5bfb902d9dbd604f56
- release-1.14 via 47f0ad48679561b452b14e7d6fe36f7abd53e33b
- release-1.13 via 84c1bd45e060fd4cba2ceeb899045d9d9cd72e56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rethink how to get the git commit id for docker image in Flink Kubernetes operator,FLINK-27759,13446663,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,nicholasjiang,wangyang0918,wangyang0918,25/May/22 02:15,16/Jan/23 20:03,04/Jun/24 20:51,16/Jan/23 20:03,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"Follow the discussion in the PR[1][2], we need to rethink how the get the git commit id properly. Currently, we rely on the .git directory. And it is a problem when building image from source release.

 

[1]. [https://github.com/apache/flink-kubernetes-operator/pull/243]

[2]. https://github.com/apache/flink-kubernetes-operator/pull/241",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 07 04:23:55 UTC 2022,,,,,,,,,,"0|z12nl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 02:18;nicholasjiang;[~wangyang0918], I will investigate the better way to get the git commit id for docker image in Flink Kubernetes operator.;;;","25/May/22 02:32;wangyang0918;Thanks for the volunteering. I believe this is not in hurry. Let's take in easy and collect more feedback.;;;","25/May/22 07:07;chesnay;Wouldn't it be trivial to embed a .git.properties file during the release process containing the hash, which is bundled in the jar? The release scripts should be able to do that.;;;","25/May/22 09:25;wangyang0918;Thanks [~chesnay] for the very nice suggestion. Maybe we could directly bundle the generated {{.flink-kubernetes-operator.version.properties}}.


{code:java}
project.version=1.0.0
git.commit.id.abbrev=2417603
git.commit.time=2022-05-23T13:42:07+0800
{code}
;;;","06/Jul/22 13:06;gyfora;Has this been fixed already or are we still looking for some new solution?;;;","07/Jul/22 04:23;wangyang0918;This is still not fixed. But it is not a high-priority ticket. We could revisit it later if necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-runtime,FLINK-27758,13446630,13417682,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,24/May/22 20:22,31/Oct/23 20:03,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,Tests,,,,0,pull-request-available,stale-assigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 22:35:07 UTC 2023,,,,,,,,,,"0|z12nds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector should not use flink-table-planner but flink-table-planner-loader,FLINK-27757,13446617,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,24/May/22 18:21,25/May/22 07:23,04/Jun/24 20:51,25/May/22 07:23,,,,,,,,,,,,,,,1.16.0,,,,Connectors / ElasticSearch,,,,,0,pull-request-available,,,,Connectors should not rely on {{flink-table-planner}} but on {{flink-table-planner-loader}} by default. We can should change this for the Elasticsearch connector as this is being externalized at the moment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 07:23:34 UTC 2022,,,,,,,,,,"0|z12naw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 07:23;martijnvisser;Fixed in Flink master: d9df7e78fc9b37bd4fd3784102385fdc4e9d5184
Fixed in Flink Elasticsearch main: 617bd96696443f6e0cd1adf773de346d24680e41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix intermittently failing test in AsyncSinkWriterTest.checkLoggedSendTimesAreWithinBounds,FLINK-27756,13446549,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chalixar,chalixar,chalixar,24/May/22 11:08,16/Jan/24 09:08,04/Jun/24 20:51,16/Jan/24 09:08,1.17.0,1.18.1,1.19.0,,,,,,,,,,,,1.17.3,1.18.2,1.19.0,,Connectors / Common,,,,,0,pull-request-available,test-stability,,,"h2. Motivation
 - One of the integration tests ({{checkLoggedSendTimesAreWithinBounds}}) of {{AsyncSinkWriterTest}} has been reported to fail intermittently on build pipeline causing blocking of new changes.
 - Reporting build is [linked |https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36009&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 16 09:08:49 UTC 2024,,,,,,,,,,"0|z12mw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 12:18;hxbks2ks;another instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36272&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","05/Jun/22 20:21;chalixar;h2. Findings.
- Test is failing 2/100 times.
- Test has tight margin 100 ms delay + 10 ms for execution.
- Extending margin to 20 ms passes tests with failure rate of 1/1000 [max margin needed is 21 ms]
- Sending 1 complete batch passes tests with failure rate of 1/1000 [max margin needed is 14ms]
- Sending 1 complete batch with extended margin passes test with rate 100/100;;;","13/Jun/22 08:44;dannycranmer;Merged a fix in [ceb285e|https://github.com/apache/flink/commit/ceb285ef5c0f2d501f2e29031e58ac52402c2a5c]. Please reopen if this reoccurs. ;;;","18/Apr/23 06:53;Sergey Nuyanzin;Reopen since there is a new reproduction on 1.17
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48214&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=7215
;;;","05/Jul/23 07:44;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50955&view=logs&j=8eee98ee-a482-5f7c-2c51-b3456453e704&t=da58e781-88fe-508b-b74c-018210e533cc&l=7533;;;","11/Jul/23 12:54;chalixar;[~Sergey Nuyanzin]thanks for reopening, I will try to take a look at it this week.;;;","17/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Oct/23 07:09;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53624&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=4300;;;","18/Dec/23 22:13;jhughes;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55636&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906;;;","18/Dec/23 22:14;jhughes;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55640&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906;;;","20/Dec/23 02:10;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55673&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906;;;","21/Dec/23 07:15;jiabao.sun;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55774&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906;;;","01/Jan/24 15:48;chalixar;[~Sergey Nuyanzin] I have refactored the test, please take a look at this ([PR|https://github.com/apache/flink/pull/24014]);;;","04/Jan/24 14:04;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56003&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906;;;","07/Jan/24 23:22;Sergey Nuyanzin;[~chalixar] thanks for looking into this
any idea how we can check that the PR fixes the issue?;;;","07/Jan/24 23:25;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55827&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=10814;;;","07/Jan/24 23:27;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55861&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=10814;;;","08/Jan/24 00:09;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55951&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=10812;;;","08/Jan/24 00:12;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55954&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=10813;;;","08/Jan/24 00:35;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55980&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=10795;;;","08/Jan/24 00:36;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55992&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=10705;;;","09/Jan/24 07:18;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56124&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=10739;;;","09/Jan/24 15:31;chalixar;Hi [~Sergey Nuyanzin]
The tests originally assumed an upperbound for sending times as configured by delay, however it discarded the delay induced by mailbox which is apparently higher on CI than local. I rewrote the test to fix this miss.
It is hard to reproduce locally since mailbox delay seems much lighter but I resampled the test and was able to detect a rare instance of the failure prior to the fix. ;;;","12/Jan/24 10:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56166&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=10414;;;","15/Jan/24 03:23;JunRuiLi;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56300&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906;;;","16/Jan/24 09:08;pnowojski;Thanks [~chalixar] for fixing this!

merged commit d92ab39 into apache:master
2b6c656ff31 into release-1.18
ca62b0070cf into release-1.17;;;",,,,,,
Introduce a filesystem catalog for table store,FLINK-27755,13446540,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/May/22 09:41,13/Jun/22 09:51,04/Jun/24 20:51,13/Jun/22 09:51,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"The Table Store has store the schema and other information on the file system so that it can actually provide a FileSystemCatalog for users to use.

This catalog is provided in JIRA:
- It supports database related management
- It supports the creation and deletion of tables
- It supports table changes, but currently can only modify the options and other information, the underlying does not yet support the schema column information modification

Currently this Catalog only supports filestore, logstore capabilities are still to be developed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 09:51:06 UTC 2022,,,,,,,,,,"0|z12mu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/22 09:51;lzljs3620320;master: 14dbb700f6d5222a03f6b6b8b0b17a1a5a6a4d9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Getting Flink config fails if multiple files in /lib match flink-dist-*,FLINK-27754,13446535,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rudi.kershaw,rudi.kershaw,rudi.kershaw,24/May/22 08:48,24/May/22 12:32,04/Jun/24 20:51,24/May/22 12:32,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,"h3. Background

When adding jars to the {{/lib}} directory any extra jar files that match the pattern {{flink-dist*.jar}} provoke an error when trying to use {{BashJavaUtils}} to get JVM parameters and dynamic configurations in {{{}config.sh{}}}. Although niche, this can be difficult to debug because there is no error message to describe the specific issue.

We already print a useful error message if no {{flink-dist*.jar}} can be found at all. This pull request adds another error message when more than one {{flink-dist*.jar}} is found at this point.
h3. Reproduction Steps
 # ./mvnw clean install -DskipTests -Dfast
 # cp flink-dist-scala/target/flink-dist-scala_2.12-1.16-SNAPSHOT.jar build-target/lib/
 # build-target/bin/start-cluster.sh
 # You should a vague error message ""NoClassDefinition"" or similar.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 12:32:58 UTC 2022,,,,,,,,,,"0|z12msw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 08:49;dannycranmer;PR: https://github.com/apache/flink/pull/19721;;;","24/May/22 12:32;dannycranmer;Merged into master @ [d891509 |https://github.com/apache/flink/commit/d891509ab78acf7c3dc1000ef9d55e9d121e1a45];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala suffix checks should check for maven errors,FLINK-27753,13446528,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,24/May/22 08:15,14/Nov/22 14:41,04/Jun/24 20:51,14/Nov/22 14:41,,,,,,,,,,,,,,,1.17.0,,,,Build System,Build System / CI,,,,0,pull-request-available,,,,"As shown in FLINK-27751 errors in maven when retrieving the dependency tree are currently ignored, causing garbage output being passed to the suffix checker.
This results in highly misleading error messages about incorrect suffixes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27751,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 14 14:41:15 UTC 2022,,,,,,,,,,"0|z12mrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/22 14:41;chesnay;master: 8205dde5178f3f3857ce0d9c13904bfa7319fd69;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change 'path' to 'root-path' in table store,FLINK-27752,13446520,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,24/May/22 07:53,25/May/22 06:46,04/Jun/24 20:51,25/May/22 06:46,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"path is easy to bother the user, he/she will misinterpret it as path of table.
Let's change it to root-path, and it will be clearer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 06:46:21 UTC 2022,,,,,,,,,,"0|z12mpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 06:46;lzljs3620320;master: 14c26f9f0fa4a9fa4094c338729036fb4166ed66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dependency resolution from repository.jboss.org fails on CI,FLINK-27751,13446510,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,hxbks2ks,hxbks2ks,24/May/22 06:46,12/Aug/22 12:31,04/Jun/24 20:51,24/May/22 08:52,1.15.0,1.16.0,,,,,,,,,,,,,1.14.5,1.15.1,1.16.0,,Build System / Azure Pipelines,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-05-24T03:50:20.5443243Z 03:50:20,543 ERROR org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker     [] - Violations found:
2022-05-24T03:50:20.5444210Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-formats/flink-sequence-file/pom.xml'.
2022-05-24T03:50:20.5445185Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hadoop-compatibility/pom.xml'.
2022-05-24T03:50:20.5446207Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-1.4/pom.xml'.
2022-05-24T03:50:20.5447186Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-2.2/pom.xml'.
2022-05-24T03:50:20.5448135Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
2022-05-24T03:50:20.5449237Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hive/pom.xml'.
2022-05-24T03:50:20.5450180Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-table/flink-sql-client/pom.xml'.
2022-05-24T03:50:20.5451049Z 	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-tests/pom.xml'.
2022-05-24T03:50:20.5452020Z 	Scala-free module 'flink-hcatalog' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
2022-05-24T03:50:20.5453369Z 	Scala-free module 'flink-sql-connector-hive-2.3.9' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.3.9/pom.xml'.
2022-05-24T03:50:20.5454388Z 	Scala-free module 'flink-sql-connector-hive-3.1.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-3.1.2/pom.xml'.
2022-05-24T03:50:20.6860887Z ==============================================================================
2022-05-24T03:50:20.6861601Z Suffix Check failed. See previous output for details.
2022-05-24T03:50:20.6862335Z ==============================================================================
2022-05-24T03:50:20.6942100Z ##[error]Bash exited with code '1'.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35994&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27753,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 08:52:27 UTC 2022,,,,,,,,,,"0|z12mnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 07:19;hxbks2ks;Different results from the same commit on release-1.15.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35968&view=results
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35987&view=results;;;","24/May/22 08:20;chesnay;Fails because repository.jboss.org is down.;;;","24/May/22 08:52;chesnay;The repo was temporarily disabled.

master: 8cbb42a839919c3163f30928254a1afd0077f6a6
1.15: 7b04b24c109a567603614d66ad2ce76e429a2521 
1.14: 82d36d0d0c1e0df408db8bc41a42d4309a112852 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The configuration of JobManagerOption.TOTAL_PROCESS_MEMORY(jobmanager.memory.process.size) not work,FLINK-27750,13446496,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Bug,,sharp,sharp,24/May/22 04:51,13/Apr/23 08:31,04/Jun/24 20:51,13/Apr/23 08:31,1.14.4,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,auto-deprioritized-major,jobmanager.memory.process.size,TOTAL_PROCESS_MEMORY,,"By constructing kubernetesClusterDescriptor and Fabric8FlinkKubeClient to deploy kubernetes application mode of job，The code is shown below.
{code:java}
//Initialize flinkConfiguration and set options including TOTAL_PROCESS_MEMORY
Configuration flinkConfiguration = GlobalConfiguration.loadConfiguration();
flinkConfiguration.set(DeploymentOptions.TARGET, KubernetesDeploymentTarget.APPLICATION.getName())
.set(PipelineOptions.JARS, Collections.singletonList(flinkDistJar))
.set(KubernetesConfigOptions.CLUSTER_ID, ""APPLICATION1"")
.set(KubernetesConfigOptions.CONTAINER_IMAGE, ""img_url"")
.set(KubernetesConfigOptions.CONTAINER_IMAGE_PULL_POLICY, KubernetesConfigOptions.ImagePullPolicy.Always)
.set(JobManagerOptions.TOTAL_PROCESS_MEMORY, MemorySize.parse(""1024M""))
.set...;

//Construct kubernetesClusterDescriptor and Fabric8FlinkKubeClient
 KubernetesClusterDescriptor kubernetesClusterDescriptor = new KubernetesClusterDescriptor(
                        flinkConfiguration,
                        new Fabric8FlinkKubeClient(
                                flinkConfiguration,
                                new DefaultKubernetesClient(),
                                Executors.newFixedThreadPool(2)
                        )
                );
ApplicationConfiguration applicationConfiguration = new ApplicationConfiguration(execArgs, null);

//deploy kubernetes application mode of job
ClusterClient<String> clusterClient = kubernetesClusterDescriptor.deployApplicationCluster(
                        new ClusterSpecification.ClusterSpecificationBuilder().createClusterSpecification(),
                        applicationConfiguration
                ).getClusterClient();

String clusterId = clusterClient.getClusterId(); {code}
As above，I set TOTAL_PROCESS_MEMORY to 1024M，The flink UI displays the following memory configuration，which is clearly correct(448+128+256+192=1024).

!image-2022-05-24-14-00-39-255.png|width=759,height=255!

But when I turn to JobManager using {_}Kubectl Describe Deployment{_}, I found that the POD memory of JobManager was always 768M, which should have been equal to TOTAL_PROCESS_MEMORY 1024M. And no matter how I adjust TOTAL_PROCESS_MEMORY parameter it doesn't work.

!image-2022-05-24-14-18-30-063.png!

The result is a POD OOMkilled when JobManager memory usage exceeds 768M.

I expect the JobManager pod to be equal to TOTAL_PROCESS_MEMORY, so I can adjust the memory to suit my needs.

Is there something WRONG with my configuration, or should JobManager's pod take up the same amount of memory as TOTAL_PROCESS_MEMORY?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/22 06:00;sharp;image-2022-05-24-14-00-39-255.png;https://issues.apache.org/jira/secure/attachment/13044105/image-2022-05-24-14-00-39-255.png","24/May/22 06:18;sharp;image-2022-05-24-14-18-30-063.png;https://issues.apache.org/jira/secure/attachment/13044106/image-2022-05-24-14-18-30-063.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Thu Apr 13 08:31:05 UTC 2023,,,,,,,,,,"0|z12mk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","31/Jul/22 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","23/Feb/23 08:42;Weijie Guo;Hi [~sharp], Have you tried a higher version of Flink? 1.14 is not supported now.;;;","13/Apr/23 08:31;xtsong;Closing the ticket due to:
1. 1.14.4 is no longer supported.
2. User should not directly access the Flink's internal classes.

For the record, the root cause here is that the JM in `flinkConfiguration` will be overwritten by  `ClusterSpecification`. The user created a `ClusterSpecification` without specifying proper memory sizes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job submit failed when JM enabled HA with flink 1.15.0,FLINK-27749,13446495,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,JasonLee,JasonLee,24/May/22 04:28,24/May/22 05:19,04/Jun/24 20:51,24/May/22 05:19,1.15.0,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,,0,,,,,"Job submit failed when JM enabled HA with flink 1.15.0，zookeeper 3.4.10
{code:java}
// code placeholder
Logs for container_1653234457991_0009_01_000001
ResourceManager
RM Home
NodeManager
Tools
2022-05-22 23:18:37,520 WARN  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Job Clusters are deprecated since Flink 1.15. Please use an Application Cluster/Application Mode instead.
2022-05-22 23:18:37,574 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
2022-05-22 23:18:37,574 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting YarnJobClusterEntrypoint (Version: 1.15.0, Scala: 2.12, Rev:3a4c113, Date:2022-04-20T19:50:32+02:00)
2022-05-22 23:18:37,574 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: root
2022-05-22 23:18:37,703 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: root
2022-05-22 23:18:37,703 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.111-b14
2022-05-22 23:18:37,703 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
2022-05-22 23:18:37,705 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 859 MiBytes
2022-05-22 23:18:37,705 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /home/jason/bigdata/jdk/jdk1.8.0_111
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop version: 2.9.0
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx939524096
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms939524096
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dflink_job_name=FlinkStreamingNewDemoHome
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xloggc:/home/jason/bigdata/hadoop/hadoop-2.9.0/logs/userlogs/application_1653234457991_0009/container_1653234457991_0009_01_000001/gc.log
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+PrintGCDetails
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+PrintGCDateStamps
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+UseGCLogFileRotation
2022-05-22 23:18:37,706 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:NumberOfGCLogFiles=5
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:GCLogFileSize=100M
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/home/jason/bigdata/hadoop/hadoop-2.9.0/logs/userlogs/application_1653234457991_0009/container_1653234457991_0009_01_000001/jobmanager.log
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:log4j.properties
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:log4j.properties
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=939524096b
2022-05-22 23:18:37,707 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-22 23:18:37,708 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
2022-05-22 23:18:37,708 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: :flink-1.14.0-1.0-SNAPSHOT.jar:lib/flink-cep-1.15.0.jar:lib/flink-connector-files-1.15.0.jar:lib/flink-csv-1.15.0.jar:lib/flink-json-1.15.0.jar:lib/flink-scala_2.12-1.15.0.jar:lib/flink-shaded-zookeeper-3.5.9.jar:lib/flink-table-api-java-uber-1.15.0.jar:lib/flink-table-planner-loader-1.15.0.jar:lib/flink-table-runtime-1.15.0.jar:lib/log4j-1.2-api-2.17.1.jar:lib/log4j-api-2.17.1.jar:lib/log4j-core-2.17.1.jar:lib/log4j-slf4j-impl-2.17.1.jar:flink-dist-1.15.0.jar:job.graph:flink-conf.yaml::/home/jason/bigdata/hadoop/hadoop-2.9.0/etc/hadoop:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-auth-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/junit-4.11.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-annotations-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mysql-connector-java-5.1.47.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jedis-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-api-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-registry-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-router-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jline-2.12.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/spark-2.4.0-yarn-shuffle.jar
2022-05-22 23:18:37,708 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
2022-05-22 23:18:37,709 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
2022-05-22 23:18:37,711 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - YARN daemon is running as: root Yarn client user obtainer: root
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.jobgraph-path, job.graph
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.jobmanager, -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.checkpoints.history, 15
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts, -Dflink_job_name=FlinkStreamingNewDemoHome
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.reporter.promgateway.jobName, FlinkStreamingNewDemoHome
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.cluster-id, application_1653234457991_0009
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, storm1
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.reporter.promgateway.groupingKey, jobname=FlinkStreamingNewDemoHome
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.web.port, 8081
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.zookeeper.path.root, /flink
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.storageDir, hdfs:///flink/recovery/ha
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.tmp.dirs, /tmp
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.watch.heartbeat.pause, 20 s
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.off-heap, true
2022-05-22 23:18:37,718 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 4
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.block.cache-size, 512M
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: cluster.evenly-spread-out-slots, false
2022-05-22 23:18:37,719 WARN  org.apache.flink.configuration.GlobalConfiguration           [] - Error while trying to split key and value in configuration file /home/jason/bigdata/hadoop/hadoop-2.9.0/tmp/nm-local-dir/usercache/root/appcache/application_1653234457991_0009/container_1653234457991_0009_01_000001/flink-conf.yaml:18: ""pipeline.classpaths: ""
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.stack-depth, 100
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.queue, flink
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.num-samples, 100
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.archive.fs.dir, hdfs:///flink/flink-jobs/
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.flink.size, 1024m
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.per-job-cluster.include-user-jar, ORDER
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 4096m
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.mode, EXACTLY_ONCE
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.local-recovery, false
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.cleanup-interval, 100 s
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.cancel.enable, true
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.incremental, true
2022-05-22 23:18:37,719 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.web.submit.enable, true
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.archive.fs.refresh-interval, 10000
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.upload.dir, /home/jason/bigdata/flink/flink-1.13.1/jars
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.port, 8082
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.interval, 300s
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.delay-between-samples, 1 s
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application-attempt-failures-validity-interval, 60000
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.zookeeper.quorum, master:2181,storm1:2181,storm2:2181
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend, rocksdb
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.writebuffer.count, 2
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.writebuffer.number-to-merge, 1
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.deployment.config-dir, /home/jason/bigdata/flink/flink-1.15.0/conf
2022-05-22 23:18:37,720 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.yarn.log-config-file, /home/jason/bigdata/flink/flink-1.15.0/conf/log4j.properties
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.num-retained, 2
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.web.port, 8082
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.enabled, true
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.connection-timeout, 360000000
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.savepoints.dir, hdfs:///flink-rockdb/savepoints
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.watch.heartbeat.interval, 5 s
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: cluster.declarative-resource-management.enabled, true
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: table.dynamic-table-options.enabled, true
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.thread.num, 4
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.savepoint.ignore-unclaimed-state, false
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application-attempts, 3
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 4
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.scope.jm.job, <job_name>.<host>.jobmanager
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.async, true
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.refresh-interval, 30 s
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.ask.timeout, 60 s
2022-05-22 23:18:37,721 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.name, FlinkStreamingNewDemoHome
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.savepoint-restore-mode, NO_CLAIM
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.idleness-timeout, 360000000
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: classloader.resolve-order, child-first
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.latency.interval, 20000
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.target, yarn-per-job
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.framesize, 20971520b
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.attached, false
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.cluster.execution-mode, DETACHED
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.scheduler, adaptive
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.externalized-checkpoint-retention, RETAIN_ON_CANCELLATION
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability, zookeeper
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.shutdown-on-attached-exit, false
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: pipeline.jars, file:/home/jason/bigdata/jar/flink-1.14.0-1.0-SNAPSHOT.jar
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.taskmanager, -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.writebuffer.size, 32M
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.latency-track-enabled, true
2022-05-22 23:18:37,722 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.dir, hdfs:///flink-rockdb/checkpoints
2022-05-22 23:18:37,737 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'jobmanager.web.port' instead of proper key 'web.port'
2022-05-22 23:18:37,738 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2022-05-22 23:18:37,747 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-22 23:18:37,747 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-22 23:18:37,748 INFO  org.apache.flink.runtime.clusterframework.BootstrapTools     [] - Overriding Flink's temporary file directories with those specified in the Flink config: /tmp
2022-05-22 23:18:37,754 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting YarnJobClusterEntrypoint.
2022-05-22 23:18:37,772 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
2022-05-22 23:18:37,808 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
2022-05-22 23:18:37,827 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to root (auth:SIMPLE)
2022-05-22 23:18:37,829 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-22 23:18:37,832 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-7584859953935097287.conf.
2022-05-22 23:18:37,837 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
2022-05-22 23:18:37,841 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-22 23:18:37,844 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Using working directory: WorkingDirectory(/tmp/jm_c8c66329c023da74ef100f27ae2044e8).
2022-05-22 23:18:37,845 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-22 23:18:38,081 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address storm1:0, bind address 0.0.0.0:0.
2022-05-22 23:18:38,620 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-05-22 23:18:38,640 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
2022-05-22 23:18:38,640 INFO  akka.remote.Remoting                                         [] - Starting remoting
2022-05-22 23:18:38,743 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@storm1:35680]
2022-05-22 23:18:38,857 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@storm1:35680
2022-05-22 23:18:39,255 INFO  org.apache.flink.runtime.blob.FileSystemBlobStore            [] - Creating highly available BLOB storage directory at hdfs:/flink/recovery/ha/application_1653234457991_0009/blob
2022-05-22 23:18:39,317 INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Enforcing default ACL for ZK connections
2022-05-22 23:18:39,318 INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Using '/flink/application_1653234457991_0009' as Zookeeper namespace.
2022-05-22 23:18:39,375 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Starting
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:zookeeper.version=3.5.9-83df9301aa5c2a5d284a9940177808c01bc35cef, built on 01/06/2021 20:03 GMT
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:host.name=storm1
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.version=1.8.0_111
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.vendor=Oracle Corporation
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.home=/home/jason/bigdata/jdk/jdk1.8.0_111/jre
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.class.path=:flink-1.14.0-1.0-SNAPSHOT.jar:lib/flink-cep-1.15.0.jar:lib/flink-connector-files-1.15.0.jar:lib/flink-csv-1.15.0.jar:lib/flink-json-1.15.0.jar:lib/flink-scala_2.12-1.15.0.jar:lib/flink-shaded-zookeeper-3.5.9.jar:lib/flink-table-api-java-uber-1.15.0.jar:lib/flink-table-planner-loader-1.15.0.jar:lib/flink-table-runtime-1.15.0.jar:lib/log4j-1.2-api-2.17.1.jar:lib/log4j-api-2.17.1.jar:lib/log4j-core-2.17.1.jar:lib/log4j-slf4j-impl-2.17.1.jar:flink-dist-1.15.0.jar:job.graph:flink-conf.yaml::/home/jason/bigdata/hadoop/hadoop-2.9.0/etc/hadoop:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-auth-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/junit-4.11.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-annotations-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mysql-connector-java-5.1.47.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jedis-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-api-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-registry-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-router-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jline-2.12.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/spark-2.4.0-yarn-shuffle.jar
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.library.path=:/home/jason/bigdata/hadoop/hadoop-2.9.0/lib/native:/home/jason/bigdata/hadoop/hadoop-2.9.0/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.io.tmpdir=/tmp
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.compiler=<NA>
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.name=Linux
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.arch=amd64
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.version=3.10.0-693.el7.x86_64
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:user.name=root
2022-05-22 23:18:39,380 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:user.home=/root
2022-05-22 23:18:39,381 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:user.dir=/home/jason/bigdata/hadoop/hadoop-2.9.0/tmp/nm-local-dir/usercache/root/appcache/application_1653234457991_0009/container_1653234457991_0009_01_000001
2022-05-22 23:18:39,381 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.memory.free=726MB
2022-05-22 23:18:39,381 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.memory.max=859MB
2022-05-22 23:18:39,381 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.memory.total=859MB
2022-05-22 23:18:39,382 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Initiating client connection, connectString=master:2181,storm1:2181,storm2:2181 sessionTimeout=60000 watcher=org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState@472a11ae
2022-05-22 23:18:39,386 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.common.X509Util [] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-05-22 23:18:39,389 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxnSocket [] - jute.maxbuffer value is 4194304 Bytes
2022-05-22 23:18:39,394 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - zookeeper.request.timeout value is 0. feature enabled=
2022-05-22 23:18:39,398 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-7584859953935097287.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.
2022-05-22 23:18:39,401 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Opening socket connection to server storm2/192.168.0.2:2181
2022-05-22 23:18:39,401 ERROR org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState [] - Authentication failed
2022-05-22 23:18:39,402 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Default schema
2022-05-22 23:18:39,402 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Socket connection established, initiating session, client: /192.168.0.3:52884, server: storm2/192.168.0.2:2181
2022-05-22 23:18:39,411 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Session establishment complete on server storm2/192.168.0.2:2181, sessionid = 0x380eabbdaf10007, negotiated timeout = 40000
2022-05-22 23:18:39,412 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: CONNECTED
2022-05-22 23:18:39,535 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Unable to read additional data from server sessionid 0x380eabbdaf10007, likely server has closed socket, closing socket connection and attempting reconnect
2022-05-22 23:18:39,534 ERROR org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Ensure path threw exception
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /flink
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:106) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:351) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:109) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:618) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.runtime.util.ZooKeeperUtils.useNamespaceAndEnsurePath(ZooKeeperUtils.java:729) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.<init>(ZooKeeperMultipleComponentLeaderElectionHaServices.java:85) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:140) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:427) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:376) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227) ~[flink-dist-1.15.0.jar:1.15.0]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_111]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) [hadoop-common-2.9.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:109) [flink-dist-1.15.0.jar:1.15.0]
2022-05-22 23:18:39,538 ERROR org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Unhandled error in curator framework, error message: Ensure path threw exception
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /flink
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:106) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:351) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:109) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:618) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.runtime.util.ZooKeeperUtils.useNamespaceAndEnsurePath(ZooKeeperUtils.java:729) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.<init>(ZooKeeperMultipleComponentLeaderElectionHaServices.java:85) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:140) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:427) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:376) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227) ~[flink-dist-1.15.0.jar:1.15.0]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_111]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) [hadoop-common-2.9.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:109) [flink-dist-1.15.0.jar:1.15.0]
2022-05-22 23:18:39,540 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /flink
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:106) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:351) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:109) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:618) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
    at org.apache.flink.runtime.util.ZooKeeperUtils.useNamespaceAndEnsurePath(ZooKeeperUtils.java:729) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.<init>(ZooKeeperMultipleComponentLeaderElectionHaServices.java:85) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:140) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:427) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:376) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227) ~[flink-dist-1.15.0.jar:1.15.0]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_111]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) [hadoop-common-2.9.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:109) [flink-dist-1.15.0.jar:1.15.0]
2022-05-22 23:18:39,543 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting YarnJobClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2022-05-22 23:18:39,636 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: SUSPENDED
2022-05-22 23:18:40,025 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-7584859953935097287.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.
2022-05-22 23:18:40,025 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Opening socket connection to server master/192.168.0.4:2181
2022-05-22 23:18:40,026 ERROR org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState [] - Authentication failed
2022-05-22 23:18:40,028 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Socket connection established, initiating session, client: /192.168.0.3:43286, server: master/192.168.0.4:2181
2022-05-22 23:18:40,035 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Session establishment complete on server master/192.168.0.4:2181, sessionid = 0x380eabbdaf10007, negotiated timeout = 40000
2022-05-22 23:18:40,036 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: RECONNECTED
 {code}
When I upgraded ZooKeeper to 3.5.9, the error was still reported
{code:java}
// code placeholder

Logs for container_1653323265354_0006_01_000001
ResourceManager
RM Home
NodeManager
Tools
2022-05-24 09:55:31,503 WARN  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Job Clusters are deprecated since Flink 1.15. Please use an Application Cluster/Application Mode instead.
2022-05-24 09:55:31,523 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
2022-05-24 09:55:31,523 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting YarnJobClusterEntrypoint (Version: 1.15.0, Scala: 2.12, Rev:3a4c113, Date:2022-04-20T19:50:32+02:00)
2022-05-24 09:55:31,523 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: root
2022-05-24 09:55:31,645 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: root
2022-05-24 09:55:31,645 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.111-b14
2022-05-24 09:55:31,645 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Arch: amd64
2022-05-24 09:55:31,647 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 859 MiBytes
2022-05-24 09:55:31,647 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /home/jason/bigdata/jdk/jdk1.8.0_111
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop version: 2.9.0
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx939524096
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms939524096
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dflink_job_name=FlinkStreamingNewDemoHome
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xloggc:/home/jason/bigdata/hadoop/hadoop-2.9.0/logs/userlogs/application_1653323265354_0006/container_1653323265354_0006_01_000001/gc.log
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+PrintGCDetails
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+PrintGCDateStamps
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:+UseGCLogFileRotation
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:NumberOfGCLogFiles=5
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:GCLogFileSize=100M
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/home/jason/bigdata/hadoop/hadoop-2.9.0/logs/userlogs/application_1653323265354_0006/container_1653323265354_0006_01_000001/jobmanager.log
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:log4j.properties
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:log4j.properties
2022-05-24 09:55:31,648 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments:
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.off-heap.size=134217728b
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.min=201326592b
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-metaspace.size=268435456b
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.heap.size=939524096b
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -D
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     jobmanager.memory.jvm-overhead.max=201326592b
2022-05-24 09:55:31,649 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: :flink-1.14.0-1.0-SNAPSHOT.jar:lib/flink-cep-1.15.0.jar:lib/flink-connector-files-1.15.0.jar:lib/flink-csv-1.15.0.jar:lib/flink-json-1.15.0.jar:lib/flink-scala_2.12-1.15.0.jar:lib/flink-shaded-zookeeper-3.6.3.jar:lib/flink-table-api-java-uber-1.15.0.jar:lib/flink-table-planner-loader-1.15.0.jar:lib/flink-table-runtime-1.15.0.jar:lib/log4j-1.2-api-2.17.1.jar:lib/log4j-api-2.17.1.jar:lib/log4j-core-2.17.1.jar:lib/log4j-slf4j-impl-2.17.1.jar:flink-dist-1.15.0.jar:job.graph:flink-conf.yaml::/home/jason/bigdata/hadoop/hadoop-2.9.0/etc/hadoop:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-auth-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/junit-4.11.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-annotations-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mysql-connector-java-5.1.47.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jedis-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-api-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-registry-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-router-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jline-2.12.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/spark-2.4.0-yarn-shuffle.jar
2022-05-24 09:55:31,650 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------
2022-05-24 09:55:31,650 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]
2022-05-24 09:55:31,653 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - YARN daemon is running as: root Yarn client user obtainer: root
2022-05-24 09:55:31,659 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.jobgraph-path, job.graph
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.jobmanager, -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.checkpoints.history, 15
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts, -Dflink_job_name=FlinkStreamingNewDemoHome
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.reporter.promgateway.jobName, FlinkStreamingNewDemoHome
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.cluster-id, application_1653323265354_0006
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, storm1
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.reporter.promgateway.groupingKey, jobname=FlinkStreamingNewDemoHome
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.web.port, 8081
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.zookeeper.path.root, /flink
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.storageDir, hdfs:///flink/recovery/ha
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.tmp.dirs, /tmp
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.watch.heartbeat.pause, 20 s
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.off-heap, true
2022-05-24 09:55:31,660 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 4
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.block.cache-size, 512M
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: cluster.evenly-spread-out-slots, false
2022-05-24 09:55:31,661 WARN  org.apache.flink.configuration.GlobalConfiguration           [] - Error while trying to split key and value in configuration file /home/jason/bigdata/hadoop/hadoop-2.9.0/tmp/nm-local-dir/usercache/root/appcache/application_1653323265354_0006/container_1653323265354_0006_01_000001/flink-conf.yaml:18: ""pipeline.classpaths: ""
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.stack-depth, 100
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.queue, flink
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.num-samples, 100
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.archive.fs.dir, hdfs:///flink/flink-jobs/
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.flink.size, 1024m
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.per-job-cluster.include-user-jar, ORDER
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 4096m
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.mode, EXACTLY_ONCE
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.local-recovery, false
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.cleanup-interval, 100 s
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.cancel.enable, true
2022-05-24 09:55:31,661 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.incremental, true
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.web.submit.enable, true
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.archive.fs.refresh-interval, 10000
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: web.upload.dir, /home/jason/bigdata/flink/flink-1.13.1/jars
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.port, 8082
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.interval, 300s
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.delay-between-samples, 1 s
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application-attempt-failures-validity-interval, 60000
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.zookeeper.quorum, master:2181,storm1:2181,storm2:2181
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend, rocksdb
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.writebuffer.count, 2
2022-05-24 09:55:31,662 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.writebuffer.number-to-merge, 1
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.deployment.config-dir, /home/jason/bigdata/flink/flink-1.15.0/conf
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.yarn.log-config-file, /home/jason/bigdata/flink/flink-1.15.0/conf/log4j.properties
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.num-retained, 2
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: historyserver.web.port, 8082
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.enabled, true
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.connection-timeout, 360000000
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.savepoints.dir, hdfs:///flink-rockdb/savepoints
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.watch.heartbeat.interval, 5 s
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: cluster.declarative-resource-management.enabled, true
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: table.dynamic-table-options.enabled, true
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.thread.num, 4
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.savepoint.ignore-unclaimed-state, false
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application-attempts, 3
2022-05-24 09:55:31,663 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 4
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.scope.jm.job, <job_name>.<host>.jobmanager
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.async, true
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.flamegraph.refresh-interval, 30 s
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.ask.timeout, 60 s
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.name, FlinkStreamingNewDemoHome
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.savepoint-restore-mode, NO_CLAIM
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: rest.idleness-timeout, 360000000
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: classloader.resolve-order, child-first
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: metrics.latency.interval, 20000
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.target, yarn-per-job
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: akka.framesize, 20971520b
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.attached, false
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.cluster.execution-mode, DETACHED
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.scheduler, adaptive
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.checkpointing.externalized-checkpoint-retention, RETAIN_ON_CANCELLATION
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability, zookeeper
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.shutdown-on-attached-exit, false
2022-05-24 09:55:31,664 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: pipeline.jars, file:/home/jason/bigdata/jar/flink-1.14.0-1.0-SNAPSHOT.jar
2022-05-24 09:55:31,665 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: env.java.opts.taskmanager, -Xloggc:<LOG_DIR>/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M
2022-05-24 09:55:31,665 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.writebuffer.size, 32M
2022-05-24 09:55:31,665 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.backend.rocksdb.latency-track-enabled, true
2022-05-24 09:55:31,665 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: state.checkpoints.dir, hdfs:///flink-rockdb/checkpoints
2022-05-24 09:55:31,680 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'jobmanager.web.port' instead of proper key 'web.port'
2022-05-24 09:55:31,680 INFO  org.apache.flink.configuration.Configuration                 [] - Config uses fallback configuration key 'rest.port' instead of key 'rest.bind-port'
2022-05-24 09:55:31,689 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-24 09:55:31,689 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-24 09:55:31,689 INFO  org.apache.flink.runtime.clusterframework.BootstrapTools     [] - Overriding Flink's temporary file directories with those specified in the Flink config: /tmp
2022-05-24 09:55:31,695 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting YarnJobClusterEntrypoint.
2022-05-24 09:55:31,712 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.
2022-05-24 09:55:31,741 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.
2022-05-24 09:55:31,761 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to root (auth:SIMPLE)
2022-05-24 09:55:31,763 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-24 09:55:31,766 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /tmp/jaas-809470794203701266.conf.
2022-05-24 09:55:31,772 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.
2022-05-24 09:55:31,775 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-24 09:55:31,778 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Using working directory: WorkingDirectory(/tmp/jm_a48a039ba0c76d84e2a942d7092db5d5).
2022-05-24 09:55:31,779 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'taskmanager.tmp.dirs' instead of proper key 'io.tmp.dirs'
2022-05-24 09:55:32,057 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address storm1:0, bind address 0.0.0.0:0.
2022-05-24 09:55:32,580 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started
2022-05-24 09:55:32,604 INFO  akka.remote.RemoteActorRefProvider                           [] - Akka Cluster not in use - enabling unsafe features anyway because `akka.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
2022-05-24 09:55:32,604 INFO  akka.remote.Remoting                                         [] - Starting remoting
2022-05-24 09:55:32,702 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@storm1:34382]
2022-05-24 09:55:32,810 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@storm1:34382
2022-05-24 09:55:33,183 INFO  org.apache.flink.runtime.blob.FileSystemBlobStore            [] - Creating highly available BLOB storage directory at hdfs:/flink/recovery/ha/application_1653323265354_0006/blob
2022-05-24 09:55:33,232 INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Enforcing default ACL for ZK connections
2022-05-24 09:55:33,233 INFO  org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Using '/flink/application_1653323265354_0006' as Zookeeper namespace.
2022-05-24 09:55:33,290 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Starting
2022-05-24 09:55:33,294 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2022-05-24 09:55:33,294 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:host.name=storm1
2022-05-24 09:55:33,294 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.version=1.8.0_111
2022-05-24 09:55:33,294 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.vendor=Oracle Corporation
2022-05-24 09:55:33,294 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.home=/home/jason/bigdata/jdk/jdk1.8.0_111/jre
2022-05-24 09:55:33,294 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.class.path=:flink-1.14.0-1.0-SNAPSHOT.jar:lib/flink-cep-1.15.0.jar:lib/flink-connector-files-1.15.0.jar:lib/flink-csv-1.15.0.jar:lib/flink-json-1.15.0.jar:lib/flink-scala_2.12-1.15.0.jar:lib/flink-shaded-zookeeper-3.6.3.jar:lib/flink-table-api-java-uber-1.15.0.jar:lib/flink-table-planner-loader-1.15.0.jar:lib/flink-table-runtime-1.15.0.jar:lib/log4j-1.2-api-2.17.1.jar:lib/log4j-api-2.17.1.jar:lib/log4j-core-2.17.1.jar:lib/log4j-slf4j-impl-2.17.1.jar:flink-dist-1.15.0.jar:job.graph:flink-conf.yaml::/home/jason/bigdata/hadoop/hadoop-2.9.0/etc/hadoop:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/hadoop-common-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-auth-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/junit-4.11.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/hadoop-annotations-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/mysql-connector-java-5.1.47.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/common/lib/jedis-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-2.9.0-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/hadoop-hdfs-native-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-api-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-registry-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-router-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-common-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-client-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/nimbus-jose-jwt-3.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-net-3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/gson-2.2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-digester-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xz-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-recipes-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-framework-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-smart-1.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/xmlenc-0.52.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jcip-annotations-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/activation-1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsch-0.1.54.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-core-1.8.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/guice-3.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/asm-3.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/commons-beanutils-1.7.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/avro-1.7.7.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsp-api-2.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/jline-2.12.1.jar:/home/jason/bigdata/hadoop/hadoop-2.9.0/share/hadoop/yarn/lib/spark-2.4.0-yarn-shuffle.jar
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.library.path=:/home/jason/bigdata/hadoop/hadoop-2.9.0/lib/native:/home/jason/bigdata/hadoop/hadoop-2.9.0/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.io.tmpdir=/tmp
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:java.compiler=<NA>
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.name=Linux
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.arch=amd64
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.version=3.10.0-693.el7.x86_64
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:user.name=root
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:user.home=/root
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:user.dir=/home/jason/bigdata/hadoop/hadoop-2.9.0/tmp/nm-local-dir/usercache/root/appcache/application_1653323265354_0006/container_1653323265354_0006_01_000001
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.memory.free=730MB
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.memory.max=859MB
2022-05-24 09:55:33,295 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Client environment:os.memory.total=859MB
2022-05-24 09:55:33,297 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper [] - Initiating client connection, connectString=master:2181,storm1:2181,storm2:2181 sessionTimeout=60000 watcher=org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState@21079a12
2022-05-24 09:55:33,300 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.common.X509Util [] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2022-05-24 09:55:33,303 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxnSocket [] - jute.maxbuffer value is 1048575 Bytes
2022-05-24 09:55:33,308 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - zookeeper.request.timeout value is 0. feature enabled=false
2022-05-24 09:55:33,311 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - SASL configuration failed. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.
javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-809470794203701266.conf'.
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.client.ZooKeeperSaslClient.<init>(ZooKeeperSaslClient.java:189) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:1161) [flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1210) [flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
2022-05-24 09:55:33,314 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Default schema
2022-05-24 09:55:33,315 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Opening socket connection to server storm1/192.168.0.3:2181.
2022-05-24 09:55:33,316 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Socket connection established, initiating session, client: /192.168.0.3:43878, server: storm1/192.168.0.3:2181
2022-05-24 09:55:33,319 ERROR org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState [] - Authentication failed
2022-05-24 09:55:33,324 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Session establishment complete on server storm1/192.168.0.3:2181, session id = 0x280f1be183a0003, negotiated timeout = 40000
2022-05-24 09:55:33,325 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: CONNECTED
2022-05-24 09:55:33,432 ERROR org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl [] - Ensure path threw exception
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /flink
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:106) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1734) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:351) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:109) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:618) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.runtime.util.ZooKeeperUtils.useNamespaceAndEnsurePath(ZooKeeperUtils.java:729) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.<init>(ZooKeeperMultipleComponentLeaderElectionHaServices.java:85) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:140) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:427) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:376) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227) ~[flink-dist-1.15.0.jar:1.15.0]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_111]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) [hadoop-common-2.9.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:109) [flink-dist-1.15.0.jar:1.15.0]
2022-05-24 09:55:33,433 ERROR org.apache.flink.runtime.util.ZooKeeperUtils                 [] - Unhandled error in curator framework, error message: Ensure path threw exception
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /flink
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:106) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1734) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:351) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:109) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:618) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.runtime.util.ZooKeeperUtils.useNamespaceAndEnsurePath(ZooKeeperUtils.java:729) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.<init>(ZooKeeperMultipleComponentLeaderElectionHaServices.java:85) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:140) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:427) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:376) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227) ~[flink-dist-1.15.0.jar:1.15.0]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_111]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) [hadoop-common-2.9.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:109) [flink-dist-1.15.0.jar:1.15.0]
2022-05-24 09:55:33,434 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Session 0x280f1be183a0003 for sever storm1/192.168.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EndOfStreamException: Unable to read additional data from server sessionid 0x280f1be183a0003, likely server has closed socket
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290) [flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
2022-05-24 09:55:33,435 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /flink
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:106) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1734) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:351) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl$1.call(NamespaceImpl.java:90) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.fixForNamespace(NamespaceImpl.java:83) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.NamespaceImpl.newNamespaceAwareEnsurePath(NamespaceImpl.java:109) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.newNamespaceAwareEnsurePath(CuratorFrameworkImpl.java:618) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.runtime.util.ZooKeeperUtils.useNamespaceAndEnsurePath(ZooKeeperUtils.java:729) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.zookeeper.ZooKeeperMultipleComponentLeaderElectionHaServices.<init>(ZooKeeperMultipleComponentLeaderElectionHaServices.java:85) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:96) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createHighAvailabilityServices(HighAvailabilityServicesUtils.java:140) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.createHaServices(ClusterEntrypoint.java:427) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:376) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:277) ~[flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:227) ~[flink-dist-1.15.0.jar:1.15.0]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_111]
    at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_111]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1886) [hadoop-common-2.9.0.jar:?]
    at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:224) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:711) [flink-dist-1.15.0.jar:1.15.0]
    at org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint.main(YarnJobClusterEntrypoint.java:109) [flink-dist-1.15.0.jar:1.15.0]
2022-05-24 09:55:33,439 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting YarnJobClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2022-05-24 09:55:33,535 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: SUSPENDED
2022-05-24 09:55:34,250 WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - SASL configuration failed. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.
javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-809470794203701266.conf'.
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.client.ZooKeeperSaslClient.<init>(ZooKeeperSaslClient.java:189) ~[flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:1161) [flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
    at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1210) [flink-shaded-zookeeper-3.6.3.jar:3.6.3-15.0]
2022-05-24 09:55:34,251 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Opening socket connection to server master/192.168.0.4:2181.
2022-05-24 09:55:34,251 ERROR org.apache.flink.shaded.curator5.org.apache.curator.ConnectionState [] - Authentication failed
2022-05-24 09:55:34,251 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Socket connection established, initiating session, client: /192.168.0.3:44618, server: master/192.168.0.4:2181
2022-05-24 09:55:34,253 INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Session establishment complete on server master/192.168.0.4:2181, session id = 0x280f1be183a0003, negotiated timeout = 40000
2022-05-24 09:55:34,253 INFO  org.apache.flink.shaded.curator5.org.apache.curator.framework.state.ConnectionStateManager [] - State change: RECONNECTED
{code}
When I upgrade ZooKeeper to 3.6.3 or 3.7.1, I can submit the job successfully","Flink: 1.15.0

zookeeper: 3.4.10/3.5.9(failed),3.6.3(success),3.7.1(success)

hadoop：2.9.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-24 04:28:23.0,,,,,,,,,,"0|z12mk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler should support operator fixed parallelism,FLINK-27748,13446473,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,long jiang,long jiang,24/May/22 01:29,02/Sep/22 09:16,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,,,,,"In the job topology, if the user specifies the concurrency of the operator, AdaptiveScheduler should support the operator's maximum parallelism equal to the user-specified parallelism during the scheduling process. And the minimum parallelism is equal to the number of slots available to the cluster. 
This is especially useful in certain scenarios,
For example, the parallelism of an operator that consumes Kafka is specified to be equal to the number of partitions. Or you want to control the write rate of the operator, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 02 09:16:26 UTC 2022,,,,,,,,,,"0|z12mf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 11:32;knaufk;IIRC the adaptive scheduler (or at least reactive mode) already takes the max parallelism for an operator into account. Could you use maxParallelism to specify an upper bound?

cc [~chesnay] ;;;","17/Aug/22 01:33;long jiang;[~knaufk]  Probably not, maxParallelism controls the total concurrency of the job, and we want to control the concurrency of a specific operator.;;;","02/Sep/22 09:16;knaufk;[~long jiang] Better late than never. You can also set the maxParallelism per Operator and that will be respect by reative mode.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator helm chart release the Chart.yaml file doesn't have an apache license header,FLINK-27747,13446472,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wangyang0918,wangyang0918,wangyang0918,24/May/22 01:22,25/May/22 02:43,04/Jun/24 20:51,25/May/22 02:43,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,,,,,,0,pull-request-available,,,,"When verifying the 1.0.0-rc1, [~gyfora] found that the Chart.yaml file doesn't have an apache license header.

It seems this is caused by {{helm package}} in the {{create_source_release.sh}}.

We also have this issue in the 0.1.0 release[1].

[1]. https://dist.apache.org/repos/dist/release/flink/flink-kubernetes-operator-0.1.0/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 02:43:54 UTC 2022,,,,,,,,,,"0|z12mew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 02:43;wangyang0918;Fixed via:

main: 710310b76d174526135e30ad236c32f74df45c78

release-1.0: 4a70fe364ec4a8a3c25efc49531cf09d50cf9f96;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink kubernetes operator docker image could not build with source release,FLINK-27746,13446469,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,nicholasjiang,wangyang0918,wangyang0918,24/May/22 01:17,25/May/22 02:42,04/Jun/24 20:51,25/May/22 02:42,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Could not build the Docker image from the source release, getting the
following error:


> [build 11/14] COPY .git ./.git:

------

failed to compute cache key: ""/.git"" not found: not found",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 02:42:47 UTC 2022,,,,,,,,,,"0|z12me8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 01:26;wangyang0918;cc [~nicholasjiang] would you like to have a look?

We should skip copying the .git directory when it does not exit.;;;","24/May/22 09:29;tison;[~wangyang0918] I think we can simply avoid COPY .git folder at all scenarios as we don't depend on those stuff.

BTW, basically, we don't assign others unless they ask for it.;;;","24/May/22 10:12;wangyang0918;I am afraid we could not simply remove the {{COPY}} of .git directory. We rely on it to get the commit id. Then it will be used in the logs.

{code:java}
[ INFO ]  Starting Flink Kubernetes Operator (Version: 1.0.0, Flink Version: 1.15.0, Rev:2417603, Date:2022-05-23T07:42:07+02:00)
{code}


This is introduced by [~nicholasjiang] and he pings me privately. That's why I assigned this ticket to him.;;;","24/May/22 10:24;tison;Thanks for sharing the background.

Then I think here we have two opinions:

1. Include .git folder in the released tarball.
2. Setup another approach to generate git properties before release and skip the related phase when building with release artifacts.

ZooKeeper uses similar maven plugin but unfortunately I find that if you build with released tarball the commit field is remain unresolved. It may not be a problem to show the info as ""${git.commit.time}"" barely but I'm not sure.

FYI, if we simply remove ""COPY .git"" the final properties are:

project.version=1.0.0
git.commit.id.abbrev=${git.commit.id.abbrev}
git.commit.time=${git.commit.time}

But to produce the docker image properly, we may still COPY .git when packaging in a git repo.;;;","24/May/22 10:42;nicholasjiang;[~tison], it's my mistake that I didn't comment on this ticket and thanks for [~wangyang0918] to explain. [~wangyang0918], shipping the .git is better solution at present and you need to update the current release process.;;;","24/May/22 10:56;wangyang0918;>> ZooKeeper uses similar maven plugin but unfortunately I find that if you build with released tarball the commit field is remain unresolved. It may not be a problem to show the info as ""${git.commit.time}"" barely but I'm not sure.

 

When building the image with source release, the UNKNOWN commit id in logs is reasonable to me.;;;","25/May/22 02:42;wangyang0918;Fixed via:

main: cf8f99b2c7cd61828ce94228d1b6b31c235d4cb8

release-1.0: d0482f91b6603bcdd28ac44130a8cd8bb553108e;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ClientUtilsTest.uploadAndSetUserArtifacts failed with NoClassDefFoundError,FLINK-27745,13446378,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,wangyang0918,hxbks2ks,hxbks2ks,23/May/22 12:33,01/Sep/22 11:18,04/Jun/24 20:51,01/Sep/22 11:18,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,0,stale-assigned,test-stability,,,"
{code:java}
2022-05-23T10:27:20.0131798Z May 23 10:27:20 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.729 s <<< FAILURE! - in org.apache.flink.runtime.client.ClientUtilsTest
2022-05-23T10:27:20.0133550Z May 23 10:27:20 [ERROR] org.apache.flink.runtime.client.ClientUtilsTest.uploadAndSetUserArtifacts  Time elapsed: 0.639 s  <<< ERROR!
2022-05-23T10:27:20.0134569Z May 23 10:27:20 org.apache.flink.util.FlinkException: Could not upload job files.
2022-05-23T10:27:20.0135587Z May 23 10:27:20 	at org.apache.flink.runtime.client.ClientUtils.uploadJobGraphFiles(ClientUtils.java:86)
2022-05-23T10:27:20.0136861Z May 23 10:27:20 	at org.apache.flink.runtime.client.ClientUtils.extractAndUploadJobGraphFiles(ClientUtils.java:62)
2022-05-23T10:27:20.0138163Z May 23 10:27:20 	at org.apache.flink.runtime.client.ClientUtilsTest.uploadAndSetUserArtifacts(ClientUtilsTest.java:137)
2022-05-23T10:27:20.0139618Z May 23 10:27:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-23T10:27:20.0140639Z May 23 10:27:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-23T10:27:20.0142022Z May 23 10:27:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-23T10:27:20.0144222Z May 23 10:27:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-23T10:27:20.0145368Z May 23 10:27:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-05-23T10:27:20.0146856Z May 23 10:27:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-05-23T10:27:20.0147934Z May 23 10:27:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-05-23T10:27:20.0148815Z May 23 10:27:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-05-23T10:27:20.0149537Z May 23 10:27:20 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-05-23T10:27:20.0150204Z May 23 10:27:20 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-05-23T10:27:20.0150848Z May 23 10:27:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:27:20.0151599Z May 23 10:27:20 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-05-23T10:27:20.0152293Z May 23 10:27:20 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-05-23T10:27:20.0153073Z May 23 10:27:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-05-23T10:27:20.0153876Z May 23 10:27:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-05-23T10:27:20.0154555Z May 23 10:27:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-05-23T10:27:20.0155189Z May 23 10:27:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-05-23T10:27:20.0155846Z May 23 10:27:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-05-23T10:27:20.0156708Z May 23 10:27:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-05-23T10:27:20.0157380Z May 23 10:27:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-05-23T10:27:20.0158056Z May 23 10:27:20 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-05-23T10:27:20.0158760Z May 23 10:27:20 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-05-23T10:27:20.0159493Z May 23 10:27:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-05-23T10:27:20.0160124Z May 23 10:27:20 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-23T10:27:20.0160740Z May 23 10:27:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:27:20.0161649Z May 23 10:27:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-23T10:27:20.0162267Z May 23 10:27:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-23T10:27:20.0162936Z May 23 10:27:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-23T10:27:20.0163607Z May 23 10:27:20 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-05-23T10:27:20.0164434Z May 23 10:27:20 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-05-23T10:27:20.0165171Z May 23 10:27:20 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-05-23T10:27:20.0165937Z May 23 10:27:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-05-23T10:27:20.0166789Z May 23 10:27:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-05-23T10:27:20.0167659Z May 23 10:27:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-05-23T10:27:20.0168723Z May 23 10:27:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-05-23T10:27:20.0169705Z May 23 10:27:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-05-23T10:27:20.0170495Z May 23 10:27:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-05-23T10:27:20.0171297Z May 23 10:27:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-05-23T10:27:20.0172093Z May 23 10:27:20 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-05-23T10:27:20.0173020Z May 23 10:27:20 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-05-23T10:27:20.0173937Z May 23 10:27:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-05-23T10:27:20.0174681Z May 23 10:27:20 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-05-23T10:27:20.0175480Z May 23 10:27:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-05-23T10:27:20.0176339Z May 23 10:27:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-05-23T10:27:20.0177183Z May 23 10:27:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-05-23T10:27:20.0178056Z May 23 10:27:20 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-05-23T10:27:20.0178773Z May 23 10:27:20 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-05-23T10:27:20.0179467Z May 23 10:27:20 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-05-23T10:27:20.0180300Z May 23 10:27:20 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-05-23T10:27:20.0182512Z May 23 10:27:20 Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.
2022-05-23T10:27:20.0183929Z May 23 10:27:20 	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:532)
2022-05-23T10:27:20.0184625Z May 23 10:27:20 	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
2022-05-23T10:27:20.0185269Z May 23 10:27:20 	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:274)
2022-05-23T10:27:20.0185973Z May 23 10:27:20 	at org.apache.flink.runtime.client.ClientUtils.uploadUserArtifacts(ClientUtils.java:150)
2022-05-23T10:27:20.0186751Z May 23 10:27:20 	at org.apache.flink.runtime.client.ClientUtils.uploadAndSetUserArtifacts(ClientUtils.java:139)
2022-05-23T10:27:20.0187507Z May 23 10:27:20 	at org.apache.flink.runtime.client.ClientUtils.uploadJobGraphFiles(ClientUtils.java:84)
2022-05-23T10:27:20.0188060Z May 23 10:27:20 	... 51 more
2022-05-23T10:27:20.0189130Z May 23 10:27:20 Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Cannot support file system for 'hdfs' via Hadoop, because Hadoop is not in the classpath, or some classes are missing from the classpath.
2022-05-23T10:27:20.0190067Z May 23 10:27:20 	at org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.create(HadoopFsFactory.java:189)
2022-05-23T10:27:20.0190799Z May 23 10:27:20 	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:528)
2022-05-23T10:27:20.0191401Z May 23 10:27:20 	... 56 more
2022-05-23T10:27:20.0192005Z May 23 10:27:20 Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration
2022-05-23T10:27:20.0193066Z May 23 10:27:20 	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:842)
2022-05-23T10:27:20.0194009Z May 23 10:27:20 	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:807)
2022-05-23T10:27:20.0194794Z May 23 10:27:20 	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:680)
2022-05-23T10:27:20.0195506Z May 23 10:27:20 	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:318)
2022-05-23T10:27:20.0196131Z May 23 10:27:20 	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:285)
2022-05-23T10:27:20.0196822Z May 23 10:27:20 	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:160)
2022-05-23T10:27:20.0197566Z May 23 10:27:20 	at org.apache.flink.runtime.fs.hdfs.HadoopFsFactory.create(HadoopFsFactory.java:168)
2022-05-23T10:27:20.0198100Z May 23 10:27:20 	... 57 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35959&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9769",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27791,FLINK-28379,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 01 11:17:29 UTC 2022,,,,,,,,,,"0|z12lu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 13:44;chesnay;https://dev.azure.com/chesnay/flink/_build/results?buildId=2652&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20

Similar instance in the SecurityUtilsTest.

This is one strange test failure...;;;","31/May/22 09:13;godfreyhe;cc [~wangyang0918];;;","31/May/22 09:21;wangyang0918;I will try to have a look in this week.;;;","01/Jun/22 12:37;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36230&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","06/Jun/22 11:42;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36309&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","08/Jun/22 12:57;chesnay;Wondering if this could be related to FLINK-27791.;;;","23/Jun/22 05:52;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37082&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8183;;;","04/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","01/Sep/22 11:17;chesnay;Fixed by FLINK-27791/FLINK-28379.;;;",,,,,,,,,,,,,,,,,,,,,,,
CheckPubSubEmulatorTest.testPull failed with AssertionError,FLINK-27744,13446366,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,hxbks2ks,hxbks2ks,hxbks2ks,23/May/22 11:58,29/Aug/22 10:39,04/Jun/24 20:51,29/Aug/22 10:39,1.16.0,,,,,,,,,,,,,,,,,,Connectors / Google Cloud PubSub,,,,,0,stale-assigned,test-stability,,,"
{code:java}
2022-05-23T10:08:12.4382583Z May 23 10:08:12 [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull  Time elapsed: 24.092 s  <<< FAILURE!
2022-05-23T10:08:12.4383262Z May 23 10:08:12 java.lang.AssertionError: expected:<1> but was:<0>
2022-05-23T10:08:12.4383939Z May 23 10:08:12 	at org.junit.Assert.fail(Assert.java:89)
2022-05-23T10:08:12.4384555Z May 23 10:08:12 	at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-23T10:08:12.4385199Z May 23 10:08:12 	at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-23T10:08:12.4385829Z May 23 10:08:12 	at org.junit.Assert.assertEquals(Assert.java:633)
2022-05-23T10:08:12.4386614Z May 23 10:08:12 	at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:78)
2022-05-23T10:08:12.4389064Z May 23 10:08:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-23T10:08:12.4389735Z May 23 10:08:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-23T10:08:12.4390436Z May 23 10:08:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-23T10:08:12.4391073Z May 23 10:08:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-23T10:08:12.4391708Z May 23 10:08:12 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-05-23T10:08:12.4392415Z May 23 10:08:12 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-05-23T10:08:12.4393116Z May 23 10:08:12 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-05-23T10:08:12.4393811Z May 23 10:08:12 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-05-23T10:08:12.4394491Z May 23 10:08:12 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-05-23T10:08:12.4395111Z May 23 10:08:12 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-05-23T10:08:12.4395718Z May 23 10:08:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:08:12.4396384Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-05-23T10:08:12.4397042Z May 23 10:08:12 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-05-23T10:08:12.4397697Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-05-23T10:08:12.4398392Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-05-23T10:08:12.4399020Z May 23 10:08:12 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-05-23T10:08:12.4399622Z May 23 10:08:12 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-05-23T10:08:12.4400236Z May 23 10:08:12 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-05-23T10:08:12.4400851Z May 23 10:08:12 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-05-23T10:08:12.4401466Z May 23 10:08:12 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-05-23T10:08:12.4402292Z May 23 10:08:12 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-05-23T10:08:12.4402950Z May 23 10:08:12 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-05-23T10:08:12.4403713Z May 23 10:08:12 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-05-23T10:08:12.4404416Z May 23 10:08:12 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-23T10:08:12.4405003Z May 23 10:08:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:08:12.4405602Z May 23 10:08:12 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-23T10:08:12.4406166Z May 23 10:08:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-23T10:08:12.4406792Z May 23 10:08:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-23T10:08:12.4407415Z May 23 10:08:12 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-05-23T10:08:12.4408134Z May 23 10:08:12 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-05-23T10:08:12.4408839Z May 23 10:08:12 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-05-23T10:08:12.4409588Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-05-23T10:08:12.4410406Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-05-23T10:08:12.4411234Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-05-23T10:08:12.4412084Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-05-23T10:08:12.4412930Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-05-23T10:08:12.4413733Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-05-23T10:08:12.4414435Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-05-23T10:08:12.4415214Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-05-23T10:08:12.4416039Z May 23 10:08:12 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-05-23T10:08:12.4416826Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-05-23T10:08:12.4417633Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-05-23T10:08:12.4418426Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-05-23T10:08:12.4419181Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-05-23T10:08:12.4419878Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-05-23T10:08:12.4420538Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-05-23T10:08:12.4421196Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-05-23T10:08:12.4421667Z May 23 10:08:12 
2022-05-23T10:08:12.8012357Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8014745Z May 23 10:08:12 [INFO] Results:
2022-05-23T10:08:12.8015399Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8015938Z May 23 10:08:12 [ERROR] Failures: 
2022-05-23T10:08:12.8016703Z May 23 10:08:12 [ERROR]   CheckPubSubEmulatorTest.testPull:78 expected:<1> but was:<0>
2022-05-23T10:08:12.8017540Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8018140Z May 23 10:08:12 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35954&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 05 10:39:15 UTC 2022,,,,,,,,,,"0|z12lrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support function params when create UDTF in flink sql,FLINK-27743,13446341,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wukong,wukong,23/May/22 10:09,23/May/22 10:09,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"In our sql sence we have data samples like below:

a|b|\{""name"":""jark"",""age"":""23""}|c

If we want to extra the inner json with Type info we have to 

declare the type info in functions and can not reuse for others

 

May be we should we do create function like this:
CREATE FUNCTION fromJson AS 'test.function.JsonTableFunc' with ('key' = 'value')
and get type info from some key ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-23 10:09:51.0,,,,,,,,,,"0|z12lls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix compatibility issues between Flink ML Stages,FLINK-27742,13446332,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,23/May/22 09:06,24/Jun/22 07:49,04/Jun/24 20:51,02/Jun/22 01:25,ml-2.0.0,,,,,,,,,,,,,,ml-2.1.0,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"It is discovered that StringIndexer and LogisticRegression in Flink ML cannot be connected in a pipeline. The reason is that the output label column of StringIndexer is integer, while LogisticRegression can only accept input data whose labels are doubles.

In order to make Flink ML stages compatible with each other, the following changes need to be made.
- For stages who can only accept double-typed inputs, update their implementation to accept any numerical type.
- For stages that generates labels as integers, make them return labels as doubles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-23 09:06:03.0,,,,,,,,,,"0|z12ljs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE when use dense_rank() and rank() in over aggregation,FLINK-27741,13446326,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,chenzihao,chenzihao,chenzihao,23/May/22 08:44,15/May/24 05:38,04/Jun/24 20:51,14/May/24 21:32,1.16.0,,,,,,,,,,,,,,1.18.2,1.19.1,1.20.0,,Table SQL / Planner,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,"There has an 'NullPointException' when use RANK() and DENSE_RANK() in over window.
{code:java}
@Test
  def testDenseRankOnOver(): Unit = {
    val t = failingDataSource(TestData.tupleData5)
      .toTable(tEnv, 'a, 'b, 'c, 'd, 'e, 'proctime.proctime)
    tEnv.registerTable(""MyTable"", t)
    val sqlQuery = ""SELECT a, DENSE_RANK() OVER (PARTITION BY a ORDER BY proctime) FROM MyTable""

    val sink = new TestingAppendSink
    tEnv.sqlQuery(sqlQuery).toAppendStream[Row].addSink(sink)
    env.execute()
  }
{code}
{code:java}
@Test
  def testRankOnOver(): Unit = {
    val t = failingDataSource(TestData.tupleData5)
      .toTable(tEnv, 'a, 'b, 'c, 'd, 'e, 'proctime.proctime)
    tEnv.registerTable(""MyTable"", t)
    val sqlQuery = ""SELECT a, RANK() OVER (PARTITION BY a ORDER BY proctime) FROM MyTable""

    val sink = new TestingAppendSink
    tEnv.sqlQuery(sqlQuery).toAppendStream[Row].addSink(sink)
    env.execute()
  }
{code}
Exception Info:
{code:java}
java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofInt$.length$extension(ArrayOps.scala:248)
	at scala.collection.mutable.ArrayOps$ofInt.length(ArrayOps.scala:248)
	at scala.collection.SeqLike.size(SeqLike.scala:104)
	at scala.collection.SeqLike.size$(SeqLike.scala:104)
	at scala.collection.mutable.ArrayOps$ofInt.size(ArrayOps.scala:242)
	at scala.collection.IndexedSeqLike.sizeHintIfCheap(IndexedSeqLike.scala:95)
	at scala.collection.IndexedSeqLike.sizeHintIfCheap$(IndexedSeqLike.scala:95)
	at scala.collection.mutable.ArrayOps$ofInt.sizeHintIfCheap(ArrayOps.scala:242)
	at scala.collection.mutable.Builder.sizeHint(Builder.scala:77)
	at scala.collection.mutable.Builder.sizeHint$(Builder.scala:76)
	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:21)
	at scala.collection.TraversableLike.builder$1(TraversableLike.scala:229)
	at scala.collection.TraversableLike.map(TraversableLike.scala:232)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.ArrayOps$ofInt.map(ArrayOps.scala:242)
	at org.apache.flink.table.planner.plan.utils.AggFunctionFactory.createDenseRankAggFunction(AggFunctionFactory.scala:454)
	at org.apache.flink.table.planner.plan.utils.AggFunctionFactory.createAggFunction(AggFunctionFactory.scala:94)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.$anonfun$transformToAggregateInfoList$1(AggregateUtil.scala:445)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.transformToAggregateInfoList(AggregateUtil.scala:435)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.transformToStreamAggregateInfoList(AggregateUtil.scala:381)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.transformToStreamAggregateInfoList(AggregateUtil.scala:361)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil.transformToStreamAggregateInfoList(AggregateUtil.scala)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.createUnboundedOverProcessFunction(StreamExecOverAggregate.java:279)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecOverAggregate.translateToPlanInternal(StreamExecOverAggregate.java:198)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.translateToTransformation(CommonExecLegacySink.java:185)
	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLegacySink.translateToPlanInternal(CommonExecLegacySink.java:154)
	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:79)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:78)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181)
	at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:223)
	at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:220)
	at org.apache.flink.table.api.bridge.scala.TableConversions.toAppendStream(TableConversions.scala:199)
	at org.apache.flink.table.planner.runtime.stream.sql.OverAggregateITCase.testDenseRankOnOver(OverAggregateITCase.scala:170)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 14 21:32:51 UTC 2024,,,,,,,,,,"0|z12lig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 08:57;chenzihao;Hi, [~jark], [~jingzhang]. What do you think of this? I have tried to fix it and it works.;;;","27/May/22 06:31;chenzihao;Hi, [~jark]. Can you help to confirm this? Thanks very much.;;;","22/Jun/22 07:45;chenzihao;Hi, [~godfreyhe] . Can you help to confirm this ticket? What do you think about this? Thanks very much.;;;","21/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","29/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:34;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","21/Aug/23 09:51;xu_shuai_;Hi [~chenzihao] , it appears that your pull request has not been approved. Would you like to continue working on it? If not, I would be happy to fix it.;;;","14/May/24 21:32;Sergey Nuyanzin;Merged to master as [40fb49dd17b3e1b6c5aa0249514273730ebe9226|https://github.com/apache/flink/commit/40fb49dd17b3e1b6c5aa0249514273730ebe9226]
1.19: [190522c2c051e0ec05213be71fb7a59a517353b1|https://github.com/apache/flink/commit/190522c2c051e0ec05213be71fb7a59a517353b1]
1.18: [1e1a7f16b6f272334d9f9a1053b657148151a789|https://github.com/apache/flink/commit/1e1a7f16b6f272334d9f9a1053b657148151a789];;;",,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-test-utils-junit,FLINK-27740,13446318,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,23/May/22 07:58,04/Nov/22 13:22,04/Jun/24 20:51,25/May/22 14:50,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 04 13:22:04 UTC 2022,,,,,,,,,,"0|z12lgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 14:50;chesnay;master: e63111189b924e0dffe6b53ecd75470e834255fd;;;","04/Nov/22 13:22;mapohl;Reverted juni5 migration for {{RetryOnFailureTest}} and {{RetryOnExceptionTest}} as those are actually intended for JUnit4's {{RetryRule}} implementation.

master: 0e27854c27707d440977efab90d59f5a8da2ae1e
1.16: e76e7819a826bb0a840142da8a8751c8a0a2a154;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add benchmark module for table store,FLINK-27739,13446303,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,23/May/22 07:13,25/Aug/22 04:32,04/Jun/24 20:51,19/Jul/22 09:06,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,stale-assigned,,,Currently there is no benchmark for table store to measure its performance. We need such module.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 09:06:57 UTC 2022,,,,,,,,,,"0|z12ldc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","19/Jul/22 09:06;lzljs3620320;master: a22c22d5394cacadca63a5d9eff996ae7badb69b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
instance KafkaSink support config topic properties,FLINK-27738,13446293,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,LCER,LCER,23/May/22 06:23,24/May/22 11:24,04/Jun/24 20:51,24/May/22 11:23,1.15.0,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,,,,,"I use KafkaSink to config Kafka information as following:

*KafkaSink.<String>builder()*
 *.setBootstrapServers(brokers)*
 *.setRecordSerializer(KafkaRecordSerializationSchema.builder()*
 *.setTopicSelector(topicSelector)*
 *.setValueSerializationSchema(new SimpleStringSchema())*
 *.build()*
 *)*
 *.setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE)*
 *.setKafkaProducerConfig(properties)*
 *.build();*

*----------------*

*I can't find any method to support config topic properties*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 11:23:10 UTC 2022,,,,,,,,,,"0|z12lb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 06:48;martijnvisser;[~LCER] Given the documentation on additional properties at https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/#additional-properties can you elaborate on what you find missing?;;;","23/May/22 16:14;LCER;[~martijnvisser] ，thank you for your reply , In your provide url,Kafka Sink section  hasn't introduce how to add properties ; I find class  org.apache.flink.connector.kafka.sink.KafkaSinkBuilder only declare setKafkaProducerConfig method , this method support properties which declare in org.apache.kafka.clients.producer.ProducerConfig class; but I want  to customer topic properties  which declare in org.apache.kafka.common.config.TopicConfig, so I didn't konw how to realize my function.

 

 

 ;;;","23/May/22 16:36;martijnvisser;[~LCER] That class is usually used with the AdminClient; what is the use case that you would like to access the TopicConfig while working from a Flink KafkaSink perspective? ;;;","24/May/22 01:48;LCER;[~martijnvisser] ，I use flink mysql cdc  collect  data from mysql to kafka, but  some table  row data size is large than kafka default config value , in this case ,throw an exception :
org.apache.kafka.common.errors.RecordTooLargeException: The message is XXX bytes when serialized which is larger than the maximum request size you have configured with  the max.request.size configuration.
so ,I want use  KafkaSinkBuilder to config Topic of  {{max.message.bytes}} proeperty to resoleve the problem;
 ;;;","24/May/22 02:38;fsk119;Do you mean you want to use the KafkaSink to modify the config of the Kafka topic? If so, I think it's not a good idea. The connector API just INSERT/MODIFY/DELTE the content in the topic, which will not influence the topic itself. In the Flink world, Catalog is used to manage the metadata including alter the topic config. 

If you want to set the KafkaProducer properties, I think the setProperties[1] is enough for you? 

[1] https://github.com/apache/flink/blob/b4bb9c8bffe1e37ad6912348d8b3bef89af42286/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaSinkBuilder.java#L122;;;","24/May/22 03:04;LCER;[~fsk119] ,thank you for your reply  , if the topic name is not exists when use KafkaSink to send message, it will automatic creation, so i want to modify the topic properties; Now I use pre generated topic to resove this problem , but I don't think is a good idea;

 ;;;","24/May/22 07:23;martijnvisser;[~LCER] We should not let KafkaSink modify topic properties. They are two separate operations, one is about creation and management of topic properties, one is about writing data to that topic. KafkaSink's only responsibility is about writing data to that topic, we should not use it for other operations. It's also not KafkaSink that creates a topic, it's the operation involved in the management of topic properties that allows for auto-creation of a topic. ;;;","24/May/22 07:33;LCER;[~martijnvisser] ，Thank you for your clarification ，can you give me some suggestion for my case?

 ;;;","24/May/22 11:20;fsk119;[~LCER] Sorry for the late response. I think you can refer to the tutorial[1]. It tells us you can set topic config by
{code:java}
./kafka-topics.sh --bootstrap-server localhost:9092 --create --topic longMessage --partitions 1 \
--replication-factor 1 --config max.message.bytes=20971520  {code}
[1]https://www.baeldung.com/java-kafka-send-large-message;;;","24/May/22 11:23;fsk119;I close the issue. If you still have problems, welcome to ping us.;;;",,,,,,,,,,,,,,,,,,,,,,
Remove legacy support for unfenced executor in FencedRpcEndpoint,FLINK-27737,13446286,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,23/May/22 05:04,13/Jun/22 07:57,04/Jun/24 20:51,13/Jun/22 07:57,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Coordination,,,,,0,pull-request-available,,,,"There are 2 main thread executors in {{{}FencedRpcEndpoint{}}}, the fenced- and unfenced-. The fenced executor was available only during a specific leader session, while the unfenced executor was used for actions out of a leader session (e.g., during the initialization). This is no longer needed because currently for all the fenced endpoints ({{{}Dispatcher{}}}, {{JobMaster}} and {{{}ResourceManager{}}}), the leader election finishes before the endpoints are initiated.

In this ticket we propose to remove the legacy codes for supporting unfenced executor and updating fencing tokens, as well as to simplify the type hierarchy of {{{}FencedRpcEndpoint{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 07:57:36 UTC 2022,,,,,,,,,,"0|z12l9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 05:32;xtsong;Thanks for pointing this out, [~aitozi]. I think this would be a nice clean-up.

Would you like to work on this?;;;","23/May/22 05:56;aitozi;Yes, I'd like to work on this.;;;","23/May/22 06:24;xtsong;Thanks. You are assigned. Please go ahead.;;;","13/Jun/22 07:57;xtsong;master: 41ac1ba13679121f1ddf14b26a36f4f4a3cc73e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar sink catch watermark error,FLINK-27736,13446279,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,syhily,ana4,ana4,23/May/22 03:23,09/Sep/22 04:06,04/Jun/24 20:51,09/Sep/22 04:06,1.15.0,,,,,,,,,,,,,,,,,,API / DataStream,Connectors / Pulsar,,,,0,,,,,"The following is my demo code.
{code:java}
public class WatermarkDemo {

    private final static String SERVICE_URL = ""pulsar://localhost:6650"";
    private final static String ADMIN_URL = ""http://localhost:8080"";

    public static void main(String[] args) throws Exception {

        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        PulsarSource<String> source = PulsarSource.builder()
                .setServiceUrl(SERVICE_URL)
                .setAdminUrl(ADMIN_URL)
                .setStartCursor(StartCursor.earliest())
                .setTopics(""ada"")
                .setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(new SimpleStringSchema()))
                .setSubscriptionName(""my-subscription"")
                .setSubscriptionType(SubscriptionType.Exclusive)
                .build();

        PulsarSink<String> sink = PulsarSink.builder()
                .setServiceUrl(SERVICE_URL)
                .setAdminUrl(ADMIN_URL)
                .setTopics(""beta"")
                .setSerializationSchema(PulsarSerializationSchema.flinkSchema(new SimpleStringSchema()))
                .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
                .build();


        DataStream stream = env.fromSource(source, WatermarkStrategy.forMonotonousTimestamps(), ""Pulsar Source"");
        stream.sinkTo(sink);

        env.execute();

    }
} {code}
It will throw the following error.
{code:java}
Caused by: java.lang.IllegalArgumentException: Invalid timestamp : '0'
    at org.apache.pulsar.shade.com.google.common.base.Preconditions.checkArgument(Preconditions.java:203)
    at org.apache.pulsar.client.impl.TypedMessageBuilderImpl.eventTime(TypedMessageBuilderImpl.java:204)
    at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.createMessageBuilder(PulsarWriter.java:216)
    at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.write(PulsarWriter.java:141)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:158)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
    at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:313)
    at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110)
    at org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.emitRecord(PulsarRecordEmitter.java:41)
    at org.apache.flink.connector.pulsar.source.reader.emitter.PulsarRecordEmitter.emitRecord(PulsarRecordEmitter.java:33)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:143)
    at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:106)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:385)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748) {code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29207,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 09 04:04:26 UTC 2022,,,,,,,,,,"0|z12l80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 06:49;martijnvisser;[~ana4] I've downgraded this in accordance with https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process;;;","09/Sep/22 04:04;syhily;[~tison] Since this is a duplicated issue. Can you help me close this issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update testcontainers dependency to v1.17.2,FLINK-27735,13446270,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,23/May/22 01:45,13/Sep/22 09:00,04/Jun/24 20:51,23/May/22 07:52,,,,,,,,,,,,,,,1.16.0,elasticsearch-3.0.0,,,Test Infrastructure,Tests,,,,0,pull-request-available,,,,"testcontainers 1.17.2 is released

Among others there is a fix for connection leak in jdbc, performance
Main benefits (based on [https://github.com/testcontainers/testcontainers-java/releases/tag/1.17.2)]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 07:52:50 UTC 2022,,,,,,,,,,"0|z12l60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 07:52;martijnvisser;Fixed in master: f026f396305858c8be3a5aeacd1aa9dd3df02c87
elasticsearch-main: cc4903997e350abc2610368e5413ac1b2f668234;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not showing checkpoint interval properly  in WebUI when checkpoint is disabled,FLINK-27734,13446247,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,22/May/22 15:44,20/Jun/22 07:26,04/Jun/24 20:51,10/Jun/22 09:43,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"Not showing checkpoint interval properly  in WebUI when checkpoint is disabled

!image-2022-05-22-23-42-46-365.png|width=1019,height=362!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/22 15:42;Feifan Wang;image-2022-05-22-23-42-46-365.png;https://issues.apache.org/jira/secure/attachment/13044001/image-2022-05-22-23-42-46-365.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 07:26:31 UTC 2022,,,,,,,,,,"0|z12l0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 10:18;Feifan Wang;Hi [~wolli] , I have opened a pr to fix this, can you take a look ? ;;;","26/May/22 02:54;Feifan Wang;Hi [~junhany] ， can you help me confirm this problem ?;;;","27/May/22 09:07;Zsigner;Hi [~Feifan Wang] 

     I took a look at the pr you submitted, why did you change '0x7ffffffffffffffff' to 9223372036854776000, 

     I tested it and it should be 9223372036854775807, and then the page will display 'Periodic checkpoints disabled', 0x7ffffffffffffffff This seems to be no problem

 ;;;","27/May/22 15:15;Zsigner;Hi [~Feifan Wang] 
So I think this value should not be changed, it may be caused by other reasons;;;","27/May/22 15:23;Feifan Wang;Thanks for reply [~Zsigner] , I want just remove single quotes around 0x7fffffffffffffff firstly , but I find the lint-staged will change ""0x7fffffffffffffff"" to  ""0; x7fffffffffffffff"". Is there some advice suggestion for deal with it better ?;;;","27/May/22 15:32;Feifan Wang;[~Zsigner] , I think the problem is strict equals operator ( === ) require same type for return true.;;;","27/May/22 16:44;Zsigner;Hi [~Feifan Wang] 

   I also found that it may be due to an additional = sign. I am going to test it and submit a PR to deal with this issue. Since you think so, don't forget that 1.15 and 1.16 versions need to submit two requests. Thank you;;;","28/May/22 15:17;Feifan Wang;Hi [~Zsigner] , I tried changing ""==="" to ""=="", but there is a error : 

{code:java}
503:24  error  Expected `===` but received `==`  @angular-eslint/template/eqeqeq
506:24  error  Expected `!==` but received `!=`  @angular-eslint/template/eqeqeq
{code}

Should we modify configuration of eslint ?;;;","28/May/22 17:05;Feifan Wang;Hi [~Zsigner] , I updated the pr with modify eslint configuration , if think that's ok, I will submit another pr for flink-1.15.;;;","09/Jun/22 02:25;junhan;Hi [~Feifan Wang] . Thank you for fixing this. I left some comments in the PR. ;;;","10/Jun/22 09:43;yunta;merged in master: 18c13fe4b6788e3ec8af95551ebe3e598b32df61;;;","10/Jun/22 10:15;Feifan Wang;Hi [~yunta] , need we merge this change to release-1.15 ?;;;","15/Jun/22 05:32;yunta;[~Feifan Wang] I think it deserves to merge to release-1.15. However, the related code has changed, you can submit a PR targets for release-1.15 branch, then we can make it merged.;;;","17/Jun/22 03:11;Feifan Wang;Hi [~yunta] , I hive submitted a [PR|https://github.com/apache/flink/pull/19999] for release-1.15.;;;","20/Jun/22 07:26;yunta;merged in release-1.15: efee7405c3acf54bf2cc79b676cb881ac5f2c362;;;",,,,,,,,,,,,,,,,,
Rework on_timer output behind watermark bug fix,FLINK-27733,13446245,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,22/May/22 15:13,24/May/22 02:28,04/Jun/24 20:51,24/May/22 02:28,1.14.4,1.15.0,,,,,,,,,,,,,1.14.5,1.15.1,1.16.0,,API / Python,,,,,0,pull-request-available,,,,"FLINK-27676 can be simplified by just checking isBundleFinished() before emitting watermark in AbstractPythonFunctionOperator, and this fix FLINK-27676 in python group window aggregate too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 02:28:57 UTC 2022,,,,,,,,,,"0|z12l0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 02:28;hxbks2ks;merged into master via a0ef9eb46ad3896d6d87595dbe364f69d583794c
merged into release-1.15 via f413c40c8ab8145d3bdea8dbc6372961a598be37
merged into release-1.14 via 945c15341b93a9bfadc7b6ce239a96c2b7baf592;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-examples-table,FLINK-27732,13446225,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/May/22 12:23,23/May/22 09:50,04/Jun/24 20:51,23/May/22 09:50,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 09:50:59 UTC 2022,,,,,,,,,,"0|z12kw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 09:50;chesnay;master: 0ad2fa90df5357e80bf385d3e846a72cb65291fa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Hugo Modules integration,FLINK-27731,13446203,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,xtsong,xtsong,22/May/22 06:21,19/Jan/23 10:27,04/Jun/24 20:51,17/Jan/23 08:34,1.16.0,,,,,,,,,,,,,,1.15.4,1.16.1,1.17.0,,Documentation,,,,,0,pull-request-available,stale-assigned,,,"Flink provides 2 ways for building the documentation: 1) using a Hugo docker image, and 2) using a local Hugo installation.

Currently, 1) is broken due to the `setup_docs.sh` script requires a local Hugo installation.

This was introduced in FLINK-27394.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27394,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jan 14 13:20:47 UTC 2023,,,,,,,,,,"0|z12kr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/22 06:23;xtsong;cc [~martijnvisser] ;;;","23/May/22 06:38;martijnvisser;[~xtsong] I don't think a local Hugo installation is required, but Go is required as documented at https://github.com/apache/flink/blob/master/docs/README.md?plain=1#L13

Edit: I stand corrected, this is not working as it's supposed to. Let me fix that. ;;;","05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","21/Dec/22 12:49;martijnvisser;We've recently decided to integrate documentation with git clone instead of Hugo modules, so I'll use this ticket to remove all Hugo module code;;;","21/Dec/22 15:58;xtsong;bq. We've recently decided to integrate documentation with git clone instead of Hugo modules

[~martijnvisser], could you point me to where the decision is made? I guess I overlooked something and would like to learn more about that.;;;","21/Dec/22 18:56;martijnvisser;[~xtsong] See FLINK-29957 and https://github.com/apache/flink/pull/21271

Main reason to go for this approach is that it simplifies the setup a lot, by not requiring Go / Hugo to be installed.;;;","22/Dec/22 01:46;xtsong;Thanks for the pointer and explanation. Sounds good to me.;;;","14/Jan/23 13:20;martijnvisser;Fixed in 
master: 439fdbd093d9b2ea09481cf6a05559a826919c38
release-1.16: 56a50453cfa0d9a564c0baffb2ffff5ccda028f0
release-1.15: 0261b752cd02e731f0b3df530b7f8ab5b96fec8f;;;",,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector document code sink has an error,FLINK-27730,13446199,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,i-liu,i-liu,22/May/22 01:55,23/May/22 07:05,04/Jun/24 20:51,23/May/22 06:59,1.14.4,,,,,,,,,,,,,,1.14.4,,,,Documentation,,,,,0,,,,,"Kafka Sink document sample code API call error.

[kafka sink|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/datastream/kafka/] 
{code:java}
KafkaSink<String> sink = KafkaSink.<String>builder()
        .setBootstrapServers(brokers)
        .setRecordSerializer(KafkaRecordSerializationSchema.builder()
            .setTopic(""topic-name"")
            .setValueSerializationSchema(new SimpleStringSchema())
            .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
            .build()
        )
        .build(); {code}
+setDeliveryGuarantee+ is the method of +KafkaSink+ not the method of {+}KafkaRecordSerializationSchema{+}, as follows:

 
{code:java}
KafkaSink<String> sink = KafkaSink.<String>builder()
        .setBootstrapServers(brokers)
        .setRecordSerializer(KafkaRecordSerializationSchema.builder()
            .setTopic(""topic-name"")
            .setValueSerializationSchema(new SimpleStringSchema())
            .build()
        )
      .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
      .build(); {code}
 

 ",Flink 1.14.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/22 01:55;i-liu;kafka-sink.png;https://issues.apache.org/jira/secure/attachment/13043992/kafka-sink.png","22/May/22 12:20;i-liu;kafka_sink.png;https://issues.apache.org/jira/secure/attachment/13043997/kafka_sink.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 07:05:08 UTC 2022,,,,,,,,,,"0|z12kq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/22 11:25;Wencong Liu;Hello [~i-liu] , please show the relevant documentation link, and what is the specific error?;;;","22/May/22 12:20;i-liu;[~Wencong Liu] [kafka sink|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/datastream/kafka/]

Error is as follows:

!kafka_sink.png!;;;","23/May/22 06:57;martijnvisser;This has been previously reported and fixed already, it probably just needs a backport;;;","23/May/22 07:05;i-liu;[~martijnvisser] OK;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support constructing StartCursor and StopCursor from MessageId ,FLINK-27729,13446160,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pemide,dianfu,dianfu,21/May/22 10:06,09/Jun/22 05:39,04/Jun/24 20:51,09/Jun/22 05:39,,,,,,,,,,,,,,,1.16.0,,,,API / Python,Connectors / Pulsar,,,,0,pull-request-available,,,,"Currently, StartCursor.fromMessageId and StopCursor.fromMessageId are still not supported in Python pulsar connectors. I think we could leverage the [pulsar Python library|https://pulsar.apache.org/api/python/#pulsar.MessageId] to implement these methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 09 05:39:14 UTC 2022,,,,,,,,,,"0|z12khk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 10:07;dianfu;[~ana4] Would you like to take this JIRA?;;;","07/Jun/22 08:31;pemide;[~dianfu] Could I take this ticket?;;;","07/Jun/22 08:35;dianfu;[~pemide] Sure. Have assigned it to you~;;;","09/Jun/22 05:39;dianfu;Merged to master via 8b35b65e2d51aa5d166d4395ecbb5cfa21ed55e7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
dockerFile build results in five vulnerabilities,FLINK-27728,13446135,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jbusche,jbusche,jbusche,21/May/22 02:02,22/May/22 23:07,04/Jun/24 20:51,22/May/22 23:07,kubernetes-operator-0.1.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"A Twistlock security scan of the default flink-kubernetes-operator currently shows five fixable vulnerabilities.  One [~wangyang0918] and I are trying to fix in [FLINK-27654|https://issues.apache.org/jira/browse/FLINK-27654].

The other four are easily addressable if we update the underlying OS.  I'll propose a PR for this later this evening.

The four vulnerabilities are: 
1.  packageName: gzip

severity: Low

cvss: 0

riskFactors: Has fix,Recent vulnerability

CVE Link:  [https://security-tracker.debian.org/tracker/CVE-2022-1271] 

Description: DOCUMENTATION: No description is available for this CVE.              STATEMENT: This bug was introduced in gzip-1.3.10 and is relatively hard to exploit.  Red Hat Enterprise Linux 6 was affected but Out of Support Cycle because gzip was not listed in Red Hat Enterprise Linux 6 ELS Inclusion List. [https://access.redhat.com/articles/4997301]             MITIGATION: Red Hat has investigated whether possible mitigation exists for this issue, and has not been able to identify a practical example. Please update the affected package as soon as possible.

2.  packageName: openssl

severity: Critical

cvss: 9.8

riskFactors: Attack complexity: low,Attack vector: network,Critical severity,Has fix,Recent vulnerability

CVE Link: [https://security-tracker.debian.org/tracker/CVE-2022-1292] 

Description: 

The c_rehash script does not properly sanitise shell metacharacters to prevent command injection. This script is distributed by some operating systems in a manner where it is automatically executed. On such operating systems, an attacker could execute arbitrary commands with the privileges of the script. Use of the c_rehash script is considered obsolete and should be replaced by the OpenSSL rehash command line tool. Fixed in OpenSSL 3.0.3 (Affected 3.0.0,3.0.1,3.0.2). Fixed in OpenSSL 1.1.1o (Affected 1.1.1-1.1.1n). Fixed in OpenSSL 1.0.2ze (Affected 1.0.2-1.0.2zd).

3.  packageName: zlib

severity: High

cvss: 7.5

riskFactors: Attack complexity: low,Attack vector: network,Has fix,High severity

CVE Link: [https://security-tracker.debian.org/tracker/CVE-2018-25032] 

Description: zlib before 1.2.12 allows memory corruption when deflating (i.e., when compressing) if the input has many distant matches.

4.  packageName: openldap

severity: Critical

cvss: 9.8

riskFactors: Attack complexity: low,Attack vector: network,Critical severity,Has fix,Recent vulnerability

CVE Link: [https://security-tracker.debian.org/tracker/CVE-2022-29155] 

Description: In OpenLDAP 2.x before 2.5.12 and 2.6.x before 2.6.2, a SQL injection vulnerability exists in the experimental back-sql backend to slapd, via a SQL statement within an LDAP query. This can occur during an LDAP search operation when the search filter is processed, due to a lack of proper escaping.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 22 23:07:00 UTC 2022,,,,,,,,,,"0|z12kc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 02:16;jbusche;Created PR [https://github.com/apache/flink-kubernetes-operator/pull/233] to address the four items.;;;","22/May/22 23:07;mbalassi;Fixed via 65ea03c744347448f2592877cc98d85c6ea36ef4 in release-1.0 and 503673259213fdf78279bccec6c3d1edddabec0e in main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate TypeSerializerUpgradeTestBase to Junit5,FLINK-27727,13446129,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,21/May/22 00:03,10/Oct/22 14:58,04/Jun/24 20:51,10/Oct/22 14:58,,,,,,,,,,,,,,,1.17.0,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29485,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 10 14:58:56 UTC 2022,,,,,,,,,,"0|z12kao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 14:58;mapohl;master: 5be0ada42b153c0518a7c46ff16c5a21825d8bd9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shade thrift and fb303 in hive connector,FLINK-27726,13446090,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,foxss,foxss,20/May/22 16:28,19/Aug/23 22:34,04/Jun/24 20:51,,1.11.6,1.12.3,1.12.4,1.12.5,1.12.7,1.13.1,1.13.2,1.13.3,1.13.5,1.13.6,1.14.2,1.14.3,1.14.4,1.15.0,,,,,Connectors / Hive,,,,,0,auto-deprioritized-minor,pull-request-available,,,"Hive connector introduced fb303 and thrift version to connect to specific hive meta store version. If user code also pull specific thrift version along with fb303 that is not same as hive connector introduced, user code will not able to connect to hive meta store.

 

This fix has been verified in production environment as part of support thrift encoded FlinkSQL for more than 6 months.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 22:34:57 UTC 2023,,,,,,,,,,"0|z12k20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:34;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create tests for TypeInformation and TypeSerializer classes in table,FLINK-27725,13446056,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,alexanderpreuss,alexanderpreuss,20/May/22 13:22,19/Aug/23 22:34,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,auto-deprioritized-major,auto-deprioritized-minor,pull-request-available,,"During the implementation of FLINK-27527 we had to add `flink-table-planner` as a test dependency to `flink-tests`.

This created test failures for the reflection tests checking test coverage for `TypeInformation` and `TypeSerializer` classes as shown here:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35851&view=logs&j=56de72e1-1902-5ae5-06bd-77ee907eed59&t=237d25ca-be06-5918-2b0a-41d0694dace8&l=6738]

To mitigate the issue and unblock FLINK-27527 we extended the whitelist as a temporary solution but there should be a better solution in place. Some of the classes are deprecated but others are not and should probably have tests created to improve the coverage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 22:34:57 UTC 2023,,,,,,,,,,"0|z12jug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 13:23;alexanderpreuss;CC: [~twalthr] ;;;","23/May/22 08:22;twalthr;[~alexanderpreuss] why do you need a dependency on `flink-table-planner`? Which classes are required? So far we were always able to keep the dependencies to a minimum. Also, the `flink-table-planner` module will pull Scala into `flink-tests` which is not a good idea.;;;","23/May/22 08:26;alexanderpreuss;[~twalthr] we need the `ExecutorFactory` to be present to run IT cases for DynamicTableFactory. As far as we could see the ExecutorFactory is only present in flink-table-planner. Until now all connectors have to depend on the planner because of this. Is there already an alternative that we are not aware of?;;;","23/May/22 08:28;fpaul;[~twalthr] can you take a look at [https://github.com/apache/flink/pull/19655] ? We are currently working on a file-based upsert sink that can be used for testing purposes to allow SQL upsert e2e test to work without any external system i.e. elasticsearch.;;;","23/May/22 08:54;twalthr;[~alexanderpreuss] You can depend on the `flink-table-planner-loader` for running ITCases. A dependency on `flink-table-planner` is only required when you need to import a concrete class that is located in the `org.apache.flink.table.planner` package in your code. Please take a look at the testing section here:
https://github.com/apache/flink/blob/master/flink-table/README.md#testing;;;","23/May/22 09:30;alexanderpreuss;[~twalthr] I just tried exchanging the planner for `flink-table-planner-loader` but then I run into this exception when running the test: 

`Class 'org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider' not found. Perhaps you forgot to add the module 'flink-table-runtime' to the classpath?`

Is there anything else I need to add?;;;","23/May/22 09:36;chesnay;Did you try adding flink-table-runtime as a dependency as the error suggested?;;;","23/May/22 09:38;twalthr;Adding `flink-table-runtime` did not solve the problem? I remember we had similar problems with the examples. Can you take a look at how the other connector modules are doing it?;;;","23/May/22 09:45;twalthr;Btw wouldn't it make sense to put the ""file-based upsert sink that can be used for testing purposes to allow SQL upsert e2e test"" into the `flink-table-test-utils` module. This is exactly the intended purpose for stable utils that also users can use in the future.;;;","23/May/22 10:06;alexanderpreuss;I just went through the other connectors and they all depend on `flink-table-planner` :/ 

 

Regarding the `flink-table-test-utils` module we could move it there but the connector is not table-only so I'm not sure if this is a good idea. WDYT?;;;","23/May/22 10:16;twalthr;What do you mean with ""connector is not table-only""? The concept of an ""upsert sink"" makes only sense in the Table/SQL ecosystem. The `flink-table-test-utils` is intended to provide everything usually required to properly test your implementation. A mocking source and sink sounds like a default to me. The other connectors only depend on `flink-table-planner` in `test` scope, I guess this is only due to the test utilities. It would be great to not rely on legacy dependencies in new connectors and follow the new design if possible.;;;","23/May/22 11:09;alexanderpreuss;Maybe the initial description was ambiguous. The `flink-table-planner` dependency we are talking about always referred to a test dependency, it is not a compile dependency for `flink-test` or any other connector right now. For all connectors the planner is included as a test dependency because without it either the ExecutorFactory is not present or, using the `flink-table-planner-loader`, we run into exceptions as I described above.

I agree that an upsert sink only makes sense in Tableland, but I think the idea of having a very simple file-based sink for general testing scenarios is valuable for DataStream as well. The alternative option available to developers right now is the FileSink which is a lot more complex.;;;","24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Aug/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:34;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,
The SubtaskCheckpointCoordinatorImpl#close() isn't called in anywhere,FLINK-27724,13446030,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,fanrui,fanrui,fanrui,20/May/22 09:54,24/May/22 08:55,04/Jun/24 20:51,24/May/22 08:55,1.14.0,1.15.0,,,,,,,,,,,,,1.16.0,,,,Runtime / Checkpointing,,,,,0,,,,,The SubtaskCheckpointCoordinatorImpl#close() isn't called in anywhere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 08:55:19 UTC 2022,,,,,,,,,,"0|z12joo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/22 13:07;fanrui;Hi [~Wencong Liu] , thanks for your feedback.

I may have missed something, could you share one or two caller?  Thanks.;;;","22/May/22 13:56;Wencong Liu;Sorry, I came to the wrong conclusion just now. In StreamTask#cancel(), Subtaskcheckbpointcoordinatorimpl#close() should probably be called.;;;","23/May/22 13:18;pnowojski;{quote}
In StreamTask#cancel(), Subtaskcheckbpointcoordinatorimpl#close() should probably be called.
{quote}
Yes, but I think on the clean shut down path {{SubtaskCheckpointCoordinatorImpl}} is never closed, right?;;;","23/May/22 14:08;fanrui;Hi [~pnowojski]  [~Wencong Liu] , it's never closed on the clean/shutdown path. Could you assign it to me? I will fix it, thanks.;;;","24/May/22 08:55;pnowojski;As [~fanrui] pointed out in an offline discussion, {{SubtaskCheckpointCoordinatorImpl}} is actually closed via call stack:
# org.apache.flink.streaming.runtime.tasks.StreamTask#cleanUp
# resourceCloser.close();
# and combination of {{resourceCloser.registerCloseable(cancelables);}} + registering {{SubtaskCheckpointCoordinatorImpl}} in {{cancelables}}.

Thanks for spotting this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Slack: revisit how this communication channel works,FLINK-27723,13446028,13446022,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,xtsong,xtsong,xtsong,20/May/22 09:47,13/Apr/23 03:19,04/Jun/24 20:51,13/Apr/23 03:19,,,,,,,,,,,,,,,,,,,,,,,31/Dec/22 00:00,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-20 09:47:03.0,,,,,,,,,,"0|z12jo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slack: add invitation link and information to Flink project website,FLINK-27722,13446027,13446022,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,xtsong,xtsong,20/May/22 09:44,02/Jun/22 07:28,04/Jun/24 20:51,02/Jun/22 07:28,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 02 07:28:16 UTC 2022,,,,,,,,,,"0|z12jo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/22 14:19;martijnvisser;I've setup https://apache-flink-slack.herokuapp.com/ which uses https://github.com/outsideris/slack-invite-automation. Unfortunately Slack has blocked off API access, only legacy applications can still use this. I'm doing a round of investigation if there's an alternative/workaround for this. ;;;","24/May/22 14:22;chesnay;So it is expected that the herokuapp mechanism doesn't work?;;;","24/May/22 14:32;martijnvisser;Currently yes, you'll see a {{Failed! missing_scope}} error when you've provided your email address. That's because Slack has blocked of usage of the Slack API and made it available only to Enterprise users (as far as I currently see). If I can't fix it like this, I'll delete the Heroku app again

The working example can be found at https://apache-airflow-slack.herokuapp.com/ -> They are still running on the API, because they've signed up earlier for this;;;","25/May/22 02:53;jark;Another workaround is using a shorturl + never expired invitation link. 

1. create an invitation link that never expires but that has a limitation with only 100 new users. 
2. create a shorturl for the invitation link using https://s.apache.org/ , e.g. https://s.apache.org/flink-slack
3. when the invitation link reaches the 100 limitations, we can re-create a new invitation link and override the existing short URL . 

This involves manual effort, especially at the beginning phase, but it should work. ;;;","25/May/22 11:59;martijnvisser;I've created https://s.apache.org/apache-flink-slack (since https://s.apache.org/flink-slack was already created by someone and redirects to the Flink project website). Can any PMC also update the URL if necessary? Perhaps it's good to try that out first, to avoid that there's only 1 person who can make the change. ;;;","26/May/22 05:13;xtsong;I tried but couldn't update it.
- w/o checking the override option, it tells me there's an existing short link
- w/ the override option checked, I got an Internal Server Error. ;;;","26/May/22 15:36;jark;[~xtsong] , I updated the https://s.apache.org/flink-slack URL to our slack invitation link.
I guess the shorturl can only be overridden by the initial creator. 
I think this is also a good thing that this can avoid malicious overriding.;;;","27/May/22 02:38;xtsong;I've also tried [this api|https://api.slack.com/methods/admin.users.invite], but the required permission scope is only available to enterprise users.

How do we know an invitation link reaches or is reaching the 100 limit? I would be less concerned about only 1 person can update the shorturl, if this person gets notified when the link reaches a threshold below the limit (e.g., 80).;;;","27/May/22 10:55;jark;Not sure how others doing this, like Airbyte and Materialize. 
It seems Airbyte also using a shorturl and redirect to the shared link. ;;;","30/May/22 14:22;martijnvisser;Materialize is also on the free plan. They are directly linking to the Slack invite link.

That is of course also something that we can consider: just include the invite link directly in our project website. The advantage of that is that any committer can republish the project website and therefore update the link. We can just point a user to a Slack/community page on the project website and update the link if outdated. It's updated as soon as you commit your changes there.

[~xtsong] There's no indicator except that the one who created the invite link can get notified that someone joined via that link. I think in the first couple of days we'll need to update the link once or twice a day to be sure. 

Airbyte uses their own shortner; the advantage of that is probably that also multiple users can change it :);;;","31/May/22 02:12;xtsong;I think including the invitation link directly on the website sounds promising. In addition to that any committer can update it, we can also explain about the limitation and point people to mailing lists for help in case of an outdated invitation link. WDYT?;;;","31/May/22 16:12;martijnvisser;I agree. Are we already OK with opening up the Flink Slack community? Then I'll create a PR to add the details to https://flink.apache.org/community.html;;;","01/Jun/22 01:33;xtsong;Thanks [~martijnvisser].

+1 to open up the Flink Slack community. I'll update the wiki accordingly. We can announce this on MLs once the PR is merged. The archive can use a bit more time. ;;;","02/Jun/22 07:28;martijnvisser;Fixed in asf-site: f51d295d3ce670bdd06476a08b0bd5f1372edf60;;;",,,,,,,,,,,,,,,,,,
Slack: set up archive,FLINK-27721,13446026,13446022,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,xtsong,xtsong,20/May/22 09:42,18/Oct/22 15:03,04/Jun/24 20:51,18/Oct/22 15:03,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/22 14:00;rmetzger;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13050292/screenshot-1.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 18 15:03:42 UTC 2022,,,,,,,,,,"0|z12jns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 08:28;xtsong;Some updates on this one:
 * [~rmetzger] helped point me to the [ashb/slackarchive|https://github.com/ashb/slackarchive] repo, which is what Apache Airflow is using.
 * The instructions on how to use the repo is outdated. I've wrote to the owner for help, no responses yet.
 * I've also looked into the codes myself, trying to figure out how to get it work. So far, I've got the project build and started, but are struggling with manually creating the required table schemas in postgresql. 
 * I've also tried the upstream project [dutchcoders/slackarchive|https://github.com/dutchcoders/slackarchive], which haven't been updated for years and I run into a bunch of disabled api issues. It seems to me ashb/slackarchive is the only project that currently works.;;;","25/May/22 08:35;rmetzger;Thanks a lot for working on this!

I'm not surprised that there are some problems getting this up and running, since this is only used by one person.
What's your overall impression of the project? Do you think it is secure to run this project on the public internet (e.g. what's the risk that there's some security issues somewhere?, or stability issues and we loose all the data accidentally?);;;","25/May/22 09:02;xtsong;I don't spot any security issue, but that might because I don't know much security things.

Stability is probably not a bit issues. It stores data in postgresql so we can easily backup them once a while. It also seems to support migrating between different db backups, which I haven't tried yet.;;;","25/May/22 09:57;rmetzger;What is blocking you from proceeding? (= what's the question you had to ask the maintainer?) Maybe I can help, or I know somebody?;;;","25/May/22 12:25;xtsong;No specific blockers. Just I'm proceeding slower than expected, due to my other works. I'm currently working on two things:
1) I've managed to get all the dependencies settled manually. Working on automating them with scripts and docker.
2) It looks like we can generate the schemas from the model defined in the current code base, by adding a bit more codes. For that, I also need 1) to make the build & deploy convenient.

BTW, I think we don't necessarily block publishing of the slack on this ticket. Based on what I've learned so far, I'm quite confident we just need a bit more time to get the archive work. And we do have quite some time before reaching the 10k messages limit.;;;","25/May/22 13:16;rmetzger;I agree, let's not block the effort on the archive!

Which server are we going to use for the archive?
Which domain are we using?

It would be very nice if we would automatically create a weekly database dump that is downloadable in a public archive. So if something happens to the archive server or the entity maintaining it, somebody else can restore the archive.;;;","26/May/22 01:56;xtsong;bq. Which server are we going to use for the archive?
I'd expect the load of the archive service to be light enough to share machine with other services that we already hold, e.g., the codespeed. However, I'm a bit concerned that the archive service might have some random, unexplainable impacts on the benchmark results. Any suggestions?

bq. Which domain are we using?
Haven't really thought about this yet.

bq. It would be very nice if we would automatically create a weekly database dump that is downloadable in a public archive.
At beginning, before the dump growth too big, we may simply upload them to wiki or github.;;;","02/Jun/22 03:21;jark;Can we use the server which hosts https://flink-packages.org/ ? 
How does flink-packages backup databases? ;;;","02/Jun/22 03:24;jark;Regarding the domain, I think we can try to buy the domain ""flink-slack.org"" which is available for now. 
""codespeed"" or other domains sound not official to me. ;;;","02/Jun/22 10:30;rmetzger;flink-slack.org sounds good! Would be nice if you could register it!

> How does flink-packages backup databases?

It runs a cron job every day creating a dump of the database (just locally). Every now and then I downloaded a dump. The problem is that this server is owned by Ververica (in Google Cloud), so I don't have access to it anymore.

> Can we use the server which hosts https://flink-packages.org/ ? 

In principle yes. The server has very little resources, but upgrading it to a bigger machine should be simple.

;;;","15/Jun/22 08:26;chesnay;Would it make sense to run this on a ASF VM that other projects can also use?;;;","15/Jun/22 09:42;xtsong;I didn't know there're ASF VMs available. How does that work? It would be wonderful if it is feasible to run this on an ASF infrastructure.

Progress updates:
It turns out the slackarchive project uses some deprecated Slack APIs which is only available to legacy applications. I'm working on migrating them to the new APIs. This could take a bit more time, as I'm working on this only on my leisure time.;;;","28/Jun/22 10:37;rmetzger;We can request VMs from infra like this: https://issues.apache.org/jira/browse/INFRA-22111.

Thanks a lot for migrating slackarchive to the new APIs!!;;;","14/Jul/22 14:19;chesnay;What's the state on this effort?;;;","18/Jul/22 04:04;xtsong;Status updates:

Right now, I have something imperfect but workable. I probably won't have time to further improve it recently. Given that we are approaching the 10k messages limit very soon, I'll try to deploy the current version.

The known limitations are:
 # *Messages are not organized in threads at frontend, making it hard for people to read.* This is the same limitation that [airflow|http://apache-airflow.slack-archives.org/] also has. Properties needed for grouping messages into threads are already captured in the database. All we need is to improve the way the messages are displayed.
 # *It's not realtime.* Slack's new event api never worked for me. So I went for an approach that periodically fetches the messages, with a configurable interval (default 1h). Consequently, new messages may take up to 1 hour to appear in the archive, which is probably fine because they can be searched in Slack anyway.
 # *It's unlikely, but still possible, to loose messages.* With Slack's conversation api, we need to first retrieve parent messages that are directly sent to the channel, then for each of them retrieve threaded messages replying to it. That means for an already retrieved thread, we cannot know whether there're new replies to it without trying to retrieve it again. Moreover, the api has a ~50/min rate limit, so we probably should not frequently retrieve replies for all threads. My current approach is to only retrieve new messages for threads started within the recent 30 days (configurable). That means new replies to a thread started more than 30 days ago can be lost, which I'd expect to be very rare.
 # *Backup is not automatic.* We can dump the database with one command, without interrupting the service. We just need to setup a cronjob to trigger and handle the dumps (uploading & cleaning).

Some numbers, FYI:
# [Slack Analytics|https://apache-flink.slack.com/admin/stats] shows we now have 9.1k total messages. In the last 30 days, only 31% of messages are sent in public channels, 67% in DMs and 1% in private channels.
# Slack archive captures public channel messages only. It captures 2.5k total messages, taking about 7~8 minutes on my laptop. The bottleneck is the Slack's api rate limit.
# A full dump of the database, containing all the 2.5k messages, channel & user information, completes almost instantly. The dumped file is 3.7MB large.

I'll try to deploy the service next. Based on the numbers, I think a dedicated VM might not be necessary. So I'd try with the flink-packages host first. BTW, I have already backed up a dump of all public messages so far, so it shouldn't be a problem if the service is not deployed by the time the 10k limit is reached. 
;;;","30/Aug/22 11:25;martijnvisser;[~xtsong] Awesome, looking forward to the next steps;;;","10/Oct/22 08:17;rmetzger;Hey, I've recently learned about linen.dev, which is a free tool to publicly archive Slack channels.
I've set it up for our workspace to try it out. I was able to upload messages older than 90 days, so the fully history should be available there. 
WDYT? https://www.linen.dev/s/apache-flink
If you like it, I would link it from the main Flink website.;;;","10/Oct/22 08:33;chesnay;Is that actually an _archive_ though? It _sounds_ more like a front to support searches via google.;;;","10/Oct/22 08:39;rmetzger;Yes, but in my understanding that was always the goal of our a Flink Slack Archive. To avoid repeat questions and making all the wisdom in Slack accessible. 
The linen.dev archive also has a search bar on top, allows users to browse the Slack w/o signing up for the community. What else do you expect from an archive?;;;","10/Oct/22 09:18;xtsong;This is awesome~! Thanks a lot, [~rmetzger].

I have only one question. Does it have a limit on how many messages / data it can store?;;;","10/Oct/22 09:27;chesnay;??What else do you expect from an archive???

From the description on the page I'd conclude they just display what is _currently_ stored in the Slack instance in a searchable manner.
I'd expect an archive to store _all_ data, even if it's no longer available in slack or the slack instance was removed altogether.;;;","10/Oct/22 09:35;martijnvisser;It looks really nice I must say. My biggest concern is related to the privacy statement / how will they earn money. By using their service, data of the Slack users will be shared with Linen. Based on the currently used ASF Privacy Policy, that's not covered (see https://privacy.apache.org/policies/privacy-policy-public.html). We should get checked if it's OK to use a service such as theirs.;;;","10/Oct/22 10:26;rmetzger;Thank you all for the positive responses.

bq. Does it have a limit on how many messages / data it can store?

We are currently on the free plan, which offers ""Unlimited message retention history"".
I have exported all public messages from our Slack instance, and uploaded it to linen.dev. So the archive shows messages from day one, and it should sync automatically from now on.

bq. My biggest concern is related to the privacy statement / how will they earn money

I share that concern. They might at some point put up ads or limit the features of the free plan. I will ask them about this.

bq. Based on the currently used ASF Privacy Policy, that's not covered

Thanks for bringing this up. Based on my understanding, the ASF PP is only covering data usage on ASF premises. The Apache Flink Slack is hosted by Slack Inc, so users signing up for the Flink Slack have to accept / agree to Slack's PP, not the ASF PP. How about extending the welcome message to include a statement that we are making the contents of the Flink slack instance public?


;;;","10/Oct/22 11:47;martijnvisser;bq. Based on my understanding, the ASF PP is only covering data usage on ASF premises. The Apache Flink Slack is hosted by Slack Inc, so users signing up for the Flink Slack have to accept / agree to Slack's PP, not the ASF PP. 

Given that we point from the ASF project website to this Slack instance, that's probably a grey zone. For certain third party integrations (like having a 3rd party search on an ASF project website) the ASF Privacy actually looked at the privacy guarantees from such a provider. 

I think we could indeed resolve that by making it explicit in the documentation where they can retrieve the invite link from. ;;;","10/Oct/22 11:59;rmetzger;Okay, unless somebody objects until tomorrow (or I receive unexpected input from linen), I'll open a PR for updating the website with the link to linen.dev and a disclaimer wrt to Privacy.;;;","10/Oct/22 14:15;chesnay;I'd still like a clarification about whether it stores all data and whether all data can be exported on demand.
I don't want to end up in a situation where everything is lost because they go down / mess up.
In particular since this services appears to be relatively new, and is (apparently) only backed by few people.;;;","11/Oct/22 14:02;rmetzger;Messages from all public channels in our Slack can actually be exported from the admin interface:
 !screenshot-1.png! 
From my understanding, the export contains all messages, even those messages not visible through the UI on the free plan.
So Slack stores all data for us.

I agree that linen is a fairly new service, but since Slack is storing all data for us, the risk is mostly around loosing URLs, once people start linking into the linen.dev archive.;;;","18/Oct/22 08:31;chesnay;??Slack is storing all data for us, the risk is mostly around loosing URLs, once people start linking into the linen.dev archive.??

I thought Slack doesn't since we're on a free plan?;;;","18/Oct/22 08:43;rmetzger;Slack is storing all messages, but it is not showing them.
In our Slack instance, you see the following message:

bq. Messages and files older than 90 days are hidden. Upgrade to a paid plan to unlock your team’s full message and file history, plus all the premium features of the Pro plan.

Slack is hiding messages, not deleting them.;;;","18/Oct/22 10:13;chesnay;oooh, ok that changes things a bit. Then +1 to use linen.dev;;;","18/Oct/22 12:59;rmetzger;Great, I'll open a PR to add it to our website.;;;","18/Oct/22 15:03;rmetzger;Merged to flink-web in https://github.com/apache/flink-web/commit/a28a09a11abed97e7408df37b5ae633b770cd3ac;;;"
Slack: Update the project website regarding communication channels,FLINK-27720,13446025,13446022,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,xtsong,xtsong,20/May/22 09:41,02/Jun/22 07:35,04/Jun/24 20:51,02/Jun/22 07:35,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 02 07:35:58 UTC 2022,,,,,,,,,,"0|z12jnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/22 07:35;xtsong;Subsumed by FLINK-27722;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Apache Flink slack workspace,FLINK-27719,13446022,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,xtsong,xtsong,xtsong,20/May/22 09:38,13/Apr/23 03:19,04/Jun/24 20:51,13/Apr/23 03:19,,,,,,,,,,,,,,,,,,,,,,,,0,stale-assigned,Umbrella,,,"This is an umbrella for tasks related to setting up the Apache Flink slack channel.

 

[Draft] Slack Management:
[https://cwiki.apache.org/confluence/display/FLINK/WIP%3A+Slack+Management]

 

[~jark] and I ([~xtsong]) have already created a slack workspace for early setups. If anyone want to help, please reach out to us for an invitation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 13 03:19:16 UTC 2023,,,,,,,,,,"0|z12jmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","13/Apr/23 02:58;yunta;I think this ticket shall be marked as resolved, right? [~xtsong];;;","13/Apr/23 03:19;xtsong;Yes. Thanks for the reminder.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix fail to count mutiple fields excpetion in Hive dialect,FLINK-27718,13446004,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,20/May/22 08:30,05/Sep/22 13:11,04/Jun/24 20:51,05/Sep/22 13:11,,,,,,,,,,,,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,,0,pull-request-available,,,,"In Hive, it's support to count multiple fields using the following sql:

 
{code:java}
select count(distinct a, b) from src
{code}
We are also expected to support it while using Hive dialect in Flink.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 05 13:11:26 UTC 2022,,,,,,,,,,"0|z12jiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/22 13:11;jark;Fixed in 
 - master: 5c41e7bc0a9355509b87a5edc43899bbf476ed61
 - release-1.16: 5ea9cc114921b3c1b2ab3cd8751ff4ef6c7c1329;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document operator compatibility guarantees,FLINK-27717,13446002,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,20/May/22 08:28,22/May/22 05:57,04/Jun/24 20:51,22/May/22 05:57,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,,,,,We should document the expected guarantees for CRD/APIs etc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-20 08:28:17.0,,,,,,,,,,"0|z12jig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Flink ML Python API docs,FLINK-27716,13445983,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,hxbks2ks,hxbks2ks,20/May/22 07:32,08/Feb/23 03:59,04/Jun/24 20:51,08/Feb/23 03:59,,,,,,,,,,,,,,,ml-2.2.0,,,,API / Python,Documentation,Library / Machine Learning,,,0,pull-request-available,,,,We can use sphinx same as pyflink or other tools to generate Python API docs of ML,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 08 03:59:39 UTC 2023,,,,,,,,,,"0|z12je8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 03:59;lindong;This is merged to the apache/flink-ml mater branch at 76d5b864dd7df95adebebe6f7e47d52ae0f689c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more Python ML examples,FLINK-27715,13445980,13429597,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,hxbks2ks,hxbks2ks,20/May/22 07:28,10/Jan/23 05:19,04/Jun/24 20:51,11/Oct/22 02:46,,,,,,,,,,,,,,,ml-2.2.0,,,,API / Python,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25497,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 11 02:46:12 UTC 2022,,,,,,,,,,"0|z12jdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 02:46;hxb;Merged into master via 91e74648bf47d1027ef3362c61456b90fcf89535;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate to java-operator-sdk v3,FLINK-27714,13445978,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,20/May/22 07:22,24/Nov/22 01:02,04/Jun/24 20:51,31/May/22 07:50,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"There are a few features planning to add to the operator:
 * Dynamic change of watched namespaces and automatic adjustment of related {{EventSources}}
 * Improved Error Handling API

also worth evaluating of:
 * Dependent resources management! See the [documentation|https://javaoperatorsdk.io/docs/dependent-resources] for more information
 * Support for following a set of namespaces in {{InformerEventSource}} and other related improvements.
 * Removal for need of {{PrimaryToSecondaryMapper}} - now handled automatically for you

https://github.com/java-operator-sdk/java-operator-sdk/releases/tag/v3.0.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 07:50:38 UTC 2022,,,,,,,,,,"0|z12jd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 07:50;gyfora;merged to main d2f1d71c2c28d566c3451f2878a301130ce63807;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Apache Beam to 2.40.0,FLINK-27713,13445977,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,ana4,ana4,20/May/22 07:21,06/Feb/23 05:18,04/Jun/24 20:51,06/Feb/23 05:18,1.15.0,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"Directly depending on Beam 2.38 will throw an error.
{code:java}
 AttributeError: module 'pyparsing' has no attribute 'downcaseTokens' {code}
 

The following is detail

https://issues.apache.org/jira/browse/BEAM-14396

[https://github.com/httplib2/httplib2/issues/207]
[https://github.com/PAIR-code/what-if-tool/issues/185#issuecomment-1013971242]",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28714,FLINK-29421,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 06 05:18:28 UTC 2023,,,,,,,,,,"0|z12jcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 07:31;martijnvisser;It might make sense to follow FLINK-25188 since that also requires a Beam update;;;","20/May/22 09:58;hxbks2ks;apche-beam is currently voting for 2.39.0-rc2 and I don't think this pyparsing issue is a blocker issue. Upgrading the dependency of apache-beam version to 2.40.0 or higher can be done after release-1.16 or later;;;","06/Feb/23 05:18;dianfu;Closing this ticket as Apache Beam has been bumped to 2.43.0 in FLINK-29421. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Job failed to start due to ""Time should be non negative""",FLINK-27712,13445963,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,sharonxr55,sharonxr55,20/May/22 05:52,14/Jun/22 15:03,04/Jun/24 20:51,10/Jun/22 10:25,1.14.4,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,0,,,,,"Happened intermittently. A restart made the issue go away.

Stack trace:
{code:java}
Time should be non negative
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
at org.apache.flink.runtime.throughput.ThroughputEMA.calculateThroughput(ThroughputEMA.java:44)
at org.apache.flink.runtime.throughput.ThroughputCalculator.calculateThroughput(ThroughputCalculator.java:80)
at org.apache.flink.streaming.runtime.tasks.StreamTask.debloat(StreamTask.java:792)
at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$scheduleBufferDebloater$4(StreamTask.java:784)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
{code}
 

JobManager error log is attached.

 

Maybe related to [Flink-25454|https://issues.apache.org/jira/browse/FLINK-25454]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/22 05:51;sharonxr55;flink_error.log.txt;https://issues.apache.org/jira/secure/attachment/13043947/flink_error.log.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 15:03:53 UTC 2022,,,,,,,,,,"0|z12j9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 07:33;martijnvisser;I think this is resolved in Flink 1.15, see FLINK-25454;;;","10/Jun/22 10:25;rmetzger;I'm closing this ticket, but please comment on it if it occurs again in newer versions!;;;","14/Jun/22 15:03;aliazov;is it possible to merge this correction also for version 1.14? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the typo of  set_topics_pattern by changing it to set_topic_pattern for Pulsar Connector,FLINK-27711,13445962,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,20/May/22 05:46,21/May/22 09:04,04/Jun/24 20:51,21/May/22 09:03,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,API / Python,Connectors / Pulsar,,,,0,pull-request-available,,,,Update set_topics_pattern to set_topic_pattern,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 21 09:03:15 UTC 2022,,,,,,,,,,"0|z12j9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 09:03;dianfu;Fixed in:
- master via 1ff8d3ec596dc9aebe4d402f6f90902e182ff70e
- release-1.15 via 454f351a2e75cc3e9f39a74a37f95d0d483bf19b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve logs to better display Execution,FLINK-27710,13445955,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,zhuzh,zhuzh,zhuzh,20/May/22 04:16,16/Mar/23 08:45,04/Jun/24 20:51,16/Mar/23 08:45,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Coordination,Runtime / Task,,,,0,pull-request-available,stale-assigned,,,"Currently, an execution is usually represented as ""{{{}job vertex name{}}} ({{{}subtaskIndex+1{}}}/{{{}vertex parallelism{}}}) ({{{}attemptId{}}})"" in logs, which may be redundant after this refactoring work. With the change of FLINK-17295, the representation of Execution in logs will be redundant. e.g. the subtask index is displayed 2 times.

Therefore, I'm proposing to change the format to be ""<{{{}job vertex name> {{}}}}(<{{{}subtaskIndex>+1{}}}/<{{{}vertex parallelism>{}}}) {{#<attemptNumber>}}  (graph: <{{{}short ExecutionGraphID>, vertex: <{}}}{{{}JobVertexID>{}}}) "" and avoid directly display the {{{}ExecutionAttemptID{}}}. This can increase the log readability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 16 08:45:47 UTC 2023,,,,,,,,,,"0|z12j80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 08:22;xtsong;I'm not entirely sure about the format.

An execution is uniquely identified by <graph-id, vertex-id, subtask-index, attempt-id>. I'm concerned that for someone how is not super familiar with the Flink internal concepts, they may misunderstand the vertex-id as the unique identifier of a task.

I'm leaning towards keeping it as is, displaying the complete execution id. It prevents confusions, at the cost of being a little verbose which IMO is fine.;;;","08/Jun/22 15:19;zhuzh;Thanks for the comments. [~xtsong] 

I think your concern is valid. I'm also not pretty sure which name pattern would be the best. I would leave this ticket open for a while and watching in ML to see if other devs/users complain about the task name in logs after the execution ID rework.;;;","08/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Mar/23 08:40;tanyuxin;[~zhuzh], I saw you have marked this as ""Deprecated at the moment"" in the PR, is this finished? If so, Maybe we can close this issue.;;;","16/Mar/23 08:45;zhuzh;Won't do at the moment until we get more inputs from users.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add comment to schema,FLINK-27709,13445952,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,20/May/22 03:45,20/May/22 09:30,04/Jun/24 20:51,20/May/22 09:30,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,We can add comment to schema for table. See `CatalogBaseTable.getComment`.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 09:30:26 UTC 2022,,,,,,,,,,"0|z12j7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 09:30;lzljs3620320;master: f93a2897761683f6eb16eb1bb864402aa8b22b57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add background compaction task for append-only table when ingesting.,FLINK-27708,13445950,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,openinx,openinx,20/May/22 03:31,25/Aug/22 02:29,04/Jun/24 20:51,11/Jul/22 03:54,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"We could still execute compaction task to merge small files in the background for append-only table.

This compaction is just to avoid a lot of small files.

Its purpose is similar to that of filesystem compaction: https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/filesystem/#file-compaction",,,,,,,,,,,,,FLINK-28108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 07:00;qingyue;image-2022-06-21-14-59-59-593.png;https://issues.apache.org/jira/secure/attachment/13045346/image-2022-06-21-14-59-59-593.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 11 03:54:58 UTC 2022,,,,,,,,,,"0|z12j6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/22 07:24;complone;I want to try to do this work. There are some studies on leveldb compact before. The current issue means that the compact function supported in MergeTreeWriter is supported in AppendOnlyWriter.;;;","21/Jun/22 02:19;lzljs3620320;Hi [~complone] 

Before submitting a PR, it is best to discuss clearly in JIRA what it is that needs to be done and what the general idea is.;;;","21/Jun/22 02:27;qingyue;Hi, [~complone] thanks for your interest in this ticket. However, the community encourages the contributor to have a consensus on the implementation and being an assignee before issuing a PR. You can take [https://developer.aliyun.com/article/774862] for a practical reference.;;;","21/Jun/22 07:07;qingyue;* Why
 ** Checkpoints will interfere with the writer, which forces the writer close, and thus the generated files may not meet the target file size.

                  !image-2022-06-21-14-59-59-593.png|width=502,height=149!
 * How
 ** Since the append-only table does not define a key, the compaction should be based on the sequence number to keep orderliness. 
 ** We could introduce an asynchronized task to collect previously committed files whose sizes are less than the target file size, sort files by min/max seq number, and then perform a concatenation rewrite. And during the prepare commit phase, the compacted files (if available) can be submitted along with the newly written files.

 

Please assign this ticket to me, cc [~lzljs3620320], thanks!;;;","11/Jul/22 03:54;lzljs3620320;master: 9f760307f9da361ddb4ee786a9f3ab1651db149c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement ManagedTableFactory#onCompactTable,FLINK-27707,13445949,13443507,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,20/May/22 03:20,10/Jun/22 13:58,04/Jun/24 20:51,10/Jun/22 10:10,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"As discussed offline, the new implementation has been modified.
 * Don't perform a scan during the planning phase. Instead, we put a flag along with part spec to indicate it is ordinary manual trigger compaction.

 * Introduce a new {{PrecommittingSinkWriter}} impl to perform dedicated compaction tasks. This writer is responsible for scanning and selecting partition and bucket according to the current sub-task id, and then creating a per-bucket compact writer to submit compaction. Since there's no data shuffled between source and sink, so all the compaction is performed when {{SinkWriterOperator#prepareCommit}} is invoked.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 10:10:28 UTC 2022,,,,,,,,,,"0|z12j6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 10:10;lzljs3620320;master: da7427662b24f6f0c150b2a19aa7b12c6597a0bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor all subclasses of FileStoreTableITCase to use the batchSql.,FLINK-27706,13445947,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,openinx,openinx,openinx,20/May/22 03:18,25/Aug/22 02:30,04/Jun/24 20:51,20/May/22 10:21,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Since we've introduced a batchSql to execute batch query for flink in FileStoreTableITCase.  Then all the subclasses can just use batch sql to submit the flink sql.

It's a minor issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 10:21:28 UTC 2022,,,,,,,,,,"0|z12j68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 10:21;lzljs3620320;master: 43326310de7af4bb3695d3dbcb184b23dab467bf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
num-sorted-run.compaction-trigger should not interfere the num-levels ,FLINK-27705,13445937,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,20/May/22 02:34,23/May/22 07:55,04/Jun/24 20:51,23/May/22 07:55,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"h3. Issue Description
The default value for MergeTreeOptions.NUM_LEVELS is not a constant, and it varies with MergeTreeOptions.NUM_SORTED_RUNS_COMPACTION_TRIGGER. If users do not specify the MergeTreeOptions.NUM_LEVELS at the first, once MergeTreeOptions.NUM_SORTED_RUNS_COMPACTION_TRIGGER is changed, and the successive write task would get a chance to fail (to be specific, when the compaction trigger size shrinks and the previous writes had triggered compaction).


h3. How to Reproduce

add a test under ForceCompactionITCase
{code:java}
@Override
protected List<String> ddl() {
    return Arrays.asList(
            ""CREATE TABLE IF NOT EXISTS T (\n""
                    + ""  f0 INT\n, ""
                    + ""  f1 STRING\n, ""
                    + ""  f2 STRING\n""
                    + "") PARTITIONED BY (f1)"",
            ""CREATE TABLE IF NOT EXISTS T1 (\n""
                    + ""  f0 INT\n, ""
                    + ""  f1 STRING\n, ""
                    + ""  f2 STRING\n""
                    + "")"");
}

@Test
public void test() throws Exception {
    bEnv.executeSql(""ALTER TABLE T1 SET ('commit.force-compact' = 'true')"");
    bEnv.executeSql(
                    ""INSERT INTO T1 VALUES(1, 'Winter', 'Winter is Coming'),""
                            + ""(2, 'Winter', 'The First Snowflake'), ""
                            + ""(2, 'Spring', 'The First Rose in Spring'), ""
                            + ""(7, 'Summer', 'Summertime Sadness')"")
            .await();
    bEnv.executeSql(""INSERT INTO T1 VALUES(12, 'Winter', 'Last Christmas')"").await();
    bEnv.executeSql(""INSERT INTO T1 VALUES(11, 'Winter', 'Winter is Coming')"").await();
    bEnv.executeSql(""INSERT INTO T1 VALUES(10, 'Autumn', 'Refrain')"").await();
    bEnv.executeSql(
                    ""INSERT INTO T1 VALUES(6, 'Summer', 'Watermelon Sugar'), ""
                            + ""(4, 'Spring', 'Spring Water')"")
            .await();
    bEnv.executeSql(
                    ""INSERT INTO T1 VALUES(66, 'Summer', 'Summer Vibe'),""
                            + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
            .await();
    bEnv.executeSql(
                    ""INSERT INTO T1 VALUES(666, 'Summer', 'Summer Vibe'),""
                            + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
            .await();
    bEnv.executeSql(""ALTER TABLE T1 SET ('num-sorted-run.compaction-trigger' = '2')"");
    bEnv.executeSql(
                    ""INSERT INTO T1 VALUES(666, 'Summer', 'Summer Vibe'),""
                            + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
            .await();
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27515,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 07:55:07 UTC 2022,,,,,,,,,,"0|z12j40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 02:35;qingyue;Stacktrace
{code:java}
java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
    at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
    at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
    at org.apache.flink.table.store.connector.ForceCompactionITCase.test(ForceCompactionITCase.java:76)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
    at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
    at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
    at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
    at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
    at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
    ... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
    at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
    at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
    at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
    at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
    at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
    at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
    at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
    at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
    at akka.dispatch.OnComplete.internal(Future.scala:300)
    at akka.dispatch.OnComplete.internal(Future.scala:297)
    at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
    at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
    at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
    at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
    at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
    at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
    at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
    at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
    at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
    at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
    at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
    at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
    at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
    at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
    at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
    at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
    at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
    at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
    at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
    at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
    at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
    at akka.actor.Actor.aroundReceive(Actor.scala:537)
    at akka.actor.Actor.aroundReceive$(Actor.scala:535)
    at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
    at akka.actor.ActorCell.invoke(ActorCell.scala:548)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
    at akka.dispatch.Mailbox.run(Mailbox.scala:231)
    at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
    ... 4 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 4, Size: 2
    at java.util.ArrayList.rangeCheck(ArrayList.java:653)
    at java.util.ArrayList.get(ArrayList.java:429)
    at org.apache.flink.table.store.file.mergetree.Levels.runOfLevel(Levels.java:68)
    at org.apache.flink.table.store.file.mergetree.Levels.updateLevel(Levels.java:137)
    at org.apache.flink.table.store.file.mergetree.Levels.lambda$new$1(Levels.java:58)
    at java.util.HashMap.forEach(HashMap.java:1288)
    at org.apache.flink.table.store.file.mergetree.Levels.<init>(Levels.java:58)
    at org.apache.flink.table.store.file.operation.FileStoreWriteImpl.createMergeTreeWriter(FileStoreWriteImpl.java:130)
    at org.apache.flink.table.store.file.operation.FileStoreWriteImpl.createWriter(FileStoreWriteImpl.java:88)
    at org.apache.flink.table.store.connector.sink.StoreSinkWriter.lambda$getWriter$0(StoreSinkWriter.java:98)
    at java.util.HashMap.computeIfAbsent(HashMap.java:1126)
    at org.apache.flink.table.store.connector.sink.StoreSinkWriter.getWriter(StoreSinkWriter.java:92)
    at org.apache.flink.table.store.connector.sink.StoreSinkWriter.write(StoreSinkWriter.java:105)
    at org.apache.flink.table.store.connector.sink.StoreSinkWriter.write(StoreSinkWriter.java:51)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:158)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748){code};;;","20/May/22 02:39;qingyue;The reason is that the previous write job had triggered compaction and generated the new data file with level 5.

Since I didn't set the num of levels explicitly, when the trigger was reduced to 2, the num of levels was reduced to 3, so the new write job failed.;;;","20/May/22 10:09;lzljs3620320;Thanks [~qingyue] for reporting.

If the user only modifies the compaction-trigger without modifying the numLevels, a possible solution is to adjust to the maximum numLevel that already exists in stores.;;;","23/May/22 07:55;lzljs3620320;master: a163dc83c0c89fe62a7aecce0ead985710eec425;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java 17 compatibility,FLINK-27704,13445867,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,jkaye,jkaye,19/May/22 16:07,20/May/22 07:35,04/Jun/24 20:51,19/May/22 18:07,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"We're looking for an update on this issue: https://issues.apache.org/jira/browse/FLINK-15736

 

Java 11 is a very old LTS version and is missing vital features from Java 14 and Java 17. A production deployment should support Java 17.

In general, I would expect compatibility for new language versions within 6 months of their release. Being pinned to old versions has significant cascading impact on technical infrastructure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 07:35:35 UTC 2022,,,,,,,,,,"0|z12iog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 18:07;chesnay;There are a number of blocking items outlined in FLINK-15736 that first need to be resolved. As the moment there is no timeline for when Java 17 support will be finalized, and afaik no one is looking into these issues at the moment.;;;","20/May/22 07:35;martijnvisser;[~jkaye] To add to Chesnay's comment, if you can help with the blocking items, that would speed-up the support of course. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileChannelManagerImplTest.testDirectoriesCleanupOnKillWithoutCallerHook failed with The marker file was not found within 10000 msecs,FLINK-27703,13445831,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaoyunhaii,hxbks2ks,hxbks2ks,19/May/22 12:51,14/Jul/22 02:23,04/Jun/24 20:51,14/Jul/22 02:23,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / Network,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-05-19T09:08:49.8088232Z May 19 09:08:49 [ERROR] Failures: 
2022-05-19T09:08:49.8090850Z May 19 09:08:49 [ERROR]   FileChannelManagerImplTest.testDirectoriesCleanupOnKillWithoutCallerHook:97->testDirectoriesCleanupOnKill:127 The marker file was not found within 10000 msecs
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35834&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9744",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 02:23:21 UTC 2022,,,,,,,,,,"0|z12igg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/22 12:07;Wencong Liu;Hello [~hxbks2ks] , I understand from code this test creates a file when start a new process, but it is not completed within 10s. Please confirm whether the test can be repeated locally?  cc [~gaoyunhaii], [~martijnvisser] ;;;","01/Jun/22 12:29;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36190&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702;;;","01/Jun/22 12:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36230&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53;;;","16/Jun/22 09:05;lincoln.86xy;another one: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36776&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","22/Jun/22 11:45;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37037&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8542;;;","28/Jun/22 07:56;gaoyunhaii;I'll have a look. ;;;","05/Jul/22 08:18;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37602&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8610

[~gaoyunhaii] Any updates from your end?;;;","08/Jul/22 02:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37849&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","08/Jul/22 03:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37836&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","11/Jul/22 02:24;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","11/Jul/22 02:28;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","11/Jul/22 02:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37959&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53;;;","11/Jul/22 02:33;hxbks2ks;HI [~gaoyunhaii], any updates on this issue?;;;","11/Jul/22 05:56;gaoyunhaii;Hi [~hxbks2ks] it seems to me that the file creation is delay either due to the process did not started in time or the file creation is delayed. I'll first extend the timeout and add some logs and keep an eye with the following runs. ;;;","13/Jul/22 10:47;xtsong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38134&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=6689;;;","14/Jul/22 02:23;hxbks2ks;Thanks [~gaoyunhaii] for the investigation.

Merged the commit 4f7ebbb4f91ad27aaf89c8c035031efe91b84171 with extending timeout and adding logs. I will keep an eye on the following runs.;;;",,,,,,,,,,,,,,,,
Flink table code splitter does not throw anything if result is not compiled,FLINK-27702,13445829,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,19/May/22 12:42,19/May/22 12:42,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"In fact there are 2 issues:
1. The code throws nothing if  splitted code is not compiled
2. If code length length than  limit then it does not check compiliation at all.

Also there are comments about that at https://github.com/apache/flink/pull/19638#discussion_r865605558
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-19 12:42:25.0,,,,,,,,,,"0|z12ig0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashMapStateBackendWindowITCase. testAggregateWindowStateReader failed with  Not all required tasks are currently running,FLINK-27701,13445824,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,hxbks2ks,hxbks2ks,19/May/22 12:30,01/Dec/22 03:21,04/Jun/24 20:51,01/Dec/22 03:21,1.16.0,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,auto-deprioritized-major,test-stability,,,"
{code:java}
2022-05-19T11:04:27.4331524Z May 19 11:04:27 [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 29.034 s <<< FAILURE! - in org.apache.flink.state.api.HashMapStateBackendWindowITCase
2022-05-19T11:04:27.4333055Z May 19 11:04:27 [ERROR] org.apache.flink.state.api.HashMapStateBackendWindowITCase.testAggregateWindowStateReader  Time elapsed: 0.105 s  <<< ERROR!
2022-05-19T11:04:27.4333765Z May 19 11:04:27 java.lang.RuntimeException: Failed to take savepoint
2022-05-19T11:04:27.4334405Z May 19 11:04:27 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:68)
2022-05-19T11:04:27.4335375Z May 19 11:04:27 	at org.apache.flink.state.api.SavepointWindowReaderITCase.testAggregateWindowStateReader(SavepointWindowReaderITCase.java:149)
2022-05-19T11:04:27.4338106Z May 19 11:04:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-19T11:04:27.4339140Z May 19 11:04:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-19T11:04:27.4339854Z May 19 11:04:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T11:04:27.4340560Z May 19 11:04:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T11:04:27.4341746Z May 19 11:04:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-05-19T11:04:27.4342797Z May 19 11:04:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-05-19T11:04:27.4343717Z May 19 11:04:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-05-19T11:04:27.4344909Z May 19 11:04:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-05-19T11:04:27.4345993Z May 19 11:04:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-05-19T11:04:27.4346981Z May 19 11:04:27 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-05-19T11:04:27.4347590Z May 19 11:04:27 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-05-19T11:04:27.4348200Z May 19 11:04:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-19T11:04:27.4348856Z May 19 11:04:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-05-19T11:04:27.4349484Z May 19 11:04:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-05-19T11:04:27.4350118Z May 19 11:04:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-05-19T11:04:27.4350899Z May 19 11:04:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-05-19T11:04:27.4352057Z May 19 11:04:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-05-19T11:04:27.4353154Z May 19 11:04:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-05-19T11:04:27.4354153Z May 19 11:04:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-05-19T11:04:27.4354936Z May 19 11:04:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-05-19T11:04:27.4355560Z May 19 11:04:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-05-19T11:04:27.4356167Z May 19 11:04:27 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-05-19T11:04:27.4356775Z May 19 11:04:27 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-05-19T11:04:27.4357358Z May 19 11:04:27 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-19T11:04:27.4357932Z May 19 11:04:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-19T11:04:27.4358500Z May 19 11:04:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-19T11:04:27.4359055Z May 19 11:04:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-19T11:04:27.4359584Z May 19 11:04:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-19T11:04:27.4360174Z May 19 11:04:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-05-19T11:04:27.4361027Z May 19 11:04:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-05-19T11:04:27.4361782Z May 19 11:04:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-05-19T11:04:27.4362698Z May 19 11:04:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-05-19T11:04:27.4363696Z May 19 11:04:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-05-19T11:04:27.4364511Z May 19 11:04:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-05-19T11:04:27.4365354Z May 19 11:04:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-05-19T11:04:27.4366227Z May 19 11:04:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-05-19T11:04:27.4367109Z May 19 11:04:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-05-19T11:04:27.4367841Z May 19 11:04:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-05-19T11:04:27.4368641Z May 19 11:04:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-05-19T11:04:27.4369554Z May 19 11:04:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-05-19T11:04:27.4370325Z May 19 11:04:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-05-19T11:04:27.4371160Z May 19 11:04:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-05-19T11:04:27.4371951Z May 19 11:04:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-05-19T11:04:27.4372694Z May 19 11:04:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-05-19T11:04:27.4373529Z May 19 11:04:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-05-19T11:04:27.4374159Z May 19 11:04:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-05-19T11:04:27.4374853Z May 19 11:04:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-05-19T11:04:27.4376596Z May 19 11:04:27 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task TumblingEventTimeWindows -> Sink: Unnamed (4/4) of job cc123103629741952fdd02bad56c2e0d is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2022-05-19T11:04:27.4377726Z May 19 11:04:27 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-05-19T11:04:27.4378358Z May 19 11:04:27 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-05-19T11:04:27.4379038Z May 19 11:04:27 	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:66)
2022-05-19T11:04:27.4379564Z May 19 11:04:27 	... 48 more
2022-05-19T11:04:27.4380796Z May 19 11:04:27 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint triggering task TumblingEventTimeWindows -> Sink: Unnamed (4/4) of job cc123103629741952fdd02bad56c2e0d is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running.
2022-05-19T11:04:27.4382003Z May 19 11:04:27 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.checkTasksStarted(DefaultCheckpointPlanCalculator.java:143)
2022-05-19T11:04:27.4383094Z May 19 11:04:27 	at org.apache.flink.runtime.checkpoint.DefaultCheckpointPlanCalculator.lambda$calculateCheckpointPlan$1(DefaultCheckpointPlanCalculator.java:105)
2022-05-19T11:04:27.4384048Z May 19 11:04:27 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2022-05-19T11:04:27.4384769Z May 19 11:04:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:444)
2022-05-19T11:04:27.4385535Z May 19 11:04:27 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T11:04:27.4386300Z May 19 11:04:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:444)
2022-05-19T11:04:27.4386997Z May 19 11:04:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:214)
2022-05-19T11:04:27.4387709Z May 19 11:04:27 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T11:04:27.4388426Z May 19 11:04:27 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:164)
2022-05-19T11:04:27.4389066Z May 19 11:04:27 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T11:04:27.4389650Z May 19 11:04:27 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T11:04:27.4390220Z May 19 11:04:27 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T11:04:27.4390801Z May 19 11:04:27 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T11:04:27.4391530Z May 19 11:04:27 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T11:04:27.4392129Z May 19 11:04:27 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T11:04:27.4392741Z May 19 11:04:27 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T11:04:27.4393507Z May 19 11:04:27 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T11:04:27.4394069Z May 19 11:04:27 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T11:04:27.4394592Z May 19 11:04:27 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T11:04:27.4395148Z May 19 11:04:27 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T11:04:27.4395710Z May 19 11:04:27 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T11:04:27.4396250Z May 19 11:04:27 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T11:04:27.4396988Z May 19 11:04:27 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T11:04:27.4397524Z May 19 11:04:27 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T11:04:27.4398029Z May 19 11:04:27 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T11:04:27.4398636Z May 19 11:04:27 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T11:04:27.4399252Z May 19 11:04:27 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T11:04:27.4399892Z May 19 11:04:27 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T11:04:27.4400584Z May 19 11:04:27 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T11:04:27.4401248Z May 19 11:04:27 
2022-05-19T11:04:28.1016282Z May 19 11:04:28 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:04:28.4564936Z May 19 11:04:28 [INFO] Running org.apache.flink.state.api.DataSetMemoryStateBackendWindowITCase
2022-05-19T11:04:41.6169202Z May 19 11:04:41 [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.5 s - in org.apache.flink.state.api.DataSetHashMapStateBackendWindowITCase
2022-05-19T11:04:42.2871933Z May 19 11:04:42 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:04:43.1327751Z May 19 11:04:43 [INFO] Running org.apache.flink.state.api.WritableSavepointWindowITCase
2022-05-19T11:04:52.9166390Z May 19 11:04:52 [INFO] Tests run: 64, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.781 s - in org.apache.flink.state.api.WritableSavepointWindowITCase
2022-05-19T11:04:53.7214732Z May 19 11:04:53 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:04:54.0678436Z May 19 11:04:54 [INFO] Running org.apache.flink.state.api.EmbeddedRocksDBStateBackendWindowITCase
2022-05-19T11:04:59.9090464Z May 19 11:04:59 [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 31.448 s - in org.apache.flink.state.api.DataSetMemoryStateBackendWindowITCase
2022-05-19T11:05:00.5934003Z May 19 11:05:00 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:05:00.9748719Z May 19 11:05:00 [INFO] Running org.apache.flink.state.api.DataSetRocksDBStateBackendWindowITCase
2022-05-19T11:05:27.3736068Z May 19 11:05:27 [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.301 s - in org.apache.flink.state.api.EmbeddedRocksDBStateBackendWindowITCase
2022-05-19T11:05:28.0468187Z May 19 11:05:28 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:05:28.3838535Z May 19 11:05:28 [INFO] Running org.apache.flink.state.api.DataSetSavepointReaderITCase
2022-05-19T11:05:33.2943885Z May 19 11:05:33 [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.315 s - in org.apache.flink.state.api.DataSetRocksDBStateBackendWindowITCase
2022-05-19T11:05:33.9728501Z May 19 11:05:33 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:05:34.3138060Z May 19 11:05:34 [INFO] Running org.apache.flink.state.api.DataSetHashMapStateBackendReaderKeyedStateITCase
2022-05-19T11:05:35.0245739Z May 19 11:05:35 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.636 s - in org.apache.flink.state.api.DataSetSavepointReaderITCase
2022-05-19T11:05:35.6749372Z May 19 11:05:35 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:05:36.5561780Z May 19 11:05:36 [INFO] Running org.apache.flink.state.api.SavepointWriterWindowITCase
2022-05-19T11:05:41.0302157Z May 19 11:05:41 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.712 s - in org.apache.flink.state.api.DataSetHashMapStateBackendReaderKeyedStateITCase
2022-05-19T11:05:41.7015574Z May 19 11:05:41 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-05-19T11:05:42.0966798Z May 19 11:05:42 [INFO] Running org.apache.flink.state.api.DataSetEmbeddedRocksDBStateBackendReaderKeyedStateITCase
2022-05-19T11:05:47.1005888Z May 19 11:05:47 [INFO] Tests run: 32, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.542 s - in org.apache.flink.state.api.SavepointWriterWindowITCase
2022-05-19T11:05:48.8436308Z May 19 11:05:48 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.743 s - in org.apache.flink.state.api.DataSetEmbeddedRocksDBStateBackendReaderKeyedStateITCase
2022-05-19T11:05:49.2465009Z May 19 11:05:49 [INFO] 
2022-05-19T11:05:49.2466675Z May 19 11:05:49 [INFO] Results:
2022-05-19T11:05:49.2467519Z May 19 11:05:49 [INFO] 
2022-05-19T11:05:49.2467898Z May 19 11:05:49 [ERROR] Errors: 
2022-05-19T11:05:49.2469382Z May 19 11:05:49 [ERROR]   HashMapStateBackendWindowITCase>SavepointWindowReaderITCase.testAggregateWindowStateReader:149-
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35839&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 01 03:21:05 UTC 2022,,,,,,,,,,"0|z12iew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Jul/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","01/Dec/22 03:21;Yanfei Lei;Since this hasn't been reproduced in 6 months, I'm going to close this ticket, please reopen it if the bug reproduces again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BroadcastOutputTest tests failed,FLINK-27700,13445814,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,gaoyunhaii,hxbks2ks,hxbks2ks,19/May/22 11:52,10/Jan/23 04:57,04/Jun/24 20:51,10/Jan/23 04:57,ml-2.0.0,,,,,,,,,,,,,,,,,,Library / Machine Learning,,,,,0,stale-assigned,test-stability,,,"
{code:java}
2022-05-19T10:17:34.1561772Z [ERROR] testBroadcastWithMultipleChain[0]  Time elapsed: 3.084 s  <<< ERROR!
2022-05-19T10:17:34.1562657Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.1563188Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.1563788Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.1564309Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.1564811Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.1565264Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1565723Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1566463Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.1566976Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1567447Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1567917Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1568439Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1568892Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.1569401Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.1570217Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.1570905Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.1571490Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1571961Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1572516Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1572944Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1573411Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.1573811Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.1574109Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.1574425Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.1574739Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.1575076Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1575529Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.1576024Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.1576506Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.1576900Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.1577299Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.1577654Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.1578036Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.1578451Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.1578817Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.1579159Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.1579498Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.1579851Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1580423Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.1580852Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1581220Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.1581581Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.1581971Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1582426Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.1582863Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.1583316Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.1583704Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.1584169Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.1584590Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.1585046Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.1585679Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.1586513Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.1587246Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.1587843Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.1588501Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.1589135Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.1589750Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.1590307Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.1590748Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-19T10:17:34.1591168Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-19T10:17:34.1591636Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.1592038Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.1592435Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.1593044Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.1593765Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.1594276Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.1594792Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.1595314Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.1595731Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.1596069Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.1596421Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.1596776Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.1597145Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.1597506Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.1598024Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1598384Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1598698Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.1599000Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.1599391Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.1599734Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.1600051Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.1600374Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.1600687Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.1600965Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.1601193Z 	... 4 more
2022-05-19T10:17:34.1601479Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.1601800Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.1602099Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.1602360Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.1602605Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.1603051Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.1603574Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.1604012Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.1606969Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.1607524Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.1608111Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.1608741Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.1609408Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.1610123Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.1631398Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1653097Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.1653562Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.1653934Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.1654253Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.1654641Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1655170Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.1655713Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.1656175Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.1656669Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1657149Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.1657464Z 		... 3 more
2022-05-19T10:17:34.1657766Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1658085Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.1658366Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.1658665Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.1659103Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.1659651Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.1660257Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.1661048Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.1661682Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.1677929Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.1678593Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.1679166Z 		... 7 more
2022-05-19T10:17:34.1679283Z 
2022-05-19T10:17:34.1679435Z [ERROR] testBroadcastWithMixedOutput[0]  Time elapsed: 1.337 s  <<< ERROR!
2022-05-19T10:17:34.1679825Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.1680312Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.1680990Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.1681497Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.1681922Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.1682360Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1682790Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1683291Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.1683969Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1684438Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1684878Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1685329Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1685770Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.1686278Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.1686891Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.1687582Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.1688161Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1688633Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1689074Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1689524Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1690204Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.1690603Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.1690922Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.1691512Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.1691844Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.1692183Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1692663Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.1693177Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.1693593Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.1694124Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.1694571Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.1695318Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.1695698Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.1697316Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.1697711Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.1698048Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.1698420Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.1698785Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1699192Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.1699606Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1699997Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.1702741Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.1703133Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1703533Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.1703989Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.1704440Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.1704811Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.1705209Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.1705629Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.1706070Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.1706708Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.1707553Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.1708287Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.1708869Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.1709524Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.1710169Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.1710749Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.1711296Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.1711745Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.1712178Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.1712573Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.1712971Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.1713572Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.1714183Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.1714859Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.1715407Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.1715939Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.1716371Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.1716777Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.1717145Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.1717508Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.1717879Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.1718260Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.1718626Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1718986Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1719301Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.1719606Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.1719945Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.1720286Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.1720606Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.1720925Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.1721223Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.1721511Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.1721741Z 	... 4 more
2022-05-19T10:17:34.1721985Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.1722304Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.1722599Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.1722854Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.1723094Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.1723538Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.1724057Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.1724487Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.1725061Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.1725586Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.1726138Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.1726750Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.1727391Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.1727905Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.1728388Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1728851Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.1729258Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.1729621Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.1774584Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.1775089Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1775806Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.1776502Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.1776954Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.1777451Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1778016Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.1778320Z 		... 3 more
2022-05-19T10:17:34.1778629Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1778956Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.1779242Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.1779529Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.1779973Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.1780540Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.1781128Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.1781746Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.1782391Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.1783004Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.1783568Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.1783954Z 		... 7 more
2022-05-19T10:17:34.1784067Z 
2022-05-19T10:17:34.1784266Z [ERROR] testBroadcastWithMixedOutputWithSideOutput[0]  Time elapsed: 1.396 s  <<< ERROR!
2022-05-19T10:17:34.1784696Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.1785155Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.1785726Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.1786254Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.1786676Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.1789034Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1789503Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1790015Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.1790674Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1791145Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1791604Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1792058Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1792496Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.1793005Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.1793635Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.1794327Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.1795026Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1795496Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1795955Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1796396Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1796929Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.1797343Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.1797667Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.1797976Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.1798402Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.1798744Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1799196Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.1799695Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.1800113Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.1800520Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.1800905Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.1801262Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.1801641Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.1802051Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.1802421Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.1802856Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.1803189Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.1803683Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1804076Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.1804494Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1805028Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.1805394Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.1805782Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1806164Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.1806609Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.1807065Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.1807449Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.1807827Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.1808246Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.1808699Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.1809335Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.1843823Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.1844818Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.1845425Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.1846087Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.1846722Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.1847372Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.1847929Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.1848388Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.1848811Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.1849209Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.1849623Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.1850411Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.1851026Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.1851553Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.1855271Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.1855806Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.1856242Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.1856606Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.1856971Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.1857336Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.1857720Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.1858109Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.1858457Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1858824Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1859149Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.1859436Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.1859772Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.1860126Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.1860450Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.1860760Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.1861073Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.1861360Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.1861575Z 	... 4 more
2022-05-19T10:17:34.1861817Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.1862143Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.1862433Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.1862691Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.1862953Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.1863384Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.1863904Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.1864345Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.1864975Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.1865504Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.1866072Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.1866716Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.1867435Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.1867953Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.1868450Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1868941Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.1869347Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.1869732Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.1870047Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.1870422Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1870944Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.1871485Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.1871943Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.1872427Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1872902Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.1873213Z 		... 3 more
2022-05-19T10:17:34.1873514Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1873827Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.1874108Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.1874403Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.1874825Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.1875389Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.1875990Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.1876601Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.1877219Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.1877830Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.1878404Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.1878779Z 		... 7 more
2022-05-19T10:17:34.1878875Z 
2022-05-19T10:17:34.1879072Z [ERROR] testBroadcastWithMultipleResultPartitions[0]  Time elapsed: 1.674 s  <<< ERROR!
2022-05-19T10:17:34.1879496Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.1879969Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.1880525Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.1881051Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.1881540Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.1881987Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1882420Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1882926Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.1883485Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1883956Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1884397Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1884846Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1901846Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.1902395Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.1903016Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.1903712Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.1904295Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1904763Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1905212Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1905658Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1906127Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.1906528Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.1906849Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.1907168Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.1907484Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.1907816Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1908419Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.1908917Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.1909319Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.1909722Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.1910122Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.1910478Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.1910840Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.1911263Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.1911623Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.1911946Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.1912296Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.1912642Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1913031Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.1913429Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1913949Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.1914302Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.1914668Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1915035Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.1915543Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.1915983Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.1916344Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.1916726Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.1917133Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.1917569Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.1918190Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.1919005Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.1919721Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.1945115Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.1945880Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.1946538Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.1947122Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.1947674Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.1948126Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.1948555Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.1948949Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.1949350Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.1949951Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.1950561Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.1951068Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.1951620Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.1952156Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.1952970Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.1953334Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.1953703Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.1954066Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.1954430Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.1954809Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.1955168Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1955530Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.1956010Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.1956310Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.1956649Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.1956991Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.1957309Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.1957629Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.1957988Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.1958279Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.1958502Z 	... 4 more
2022-05-19T10:17:34.1958747Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.1959060Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.1959356Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.1959619Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.1959868Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.1960314Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.1960840Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.1961268Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.1961947Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.1962447Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.1962965Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.1963719Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.1964359Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.1964876Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.1965358Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1965817Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.1966216Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.1966580Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.1966866Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.1967229Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1967989Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.1968531Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.1968973Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.1969456Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.1970176Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.1970494Z 		... 3 more
2022-05-19T10:17:34.1970787Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.1971125Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.1971407Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.1971689Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.1972140Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.1972702Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.1973364Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.1973983Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.1974616Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.1975273Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.1975838Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.1976216Z 		... 7 more
2022-05-19T10:17:34.1976325Z 
2022-05-19T10:17:34.1976492Z [ERROR] testBroadcastWithResultPartition[0]  Time elapsed: 1.802 s  <<< ERROR!
2022-05-19T10:17:34.1976885Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.1977343Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.1977914Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.1978436Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.1978866Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.1979299Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1979744Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1980247Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.1980751Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1981219Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1981784Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1982218Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1982636Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.1983124Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.1983732Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.1984401Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.1984943Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.1985500Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.1985923Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.1986524Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.1986978Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.1987373Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.1987846Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.1988154Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.1988476Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.1988819Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1989286Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.1989845Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.1990272Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.1990686Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.1991078Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.1991441Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.1991871Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.1992300Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.1992675Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.1993017Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.1993378Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.1993724Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.1994125Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.1994552Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1994919Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.1995285Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.1995669Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.1996047Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.1996485Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.1996926Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.1997305Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.1997681Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.1998093Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.1998542Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.1999185Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2000014Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2000748Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2001338Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2001996Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2002625Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2003197Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2003748Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2004204Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2004620Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2005009Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2005413Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2006051Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2006657Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2007293Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2007819Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2008368Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2008787Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2009139Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2009476Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2009991Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2013006Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2013534Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2013885Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2014237Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2014559Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2014843Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2015342Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2015693Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2016006Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2016313Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2016624Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2016911Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2017122Z 	... 4 more
2022-05-19T10:17:34.2017361Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2017693Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2017975Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2018229Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2018481Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2018913Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2019435Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2019872Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2020340Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2020868Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2021432Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2022065Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2022720Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2023229Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2023722Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2024208Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2024607Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2024980Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2025440Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2025590Z 
2022-05-19T10:17:34.2025734Z [ERROR] testBroadcastWithChain[0]  Time elapsed: 1.868 s  <<< ERROR!
2022-05-19T10:17:34.2026099Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2026571Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2027139Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2027725Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2028151Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2028598Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2029046Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2029657Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2030163Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2030617Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2031066Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2031492Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2031927Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2032674Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2033274Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2033945Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2034685Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2035152Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2035594Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2036042Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2036515Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2036927Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2037230Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2037549Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2037869Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2038205Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2038685Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2039325Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2039912Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2040372Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2040789Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2041200Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2041574Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2042012Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2042455Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2042804Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2043153Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2043864Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2044279Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2044747Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2045135Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2045501Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2045889Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2046260Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2046705Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2047144Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2047513Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2047909Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2048328Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2048783Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2049407Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2050435Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2051179Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2051776Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2052421Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2053067Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2053647Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2054195Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2054640Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2055071Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2055475Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2055867Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2056472Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2057079Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2057600Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2058129Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2058658Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2059081Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2059510Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2059864Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2060230Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2060611Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2060977Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2061339Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2061744Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2062062Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2062355Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2062687Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2063043Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2063348Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2063667Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2063976Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2064247Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2064470Z 	... 4 more
2022-05-19T10:17:34.2064707Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2065020Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2065316Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2065569Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2065808Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2066282Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2066838Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2067286Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2067744Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2068282Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2068853Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2069502Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2070148Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2070677Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2071186Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2071665Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2072079Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2072452Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2072758Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2073120Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2073659Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.2074194Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.2074651Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.2075123Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2075607Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.2075964Z 		... 3 more
2022-05-19T10:17:34.2076254Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2076577Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.2076855Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.2077141Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.2077619Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.2078181Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.2078777Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.2079375Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.2080006Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.2080612Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.2081185Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.2081554Z 		... 7 more
2022-05-19T10:17:34.2081664Z 
2022-05-19T10:17:34.2081833Z [ERROR] testBroadcastWithMultipleChain[1]  Time elapsed: 2.042 s  <<< ERROR!
2022-05-19T10:17:34.2082223Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2082680Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2083245Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2083768Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2084199Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2084631Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2085078Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2085582Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2086101Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2086551Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2087007Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2087453Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2087913Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2088403Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2089026Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2089713Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2090399Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2090852Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2091308Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2091759Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2092283Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2092696Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2093015Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2093454Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2093754Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2094090Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2094600Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2095191Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2095760Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2096166Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2096564Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2096904Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2097279Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2097700Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2098062Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2098394Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2098746Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2099101Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2099477Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2099890Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2100431Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2100792Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2101162Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2101537Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2101991Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2102424Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2102807Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2103257Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2103679Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2104124Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2104759Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2105600Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2106364Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2106943Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2107593Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2108239Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2108855Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2109408Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2109860Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2110288Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2110670Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2111122Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2111728Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2112346Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2112852Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2113396Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2113927Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2114338Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2114696Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2115057Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2115419Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2115789Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2116173Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2116532Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2116876Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2117204Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2117502Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2117838Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2118180Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2118494Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2118816Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2119115Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2119404Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2119632Z 	... 4 more
2022-05-19T10:17:34.2119863Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2120312Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2120605Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2120842Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2121088Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2121517Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2122027Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2122440Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2122896Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2123418Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2123954Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2124575Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2125260Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2125777Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2126248Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2126721Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2127122Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2127524Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2127811Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2128173Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2128694Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.2129202Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.2129643Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.2130321Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2130801Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.2131085Z 		... 3 more
2022-05-19T10:17:34.2131380Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2131700Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.2131962Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.2132255Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.2132683Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.2133229Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.2133801Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.2134395Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.2135006Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.2135597Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.2136139Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.2136503Z 		... 7 more
2022-05-19T10:17:34.2136609Z 
2022-05-19T10:17:34.2136765Z [ERROR] testBroadcastWithMixedOutput[1]  Time elapsed: 1.879 s  <<< ERROR!
2022-05-19T10:17:34.2137131Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2137590Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2138143Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2138650Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2139060Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2139496Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2139932Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2140412Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2141027Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2141530Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2141948Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2142347Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2142761Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2143461Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2144417Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2145059Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2145591Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2146026Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2146446Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2146845Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2147279Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2147657Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2147941Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2148239Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2148532Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2148841Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2149278Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2149749Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2150144Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2150683Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2151084Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2151604Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2152019Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2152481Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2152923Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2153339Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2153728Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2154327Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2154752Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2155247Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2155701Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2156106Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2156602Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2157056Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2157572Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2158147Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2158627Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2159090Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2159530Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2160067Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2160838Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2161786Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2162542Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2163220Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2163946Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2164690Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2165289Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2165917Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2166450Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2166956Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2167452Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2167932Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2168620Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2169259Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2170019Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2170679Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2171294Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2171743Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2172175Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2172653Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2173039Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2173504Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2173963Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2174403Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2174932Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2175324Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2175683Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2176045Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2176460Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2176924Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2177421Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2177803Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2178270Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2178613Z 	... 4 more
2022-05-19T10:17:34.2178880Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2179282Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2203623Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2203977Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2204228Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2204830Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2205366Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2205814Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2206278Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2206831Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2207406Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2208049Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2208689Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2209221Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2209723Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2210389Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2210813Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2211191Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2211510Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2211875Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2212421Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.2212959Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.2213423Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.2213893Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2214380Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.2214688Z 		... 3 more
2022-05-19T10:17:34.2214975Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2215313Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.2215592Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.2215875Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.2216312Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.2216871Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.2217478Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.2218079Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.2218704Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.2219311Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.2219968Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.2220334Z 		... 7 more
2022-05-19T10:17:34.2220441Z 
2022-05-19T10:17:34.2220639Z [ERROR] testBroadcastWithMixedOutputWithSideOutput[1]  Time elapsed: 1.831 s  <<< ERROR!
2022-05-19T10:17:34.2221066Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2221585Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2222157Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2222680Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2223114Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2223548Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2224002Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2224507Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2225040Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2225491Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2225951Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2226399Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2226828Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2227331Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2227957Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2228650Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2229213Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2229683Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2230140Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2230590Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2231048Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2231456Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2231780Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2232089Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2232409Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2232752Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2233232Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2233727Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2234159Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2234578Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2234970Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2235333Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2235780Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2236223Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2236580Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2236923Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2237288Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2237682Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2238088Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2238514Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2238904Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2239255Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2239634Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2240004Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2240437Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2240879Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2241304Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2241701Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2242105Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2242560Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2243203Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2244046Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2244764Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2245707Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2246372Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2247021Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2247585Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2247847Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2248024Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2248259Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2248400Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2248655Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2248995Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2249240Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2249485Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2249839Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2250075Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2250336Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2250506Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2250682Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2250855Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2251030Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2251269Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2251438Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2251603Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2251741Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2251881Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2252059Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2252217Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2252340Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2252499Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2252631Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2252764Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2252834Z 	... 4 more
2022-05-19T10:17:34.2252985Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2253146Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2253269Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2253369Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2253488Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2253788Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2253993Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2254212Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2254448Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2254735Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2255001Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2255345Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2255631Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2255854Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2256111Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2256324Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2256503Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2256675Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2256791Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2257029Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2257309Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.2257536Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.2257746Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.2258047Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2258261Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.2258331Z 		... 3 more
2022-05-19T10:17:34.2258530Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2258646Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.2258818Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.2258954Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.2259226Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.2259499Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.2259810Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.2260096Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.2260421Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.2260692Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.2260982Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.2261039Z 		... 7 more
2022-05-19T10:17:34.2261048Z 
2022-05-19T10:17:34.2261242Z [ERROR] testBroadcastWithMultipleResultPartitions[1]  Time elapsed: 1.83 s  <<< ERROR!
2022-05-19T10:17:34.2261451Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2261697Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2261999Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2262215Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2262420Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2262644Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2262854Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2263116Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2263344Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2263559Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2263777Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2263987Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2264200Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2264469Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2264805Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2265146Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2265363Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2265579Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2265799Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2266051Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2266290Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2266436Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2266582Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2266732Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2266913Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2267092Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2267370Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2267585Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2267779Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2267981Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2268168Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2268321Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2268531Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2268730Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2268879Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2269051Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2269216Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2269390Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2269596Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2269791Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2269954Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2270115Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2270306Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2270473Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2270729Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2270902Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2271086Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2271271Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2271477Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2271690Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2272080Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2272524Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2272804Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2273104Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2273445Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2273777Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2274051Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2274309Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2274481Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2274742Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2274887Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2275131Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2275469Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2275729Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2275974Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2276254Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2276488Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2276645Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2276814Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2276983Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2277148Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2277332Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2277502Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2277670Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2277836Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2277960Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2278096Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2278267Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2278427Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2278563Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2278822Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2278947Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2279234Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2279291Z 	... 4 more
2022-05-19T10:17:34.2279431Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2279595Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2279712Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2279825Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2279938Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2280239Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2280442Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2280647Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2280880Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2281166Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2281432Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2281831Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2282117Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2282340Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2282594Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2282855Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2283023Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2283196Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2283311Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2283543Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2283838Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.2284064Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.2284273Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.2284525Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2284728Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.2284797Z 		... 3 more
2022-05-19T10:17:34.2284996Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2285111Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.2285250Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.2285385Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.2285662Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.2285931Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.2286229Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.2286516Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.2286842Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.2287106Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.2287398Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.2287469Z 		... 7 more
2022-05-19T10:17:34.2287477Z 
2022-05-19T10:17:34.2287646Z [ERROR] testBroadcastWithResultPartition[1]  Time elapsed: 1.95 s  <<< ERROR!
2022-05-19T10:17:34.2287852Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2288097Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2288387Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2288595Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2288800Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2289018Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2289226Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2289501Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2290086Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2290310Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2290529Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2290722Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2290990Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2291267Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2291612Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2291952Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2292183Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2292397Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2292618Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2292824Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2293056Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2293211Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2293358Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2293506Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2293656Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2293835Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2294115Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2294333Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2294509Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2294721Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2294908Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2295064Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2295276Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2295484Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2295634Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2295807Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2295980Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2296140Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2296345Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2296551Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2296715Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2296887Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2297077Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2297305Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2297561Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2297723Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2297911Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2298098Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2298349Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2298579Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2298970Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2299422Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2299704Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2300002Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2300329Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2300619Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2300894Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2301158Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2301330Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2301564Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2301709Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2301955Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2302293Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2302536Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2302785Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2303065Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2303303Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2303475Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2303641Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2303813Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2303981Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2304155Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2304323Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2304489Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2304655Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2304790Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2304927Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2305099Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2305257Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2305425Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2305584Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2305715Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2305848Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2305918Z 	... 4 more
2022-05-19T10:17:34.2306062Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2306222Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2306337Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2306480Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2306598Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2306897Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2307101Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2307316Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2307552Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2307837Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2308100Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2308446Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2308737Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2308962Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2309221Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2309439Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2309617Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2309791Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2309899Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2309908Z 
2022-05-19T10:17:34.2310051Z [ERROR] testBroadcastWithChain[1]  Time elapsed: 1.332 s  <<< ERROR!
2022-05-19T10:17:34.2310244Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-05-19T10:17:34.2310487Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-05-19T10:17:34.2310786Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2022-05-19T10:17:34.2310994Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-05-19T10:17:34.2311199Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-05-19T10:17:34.2311420Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2311629Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2311909Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2022-05-19T10:17:34.2312139Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2312345Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2312564Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2312770Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2312985Z 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-05-19T10:17:34.2313252Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-05-19T10:17:34.2313675Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-05-19T10:17:34.2314011Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-05-19T10:17:34.2314241Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-05-19T10:17:34.2314494Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-05-19T10:17:34.2314703Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-05-19T10:17:34.2314911Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-05-19T10:17:34.2315152Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-05-19T10:17:34.2315309Z 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-05-19T10:17:34.2315459Z 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-05-19T10:17:34.2315610Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-05-19T10:17:34.2315760Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-05-19T10:17:34.2315936Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2316208Z 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-05-19T10:17:34.2316426Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-05-19T10:17:34.2316620Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-05-19T10:17:34.2316822Z 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-05-19T10:17:34.2317012Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-05-19T10:17:34.2317171Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-05-19T10:17:34.2317387Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2022-05-19T10:17:34.2317601Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2022-05-19T10:17:34.2317751Z 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2022-05-19T10:17:34.2317909Z 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2022-05-19T10:17:34.2318077Z 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2022-05-19T10:17:34.2318249Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-05-19T10:17:34.2318452Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2022-05-19T10:17:34.2318654Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2318820Z 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2022-05-19T10:17:34.2318995Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2022-05-19T10:17:34.2319185Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2022-05-19T10:17:34.2319340Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2022-05-19T10:17:34.2319597Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2022-05-19T10:17:34.2319774Z 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-05-19T10:17:34.2319958Z 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-05-19T10:17:34.2320142Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-05-19T10:17:34.2320345Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-19T10:17:34.2320567Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-05-19T10:17:34.2321002Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-05-19T10:17:34.2321443Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-05-19T10:17:34.2321709Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2022-05-19T10:17:34.2322048Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2022-05-19T10:17:34.2322389Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2022-05-19T10:17:34.2322676Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2022-05-19T10:17:34.2322950Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2022-05-19T10:17:34.2323212Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2022-05-19T10:17:34.2323382Z 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2022-05-19T10:17:34.2323736Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-19T10:17:34.2323872Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-19T10:17:34.2324096Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2022-05-19T10:17:34.2324427Z 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-05-19T10:17:34.2324678Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2022-05-19T10:17:34.2324915Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-05-19T10:17:34.2325355Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-05-19T10:17:34.2325595Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-05-19T10:17:34.2325768Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-05-19T10:17:34.2325934Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-05-19T10:17:34.2326090Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-05-19T10:17:34.2326256Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-05-19T10:17:34.2326445Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-05-19T10:17:34.2326613Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-05-19T10:17:34.2326778Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2326942Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-05-19T10:17:34.2327079Z 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-05-19T10:17:34.2327221Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-05-19T10:17:34.2327382Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-05-19T10:17:34.2327538Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-05-19T10:17:34.2327674Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-05-19T10:17:34.2327832Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-05-19T10:17:34.2327966Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-05-19T10:17:34.2328095Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-05-19T10:17:34.2328168Z 	... 4 more
2022-05-19T10:17:34.2328310Z Caused by: java.lang.IllegalThreadStateException
2022-05-19T10:17:34.2328458Z 	at java.lang.ThreadGroup.addUnstarted(ThreadGroup.java:867)
2022-05-19T10:17:34.2328571Z 	at java.lang.Thread.init(Thread.java:407)
2022-05-19T10:17:34.2328685Z 	at java.lang.Thread.init(Thread.java:351)
2022-05-19T10:17:34.2328798Z 	at java.lang.Thread.<init>(Thread.java:601)
2022-05-19T10:17:34.2329146Z 	at org.apache.flink.runtime.taskmanager.DispatcherThreadFactory.newThread(DispatcherThreadFactory.java:48)
2022-05-19T10:17:34.2329349Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.<init>(ThreadPoolExecutor.java:619)
2022-05-19T10:17:34.2329566Z 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:932)
2022-05-19T10:17:34.2329908Z 	at java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1603)
2022-05-19T10:17:34.2330238Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:334)
2022-05-19T10:17:34.2330510Z 	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533)
2022-05-19T10:17:34.2330871Z 	at org.apache.flink.streaming.runtime.tasks.SystemProcessingTimeService.registerTimer(SystemProcessingTimeService.java:112)
2022-05-19T10:17:34.2331160Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.scheduleBufferDebloater(StreamTask.java:779)
2022-05-19T10:17:34.2331389Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:758)
2022-05-19T10:17:34.2331645Z 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2331857Z 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2022-05-19T10:17:34.2332038Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2022-05-19T10:17:34.2332197Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2022-05-19T10:17:34.2332311Z 	at java.lang.Thread.run(Thread.java:750)
2022-05-19T10:17:34.2332546Z 	Suppressed: java.lang.Exception: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2332838Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1024)
2022-05-19T10:17:34.2333072Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:928)
2022-05-19T10:17:34.2333280Z 		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:940)
2022-05-19T10:17:34.2333536Z 		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2022-05-19T10:17:34.2333758Z 		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:940)
2022-05-19T10:17:34.2333830Z 		... 3 more
2022-05-19T10:17:34.2334016Z 	Caused by: java.lang.AssertionError: Number of received records does not consistent expected:<100> but was:<0>
2022-05-19T10:17:34.2334132Z 		at org.junit.Assert.fail(Assert.java:89)
2022-05-19T10:17:34.2334273Z 		at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-19T10:17:34.2334406Z 		at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-19T10:17:34.2334679Z 		at org.apache.flink.iteration.broadcast.BroadcastOutputTest$CheckResultSink.close(BroadcastOutputTest.java:239)
2022-05-19T10:17:34.2334949Z 		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
2022-05-19T10:17:34.2335260Z 		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114)
2022-05-19T10:17:34.2335544Z 		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:141)
2022-05-19T10:17:34.2335859Z 		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:127)
2022-05-19T10:17:34.2336123Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1035)
2022-05-19T10:17:34.2336411Z 		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:1021)
2022-05-19T10:17:34.2336479Z 		... 7 more
2022-05-19T10:17:34.2336487Z 
2022-05-19T10:17:34.2336762Z [INFO] Running org.apache.flink.iteration.compile.DraftExecutionEnvironmentSwitchWrapperTest
2022-05-19T10:17:34.2339417Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 s - in org.apache.flink.iteration.compile.DraftExecutionEnvironmentSwitchWrapperTest
2022-05-19T10:17:34.2339808Z [INFO] Running org.apache.flink.iteration.IterationConstructionTest
2022-05-19T10:17:34.3146359Z [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.115 s - in org.apache.flink.iteration.IterationConstructionTest
2022-05-19T10:17:34.3146820Z [INFO] Running org.apache.flink.iteration.progresstrack.OperatorEpochWatermarkTrackerTest
2022-05-19T10:17:34.3161293Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 s - in org.apache.flink.iteration.progresstrack.OperatorEpochWatermarkTrackerTest
2022-05-19T10:17:34.6772965Z [INFO] 
2022-05-19T10:17:34.6773333Z [INFO] Results:
2022-05-19T10:17:34.6773597Z [INFO] 
2022-05-19T10:17:34.6773787Z [ERROR] Errors: 
2022-05-19T10:17:34.6774912Z [ERROR]   BroadcastOutputTest.testBroadcastWithChain » JobExecution Job execution failed...
2022-05-19T10:17:34.6775568Z [ERROR]   BroadcastOutputTest.testBroadcastWithChain » JobExecution Job execution failed...
2022-05-19T10:17:34.6776205Z [ERROR]   BroadcastOutputTest.testBroadcastWithMixedOutputWithSideOutput » JobExecution ...
2022-05-19T10:17:34.6776923Z [ERROR]   BroadcastOutputTest.testBroadcastWithMixedOutputWithSideOutput » JobExecution ...
2022-05-19T10:17:34.6777577Z [ERROR]   BroadcastOutputTest.testBroadcastWithMixedOutput » JobExecution Job execution ...
2022-05-19T10:17:34.6778199Z [ERROR]   BroadcastOutputTest.testBroadcastWithMixedOutput » JobExecution Job execution ...
2022-05-19T10:17:34.6778808Z [ERROR]   BroadcastOutputTest.testBroadcastWithMultipleChain » JobExecution Job executio...
2022-05-19T10:17:34.6779439Z [ERROR]   BroadcastOutputTest.testBroadcastWithMultipleChain » JobExecution Job executio...
2022-05-19T10:17:34.6780096Z [ERROR]   BroadcastOutputTest.testBroadcastWithMultipleResultPartitions » JobExecution J...
2022-05-19T10:17:34.6780774Z [ERROR]   BroadcastOutputTest.testBroadcastWithMultipleResultPartitions » JobExecution J...
2022-05-19T10:17:34.6781420Z [ERROR]   BroadcastOutputTest.testBroadcastWithResultPartition » JobExecution Job execut...
2022-05-19T10:17:34.6782061Z [ERROR]   BroadcastOutputTest.testBroadcastWithResultPartition » JobExecution Job execut...
{code}
https://github.com/apache/flink-ml/runs/6505261269?check_suite_focus=true
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 05 10:39:16 UTC 2022,,,,,,,,,,"0|z12ico:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align atPublishTime method of StopCursor class for Pulsar connector,FLINK-27699,13445813,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,19/May/22 11:49,21/May/22 15:20,04/Jun/24 20:51,21/May/22 15:20,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,Connectors / Pulsar,,,,0,pull-request-available,,,,"StopCursor#atEventTime is deprecated, align to StopCursor#{color:#172b4d}atPublishTime.{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 21 15:20:09 UTC 2022,,,,,,,,,,"0|z12icg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 15:20;dianfu;Merged to master via 18a967f8ad7b22c2942e227fb84f08f552660b5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-api-java-bridge,FLINK-27698,13445794,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,19/May/22 09:03,20/May/22 08:01,04/Jun/24 20:51,20/May/22 08:01,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 08:01:39 UTC 2022,,,,,,,,,,"0|z12i88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 08:01;chesnay;master: d9932327dcb9eb9fa993621531047130b5bc49a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restoring from a checkpoint will start duplicated application when mixing use streaming sinks and sql sinks in code.,FLINK-27697,13445791,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,brucekellan,brucekellan,19/May/22 08:40,19/May/22 08:40,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"I mixing use `flink sql` and `DataStream` with two sink. When I restore from a checkpoint, it restart two application.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-19 08:40:33.0,,,,,,,,,,"0|z12i7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add bin-pack strategy to split the whole bucket data files into several small splits,FLINK-27696,13445775,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,openinx,openinx,19/May/22 07:21,25/Aug/22 02:30,04/Jun/24 20:51,21/Jun/22 05:47,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"We don't have to assign each task with a whole bucket data files. Instead, we can use some algorithm ( such as bin-packing) to split the whole bucket data files into multiple fragments to improve the job parallelism.

For merge tree table:
Suppose now there are files: [1, 2] [3, 4] [5, 180] [5, 190] [200, 600] [210, 700]
Files without intersection are not related, we do not need to put all files into one split, we can slice into multiple splits, multiple parallelism execution is faster. Nor can we slice too fine, we should make each split as large as possible with 128 MB, so use BinPack to slice, the final result will be:
 * split1: [1, 2] [3, 4]
 * split2: [5, 180] [5, 190]
 * split3: [200, 600] [210, 700]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 21 05:47:43 UTC 2022,,,,,,,,,,"0|z12i40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/22 05:47;lzljs3620320;master: c80f600c149fc90f8522f43af67f0ab2a713b57f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTransactionLogITCase failed on azure due to Could not find a valid Docker environment,FLINK-27695,13445767,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Cannot Reproduce,,hxbks2ks,hxbks2ks,19/May/22 06:18,16/Oct/23 12:15,04/Jun/24 20:51,16/Oct/23 12:15,1.14.4,,,,,,,,,,,,,,,,,,Build System / Azure Pipelines,Connectors / Kafka,,,,0,auto-deprioritized-major,auto-deprioritized-minor,test-stability,,"
{code:java}
022-05-19T02:04:23.9190098Z May 19 02:04:23 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.404 s <<< FAILURE! - in org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase
2022-05-19T02:04:23.9191182Z May 19 02:04:23 [ERROR] org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase  Time elapsed: 7.404 s  <<< ERROR!
2022-05-19T02:04:23.9192250Z May 19 02:04:23 java.lang.IllegalStateException: Could not find a valid Docker environment. Please see logs and check configuration
2022-05-19T02:04:23.9193144Z May 19 02:04:23 	at org.testcontainers.dockerclient.DockerClientProviderStrategy.lambda$getFirstValidStrategy$4(DockerClientProviderStrategy.java:156)
2022-05-19T02:04:23.9194653Z May 19 02:04:23 	at java.util.Optional.orElseThrow(Optional.java:290)
2022-05-19T02:04:23.9196179Z May 19 02:04:23 	at org.testcontainers.dockerclient.DockerClientProviderStrategy.getFirstValidStrategy(DockerClientProviderStrategy.java:148)
2022-05-19T02:04:23.9197995Z May 19 02:04:23 	at org.testcontainers.DockerClientFactory.getOrInitializeStrategy(DockerClientFactory.java:146)
2022-05-19T02:04:23.9199486Z May 19 02:04:23 	at org.testcontainers.DockerClientFactory.client(DockerClientFactory.java:188)
2022-05-19T02:04:23.9200666Z May 19 02:04:23 	at org.testcontainers.DockerClientFactory$1.getDockerClient(DockerClientFactory.java:101)
2022-05-19T02:04:23.9202109Z May 19 02:04:23 	at com.github.dockerjava.api.DockerClientDelegate.authConfig(DockerClientDelegate.java:107)
2022-05-19T02:04:23.9203065Z May 19 02:04:23 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:316)
2022-05-19T02:04:23.9204641Z May 19 02:04:23 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1066)
2022-05-19T02:04:23.9205765Z May 19 02:04:23 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:29)
2022-05-19T02:04:23.9206568Z May 19 02:04:23 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-19T02:04:23.9207497Z May 19 02:04:23 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-19T02:04:23.9208246Z May 19 02:04:23 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-19T02:04:23.9208887Z May 19 02:04:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-19T02:04:23.9209691Z May 19 02:04:23 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-19T02:04:23.9210490Z May 19 02:04:23 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
2022-05-19T02:04:23.9211246Z May 19 02:04:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-05-19T02:04:23.9211989Z May 19 02:04:23 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-05-19T02:04:23.9212682Z May 19 02:04:23 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-05-19T02:04:23.9213391Z May 19 02:04:23 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2022-05-19T02:04:23.9214305Z May 19 02:04:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-05-19T02:04:23.9215044Z May 19 02:04:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-05-19T02:04:23.9215809Z May 19 02:04:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-05-19T02:04:23.9216576Z May 19 02:04:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-05-19T02:04:23.9217523Z May 19 02:04:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-05-19T02:04:23.9218275Z May 19 02:04:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-05-19T02:04:23.9219099Z May 19 02:04:23 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
2022-05-19T02:04:23.9220028Z May 19 02:04:23 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
2022-05-19T02:04:23.9220795Z May 19 02:04:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
2022-05-19T02:04:23.9221598Z May 19 02:04:23 	at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
2022-05-19T02:04:23.9222614Z May 19 02:04:23 	at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
2022-05-19T02:04:23.9223439Z May 19 02:04:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
2022-05-19T02:04:23.9224221Z May 19 02:04:23 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
2022-05-19T02:04:23.9225082Z May 19 02:04:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
2022-05-19T02:04:23.9225971Z May 19 02:04:23 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-05-19T02:04:23.9226910Z May 19 02:04:23 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2022-05-19T02:04:23.9227916Z May 19 02:04:23 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2022-05-19T02:04:23.9228710Z May 19 02:04:23 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2022-05-19T02:04:23.9229468Z May 19 02:04:23 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35816&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=f7d83ad5-3324-5307-0eb3-819065cdcb38
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 19 22:34:58 UTC 2023,,,,,,,,,,"0|z12i28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Jul/22 22:38;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","11/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Aug/23 22:34;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move lint-python log location,FLINK-27694,13445766,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,19/May/22 06:18,21/May/22 09:33,04/Jun/24 20:51,21/May/22 09:33,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,Test Infrastructure,,,,0,pull-request-available,,,,"Some logs are in the wrong location, we need to move them to the inside of the 'if' statement.

 

before
{code:java}
print_function ""STEP"" ""installing wget...""
if [ $STEP -lt 1 ]; then
  install_wget ${SUPPORT_OS[$os_index]}
  STEP=1
  checkpoint_stage $STAGE $STEP
fi
print_function ""STEP"" ""install wget... [SUCCESS]"" {code}
after
{code:java}
if [ $STEP -lt 1 ]; then
  print_function ""STEP"" ""installing wget...""
  install_wget ${SUPPORT_OS[$os_index]}
  STEP=1
  checkpoint_stage $STAGE $STEP
  print_function ""STEP"" ""install wget... [SUCCESS]"" 
fi
{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 21 09:33:37 UTC 2022,,,,,,,,,,"0|z12i20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/May/22 09:33;dianfu;Merged to master via 26deac8494d2f177898ec3dce3bfe7a2753c4849;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support local recovery for non-materialized part(write, restore, discard)",FLINK-27693,13445765,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,19/May/22 06:18,09/Aug/22 10:32,04/Jun/24 20:51,09/Aug/22 10:32,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Support local recovery for non-materialized part(write, restore, discard)",,,,,,,,,,,,,,FLINK-25458,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 10:32:43 UTC 2022,,,,,,,,,,"0|z12i1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 10:32;roman;Merged as 1f9632a07199854c0225bd7f416c038fbf59abe0..52eb7e76b5d66ff5c4d9d4af8a213b5b8f9f8322;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support local recovery for materialized part(write, restore, discard)",FLINK-27692,13445764,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,19/May/22 06:15,29/Dec/23 02:23,04/Jun/24 20:51,26/Jul/22 20:25,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"Support local recovery for materialized part(write, restore, discard)",,,,,,,,,,,,,,FLINK-25458,,,,,,,,,,,,,,,,,FLINK-33957,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 20:25:32 UTC 2022,,,,,,,,,,"0|z12i1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 20:25;roman;Merged into master as aef75be34d99c737f5c565703a971027ac44f855..52519a8eb695c9523c546439c66910b15f19be20
Thanks for the contribution [~Yanfei Lei]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RankHarnessTest. testUpdateRankWithRowNumberSortKeyDropsToNotLast test failed with result mismatch,FLINK-27691,13445763,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxbks2ks,hxbks2ks,19/May/22 06:11,29/Dec/22 02:01,04/Jun/24 20:51,29/Dec/22 02:01,1.16.0,1.17.0,,,,,,,,,,,,,1.16.1,1.17.0,,,Table SQL / Runtime,,,,,0,pull-request-available,test-stability,,,"
{code:java}
2022-05-19T04:34:04.2677138Z May 19 04:34:04 [ERROR] RankHarnessTest.testUpdateRankWithRowNumberSortKeyDropsToNotLast
2022-05-19T04:34:04.2689553Z May 19 04:34:04 [ERROR]   Run 1: [result mismatch] 
2022-05-19T04:34:04.2690614Z May 19 04:34:04 expected: [+I(a,1,100,1),
2022-05-19T04:34:04.2691128Z May 19 04:34:04     +I(b,1,90,2),
2022-05-19T04:34:04.2691552Z May 19 04:34:04     +I(c,1,90,3),
2022-05-19T04:34:04.2692235Z May 19 04:34:04     +I(d,1,80,4),
2022-05-19T04:34:04.2692634Z May 19 04:34:04     +I(e,1,80,5),
2022-05-19T04:34:04.2693060Z May 19 04:34:04     +I(f,1,70,6),
2022-05-19T04:34:04.2693468Z May 19 04:34:04     +U(b,1,80,5),
2022-05-19T04:34:04.2693874Z May 19 04:34:04     +U(c,1,90,2),
2022-05-19T04:34:04.2694282Z May 19 04:34:04     +U(d,1,80,3),
2022-05-19T04:34:04.2694670Z May 19 04:34:04     +U(e,1,80,4),
2022-05-19T04:34:04.2696097Z May 19 04:34:04     -U(b,1,90,2),
2022-05-19T04:34:04.2696718Z May 19 04:34:04     -U(c,1,90,3),
2022-05-19T04:34:04.2697298Z May 19 04:34:04     -U(d,1,80,4),
2022-05-19T04:34:04.2698102Z May 19 04:34:04     -U(e,1,80,5)]
2022-05-19T04:34:04.2698758Z May 19 04:34:04  but was: [+I(a,1,100,1),
2022-05-19T04:34:04.2699189Z May 19 04:34:04     +I(b,1,90,1),
2022-05-19T04:34:04.2699607Z May 19 04:34:04     +I(c,1,90,2),
2022-05-19T04:34:04.2700017Z May 19 04:34:04     +I(d,1,80,3),
2022-05-19T04:34:04.2712164Z May 19 04:34:04     +I(e,1,80,4),
2022-05-19T04:34:04.2712777Z May 19 04:34:04     +I(f,1,70,5),
2022-05-19T04:34:04.2713191Z May 19 04:34:04     +U(b,1,80,4),
2022-05-19T04:34:04.2713621Z May 19 04:34:04     +U(c,1,90,1),
2022-05-19T04:34:04.2714029Z May 19 04:34:04     +U(d,1,80,2),
2022-05-19T04:34:04.2714435Z May 19 04:34:04     +U(e,1,80,3),
2022-05-19T04:34:04.2715272Z May 19 04:34:04     -U(b,1,90,1),
2022-05-19T04:34:04.2715847Z May 19 04:34:04     -U(c,1,90,2),
2022-05-19T04:34:04.2716420Z May 19 04:34:04     -U(d,1,80,3),
2022-05-19T04:34:04.2716990Z May 19 04:34:04     -U(e,1,80,4)]
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35815&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=10445
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 28 09:50:30 UTC 2022,,,,,,,,,,"0|z12i1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 07:31;godfreyhe;cc [~lzljs3620320] could you help fixing this?;;;","31/May/22 08:13;lzljs3620320;Ah... I try to re-produce it, but can not work, It appears to be a non-random test and should not be unstable.

[~lincoln.86xy] Do you have some comments? I don't know FLINK-24704 is related.;;;","31/May/22 09:09;lincoln.86xy;[~lzljs3620320] Yes, this is a deterministic test and shouldn't get random result. I can't reproduce it locally yet that done a 2000 times repeat testing. And I'll try to  analyze other possibilities.;;;","31/May/22 09:15;godfreyhe;Thanks [~lincoln.86xy];;;","16/Jun/22 12:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36764&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","05/Jul/22 07:23;martijnvisser;Downgrading to Major due to lack of occurrences ;;;","21/Jul/22 01:52;kevin.cyj;[https://dev.azure.com/kevin-flink/flink/_build/results?buildId=582&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f];;;","26/Jul/22 06:39;renqs;An instance on my own Azure pipeline:
https://dev.azure.com/renqs/Apache%20Flink/_build/results?buildId=345&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f&l=10284;;;","09/Aug/22 03:47;leonard;https://dev.azure.com/leonardBang/Azure_CI/_build/results?buildId=713&view=ms.vss-test-web.build-test-results-tab&runId=20042&resultId=111227&paneView=debug;;;","09/Aug/22 08:18;yunfengzhou;https://dev.azure.com/yurizhouyunfeng/Flink/_build/results?buildId=50&view=ms.vss-test-web.build-test-results-tab&runId=2840&resultId=111281&paneView=debug;;;","15/Aug/22 06:10;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39961&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","15/Aug/22 06:11;hxbks2ks;Hi [~lincoln.86xy], could you help take an another look? Thanks.;;;","19/Aug/22 10:17;lincoln.86xy;[~hxbks2ks] seems happened when under some workload in ci machine, still cannot reproduce it locally, I'll continue looking into it.;;;","13/Sep/22 03:15;hxb;Given that it hasn't appeared for a month, I will close this issue. Please reopen the ticket if the case appears again. ;;;","02/Dec/22 10:56;mapohl;Reopening the issue. We've seen another test failure of that kind:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43664&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13092;;;","22/Dec/22 08:29;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44160&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12641;;;","22/Dec/22 08:29;mapohl;[~lzljs3620320] [~godfreyhe] Could you have a look at it?;;;","22/Dec/22 12:49;lincoln.86xy;[~mapohl] I think it is most probably caused by the unnecessary state ttl of RankHarnessTest which will lead unstable result when execution gets slow (and I checked recent failures, all of their execution duration is greater than 2 second while state ttl in the test is 1 second). I submitted a pr, hope this can fix this unstable case.

;;;","23/Dec/22 07:48;mapohl;Thanks [~lincoln.86xy]. Could you find someone to review this PR to double-check whether that's reasonable?;;;","23/Dec/22 08:10;leonard;I'll help review this PR, [~mapohl];;;","28/Dec/22 09:50;leonard;master(1.17):c07167b6f01296da11176818382fa9cdf6f985da
1.16:31f1e235f878a3dcbcf3358a2be92e95c1cef4b6;;;",,,,,,,,,,,
Add Python documentation and examples for Pulsar connector,FLINK-27690,13445748,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,19/May/22 03:48,25/May/22 01:35,04/Jun/24 20:51,25/May/22 01:35,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,Documentation,Examples,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 01:35:07 UTC 2022,,,,,,,,,,"0|z12hy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 03:48;ana4;I would like to take this. [~dianfu] ;;;","19/May/22 05:19;dianfu;[~ana4] Thanks. Have assigned it to you~;;;","25/May/22 01:35;dianfu;Merged to master via 8a2a396970156f1ddecf88a6498162506eb25069;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Connector support PulsarSchema,FLINK-27689,13445746,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ana4,ana4,19/May/22 03:39,19/May/22 03:45,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"Currently, Python Pulsar Connector only supports Flink Schema, we also need to support Pulsar Schema. 

Note: We need to support the enableSchemaEvolution method as we support PulsarSchema.

The following is detail.

[https://github.com/apache/flink/pull/19682#discussion_r872131355]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-19 03:39:05.0,,,,,,,,,,"0|z12hxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add FlinkOperatorEventListener interface,FLINK-27688,13445707,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,mbalassi,mbalassi,18/May/22 19:50,20/Jun/22 10:51,04/Jun/24 20:51,20/Jun/22 10:51,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Currently the [EventUtils|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/EventUtils.java] utility that we use to publish events for the operator has an implementation that is tightly coupled with the [Kubernetes Events|https://www.containiq.com/post/kubernetes-events] mechanism.

I suggest to enhance this with a pluggable event interface, which could be implemented by our users to support their event messaging system of choice. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 10:51:17 UTC 2022,,,,,,,,,,"0|z12how:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 19:55;gyfora;I think we should keep the Kubernetes event mechanism and add an FlinkOperatorEventListener interface which could expose callbacks on status changes and event triggers.;;;","20/Jun/22 10:51;gyfora;merged to main 869c029dc0662299ce34d1570510e65f4fe3794d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink shouldn't assume temp folders keep existing when unused,FLINK-27687,13445678,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gael,gael,18/May/22 16:53,19/May/22 06:35,04/Jun/24 20:51,,1.14.4,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,0,,,,,"In SpanningWrapper.createSpillingChannel, it assumes that the folder in which we create the file exists. However, this is not the case in the following scenario (which actually happened to us today):
 * The temp folders were created a while ago (I assume on startup of the task-manager) in the /tmp folder. They weren't used for a while, probably because we didn't have any record big enough to trigger it.
 * The cleanup cron for /tmp did its job and deleted those old folders in /tmp.
 * We deployed a new version of the job that actually needed the folders, and it crashed.

=> Not sure if it should be SpanningWrapper's responsability to create the folder if it doesn't exist anymore, though, but I'm not familiar enough with Flink's internal to make a guess as to what class should do it. The problem occurred to us on SpanningWrapper, but it can probably happen in other places as well.

More generally, assuming that folders and files in /tmp won't get deleted at some point doesn't seem correct to me. The [documentation for io.tmp.dirs|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/] recommands that it shouldn't be purged, but we do need to clean up at some point. If that is not the case, then the documentation should be updated to indicate that this is not a recommendation but mandatory, and that purges will break the jobs (not just trigger a recovery).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-18 16:53:47.0,,,,,,,,,,"0|z12hig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only patch status when the status actually changed,FLINK-27686,13445672,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,18/May/22 15:44,20/May/22 06:13,04/Jun/24 20:51,20/May/22 06:13,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The StatusHelper class currently always patches the status regardless if it changed or not.

We should use an ObjectMapper and simply compare the ObjectNode representations and only patch if there is any change.

 

(I think we cannot directly compare the status objects because some of the content comes from getters and are not part of the equals implementation)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 06:13:53 UTC 2022,,,,,,,,,,"0|z12hh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 15:44;gyfora;cc [~wangyang0918] ;;;","20/May/22 06:13;gyfora;merged to
main 0e537308975f29a6dc129d047f3db337122cc3ca
release-1.0 8f4cc547b898804852f82b5ee058e46e5c853537;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add scale subresource,FLINK-27685,13445640,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,18/May/22 13:05,10/Jun/22 10:48,04/Jun/24 20:51,10/Jun/22 10:20,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We should define a scale subresource for the deployment/sessionjob resources that allows us to use the `scale` command or even hook in the HPA.

I suggest to use parallelism as the ""replicas"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 10:20:16 UTC 2022,,,,,,,,,,"0|z12ha0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 10:20;gyfora;merged to main 1dd9850fb37d4f7d572f46b247470bda13fb3660;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaConsumerBase could record partitions offset when GROUP_OFFSETS,FLINK-27684,13445636,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,SilkyAlex,SilkyAlex,18/May/22 12:42,30/May/22 08:09,04/Jun/24 20:51,30/May/22 08:09,1.14.3,,,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,,,,,"when FlinkKafkaConsumerBase startupMode been set with:

EARLIEST/LATEST/TIMESTAMP/GROUP_OFFSETS

the log when startup are not record current partitions's offsets, that makes difficult to locate starup offsets for check something data problem.

 

we could record it for a better world.

 
{code:java}
2022-04-15 22:27:58.802 INFO  [95] org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 11 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='kafka_topic', partition=4}]
2022-04-15 22:27:58.802 INFO  [94] org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 5 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='kafka_topic', partition=14}]
2022-04-15 22:27:58.805 INFO  [92] org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase - Consumer subtask 3 will start reading the following 1 partitions from the committed group offsets in Kafka: [KafkaTopicPartition{topic='kafka_topic', partition=12, wish here to log offsets}] {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 08:09:07 UTC 2022,,,,,,,,,,"0|z12h94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 12:51;martijnvisser;[~SilkyAlex] Thanks for opening the ticket, but the FlinkKafkaConsumer is deprecated and replaced by KafkaSource. We won't introduce any new features or improvements to FlinkKafkaConsumer.;;;","30/May/22 08:09;SilkyAlex;[~martijnvisser] Thanks for reply, i will try KafkaSource for test.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Insert into (column1, column2) Values(.....) fails with SQL hints",FLINK-27683,13445610,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuanyu66,xuanyu66,xuanyu66,18/May/22 10:28,27/Oct/23 09:15,04/Jun/24 20:51,13/Jun/22 08:31,1.14.0,1.15.1,1.16.0,,,,,,,,,,,,1.14.6,1.15.1,1.16.0,,Table SQL / Planner,,,,,0,pull-request-available,,,,"When I try to use statement `Insert into (column1, column2) Values(.....)` with SQL hints, it throw some exception, which is certainly a bug.

 
 * Sql 1
{code:java}
INSERT INTO `tidb`.`%s`.`%s` /*+ OPTIONS('tidb.sink.update-columns'='c2, c13')*/   (c2, c13) values(11111, 12.12) {code}

 * 
 ** result 1
!screenshot-1.png!

 * Sql 2
{code:java}
INSERT INTO `tidb`.`%s`.`%s` (c2, c13) /*+ OPTIONS('tidb.sink.update-columns'='c2, c13')*/  values(11111, 12.12)
{code}

 * 
 ** result 2
!screenshot-2.png!

 * Sql 3
{code:java}
INSERT INTO `tidb`.`%s`.`%s` (c2, c13)  values(11111, 12.12)
{code}

 * 
 ** result3 : success",,,,,,,,,,,,,,,,,,,,,,,,FLINK-26259,,,,,,,,,,,,,,,,,,,,,,"18/May/22 10:29;xuanyu66;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13043841/screenshot-1.png","18/May/22 10:31;xuanyu66;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13043842/screenshot-2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 08:31:01 UTC 2022,,,,,,,,,,"0|z12h3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 02:25;fsk119;The main reason is that it tries to cast the SqlTabelRef to SqlIdentifier. ;;;","20/May/22 03:02;xuanyu66;[~fsk119] It's a bug, right?;;;","20/May/22 03:11;fsk119;Yes. I think we can fix this. Are you interested in this issue?;;;","20/May/22 08:29;xuanyu66;[~fsk119] Yes, sure;;;","24/May/22 04:35;fsk119;You can open a PR and ping me when you are ready.;;;","30/May/22 13:08;xuanyu66;[~fsk119] Would you tell me where can I add an integration test for this sql?

`JdbcDynamicTableSinkITCase`?;;;","31/May/22 02:56;xuanyu66;https://github.com/apache/flink/pull/19847;;;","06/Jun/22 06:19;godfreyhe;Thanks for the fix, could you create a PR for release-1.14 since there are some conflicts [~xuanyu66];;;","06/Jun/22 06:21;godfreyhe;Fixed in 
master: 9bcc7fd20420bbf90f4b67d98c521a8ddf775d4e
1.15.1: e0af037d9910b6cfd4cc3fd8937289f939bb6d9b;;;","06/Jun/22 06:46;xuanyu66;[~godfreyhe] OK;;;","07/Jun/22 06:34;xuanyu66;[https://github.com/apache/flink/pull/19892] [~godfreyhe] ;;;","13/Jun/22 08:31;godfreyhe;Fixed in 1.14.6: ca16d28a741434f78fba508d8050786cd3281793;;;",,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate ComparatorTestBase to Junit5,FLINK-27682,13445608,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/May/22 10:16,19/May/22 08:06,04/Jun/24 20:51,19/May/22 08:06,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,Several modules depend on it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 08:06:27 UTC 2022,,,,,,,,,,"0|z12h2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 08:06;chesnay;master: df307518ed00d82d55b99fc25c961e5188beb08f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the availability of Flink when the RocksDB file is corrupted.,FLINK-27681,13445592,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,mayuehappy,Ming Li,Ming Li,18/May/22 09:06,14/Dec/23 17:27,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,0,pull-request-available,,,,"We have encountered several times when the RocksDB checksum does not match or the block verification fails when the job is restored. The reason for this situation is generally that there are some problems with the machine where the task is located, which causes the files uploaded to HDFS to be incorrect, but it has been a long time (a dozen minutes to half an hour) when we found this problem. I'm not sure if anyone else has had a similar problem.

Since this file is referenced by incremental checkpoints for a long time, when the maximum number of checkpoints reserved is exceeded, we can only use this file until it is no longer referenced. When the job failed, it cannot be recovered.

Therefore we consider:
1. Can RocksDB periodically check whether all files are correct and find the problem in time?
2. Can Flink automatically roll back to the previous checkpoint when there is a problem with the checkpoint data, because even with manual intervention, it just tries to recover from the existing checkpoint or discard the entire state.
3. Can we increase the maximum number of references to a file based on the maximum number of checkpoints reserved? When the number of references exceeds the maximum number of checkpoints -1, the Task side is required to upload a new file for this reference. Not sure if this way will ensure that the new file we upload will be correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 07:06;mayuehappy;image-2023-08-23-15-06-16-717.png;https://issues.apache.org/jira/secure/attachment/13062363/image-2023-08-23-15-06-16-717.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 14 17:27:30 UTC 2023,,,,,,,,,,"0|z12gzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/22 03:19;Yanfei Lei;I think the correctness of the file should be guaranteed by other systems, Although Flink can achieve the suggestions you listed, these things would introduce some overheads and increase the complexity of Flink.

 I would prefer to have some external ways to solve this issue, e.g. do native savepoint/full checkpoint periodically to reduce the number of data to replay. WDYT?

 ;;;","01/Dec/22 03:39;Ming Li;[~Yanfei Lei] thank you for your reply.

In our production, the underlying environment may produce some errors, resulting in corrupted files. In addition, Flink stores local files in the form of a single copy. When a problematic file is uploaded to DFS as a checkpoint, this checkpoint will be unavailable.


Can this question be simplified to whether checkpoint files need to be double-checked at the Flink layer to ensure that errors in the underlying environment will not cause checkpoint file errors?;;;","18/Aug/23 02:20;masteryhx;Hi, [~Ming Li] Thanks for reporting this.
We also saw some similar cases.

I think the core reason is lacking checking checksum when checkpointing , so even if the checksum is incorrect, error files are still be uploaded but checkpoints still be marked as completed. 
{quote}Can RocksDB periodically check whether all files are correct and find the problem in time?
{quote}
I think It's better to check every CP files that will be uploaded so that we could avoid local corrupted files affecting remote checkpoint files. But this may increase costs and decrease the performace of checkpoints. So maybe a configuration could be introduced ? WDYT?
{quote}Can Flink automatically roll back to the previous checkpoint when there is a problem with the checkpoint data, because even with manual intervention, it just tries to recover from the existing checkpoint or discard the entire state.
{quote}
I think it makes sense, but just as I said, we must make sure that every checkpoints will not be influenced by the local corrupted files.;;;","23/Aug/23 07:06;mayuehappy;[~masteryhx] 
I'd like to contribute it
I think there might be two ways to check if the file data is correct, One is to check whether the file data is correct by setting DBOptions#setParanoidChecks to read the file after it is generated, but this may cause some read amplification and CPU overhead. another one is Manually call db.VerifyChecksum() to check the correctness of the file when making checkpoint ? WDYT ? ;;;","28/Aug/23 06:54;masteryhx;[~mayuehappy] Do you mean calling db.VerifyChecksum() in the async thread of checkpoint ?

I just rethinked all interfaces rocksdb provided, this may also bring too much cost which may result in unavailable checkpoint when enabling this option.
{code:java}
the API call may take a significant amount of time to finish{code}
I think the best way is to only verify the checksum of incremental SST to reduce the cost, but seems rocksdb haven't provided the interface to verify in the SST level.

 ;;;","30/Oct/23 03:11;mayuehappy;[~masteryhx]  
If we want to only check the checksum of incremental sst, can we use the SstFileReader.verifyChecksum() ?
https://github.com/ververica/frocksdb/blob/FRocksDB-6.20.3/java/src/main/java/org/rocksdb/SstFileReader.java;;;","02/Nov/23 06:39;masteryhx;[~mayuehappy] Thanks for the reminder. I think it's doable. 
Just assigned to you. Please go ahead.

Just as we discussed: we could introduce a new configuration to enable this, and we could just verify the incrementall SST.;;;","21/Nov/23 09:31;mayuehappy;[~masteryhx] Sorry for the late reply, I submitted a draft PR, please take a look when you have time.;;;","24/Nov/23 07:45;fanrui;{quote}In our production, the underlying environment may produce some errors, resulting in corrupted files. In addition, Flink stores local files in the form of a single copy. When a problematic file is uploaded to DFS as a checkpoint, this checkpoint will be unavailable.
{quote}
Hi [~Ming Li] [~mayuehappy] , I did a quick review for this PR. But I still don't know why the file is corrupted, would you mind describing it in detail?

I'm worried that flink add check can't completely solve the problem of file corruption. Is it possible that file corruption occurs after flink check but before uploading the file to hdfs?;;;","24/Nov/23 09:08;mayuehappy;[~fanrui]
{quote}But I still don't know why the file is corrupted, would you mind describing it in detail?
{quote}
In our production environment, most files are damaged due to hardware failures on the machine where the file is written, such as (Memory CE or SSD disk hardware failure). Under the default Rocksdb option, after a damaged SST is created, if there is no Compaction or Get/Iterator to access this file, DB can always run normally. But when the task fails and recovers from Checkpoint, there may be other Get requests or Compactions that will read this file, and the task will fail at this time.
{quote} Is it possible that file corruption occurs after flink check but before uploading the file to hdfs?
{quote}
Strictly speaking, I think it is possible for file corruption to occur during the process of uploading and downloading to local. It might be better if Flink can add the file verification mechanism during Checkpoint upload and download processes. But as far as I know, most DFSs have data verification mechanisms, so at least we have not encountered this situation in our production environment. Most file corruption occurs before being uploaded to HDFS.;;;","24/Nov/23 09:48;fanrui;Thanks for the quick response.
{quote}Strictly speaking, I think it is possible for file corruption to occur during the process of uploading and downloading to local. It might be better if Flink can add the file verification mechanism during Checkpoint upload and download processes. But as far as I know, most DFSs have data verification mechanisms, so at least we have not encountered this situation in our production environment. Most file corruption occurs before being uploaded to HDFS.
{quote}
Sorry, my expression may not be clear.

Yeah, most DFSs have data verification mechanisms. Please see this comment, that's my concern. [https://github.com/apache/flink/pull/23765#discussion_r1404136470]

 
{quote}Under the default Rocksdb option, after a damaged SST is created, if there is no Compaction or Get/Iterator to access this file, DB can always run normally. 
{quote}
If so, when this situation is discovered, is it reasonable to let the checkpoint fail?

From a technical perspective, a solution with less impact on the job might be:
 * If this file is uploaded to hdfs before, flink try to download it, and let rocksdb become health.
 * If this file isn't uploaded to hdfs, flink job should fail directly, right?

If we only fail the current checkpoint, and execution.checkpointing.tolerable-failed-checkpoints > 0, the job will continue to run. And flink job will fail later (when this file is read.). And then job will recover from latest checkpoint, flink job will consume more duplicate data than fail job directly.

Please correct me if I misunderstood anything, thanks~

Also, I'd like to cc [~pnowojski] , he might also be interested in this JIRA.;;;","24/Nov/23 10:17;pnowojski;Thanks [~fanrui] for pinging me, and double thanks to [~mayuehappy] for tackling this issue.

{quote}
In our production environment, most files are damaged due to hardware failures on the machine where the file is written
{quote}

Wouldn't the much more reliable and faster solution be to enable CRC on the local filesystem/disk that Flink's using? Benefits of this approach:
* no changes to Flink/no increased complexity of our code base
* would protect from not only errors that happen to occur between writing the file and uploading to the DFS, but also from any errors that happen at any point of time
* would amortise the performance hit. Instead of amplifying reads by 100%, error correction bits/bytes are a small fraction of the payload, so the performance penalty would be at every read/write access but ultimately a very small fraction of the total cost of reading 

Assuming we indeed want to verify files, doing so during the checkpoint's async phase is and failing the whole checkpoint if verification fails is a good enough solution, that doesn't complicate the code too much.

{quote}
If this file is uploaded to hdfs before, flink try to download it, and let rocksdb become health.
If this file isn't uploaded to hdfs, flink job should fail directly, right?

If we only fail the current checkpoint, and execution.checkpointing.tolerable-failed-checkpoints > 0, the job will continue to run. And flink job will fail later (when this file is read.). And then job will recover from latest checkpoint, flink job will consume more duplicate data than fail job directly.
{quote}
[~fanrui] In what scenario file that we are verifying before the upload, could have been uploaded before?;;;","24/Nov/23 10:28;fanrui;{quote}In what scenario file that we are verifying before the upload, could have been uploaded before?
{quote}
What I mean is that we not only cover file corruption when uploading files, we can also cover file corruption during data processing.

If one file is uploaded to hdfs in the previous checkpoint, and it's corrupted now, we can download it from hdfs to reduce the impact for this job.;;;","24/Nov/23 12:39;pnowojski;Ahh got it. In that case I would argue always failing over the job is good enough. This whole scenario should only happen very very rarely, and it's fine if we have one way or another a solution that simply protects from corrupted state. Again I would argue some FS/disk level CRCs should be the way to go.;;;","27/Nov/23 03:28;mayuehappy;Hi [~pnowojski] and [~fanrui] , thanks for your repiles.  
{quote}If one file is uploaded to hdfs in the previous checkpoint, and it's corrupted now
{quote}
If the file uploaded in HDFS is good, but it may be corrupted by local disk after download during data processing,  can this problem be solved by scheduling the TM to another machine after Failover ? Is it more important to ensure that the Checkpoint data on HDFS is available. ? BTW, we don’t seem to have encountered this situation in our actual production environment. I don’t know if you have actually encountered it, or whether we still need to consider this situation.;;;","27/Nov/23 04:31;masteryhx;[~pnowojski] [~fanrui]  Thanks for joining in the discussion.

Thanks [~mayuehappy] for expalining the case which we also saw in our production environment.

Let me also try to share my thoughts about your questions.
{quote}I'm worried that flink add check can't completely solve the problem of file corruption. Is it possible that file corruption occurs after flink check but before uploading the file to hdfs?
{quote}
I think the concern is right.

Actually, file corruption may occurs in all stages:
 # File generation at runtime (RocksDB memtable flush or Compaction)
 # Uploading when checkpoint (local file -> memory buffer -> network transfer -> DFS)
 # Downloading when recovery(reversed path with 2)

 

For the first situation: 
 * File corruption will not affect the read path because the checksum will be checked when reading rocksdb block. The job will failover when read the corrupted one.
 * So the core problem is that a corruped file which is not read at runtime will be uploaded to remote DFS when checkpoint. It will affect the normal processing once failover which will have severe consequence especially for high priority job.
 * That's why we'd like to focus on not uploading the corruped files (Also for just fail the job simply to make job restore from the last complete checkpoint).

For the second and third situations:
 * The ideal way is that we should unify the checksum machnism of local db and remote DFS.
 * Many FileSystems supports to pass the file checksum and verify it in their remote server. We could use this to verify the checksum end-to-end.
 * But this may introduce a new API in some public classes like FileSystem which is a bigger topic.
 * As we also saw many cases like [~mayuehappy] mentioned. So I think maybe we could resolve this case at first. I'd also like to see we have the ideal way to go if it worth doing.;;;","27/Nov/23 09:34;mayuehappy;[~masteryhx] Thanks for the explaining , I agree with it;;;","27/Nov/23 10:16;fanrui;{quote} * That's why we'd like to focus on not uploading the corruped files (Also for just fail the job simply to make job restore from the last complete checkpoint).{quote}
Fail job directly is fine for me, but I guess the PR doesn't fail the job, it just fails the current checkpoint, right?

 
{quote}File corruption will not affect the read path because the checksum will be checked when reading rocksdb block. The job will failover when read the corrupted one.
{quote}
If the checksum is called for each reading, can we think the check is very quick? If so, could we enable it directly without any option? Hey [~mayuehappy]  , could you provide some simple benchmark here?;;;","28/Nov/23 04:29;mayuehappy; 
{quote}Fail job directly is fine for me, but I guess the PR doesn't fail the job, it just fails the current checkpoint, right?
{quote}
I think it may be used together with the {*}execution.checkpointing.tolerable-failed-checkpoints{*}, or generally speaking, if it is a high-quality job, users will also pay attention to whether the cp production is successful.
{quote}could you provide some simple benchmark here?
{quote}
I did some testing on my local machine. It takes about 60 to 70ms to check a 64M sst file. Checking a 10GB rocksdb instance takes about 10 seconds. More detailed testing may be needed later.


 ;;;","30/Nov/23 02:18;masteryhx;{quote}Fail job directly is fine for me, but I guess the PR doesn't fail the job, it just fails the current checkpoint, right?
{quote}
Yeah, I think failing the checkpoint maybe also fine currently. It will not affect the correctness of the running job.

The downside is that the job has to rollback to the older checkpoint. But there should be some policies for high-quality job just as [~mayuehappy] mentioned.
{quote}If the checksum is called for each reading, can we think the check is very quick? If so, could we enable it directly without any option? Hey [~mayuehappy]  , could you provide some simple benchmark here?
{quote}
The check at runtime is block level, whose overhead should be little (rocksdb always need to read the block from the disk at runtime, so the checksum could be calculated easily).

But the checksum in file level will always be done with extra overhead, and the overhead will be bigger if the state is very large, so that's why I'd like to suggest it as an option. Also appreciate and look forward the benchmark result of [~mayuehappy] ;;;","30/Nov/23 06:55;fanrui;Hey [~mayuehappy]  [~masteryhx] , thanks for your feedback.:)
{quote}The downside is that the job has to rollback to the older checkpoint. But there should be some policies for high-quality job just as [~mayuehappy] mentioned.
{quote}
My concern is that if we found the file is corrupted, and fail the checkpoint. The job will continue to run (if tolerable-failed-checkpoints > 0),  and all checkpoints cannot be completed in the future.

However,  the job must fail in the future(When the corrupted block is read or compacted, or checkpoint failed number >= tolerable-failed-checkpoint). Then it will rollback to the older checkpoint.

The older checkpoint must be before we found the file is corrupted. Therefore, it is useless to run a job between the time it is discovered that the file is corrupted and the time it actually fails.

In brief, tolerable-failed-checkpoint can work, but the extra cost isn't necessary.

BTW, if failing job directly, this [comment|https://github.com/apache/flink/pull/23765#discussion_r1404136470] will be solved directly.
{quote}The check at runtime is block level, whose overhead should be little (rocksdb always need to read the block from the disk at runtime, so the checksum could be calculated easily).
{quote}
Thanks [~masteryhx] for the clarification.

 
{quote}Wouldn't the much more reliable and faster solution be to enable CRC on the local filesystem/disk that Flink's using? Benefits of this approach:
 * no changes to Flink/no increased complexity of our code base
 * would protect from not only errors that happen to occur between writing the file and uploading to the DFS, but also from any errors that happen at any point of time
 * would amortise the performance hit. Instead of amplifying reads by 100%, error correction bits/bytes are a small fraction of the payload, so the performance penalty would be at every read/write access but ultimately a very small fraction of the total cost of reading{quote}
[~pnowojski] 's comment also directly causes the job to fail? I'm not familiar with how to enable CRC for filesystem/disk? Would you mind describing it in detail?;;;","30/Nov/23 11:51;pnowojski;{quote}

However,  the job must fail in the future(When the corrupted block is read or compacted, or checkpoint failed number >= tolerable-failed-checkpoint). Then it will rollback to the older checkpoint.

The older checkpoint must be before we found the file is corrupted. Therefore, it is useless to run a job between the time it is discovered that the file is corrupted and the time it actually fails.

In brief, tolerable-failed-checkpoint can work, but the extra cost isn't necessary.

{quote}

^^^ This

{quote}

I did some testing on my local machine. It takes about 60 to 70ms to check a 64M sst file. Checking a 10GB rocksdb instance takes about 10 seconds. More detailed testing may be needed later.

{quote}

That's a non trivial overhead. Prolonging checkpoint for 10s in many cases (especially low throughput large state jobs) will be prohibitively expensive, delaying rescaling, e2e exactly once latency, etc. 1s+ for 1GB might also be less then ideal to enable by default.

{quote}

I'm not familiar with how to enable CRC for filesystem/disk? Would you mind describing it in detail?

{quote}

Actually, aren't all of the disks basically have some form of CRC these days? I'm certain that's true about SSDs. Having said that, can you [~masteryhx] rephrase and elaborate on those 3 scenarios that you think we need to protect from? Especially where does the corruption happen?;;;","05/Dec/23 08:42;masteryhx;Sorry for the late reply.
{quote}However,  the job must fail in the future(When the corrupted block is read or compacted, or checkpoint failed number >= tolerable-failed-checkpoint). Then it will rollback to the older checkpoint.

The older checkpoint must be before we found the file is corrupted. Therefore, it is useless to run a job between the time it is discovered that the file is corrupted and the time it actually fails.

In brief, tolerable-failed-checkpoint can work, but the extra cost isn't necessary.

BTW, if failing job directly, this [comment|https://github.com/apache/flink/pull/23765#discussion_r1404136470] will be solved directly.
{quote}
Thanks for the detailed clarification.

I rethinked this, seems that failing the job is more reasonable than failing current checkpoint. I'm +1 if we could do that.
{quote}That's a non trivial overhead. Prolonging checkpoint for 10s in many cases (especially low throughput large state jobs) will be prohibitively expensive, delaying rescaling, e2e exactly once latency, etc. 1s+ for 1GB might also be less then ideal to enable by default.
{quote}
Cannot agree more.
{quote}Actually, aren't all of the disks basically have some form of CRC these days? I'm certain that's true about SSDs. Having said that, can you [~masteryhx] rephrase and elaborate on those 3 scenarios that you think we need to protect from? Especially where does the corruption happen?
{quote}
IIUC, Once we have IO operations about the SST, the file maybe corrupted even if it may happen very rarely.

RocksDB also shares some situations about using full file checksum[1] which is related to our usage:
 # local file which is prepared to upload: as you could see ""verify the SST file when the whole file is read in DB (e.g., compaction)."" in [1], and checksum in block level at runtime cannot guarantee the correctness of the SST which we could focus on at first.
 # Uploading and Downloaing: Firstly, disk IO and network IO may make the data error. Secondly, remote storage is not always reliable. So the checksum could be used when SST files are copied to other places (e.g., backup, move, or replicate) or stored remotely.

IIUC, checksum in SST level could guarantee the correctness of local file.

And checksum in filesystem level could guarantee the correctness of uploading and downloading.

[1] https://github.com/facebook/rocksdb/wiki/Full-File-Checksum-and-Checksum-Handoff;;;","06/Dec/23 02:58;mayuehappy;[~masteryhx] [~pnowojski] [~fanrui]  Thanks for the discussion

So, IIUC, currently I think the consensus conclusion is that we need to make the job fail if there is file corruption on check , right ? 

For now failure in the checkpoint asynchronous phase will not cause the job to fail. Should we open another ticket to support the ability to ""fail the job if some special exception is occured during the checkpoint asynchronous phase""?

 

 ;;;","06/Dec/23 10:43;pnowojski;{quote}

IIUC, checksum in SST level could guarantee the correctness of local file.

{quote}

Yes, I think we don't need any extra protection for corruption of the local files. From the document you shared RocksDB will throw some error every time we try to read a corrupted block

{quote}

And checksum in filesystem level could guarantee the correctness of uploading and downloading.

{quote}

Now I'm not so sure about it. Now that I think about it more, checksums on the filesystem level or the HDD/SSD level wouldn't protect us from a corruption happening after reading the bytes from local file, but before those bytes are acknowledged by the DFS/object store. 

A neat way would be to calculate the checksum locally, when writing the SST file to the local file system (""Full File Checksum Design"" from the document [~masteryhx]  shared?), without any significant overhead (bytes that we want to verify would be after all already in the RAM). Next if we could cheaply verify that the uploaded file to the DFS still has the same checksum as computed during creation of that file, we could make sure that no matter what, we always have valid files in the DFS, that we can fallback to everytime RocksDB detects a data corruption when accessing and SST file locally.

It looks like this might be do-able in one [1] of the two [2] ways. At least for the S3. 

[1] [https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html#checking-object-integrity-md5]

I don't know if AWS's check against the {{Content-MD5}} field is for free. As far as I understand it, it could be implement to be almost for free, but the docs do not mention that.

[2] https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html#trailing-checksums 

Here the docs are saying that this is for free, but it looks like this is calculating a new checksum during the upload process. So the question would be, could we retrieve that checksum and compare it against our locally computed one?

 

[~mayuehappy] , if we decide to go this direction, then the change to fail a job after checksum mismatching during the async phase could be implemented easily here: {{{}org.apache.flink.runtime.checkpoint.CheckpointFailureManager{}}}. I don't think we need an extra ticket for that, separate commit in the same PR will suffice. ;;;","07/Dec/23 13:06;masteryhx;{quote}Yes, I think we don't need any extra protection for corruption of the local files. From the document you shared RocksDB will throw some error every time we try to read a corrupted block
{quote}
Yes, reading a corrupted block must be checked which is safe.

But the write operation (e.g. flush, compaction) may introduce a new corrupted file which may not be checked.

And the corrupted file maybe just uploaded to remote storage without any check like reading block checksum when checkpoint if we don't check it manually.
{quote}Now I'm not so sure about it. Now that I think about it more, checksums on the filesystem level or the HDD/SSD level wouldn't protect us from a corruption happening after reading the bytes from local file, but before those bytes are acknowledged by the DFS/object store. 
{quote}
Yes, you're right. That's what I mentioned before about the end-to-end checksum (verify the file correctness from local to remote by unified checksum). And Thanks for sharing detailed infos about S3.

""But this may introduce a new API in some public classes like FileSystem which is a bigger topic."" , maybe need a FLIP ?

We also have tried to add this end-to-end checksum in our internal Flink version which is doable for many file systems.

We could also contribute this after we have verified the benefits and performance cost if worthy doing.;;;","07/Dec/23 14:56;pnowojski;[~masteryhx] , yes that would require a new FLIP, as one way or another we would have to verify checksums of the uploaded files to the DFS. Also please note my hints/ideas that we might not need any dedicated checksum counting, as it should be possible to hook in and get/calculate checksums on the fly during file creation/file upload.;;;","08/Dec/23 04:02;mayuehappy;{quote}And the corrupted file maybe just uploaded to remote storage without any check like reading block checksum when checkpoint if we don't check it manually.
{quote}
So I understand that we can do this manual check in this ticket first. If the file is detected to be corrupted, we can fail the job. Is this a good choice? ;;;","08/Dec/23 10:15;pnowojski;{quote}

So I understand that we can do this manual check in this ticket first. If the file is detected to be corrupted, we can fail the job. Is this a good choice? 

{quote}

I would prefer to avoid the approach that was proposed here in the ticket:

[i] before uploading the file, scan it completely block by block for the data consistency using RocksDB's mechanisms. 

The problems:
 * Performance overhead
 * Works only for RocksDB
 * Doesn't protect us fully from the data corruption. Corruption can happen just after we checked it locally, but before we uploaded to the DFS

So rephrasing what I was proposing in my messages above:

[ii] 
 # Calculate some checksum *[A]* ON THE FLY, at the same time that the state file is being written/created. For RocksDB that would require hooking up with the RocksDB itself. It would be easier for the HashMap state backend. But it would have zero additional IO cost, and some minor CPU cost (negligible compared to the IO access)
 # Remember the checksum *[A]*  until:
 # Depending what the DFS supports, either:
 ** preferably, verify against the checksum *[A]* ON THE FLY, when file is being uploaded to the DFS. In principle, if implemented by the DFS properly, this should be again basically for free, without and additional IO cost. S3 might actually support that via  [1] or [2].
 ** after uploading, use DFS api to remotely calculate checksum of the uploaded file, and compare it against the checksum *[A].* S3 does support it [3], quoting:

{quote}

After uploading objects, you can get the checksum value and compare it to a precalculated or previously stored checksum value calculated using the same algorithm.

{quote}

 

Note, we would have to ensure, that checksum  [A] is always calculated the same way, both in the statebackend (RocksDB) and DFS (S3). I have no idea if RocksDB supports something like that, but if not:
 * it should be a welcome contribution by RocksDB maintainers
 * implementing a hook on our side in our FRocksDB fork doesn't sound too complicated. I would hope it would only require wrapping some {{OutByteStream}} class and that's it.

 

[1] [https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html#checking-object-integrity-md5]

[2] [https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html#trailing-checksums] 

[3] [https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html#using-additional-checksums]

 

 ;;;","14/Dec/23 03:41;mayuehappy;[~pnowojski] Thanks for your suggestion, I think this is a perfect solution. But it sounds like there is still a long way to go to implement this plan. Do we have any specific plans to do this?;;;","14/Dec/23 17:27;pnowojski;I could help reviewing and generally speaking support this effort, including discussions around the FLIP (this change would have to modify `FileSystem` API - DFS has to support in someway checksum verification). The general plan should be:
 # Someone would have to investigate what's possible without modification of RocksDB, and works at least with S3. Some tests against S3 and raw benchmarks without using Flink would be needed. For example a simple standalone Java app, uploading the a while verifying the checksum at the same time. Definitely people will be asking what about other DFSs. It would be nice to do a research with Azure/GCP also (might be just looking into the docs without testing).
 # Create a FLIP. Ideally already with this zero-overhead solution. But if RocksDB would be problematic, something that's already designed with the zero-overhead in ming for the long term, and some intermediate temporary solution would be probably also fine.;;;",
Disable PulsarSinkITCase and PulsarSourceITCase on JDK 11,FLINK-27680,13445591,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,18/May/22 09:05,19/May/22 08:36,04/Jun/24 20:51,19/May/22 08:36,1.14.5,1.15.1,1.16.0,,,,,,,,,,,,1.16.0,,,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"Since Pulsar doesn't yet support Java 11, we should make sure that the Pulsar tests don't run when testing JDK11. This is the case already for the e2e tests, but not yet for the connector tests. We should disable this too. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 08:36:21 UTC 2022,,,,,,,,,,"0|z12gz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 09:05;martijnvisser;Example error: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35769&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=38968;;;","19/May/22 08:36;martijnvisser;Fixed in master: e049ceaa9c3570efcb8299a1c40229e7155556ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for append-only table with log store,FLINK-27679,13445583,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,openinx,openinx,18/May/22 08:18,09/Jan/23 03:08,04/Jun/24 20:51,09/Jan/23 03:08,,,,,,,,,,,,,,,table-store-0.3.0,,,,,,,,,0,pull-request-available,,,,Will publish separate PR to support append-only table for log table.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 04 09:12:18 UTC 2022,,,,,,,,,,"0|z12gxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jul/22 09:12;lzljs3620320;Just add tests for append-only table log integration.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support append-only table for file store.,FLINK-27678,13445582,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,openinx,openinx,openinx,18/May/22 08:17,25/Aug/22 02:30,04/Jun/24 20:51,20/May/22 02:21,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,Let me publish a separate PR for supporting append-only table in flink table store's file store.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 02:21:28 UTC 2022,,,,,,,,,,"0|z12gx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 02:21;lzljs3620320;master: 59282f71c84887e3f16af81890558f9593bdaa79;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kubernetes reuse rest.bind-port, but do not support a range of ports",FLINK-27677,13445581,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,tartarus,tartarus,18/May/22 08:17,16/Mar/23 08:58,04/Jun/24 20:51,16/Mar/23 08:58,1.15.0,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,,"k8s module reuse the rest options {color:#DE350B}rest.bind-port{color},
but do not support a range of ports

{code:java}
   /**
     * Parse a valid port for the config option. A fixed port is expected, and do not support a
     * range of ports.
     *
     * @param flinkConfig flink config
     * @param port port config option
     * @return valid port
     */
    public static Integer parsePort(Configuration flinkConfig, ConfigOption<String> port) {
        checkNotNull(flinkConfig.get(port), port.key() + "" should not be null."");

        try {
            return Integer.parseInt(flinkConfig.get(port));
        } catch (NumberFormatException ex) {
            throw new FlinkRuntimeException(
                    port.key()
                            + "" should be specified to a fixed port. Do not support a range of ports."",
                    ex);
        }
    }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 04:26:50 UTC 2022,,,,,,,,,,"0|z12gww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 08:22;tartarus;[~tison]  I think the behavior of this configuration on k8s and yarn is inconsistent, and it is not mentioned in the description.

Do you have any advice on this issue?;;;","18/May/22 12:19;wangyang0918;The reason why we do not support port range is that we need to a fixed rest bind-port when creating the k8s service. Since each pod have an individual network interface, we do not care about the port conflicts.

 
{code:java}
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: ""2022-05-15T03:55:08Z""
  labels:
    app: flink-example-statemachine
    type: flink-native-kubernetes
  name: flink-example-statemachine-rest
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: Deployment
    name: flink-example-statemachine
    uid: 30eb5df5-328d-4c38-b31f-b791d20b1bbe
  resourceVersion: ""6045181""
  uid: 621a33fb-e960-44d2-95e6-6ed3edf519ca
spec:
  clusterIP: 10.103.224.2
  clusterIPs:
  - 10.103.224.2
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8081
    protocol: TCP
    targetPort: 8081
  selector:
    app: flink-example-statemachine
    component: jobmanager
    type: flink-native-kubernetes
  sessionAffinity: None
  type: ClusterIP {code};;;","19/May/22 03:05;tartarus;[~wangyang0918]  thank you for your reply!

The scenarios of k8s and yarn are different. If we configure rest.bind-port as a port range, the UT in KubernetesClusterDescriptorTest will fail.

So my suggestion is:
1. Modify the description information of rest.bind-port and add that only fixed ports are currently supported in the k8s environment;
2. Add a configuration item for k8s, such as kubernetes.rest.bind-port;
3. KubernetesUtils adds the ability to handle port ranges and select a port from them;

Do you have any other suggestions?;;;","19/May/22 04:26;wangyang0918;If you configure rest.bind-port as a port range, the UT in KubernetesClusterDescriptorTest will certainly fail. Because we do not support this.

I admit that the {{rest.bind-port}} is not fully respected by native K8s deployment, as well as {{blob.server.port}}, {{taskmanager.rpc.port}}, etc. But it is trivial since users do not need to configure a port range when deploying on K8s.

For your suggestions, the #2 is unnecessary and will make the users more confused. And #3, we should not select a port from the range in the  {{KubernetesUtils}}. Instead, it needs to be done in the JobManager side. And then the k8s rest service target port also needs to be updated accordingly. Then it is an overkill.

If you insist, I agree we could update the description for all the port range related config options. Even though, I prefer to believe it is not really necessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output records from on_timer are behind the triggering watermark in PyFlink,FLINK-27676,13445574,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,18/May/22 07:44,19/May/22 12:24,04/Jun/24 20:51,19/May/22 12:24,1.14.4,1.15.0,,,,,,,,,,,,,1.14.5,1.15.1,1.16.0,,API / Python,,,,,0,pull-request-available,,,,"Currently, when dealing with watermarks in AbstractPythonFunctionOperator, super.processWatermark(mark) is called, which advances watermark in timeServiceManager thus triggering timers and then emit current watermark. However, timer triggering is not synchronous in PyFlink (processTimer only put data into beam buffer), and when remote bundle is closed and output records produced by on_timer function finally arrive at Java side, they are already behind the triggering watermark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 12:24:17 UTC 2022,,,,,,,,,,"0|z12gvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 12:24;hxbks2ks;Merged into master via 124e4adb04196e5d56974aead02e61a9bd5bf2cf
Merged into release-1.15 via f26831b85f90324ce9097f3d18d220f1071d0868
Merged into release-1.14 via a5eee994e6417a9e2f1bb6709b768bf4a039d658;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve manual savepoint tracking,FLINK-27675,13445552,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,morhidi,gyfora,gyfora,18/May/22 05:13,24/Nov/22 01:02,04/Jun/24 20:51,19/May/22 13:23,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"There are 2 problems with the manual savpeoint result observing logic that can cause the reconciler to not make progress with the deployment (recoveries, upgrades etc).
 # Whenever the jobmanager deployment is not in READY state or the job itself is not RUNNING, the trigger info must be reset and we should not try to query it anymore. Flink will not retry the savepoint if the job fails, restarted anyways.
 # If there is a sensible error when fetching the savepoint status (such as: 
There is no savepoint operation with triggerId=xxx for job ) we should simply reset the trigger. These errors will never go away on their own and will simply cause the deployment to get stuck in observing/waiting for a savepoint to complete",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 13:23:40 UTC 2022,,,,,,,,,,"0|z12gqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 13:23;wangyang0918;Fixed via:
main: 8c21487104ac4073d127f4c3b1b1591279f2318d
release-1.0: 851b9059055c849bd4fd2d0c4293b6774a911354;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch6SinkE2ECase test hangs,FLINK-27674,13445523,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,hxbks2ks,hxbks2ks,18/May/22 02:05,20/May/22 08:52,04/Jun/24 20:51,20/May/22 08:52,1.16.0,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,0,test-stability,,,,"
{code:java}
2022-05-17T14:23:20.2220667Z May 17 14:23:20 [INFO] Running org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase
2022-05-17T16:46:42.2757438Z ==========================================================================================
2022-05-17T16:46:42.2769104Z === WARNING: This task took already 95% of the available time budget of 284 minutes ===
2022-05-17T16:46:42.2769752Z ==========================================================================================
2022-05-17T16:46:42.2777415Z ==============================================================================
2022-05-17T16:46:42.2778082Z The following Java processes are running (JPS)
2022-05-17T16:46:42.2778711Z ==============================================================================
2022-05-17T16:46:42.4370838Z 366642 surefirebooter7296810110127514313.jar
2022-05-17T16:46:42.4371962Z 323278 Launcher
2022-05-17T16:46:42.4378950Z 384312 Jps
2022-05-17T16:46:42.4452836Z ==============================================================================
2022-05-17T16:46:42.4453843Z Printing stack trace of Java process 366642
2022-05-17T16:46:42.4454796Z ==============================================================================
2022-05-17T16:46:42.9967200Z 2022-05-17 16:46:42
2022-05-17T16:46:42.9968158Z Full thread dump OpenJDK 64-Bit Server VM (25.332-b09 mixed mode):
2022-05-17T16:46:42.9968402Z 
2022-05-17T16:46:42.9968953Z ""Attach Listener"" #6017 daemon prio=9 os_prio=0 tid=0x00007f4cf0007000 nid=0x5dd53 waiting on condition [0x0000000000000000]
2022-05-17T16:46:42.9969533Z    java.lang.Thread.State: RUNNABLE
2022-05-17T16:46:42.9969714Z 
2022-05-17T16:46:42.9970494Z ""ForkJoinPool-1-worker-0"" #6011 daemon prio=5 os_prio=0 tid=0x00007f4b4ed92800 nid=0x5cfea waiting on condition [0x00007f4b35b9b000]
2022-05-17T16:46:42.9971118Z    java.lang.Thread.State: TIMED_WAITING (parking)
2022-05-17T16:46:42.9971752Z 	at sun.misc.Unsafe.park(Native Method)
2022-05-17T16:46:42.9972494Z 	- parking to wait for  <0x00000000939d6cd0> (a java.util.concurrent.ForkJoinPool)
2022-05-17T16:46:42.9973134Z 	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
2022-05-17T16:46:42.9973801Z 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
2022-05-17T16:46:43.0113835Z 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-05-17T16:46:43.0114433Z 
2022-05-17T16:46:43.0118656Z ""flink-rest-client-netty-thread-1"" #6006 daemon prio=5 os_prio=0 tid=0x00007f4b4ed8f000 nid=0x5bb03 runnable [0x00007f4b34181000]
2022-05-17T16:46:43.0119406Z    java.lang.Thread.State: RUNNABLE
2022-05-17T16:46:43.0120063Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2022-05-17T16:46:43.0120921Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2022-05-17T16:46:43.0121666Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2022-05-17T16:46:43.0122507Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2022-05-17T16:46:43.0123735Z 	- locked <0x00000000e4e90c80> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2022-05-17T16:46:43.0124873Z 	- locked <0x00000000e4e90a90> (a java.util.Collections$UnmodifiableSet)
2022-05-17T16:46:43.0125863Z 	- locked <0x00000000e4e909b8> (a sun.nio.ch.EPollSelectorImpl)
2022-05-17T16:46:43.0126648Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2022-05-17T16:46:43.0127370Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2022-05-17T16:46:43.0128211Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
2022-05-17T16:46:43.0129260Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:810)
2022-05-17T16:46:43.0130119Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457)
2022-05-17T16:46:43.0131134Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
2022-05-17T16:46:43.0137869Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2022-05-17T16:46:43.0138683Z 	at java.lang.Thread.run(Thread.java:750)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35749&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 07:13:07 UTC 2022,,,,,,,,,,"0|z12gk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 07:13;martijnvisser;[~hxbks2ks] I think that one is resolved already via FLINK-24433 - That was fixed in a merge straight after this one. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-api-scala,FLINK-27673,13445512,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,17/May/22 21:51,18/May/22 07:43,04/Jun/24 20:51,18/May/22 07:43,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 07:43:00 UTC 2022,,,,,,,,,,"0|z12ghk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 07:43;chesnay;master: 457c3dc6ce4b869de07f13edae30a8d4580b6217;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-common,FLINK-27672,13445494,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,17/May/22 19:12,20/May/22 08:07,04/Jun/24 20:51,20/May/22 08:07,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 08:07:53 UTC 2022,,,,,,,,,,"0|z12gdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 08:07;chesnay;master: f1b4742b87de6175289539ca3c83c4e33ba4e825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish SNAPSHOT docker images,FLINK-27671,13445457,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,17/May/22 15:50,01/Dec/22 15:33,04/Jun/24 20:51,24/May/22 08:45,,,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,"A discussion on the Flink-Dev mailing list [1] concluded that it is desirable to add the ability to publish SNAPSHOT Docker images of Apache Flink to GHCR. 


[1] [https://lists.apache.org/thread/0h60qz8vrw980n1vscz3pxcsv3c3h24m]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 24 08:45:42 UTC 2022,,,,,,,,,,"0|z12g5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 16:31;mbalassi;Fyi: [https://github.com/apache/flink-kubernetes-operator/blob/main/.github/workflows/docker_push.yml]

Please add support to arm architecture too as the above GHA implementation does.;;;","18/May/22 10:20;martijnvisser;[~mbalassi] Do you mean to provide an ARM snapshot image from the Flink installation? I don't think that is an easy thing to do at the moment, given that Flink doesn't support ARM at the moment. I would propose to do that at a later stage since this is a blocker for externalizing the connectors. ;;;","24/May/22 08:45;chesnay;docker-master: 300276dd493b640c2b7fd93b0bbbb3d89cc3a49d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python wrappers for Kinesis Sinks,FLINK-27670,13445449,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,CrynetLogistics,CrynetLogistics,17/May/22 14:40,21/Jun/22 00:45,04/Jun/24 20:51,20/Jun/22 14:05,,,,,,,,,,,,,,,,,,,API / Python,Connectors / Kinesis,,,,0,,,,,"Create Python Wrappers for the new Kinesis Streams sink and the Kinesis Firehose sink.

An example implementation may be found here [https://github.com/apache/flink/pull/15491/files] for the old Kinesis sink.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21966,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 14:05:21 UTC 2022,,,,,,,,,,"0|z12g3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 15:00;CrynetLogistics;This task is the Sink portion of the parent task FLINK-21966;;;","20/Jun/22 14:05;CrynetLogistics;Completed in FLINK-21966;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-file-sink-common,FLINK-27669,13445438,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,17/May/22 13:56,18/May/22 07:40,04/Jun/24 20:51,18/May/22 07:40,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 07:40:00 UTC 2022,,,,,,,,,,"0|z12g14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 07:40;chesnay;master: 7a7d20a5b1ce2acec0c170380cdee7ce37ca9cf6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document dynamic operator configuration,FLINK-27668,13445428,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,gyfora,gyfora,17/May/22 13:24,01/Jun/22 09:43,04/Jun/24 20:51,01/Jun/22 09:43,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The Kubernetes operator now support dynamic config changes through the operator configmap.

This feature is not documented properly and it should be added to the operations/configuration page",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 01 09:43:42 UTC 2022,,,,,,,,,,"0|z12fyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/22 05:23;nicholasjiang;[~gyfora], could you please assign this ticket to me? I have pushed a pull request for document.;;;","01/Jun/22 09:43;gyfora;merged

main: c652365944e5d38a7ec48600fd4f8a25751aec57

release-1.0: 2e55d4193adc22b1025ff8c8c48042a6d5cfaa3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"YARNHighAvailabilityITCase fails with ""Failed to delete temp directory /tmp/junit1681""",FLINK-27667,13445400,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ferenc-csaky,martijnvisser,martijnvisser,17/May/22 11:02,29/Sep/22 14:19,04/Jun/24 20:51,22/Jun/22 07:05,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Deployment / YARN,,,,,0,pull-request-available,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35733&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=29208
 
{code:bash}
May 17 08:36:22 [INFO] Results: 
May 17 08:36:22 [INFO] 
May 17 08:36:22 [ERROR] Errors: 
May 17 08:36:22 [ERROR] YARNHighAvailabilityITCase » IO Failed to delete temp directory /tmp/junit1681... 
May 17 08:36:22 [INFO] 
May 17 08:36:22 [ERROR] Tests run: 28, Failures: 0, Errors: 1, Skipped: 0 
May 17 08:36:22 [INFO] 
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 22 07:07:05 UTC 2022,,,,,,,,,,"0|z12fsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 10:42;chesnay;Aww man, so the junit clean up fails if some concurrent cleanup within the tmp dir is running...;;;","20/May/22 08:22;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35856&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","23/May/22 12:35;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35960&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","25/May/22 07:13;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36024&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","26/May/22 02:24;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36053&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","30/May/22 16:07;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36183&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","31/May/22 11:57;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36190&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","31/May/22 11:59;godfreyhe;cc [~wangyang0918];;;","01/Jun/22 12:26;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36190&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f;;;","06/Jun/22 11:30;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36302&view=logs&j=298e20ef-7951-5965-0e79-ea664ddc435e&t=d4c90338-c843-57b0-3232-10ae74f00347;;;","06/Jun/22 12:13;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36316&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","07/Jun/22 09:31;martijnvisser;[~bgeng777] Could you perhaps help out with this test stability? ;;;","11/Jun/22 05:26;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36544&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=29516;;;","13/Jun/22 02:47;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36529&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","16/Jun/22 12:04;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36764&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=29161;;;","16/Jun/22 12:12;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36793&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","17/Jun/22 10:22;bgeng777;hi [~ferenc-csaky] [~chesnay] , I did some investigation and hope it can help us to enhence the stability:

In {{{}YARNHighAvailabilityITCase{}}}, we currently overwrite the {{teardown()}} method of the {{YarnTestBase}} (see [codes|https://github.com/apache/flink/blob/3ae4c6f5a48105d00807e8ce02e70d4c092cbf40/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNHighAvailabilityITCase.java#L126] here) and as a result, only {{{}YARNHighAvailabilityITCase{}}}'s {{teardown()}} will be executed after all HA tests finish.

The above behavior may lead to potential race condition: 
{{YARNHighAvailabilityITCase}} relies on the {{@TempDir protected static File tmp}} defined in {{YarnTestBase}} as YARN's parent dir of staging dir of each YARN application to launch the mini YARN cluster using {{{}RawLocalFileSystem{}}}. According to JUnit5's [doc|https://junit.org/junit5/docs/5.4.1/api/org/junit/jupiter/api/io/TempDir.html], the TempDir will be deleted recursively when the test class has finished execution. But as the {{teardown()}} method of the base class is not executed, there is no guarantee when the mini YARN cluster will be cleaned up(e.g. deleting staging dir like {{{}/tmp/junit1681458499635252469/.flink/application_1652775626514_0003{}}}).
As a result, when JUnit wants to delete TempDir and it happens to see the staging dir, it is possible that the staging dir is deleted by YARN's cleanup method before being deleted by JUnit, which incurs the exception of {{{}java.io.IOException: Failed to delete temp directory{}}}.

I tried to add the call of the teardown method of the base class (see [codes|https://github.com/bgeng777/flink/commit/c4a4c8c8d4d1dafaa2875dd8d88133e38a78d438] here) and manually triggered the test for 5 times(test[1|https://dev.azure.com/samuelgeng7/Flink/_build/results?buildId=119&view=results], [2|https://dev.azure.com/samuelgeng7/Flink/_build/results?buildId=120&view=results], [3|https://dev.azure.com/samuelgeng7/Flink/_build/results?buildId=121&view=results], [4|https://dev.azure.com/samuelgeng7/Flink/_build/results?buildId=122&view=results], [5|https://dev.azure.com/samuelgeng7/Flink/_build/results?buildId=123&view=results]) in my own azure pipeline. All of them passed the misc tests.

 

One further question is that I am not so sure why this test keeps stable in JUnit 4. Maybe the @TempDir annotation behaves somehow differently. 

 ;;;","21/Jun/22 11:33;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=36979&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b;;;","21/Jun/22 11:51;martijnvisser;[~bgeng777] Thanks for checking this! Could you open a PR for that?;;;","21/Jun/22 14:47;ferenc-csaky;[~martijnvisser] [~bgeng777] FYI: I ended up incorporating those changes as well in the #20012 PR.;;;","22/Jun/22 07:05;martijnvisser;Fixed in master: a7c1896a33ce678a17c74e45c4f34eb54270c493;;;","22/Jun/22 07:05;bgeng777;Hi [~martijnvisser] , I see [~ferenc-csaky]  has created the PR which LGTM. The change in my mind is exactly the same as his PR and I may not need add other changes. ;;;","22/Jun/22 07:07;martijnvisser;[~bgeng777] Yes I saw, thanks for that! I've merged the PR :);;;",,,,,,,,,
Cover manual savepoint triggering in E2E tests,FLINK-27666,13445396,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,gyfora,gyfora,17/May/22 10:36,21/May/22 15:35,04/Jun/24 20:51,21/May/22 15:35,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We should extend our e2e tests so that they cover manual savepoint triggering using the savepointTriggerNonce.

We should verify that savepoint was triggered, recorded in the status and trigger information cleared afterwards.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 21 15:35:09 UTC 2022,,,,,,,,,,"0|z12fs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 04:13;aitozi;I could work on this.  please assign this to me, I will try to push a PR today;;;","21/May/22 15:35;gyfora;Merged:
main - 451a260182137ebaf2434bee2b87208013c7e3b9

release-1.0 - a743c52576e5edf3a8656211c255dd02c2e07f0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize event triggering on DeploymentFailedExceptions,FLINK-27665,13445389,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,17/May/22 10:14,24/Nov/22 01:03,04/Jun/24 20:51,18/May/22 09:03,kubernetes-operator-0.1.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Use `EventUtils` when handling `DeploymentFailedExceptions` to avoid appending new events on every reconcile loop:

!image-2022-05-17-12-13-19-489.png|width=1365,height=575!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/22 10:08;image-2022-05-17-12-08-42-597.png;https://issues.apache.org/jira/secure/attachment/13043780/image-2022-05-17-12-08-42-597.png","17/May/22 10:13;image-2022-05-17-12-13-19-489.png;https://issues.apache.org/jira/secure/attachment/13043779/image-2022-05-17-12-13-19-489.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 09:03:07 UTC 2022,,,,,,,,,,"0|z12fqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 09:03;morhidi;e847662b157a8213f802fefabecc7fe88dbc8cc1 merged to main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cron_snapshot_deployment_maven fails due to JavaDoc building error,FLINK-27664,13445378,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,alexanderpreuss,martijnvisser,martijnvisser,17/May/22 09:29,17/May/22 11:49,04/Jun/24 20:51,17/May/22 11:49,1.15.0,,,,,,,,,,,,,,,,,,Build System,,,,,0,test-stability,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35684&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7&l=14026

{code:bash}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.9.1:jar (attach-javadocs) on project flink-architecture-tests-production: MavenReportException: Error while creating archive:
[ERROR] Exit code: 1 - javadoc: error - class file for org.junit.platform.commons.annotation.Testable not found
[ERROR] 
[ERROR] Command line was: /usr/lib/jvm/java-8-openjdk-amd64/jre/../bin/javadoc -Xdoclint:none @options @packages
[ERROR] 
[ERROR] Refer to the generated Javadoc files in '/__w/1/s/flink-architecture-tests/flink-architecture-tests-production/target/apidocs' dir.
[ERROR] -> [Help 1]
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 11:49:44 UTC 2022,,,,,,,,,,"0|z12fo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 10:33;chesnay;Isn't this a duplicate of FLINK-27167? Just need a backport...;;;","17/May/22 10:59;martijnvisser;Thanks [~chesnay] - CC [~alexanderpreuss];;;","17/May/22 11:49;alexanderpreuss;Indeed, I created a backport to 1.15 with the id of the original PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upsert-kafka can't process delete message from upsert-kafka sink,FLINK-27663,13445368,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,pensz,pensz,17/May/22 09:03,19/May/22 03:15,04/Jun/24 20:51,19/May/22 03:15,1.13.6,1.14.4,1.15.0,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,0,,,,,"upsert-kafka write DELETE data as Kafka messages with null values (indicate tombstone for the key).

But when use upsert-kafka as a source table to consumer kafka messages write by upsert-kafka sink, DELETE messages will be ignored.

 

related sql :

 

 
{code:java}
create table order_system_log(
  id bigint,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
 'connector' = 'upsert-kafka',
 'topic' = 'test_use',
 'properties.bootstrap.servers' = 'your broker',
 'properties.group.id' = 'your group id',
 'value.json.fail-on-missing-field' = 'false',
 'value.json.ignore-parse-errors' = 'true',
 'key.json.fail-on-missing-field' = 'false',
 'key.json.ignore-parse-errors' = 'true',
 'key.format' = 'json',
 'value.format' = 'json'
);
select
*
from
order_system_log
;
{code}
 

 

The problem may be produced by DeserializationSchema#deserialize,

this method does not collect data while subclass's deserialize return null.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 02:47:22 UTC 2022,,,,,,,,,,"0|z12fls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 09:08;martijnvisser;[~pensz] What is the error that you're getting? Based on https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/upsert-kafka/ I would expect that in changelog mode you would get a record with a null value. ;;;","17/May/22 11:30;pensz;[~martijnvisser] thanks for your reply.

There is no error. delete message write by upsert-kafka is ignored.

for example:

kafka message: key: \{""id"":1} value: null

expect: RowData(RowKind = DELETE, id=1)

actual: no RowData collected.;;;","17/May/22 12:05;martijnvisser;[~pensz] Hmmm I would actually expect that there is a test that validates this; if not, then a test should definitely be added for this. But I don't know too much above specific functionality tbh;;;","18/May/22 03:41;pensz; 

1. produce a DELETE message to kafka 
{code:java}
echo '{""id"":1}#null' | kafka-console-producer --broker-list $broker --topic test_use --property ""parse.key=true"" --property ""key.separator=#""{code}
 
2. run a flink sql job:
{code:java}
SET sql-client.execution.result-mode = tableau;
create table delete_test(
  id bigint,
  PRIMARY KEY (id) NOT ENFORCED
) WITH ( 'connector' = 'upsert-kafka',
 'topic' = 'test_use',
 'properties.bootstrap.servers' = 'your broker',
 'properties.group.id' = 'your group id',
 'value.json.fail-on-missing-field' = 'false',
 'value.json.ignore-parse-errors' = 'true',
 'key.json.fail-on-missing-field' = 'false',
 'key.json.ignore-parse-errors' = 'true',
 'key.format' = 'json',
 'value.format' = 'json');
select
*
from
delete_test
;
{code}

 

3.  expect : 
 
{code:java}
+----+----------------------+
| op |                   id |
+----+----------------------+
| -D |                    1 | {code}
 
 
actual: no output
 ;;;","18/May/22 03:53;pensz;I have changed component to {{{}Connectors/Kafka{}}}.

Flink miss unit test on following method.
{code:java}
org.apache.flink.streaming.connectors.kafka.table.DynamicKafkaDeserializationSchema#deserialize(org.apache.kafka.clients.consumer.ConsumerRecord<byte[],byte[]>, org.apache.flink.util.Collector<org.apache.flink.table.data.RowData>){code};;;","18/May/22 07:15;martijnvisser;[~Leonard] What do you think? 
Also CC [~renqs];;;","18/May/22 10:38;renqs;[~pensz] I checked the code and tried to reproduce your case. Upsert Kafka did process delete message from Kafka so it's not a bug. The reason you didn't see the DELETE event in SQL client is that the changelog normalizer did the trick. 

If you check the execution plan of SQL ""SELECT * FROM delete_table"", you'll find a ChangelogNormalize node, which creates a {{ProcTimeDeduplicateKeepLastRowFunction}} to drop invalid changelog event (like a DELELE without an associated INSERT in the stream before), so in SQL client you can't see the DELETE you made manually.

If you create an INSERT then a DELETE manually in Kafka, you could see these two events in SQL client, with ""sql-client.execution.result-mode"" set to ""changelog"". ;;;","18/May/22 11:40;pensz;[~renqs] thanks for your reply.

I have set sql-client.execution.result-mode = tableau with sql-client -f to view changelog events.

I know ChangelogNormalize node will drop invalid event.

To confirm this ,  I have tried insert a row before DELETE.

 
{code:java}
echo '{""id"":1}#{""id"":1}' | kafka-console-producer --broker-list $broker --topic test_use --property ""parse.key=true"" --property ""key.separator=#""
echo '{""id"":1}#null' | kafka-console-producer --broker-list $broker --topic test_use --property ""parse.key=true"" --property ""key.separator=#""
{code}

 

3.  expect : 
 
{code:java}
+----+----------------------+
| op |                   id |
+----+----------------------+
| +I |                    1 | 
| -D |                    1 | {code}
 

actual:
{code:java}
+----+----------------------+
| op |                   id |
+----+----------------------+
| +I |                    1 | {code}
 

Flink version: 1.13.0;;;","18/May/22 11:57;renqs;[~pensz] I doubt that the ""null"" in your echo command will be finally converted to a 4-letter string ""null"" in Kafka. Could you try to write a piece of Java code to produce a record with empty value into Kafka? ;;;","18/May/22 13:39;pensz;yes, null is 4-letter string ""null"".  not null.

""null"" or """" is deserialized as null in json.

So , is it more convenient to test to support it :) ?

 

 ;;;","19/May/22 02:47;renqs;[~pensz] [KIP-810|https://cwiki.apache.org/confluence/display/KAFKA/KIP-810%3A+Allow+producing+records+with+null+values+in+Kafka+Console+Producer] introduces new property in Kafka console producer to support producing null values, but it's only available from Kafka 3.2, so I'm afraid you have to write Java code to create a {{ProducerRecord}} with null value and produce into Kafka with {{KafkaProducer}} if you are using Kafka <3.2.;;;",,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate TypeInformationTestBase to Junit5,FLINK-27662,13445345,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,17/May/22 07:35,17/May/22 10:55,04/Jun/24 20:51,17/May/22 10:55,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,"The task is a follow up for the feedback comment
https://github.com/apache/flink/pull/19716#discussion_r873685730",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 10:55:53 UTC 2022,,,,,,,,,,"0|z12fgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 10:55;chesnay;master: 62d64eb48139473f7dfd622f6fff723d21e65a5c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Metric]Flink-Metrics PrometheusPushGatewayReporter support pushgateway http authentication,FLINK-27661,13445339,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jianjiao,jianjiao,17/May/22 07:08,15/Feb/23 09:09,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / Metrics,,,,,1,features,pull-request-available,,,"Pushgateway has been deployed via http_config using basic_auth_users:  [ <string>: <secret> ... ] 

requires user`s username and password otherwise only Unauthorized we can get return.

We deploy our flink-cluster in yarn mode .And want to use PrometheusPushGatewayReporter to see what's going on with our task.

But I found that Metrics Reporter : PrometheusPushGatewayReporter does not support pushgateway authentication. So the metrics data in on YARN mode cannot be reported to our pushGateway. 

This feature is already implemented in Flinks-1.16.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Java,,Mon Jul 25 22:37:36 UTC 2022,,,,,,,,,,"0|z12ffc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 14:13;shuimu;Well done;;;","17/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","25/Jul/22 22:37;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table API support create function using customed jar,FLINK-27660,13445334,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ptjhuang,lsy,lsy,17/May/22 06:54,13/Sep/22 10:49,04/Jun/24 20:51,13/Sep/22 10:49,1.16.0,,,,,,,,,,,,,,1.17.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 13 10:49:53 UTC 2022,,,,,,,,,,"0|z12fe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/22 15:33;ZhenqiuHuang;[~lsy] [~jark]

Looks like implementation of this jira depends on the.
1. https://issues.apache.org/jira/browse/FLINK-27651
2. https://issues.apache.org/jira/browse/FLINK-27658

I want to clarify the scope of the ticket, correct me if I am wrong.
1. Add APIs in table environment for create function with Resource URI
2. Use Resource URI to download to local path if it is remote resource. 
3. Use local file path for URLclassloader to initialize udf and register to function catalog.

Some questions about the implementation.
1. What local path we are recommending for resource downloading? 
2. For remote resource, What's the recommendation schema (hdfs / http) for unit test and integration test?




;;;","13/Jun/22 02:28;lsy;Hi, [~ptjhuang], it also depends on [FLINK-27861|https://issues.apache.org/jira/browse/FLINK-27861].  The introduced `UserResourceManager` manage all registered resources including downloading resource, so we can start this ticket after the above pr merge.

>> I want to clarify the scope of the ticket, correct me if I am wrong.

These three point you understand are right.

>> Some questions about the implementation.
 # In FLINK-27861 we introduce an option to control the local path, default value is io.tmp.dirs.
 # Flink `FileSystem` can't support http scheme. For hdfs, I think we only can test manully.

 ;;;","13/Jun/22 16:07;ZhenqiuHuang;[~lsy]

Make sense. I  have started working on the change. Will wait for your changes in FLINK-27861 to merge. ;;;","13/Sep/22 10:49;jark;Fixed in master: fbd369ce40665a436ecae65d6f90963d907193ab to 3ff0ef8474f4c1da7ad21690d82dbbfa1daed868;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Planner support to use jar which is registered by ""USING JAR"" syntax",FLINK-27659,13445329,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,lsy,lsy,lsy,17/May/22 06:44,13/Dec/22 04:01,04/Jun/24 20:51,20/Jul/22 09:41,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 20 09:41:22 UTC 2022,,,,,,,,,,"0|z12fd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 09:41;fsk119;Implemented in the master: cfc107716357c49b14ed59e0359e656764ea7f99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkUserCodeClassLoader expose addURL method to allow to register jar dynamically,FLINK-27658,13445327,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,17/May/22 06:41,14/Jun/22 04:16,04/Jun/24 20:51,14/Jun/22 04:16,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / Task,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 04:16:50 UTC 2022,,,,,,,,,,"0|z12fco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/22 04:16;zhuzh;Done via:

2e37a06368596ca9ed04366a879f2facfb7ef509;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement remote operator state backend in PyFlink,FLINK-27657,13445324,13444563,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Juntao Hu,Juntao Hu,Juntao Hu,17/May/22 06:09,30/May/22 11:24,04/Jun/24 20:51,30/May/22 11:24,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,"This is for supporting broadcast state, exisintg map state implementation and caching handler can be reused.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 11:24:26 UTC 2022,,,,,,,,,,"0|z12fc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/22 11:24;hxbks2ks;Merged into master via 9be49ff871feace87aed9d4e3f8132bcf0cd3945;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add parquet file format ,FLINK-27656,13445300,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,openinx,openinx,17/May/22 02:26,25/Aug/22 02:31,04/Jun/24 20:51,15/Jun/22 02:39,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,,,,,"The flink table store does not support parquet file format now. 

Will try to publish a PR to include parquet file format in the flink table store.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 06:03:11 UTC 2022,,,,,,,,,,"0|z12f6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 02:47;lzljs3620320;There is a Jira for parquet. We can link it.;;;","17/May/22 06:03;liyubin117;Hi, parquet file format has been supported in https://issues.apache.org/jira/browse/FLINK-27100

and I am making parquet as a built-in format in table-store, see details in https://issues.apache.org/jira/browse/FLINK-27207

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Avro File statistic collector,FLINK-27655,13445298,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,openinx,openinx,17/May/22 02:25,25/Aug/22 02:33,04/Jun/24 20:51,20/Jul/22 03:16,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,,,,,"Currently, the flink table store's avro file writer don't provide its File statistic collector. So we have to use the generic FieldStatsCollector. 

In fact, the correct direction is:  Making all format writer has their own FileStatsCollector, so that we can just parse the columnar statistic from the file tailer, instead of comparing each column max-min when writing the records into the columnar file. 

In this way,  I think we can just remove the FileFormatImpl class and FieldStatsCollector class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 20 03:16:03 UTC 2022,,,,,,,,,,"0|z12f68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 03:16;lzljs3620320;Avro dose not provide statistics.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Older jackson-databind found in /flink-kubernetes-shaded-1.0-SNAPSHOT.jar,FLINK-27654,13445276,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wangyang0918,jbusche,jbusche,16/May/22 22:57,22/May/22 23:15,04/Jun/24 20:51,22/May/22 23:15,kubernetes-operator-0.1.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"A twistlock security scan of the latest kubernetes flink operator is showing an older version of jackson-databind in the /flink-kubernetes-shaded-1.0-SNAPSHOT.jar file.  I don't know how to control/update the contents of this snapshot file.  

I see this in the report (Otherwise, everything else looks good!):

======
severity: High

cvss: 7.5 

riskFactors: Attack complexity: low,Attack vector: network,DoS,Has fix,High severity

cve: CVE-2020-36518

Link: [https://nvd.nist.gov/vuln/detail/CVE-2020-36518]

packageName: com.fasterxml.jackson.core_jackson-databind

packagePath: /flink-kubernetes-operator-1.0-SNAPSHOT-shaded.jar

description: jackson-databind before 2.13.0 allows a Java StackOverflow exception and denial of service via a large depth of nested objects.

=====

I'd be glad to try to fix it, I'm just not sure how the jackson-databind versions are controlled in this /flink-kubernetes-operator-1.0-SNAPSHOT-shaded.jar ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 22 23:15:16 UTC 2022,,,,,,,,,,"0|z12f20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 02:52;wangyang0918;Thanks [~jbusche] for reporting this ticket.

It seems that we are bundling the com.fasterxml.jackson.core:jackson-databind:jar:2.13.2.2:compile in the flink-kubernetes-operator-1.0-SNAPSHOT-shaded.jar. And this version does not have vulnerability.

 

The dependency is introduced by the flink-kubernetes module in the upstream project Flink. You could find the pom here[1].

 

[1]. https://github.com/apache/flink/blob/master/flink-kubernetes/pom.xml#L63;;;","19/May/22 23:54;jbusche;Thanks [~wangyang0918] 

It looks like the kubernetes client version [5.5.0|https://github.com/fabric8io/kubernetes-client/blob/master/CHANGELOG.md#550-2021-06-30] is pretty old.
[5.12.2|https://github.com/fabric8io/kubernetes-client/blob/master/CHANGELOG.md#5122-2022-04-06] is the latest v5 client version...  I can try putting a PR in the upstream flink repo, but I'm not sure how to test it to ensure I'm not breaking something there.  I'd feel badly if I ended up unintentionally breaking both products, and not certain that the 5.12.2 version will update the jackson-databind in the end anyway.

Any suggestions on the best way to proceed? I'm happy to try!

 ;;;","20/May/22 02:39;wangyang0918;[~jbusche] You are right. The kubernetes-client in Flink project is a little old and I am not against with bumping the version. The reason we are lazy to update the kubernetes-client is that Flink only depends some core features(e.g. creating deployment/pod/configmap/service, leader election, watch/informer) and they are stable enough now. Currently, these functionalities has already been covered by the e2e tests in Flink project[1]. It is not a burden to bump the version. If you want to do this, we could create a new dedicated ticket.

I have to clarify one more thing. In Flink project, we do not need to bump the kubernetes-client version to update the jackson-databind. Actually, the version is managed by parent pom[2] via maven dependencyManagement.

This ticket also inspires me to verify the bundled the jackson-databind in the flink-kubernetes-operator module. The version is ""com.fasterxml.jackson.core:jackson-databind:jar:2.13.1:compile"". It is introduced by ""io.fabric8:kubernetes-client:jar:5.12.1:compile"". From the maven repository, 2.13.1 has one known vulnerability[3].

Would you like to create a PR to fix this? I believe it is simple since we could use dependencyManagement in the parent pom to pin the jackson version just like Flink project.


[1]. https://github.com/apache/flink/blob/release-1.15/flink-end-to-end-tests/test-scripts
[2]. https://github.com/apache/flink/blob/release-1.15/pom.xml#L563
[3]. https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind;;;","20/May/22 03:07;wangyang0918;Of cause, we could also bump the kubernetes-client in flink-kubernetes-operator/pom.xml from 5.12.1 to 5.12.2[1]. But it still does not fix the vulnerability and we still need the above dependencyManagement solution.

[1]. https://mvnrepository.com/artifact/io.fabric8/kubernetes-client/5.12.2;;;","20/May/22 03:10;wangyang0918;I am making this ticket as a blocker now since we should fix the known vulnerability before releasing.;;;","22/May/22 03:18;wangyang0918;I am assigning this ticket to myself so that we could prepare the first release candidate on next Monday.;;;","22/May/22 23:10;mbalassi;Thanks for the report [~jbusche] and the fix [~wangyang0918].;;;","22/May/22 23:15;mbalassi;Fixed as 3fee956b628373f2939bf7e0d30fa747da441b92 in main and as 269a964371b52af66441459c3806bd1b89260da4 in release-1.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Pulsar Connector bug: The startCursor has been setted default value of ""MessageId.earliest"", Every time to restart the job，the Consumer will do the seek operation.",FLINK-27653,13445218,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wawa,wawa,16/May/22 15:43,16/May/22 15:43,04/Jun/24 20:51,,1.14.3,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,,,,,"Pulsar Connector bug: The startCursor has been setted default value of 'MessageId.earliest', Every time to restart the job，the Consumer will do the seek operation.

Of course，we can set like this : '.setStartCursor(StartCursor.latest())', then, when the job restarted, it will do this seek operation : consumer.seek(MessageId.latest). As a result，some messages will be lost.

What we really want is , the consumer can subscribes from where it stopped.

In general, subscribes from 'earliest' or 'latest', we can use the below operation instead of seek:

[ConsumerBuilder|https://pulsar.apache.org/api/client/2.9.0-SNAPSHOT/org/apache/pulsar/client/api/ConsumerBuilder.html]<[T|https://pulsar.apache.org/api/client/2.9.0-SNAPSHOT/org/apache/pulsar/client/api/ConsumerBuilder.html]> subscriptionInitialPosition([SubscriptionInitialPosition |https://pulsar.apache.org/api/client/2.9.0-SNAPSHOT/org/apache/pulsar/client/api/SubscriptionInitialPosition.html]subscriptionInitialPosition)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 15:43:57.0,,,,,,,,,,"0|z12epk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactManager.Rewriter cannot handle different partition keys invoked compaction,FLINK-27652,13445214,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,16/May/22 15:03,29/May/22 13:25,04/Jun/24 20:51,19/May/22 02:09,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"h3. Issue Description

When enabling {{commit.force-compact}} for the partitioned managed table, there had a chance that the successive synchronized
writes got failure.  {{rewrite}} method messing up with the wrong data file with the {{partition}} and {{{}bucket{}}}.
{code:java}
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.FileNotFoundException: File file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit5920507275110651781/junit4163667468681653619/default_catalog.catalog/default_database.db/T1/f1=Autumn/bucket-0/data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not exist or the user running Flink ('jane.cjm') has insufficient permissions to access it. at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:172)
{code}
However, data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not belong to partition Autumn. It seems like the rewriter found the wrong partition/bucket with the wrong file.
h3. How to Reproduce
{code:java}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.flink.table.store.connector;

import org.junit.Test;

import java.util.Collections;
import java.util.List;
import java.util.concurrent.ExecutionException;

/** A reproducible case. */
public class ForceCompactionITCase extends FileStoreTableITCase {

    @Override
    protected List<String> ddl() {
        return Collections.singletonList(
                ""CREATE TABLE IF NOT EXISTS T1 (""
                        + ""f0 INT, f1 STRING, f2 STRING) PARTITIONED BY (f1)"");
    }

    @Test
    public void test() throws ExecutionException, InterruptedException {
        bEnv.executeSql(""ALTER TABLE T1 SET ('num-levels' = '3')"");
        bEnv.executeSql(""ALTER TABLE T1 SET ('commit.force-compact' = 'true')"");
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(1, 'Winter', 'Winter is Coming')""
                                + "",(2, 'Winter', 'The First Snowflake'), ""
                                + ""(2, 'Spring', 'The First Rose in Spring'), ""
                                + ""(7, 'Summer', 'Summertime Sadness')"")
                .await();
        bEnv.executeSql(""INSERT INTO T1 VALUES(12, 'Winter', 'Last Christmas')"").await();
        bEnv.executeSql(""INSERT INTO T1 VALUES(11, 'Winter', 'Winter is Coming')"").await();
        bEnv.executeSql(""INSERT INTO T1 VALUES(10, 'Autumn', 'Refrain')"").await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(6, 'Summer', 'Watermelon Sugar'), ""
                                + ""(4, 'Spring', 'Spring Water')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(66, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(6666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(66666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(666666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
        bEnv.executeSql(
                        ""INSERT INTO T1 VALUES(6666666, 'Summer', 'Summer Vibe'),""
                                + "" (9, 'Autumn', 'Wake Me Up When September Ends')"")
                .await();
    }
}

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27515,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 02:09:15 UTC 2022,,,,,,,,,,"0|z12eoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 15:04;qingyue;h3. Full Stacktrace
{code:java}
org.apache.flink.table.store.connector.ForceCompactionITCase.test(ForceCompactionITCase.java:65)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
	at akka.dispatch.OnComplete.internal(Future.scala:300)
	at akka.dispatch.OnComplete.internal(Future.scala:297)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.FileNotFoundException: File file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit5920507275110651781/junit4163667468681653619/default_catalog.catalog/default_database.db/T1/f1=Autumn/bucket-0/data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not exist or the user running Flink ('jane.cjm') has insufficient permissions to access it.
	at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:172)
	at org.apache.flink.table.store.connector.sink.StoreSinkWriter.prepareCommit(StoreSinkWriter.java:51)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emitCommittables(SinkWriterOperator.java:196)
	at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.endInput(SinkWriterOperator.java:183)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.FileNotFoundException: File file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit5920507275110651781/junit4163667468681653619/default_catalog.catalog/default_database.db/T1/f1=Autumn/bucket-0/data-59826283-c5d1-4344-96ae-2203d4e60a57-0 does not exist or the user running Flink ('jane.cjm') has insufficient permissions to access it.
		at org.apache.flink.table.store.connector.sink.StoreSinkWriter.closeWriter(StoreSinkWriter.java:217)
		at org.apache.flink.table.store.connector.sink.StoreSinkWriter.close(StoreSinkWriter.java:226)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:233)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:222)
		at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.close(SinkWriterOperator.java:216)
		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163)
		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
		at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916)
		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930)
		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930)
		... 3 more
{code};;;","17/May/22 02:39;lzljs3620320;There is indeed a bug here, the reason is that the partition object being reused should be copied inside the StoreSinkWriter:
{code:java}
private RecordWriter getWriter(BinaryRowData partition, int bucket) {
    Map<Integer, RecordWriter> buckets = writers.get(partition);
    if (buckets == null) {
        buckets = new HashMap<>();
        writers.put(partition.copy(), buckets);
    }

    return buckets.computeIfAbsent(
            bucket,
            k ->
                    overwrite
                            ? fileStoreWrite.createEmptyWriter(
                                    partition.copy(), bucket, compactExecutor)
                            : fileStoreWrite.createWriter(partition.copy(), bucket, compactExecutor));
} {code};;;","19/May/22 02:09;lzljs3620320;master: bc46116bef59be033db8b85655e53e479113f462;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support CREATE FUNCTION USING JAR syntax,FLINK-27651,13445213,13256176,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,16/May/22 15:01,12/Jun/22 05:11,04/Jun/24 20:51,12/Jun/22 05:11,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 12 05:11:42 UTC 2022,,,,,,,,,,"0|z12eog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/22 05:11;jark;Fixed in master: c1b32be0faa29ecf0b7b358404aba534cba59903;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First environment variable of top level pod template is lost,FLINK-27650,13445207,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,sparadis,sparadis,16/May/22 14:26,20/Jul/22 13:01,04/Jun/24 20:51,15/Jul/22 06:44,kubernetes-operator-0.1.0,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,,,,,"I am using the Flink operator image *apache/flink-kubernetes-operator:0.1.0* to deploy Flink 1.14.4 job. The deployment manifest makes use of pod template feature to inject environment variable to control structured JSON logging.

I noticed the first defined environment variable is never injected into the JobManager nor TaskManager pods. The work around is to define a dummy env. var.

Here's the manifest template. This gets processed by a tool that will first expand ${ENV_VAR} reference with values provided by our CI pipeline. We should not have to create the FLINK_COORDINATES_DUMMY env var.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/22 14:32;sparadis;flink-27650.yaml;https://issues.apache.org/jira/secure/attachment/13043715/flink-27650.yaml",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 18:39:57 UTC 2022,,,,,,,,,,"0|z12en4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 18:58;gyfora;Thank you [~sparadis] for reporting this, we'll take a look :);;;","16/May/22 19:18;gyfora;[~thw] , I could reproduce this. Seems to be a bug in the array merging logic. Can be reproduced easily with the following testcase:


{noformat}
@Test
public void testEnvMerge() {
    var container1 = new Container();
    container1.setName(""c"");
    container1.setEnv(List.of(envVar(""k1"", ""v1""), envVar(""k2"", ""v2"")));

    var container2 = new Container();
    container2.setName(""c"");
    container2.setEnv(List.of(envVar(""k3"", ""v3""), envVar(""k4"", ""v4"")));

    var pod1 =
            TestUtils.getTestPod(
                    ""pod1 hostname"", ""pod1 api version"", Arrays.asList(container1));
    var pod2 =
            TestUtils.getTestPod(
                    ""pod2 hostname"", ""pod2 api version"", Arrays.asList(container2));

    var mergedEnv = FlinkUtils.mergePodTemplates(pod1, pod2).getSpec()
            .getContainers()
            .get(0)
            .getEnv();

    assertEquals(List.of(envVar(""k1"", ""v1""), envVar(""k2"", ""v2""), envVar(""k3"", ""v3""), envVar(""k4"", ""v4"")), mergedEnv);
}{noformat};;;","16/May/22 20:14;thw;That's as per [https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L83]

Maybe it is better to just override?

This isn't a ""blocker"" though.

 ;;;","17/May/22 12:27;sparadis;Thanks for the quick response/triage;;;","17/May/22 12:52;gyfora;In other words, if your jm/tm template defines N environment variables, due to how the merging is implemented, the first N variables of the top level template will be ignored.

You can definitely work around this for now and we will see if there is anything we can improve on this.;;;","17/May/22 18:39;sparadis;Okay then we can defined all variable topmost and override as needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce the number of outputted log lines by Elasticsearch6SinkE2ECase,FLINK-27649,13445195,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alexanderpreuss,martijnvisser,martijnvisser,16/May/22 13:46,17/May/22 04:26,04/Jun/24 20:51,16/May/22 19:33,,,,,,,,,,,,,,,1.16.0,,,,Connectors / ElasticSearch,,,,,0,pull-request-available,test-stability,,,"The current ElasticSearch tests create a large number of log lines, see https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35694&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=14702 as an example.

We should disable the logging by default. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 19:33:32 UTC 2022,,,,,,,,,,"0|z12ekg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 19:33;martijnvisser;Fixed in master: 772ac6341e704c31fd4b820ce98dfff94181b209;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review example YAMLs in the documentation,FLINK-27648,13445187,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,ConradJam,gyfora,gyfora,16/May/22 13:14,22/Jun/22 13:23,04/Jun/24 20:51,22/Jun/22 13:10,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,,,,,"The various documentation pages contain example yamls for FlinkDeployments that do not reflect the latest state of the project.

Some of these wouldn't even run anymore, we should review and update these",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 22 13:23:15 UTC 2022,,,,,,,,,,"0|z12eio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jun/22 13:52;ConradJam;Hi，[~gyfora] I want to take ticket，Please help to assign this ticket to me. Thanks.;;;","07/Jun/22 13:58;gyfora;Thank you! I think we fixed most of the examples but it would be good to check to be safe :) ;;;","22/Jun/22 13:23;ConradJam;I plan to retest the 1.1 version when it is ready for release [~gyfora] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Metrics documentation to include newly added metrics,FLINK-27647,13445185,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,16/May/22 13:11,24/Nov/22 01:02,04/Jun/24 20:51,20/May/22 13:10,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We now support a few operator specific metrics out of the box, we should improve the metrics documentation to highlight these",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 13:10:20 UTC 2022,,,,,,,,,,"0|z12ei8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 13:10;wangyang0918;Fixed via:
main: ea86018735c934549f6be4b19111c633b1386b7c
release-1.0: eaf96a1449da2114a4276f42d86fcab1ada53672;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Roadmap page for Flink Kubernetes operator,FLINK-27646,13445184,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ConradJam,gyfora,gyfora,16/May/22 13:09,07/Jun/22 10:54,04/Jun/24 20:51,07/Jun/22 10:54,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,We should create a dedicated wiki page for the current roadmap of the operator and link it to the overview page in our docs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/22 06:05;ConradJam;image-2022-05-21-14-05-06-376.png;https://issues.apache.org/jira/secure/attachment/13043988/image-2022-05-21-14-05-06-376.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 07 10:54:39 UTC 2022,,,,,,,,,,"0|z12ei0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 07:53;ConradJam;Hi [~gyfora] ，I want to try this ticket, can i take it ?;;;","17/May/22 12:58;gyfora;Hi [~ConradJam] . You can take the ticket if you would like of course.

In any case it would be good to sync together on the actual roadmap (the content of the page);;;","20/May/22 01:48;ConradJam;OK，I would like to ask is this page built on Flink's Confluence? Or do we build new ways paths [~gyfora] ;;;","20/May/22 05:42;gyfora;I think we have two alternatives here:

1. Include the roadmap as part of the Apache Flink Roadmap Page: [https://flink.apache.org/roadmap.html#stateful-functions]
2. Create our own roadmap page in kubernetes operator docs and link it to the Flink roadmap page

 

What do you think?;;;","21/May/22 06:04;ConradJam;I think no problem, it helps keep the flink main roadmap page roadmap pages clean.

My idea is to create a roadmap page under k8s operator - development, and then improve the relevant information. As shown below

!image-2022-05-21-14-05-06-376.png!

At lastest link this url to the Apache Flink Roadmap Page: [https://flink.apache.org/roadmap.html#|https://flink.apache.org/roadmap.html#stateful-functions]

what do you think this idea？ [~gyfora] ;;;","21/May/22 06:11;gyfora;Sounds good!
I think we should also link this from the git main README and the Concepts/overview page.

We have compiled some roadmap items with [~matyas] as we are preparing for the release:
{noformat}
Our intention is to advance further on the Operator Maturity Model by adding more dynamic/automatic features

- Standalone deployment mode support [FLIP-225]
- Auto-scaling using Horizontal Pod Autoscaler
- Dynamic change of watched namespaces
- Pluggable Status and Event reporters (Making it easier to integrate with proprietary control planes)
{noformat}
We could start with something like this and iterate on it :) ;;;","21/May/22 06:24;ConradJam;OK, I'm going to organize our Roadmap page based on what you described above. Let me get to work, when I'm done, I'll leave it to you to review the detailed descriptions and citations of the routes, see if my descriptions are correct, and make suggestions on what needs to be modified 😁;;;","07/Jun/22 10:54;gyfora;merged to main b727d432930d414c113d9878dca079d6db9cec6d;;;",,,,,,,,,,,,,,,,,,,,,,,,
Update overview / supported features page for 1.0.0,FLINK-27645,13445183,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,16/May/22 13:08,24/Nov/22 01:02,04/Jun/24 20:51,19/May/22 12:26,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"A lot of new features have been implemented and Flink 1.15 support also brings a lot of valuable additions.

We should update the overview page with the supported features to reflect the new developments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 19 12:26:38 UTC 2022,,,,,,,,,,"0|z12ehs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 12:26;wangyang0918;Fixed via:
main: 8c7329ef7a6414e5a82fc386eff633e8c6bf1f59
release-1.0: ba44163cdfd27a6fe8cdf35ae8c7a057e1a3c048;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CRD documentation with new spec/status changes,FLINK-27644,13445182,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,16/May/22 13:07,22/May/22 05:57,04/Jun/24 20:51,22/May/22 05:57,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,,,,,"There are a number of new features / changes that are not reflected in the current documentation for the CRD.

We should update these for the release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 13:07:43.0,,,,,,,,,,"0|z12ehk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document new deployment lifecycle features for the operator,FLINK-27643,13445180,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,16/May/22 13:05,18/May/22 07:22,04/Jun/24 20:51,18/May/22 07:22,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"We should document the changes and new features to the core lifecycle management logic, including:
 * JM Deployment Recovery
 * Rollbacks
 * Any changed upgrade behavior
 * 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 07:22:52 UTC 2022,,,,,,,,,,"0|z12eh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 07:22;gyfora;merged to main 4e1f5d89ea3ad3c726b9351200872e47b663e365;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make sure that the Elasticsearch E2E tests only try a limited amount of retries in case of test failures,FLINK-27642,13445178,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,martijnvisser,martijnvisser,16/May/22 12:59,30/Sep/22 08:52,04/Jun/24 20:51,30/Sep/22 08:52,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,0,,,,,"The current Elasticsearch E2E tests keep retrying the test infinitely; we should limit the number of retries and else cancel the CI run. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 12:59:54.0,,,,,,,,,,"0|z12ego:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create view lost Time attribute in Hive Catalog,FLINK-27641,13445168,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,wudi1205,wudi1205,16/May/22 12:30,17/Aug/22 10:03,04/Jun/24 20:51,,1.12.3,1.14.4,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,"Create table in hive catalog with the following sql state. :

 
{code:java}
CREATE TABLE user_score(
  username varchar,
  score varchar,
  proctime AS PROCTIME()
) with (
  'connector'='datagen',
  'rows-per-second'='2',
  'fields.score.length'='2',
  'fields.username.length'='2'
);{code}
 

We can get the description:

 
{code:java}
DESCRIBE user_score;
+----------+-----------------------------+-------+-----+---------------+-----------+
|     name |                        type |  null | key |        extras | watermark |
+----------+-----------------------------+-------+-----+---------------+-----------+
| username |                      STRING |  true |     |               |           |
|    score |                      STRING |  true |     |               |           |
| proctime | TIMESTAMP_LTZ(3) *PROCTIME* | false |     | AS PROCTIME() |           |
+----------+-----------------------------+-------+-----+---------------+-----------+
{code}
 

 

However,view create in hive catalog will lost Time attribute in the proctime field:

 
{code:java}
create view view_score_hive_catalog as select * from user_score;{code}
 
{code:java}
DESCRIBE view_score_hive_catalog;
 
+----------+------------------+-------+-----+--------+-----------+
|     name |             type |  null | key | extras | watermark |
+----------+------------------+-------+-----+--------+-----------+
| username |           STRING |  true |     |        |           |
|    score |           STRING |  true |     |        |           |
| proctime | TIMESTAMP_LTZ(3) | false |     |        |           |
+----------+------------------+-------+-----+--------+-----------+
{code}
 

 

Otherwise,when we excute the same state. in default catalog, things are going to change:
{code:java}
Create view view_score_mem_catalog as select * from myhive.[hive_database].user_score;{code}
 
{code:java}
DESCRIBE view_score_mem_catalog;
+----------+-----------------------------+-------+-----+--------+-----------+
|     name |                        type |  null | key | extras | watermark |
+----------+-----------------------------+-------+-----+--------+-----------+
| username |                      STRING |  true |     |        |           |
|    score |                      STRING |  true |     |        |           |
| proctime | TIMESTAMP_LTZ(3) *PROCTIME* | false |     |        |           |
+----------+-----------------------------+-------+-----+--------+-----------+
{code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 12:30:56.0,,,,,,,,,,"0|z12eeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink not compiling, flink-connector-hive_2.12 is missing jhyde pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde ",FLINK-27640,13445158,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,foxss,pnowojski,pnowojski,16/May/22 12:07,13/Apr/23 09:43,04/Jun/24 20:51,12/Jan/23 12:24,1.16.0,,,,,,,,,,,,,,1.16.2,1.17.0,,,Build System,Connectors / Hive,,,,0,pull-request-available,stale-assigned,,,"When clean installing whole project after cleaning local {{.m2}} directory I encountered the following error when compiling flink-connector-hive_2.12:
{noformat}
[ERROR] Failed to execute goal on project flink-connector-hive_2.12: Could not resolve dependencies for project org.apache.flink:flink-connector-hive_2.12:jar:1.16-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-exec:jar:2.3.9 -> org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Failed to read artifact descriptor for org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde: Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [conjars (http://conjars.org/repo, default, releases+snapshots), apache.snapshots (http://repository.apache.org/snapshots, default, snapshots)] -> [Help 1]
{noformat}
I've solved this by adding 
{noformat}
        <repository>
            <id>spring-repo-plugins</id>
            <url>https://repo.spring.io/ui/native/plugins-release/</url>
        </repository>
{noformat}
to ~/.m2/settings.xml file. ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27894,FLINK-29201,FLINK-30362,FLINK-28406,FLINK-29404,,,,,,FLINK-31658,BIGTOP-3929,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 29 19:31:47 UTC 2023,,,,,,,,,,"0|z12ec8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 14:32;chesnay;FYI this is because you are using a newer maven version than recommended, which blocks http repositories.

The conjars repo is defined in the hive poms. We could override those, but it's not really ideal because this is only required for hive 2.X, and not 3.X that we also support.

;;;","19/May/22 11:23;chesnay;You could add this to your maven settings.xml:

{code}
<settings>
    <mirrors>
        <mirror>
            <id>conjars-https</id>
            <url>https://conjars.org/repo/</url>
            <mirrorOf>conjars</mirrorOf>
        </mirror>
    </mirrors>
</settings>
{code}

This has the advantage that we don't have to sprinkle the repo setting into different modules, and we'll continue to use conjars only for pentaho and nothing else. (If we were to set it in the poms then Maven would try to download everything from there first in the respective module).;;;","21/May/22 06:58;rmetzger;I just ran into this too. It is a bit annoying for first time contributors I guess.

In FLINK-26034, we added a maven wrapper. I'll raise a PR to update the build instructions in the README;;;","05/Jul/22 15:21;jingge;[~rmetzger] have you fired the PR to update the README? Would you like to link it to this Jira? Thanks!;;;","04/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Aug/22 19:04;liuml07;It failed to build with the same error when I used the mvn wrapper as well.
{code:java}
$ ./mvnw -v
Apache Maven 3.2.5...
$ ./mvnw clean package -DskipTests{code}
Adding the {{conjars}} mirror in settings.xml did not resolve the problem. I use ARM64 macOS but I do not think that matters.;;;","11/Dec/22 16:51;foxss;shall we exclude org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde instead, I don't think this package is used.

here is how hudi community fix  https://github.com/apache/hudi/pull/3034;;;","20/Dec/22 15:19;martijnvisser;[~luoyuxia] Do you know if this package is needed in the Hive connector, or can it safely be excluded?;;;","21/Dec/22 01:53;luoyuxia;I think it can safely be excluded since org.pentaho:pentaho-aggdesigner-algorithm is just a third level transitive dependency (hive-exec - > calcite-core -> pentaho-aggdesigner-algorithm) and I think we won't need it.;;;","11/Jan/23 14:07;martijnvisser;Fixed in master: 78ab08bfc8e28f36df77fdb1be06831a98bcc43d;;;","11/Jan/23 16:22;martijnvisser;Re-opening because pentaho still appears in the dependency tree, probably an exclusion was missed;;;","12/Jan/23 12:24;martijnvisser;Fixed in master: 881627dcda9f5cdea9e095d7aff854fbf9508785;;;","29/Mar/23 19:31;Sergey Nuyanzin;Fixed in 1.16: 36f39712d289b6b7b0b957a33bd1891266e3f992;;;",,,,,,,,,,,,,,,,,,,
"Flink JOIN uses the now() function when inserting data, resulting in data that cannot be deleted",FLINK-27639,13445157,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lvycc,lvycc,16/May/22 12:05,25/Aug/22 07:20,04/Jun/24 20:51,25/Aug/22 07:20,1.14.4,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,,,,,"I use the now() function as the field value when I insert data using SQL ，but I can't delete the inserted data，here is my sql：
{code:java}
//代码占位符
CREATE TABLE t_order (
    order_id INT,
    order_name STRING,
    product_id INT,
    user_id INT,
    PRIMARY KEY(order_id) NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'localhost',
    'port' = '3306',
    'username' = 'root',
    'password' = 'ycc123',
    'database-name' = 'wby_test',
    'table-name' = 't_order'
);
CREATE TABLE t_logistics (
    logistics_id INT,
    logistics_target STRING,
    logistics_source STRING,
    logistics_time TIMESTAMP(0),
    order_id INT,
    PRIMARY KEY(logistics_id) NOT ENFORCED
) WITH (
    'connector' = 'mysql-cdc',
    'hostname' = 'localhost',
    'port' = '3306',
    'username' = 'root',
    'password' = 'ycc123',
    'database-name' = 'wby_test',
    'table-name' = 't_logistics'
);
CREATE TABLE t_join_sink (
    order_id INT,
    order_name STRING,
    logistics_id INT,
    logistics_target STRING,
    logistics_source STRING,
    logistics_time timestamp,
    PRIMARY KEY(order_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://localhost:3306/wby_test?characterEncoding=utf8&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai',
    'table-name' = 't_join_sink',
    'username' = 'root',
    'password' = 'ycc123'
);
INSERT INTO t_join_sink
SELECT ord.order_id,
ord.order_name,
logistics.logistics_id,
logistics.logistics_target,
logistics.logistics_source,
now()
FROM t_order AS ord
LEFT JOIN t_logistics AS logistics ON ord.order_id=logistics.order_id; {code}
The debug finds that SinkUpsertMaterializer causes the problem ，the result of the now() function changes when I delete the data，therefore, the delete operation is ignored

But what can I do to avoid this problem？",,,,,,,,,,,,,,,,,,,FLINK-27849,,,,,,,,,,,,FLINK-27849,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 07:08:40 UTC 2022,,,,,,,,,,"0|z12ec0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 03:19;fsk119;Hi. Could you also share the test data with us and we can also test in our local environment?;;;","25/May/22 03:47;lvycc;Run  init  sql in Mysql:
{code:java}
//代码占位符
CREATE TABLE `t_order`  (
  `order_id` int NOT NULL AUTO_INCREMENT COMMENT '订单id',
  `order_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL COMMENT '订单名称',
  `product_id` int NULL DEFAULT NULL COMMENT '商品id',
  `user_id` int NULL DEFAULT NULL COMMENT '用户id',
  PRIMARY KEY (`order_id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 7 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of t_order
-- ----------------------------
INSERT INTO `t_order` VALUES (1, '李四买吹风机4', 1, 2);
INSERT INTO `t_order` VALUES (2, '王二买电饭煲4', 2, 1);
INSERT INTO `t_order` VALUES (3, '小明买口罩4', 3, 4);
INSERT INTO `t_order` VALUES (4, '张三买吹风机3', 1, 3);

CREATE TABLE `t_logistics`  (
  `logistics_id` int NOT NULL AUTO_INCREMENT,
  `logistics_target` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_source` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_time` datetime(0) NULL DEFAULT NULL,
  `order_id` int NULL DEFAULT NULL,
  PRIMARY KEY (`logistics_id`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 6 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of t_logistics
-- ----------------------------
INSERT INTO `t_logistics` VALUES (1, '湖南省长沙市', '广东省揭阳市', '2022-04-07 11:46:52', 1);
INSERT INTO `t_logistics` VALUES (2, '广东省深圳市', '浙江省杭州市', '2022-04-06 11:48:09', 2);
INSERT INTO `t_logistics` VALUES (3, '北京市', '山东省青岛市', '2022-04-06 11:48:50', 3);
INSERT INTO `t_logistics` VALUES (4, '上海市', '广东省揭阳市', '2022-04-07 11:49:24', 4);

CREATE TABLE `t_join_sink`  (
  `order_id` int NOT NULL,
  `order_name` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_id` int NULL DEFAULT NULL,
  `logistics_target` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_source` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL,
  `logistics_time` datetime(0) NULL DEFAULT NULL,
  PRIMARY KEY (`order_id`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic; {code}
Then start the flink  job, in the end, run delete sql
{code:java}
//代码占位符
delete from t_order where id = 1;{code}
the table `t_join_sink` not delete;;;","31/May/22 07:00;lincoln.86xy;Hi [~lvycc], thanks for reporting this!  

For now, a fast solution is remove the temporal function 'now()' before join operation.

It's a long-standing issue in streaming, and we're planning to solve it in FLINK-27849.;;;","31/May/22 07:08;lvycc;OK, Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
failed to join with table function,FLINK-27638,13445149,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,SpongebobZ,SpongebobZ,16/May/22 11:09,23/May/22 10:22,04/Jun/24 20:51,,1.14.3,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"# register two table function named `FUNC_A` and `FUNC_B`
 # left join with FUNC_A
 # inner join with FUNC_B
 # schedule the dml

after these steps, I found the task of FUNC_A keeped running but the task of FUNC_B turned to be finished in serveral seconds. And I am not sure that the unnormal task lead to empty output of the dml.

!image-2022-05-16-19-40-23-179.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/22 11:40;SpongebobZ;image-2022-05-16-19-40-23-179.png;https://issues.apache.org/jira/secure/attachment/13043698/image-2022-05-16-19-40-23-179.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 10:22:04 UTC 2022,,,,,,,,,,"0|z12ea8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 03:23;fsk119;The source in the graph is  Values source? If so, the source outputs all data(send 13B), the task is finished. I think it's not a bug.;;;","23/May/22 10:22;SpongebobZ;[~fsk119] I had not set any finish condition in the table function. By the way it run normally when I don't join with any other table function in the graph.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the log information when the asynchronous part of checkpoint is canceled,FLINK-27637,13445126,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Wencong Liu,wanglijie,wanglijie,16/May/22 08:41,17/Aug/23 22:35,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,0,pull-request-available,stale-assigned,,,"When the checkpoint is aborted due to expiration, the tasks whose asynchronous part of checkpoint is not completed will print following logs:
{code:java}
60477 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - DeclineSink (1/1)#0 - asynchronous part of checkpoint 2 could not be completed.
java.util.concurrent.CancellationException: null
    at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:1.8.0_241]
    at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_241]
    at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:645) ~[classes/:?]
    at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[classes/:?]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [classes/:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_241]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_241]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_241] {code}
 

Maybe we can optimize the logs to make it more friendly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 22:35:07 UTC 2023,,,,,,,,,,"0|z12e54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add data type coverage and sync / async tests for catalog in connector testing framework,FLINK-27636,13445123,13445117,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,16/May/22 08:32,16/May/22 08:32,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 08:32:49.0,,,,,,,,,,"0|z12e4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add data type coverage and abilities test cases for table connectors in testing framework,FLINK-27635,13445122,13445117,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,16/May/22 08:32,16/May/22 08:32,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 08:32:00.0,,,,,,,,,,"0|z12e48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add component failure cases in connector testing framework,FLINK-27634,13445121,13445117,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,16/May/22 08:31,16/May/22 08:31,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 08:31:02.0,,,,,,,,,,"0|z12e40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add FLIP-33 metric validation case in connector testing framework,FLINK-27633,13445119,13445117,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,16/May/22 08:30,16/May/22 08:30,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 08:30:14.0,,,,,,,,,,"0|z12e3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve connector testing framework to support more cases,FLINK-27632,13445117,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,16/May/22 08:21,11/Mar/24 12:44,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,1.20.0,,,,Tests,,,,,0,,,,,"In order to make connector testing framework available for more connectors, including Table /SQL connectors, more test cases are required to cover more scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 08:21:24.0,,,,,,,,,,"0|z12e34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datastream job combined with table job,FLINK-27631,13445096,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,16/May/22 06:58,17/May/22 01:06,04/Jun/24 20:51,,1.13.6,,,,,,,,,,,,,,,,,,API / DataStream,Table SQL / API,,,,0,,,,,"Datastream job combined with table job

One datastream, write two sink, one uses datastream API: datastream addSink(..); After another SQL: insert is used, it is converted to another table_ table from table1； Perform the task like this without using streamexecutionenvironment Execute() executes the addsink operator task, but cannot execute the SQL task; Do not use streamtableenvironment Executesql(), but the addsink operator task will not be executed. If two streamexecutionenvironment Execute() and streamtableenvironment If executesql() is written, two tasks will be executed. Is there any way to put two sink in one task?

 

!image-2022-05-16-14-57-09-836.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/22 06:57;waywtdcc;image-2022-05-16-14-57-09-836.png;https://issues.apache.org/jira/secure/attachment/13043678/image-2022-05-16-14-57-09-836.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 06:58:58.0,,,,,,,,,,"0|z12dyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maven-source-plugin for table planner values connector for debug,FLINK-27630,13445095,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,16/May/22 06:58,02/Jun/22 12:08,04/Jun/24 20:51,02/Jun/22 12:08,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,pull-request-available,,,,"add test source jar to reponsitory  when user use this values connector in flink-table-planner just like kafka/pulsar connector. So user can find the values source code.

and we just need upload the **/factories/** because it will be too large to upload all the flink-table-planner test source code.

 
{code:java}
// code placeholder
just like kafka/pulsar
<plugin>
   <groupId>org.apache.maven.plugins</groupId>
   <artifactId>maven-source-plugin</artifactId>
   <executions>
      <execution>
         <id>attach-test-sources</id>
         <goals>
            <goal>test-jar-no-fork</goal>
         </goals>
         <configuration>
            <archive>
               <!-- Globally exclude maven metadata, because it may accidentally bundle files we don't intend to -->
               <addMavenDescriptor>false</addMavenDescriptor>
            </archive>
            <includes>
               <include>**/factories/**</include>
               <include>META-INF/LICENSE</include>
               <include>META-INF/NOTICE</include>
            </includes>
         </configuration>
      </execution>
   </executions>
</plugin> {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 02 12:08:32 UTC 2022,,,,,,,,,,"0|z12dy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 06:59;jackylau;hi [~jark] , what you think?;;;","17/May/22 08:03;jark;[~jackylau], sounds good to me. Please add comments to the newly introduced plugin to explain the reason.;;;","17/May/22 08:08;jackylau;thanks to [~jark] , i modify the description of this issue.;;;","27/May/22 02:26;jackylau;hi [~twalthr] , do you have time to review it? ;;;","02/Jun/22 12:08;jark;Fixed in master: 5493a386ae4f3cfbcbe1c19444130f788392860c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store throws NullPointerException when pushing down NotEqual predicate to a column consisting of nulls,FLINK-27629,13445089,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,16/May/22 06:39,16/May/22 06:58,04/Jun/24 20:51,16/May/22 06:58,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Run the following Flink SQL to reproduce this issue.
{code}
Flink SQL> create table S ( a double ) with ( 'path' = '/tmp/store' );
[INFO] Execute statement succeed.

Flink SQL> insert into S values (cast(null as double)), (cast(null as double));
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: edb2ce383c00b2f635759dee70add73d


Flink SQL> select * from S where a <> 1;
[ERROR] Could not execute SQL statement. Reason:
java.util.concurrent.ExecutionException: java.lang.NullPointerException
{code}

The exception stack is
{code}
Caused by: java.lang.NullPointerException
	at java.lang.Double.compareTo(Double.java:978) ~[?:1.8.0_151]
	at java.lang.Double.compareTo(Double.java:49) ~[?:1.8.0_151]
	at org.apache.flink.table.store.file.predicate.Literal.compareValueTo(Literal.java:60) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at org.apache.flink.table.store.file.predicate.NotEqual.test(NotEqual.java:50) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at org.apache.flink.table.store.file.operation.FileStoreScanImpl.filterManifestEntry(FileStoreScanImpl.java:287) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174) ~[?:1.8.0_151]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:270) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[?:1.8.0_151]
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721) ~[?:1.8.0_151]
	at java.util.stream.AbstractTask.compute(AbstractTask.java:316) ~[?:1.8.0_151]
	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734) ~[?:1.8.0_151]
	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714) ~[?:1.8.0_151]
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) ~[?:1.8.0_151]
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_151]
	at org.apache.flink.table.store.file.operation.FileStoreScanImpl.lambda$plan$3(FileStoreScanImpl.java:221) ~[flink-table-store-dist-0.2-SNAPSHOT.jar:0.2-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_151]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157) ~[?:1.8.0_151]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 06:58:39 UTC 2022,,,,,,,,,,"0|z12dww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 06:58;lzljs3620320;master: c88da6c39877f30bb4a7b4dc1ca5710f93a188eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store records and fetches incorrect results with NaN,FLINK-27628,13445071,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,TsReaper,TsReaper,16/May/22 04:48,29/Mar/23 01:52,04/Jun/24 20:51,29/Mar/23 01:52,table-store-0.2.0,,,,,,,,,,,,,,,,,,Table Store,,,,,0,,,,,"Use the following test data and SQL to reproduce this issue.

gao.csv:
{code}
1.0,2.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
1.0,1.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
1.0,0.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
-1.0,0.0,aaaaaaaaaaaaaaa
1.0,-1.0,aaaaaaaaaaaaaaa
1.0,-2.0,aaaaaaaaaaaaaaa
{code}

Flink SQL:
{code}
Flink SQL> create table T ( a double, b double, c string ) WITH ( 'connector' = 'filesystem', 'path' = '/tmp/gao.csv', 'format' = 'csv' );
[INFO] Execute statement succeed.

Flink SQL> create table S ( a string, b double ) WITH ( 'path' = '/tmp/store' );
[INFO] Execute statement succeed.

Flink SQL> insert into S select c, a / b from T;
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 851d7b3c233061733bdabbf30f20d16f


Flink SQL> select c, a / b from T;
+-----------------+-----------+
|               c |    EXPR$1 |
+-----------------+-----------+
| aaaaaaaaaaaaaaa |       0.5 |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |       1.0 |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |  Infinity |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa |      -1.0 |
| aaaaaaaaaaaaaaa |      -0.5 |
+-----------------+-----------+
9 rows in set

Flink SQL> select * from S;
+-----------------+-----------+
|               a |         b |
+-----------------+-----------+
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa |      -1.0 |
| aaaaaaaaaaaaaaa |      -0.5 |
+-----------------+-----------+
9 rows in set
{code}

Note that this issue may also affect {{FieldStatsCollector}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27627,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 04:48:59.0,,,,,,,,,,"0|z12dsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Incorrect result when order by (string, double) pair with NaN values",FLINK-27627,13445065,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,TsReaper,TsReaper,16/May/22 03:43,16/May/22 04:50,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,0,,,,,"Use these test data and SQL to reproduce this exception.

gao.csv:
{code}
1.0,2.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
1.0,1.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
1.0,0.0,aaaaaaaaaaaaaaa
0.0,0.0,aaaaaaaaaaaaaaa
-1.0,0.0,aaaaaaaaaaaaaaa
1.0,-1.0,aaaaaaaaaaaaaaa
1.0,-2.0,aaaaaaaaaaaaaaa
{code}

Flink SQL:
{code}
Flink SQL> create table T ( a double, b double, c string ) WITH ( 'connector' = 'filesystem', 'path' = '/tmp/gao.csv', 'format' = 'csv' );
[INFO] Execute statement succeed.

Flink SQL> create table S ( a string, b double ) WITH ( 'connector' = 'filesystem', 'path' = '/tmp/gao2.csv', 'format' = 'csv' );
[INFO] Execute statement succeed.

Flink SQL> insert into S select c, a / b from T;
[INFO] Submitting SQL update statement to the cluster...
[INFO] SQL update statement has been successfully submitted to the cluster:
Job ID: 8c98f5bb99c2dcd28f13def916e2178a


Flink SQL> select * from S order by a, b;
+-----------------+-----------+
|               a |         b |
+-----------------+-----------+
| aaaaaaaaaaaaaaa |       0.5 |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |       1.0 |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |  Infinity |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa |      -1.0 |
| aaaaaaaaaaaaaaa |      -0.5 |
+-----------------+-----------+
9 rows in set

Flink SQL> select * from S order by b;
+-----------------+-----------+
|               a |         b |
+-----------------+-----------+
| aaaaaaaaaaaaaaa | -Infinity |
| aaaaaaaaaaaaaaa |      -1.0 |
| aaaaaaaaaaaaaaa |      -0.5 |
| aaaaaaaaaaaaaaa |       0.5 |
| aaaaaaaaaaaaaaa |       1.0 |
| aaaaaaaaaaaaaaa |  Infinity |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |       NaN |
| aaaaaaaaaaaaaaa |       NaN |
+-----------------+-----------+
9 rows in set
{code}

As is shown above, when order by a (string, double) pair the result is incorrect, while order by a double column separately yields the correct result.

This is because {{BinaryIndexedSortable}} uses two comparators, the normalized key comparator which directly compares memory segments, and the record comparator which compares actual column values. If the length of sort keys are not determined (for example if the sort keys contain strings) the normalized key comparator cannot fully determine the order and it will fall back to the record comparator.

As we can see in {{GenerateUtils#generateCompare}}, record comparator compares double values directly with {{<}} and {{>}}. However for {{Double.NaN}}, every binary comparator except {{!=}} will return false, which causes this issue.

Note that we cannot simply change {{GenerateUtils#generateCompare}}. This is because comparing {{NaN}} in SQL should also return false except for {{<>}}. It is the sorting operator that requires a specific order. That is to say, the current implementation of {{GenerateUtils#generateCompare}} is correct for comparing, but not for sorting. Maybe we should generate a special comparator for all sorting operators?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27628,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 03:43:40.0,,,,,,,,,,"0|z12drk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce pre-aggregated merge to table store,FLINK-27626,13445064,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hannankan,lzljs3620320,lzljs3620320,16/May/22 03:40,19/Sep/22 09:20,04/Jun/24 20:51,19/Sep/22 09:18,,,,,,,,,,,,,,,table-store-0.3.0,,,,Table Store,,,,,0,pull-request-available,stale-major,,,"We can introduce richer merge strategies, one of which is already introduced is PartialUpdateMergeFunction, which completes non-NULL fields when merging. We can introduce more powerful merge strategies, such as support for pre-aggregated merges.

Usage 1:
CREATE TABLE T (
    pk STRING PRIMARY KEY NOT ENFOCED,
    sum_field1 BIGINT,
    sum_field1 BIGINT
) WITH (
     'merge-engine' = 'aggregation',
     'sum_field1.aggregate-function' = 'sum',
     'sum_field2.aggregate-function' = 'sum'
);

INSERT INTO T VALUES ('pk1', 1, 1);
INSERT INTO T VALUES ('pk1', 1, 1);
SELECT * FROM T;
=> output 'pk1', 2, 2

Usage 2:
CREATE MATERIALIZED VIEW T
with (
    'merge-engine' = 'aggregation'
) AS SELECT
    pk,
    SUM(field1) AS sum_field1,
    SUM(field2) AS sum_field1
FROM source_t
GROUP BY pk ;

This will start a stream job to synchronize data, consume source data, and write incrementally to T. This data synchronization job has no state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29189,FLINK-29331,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 19 09:18:25 UTC 2022,,,,,,,,,,"0|z12drc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","19/Sep/22 09:18;lzljs3620320;master: 69367649cba9af2d2628287d84ea48e254d315b0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add query hint for async lookup join,FLINK-27625,13445062,13445058,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/May/22 03:31,10/Aug/22 06:04,04/Jun/24 20:51,10/Aug/22 06:04,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,,,,,"The hint name discuss thread: https://lists.apache.org/thread/jm9kg33wk9z2bvo2b0g5bp3n5kfj6qv8

FLINK-27623 adds a global parameter 'table.exec.async-lookup.output-mode' for table users so that all three control parameters related to async I/O can be configured at the same job level.
As planned in the issue, we‘d like to go a step further to offer more precise control for async join operation more than job level config, to introduce a new join hint: ‘ASYNC_LOOKUP’.

For the hint option, for intuitive and user-friendly reasons, we want to support both simple and kv forms, with all options except table name being optional (use job level configuration if not set)

1. simple form: (ordered hint option list)
```
ASYNC_LOOKUP('tableName'[, 'output-mode', 'buffer-capacity', 'timeout'])
optional:
output-mode
buffer-capacity
timeout
```

Note: since Calcite currently does not support the mixed type hint options,
the table name here needs to be a string instead of an identifier. (For
`SqlHint`: The option format can not be mixed in, they should either be all
simple identifiers or all literals or all key value pairs.) We can improve
this after Calcite support.

2. kv form: (support unordered hint option list)
```
ASYNC_LOOKUP('table'='tableName'[, 'output-mode'='ordered|allow-unordered',
'capacity'='int', 'timeout'='duration'])

optional kvs:
'output-mode'='ordered|allow-unordered'
'capacity'='int'
'timeout'='duration'
```

e.g., if the job level configuration is:
```
table.exec.async-lookup.output-mode: ORDERED
table.exec.async-lookup.buffer-capacity: 100
table.exec.async-lookup.timeout: 180s
```

then the following hints:
```
1. ASYNC_LOOKUP('dim1', 'allow-unordered', '200', '300s')
2. ASYNC_LOOKUP('dim1', 'allow-unordered', '200')
3. ASYNC_LOOKUP('table'='dim1', 'output-mode'='allow-unordered')
4. ASYNC_LOOKUP('table'='dim1', 'timeout'='300s')
5. ASYNC_LOOKUP('table'='dim1', 'capacity'='300')
```

are equivalent to:
```
1. ASYNC_LOOKUP('dim1', 'allow-unordered', '200', '300s')
2. ASYNC_LOOKUP('dim1', 'allow-unordered', '200', '180s')
3. ASYNC_LOOKUP('table'='dim1', 'output-mode'='allow-unordered',
'capacity'='100', 'timeout'='180s')
4. ASYNC_LOOKUP('table'='dim1', 'output-mode'='ordered', 'capacity'='100',
'timeout'='300s')
5. ASYNC_LOOKUP('table'='dim1', 'output-mode'='ordered', 'capacity'='300',
'timeout'='180s')
```

In addition, if the lookup source implements both sync and async table
function, the planner prefers to choose the async function when the
'ASYNC_LOOKUP' hint is specified.


",,,,,,,,,,,,,,,,,,,FLINK-28848,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 06:03:48 UTC 2022,,,,,,,,,,"0|z12dqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 06:03;lincoln.86xy;this was fixed by FLINK-28848;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add plan validation when 'table.exec.async-lookup.output-mode' is unordered,FLINK-27624,13445061,13445058,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/May/22 03:28,18/May/22 07:09,04/Jun/24 20:51,18/May/22 07:09,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / Planner,,,,,0,,,,,Add plan validation when 'table.exec.async-lookup.output-mode' is unordered.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 07:09:09 UTC 2022,,,,,,,,,,"0|z12dqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 07:09;lincoln.86xy;This can be covered by FLINK-27623, so cancel the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add 'table.exec.async-lookup.output-mode' to ExecutionConfigOptions,FLINK-27623,13445060,13445058,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/May/22 03:27,15/Jul/22 08:32,04/Jun/24 20:51,15/Jul/22 08:32,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,stale-assigned,,,"Add 'table.exec.async-lookup.output-mode' to ExecutionConfigOptions so that users can configure the async lookup join's result order if needed.

ORDERED by default (behavior the same as now)
ALLOW_UNORDERED if users allow unordered result and this does not break correctness(if lookup join's input is insert only) then will turn to AsyncDataStream.OutputMode.UNORDERED mode, usually gain higher throughput.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 15 08:32:57 UTC 2022,,,,,,,,,,"0|z12dqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/22 10:39;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","15/Jul/22 08:32;godfreyhe;Fixed in master: c27fd8dc72ceac7631b5f7482db1e9a14b339f68;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make `AsyncDataStream.OutputMode` configurable for table module,FLINK-27622,13445058,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/May/22 03:25,18/Sep/22 13:21,04/Jun/24 20:51,11/Aug/22 04:27,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,,,,,"The `AsyncDataStream.OutputMode` is hardcoded to 'AsyncDataStream.OutputMode.ORDERED' for now:
{code}
//   org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecLookupJoin

        // force ORDERED output mode currently, optimize it to UNORDERED
        // when the downstream do not need orderness
        return new AsyncWaitOperatorFactory<>(
                asyncFunc, asyncTimeout, asyncBufferCapacity, AsyncDataStream.OutputMode.ORDERED);
{code}

It should be configurable to users same as the other two async options  'table.exec.async-lookup.buffer-capacity' & 'table.exec.async-lookup.timeout'.

Also, there must be some plan validation for correctness concern when output mode is unordered(that's the reason I know why not be exposed before).

Further, we should offer more precisely control for async join operation more than job level config, e.g., an async lookup join hint can do this per-join operation.

It's the time to get this work!
",,,,,,,,,,,,,,,,,,,,FLINK-16332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,,Tue Aug 09 12:41:25 UTC 2022,,,,,,,,,,"0|z12dq0:",9223372036854775807,"It is recommend to set the new option 'table.exec.async-lookup.output-mode' to 'ALLOW_UNORDERED' when no stritctly output order is needed, this will yield significant performance gains on append-only streams",,,,,,,,,,,,,,,,,,,"09/Aug/22 12:41;rmetzger;Thanks for working on this! This is an important performance improvement!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
expose SafetyNetWrapperClassLoader from private to public and add addUrl method for flink client,FLINK-27621,13445054,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,16/May/22 02:18,17/May/22 04:29,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Runtime / Task,,,,,0,,,,,expose SafetyNetWrapperClassLoader from private to public and add addUrl method for flink client when user need dynamic add jars,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 04:29:10 UTC 2022,,,,,,,,,,"0|z12dp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 02:20;jackylau;hi [~arvid] , what you think?;;;","16/May/22 03:40;fsk119;I am +1 for this feature. Let me share some thoughs about this.

In many cases, we need add jar or remove jar for the user classloader in the interactive gateway.  For example,  user can

```
Flink SQL> ADD JAR 'path'; -- path to 

Flink SQL> SELECT udf(field) from mytable; -- SUBMIT the job 

Flink SQL >  ADD JAR 'path_to_connector'

Flink SQL > INSERT INTO sink_in_added_jar SELECT  udf(field) from mytable;
```
But currently we just create a new classloader when the user add jar or remove jar in the SQL Client side. However, create a new classloader doesn't mean the classes that are loaded by the old classloader will be collected.  Actually unloading the classes is not an easy work. It requires no one refers to the classes that loaded by the old classloader.  However, it relies on the usage and dependencies. For example, one class(e.g. Hadoop Configuration) is loaded by the app classloader and it caches the class(e.g. connector class) loaded by the child classloader. If the cache is not cleared, the old classloader can not be collected.

With this feature, we are able to reuse the classloader per session and reduce the possiblities of the OOM.

;;;","17/May/22 04:29;lsy;Hi, [~jackylau] thanks for your propose, we will support such mutable classloader in [FLINK-14055|https://issues.apache.org/jira/browse/FLINK-14055], this ticket is duplicated, we will close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support percent_rank,FLINK-27620,13445050,13445044,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,16/May/22 01:43,05/Aug/22 13:43,04/Jun/24 20:51,05/Aug/22 13:43,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 13:43:26 UTC 2022,,,,,,,,,,"0|z12do8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 13:43;jark;Fixed in master: b4df097513be1ec0b82b5a110f93606388747765;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support NTILE,FLINK-27619,13445049,13445044,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,16/May/22 01:42,06/Aug/22 12:47,04/Jun/24 20:51,06/Aug/22 12:47,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 06 12:47:33 UTC 2022,,,,,,,,,,"0|z12do0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 12:47;jark;Fixed in master: 3e1018e0cbc6d28d59f2d0658c6a94b044484f67 and 8ac7e9724a60d9f90ede81b8e1bb13b116623ff3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support CumeDist,FLINK-27618,13445046,13445044,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,16/May/22 01:40,23/Jul/22 04:45,04/Jun/24 20:51,23/Jul/22 04:45,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jul 23 04:45:12 UTC 2022,,,,,,,,,,"0|z12dnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/22 04:45;jark;Fixed in master: 8017a417515cb87b1d15ef1295ecac73126b40f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more analysis functions  for Flink batch,FLINK-27617,13445044,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,16/May/22 01:40,06/Aug/22 12:47,04/Jun/24 20:51,06/Aug/22 12:47,,,,,,,,,,,,,,,1.16.0,,,,Table SQL / API,,,,,0,pull-request-available,,,,"Currently, there're some general analysis functions missed in Flink. We should support it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-16 01:40:19.0,,,,,,,,,,"0|z12dmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the spelling mistakes of the comment word,FLINK-27616,13444977,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,codingcaproni,codingcaproni,14/May/22 11:57,25/Jul/22 01:31,04/Jun/24 20:51,25/Jul/22 01:31,1.14.3,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,pull-request-available,stale-major,,,"org.apache.flink.table.planner.plan.rules.logical.PushFilterIntoSourceScanRuleBase#canPushdownFilter comment spelling error, modified from wether to whether.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 24 22:37:57 UTC 2022,,,,,,,,,,"0|z12d80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document how to define namespaceSelector for k8s operator's webhook for different k8s versions,FLINK-27615,13444969,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bgeng777,bgeng777,bgeng777,14/May/22 09:55,25/May/22 06:52,04/Jun/24 20:51,25/May/22 06:52,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"In our webhook, to support {{{}watchNamespaces{}}}, we rely on the {{kubernetes.io/metadata.name}} to filter the validation requests. However, this label will be automatically added to a namespace only since k8s 1.21.1 due to k8s' [release notes|https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#v1211] .  If users run the flink k8s operator on older k8s versions, they have to add such label by themselevs to support the feature of namespaceSelector in our webhook.

As a result, if we want to support the feature defaultly, we may need to emphasize that users should use k8s >= v1.21.1 to run the flink k8s operator.

 

Due to aitozi's advice, it may be better to add document of how to support the nameSelector based validation instead of limiting the k8s version to be >= 1.21.1 to support more users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 06:52:20 UTC 2022,,,,,,,,,,"0|z12d68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/22 10:10;aitozi;what about supporting user to define their own namespace selector label, this will let more k8s version can be supported. Certainly, we also have to document it.

ps: thanks for your link, I also try to find this feature documentation before :);;;","14/May/22 10:22;bgeng777;[~aitozi] I think your proposal makes sense.

I checked the release notes of k8s and just noticed that the 1.21.1 was just released 1 year ago. It looks like k8s iterates much more quickly than flink. IIUC, users, especially those use k8s in production, will not update k8s to new versions frequently. 
I was doing my tests on k8s 1.20.4. Currently, besides the nameSelector feature, I have not met other issues. To support more k8s versions, I agree with you on makeing this feature non-default with better documents ;;;","15/May/22 08:51;wangyang0918;+1 for the customized namespace selector.;;;","18/May/22 09:40;bgeng777;[~wangyang0918] I'd like to take this ticket to alert the k8s version requirement and add some examples of customized namespace selectors. Can you assign it to me?;;;","25/May/22 06:52;wangyang0918;Fixed via:
main: e04a91d8cef121e4ad078674de935a80f21d4481
release-1.0: 207b17b6f98569d8c5922c44c48dd7160d43d2b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Use informer in webhook to avoid query apiserver at each update,FLINK-27614,13444960,13434929,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,14/May/22 08:34,16/May/22 03:58,04/Jun/24 20:51,16/May/22 03:58,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 03:58:19 UTC 2022,,,,,,,,,,"0|z12d48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 03:58;wangyang0918;Fixed via:

main: 6c9c45250a49b7ebf5484aae820822e8c5e73fbc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add label for the session job to help list the session jobs in the same session cluster,FLINK-27613,13444952,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,14/May/22 08:00,14/Jun/22 12:38,04/Jun/24 20:51,14/Jun/22 12:38,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The custorm resource do not supported list by field selector, So I think add a label to get the session jobs under the same session cluster will be more convenient",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 14 12:38:27 UTC 2022,,,,,,,,,,"0|z12d2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/22 09:03;wangyang0918;This is only for filtering out the session jobs owned by specific session deployment easier. Right?;;;","15/May/22 11:22;aitozi;Yes, especially when we have to do operation on all the jobs of a session cluster. ;;;","14/Jun/22 12:38;gyfora;merged to main bf60428099e8f059c8115550f32a4033efd9d196;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate waring events when deleting the session cluster,FLINK-27612,13444941,13434929,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,14/May/22 06:45,17/May/22 07:08,04/Jun/24 20:51,17/May/22 07:08,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 07:08:52 UTC 2022,,,,,,,,,,"0|z12d00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 07:08;gyfora;merged to main 7ac7e991c51ecd906c86544deca038a128e413b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during Flink-Pulsar checkpoint notification,FLINK-27611,13444936,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,longtimer,longtimer,14/May/22 04:38,16/Sep/22 01:36,04/Jun/24 20:51,16/Sep/22 01:36,1.15.0,,,,,,,,,,,,,,1.14.6,1.15.3,1.16.0,,Connectors / Pulsar,,,,,0,pull-request-available,,,,"When attempting to run a job that was working in 1.12.7, but upgraded to 1.15.0, the following exception is occurring outside of the control of my own code:

 
java.util.ConcurrentModificationException
    at java.base/java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1208)
    at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1244)
    at java.base/java.util.TreeMap$EntryIterator.next(TreeMap.java:1239)
    at org.apache.flink.connector.pulsar.source.reader.source.PulsarUnorderedSourceReader.notifyCheckpointComplete(PulsarUnorderedSourceReader.java:129)
    at org.apache.flink.streaming.api.operators.SourceOperator.notifyCheckpointComplete(SourceOperator.java:511)
    at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:409)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:343)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1384)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$14(StreamTask.java:1325)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$17(StreamTask.java:1364)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.base/java.lang.Thread.run(Thread.java:829)",,,,,,,,,,,,,,,,,,,FLINK-28934,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 16 01:36:35 UTC 2022,,,,,,,,,,"0|z12cyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 11:26;martijnvisser;[~syhily] Can you take a look?;;;","16/May/22 21:42;syhily;I can. This should be a bug on the Pulsar connector. It affects all the available releases.;;;","24/May/22 14:00;longtimer;Does the most recent comment mean that it should go to the Pulsar project or is the issue with code in the Flink project?;;;","24/May/22 14:17;chesnay;It is an issue on the Flink side.;;;","24/May/22 14:20;chesnay;This looks a bit like a basic modify-collection-within-a-loop issue. May be trivial to fix by working against an {{Iterator}}.;;;","23/Jun/22 19:44;longtimer;Is there a possibility of this getting into the 1.15.1 patch release? I do know the process for inclusion but since this affects all available releases, it really blocks usage with Pulsar.;;;","24/Jun/22 06:39;martijnvisser;[~longtimer] The voting on Flink 1.15.1 is already happening, so that won't be possible. This can potentially go in Flink 1.15.2 but that also depends on the overall Pulsar stability (which is currently not in the state where it should be);;;","18/Aug/22 12:36;longtimer;May I ask why this fix was not included in the 1.15.2 release? It really blocks usage of 1.15 for us.;;;","29/Aug/22 08:28;syhily;[~longtimer] Sorry. we are working on the test stable. This would be included in next release.;;;","29/Aug/22 12:21;martijnvisser;[~longtimer] Most simply said because it wasn't completed yet. It's being worked on as [~syhily] mentioned, but releases are not bound to open tickets unless they are considered Blockers (which this ticket isn't). ;;;","30/Aug/22 20:15;longtimer;Understood, unfortunately, it is a blocker for us, just not one for the larger Flink ecosystem.;;;","16/Sep/22 01:36;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,,
PrometheusPushGatewayReporter doesn't support basic authentication,FLINK-27610,13444898,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hong7j,hong7j,13/May/22 19:06,13/May/22 19:06,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,,,,,"PrometheusPushGatewayReporter doesn't support basic authentication. I would like to add two configurations (username,password) to support access to pushGateway which requires authentication",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-13 19:06:54.0,,,,,,,,,,"0|z12cqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tracking flink-version and flink-revision in FlinkDeploymentStatus,FLINK-27609,13444857,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,13/May/22 13:51,24/Nov/22 01:02,04/Jun/24 20:51,17/May/22 07:09,kubernetes-operator-0.1.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"The rest api can provide accurate versioning information through the config endpoint:

[https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#config]

The operator should propagate such fields in the status:
 * flink-version
 * flink-revision

This greatly improves the ability to identify malicious Flink versions (CVE affected, deprecated, etc.) in managed environments. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 07:09:46 UTC 2022,,,,,,,,,,"0|z12chc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 07:09;gyfora;merged to main a432dee1b295c2fecc0dbb9b36b5e703cbbce08e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink may throw PartitionNotFound Exception if the downstream task reached Running state earlier than it's upstream task,FLINK-27608,13444852,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zlzhang0122,zlzhang0122,13/May/22 13:35,18/May/22 16:07,04/Jun/24 20:51,,1.14.2,,,,,,,,,,,,,,,,,,Runtime / Network,,,,,0,,,,,"Flink streaming job deployment may throw PartitionNotFound Exception if the downstream task reached Running state earlier than its upstream task and after maximum backoff for partition requests passed.But the config of taskmanager.network.request-backoff.max is not eay to decide. Can we use a loop awaiting the upstream task partition be ready?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/22 11:43;zlzhang0122;exception.txt;https://issues.apache.org/jira/secure/attachment/13043699/exception.txt",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 16:07:04 UTC 2022,,,,,,,,,,"0|z12cg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/22 18:50;Thesharing;When a PartitionNotFoundException is thrown in the scenario you mentioned above, it will be handled in by the logic located at {{org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler:298}}.

The task will try to {{requestPartitionProducerState}} from the JobManager. If the upstream task is not ready (for example, in the DEPLOYING or INITIALIZING state), the SingleInputGate will try to retrigger another partition request until the partition is consumable.;;;","16/May/22 11:57;zlzhang0122;[~Thesharing] Thanks for your detailed reply! I think the scenario you have mentioned is very useful and is one of the scenarios. The case I have met is another scenario, in that case, the akka message maybe miss or timeout, and I have upload [^exception.txt]  to describe it. Correct me if I'm wrong. Thanks!;;;","16/May/22 18:42;Thesharing;I've read the log you uploaded. In fact, there's only one scenario here. The retry mechanism is always working. In your log, you can see that the exception happens in the method {{retriggerPartitionRequest}} located in the SingleInputGate. The exception is thrown out because there are too many failed retries for the partition request, and the backoff time has reached the maximum value. The partition request shouldn't failed that many times. Thus, you should find out why it takes such a long time to deploy the upstream tasks. Or you could increase the value of the config {{taskmanager.network.request-backoff.max}}, which allows more retries for the partition request.;;;","18/May/22 16:07;zlzhang0122;[~Thesharing] First, thanks for your quickly response and really detailed explanation. And yes, I agree with you, there is only one scenario here because it is a distributed environment. The reason why it takes such a long time to deploy the upstream tasks is the upstream tasks has a large state to restore. And sometimes this may be happen very frequently. So the problem comes back to the beginning that  the config of taskmanager.network.request-backoff.max is not easy to decide and can we have some better solution to deal with it? Thanks again!!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-files,FLINK-27607,13444848,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,13/May/22 13:19,01/Jul/22 10:20,04/Jun/24 20:51,18/May/22 08:54,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 08:54:18 UTC 2022,,,,,,,,,,"0|z12cfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 08:54;chesnay;master: 29c72ce8050c9f6b16b1630a9692d129c500a7e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompileException when using UDAF with merge() method,FLINK-27606,13444841,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,lsy,lsy,13/May/22 12:28,17/Jun/22 13:03,04/Jun/24 20:51,08/Jun/22 09:56,1.15.0,1.16.0,,,,,,,,,,,,,1.15.1,1.16.0,,,Table SQL / Runtime,,,,,0,pull-request-available,,,,"If the SQL contains a UDAF with {{merge()}} method, the job will throw an exception like following in cluster mode. 

Flink generates code to use {{SingleElementIterator}} class to call merge() of UDAF. However, the {{SingleElementIterator}} is in flink-table-planner module and planner scala-free was introduced since release 1.15. Therefore, user classloader can't find {{SingleElementIterator}} class. 

We can move {{SingleElementIterator}} to flink-table-runtime to fix this problem. 

{code}
2022-05-10 17:41:46,621 WARN  org.apache.flink.table.runtime.generated.GeneratedClass      [] - Failed to compile split code, falling back to original code
  org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
      at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction.open(MiniBatchGlobalGroupAggFunction.java:142) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.operators.bundle.AbstractMapBundleOperator.open(AbstractMapBundleOperator.java:86) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568) [flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at java.lang.Thread.run(Thread.java:834) [?:?]
  Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      ... 14 more
  Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
      at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      ... 14 more
  Caused by: org.codehaus.commons.compiler.CompileException: Line 11, Column 28: Cannot determine simple type name ""org""
      at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
      at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215) ~[flink-table-runtime-1.15-vvr-6.0-SNAPSHOT.jar:1.15-vvr-6.0-SNAPSHOT]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27968,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 08 03:03:47 UTC 2022,,,,,,,,,,"0|z12cds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/22 03:03;jark;Fixed in 
 - master: 24cb706c20559e1921d95d8137183bf931f62911
 - 1.15.1: ca05ad9f0911500d90d5e3b8e8e9fb9fed60482e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade mockito to a version where static function mocking is possible,FLINK-27605,13444819,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,13/May/22 10:56,23/May/22 06:44,04/Jun/24 20:51,23/May/22 06:44,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,"There are some external APIs used by Flink which are designed in a way which is super hard to test.
Such a pattern is when a class instance is only available through a static function and no publicly available constructor exists.
A good example to this is ""UserGroupInformation"" class in Hadoop library.

We've listed many different possibilities how to make it testable in the following place: https://github.com/apache/flink/pull/19372#discussion_r849691193
After deep consideration we've agreed to use static function mocking in such exceptional cases in order to make code clean and maintainable.

It should be emphasized that the general direction not changed in terms of Mockito and Powermock usage. It should be avoided as much as possible.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 23 06:43:52 UTC 2022,,,,,,,,,,"0|z12c8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/22 06:43;dmvk;master: 220ef999d2a353fd52cc0aa1a93c26d9b696c1ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink sql read hive on hbase throw NPE,FLINK-27604,13444817,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,18579099920@163.com,18579099920@163.com,13/May/22 10:45,11/Oct/22 02:54,04/Jun/24 20:51,11/Oct/22 02:54,1.13.6,,,,,,,,,,,,,,,,,,Connectors / Hive,,,,,0,,,,,"I have some table data on hbase, I usually read the hbase data by loading external tables through hive, I want to read the data through flink sql by reading hive tables, when I try with sql-client I get an error. I don't know if there is any way to solve this problem, but I can read the data using the spark engine.
----
Environment：
flink:1.13.6
hive:2.1.1-cdh6.2.0
hbase:2.1.0-cdh6.2.0
flinksql Execution tools：flink sql client 
sql submit mode：yarn-per-job
----
flink lib directory
antlr-runtime-3.5.2.jar
flink-csv-1.13.6.jar
flink-dist_2.11-1.13.6.jar
flink-json-1.13.6.jar
flink-shaded-zookeeper-3.4.14.jar
flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar
flink-table_2.11-1.13.6.jar
flink-table-blink_2.11-1.13.6.jar
guava-14.0.1.jar
hadoop-mapreduce-client-core-3.0.0-cdh6.2.0.jar
hbase-client-2.1.0-cdh6.2.0.jar
hbase-common-2.1.0-cdh6.2.0.jar
hbase-protocol-2.1.0-cdh6.2.0.jar
hbase-server-2.1.0-cdh6.2.0.jar
hive-exec-2.1.1-cdh6.2.0.jar
hive-hbase-handler-2.1.1-cdh6.2.0.jar
htrace-core4-4.1.0-incubating.jar
log4j-1.2-api-2.17.1.jar
log4j-api-2.17.1.jar
log4j-core-2.17.1.jar
log4j-slf4j-impl-2.17.1.jar
protobuf-java-2.5.0.jar
----
step:
hive create table stament:
{code:java}
CREATE EXTERNAL TABLE `ods`.`student`(
  `row_key` string, 
  `name` string,
  `age` int,
  `addr` string 
) 
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key,FINAL:NAME,FINAL:AGE,FINAL:ADDR,'serialization.format'='1')
TBLPROPERTIES (
  'hbase.table.name'='ODS:STUDENT'); {code}
catalog：hive catalog 
sql: select * from ods.student;
----
error:
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Could not execute SQL statement.
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:215) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:235) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:479) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:412) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.lambda$executeStatement$0(CliClient.java:327) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at java.util.Optional.ifPresent(Optional.java:159) ~[?:1.8.0_191]
    at org.apache.flink.table.client.cli.CliClient.executeStatement(CliClient.java:327) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:297) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:221) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client_2.11-1.13.6.jar:1.13.6]
Caused by: org.apache.flink.connectors.hive.FlinkHiveException: Unable to instantiate the hadoop input format
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:100) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:212) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:207) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:123) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:96) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:114) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:70) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:791) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1225) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:90) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    ... 12 more
Caused by: java.lang.NullPointerException
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_191]
    at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_191]
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createMRSplits(HiveSourceFileEnumerator.java:94) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:71) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:212) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:207) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:123) ~[flink-sql-connector-hive-2.2.0_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:96) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:247) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:114) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:70) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:69) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165) ~[flink-table-blink_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1518) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:791) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1225) ~[flink-table_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:90) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:213) ~[flink-sql-client_2.11-1.13.6.jar:1.13.6]
    ... 12 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 11 02:53:10 UTC 2022,,,,,,,,,,"0|z12c8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 02:53;luoyuxia;[~18579099920@163.com] Thanks for reporting it. Currently, it's not supported to hbase data via Hive in Flink.  But I think we may need to support it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hive dialect supports ""reload function""",FLINK-27603,13444810,13510724,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,luoyuxia,luoyuxia,luoyuxia,13/May/22 10:18,14/Mar/24 07:36,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,Connectors / Hive,,,,,0,pull-request-available,stale-assigned,,,"In Hive, by ""reload function"", the user can use the function defined in other session.

But in flink, it will always get the function from the catalog, and if it's hive catalog, it'll load all the user defined functions from the metastore.

So, there's no need to ""reload function"" explicitly for it actually is done implicitly.

But when use Hive dialect in Flink, it'll throw an unsupported exception for  the statement  ""reload function"".

I'm wondering keep the current behavior to throw excpetion or consider it as an 'NopOperation'.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 21 22:37:58 UTC 2022,,,,,,,,,,"0|z12c6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jun/22 04:09;tartarus;Thanks for your work!

I think the reload function is the habit of many users in hive, 

Flink can already achieve the effect of reload function by default, so we can not throw exception to avoid causing trouble to users.;;;","21/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Python ML examples in docs,FLINK-27602,13444806,13429597,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,13/May/22 09:43,20/May/22 08:05,04/Jun/24 20:51,20/May/22 08:05,ml-2.1.0,,,,,,,,,,,,,,ml-2.1.0,,,,API / Python,Library / Machine Learning,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 08:05:39 UTC 2022,,,,,,,,,,"0|z12c60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 08:05;hxbks2ks;Merged into master via 93f8a1d00475363ca92d24b35f92c59d87ae1187;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translation in Korean,FLINK-27601,13444795,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Incomplete,,djkooks,djkooks,13/May/22 09:17,26/Feb/23 12:18,04/Jun/24 20:51,26/Feb/23 12:18,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,"Hello,

I'm a starter in Flink(but will use in production at near future), currently reviewing the documents.

Is there some Korean translation work going on of this project? I want to support if there are(or make from scratch if there aren't).
 
Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-13 09:17:38.0,,,,,,,,,,"0|z12c3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add basic logs for coping and deleting jars,FLINK-27600,13444779,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,haoxin,haoxin,13/May/22 08:01,17/May/22 07:09,04/Jun/24 20:51,17/May/22 07:09,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Because we delete the jar after submitting the job, it's better to leave some logs for the end-users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/211,,,,,,,,,,9223372036854775807,,,,,Tue May 17 07:09:10 UTC 2022,,,,,,,,,,"0|z12c00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 07:09;gyfora;merged to main cdae4c9e6e8d4aafeed53f3617b0ea213e032515;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup flink-connector-rabbitmq repository,FLINK-27599,13444776,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,13/May/22 07:36,30/Sep/22 07:23,04/Jun/24 20:51,25/May/22 12:01,,,,,,,,,,,,,,,rabbitmq-3.0.0,,,,Connectors/ RabbitMQ,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-13 07:36:29.0,,,,,,,,,,"0|z12bzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the exception message when mixing use Python UDF and Pandas UDF,FLINK-27598,13444767,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxb,dianfu,dianfu,13/May/22 06:43,11/Oct/22 02:46,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"For the following job:
{code}

import argparse
from decimal import Decimal

from pyflink.common import Row
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, DataTypes
from pyflink.table.udf import AggregateFunction, udaf


class DeduplicatedSum(AggregateFunction):
def create_accumulator(self):
return \{int(0), float(0)}

def get_value(self, accumulator) -> float:
sum(accumulator.values())

def accumulate(self, accumulator, k: int, v: float):
if k not in accumulator:
accumulator[k] = v

def retract(self, accumulator, k: int, v: float):
if k in accumulator:
del accumulator[k]


deduplicated_sum = udaf(f=DeduplicatedSum(),
func_type=""pandas"",
result_type=DataTypes.DOUBLE(),
input_types=[DataTypes.BIGINT(), DataTypes.DOUBLE()])


class FirstValue(AggregateFunction):
def create_accumulator(self):
return [int(-1), float(0)]

def get_value(self, accumulator) -> float:
return accumulator[1]

def accumulate(self, accumulator, k: int, v: float):
ck = accumulator[0]
if ck > k:
accumulator[0] = k
accumulator[1] = v


first_value = udaf(f=FirstValue(),
result_type=DataTypes.DOUBLE(),
func_type=""pandas"",
input_types=[DataTypes.BIGINT(), DataTypes.DOUBLE()])


class LastValue(AggregateFunction):
def create_accumulator(self):
return [int(-1), float(0)]

def get_value(self, accumulator: Row) -> float:
return accumulator[1]

def accumulate(self, accumulator: Row, k: int, v: float):
ck = accumulator[0]
if ck < k:
accumulator[0] = k
accumulator[1] = v


last_value = udaf(f=LastValue(),
func_type=""pandas"",
result_type=DataTypes.DOUBLE(),
input_types=[DataTypes.BIGINT(), DataTypes.DOUBLE()])


def create_source_table_trades(table_env):
source = f""""""
CREATE TABLE src_trade (
`id` VARCHAR
,`timestamp` BIGINT
,`side` VARCHAR
,`price` DOUBLE
,`size` DOUBLE
,`uniqueId` BIGINT
,ts_micro AS `timestamp`
,ts_milli AS `timestamp` / 1000
,ts AS TO_TIMESTAMP_LTZ(`timestamp` / 1000, 3)
,WATERMARK FOR ts AS ts - INTERVAL '0.001' SECOND
) WITH (
'connector' = 'datagen')
""""""
table_env.execute_sql(source)


def create_sink_table(table_env):
sink = f""""""
CREATE TABLE dst_kline (
wst TIMESTAMP_LTZ(3)
,wet TIMESTAMP_LTZ(3)
,otm BIGINT
,ot TIMESTAMP_LTZ(3)
,ctm BIGINT
,ct TIMESTAMP_LTZ(3)
,ptm BIGINT
,pt TIMESTAMP_LTZ(3)
,`open` DOUBLE
,`close` DOUBLE
,`high` DOUBLE
,`low` DOUBLE
,`vol` DOUBLE -- total trade volume
,`to` DOUBLE -- total turnover value
,`rev` INT -- revision, something we might use for versioning
,`gap` INT -- if this value is reliable
,PRIMARY KEY(wst) NOT ENFORCED
) WITH (
'connector' = 'print'
)
""""""
table_env.execute_sql(sink)


def kafka_src_topic(value):
if not len(value.split('-')) == 5:
raise argparse.ArgumentTypeError(""{} is not a valid kafka topic"".format(value))
return value


def interval(value):
i = []
prev_num = []
for character in value:
if character.isalpha():
if prev_num:
num = Decimal(''.join(prev_num))
if character == 'd':
i.append(f""'\{num}' DAYS"")
elif character == 'h':
i.append(f""'\{num}' HOURS"")
elif character == 'm':
i.append(f""'\{num}' MINUTES"")
elif character == 's':
i.append(f""'\{num}' SECONDS"")
prev_num = []
elif character.isnumeric() or character == '.':
prev_num.append(character)
return "" "".join(i)


def fetch_arguments_flink_kline():
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--bootstrap-servers', type=str, required=True)
parser.add_argument('--src-topic', type=kafka_src_topic)
parser.add_argument('--consume-mode', type=str, default='group-offsets',
choices=['group-offsets', 'latest-offset'],
help='scan.startup.mode for kafka')
parser.add_argument('--interval', type=str, default='20s',
help='output interval e.g. 5d4h3m1s, default to 20s')
parser.add_argument('--force-test', action='store_true')
parser.add_argument('--consumer-group-hint', type=str, default='1')
args = parser.parse_args()
if args.force_test and args.consumer_group_hint == '1':
parser.error(""With --force-test, should not use default '1' for --consumer-group-hint"")
return args


def main():
# args = fetch_arguments_flink_kline()
# parts = args.src_topic.split('-')
# _, e, p, s, _ = parts
# dst_topic = f'\{e}-\{p}-\{s}-Kline\{args.interval}'

env = StreamExecutionEnvironment.get_execution_environment()
table_env = StreamTableEnvironment.create(env)
# table_env.get_config().get_configuration().set_boolean(""table.exec.emit.early-fire.enabled"", True)
# table_env.get_config().get_configuration().set_string(""table.exec.emit.early-fire.delay"", ""0 s"")
table_env.get_config().get_configuration().set_string(""table.exec.emit.allow-lateness"", ""1 h"")
# table_env.get_config().get_configuration().set_boolean(""table.exec.emit.late-fire.enabled"", True)
# table_env.get_config().get_configuration().set_string(""table.exec.emit.late-fire.delay"", ""0 s"")
table_env.create_temporary_function(""deduplicated_sum"", deduplicated_sum)
table_env.create_temporary_function(""first_value"", first_value)
table_env.create_temporary_function(""last_value"", last_value)

create_source_table_trades(table_env)
create_sink_table(table_env)
stmt = f""""""
INSERT INTO dst_kline
SELECT TUMBLE_START(ts, INTERVAL '1' DAY)
,TUMBLE_END(ts, INTERVAL '1' DAY)
,MIN(ts_milli)
,MIN(ts) AS st
,MAX(ts_milli)
,MAX(ts) AS et
,EXTRACT(MILLISECOND FROM CURRENT_TIMESTAMP) + UNIX_TIMESTAMP() * 1000
,CURRENT_TIMESTAMP
,first_value(ts_micro, price)
,last_value(ts_micro, price)
,MAX(price)
,MIN(price)
,deduplicated_sum(uniqueId, `size`)
,deduplicated_sum(uniqueId, price * `size`)
,1
,CAST((MAX(ts_milli) - MIN(ts_milli)) / 1000 AS INT)
FROM src_trade
GROUP BY TUMBLE(ts, INTERVAL '1' DAY)
""""""
table_env.execute_sql(stmt)


if __name__ == '__main__':
    main()
{code}

It throws the following exception:
{code}
Traceback (most recent call last):
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/tests/kl(3).py"", line 207, in <module>
    main()
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/tests/kl(3).py"", line 203, in main
    table_env.execute_sql(stmt)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/table/table_environment.py"", line 876, in execute_sql
    return TableResult(self._j_tenv.executeSql(stmt))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/py4j/protocol.py"", line 328, in get_return_value
    format(target_id, ""."", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o10.executeSql.
: java.lang.NullPointerException
    at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:59)
    at org.apache.flink.table.functions.python.PythonFunctionInfo.<init>(PythonFunctionInfo.java:45)
    at org.apache.flink.table.functions.python.PythonAggregateFunctionInfo.<init>(PythonAggregateFunctionInfo.java:36)
    at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.extractPythonAggregateFunctionInfosFromAggregateCall(CommonPythonUtil.java:236)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.createPandasPythonStreamWindowGroupOneInputTransformation(StreamExecPythonGroupWindowAggregate.java:365)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecPythonGroupWindowAggregate.translateToPlanInternal(StreamExecPythonGroupWindowAggregate.java:264)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
    at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:88)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:250)
    at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:114)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:134)
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:71)
    at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(StreamPlanner.scala:70)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:70)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:185)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1665)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:752)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:872)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:742)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
{code}

The exception message is confusing and should be improved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-13 06:43:33.0,,,,,,,,,,"0|z12bxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support get column statistic for Hive partition table,FLINK-27597,13444740,13444738,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,13/May/22 02:15,29/Aug/22 07:52,04/Jun/24 20:51,09/Aug/22 06:15,,,,,,,,,,,,,,,1.16.0,,,,Connectors / Hive,,,,,0,pull-request-available,,,,"Currently, for Hive paritioned table, we don't return any statistic which isn't friendly to sql optimization when it comes to partitioned table. So we need to  return statistic for hive parition table.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-13 02:15:09.0,,,,,,,,,,"0|z12brc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] Optimize Hive source ,FLINK-27596,13444738,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,13/May/22 02:08,11/Mar/24 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,Connectors / Hive,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-13 02:08:34.0,,,,,,,,,,"0|z12bqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Helm: We should make security context configurable,FLINK-27595,13444662,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,haoxin,haoxin,haoxin,12/May/22 16:48,13/May/22 01:46,04/Jun/24 20:51,13/May/22 01:46,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,We can extract pod security context to Helm's Values to make it configurable.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 13 01:46:00 UTC 2022,,,,,,,,,,"0|z12ba0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/22 01:46;wangyang0918;Fixed via:

main: af086601fd0d209540b90368e9883a87e08af98a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only recover JM deployment if HA metadata available,FLINK-27594,13444616,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,12/May/22 13:05,16/May/22 09:37,04/Jun/24 20:51,16/May/22 09:37,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,,,,,"This ticket is related to https://issues.apache.org/jira/browse/FLINK-27572

The deployment recovery logic for list jobmanager deployments simply performs a restoreFromLasteSavepoint operation currently.

This is incorrect in cases where the HA metadata is not available as it might lead to accidentally restoring from an older state.

We should verify that HA metadata is present and simply perform a deployOperation. Once we have this we can actually make the recovery default true for all versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 09:37:41 UTC 2022,,,,,,,,,,"0|z12azs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 13:06;gyfora;cc [~wangyang0918] ;;;","12/May/22 17:54;thw;[~gyfora] are you saying that when no HA metadata is available and the upgrade mode is LAST_STATE then the operator should keep the deployment in error state? I think that would be correct. When the upgrade mode is SAVEPOINT, then it can go back to that savepoint?

I also think that with LAST_STATE we should pick either last checkpoint or savepoint, whichever is more recent.;;;","13/May/22 08:57;gyfora;Yes, [~thw] what you are saying is part of that. All in all I am working on hardening and at the same type simplifying the logic with these sanity checks around HA data.

The ultimate goal is to avoid situations where you accidentally restore from empty state or a stale savepoint bacause the HA data disappeared for some reason.;;;","16/May/22 09:37;gyfora;merged to main a0aca64bff2c9dc355ed2f9a907f861ffaf556df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet/Orc format reader optimization,FLINK-27593,13444611,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,12/May/22 11:58,11/Mar/24 12:43,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Runtime,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17778,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 28 22:59:16 UTC 2022,,,,,,,,,,"0|z12ayo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/22 03:45;lzljs3620320;There are already quite a few related jiras out there, you can link to them.;;;","13/May/22 04:36;lsy;[~lzljs3620320] Ok，that's good for me.;;;","28/May/22 22:59;jingge;[~lsy] would you please describe what the issues are and what should be exactly done for this task?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WebUI - Status CANCELING not showing in overview,FLINK-27592,13444599,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,peter.schrott,peter.schrott,12/May/22 11:03,16/Mar/23 08:50,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,0,,,,,"Cluster Version: 1.15.0, Standalone
Job Version: 1.15.0

Status ""CANCELING"" is not showing / visible in WebUI jobs overview. Compare attached screenshot.

Status of the Job was actually ""CANCELING"" when checking with Flink CLI list command.

!image-2022-05-12-13-01-25-266.png|width=919,height=164!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/22 11:01;peter.schrott;image-2022-05-12-13-01-25-266.png;https://issues.apache.org/jira/secure/attachment/13043560/image-2022-05-12-13-01-25-266.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 16 08:50:55 UTC 2023,,,,,,,,,,"0|z12aw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 08:50;tanyuxin;[~peter.schrott], I can not reproduce this in my local env, could you reproduce the issue and give more details, such as logs, etc? And maybe you could try it with the new 1.15.4 or the newer version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the plan for batch queries when statistics is unavailable ,FLINK-27591,13444592,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,godfreyhe,godfreyhe,12/May/22 10:33,11/Mar/24 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,,,,,"This jira is an umbrella issue, which aims to improve the plan for batch queries when statistics is unavailable.
Currently, when statistics is unavailable, the planner will give default cost, which may lead to the planner choosing bad plan, such as: wrong broadcast join plan will cause a lot of network shuffle and OOM.

We can detect whether the source tables have statistics. if not, join order, hash join can be disabled.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 10 03:36:29 UTC 2022,,,,,,,,,,"0|z12aug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/22 03:36;337361684@qq.com;Hi, [~godfreyhe] , I would like to get this ticket. Could you assign this to me, Thanks a lot!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide endpoint with information about leader Jobmanager in HA setups via Zookeeper,FLINK-27590,13444589,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dhrapate,dhrapate,12/May/22 10:29,12/May/22 10:29,04/Jun/24 20:51,,1.14.0,1.15.0,,,,,,,,,,,,,,,,,Runtime / REST,,,,,0,,,,,"In High Availability setups via Zookeeper, it would be nice to provide information about the leader jobmanager. Although, the job submission and all the operations can be performed via any jobmanager. It would be nice to expose data about leader jobmanager due to following reasons
 * Since there are multiple job managers in HA setup, each of them will expose a web ui and a rest endpoint. It is difficult to know which of the endpoint is backed by active jobmanager. Moreover, we cannot access the taskmanager logs from the flink ui exposed by the standby jobmanager. [^error.log]
 * If the flink UI/REST endpoint is fronted by a Load Balancer, it would be nice to detect active jobmanager so that all the UI requests can be routed to the active jobmanager to avoid having issues in accessing logs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/22 10:26;dhrapate;error.log;https://issues.apache.org/jira/secure/attachment/13043558/error.log",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-12 10:29:27.0,,,,,,,,,,"0|z12ats:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect implementation of TableStoreDateObjectInspector,FLINK-27589,13444588,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,12/May/22 10:21,17/May/22 07:42,04/Jun/24 20:51,12/May/22 11:07,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Flink store date as an integer internally, where this integer represents epoch day. However {{TableStoreDateObjectInspector}} currently see this integer as epoch millisecond.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 11:07:36 UTC 2022,,,,,,,,,,"0|z12atk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 11:07;lzljs3620320;master:

67b78417b6db108712e0d85b871ad380cc588c04

bb2879bbbc7be3a3451af33f7bdd03bba0c1a025;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update broadcast state related docs for PyFlink,FLINK-27588,13444575,13444563,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,12/May/22 09:58,11/Aug/22 11:24,04/Jun/24 20:51,11/Aug/22 11:24,,,,,,,,,,,,,,,1.16.0,,,,API / Python,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 11:24:56 UTC 2022,,,,,,,,,,"0|z12aqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 02:21;ana4;[~dianfu] I would like to take this.;;;","16/May/22 09:41;dianfu;[~ana4] Thanks. I have assigned it to you~ Note that this feature is still in-progress and the documentation needs to be added after the feature is finished.;;;","22/Jul/22 01:32;dianfu;Hi [~ana4], the related functionalities about broadcast state have already been finished and so you could add the documentation when it's convenient for you! Thanks in advance!;;;","08/Aug/22 08:40;ana4;[~dianfu] [~hxbks2ks] Sorry I'm late. Since the code is frozen immediately, I have a fear that I can't finish this in time. Could you address this PR? [~Juntao Hu] ;;;","11/Aug/22 11:24;dianfu;Merged to master via 4739a5c88ccc27dca3a19ad9834ca39d4ebb9acf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support keyed co-broadcast processing in PyFlink,FLINK-27587,13444574,13444563,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Juntao Hu,Juntao Hu,Juntao Hu,12/May/22 09:57,22/Jul/22 01:30,04/Jun/24 20:51,18/Jul/22 02:36,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,Support KeyedStream.connect(BroadcastStream).process(<KeyedBroadcastProcessFunction>).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 02:36:07 UTC 2022,,,,,,,,,,"0|z12aqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 09:06;Zsigner;Hi [~Juntao Hu] 
This issue seems to be submitted repeatedly, you see FLINK-27586 https://issues.apache.org/jira/browse/FLINK-27586
 
 ;;;","03/Jul/22 10:20;Juntao Hu;[~Zsigner] Oh I separate broadcast process implementation into two issues for non-keyed and keyed, under [FLINK-27584]

 ;;;","18/Jul/22 02:36;hxbks2ks;Merged into master via 7a9016cca05aeb55cf6d66a163d96fbd75e42963;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support non-keyed co-broadcast processing in PyFlink,FLINK-27586,13444572,13444563,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Juntao Hu,Juntao Hu,Juntao Hu,12/May/22 09:55,08/Jul/22 13:42,04/Jun/24 20:51,30/Jun/22 02:24,1.16.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,Support DataStream.connect(BroadcastStream).process(<BroadcastProcessFunction>).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28388,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 30 02:24:05 UTC 2022,,,,,,,,,,"0|z12aq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jun/22 02:24;hxbks2ks;Merged into master via dfdf7afb047d1b8af581883bf208e4d8a30a116f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcOutpuFormat fails to flush data on checkpoint and causes data loss,FLINK-27585,13444571,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,Paul Lin,Paul Lin,12/May/22 09:48,01/Jul/22 10:59,04/Jun/24 20:51,,1.12.3,,,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,0,,,,,"Currently, JdbcOutpuFormat may periodically flush the batches, and store the exception if there's one. The exception would not be thrown out to flink runtime immediately, instead, it would be checked when a new record in written or flush() is called on checkpoint snapshot.

However, in a job I see, when there's an exception already, the flush() called by checkpointed function would not recognize the exception, and the checkpoint would still succeed regardless of the flush exception, which makes the failed batch silently lost.

I'm upgrading it to 1.14 to see if it happens still.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 01 10:59:23 UTC 2022,,,,,,,,,,"0|z12aps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jul/22 10:59;2011aad;It seems snapshotState() will throws the exception stored before and thus will not checkpoint successfully.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support broadcast state in PyFlink DataStream API,FLINK-27584,13444563,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Juntao Hu,dianfu,dianfu,12/May/22 09:23,11/Aug/22 11:25,04/Jun/24 20:51,11/Aug/22 11:25,,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,This is an umbrella JIRA to support broadcast state in PyFlink DataStream API to align with the Java API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-12 09:23:06.0,,,,,,,,,,"0|z12ao0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the plan for TPC-DS queries,FLINK-27583,13444558,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,godfreyhe,godfreyhe,12/May/22 08:58,11/Mar/24 12:44,04/Jun/24 20:51,,,,,,,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,0,,,,,"This jira is an umbrella issue, which aims to fix and improve the plan for TPC-DS
including: fix join order bad case, improve multiple input, fix some cost model bad case, etc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-12 08:58:53.0,,,,,,,,,,"0|z12amw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 modify the number of parameters,FLINK-27582,13444557,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,zeliu,zeliu,12/May/22 08:54,12/May/22 09:21,04/Jun/24 20:51,12/May/22 09:20,1.15.0,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,,,,,"The number of arguments is 4, not 5


parameters:
--input-topic <topic> 
--output-topic <topic>
--bootstrap.servers <kafka brokers>
--group.id <some id>",,,,,,,,,,,,,,,,,,,,,,,,FLINK-27581,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 09:20:23 UTC 2022,,,,,,,,,,"0|z12amo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 08:56;zeliu;[https://github.com/apache/flink/pull/19706];;;","12/May/22 09:20;martijnvisser;Please see the review comment in the PR. For future reference, please also follow the code contribution guide https://flink.apache.org/contributing/contribute-code.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 modify the number of parameters,FLINK-27581,13444556,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,zeliu,zeliu,12/May/22 08:53,12/May/22 09:21,04/Jun/24 20:51,12/May/22 09:21,1.15.0,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,,,,,"The number of arguments is 4, not 5


parameters:
--input-topic <topic> 
--output-topic <topic>
--bootstrap.servers <kafka brokers>
--group.id <some id>",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27582,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-12 08:53:53.0,,,,,,,,,,"0|z12amg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement filter pushdown for TableStoreHiveStorageHandler,FLINK-27580,13444550,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,12/May/22 08:36,25/Aug/22 02:31,04/Jun/24 20:51,18/May/22 08:37,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,Filter pushdown is a critical optimization for sources as it can decrease number of records to read. Hive provides a {{HiveStoragePredicateHandler}} interface for this purpose. We need to implement this interface in {{TableStoreHiveStorageHandler}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 08:37:16 UTC 2022,,,,,,,,,,"0|z12al4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 08:37;lzljs3620320;master: 660e72b147dc5a4d4ea945b847f4bec589e14cbf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The param client.timeout can not be set by dynamic properties when stopping the job ,FLINK-27579,13444545,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paul8263,Jiangang,Jiangang,12/May/22 08:23,27/Oct/22 09:36,04/Jun/24 20:51,26/Jul/22 09:18,1.16.0,,,,,,,,,,,,,,1.15.3,1.16.0,,,Client / Job Submission,,,,,0,pull-request-available,,,,"The default client.timeout value is one minute which may be too short when stop-with-savepoint for big state jobs.

When we stop the job by dynamic properties(-D or -yD for yarn), the client.timeout is not effective.

From the code, we can see that the dynamic properties are only effect for run command. We should support it for stop command.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29749,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 02:36:16 UTC 2022,,,,,,,,,,"0|z12ak0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/22 06:30;paul8263;Hi [~Jiangang] ,

The option `client.timeout` is read only from flink configuration file. The dynamic properties take no effect.

I would like to solve this problem.;;;","25/May/22 01:59;Jiangang;[~paul8263] Thanks for the pr. We need a flink committer to verify the problem. [~wangyang0918] , could you please have a look. Thanks.;;;","25/May/22 03:52;wangyang0918;The root cause is we do not get the {{clientTimeout}} from effective configuration when doing stop/list/etc. operations. Right?;;;","25/May/22 09:16;Jiangang;[~wangyang0918] Yes, you are right. The method getEffectiveConfiguration is not called for some actions, like stop and cancel. Although most of configs take effect when the job starts, there exist some configs in other actions. The config clientTimeout is just one.

In this case, I think that the simplest way to fix is to fetch clientTimeout from the effectiveConfiguration. If we consider more, all the actions should get configs from effectiveConfiguration instead of the initial configuration in the constructor. 

What do you think?

 ;;;","25/May/22 13:07;wangyang0918;Yes. I lean to fix this issue by get the clientTimeout from effective configuration, not the original conf.;;;","26/May/22 07:36;Jiangang;[~paul8263] Would you like to solve it? If so, you can refer to the discussion. If not, I would like to solve it. Thanks.;;;","27/May/22 00:28;paul8263;Hi [~Jiangang] ,

I would like to do this. Please have a look at PR  [#19722|https://github.com/apache/flink/pull/19772].Thanks.;;;","01/Jun/22 13:22;Jiangang;[~wangyang0918] Can you review the code? Thanks.;;;","05/Jul/22 06:12;paul8263;Hi [~wangyang0918] ，

Could you help review the code please? Thanks.;;;","06/Jul/22 09:25;wangyang0918;Really sorry for the late response. I will get to this PR in this week.;;;","12/Jul/22 01:05;paul8263;Hi [~wangyang0918] ,

Thank you very much for your suggestions and code review.

If there is something need to update in this PR, please let me know.;;;","26/Jul/22 09:18;wangyang0918;Fixed via:

master: c4b107835c8bdae0667efcbdfe52aa3a34aec894;;;","26/Jul/22 09:19;wangyang0918;Thanks [~paul8263] and [~Jiangang] for your contribution.;;;","27/Oct/22 02:36;wangyang0918;When working for FLINK-29749, I realize that this ticket also needs to be backported to release-1.15. ;;;",,,,,,,,,,,,,,,,,,
"Elasticsearch6SinkE2ECase.testScaleUp fails after ""Exhausted retry attempts""",FLINK-27578,13444540,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,alexanderpreuss,martijnvisser,martijnvisser,12/May/22 07:56,13/May/22 10:56,04/Jun/24 20:51,13/May/22 10:56,,,,,,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,0,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35588&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=117547


{code:java}
--------------------------------------------------------------------------------
Test org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.testScaleUp failed with:
org.apache.flink.util.FlinkException: Exhausted retry attempts.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:173)
	at org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.checkResultWithSemantic(ElasticsearchSinkE2ECaseBase.java:76)
	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.restartFromSavepoint(SinkTestSuiteBase.java:326)
	at org.apache.flink.connector.testframe.testsuites.SinkTestSuiteBase.testScaleUp(SinkTestSuiteBase.java:201)
{code}

These test runs are also flooding the logs (see above), which causes all E2E tests to fail due to running out of disk space on the Azure provided machines. See https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35599&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a or https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35573&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a for example. 

This issue most likely causes FLINK-24433",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 13 10:56:00 UTC 2022,,,,,,,,,,"0|z12aiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 08:49;martijnvisser;The error message implies that Elasticsearch is in read-only mode due to lack of disk space, so it's probably suffering (instead of causing) FLINK-24433. ;;;","13/May/22 10:56;martijnvisser;This issue is resolved via FLINK-24433;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support windowAll operator in Python DataStream API,FLINK-27577,13444536,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,cun8cun8,cun8cun8,12/May/22 07:41,12/May/22 07:46,04/Jun/24 20:51,12/May/22 07:46,1.15.0,,,,,,,,,,,,,,,,,,,,,,,0,,,,,Add the windowall operator to align the windowsall operator already supported in the Java API.,,,,,,,,,,,,,,,,,,,,,,FLINK-26480,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-12 07:41:29.0,,,,,,,,,,"0|z12ai0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink will request new pod when jm pod is delete, but will remove when TaskExecutor exceeded the idle timeout ",FLINK-27576,13444397,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhisheng,zhisheng,11/May/22 12:11,30/Mar/23 08:51,04/Jun/24 20:51,30/Mar/23 08:51,1.12.0,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,0,,,,,"flink 1.12.0 enable the ha(zk) and checkpoint, when i use kubectl delete the jm pod, the job will  request new jm pod failover from the last checkpoint , it is ok.  But it will request new tm pod again, but not use actually, the new tm pod will closed when TaskExecutor exceeded the idle timeout . actually it will use the old tm, why need to request for new tm pod? whether the job will fail if the cluster has no resource for the new tm？Can we optimize and reuse the old tm directly？

 

[^jobmanager_log.txt]

^!image-2022-05-11-20-06-58-955.png!^

^!image-2022-05-11-20-08-01-739.png|width=857,height=324!^",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24713,,,,,,,,,,,,,,,,,,,"11/May/22 12:07;zhisheng;image-2022-05-11-20-06-58-955.png;https://issues.apache.org/jira/secure/attachment/13043527/image-2022-05-11-20-06-58-955.png","11/May/22 12:08;zhisheng;image-2022-05-11-20-08-01-739.png;https://issues.apache.org/jira/secure/attachment/13043526/image-2022-05-11-20-08-01-739.png","11/May/22 12:00;zhisheng;jobmanager_log.txt;https://issues.apache.org/jira/secure/attachment/13043528/jobmanager_log.txt",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 01:50:52 UTC 2022,,,,,,,,,,"0|z129nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 14:36;aitozi;Hi [~zhisheng] there is a ticket tracking this https://issues.apache.org/jira/browse/FLINK-24713 I will open a PR for this soon ;;;","16/May/22 08:33;zhisheng;hi [~aitozi] , Have you started a PR?;;;","17/May/22 01:57;aitozi;I have not started, I will prepare the PR this week.;;;","22/May/22 14:25;aitozi;Hi [~zhisheng] I have opened a PR for this, please let me know if you have any suggestion, thanks

 

https://github.com/apache/flink/pull/19786;;;","26/Jul/22 01:50;aitozi;Hi [~zhisheng], I think this problem have been fixed via https://github.com/apache/flink/pull/20256 . Could you try on that by configure a suitable {{resourcemanager.previous-worker.recovery.timeout}}. If it works, this ticket could be closed. Looking forward to your feedback :). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PyFlink in Python3.9,FLINK-27575,13444385,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,ana4,hxbks2ks,hxbks2ks,11/May/22 11:32,12/May/22 03:00,04/Jun/24 20:51,12/May/22 02:59,1.16.0,,,,,,,,,,,,,,,,,,API / Python,,,,,0,,,,,"Currently, PyFlink only supports Python 3.6,3.7 & 3.8. We need to support Python3.9 in release-1.16",,,,,,,,,,,,,,,,,,,,,,,FLINK-25188,,,,FLINK-27058,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 02:59:40 UTC 2022,,,,,,,,,,"0|z129kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 12:23;ana4;[~hxbks2ks] I would like to take this.;;;","11/May/22 12:25;hxbks2ks;Thanks [~ana4]. I have assigned it to you.;;;","12/May/22 02:59;dianfu;[~hxbks2ks] [~ana4] I'm closing this ticket as a duplicate of FLINK-27058 which is created earlier and has more context in the JIRA description. Let's support it in FLINK-27058.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[QUESTION] In Flink k8s Application mode with HA can not using History Server for history backend,FLINK-27574,13444381,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Not A Problem,,tanjialiang,tanjialiang,11/May/22 11:07,11/May/22 11:37,04/Jun/24 20:51,11/May/22 11:37,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"In Flink k8s application mode with high-availability, it's job id always 0000000000, but in history server, it make job's id for the key. Can I modify the job id in HA?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 11:37:47 UTC 2022,,,,,,,,,,"0|z129k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 11:37;martijnvisser;[~tanjialiang] Please ask questions to the User mailing list instead of via Jira tickets. See https://flink.apache.org/community.html for all details;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuring a new random job result store directory,FLINK-27573,13444378,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,morhidi,morhidi,,11/May/22 10:41,24/Nov/22 01:02,04/Jun/24 20:51,12/May/22 14:01,kubernetes-operator-0.1.0,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Create a random job result store directory to work around:

https://issues.apache.org/jira/browse/FLINK-27569",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27569,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 14:01:16 UTC 2022,,,,,,,,,,"0|z129jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 14:01;gyfora;merged to main 73369b851f2cd92a6818bb84e21157518d63a48d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify HA Metadata present before performing last-state restore,FLINK-27572,13444345,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,11/May/22 08:28,19/May/22 10:21,04/Jun/24 20:51,16/May/22 09:37,,,,,,,,,,,,,,,kubernetes-operator-1.0.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"When we restore a job using the last-state logic we need to verify that the HA metadata has not been deleted. And if it's not there we need to simply throw an error because this requires manual user intervention.

This only applies when the FlinkDeployment is not already in a suspended state with recorded last state information.

The problem be reproduced easily in 1.14 by triggering a fatal job error. (turn of restart-strategy and kill TM for example). In these cases HA metadata will be removed, and the next last-state upgrade should throw an error instead of restoring from a completely empty state. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 09:37:06 UTC 2022,,,,,,,,,,"0|z129c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 08:28;gyfora;cc [~wangyang0918] [~matyas] ;;;","12/May/22 03:34;wangyang0918;This is only necessary for Flink 1.14 and previous versions since FLINK-27495 will cover the 1.15 and later. Right?

Not considering the ZK HA, maybe we could simply verify the existence of HA ConfigMaps.

Another question is how could the users fix this manually? They need to find out the latest external checkpoint and specify it via {{{}execution.savepoint.path{}}}.;;;","12/May/22 05:29;gyfora;[~wangyang0918] It is most useful in 1.14 but due to the ResultStore limitations as we have seen there are cases when its required also in 1.15 (job completes/fails and jobmanager pod dies before the first subsequent observe).

I think verifying the existence of the configmaps should be enough yes.

Yes manual fix is to find the latest external checkpoint/savepoint manually but I think you need to delete the flinkdeployment resource completely and recreate while specifying initialSavepointPath. The savepoint config that you mentioned is basically ignored the way we use it.;;;","16/May/22 09:37;gyfora;merged to main a0aca64bff2c9dc355ed2f9a907f861ffaf556df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Recognize ""less is better"" benchmarks in regression detection script",FLINK-27571,13444342,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,roman,roman,11/May/22 08:22,08/Feb/23 10:45,04/Jun/24 20:51,08/Feb/23 10:45,1.16.0,,,,,,,,,,,,,,1.17.0,,,,Benchmarks,,,,,0,pull-request-available,,,,"Example benchmark:

[http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=schedulingDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200]

 

[Proposed solution|https://issues.apache.org/jira/browse/FLINK-27555?focusedCommentId=17534423&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17534423]:
{quote}
I think #2 is the correct way.
Maybe we can modify the save_jmh_result.py to correctly set the 'units' and the 'lessisbetter' fields of benchmark results. The 'units' is already contained in the jmh result and the 'lessisbetter' can be derived from the mode(false if it is 'thrpt' mode, otherwise true). An example of the jmh result format can be found at https://i.stack.imgur.com/vB3fV.png.
This can fix the web UI as well as the REST result, and then the regression_report.py will be able to identify which benchmarks are ""less is better"" and treat them differently.
{quote}

 ",,,,,,,,,,,,,,FLINK-29825,,,,FLINK-27555,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/22 08:22;roman;Screenshot_2022-05-09_10-33-11.png;https://issues.apache.org/jira/secure/attachment/13043515/Screenshot_2022-05-09_10-33-11.png","29/Dec/22 06:40;Yanfei Lei;image-2022-12-29-14-39-59-976.png;https://issues.apache.org/jira/secure/attachment/13054203/image-2022-12-29-14-39-59-976.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 08 10:45:07 UTC 2023,,,,,,,,,,"0|z129bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/22 06:56;Yanfei Lei;Hi [~roman] , could you please help take a look [pull/63|https://github.com/apache/flink-benchmarks/pull/63] in your free time? thanks :)

 

Thanks to [pull/55|https://github.com/apache/flink-benchmarks/pull/55], the information for future new benchmarks will be correctly stored.

But for the old existing benchmarks, their information cannot be changed only through ‘http://url/result/add/json/’.  Because *codespeed* stores the basic information of benchmark when a result is added {*}for the first time{*}, and will not update it later( related code is [here|https://github.com/tobami/codespeed/blob/263860bc298fd970c8466b3161de386582e4f801/codespeed/results.py#L61]).  So I logged into the Jenkins master and modified the `codespeed_benchmark` table in SQLite, currently ""less is better"" can be displayed normally on the [timeline|http://codespeed.dak8s.net:8000/timeline/#/?ben=schedulingDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200&exe=1,3,5,6,8,9]. ;;;","29/Dec/22 23:20;roman;Thanks for picking this up [~Yanfei Lei]!
I'll take a look.;;;","08/Feb/23 10:45;pnowojski;merged commit b21a946 into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint path error does not cause the job to stop,FLINK-27570,13444341,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Underwood,Underwood,Underwood,11/May/22 08:12,12/Aug/22 07:43,04/Jun/24 20:51,12/Aug/22 07:41,1.14.4,1.15.0,1.16.0,,,,,,,,,,,,1.15.2,1.16.0,,,Runtime / Checkpointing,,,,,0,pull-request-available,,,,"I configured the wrong checkpoint path when starting the job, and set：
{code:java}
conf.set (executioncheckpointingoptions. Tolerable_failure_number, 0);
env setRestartStrategy(RestartStrategies.noRestart());
{code}
The job is expected to stop due to a checkpoint error, but the job is still running.

Here is my job configuration and environment：

!image-2022-05-11-16-13-20-709.png!

!image-2022-05-11-16-12-11-818.png!

!image-2022-05-11-16-12-22-157.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/22 08:12;Underwood;image-2022-05-11-16-12-11-818.png;https://issues.apache.org/jira/secure/attachment/13043512/image-2022-05-11-16-12-11-818.png","11/May/22 08:12;Underwood;image-2022-05-11-16-12-22-157.png;https://issues.apache.org/jira/secure/attachment/13043513/image-2022-05-11-16-12-22-157.png","11/May/22 08:13;Underwood;image-2022-05-11-16-13-20-709.png;https://issues.apache.org/jira/secure/attachment/13043514/image-2022-05-11-16-13-20-709.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 07:41:14 UTC 2022,,,,,,,,,,"0|z129b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/22 07:43;Zsigner;Hi 
Can you provide the details of the checkpoint page? If you enable checkpoint and fill in the wrong path, the checkpoint should not be successful and a hdfs file should be generated.
 ;;;","11/Aug/22 14:48;Underwood;[~martijnvisser] [~MartijnVisser] 

I submitted a PR to solve this problem, could you please take a review?

[https://github.com/apache/flink/pull/20091];;;","12/Aug/22 07:41;roman;Merged into master as 88b309b7dcad269ad084eab5e2944724daf6dee4,
into 1.15 as b24c51611628807ecb78337dc7adae70d359c5b5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Terminated Flink job restarted from empty state when execution.shutdown-on-application-finish is false,FLINK-27569,13444322,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Won't Fix,,gyfora,gyfora,11/May/22 06:59,20/Jun/22 15:06,04/Jun/24 20:51,20/Jun/22 15:06,1.15.0,,,,,,,,,,,,,,,,,,Deployment / Kubernetes,Runtime / Checkpointing,,,,0,,,,,"When Jobmanager HA is enabled and execution.shutdown-on-application-finish = false, terminated jobs (failed, cancelled etc) will be resubmitted from a compeltely empty state on jobmanager failover.

Please see the following situation. Flink 1.15, HA enabled, shutdown on app finish off:

1. Submit Flink application cluster
2. Call cancel with savepoint -> see logs below

job succesfully finishes with savepoint
{noformat}
2022-05-11 06:42:48,562 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 00000000000000000000000000000000 reached terminal state FINISHED.
2022-05-11 06:42:48,624 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 00000000000000000000000000000000 has been registered for cleanup in the JobResultStore after reaching a terminal state.
2022-05-11 06:42:48,626 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'State machine job' (00000000000000000000000000000000).
2022-05-11 06:42:48,629 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStore [] - Shutting down
2022-05-11 06:42:48,647 INFO  org.apache.flink.kubernetes.highavailability.KubernetesCheckpointIDCounter [] - Shutting down.
2022-05-11 06:42:48,647 INFO  org.apache.flink.kubernetes.highavailability.KubernetesCheckpointIDCounter [] - Removing counter from ConfigMap basic-checkpoint-ha-example-00000000000000000000000000000000-config-map
2022-05-11 06:42:48,652 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [0cdb18eefcb2133049223214d4716fa0].
2022-05-11 06:42:48,653 INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [bf5ece74692d786f6ba2b067c76ee1d9].
2022-05-11 06:42:48,653 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 220ea961c86ea8042fde2151fd05a5c9: Stopping JobMaster for job 'State machine job' (00000000000000000000000000000000).
2022-05-11 06:42:48,653 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Stopping DefaultLeaderRetrievalService.
2022-05-11 06:42:48,653 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver [] - Stopping KubernetesLeaderRetrievalDriver{configMapName='basic-checkpoint-ha-example-cluster-config-map'}.
2022-05-11 06:42:48,655 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer [] - Stopped to watch for default/basic-checkpoint-ha-example-cluster-config-map, watching id:9a1bc36b-6a76-4970-96a0-945e9a12b66d
2022-05-11 06:42:48,655 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Stopping DefaultLeaderRetrievalService.
2022-05-11 06:42:48,655 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver [] - Stopping KubernetesLeaderRetrievalDriver{configMapName='basic-checkpoint-ha-example-cluster-config-map'}.
2022-05-11 06:42:48,655 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer [] - Stopped to watch for default/basic-checkpoint-ha-example-cluster-config-map, watching id:5facec4c-d888-43b4-88d0-d1f34912d35a
2022-05-11 06:42:48,655 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Disconnect job manager 969eeac09f5cf4813103003495204620@akka.tcp://flink@172.17.0.6:6123/user/rpc/jobmanager_2 for job 00000000000000000000000000000000 from the resource manager.
2022-05-11 06:42:48,660 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Stopping DefaultLeaderElectionService.
2022-05-11 06:42:48,723 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices [] - Clean up the high availability data for job 00000000000000000000000000000000.
2022-05-11 06:42:48,753 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Removed job graph 00000000000000000000000000000000 from KubernetesStateHandleStore{configMapName='basic-checkpoint-ha-example-cluster-config-map'}.
2022-05-11 06:42:48,758 INFO  org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionHaServices [] - Finished cleaning up the high availability data for job 00000000000000000000000000000000.
2022-05-11 06:42:50,321 INFO  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application completed SUCCESSFULLY{noformat}
!Screenshot 2022-05-11 at 08.46.51.png|width=882,height=106!

3. Trigger JobManager failover

Jobmanager recovers, but resubmits job from empty state:
{noformat}
2022-05-11 06:48:04,535 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Job 00000000000000000000000000000000 is submitted.
2022-05-11 06:48:04,535 INFO  org.apache.flink.client.deployment.application.executors.EmbeddedExecutor [] - Submitting Job with JobId=00000000000000000000000000000000.
2022-05-11 06:48:04,629 INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Recovered 0 pods from previous attempts, current attempt id is 1.
2022-05-11 06:48:04,629 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Recovered 0 workers from previous attempt.
2022-05-11 06:48:04,650 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'State machine job' (00000000000000000000000000000000).
2022-05-11 06:48:04,652 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'State machine job' (00000000000000000000000000000000).
2022-05-11 06:48:04,746 INFO  org.apache.flink.runtime.jobmanager.DefaultJobGraphStore     [] - Added JobGraph(jobId: 00000000000000000000000000000000) to KubernetesStateHandleStore{configMapName='basic-checkpoint-ha-example-cluster-config-map'}.
2022-05-11 06:48:04,826 INFO  org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService [] - Starting DefaultLeaderElectionService with org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter@370a1b27.
2022-05-11 06:48:04,838 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
2022-05-11 06:48:04,843 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'State machine job' (00000000000000000000000000000000).
2022-05-11 06:48:04,926 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for State machine job (00000000000000000000000000000000).
2022-05-11 06:48:04,955 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='basic-checkpoint-ha-example-00000000000000000000000000000000-config-map'}.
2022-05-11 06:48:04,959 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='basic-checkpoint-ha-example-00000000000000000000000000000000-config-map'}.
2022-05-11 06:48:04,959 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Trying to fetch 0 checkpoints from storage.
2022-05-11 06:48:04,974 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job State machine job (00000000000000000000000000000000).
2022-05-11 06:48:04,974 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.
2022-05-11 06:48:05,032 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 new pipelined regions in 0 ms, total 1 pipelined regions currently.
2022-05-11 06:48:05,035 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - No state backend has been configured, using default (HashMap) org.apache.flink.runtime.state.hashmap.HashMapStateBackend@670a312c
2022-05-11 06:48:05,035 INFO  org.apache.flink.runtime.state.StateBackendLoader            [] - State backend loader loads the state backend as HashMapStateBackend
2022-05-11 06:48:05,036 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Checkpoint storage is set to 'jobmanager'
2022-05-11 06:48:05,053 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - No checkpoint found during restore.
2022-05-11 06:48:05,058 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@bdfaf5f for State machine job (00000000000000000000000000000000).
2022-05-11 06:48:05,065 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Starting DefaultLeaderRetrievalService with KubernetesLeaderRetrievalDriver{configMapName='basic-checkpoint-ha-example-cluster-config-map'}.
2022-05-11 06:48:05,066 INFO  org.apache.flink.kubernetes.kubeclient.resources.KubernetesConfigMapSharedInformer [] - Starting to watch for default/basic-checkpoint-ha-example-cluster-config-map, watching id:83411d91-3094-46c5-b2cc-0576bf5cc161
2022-05-11 06:48:05,126 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job 'State machine job' (00000000000000000000000000000000) under job master id 9c63401786b3856e5c8a0cf069e44198.
2022-05-11 06:48:05,132 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]
{noformat}
!Screenshot 2022-05-11 at 08.50.03.png|width=861,height=79!

 

In addition, checkpoint history is also lost (which is probably the main cause of the issue)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27573,,,,,,,,,,,,"11/May/22 06:57;gyfora;Screenshot 2022-05-11 at 08.46.51.png;https://issues.apache.org/jira/secure/attachment/13043503/Screenshot+2022-05-11+at+08.46.51.png","11/May/22 06:59;gyfora;Screenshot 2022-05-11 at 08.50.03.png;https://issues.apache.org/jira/secure/attachment/13043502/Screenshot+2022-05-11+at+08.50.03.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 20 15:05:51 UTC 2022,,,,,,,,,,"0|z1296w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 07:58;gyfora;cc [~wangyang0918] [~Nicolaus Weidner] ;;;","11/May/22 08:39;wangyang0918;I will take a look on this ticket.;;;","11/May/22 08:48;gyfora;Seems like setting
job-result-store.delete-on-commit: ""false""

Makes it almost work. At least the job is not resubmitted, even if the job data is not recovered :) ;;;","11/May/22 09:04;wangyang0918;The {{job-result-store.delete-on-commit=false}} will also force users to delete the job result store manually if they want to recover the job.;;;","11/May/22 09:17;gyfora;The operator can do that also when it deletes the cluster, so it will work for our use case;;;","11/May/22 09:27;gyfora;Correction [~wangyang0918] : the operator cannot delete it because it could be anywhere. But we could maybe configure a new random jobresult store dir when we start as a workaround. ;;;","11/May/22 10:09;wangyang0918;Yeah. Configuring a new random job result store directory is a work around. It just sounds like job result files leak. Users are not aware of this and do not clean up them forever.;;;","11/May/22 10:17;gyfora;I agree that this is not nice and should be fixed in the Flink side somehow, I will leave this Jiraopen to track a proper fix for Flink. And open a new one for the operator side workaround ;;;","12/May/22 02:27;wangyang0918;cc @[~mapohl] Would you like to share some insights on this ticket? It seems that the job result store could not work together with {{{}execution.shutdown-on-application-finish = false{}}}.;;;","18/May/22 13:14;mapohl;Hi [~gyfora], thanks for your patience. I was off the keyboard for two weeks. Thanks for jumping in [~wangyang0918]. As far as I understand, you're trying to avoid the job to be restarted after a JobManager failover after the job terminated in an Flink cluster run in application mode. That's the exact use-case for which {{job-result-store.delete-on-commit}} was introduced (see FLINK-11813).

The application cluster failover requires a JRS entry to be around to know whether the job was already started and finished (and, therefore, doesn't need to be resubmitted). In that case, the user takes over the responsibility to clean up the JobResultStore entry as [~wangyang0918] pointed out. So far, it doesn't look like we're observing unexpected behavior here.

[~gyfora] may you confirm my observation or do I miss something?;;;","20/Jun/22 14:49;mapohl;[~gyfora] can you confirm my conclusion? Or am I missing something?;;;","20/Jun/22 15:05;gyfora;Makes sense [~mapohl] , closing this ticket;;;",,,,,,,,,,,,,,,,,,,,
Cannot submit JobGraph,FLINK-27568,13444299,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,longtimer,longtimer,11/May/22 03:16,11/May/22 03:16,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"I recently upgraded from 1.12.7 to 1.15.0 and encountered an issue in submitting jobs via the command line. The following exception gets generated during submission. The jobs have been submitted in the past on the same machine in the 1.12.7 release without issue.

```org.apache.flink.util.FlinkException: Failed to execute job 'TestIngester'.
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2108)
        at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:188)
        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:119)
        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1969)
        at com.server.processing.TestIngesterJ.main(TestIngesterJ.java:133)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:836)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:247)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1078)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1156)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1156)
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
        at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$11(RestClusterClient.java:434)
        at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986)
        at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
        at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:373)
        at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
        at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610)
        at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085)
        at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.handler.RestHandlerException: Could not upload job files.
        at org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.lambda$uploadJobGraphFiles$4(JobSubmitHandler.java:201)
        at java.base/java.util.concurrent.CompletableFuture.biApply(CompletableFuture.java:1236)
        at java.base/java.util.concurrent.CompletableFuture$BiApply.tryFire(CompletableFuture.java:1205)
        at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
        at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.util.FlinkException: Could not upload job files.
        at org.apache.flink.runtime.client.ClientUtils.uploadJobGraphFiles(ClientUtils.java:86)
        at org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.lambda$uploadJobGraphFiles$4(JobSubmitHandler.java:195)
        ... 10 more
Caused by: java.io.IOException: Could not connect to BlobServer at address localhost/127.0.0.1:33831
        at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:103)
        at org.apache.flink.runtime.rest.handler.job.JobSubmitHandler.lambda$null$3(JobSubmitHandler.java:199)
        at org.apache.flink.runtime.client.ClientUtils.uploadJobGraphFiles(ClientUtils.java:82)
        ... 11 more
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
        at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
        at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
        at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.base/java.net.Socket.connect(Socket.java:609)
        at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:97)
        ... 13 more
]
        at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:532)
        at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:512)
        at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
        ... 4 more

```

One part of the error that stands out is the text 'Could not connect to BlobServer at address localhost/127.0.0.1:33831' because the JobManager is not running on the same server and the JobManager on the other server outputs the following error which indicates that it did receiver some communication the first server where job submission is attempted:

```

2022-05-11 02:58:39,853 ERROR org.apache.flink.runtime.rest.handler.job.JobSubmitHandler   [] - Exception occurred in REST handler: Could not upload job files.

```

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-11 03:16:36.0,,,,,,,,,,"0|z1291s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-aws-kinesis-streams,FLINK-27567,13444189,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,10/May/22 14:28,17/May/22 07:09,04/Jun/24 20:51,17/May/22 07:09,,,,,,,,,,,,,,,1.16.0,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 07:09:29 UTC 2022,,,,,,,,,,"0|z128ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/May/22 07:09;chesnay;master: a10fd2314269e0686d9fbbd3eacb470a03432ef3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-aws-kinesis-firehose,FLINK-27566,13444180,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,10/May/22 13:40,16/May/22 12:54,04/Jun/24 20:51,16/May/22 12:54,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 12:54:36 UTC 2022,,,,,,,,,,"0|z128bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/22 12:54;chesnay;master: 461bddecc683a0a41c4a3ea76a39f4836b871667;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump minimist from 1.2.5 to 1.2.6 in /flink-runtime-web/web-dashboard,FLINK-27565,13444173,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,10/May/22 13:14,11/May/22 06:27,04/Jun/24 20:51,11/May/22 06:27,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Web Frontend,,,,,0,pull-request-available,,,,"It's recommended to use version 1.2.6 or later:

* https://security.snyk.io/vuln/SNYK-JS-MINIMIST-2429795 (version <=1.2.5)
* https://snyk.io/vuln/SNYK-JS-MINIMIST-559764 (version <=1.2.3)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 06:27:42 UTC 2022,,,,,,,,,,"0|z128a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 06:27;airblader;Resolved on master

commit 1e980a33e76c9725f545795d12a548e967f80063
[runtime-web] Bump minimist from 1.2.5 to 1.2.6 in /flink-runtime-web/web-dashboard. This closes #19501;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split PyFlink DataStream connectors into separate files,FLINK-27564,13444167,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,10/May/22 12:19,13/May/22 08:42,04/Jun/24 20:51,13/May/22 08:42,,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,"Currently all the connectors are located in [connectors.py|https://github.com/apache/flink/blob/master/flink-python/pyflink/datastream/connectors.py]. As more and more connectors are added, it makes this file more and more big. Besides, it's confusing for users as it's not clear for users which classes belonging to which connectors. It would be great to split different connectors into different files, e.g. pyflink/datastream/connectors/jdbc.py, 
pyflink/datastream/connectors/file_system.py, etc.

However, it should be taken into account to keep backward compatibility when performing this refactor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 13 08:42:48 UTC 2022,,,,,,,,,,"0|z1288w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 09:41;ana4;I would like to take this. [~dianfu] ;;;","13/May/22 01:00;dianfu;[~ana4] Oh, sorry, just saw that. I'm already working on this and have submitted a PR. ;;;","13/May/22 08:42;dianfu;Merged to master via 3595a73d8cb0750cafcc4b7dc7ff91ef86ed7147;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Resource Providers - Yarn doc page has minor display error,FLINK-27563,13444141,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wanglijie,zhongwei,zhongwei,10/May/22 09:45,11/May/22 07:01,04/Jun/24 20:51,11/May/22 07:01,1.15.0,,,,,,,,,,,,,,1.15.1,1.16.0,,,Documentation,,,,,0,pull-request-available,,,,"doc link: [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/resource-providers/yarn/]

screen shot:

!image-2022-05-10-17-44-37-241.png|width=811,height=301!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/22 09:44;zhongwei;image-2022-05-10-17-44-37-241.png;https://issues.apache.org/jira/secure/attachment/13043458/image-2022-05-10-17-44-37-241.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 07:01:56 UTC 2022,,,,,,,,,,"0|z12834:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 11:03;wanglijie;This was introduced in [PR18450|https://github.com/apache/flink/pull/18450] , I will fix it.

 ;;;","11/May/22 07:01;wangyang0918;Fixed via:

master: 061f596096156231b7f899583cb26b678896594a

release-1.15: e779f3c7cc7b84e0deb09a5f605607a64a6e28e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating HDFS table in SQL client throws UnsupportedFileSystemSchemeException even if hadoop jar is added through --jar argument,FLINK-27562,13444137,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,TsReaper,TsReaper,10/May/22 09:19,10/May/22 10:53,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / Client,,,,,0,,,,,"I've downloaded flink-shaded-hadoop-2-uber-2.8.3-10.0.jar and started a SQL client process with command {{bin/sql-client --jar /path/to/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar}}. However when I create an HDFS table in SQL client it complains that
{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not in the classpath/dependencies.
{code}

I've also tried the {{add jar}} statement and that doesn't help either.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 10 10:53:43 UTC 2022,,,,,,,,,,"0|z12828:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 10:53;martijnvisser;I believe that's by design. Filesystem implementations are based on plugins and need to be added before the cluster is started to be on the correct classpath. See also https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/filesystems/plugins/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-connector-aws-base,FLINK-27561,13444132,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,10/May/22 08:40,18/May/22 07:21,04/Jun/24 20:51,18/May/22 07:21,,,,,,,,,,,,,,,1.16.0,,,,Tests,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 07:21:42 UTC 2022,,,,,,,,,,"0|z12814:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/22 07:21;chesnay;master: dbfb2ac7302152dac237282ee656d010c76e46c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor SimpleStateRequestHandler for PyFlink state,FLINK-27560,13444097,13444563,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/May/22 03:04,12/May/22 09:24,04/Jun/24 20:51,12/May/22 09:19,1.15.0,,,,,,,,,,,,,,1.16.0,,,,API / Python,,,,,0,pull-request-available,,,,"Currently SimpleStateRequestHandler.java for handling beam state request from python side is coupled with keyed-state logic, which could be refactored to reduce code duplication when implementing operator state (list/broadcast state can be fit into bag/map logic).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 09:19:28 UTC 2022,,,,,,,,,,"0|z127tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/22 09:19;dianfu;Merged to master via 63f0871d0f3378e3c7a2716db4fa7b41f319e2d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some question about flink operator state,FLINK-27559,13444093,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,Underwood,Underwood,10/May/22 02:52,10/May/22 03:04,04/Jun/24 20:51,10/May/22 03:04,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,"I hope to get two answers to Flink's maintenance status:

 

1. Does custompartition support saving status? In my usage scenario, the partition strategy is dynamically adjusted, which depends on the data in datastream. I hope to make different partition strategies according to different data conditions.

 

For a simple example, I want the first 100 pieces of data in datastream to be range partitioned and the rest of the data to be hash partitioned. At this time, I may need a count to identify the number of pieces of data that have been processed. However, in custompartition, this is only a local variable, so there seem to be two problems: declaring variables in this way can only be used in single concurrency, and it seems that they cannot be counted across slots; In this way, the count data will be lost during fault recovery.

 

Although Flink already has operator state and key value state, custompartition is not an operator, so I don't think it can solve this problem through state. I've considered introducing a zookeeper to save the state, but the introduction of new components will make the system bloated. I don't know whether there is a better way to solve this problem.

 

2. How to make multiple operators share the same state, and even all parallel subtasks of different operators share the same state?

 

For a simple example, my stream processing is divided into four stages: source - > mapa - > mapb - > sink. I hope to have a status count to count the total amount of data processed by all operators. For example, if the source receives one piece of data, then count + 1 when mapa is processed and count + 1 when mapb is processed. Finally, after this piece of data is processed, the value of count is 2.

 

I don't know if there is such a state saving mechanism in Flink, which can meet my scenario and recover from failure at the same time. At present, we can still think of using zookeeper. I don't know if there is a better way.",Flink 1.14.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 10 03:04:43 UTC 2022,,,,,,,,,,"0|z127sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 03:04;yunta;[~Underwood] Flink's JIRA is not a place to ask questions, please ask these questions in the mailing list: https://flink.apache.org/community.html#mailing-lists;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a new optional option for TableStoreFactory to represent planned manifest entries,FLINK-27558,13443996,13443507,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,09/May/22 15:01,13/May/22 04:23,04/Jun/24 20:51,13/May/22 04:23,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"When 
{code:java}
TableStoreFactory.onCompactTable
{code}
gets called, the planned manifest entries need to be injected back into the enriched options, and we need a new key to represent it.

The explained plan should be like 

{code}
== Abstract Syntax Tree ==
LogicalSink(table=[default_catalog.default_database.T0], fields=[f0, f1, f2], hints=[[[OPTIONS options:{num-sorted-run.compaction-trigger=2, path=file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit6150615541165154705/junit667583448612916332/, compaction.scanned-manifest={""snapshotId"":4,""manifestEntries"":{""__DEFAULT__"":{""bucket-0"":[{""fileName"":""data-05f48089-3b58-4a29-8274-9105bfdce224-0"",""minKey"":[""f0=6"",""f1=Jane Eyre"",""f2=9.9""],""maxKey"":[""f0=6"",""f1=Jane Eyre"",""f2=9.9""]},{""fileName"":""data-980d06b9-50c6-4540-ae01-d5c1ff688c4d-0"",""minKey"":[""f0=5"",""f1=Northanger Abby"",""f2=8.6""],""maxKey"":[""f0=5"",""f1=Northanger Abby"",""f2=8.6""]},{""fileName"":""data-beadf34e-9918-4abd-a271-1dcd9733dfb7-0"",""minKey"":[""f0=3"",""f1=The Mansfield Park"",""f2=7.0""],""maxKey"":[""f0=4"",""f1=Sense and Sensibility"",""f2=9.0""]},{""fileName"":""data-5e772406-6b80-4b2f-b6cc-20bf79a11369-0"",""minKey"":[""f0=1"",""f1=Pride and Prejudice"",""f2=9.0""],""maxKey"":[""f0=2"",""f1=Emma"",""f2=8.5""]}]}}}}]]])
+- LogicalTableScan(table=[[default_catalog, default_database, T0]], hints=[[[OPTIONS inheritPath:[] options:{num-sorted-run.compaction-trigger=2, path=file:/var/folders/xd/9dp1y4vd3h56kjkvdk426l500000gn/T/junit6150615541165154705/junit667583448612916332/, compaction.scanned-manifest={""snapshotId"":4,""manifestEntries"":{""__DEFAULT__"":{""bucket-0"":[{""fileName"":""data-05f48089-3b58-4a29-8274-9105bfdce224-0"",""minKey"":[""f0=6"",""f1=Jane Eyre"",""f2=9.9""],""maxKey"":[""f0=6"",""f1=Jane Eyre"",""f2=9.9""]},{""fileName"":""data-980d06b9-50c6-4540-ae01-d5c1ff688c4d-0"",""minKey"":[""f0=5"",""f1=Northanger Abby"",""f2=8.6""],""maxKey"":[""f0=5"",""f1=Northanger Abby"",""f2=8.6""]},{""fileName"":""data-beadf34e-9918-4abd-a271-1dcd9733dfb7-0"",""minKey"":[""f0=3"",""f1=The Mansfield Park"",""f2=7.0""],""maxKey"":[""f0=4"",""f1=Sense and Sensibility"",""f2=9.0""]},{""fileName"":""data-5e772406-6b80-4b2f-b6cc-20bf79a11369-0"",""minKey"":[""f0=1"",""f1=Pride and Prejudice"",""f2=9.0""],""maxKey"":[""f0=2"",""f1=Emma"",""f2=8.5""]}]}}}}]]])
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 13 04:23:44 UTC 2022,,,,,,,,,,"0|z12780:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/May/22 04:23;lzljs3620320;master: a3270a64b06606b6407866e88ae4ea5092cff7df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit compaction when prepareCommit is triggered,FLINK-27557,13443983,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Do,,qingyue,qingyue,09/May/22 13:29,10/Jun/22 14:01,04/Jun/24 20:51,10/Jun/22 13:59,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Currently, FileStoreWrite will scan and plan files when creating a non-empty writer. We should also create a non-empty writer for non-rescale compaction cases but instead use the pre-planned manifest entries.

The sink records should be dumped, and when prepareCommit is invoked, it should trigger submitCompaction. This will generate two snapshots, the first commit kind is ""ADD"" with empty new files, and the second commit kind is ""COMPACT"" with specified manifest entries marked as deleted and compacted files marked as added.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/22 07:07;qingyue;normal-compaction.png;https://issues.apache.org/jira/secure/attachment/13043881/normal-compaction.png","19/May/22 07:08;qingyue;optimized-normal-compaction.png;https://issues.apache.org/jira/secure/attachment/13043883/optimized-normal-compaction.png","19/May/22 07:07;qingyue;rescale-bucket-compaction.png;https://issues.apache.org/jira/secure/attachment/13043882/rescale-bucket-compaction.png",,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 10 13:59:44 UTC 2022,,,,,,,,,,"0|z12754:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 06:50;lzljs3620320;Can we just use overwrite?
 * overwrite specific manifest entries.
 * overwrite whole table or some whole partition. (eg: when rescale in compaction);;;","19/May/22 07:08;qingyue;{quote}Can we just use overwrite?
 * overwrite specific manifest entries.
 * overwrite whole table or some whole partition. (eg: when rescale in compaction){quote}
Overwrite means the writer cannot accept the specified manifest entries as restored files, so how to pass them to FileStoreCommit as compact before(mark as delete) is a new question. Beyond that, overwrite corresponds to the commit kind ""OVERWRITE"", but it should be more suitable to use ""COMPACT"" in this situation.

I rethink it, and maybe we don't need to generate the new files for the non-rescale compaction.
 * The simplest way is to build the levels from the restored files, don't sink records to MemTable, and submit compaction when precommit is invoked.
 * In the future, maybe we can introduce a new dedicated compaction source to restore LSM, perform compaction and commit, which avoids the shuffle cost.

 

 

 ;;;","10/Jun/22 13:59;qingyue;The impl has been refactored.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in checkpointSingleInput.UNALIGNED on 29.04.2022,FLINK-27556,13443935,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,09/May/22 08:35,08/Jul/22 15:00,04/Jun/24 20:51,08/Jul/22 15:00,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Benchmarks,Runtime / Checkpointing,,,,0,pull-request-available,,,,http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25511,,,,,,,,,,,,,,,,,"09/May/22 08:36;roman;Screenshot_2022-05-09_10-35-57.png;https://issues.apache.org/jira/secure/attachment/13043408/Screenshot_2022-05-09_10-35-57.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 08 15:00:42 UTC 2022,,,,,,,,,,"0|z126ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 08:44;roman;[~akalashnikov] , [~dwysakowicz] could you please take a look at this?;;;","09/May/22 13:04;akalashnikov;I will take a look;;;","19/May/22 11:38;akalashnikov;[~roman], as I can see the cause of this regression is FLINK-25511(the first commit 5b58ef66a8835bf22db23e1d9836a1e9f4d94045). More precisely, the reason is a new field in ByteStreamStateHandle. As I understand, our microbenchmark is too sensitive to even such a small extra data which we write to each checkpoint now. So we need to think about how it is important for us.
Btw, right now we write 'handleName' twice - directly in 'ByteStreamStateHandle' and inside 'physicalID'. Was it the target? or we can avoid duplication?;;;","25/May/22 10:01;roman;Thanks for looking into it [~akalashnikov].
I'm assuming by writing you mean Java serialization, not MetadataV2V3SerializerBase. That wasn't intended.
I could try to revert https://github.com/apache/flink/pull/19550#discussion_r858603609 and see whether it helps;;;","08/Jul/22 15:00;roman;Merged into master as 231e96b205edadcf9cbccf19d5cd1b414c2eadea.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in schedulingDownstreamTasks on 02.05.2022,FLINK-27555,13443934,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,roman,roman,09/May/22 08:32,11/May/22 08:25,04/Jun/24 20:51,11/May/22 08:25,1.16.0,,,,,,,,,,,,,,,,,,Benchmarks,,,,,0,,,,,http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=schedulingDownstreamTasks.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200,,,,,,,,,,,,,,,FLINK-27571,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/22 08:33;roman;Screenshot_2022-05-09_10-33-11.png;https://issues.apache.org/jira/secure/attachment/13043407/Screenshot_2022-05-09_10-33-11.png",,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 08:25:38 UTC 2022,,,,,,,,,,"0|z126u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 08:42;roman;[~mapohl] could you please take a look at this?

cc: [~maguowei], [~chesnay];;;","10/May/22 04:01;zhuzh;This benchmark result is how much time it takes to do the scheduling. Therefore, it is actually an improvement instead of a regression.
The improvement is caused by FLINK-27460 which removed some logics which are no longer needed.

I can see it is the description on the left of the dashboard that is causing the confusion.
But I'm not sure whether there is a way to change the description for certain benchmarks to clearly show that it is ""Time cost (less is better)"".;;;","10/May/22 10:45;roman;Thanks for the clarification [~zhuzh].

 

Besides of the description, there is also a regression script added in [https://github.com/apache/flink-benchmarks/pull/50]

which actually reported this change as a regression.

 

To prevent such false positives, I see three options:
 # modify the benchmark (e.g. report 1/time)
 # modify the script so that it would treat distinguish ""lower is better"" benchmarks differently
 # ignore the benchmark in script

WDYT?;;;","10/May/22 15:45;zhuzh;I think #2 is the correct way. 
Maybe we can modify the [save_jmh_result.py|https://github.com/apache/flink-benchmarks/blob/master/save_jmh_result.py] to correctly set the 'units' and the 'lessisbetter' fields of benchmark results. The 'units' is already contained in the jmh result and the 'lessisbetter' can be derived from the mode(false if it is 'thrpt' mode, otherwise true). An example of the jmh result format can be found at https://i.stack.imgur.com/vB3fV.png.
This can fix the web UI as well as the REST result, and then the [regression_report.py|https://github.com/apache/flink-benchmarks/blob/master/regression_report.py] will be able to identify which benchmarks are ""less is better"" and treat them differently.
;;;","11/May/22 07:18;smattheis;Would it make sense to create a new issue about fixing/improving the reporting?;;;","11/May/22 08:20;roman;I think it would be more clear to have a separate ticket to update the script and close this one - will do that.

[~zhuzh] comment makes a lot of sense, I'll quote it in the new ticket.;;;","11/May/22 08:25;roman;Closing as Not a bug.

Created FLINK-27571 (please feel free to take it);;;",,,,,,,,,,,,,,,,,,,,,,,,,
The asf-site does not build on Apple M1 ARM (Silicon),FLINK-27554,13443909,13437160,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,becket_qin,becket_qin,09/May/22 06:56,23/Feb/23 14:31,04/Jun/24 20:51,23/Feb/23 14:31,1.15.0,,,,,,,,,,,,,,,,,,Project Website,,,,,0,,,,,"It looks that the ASF website does not build on my laptop with Apple silicon. It errors out when installing libv8 via the following command:
{noformat}
gem install libv8 -v '3.16.14.19' --source 'https://rubygems.org/'
{noformat}
The error logs are following:
{noformat}
current directory: /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/ext/libv8

/System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/bin/ruby -I /System/Library/Frameworks/Ruby.framework/Versions/2.6/usr/lib/ruby/2.6.0 -r ./siteconf20220509-16154-19vsxkp.rb extconf.rb

creating Makefile

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/disable-building-tests.patch

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/disable-werror-on-osx.patch

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/disable-xcode-debugging.patch

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/do-not-imply-vfp3-and-armv7.patch

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/do-not-use-MAP_NORESERVE-on-freebsd.patch

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/do-not-use-vfp2.patch

Applying /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/patches/fPIC-for-static.patch

Compiling v8 for x64

Using python 2.7.18

Using compiler: c++ (clang version 13.1.6)

Unable to find a compiler officially supported by v8.

It is recommended to use GCC v4.4 or higher

Beginning compilation. This will take some time.

Building v8 with env CXX=c++ LINK=c++  /usr/bin/make x64.release vfp2=off vfp3=on hardfp=on ARFLAGS.target=crs werror=no

GYP_GENERATORS=make \

 build/gyp/gyp --generator-output=""out"" build/all.gyp \

               -Ibuild/standalone.gypi --depth=. \

               -Dv8_target_arch=x64 \

               -S.x64  -Dv8_enable_backtrace=1 -Dv8_can_use_vfp2_instructions=false -Dv8_can_use_vfp3_instructions=true -Darm_fpu=vfpv3 -Dwerror='' -Dv8_use_arm_eabi_hardfloat=true

  CXX(target) /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/vendor/v8/out/x64.release/obj.target/preparser_lib/src/allocation.o

clang: warning: include path for libstdc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]

In file included from ../src/allocation.cc:33:

../src/utils.h:33:10: fatal error: 'climits' file not found

#include <climits>

         ^~~~~~~~~

1 error generated.

make[1]: *** [/Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/vendor/v8/out/x64.release/obj.target/preparser_lib/src/allocation.o] Error 1

make: *** [x64.release] Error 2

/Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/ext/libv8/location.rb:36:in `block in verify_installation!': libv8 did not install properly, expected binary v8 archive '/Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/vendor/v8/out/x64.release/obj.target/tools/gyp/libv8_base.a'to exist, but it was not found (Libv8::Location::Vendor::ArchiveNotFound)

 from /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/ext/libv8/location.rb:35:in `each'

 from /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/ext/libv8/location.rb:35:in `verify_installation!'

 from /Library/Ruby/Gems/2.6.0/gems/libv8-3.16.14.19/ext/libv8/location.rb:26:in `install!'

 from extconf.rb:7:in `<main>'

 

extconf failed, exit code 1

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 23 14:31:44 UTC 2023,,,,,,,,,,"0|z126oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/22 05:42;xtsong;I've made a little progress on this.

I ran into the same error with the `docker-build.sh`. Inspired by this [comment|https://github.com/rubyjs/libv8/issues/315#issuecomment-846475866], I tried with the `amd64` image, and it worked! The full command is:
{code:java}
docker run --rm --volume=""$PWD:/srv/flink-web"" --expose=4000 -p 4000:4000 -it --platform linux/amd64 ruby:2.6.0 bash -c ""cd /srv/flink-web && gem install bundler && ./build.sh $@""{code}
Unfortunately, while this works for `./docker-build.sh` (the generated HTML files look good), I ran into another error with `./docker-build.sh -p`.
{code:java}
Configuration file: /srv/flink-web/_config.yml
            Source: /srv/flink-web
       Destination: /srv/flink-web/content
 Incremental build: disabled. Enable with --incremental
      Generating...
                    done in 85.06 seconds.
bundler: failed to load command: jekyll (/srv/flink-web/.rubydeps/ruby/2.6.0/bin/jekyll)
Traceback (most recent call last):
	40: from /usr/local/bundle/bin/bundle:23:in `<main>'
	39: from /usr/local/bundle/bin/bundle:23:in `load'
	38: from /usr/local/bundle/gems/bundler-2.3.13/exe/bundle:36:in `<top (required)>'
	37: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/friendly_errors.rb:103:in `with_friendly_errors'
	36: from /usr/local/bundle/gems/bundler-2.3.13/exe/bundle:48:in `block in <top (required)>'
	35: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/cli.rb:25:in `start'
	34: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/vendor/thor/lib/thor/base.rb:485:in `start'
	33: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/cli.rb:31:in `dispatch'
	32: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/vendor/thor/lib/thor.rb:392:in `dispatch'
	31: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/vendor/thor/lib/thor/invocation.rb:127:in `invoke_command'
	30: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/vendor/thor/lib/thor/command.rb:27:in `run'
	29: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/cli.rb:483:in `exec'
	28: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/cli/exec.rb:23:in `run'
	27: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/cli/exec.rb:58:in `kernel_load'
	26: from /usr/local/bundle/gems/bundler-2.3.13/lib/bundler/cli/exec.rb:58:in `load'
	25: from /srv/flink-web/.rubydeps/ruby/2.6.0/bin/jekyll:23:in `<top (required)>'
	24: from /srv/flink-web/.rubydeps/ruby/2.6.0/bin/jekyll:23:in `load'
	23: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/jekyll-3.0.5/bin/jekyll:17:in `<top (required)>'
	22: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/mercenary-0.3.6/lib/mercenary.rb:19:in `program'
	21: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/mercenary-0.3.6/lib/mercenary/program.rb:42:in `go'
	20: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/mercenary-0.3.6/lib/mercenary/command.rb:220:in `execute'
	19: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/mercenary-0.3.6/lib/mercenary/command.rb:220:in `each'
	18: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/mercenary-0.3.6/lib/mercenary/command.rb:220:in `block in execute'
	17: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/jekyll-3.0.5/lib/jekyll/commands/serve.rb:26:in `block (2 levels) in init_with_program'
	16: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/jekyll-3.0.5/lib/jekyll/commands/build.rb:39:in `process'
	15: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/jekyll-3.0.5/lib/jekyll/commands/build.rb:72:in `watch'
	14: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/jekyll-watch-1.5.1/lib/jekyll/watcher.rb:26:in `watch'
	13: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/listener.rb:92:in `start'
	12: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/fsm.rb:72:in `transition'
	11: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/fsm.rb:105:in `transition_with_callbacks!'
	10: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/fsm.rb:124:in `call'
	 9: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/fsm.rb:124:in `instance_eval'
	 8: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/listener.rb:71:in `block in <class:Listener>'
	 7: from /usr/local/lib/ruby/2.6.0/forwardable.rb:230:in `start'
	 6: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/adapter/base.rb:66:in `start'
	 5: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/adapter/base.rb:42:in `configure'
	 4: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/adapter/base.rb:42:in `each'
	 3: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/adapter/base.rb:47:in `block in configure'
	 2: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/adapter/linux.rb:29:in `_configure'
	 1: from /srv/flink-web/.rubydeps/ruby/2.6.0/gems/listen-3.7.1/lib/listen/adapter/linux.rb:29:in `new'
/srv/flink-web/.rubydeps/ruby/2.6.0/gems/rb-inotify-0.10.1/lib/rb-inotify/notifier.rb:69:in `initialize': Function not implemented - Failed to initialize inotify (Errno::ENOSYS)
{code}
I did a little search. It seems `inotify` is something used for watching file changes, which is not available on M1, and `jekyll` tries to load it even in the non-incremental mode. I don't find a good solution to this.

So in conclusion, *I'm able to build the website locally, but not able to preview it.* I guess that's not really helpful.;;;","14/May/22 06:53;becket_qin;Thanks for digging into this, [~xtsong]. I think this is a good progress. I am wondering if [~vthinkxie] would have some idea about how to make this work.;;;","19/May/22 12:32;martijnvisser;In the end, it all boils down to Ruby dependency {{therubyracer}} which is no longer maintained and won't work on arm64 architectures. I'm wondering if it makes more sense to focus on FLINK-22922 as an 'easier' fix. ;;;","23/Feb/23 14:31;martijnvisser;This has been resolved now that FLINK-22922 is completed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clarify the semantic of RecordWriter interface.,FLINK-27553,13443908,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,openinx,openinx,09/May/22 06:55,15/Jun/22 03:25,04/Jun/24 20:51,15/Jun/22 03:25,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-09 06:55:13.0,,,,,,,,,,"0|z126og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus metrics disappear after starting a job,FLINK-27552,13443906,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,eskabetxe,eskabetxe,09/May/22 06:50,16/May/22 10:19,04/Jun/24 20:51,16/May/22 10:19,1.15.0,,,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,0,,,,,"I have a Standalone cluster (with jobmanager and taskmanager on same machine) on 1.14.4 and I'm testing the migration to 1.15.0

But I keep losing the taskmanager metrics when I start a job on the 1.15 cluster

I use the same configuration as in the previous cluster

{{  }}
{code:java}
metrics.reporters: prom 
metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory 
metrics.reporter.prom.port: 9250-9251{code}
{{ }}
If the cluster is running without jobs I can see the metrics on port 9250 for jobmanager and on port 9251 for taskmanager

If I start a job, the metrics from taskmanager disappear and if I stop the job the metrics come live again

What am I missing?
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27487,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 15:14:05 UTC 2022,,,,,,,,,,"0|z126o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 07:12;peter.schrott;If you are using Kafka source / sink you might not be doing something wrong. There is a bug in Flink 1.15.0. Please find more infos here:
[https://lists.apache.org/thread/6bd9vmcroh7576d7h1kdcd8czf0b4l73]
You also can find a ""workaround"" in the thread.

There is already a Jira Ticket for Flink created:
[https://issues.apache.org/jira/projects/FLINK/issues/FLINK-27487?filter=allopenissues];;;","09/May/22 08:27;eskabetxe;Hi [~peter.schrott] , thanks for the links..

Yes I have a KafkaSource..

I try to disable de KafaMetrics 
{code:java}
kafkaSourceBuilder.setProperty(KafkaSourceOptions.REGISTER_KAFKA_CONSUMER_METRICS.key(), ""false"");
{code}
But I continue without taskmanager metrics;;;","09/May/22 08:45;peter.schrott;[~eskabetxe] if you have a KafkaSink, you need to disable the metrics for it too.;;;","09/May/22 08:50;eskabetxe;nop, only a KafkaSource

the sink is a custom Jdbc implemented with sink2;;;","09/May/22 10:20;peter.schrott;[~eskabetxe] oke! In the mailing list thread Cheasney also gave a way to forward j.u.l. loggings to slf4j, so you could maybe investigate further.;;;","09/May/22 15:14;eskabetxe;I was testing with a job that was a KafkaSink (sorry)

disabling kafkasink metrics works..

 

this could be closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Consider implementing our own status update logic,FLINK-27551,13443903,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,09/May/22 06:21,24/Nov/22 01:03,04/Jun/24 20:51,11/May/22 12:43,,,,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"If a custom resource version is applied while in the middle of a reconcile loop (for the same resource but previous version) the status update will throw an error and re-trigger reconciliation.

In our case this might be problematic as it would mean we would retry operations that are not necessarily retriable and might require manual user intervention.

Please see: [https://github.com/java-operator-sdk/java-operator-sdk/issues/1198]

I think we should consider implementing our own status update logic that is independent of the current resource version to make the flow more robust.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 12:43:59 UTC 2022,,,,,,,,,,"0|z126nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 06:21;gyfora;cc [~morhidi] [~wangyang0918] ;;;","09/May/22 07:24;morhidi;Just thinking out loud: Retrying status updates in case of a version conflict makes no sense, it'll never succeed. We should probably just let the current status update go and wait another reconcile loop to deal with it.;;;","09/May/22 07:58;wangyang0918;I am trying to understand this issue. Do you mean the resource version conflicts will happen when the CR is updated externally in the middle of a reconcile loop? This is due to java-operator-sdk is doing the ""lock resource version and then update"". Right?;;;","09/May/22 08:35;morhidi;Yes, and according to [https://github.com/java-operator-sdk/java-operator-sdk/issues/1198] the retries are supposed to deal with this scenario. I was wondering why is not the casein the operator?;;;","09/May/22 10:25;wangyang0918;It seems they would like to use ""patch"" instead of ""lock-and-update"" in the java-operator-sdk. After then, we will avoid the conflicts. Right?;;;","09/May/22 11:26;gyfora;[~wangyang0918] Yes you are right this is due to the locking behaviour, this might be fixed in the josdk 2.1.3, I will test.

But still I think we should consider making FlinkDeployment status updates manually also. There are cases where we need to persist information into the status before we can proceed. For this we would now need to retrigger reconciliation with an intermediate state persisted (such as storing last savepoint info before shutting down a failed cluster). In these cases we should simply make the status update ourself. This would simplify the logic and eliminate some corner cases .;;;","10/May/22 05:45;wangyang0918;I agree with you that we could have our own status update logic. How to deal with the {{updateErrorStatus}}?;;;","11/May/22 12:43;gyfora;Merged to main c8a431040c6d65130697d57fb539bc926e6f51bf;;;",,,,,,,,,,,,,,,,,,,,,,,,
Remove checking yarn queues before submitting job to Yarn,FLINK-27550,13443899,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zuston,zuston,09/May/22 06:06,21/Aug/23 22:35,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Deployment / YARN,,,,,0,auto-deprioritized-major,pull-request-available,,,"When invoking the method of {{checkYarnQueues}} in YarnClusterDescriptor, 
it will check the specified yarnQueue whether exists in the queues gotten by the YarnClient.QueueInfo.

However when using the capacity-scheduler, the yarn queues path should be retrieved by the api of {{QueueInfo.getQueuePath}}
instead of {{getQueueName}}. 

Due to this, it will always print out the yarn all queues log, but it also can be submitted to Yarn successfully.

The api of getQueuePath is introduced in the latest hadoop version(https://issues.apache.org/jira/browse/YARN-10658), so it's hard to
solve this problem in the older hadoop cluster.

According to the above description, the process of checking is unnecessary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:22 UTC 2023,,,,,,,,,,"0|z126mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 09:50;Wencong Liu;cc [~yangwang166] ;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent bytecode version when enable jdk11 or higher version,FLINK-27549,13443897,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,taoran,taoran,09/May/22 05:26,09/May/22 05:42,04/Jun/24 20:51,,1.14.4,1.15.0,,,,,,,,,,,,,,,,,API / Scala,,,,,0,,,,,"Current flink project build with jdk1.8 is ok.
when enable jdk11, it has some scenarios.
1. Using jdk11 but source 1.8 target 1.8   it's ok.
2. Using jdk11 and with jdk11-target source 11 and target 11. build java ok. But the generated bytecode version of scala class is already 52(jdk8).  We can test with any of a scala class under jdk11 build result. e.g.

{code:java}
javap -verbose AggregateDataSet.class | grep major
major version: 52
{code}


I have test many times and make conclusion that flink jdk11 now is not a completed and pure jdk11 version because of inconsistent bytecode version with java & scala. And it may cause some problems, for example:

1. inconsistent bytecode version between java & scala
2. Using reflection about class may generate some unkown errors
3. Runtime jdk11 environment can not optimize these lower bytecode.

I have dipped into this problem and found out we must  upgrade scala to 2.13(2.11 & 2.12 only support generate bytecode version <=1.8). the 2.13 can support generate scala target 11 bytecode and higher and jdk8.
We can see from scalac, the first is 2.12 the next is scala 2.13:
 !screenshot-1.png! 
 !screenshot-2.png! 

By the way, even if scala-2.12 support generare 11 bytecode, current flink build also fail. Because the config about scala-maven-plugin is error, the correct format about jdk11 is target:11 not target:jvm-11, the latter format only for <=1.8 with scala-2.11 and 2.12.


{code:java}
<plugin>
					<groupId>net.alchim31.maven</groupId>
					<artifactId>scala-maven-plugin</artifactId>
					<version>3.2.2</version>
					<configuration>
						<args>
							<arg>-nobootcp</arg>
							<arg>-target:jvm-${target.java.version}</arg>
						</args>
						<jvmArgs>
							<arg>-Xss2m</arg>
						</jvmArgs>
					</configuration>
				</plugin>
{code}

I wonder current the community whether to support the completed jdk11 version? and  if not, at least we need to comment in pom or flink docs to let users know about this problem?
",,,,,,,,,,,FLINK-27547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/22 05:41;taoran;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13043392/screenshot-1.png","09/May/22 05:41;taoran;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13043393/screenshot-2.png",,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-09 05:26:42.0,,,,,,,,,,"0|z126m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve quick-start of table store,FLINK-27548,13443894,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,monster#12,lzljs3620320,lzljs3620320,09/May/22 05:18,19/Mar/23 05:49,04/Jun/24 20:51,19/Mar/23 05:49,,,,,,,,,,,,,,,table-store-0.4.0,,,,Table Store,,,,,0,,,,,"When the quick-start is completed, the stream job needs to be killed on the flink page and the table needs to be dropped.
But the exiting of the stream job is asynchronous and we need to wait a while between these two actions. Otherwise there will be residue in the file directory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 01 03:11:39 UTC 2023,,,,,,,,,,"0|z126lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 03:20;monster#12;hi [~lzljs3620320] ,please assign to me; thx。;;;","01/Mar/23 03:11;lzljs3620320;[~monster#12] Thanks assgined;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hardcode of pom aboout java11 & java17 target java version may cause hidden error,FLINK-27547,13443893,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,taoran,taoran,09/May/22 05:08,09/May/22 05:36,04/Jun/24 20:51,,1.14.4,1.15.0,,,,,,,,,,,,,,,,,API / Scala,,,,,0,,,,,"Current flink parent pom property <target.java.version>1.8</target.java.version> by default. And many modules inherit this property. But java11-target & java17-target profile not update this property synchronously when enable it.

Because  the hardcode of profile java11-target (java17 with same problem). In a word, current flink build pom has various ways to control java version (it's a bad way).  

{code:java}
<profile>
			<id>java11-target</id>
			<build>
				<plugins>
					<plugin>
						<groupId>org.apache.maven.plugins</groupId>
						<artifactId>maven-compiler-plugin</artifactId>
						<configuration>
							<source>11</source>
							<target>11</target>
                                                   ...
						</configuration>
					</plugin>
				</plugins>
			</build>
		</profile>
{code}
And I think we should use property target.java.version the one way to control the java version like `release` profile. it may like:


{code:java}
<profile>
			<id>java11-target</id>
                         <properties>
				<target.java.version>11</scala.version>
			</properties>
			<build>
				<plugins>
					<plugin>
						<groupId>org.apache.maven.plugins</groupId>
						<artifactId>maven-compiler-plugin</artifactId>
						<configuration>
							<source>${target.java.version}</source>
							<target>${target.java.version}</target>
                                                   ...
						</configuration>
					</plugin>
				</plugins>
			</build>
		</profile>
{code}

Otherwise, It will cause build error when we set target.java.version for 11.  Because current scala-maven-plugin with scala-2.11 or 2.12 only support <=1.8 jvm version (this is an another deep issue with FLINK-27547 below).

[ERROR] scalac error: 'jvm-11' is not a valid choice for '-target'
[INFO]   scalac -help  gives more information
[ERROR] scalac error: bad option: '-target:jvm-11'
[INFO]   scalac -help  gives more information
",,,,,,,,,,FLINK-27549,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-09 05:08:06.0,,,,,,,,,,"0|z126l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add append only writer which implements the RecordWriter interface.,FLINK-27546,13443884,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,openinx,openinx,openinx,09/May/22 03:44,25/Aug/22 02:31,04/Jun/24 20:51,11/May/22 08:42,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"We already has a DataFileWriter in our current flink table store, but this DataFileWriter was designed to flush the records which were sorted by their keys.

For the append-only table, the records to write will not have any keys or sort orders. So let's introduce an append-only writer to ingest the append-only records.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 08:42:58 UTC 2022,,,,,,,,,,"0|z126j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/May/22 08:42;lzljs3620320;master: 925e8cea9f207d1a6f78ab3b0122d1c2d5bd7023;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update examples in PyFlink shell,FLINK-27545,13443877,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,09/May/22 02:31,09/May/22 06:15,04/Jun/24 20:51,09/May/22 06:15,1.14.0,1.15.0,,,,,,,,,,,,,1.14.5,1.15.1,1.16.0,,API / Python,Examples,,,,0,pull-request-available,,,,The examples in pyflink.shell is outdated and we should update it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 06:15:40 UTC 2022,,,,,,,,,,"0|z126hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 06:15;dianfu;Fixed in:
- master via da0e2587a92d60098bc76f55bbc95d5de55e5a40
- release-1.15 via bc0cdc5d63068543c0a91b6977e02de659ebd080
- release-1.14 via 23c11ac50931f86b7040111592af7a9b497b517f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Example code in 'Structure of Table API and SQL Programs' is out of date and cannot run,FLINK-27544,13443875,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yckkcy,yckkcy,yckkcy,09/May/22 02:02,13/Jun/22 13:28,04/Jun/24 20:51,13/Jun/22 13:28,1.14.0,1.14.2,1.14.3,1.14.4,1.15.0,,,,,,,,,,1.16.0,,,,Documentation,,,,,0,pull-request-available,,,,"The example code in [Structure of Table API and SQL Programs|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/common/#structure-of-table-api-and-sql-programs] of ['Concepts & Common API'|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/common/] is out of date and when user run this piece of code, they will get the following result:



{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.SinkTable'.

Table options are:

'connector'='blackhole'
'rows-per-second'='1'
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:262)
	at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:421)
	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:222)
	at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:178)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:178)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:782)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:861)
	at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:56)
	at com.yck.TestTableAPI.main(TestTableAPI.java:43)
Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'blackhole'.

Unsupported options:

rows-per-second

Supported options:

connector
property-version
	at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624)
	at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914)
	at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978)
	at org.apache.flink.connector.blackhole.table.BlackHoleTableSinkFactory.createDynamicTableSink(BlackHoleTableSinkFactory.java:64)
	at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259)
	... 19 more
{code}

I think this mistake would drive users crazy when they first fry Table API & Flink SQL since this is the very first code they see.

Overall this code is outdated in two places:
1. The Query creating temporary table should be 
{code:sql}
CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable (EXCLUDING OPTIONS) 
{code}
instead of 
{code:sql}
CREATE TEMPORARY TABLE SinkTable WITH ('connector' = 'blackhole') LIKE SourceTable 
{code} which missed {code:sql}
(EXCLUDING OPTIONS) 
{code} sql_like_pattern
2. The part creating a source table should be 

{code:java}
        tableEnv.createTemporaryTable(""SourceTable"", TableDescriptor.forConnector(""datagen"")
                .schema(Schema.newBuilder()
                        .column(""f0"", DataTypes.STRING())
                        .build())
                .option(DataGenConnectorOptions.ROWS_PER_SECOND, 1L)
                .build());
{code}
instead of  
{code:java}
tableEnv.createTemporaryTable(""SourceTable"", TableDescriptor.forConnector(""datagen"")
    .schema(Schema.newBuilder()
      .column(""f0"", DataTypes.STRING())
      .build())
    .option(DataGenOptions.ROWS_PER_SECOND, 100)
    .build());
{code}

since the class DataGenOptions was replaced by class DataGenConnectorOptions in 
[this commit|https://github.com/apache/flink/pull/16334/commits/865e71fade2890c584d2aaf28af366249f116f2d#diff-a2b4ad2b1792b147efc81895f0dc1d0d092fbce56f563d1b37b73a2619f29a13]

The test code is in my [github Repository(version 1.15)|https://github.com/ChengkaiYang2022/flink-test/blob/main/flink115/src/main/java/com/yck/TestTableAPI.java#L22] and [version 1.14|https://github.com/ChengkaiYang2022/flink-test/blob/main/flink114/src/main/java/com/yck/TestTableAPI.java]


The affected versions are 1.15 and 1.14.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 13:28:54 UTC 2022,,,,,,,,,,"0|z126h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 02:34;yckkcy;cc [~gaoyunhaii] would you mind to take a look at this?;;;","09/May/22 08:15;martijnvisser;[~yckkcy] Could you work on fixing the issues? ;;;","09/May/22 08:23;yckkcy;[~martijnvisser] sure! ;;;","13/Jun/22 13:28;martijnvisser;Fixed in master: 041b24b2bcd251023b87b692a671dfc42585c858;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hide column statistics collector inside the file format writer.,FLINK-27543,13443848,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,openinx,lzljs3620320,lzljs3620320,08/May/22 11:56,18/May/22 06:56,04/Jun/24 20:51,18/May/22 06:56,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Lots of `fileStatsExtractor == null` looks bad.
I think we can have a `StatsProducer` to unify `StatsExtractor` and `StatsCollector`. To reduce caller complexity.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 18 06:56:28 UTC 2022,,,,,,,,,,"0|z126b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 07:49;openinx;I think a better approach is hiding the `fileStatsExtractor` or `StatsExtractor` inside the writer which was created by the file format's writer factory. 

Let me publish a PR for this.;;;","18/May/22 06:56;lzljs3620320;master: bc0ff529e01df872c7556f4c4bf9244c9191960f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add end to end tests for Hive to read external table store files,FLINK-27542,13443765,13441045,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,07/May/22 09:47,17/Jun/22 07:29,04/Jun/24 20:51,17/Jun/22 07:29,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,To ensure that jar produced by flink-table-store-hive module can actually work in real Hive system we need to add end to end tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 08:47:29 UTC 2022,,,,,,,,,,"0|z125so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 08:47;lzljs3620320;master:

ad9e09dbd9621ff2935f193c53a35b355c76fac6

2bce9d695f06c2775902532b0ccda5b0f0a6a514

949415fd03ae1caf71a85309eb58d38e02b4a5c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support distribute by For FlinkSql,FLINK-27541,13443760,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Tutuya,Tutuya,07/May/22 09:12,27/Oct/23 10:47,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,3,,,,,"Now we cann't add a shuffle-operation in a sql-job.
Sometimes , for example,  I have a kafka-source(three partitions) with parallelism three. And then I have a lookup-join function, I want process the data distribute by id so that the data can split into thre parallelism evenly (The source maybe slant seriously).  
In DataStream API i can do it with keyby(),  but it's so sad that i can  do nothing when i use a sql;
Maybe we can do it like 'select id, f1,f2 from sourceTable distribute by id' like we do it in SparkSql.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 10:47:09 UTC 2023,,,,,,,,,,"0|z125rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/22 09:57;514793425@qq.com;Hi, is there any progress on this?;;;","25/Oct/22 09:56;zhouqi;vote +1;;;","27/Oct/23 10:47;yunfanfighting@foxmail.com;[~Tutuya]Hello, are you still working with this issue. I have do some work with it. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let FileStoreSource accept pre-planned manifest entries,FLINK-27540,13443753,13443507,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,07/May/22 07:33,10/May/22 12:26,04/Jun/24 20:51,10/May/22 12:26,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"When manual compaction is triggered, the manifest entries are collected at the planning phase already(to accelerate the compaction). The source does not need to scan and plan again during the runtime.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 10 12:26:53 UTC 2022,,,,,,,,,,"0|z125q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/22 12:26;lzljs3620320;master: 5c2f1b4ac8c503e9d4212ee6d65b63dd355f02c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support consuming update and delete changes In Windowing TVFs,FLINK-27539,13443752,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hjw,hjw,07/May/22 07:30,06/Feb/24 07:46,04/Jun/24 20:51,06/Feb/24 02:13,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / API,,,,,0,,,,,"custom_kafka is a cdc table

sql:
{code:java}
select DATE_FORMAT(window_end,'yyyy-MM-dd') as date_str,sum(money) as total,name
from TABLE(CUMULATE(TABLE custom_kafka,descriptor(createtime),interval '1' MINUTES,interval '1' DAY ))
where status='1'
group by name,window_start,window_end;
{code}

Error

{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: StreamPhysicalWindowAggregate doesn't support consuming update and delete changes which is produced by node TableSourceScan(table=[[default_catalog, default_database, custom_kafka, watermark=[-(createtime, 5000:INTERVAL SECOND)]]], fields=[name, money, status, createtime, operation_ts])
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.createNewNode(FlinkChangelogModeInferenceProgram.scala:396)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:315)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChild(FlinkChangelogModeInferenceProgram.scala:353)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.$anonfun$visitChildren$1(FlinkChangelogModeInferenceProgram.scala:342)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.$anonfun$visitChildren$1$adapted(FlinkChangelogModeInferenceProgram.scala:341)
 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
 at scala.collection.immutable.Range.foreach(Range.scala:155)
 at scala.collection.TraversableLike.map(TraversableLike.scala:233)
 at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
 at scala.collection.AbstractTraversable.map(Traversable.scala:104)
 at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChildren(FlinkChangelogModeInferenceProgram.scala:341)
{code}


But I found Group Window Aggregation is works when use cdc table
{code:java}
select DATE_FORMAT(TUMBLE_END(createtime,interval '10' MINUTES),'yyyy-MM-dd') as date_str,sum(money) as total,name
from custom_kafka
where status='1'
group by name,TUMBLE(createtime,interval '10' MINUTES)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20281,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 06 07:46:45 UTC 2024,,,,,,,,,,"0|z125ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/22 12:23;martijnvisser;[~hjw] Could you explain your use case for this? I don't see how you could want to change the outcome of a previous window result. It would mean that any given result could be changed at any moment in time. I'm curious what [~jingzhang] thinks on this. ;;;","10/May/22 07:25;hjw;Thanks for your response [~martijnvisser]
I want to I want to count the total amount of money in a day. But the money of record could be changed.
Ex：
name money  createtime
a         100       2022-05-10 15:23:09

The  window result 
name total  
a        100

money changed
a         100->200       2022-05-10 15:23:09

The  window result 
name total  
a        100->200
;;;","10/May/22 11:32;martijnvisser;If you're interested in the total amount of money for a day, then your TUMBLE function would also be a day, right? I would expect that during an open window, any update or delete would be processed and when that window ends, you can the final result. I don't see any value in updating the value of a previous window when that window has closed. ;;;","12/May/22 08:19;hjw;[~martijnvisser]  yes, I  am  interested in the total amount of money for a day. In fact, I perfer Calculate Window to
TUMBLE Window, because  I want need the CUMULATE window result every time step size .  But  Calculate Window is not supported in Group Window Aggregation.
By the way, My source table is a cdc table , so  update and delete  should be  handle correct in Window.
 thx.;;;","11/Aug/22 10:19;kdrcetintas;Hello [~martijnvisser],

I do have a similar requirement. CDC to CDC processing with window grouped output,
Is there any planned action to cover that soon?

Thanks;;;","30/Aug/22 13:27;martijnvisser;[~kdrcetintas] It's not planned at the moment

[~jingzhang] Could this be a bug in the CUMULATE TVF implementation?;;;","05/Feb/24 15:59;martijnvisser;[~qingyue] With FLINK-20281 completed, is this one still open or also resolved? The PR also mentions this ticket;;;","06/Feb/24 02:12;qingyue;Hi [~martijnvisser], this issue duplicates FLINK-20281, and I think it can be closed now. ;;;","06/Feb/24 07:46;martijnvisser;Thank you [~qingyue]!;;;",,,,,,,,,,,,,,,,,,,,,,,
Change flink.version 1.15-SNAPSHOT to 1.15.0 in table store,FLINK-27538,13443736,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,lzljs3620320,lzljs3620320,07/May/22 02:49,15/Jun/22 02:38,04/Jun/24 20:51,15/Jun/22 02:38,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,,,,,"* change flink.version
 * Use flink docker in E2eTestBase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-07 02:49:06.0,,,,,,,,,,"0|z125m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove requirement for Async Sink's RequestEntryT to be serializable,FLINK-27537,13443680,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,DavidLiu001,CrynetLogistics,CrynetLogistics,06/May/22 16:16,27/Jul/22 13:38,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Connectors / Common,,,,,0,,,,,"Currently, in AsyncSinkBase and it's dependent classes, e.g. the sink writer, element converter etc., the RequestEntryT generic type is required to be serializable.

However, this requirement no longer holds and there is nothing that actually requires this.

Proposed approach:
 * Remove the extends serializable from the generic type RequestEntryT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 13:38:41 UTC 2022,,,,,,,,,,"0|z1259s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 10:38;DavidLiu001;Hello,Zichen Liu. I would like to deal with it.  Can you assign this ticket to me? Thank you.;;;","21/Jul/22 10:42;CrynetLogistics;Hello [~DavidLiu001] Thank you for offering. I'm delighted you want to make this change. [~dannycranmer] would you mind assigning David to this jira?;;;","21/Jul/22 11:06;dannycranmer;[~CrynetLogistics] yes, [~DavidLiu001] assigned to you. I can pickup the code review.;;;","22/Jul/22 06:30;DavidLiu001;The following files will be covered.

[flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/AsyncSinkBase.java|https://github.com/apache/flink/blob/423143c1a9dcfba2c8ddc08f4c785451b82802be/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/AsyncSinkBase.java]

 

[flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/table/sink/AsyncDynamicTableSink.java|https://github.com/apache/flink/blob/bdfabeba60db1096ed7e100ed12c4837da5935d7/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/table/sink/AsyncDynamicTableSink.java]

 

[flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/AsyncSinkBaseBuilder.java|https://github.com/apache/flink/blob/25ecc0b9202050b340fb4d6fc95b0585a8119937/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/AsyncSinkBaseBuilder.java]

 

[flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/writer/AsyncSinkWriter.java|https://github.com/apache/flink/blob/a2df2665b6ff411a2aeb9b204fd9d46a2af0ecfa/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/writer/AsyncSinkWriter.java];;;","22/Jul/22 08:11;chesnay;Please explain why this is no longer required, ideally linking to the tickets that made this unnecessary.

AFAICT you would also need to touch the {{BufferedRequestState}} btw.;;;","27/Jul/22 13:38;DavidLiu001;[~CrynetLogistics]  Any comment ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Rename method parameter in AsyncSinkWriter,FLINK-27536,13443643,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,DavidLiu001,CrynetLogistics,CrynetLogistics,06/May/22 13:53,27/Jul/22 07:42,04/Jun/24 20:51,27/Jul/22 07:42,1.15.0,,,,,,,,,,,,,,1.16.0,,,,Connectors / Common,,,,,0,pull-request-available,,,,"Change the abstract method's parameter naming in AsyncSinkWriter.

From

  Consumer<List<RequestEntryT>> requestResult

to

  Consumer<List<RequestEntryT>> requestToRetry

or something similar.

 

This is because the consumer here is supposed to accept a list of requests that need to be retried.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 07:42:35 UTC 2022,,,,,,,,,,"0|z1251k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 10:39;DavidLiu001;Hello,Zichen Liu. I would like to deal with it.  Can you assign this ticket to me? Thank you.;;;","21/Jul/22 10:43;CrynetLogistics;Hi [~DavidLiu001], awesome. [~dannycranmer] would you mind assigning please?;;;","21/Jul/22 11:08;dannycranmer;Done. Happy to pickup the review;;;","22/Jul/22 05:12;DavidLiu001;The pull request

[https://github.com/apache/flink/pull/20360]

Please help review it.

 ;;;","22/Jul/22 07:33;dannycranmer;[~DavidLiu001] thanks for the PR, I have left a comment;;;","22/Jul/22 10:26;DavidLiu001;[~dannycranmer]  please review the feedback in PR. Thanks;;;","26/Jul/22 14:02;DavidLiu001;[~dannycranmer]  Please help review this Pull Request again，which has been updated. 

[https://github.com/apache/flink/pull/20360];;;","27/Jul/22 07:42;dannycranmer;Merged to master, thanks for the contribution [~DavidLiu001];;;",,,,,,,,,,,,,,,,,,,,,,,,
Optimize the unit test execution time,FLINK-27535,13443624,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,lindong,lindong,06/May/22 12:42,19/Apr/23 01:52,04/Jun/24 20:51,19/Apr/23 01:52,,,,,,,,,,,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,0,pull-request-available,,,,"Currently `mvn package` takes 10 minutes to complete in Github actions. A lot of time is spent in running unit tests for algorithms. For example, LogisticRegressionTest takes 82 seconds and KMeansTest takes 43 seconds in [1]. 

This time appears to be more than expected. And it will considerably reduce developer velocity if a developer needs to wait for hours to get test results once we have 100+ algorithms in Flink ML.

We should understand why it takes 82 seconds to run e.g. LogisticRegressionTest and see if there is a way to optimize the test execution time.

[1] https://github.com/apache/flink-ml/runs/6319402103?check_suite_focus=true.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 01:51:23 UTC 2023,,,,,,,,,,"0|z124xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:51;lindong;Merged to apache/flink-ml master branch d08b75bacb5a04fa6259b8cc552408dcfea60a94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apply scalafmt to 1.15 branch,FLINK-27534,13443622,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,06/May/22 12:37,16/May/22 12:59,04/Jun/24 20:51,16/May/22 12:59,,,,,,,,,,,,,,,1.15.1,,,,Build System,,,,,0,pull-request-available,,,,"As discussed on the mailing list:

https://lists.apache.org/thread/9jznwjh73jhcncnx46531kzyr0q7pz90

We backport scalafmt to 1.15 to ease merging of patches.

This includes FLINK-27317, FLINK-27232, and FLINK-26553.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 12:59:52 UTC 2022,,,,,,,,,,"0|z124x4:",9223372036854775807,Scala code has been reformatted and uses scalafmt for easier backporting from the master branch.,,,,,,,,,,,,,,,,,,,"16/May/22 12:59;twalthr;commit 6d68fd7e1c22562ee38ee287e17746859763ddb0
[build] Ignore scalafmt commit in .git-blame-ignore-revs

commit 5d5d4aee5079e498435dc9afe22b1d6514dbda2b
[build] Format code with Spotless/scalafmt;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable AdaptiveSchedulerSimpleITCase#testJobCancellationWhileRestartingSucceeds,FLINK-27533,13443609,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/May/22 11:02,06/May/22 11:48,04/Jun/24 20:51,06/May/22 11:48,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Runtime / Coordination,Tests,,,,0,,,,,"https://dev.azure.com/chesnay/flink/_build/results?buildId=2599&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20

{code}
May 06 10:30:22 [ERROR] org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerSimpleITCase.testJobCancellationWhileRestartingSucceeds  Time elapsed: 0.836 s  <<< ERROR!
May 06 10:30:22 org.apache.flink.util.FlinkException: Exhausted retry attempts.
May 06 10:30:22 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:173)
May 06 10:30:22 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:158)
May 06 10:30:22 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerSimpleITCase.testJobCancellationWhileRestartingSucceeds(AdaptiveSchedulerSimpleITCase.java:128)
May
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 06 11:48:47 UTC 2022,,,,,,,,,,"0|z124u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/22 11:48;chesnay;master: 88a180392b1b67a279b59a6bb71e615907119a0f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop flink-clients test-jar,FLINK-27532,13443577,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,06/May/22 09:35,25/May/22 09:15,04/Jun/24 20:51,25/May/22 09:15,,,,,,,,,,,,,,,1.16.0,,,,Build System,Command Line Client,,,,0,pull-request-available,,,,The test-jar is actually unused and could be removed entirely.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 09:15:56 UTC 2022,,,,,,,,,,"0|z124n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/22 09:15;chesnay;master: 391f5f33d5616ecab819156fd3537d1ba46723c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add explict doc on data consistency problem of RocksDB's map state iterator,FLINK-27531,13443566,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,yunta,yunta,yunta,06/May/22 08:56,06/May/22 08:56,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Documentation,,,,,0,,,,,"Since RocksDB map state is introduced, there exists data consistency problem for iteration. This is not well documented and deserves to add explict documentation on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-06 08:56:48.0,,,,,,,,,,"0|z124ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-227: Support overdraft buffer,FLINK-27530,13443562,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,06/May/22 08:44,19/Sep/22 10:06,04/Jun/24 20:51,29/Jun/22 14:04,,,,,,,,,,,,,,,1.16.0,,,,Runtime / Checkpointing,Runtime / Network,,,,0,,,,,"This is the umbrella issue for the feature of unaligned checkpoints. Refer to the [FLIP-227|https://cwiki.apache.org/confluence/display/FLINK/FLIP-227%3A+Support+overdraft+buffer]  for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 19 10:06:12 UTC 2022,,,,,,,,,,"0|z124js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/22 14:04;pnowojski;Again thanks for your efforts [~fanrui] :);;;","29/Jun/22 14:57;fanrui;Thanks a lot [~pnowojski]  [~akalashnikov] [~dwysakowicz]  for participating in discussions, providing suggestions and code review. I'm happy that Unaligned Checkpoint can solve more scenarios, and I am honored to be involved in the community.;;;","19/Sep/22 09:30;godfreyhe;[~pnowojski][~fanrui] could you update the release notes ?;;;","19/Sep/22 09:44;pnowojski;[~godfreyhe] there were no braking changes that user should be aware of after this change, so there is no need for release notes. Unless we have changed the rules of how are we using the release notes field that I have not been aware of? If that's the case sorry.;;;","19/Sep/22 10:06;fanrui;Hi [~godfreyhe] , as I understand, [~pnowojski]  has written the release note in FLINK-26762 before, that is :
{code:java}
New concept of overdraft network buffers was introduced to mitigate effects of uninterruptible blocking a subtask thread during back pressure. Starting from 1.16.0 Flink subtask can request by default up to 5 extra (overdraft) buffers over the regular configured amount (you can read more about this in the documentation: https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/memory/network_mem_tuning/#overdraft-buffers). This change can slightly increase memory consumption of the Flink Job. To restore the older behaviour you can set `taskmanager.network.memory.max-overdraft-buffers-per-gate` to zero.{code}
 

In theory it has no negative effects, but it is helpful for Unaligned Checkpoints, and I'm not sure whether to introduce this feature in the Flink-1.16 Release note.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSourceSplitEnumerator sourceIndex using error Integer check,FLINK-27529,13443553,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taoran,taoran,taoran,06/May/22 08:18,05/Dec/23 02:49,04/Jun/24 20:51,05/Dec/23 02:49,1.14.4,1.15.0,1.15.1,,,,,,,,,,,,1.19.0,,,,Connectors / HybridSource,,,,,1,pull-request-available,stale-assigned,,,"Currently HybridSourceSplitEnumerator check readerSourceIndex using Integer type but == operator. 


As hybrid source definition, it can concat with more than 2 child sources. so currently works just because Integer cache(only works <=127), if we have more sources will fail on error. In a word, we can't use == to compare Integer index unless we limit hybrid sources only works <=127.

e.g.
{code:java}
        Integer i1 = 128;
        Integer i2 = 128;
        System.out.println(i1 == i2);
        int i3 = 128;
        int i4 = 128;
        System.out.println((Integer) i3 == (Integer) i4);
{code}

It will show false, false.

HybridSource Integer index comparison is below:
{code:java}
@Override
        public Map<Integer, ReaderInfo> registeredReaders() {
            ....
            Integer lastIndex = null;
            for (Integer sourceIndex : readerSourceIndex.values()) {
                if (lastIndex != null && lastIndex != sourceIndex) {
                    return filterRegisteredReaders(readers);
                }
                lastIndex = sourceIndex;
            }
            return readers;
        }

        private Map<Integer, ReaderInfo> filterRegisteredReaders(Map<Integer, ReaderInfo> readers) {
            Map<Integer, ReaderInfo> readersForSource = new HashMap<>(readers.size());
            for (Map.Entry<Integer, ReaderInfo> e : readers.entrySet()) {
                if (readerSourceIndex.get(e.getKey()) == (Integer) sourceIndex) {
                    readersForSource.put(e.getKey(), e.getValue());
                }
            }
            return readersForSource;
        }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 22:35:07 UTC 2023,,,,,,,,,,"0|z124hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/22 22:42;thw;[~lemonjing] can you please add information what actual problem you have observed?;;;","09/May/22 03:06;taoran;[~thw] Thanks for reviewing, I have add some context for this issue and more details at https://github.com/apache/flink/pull/19654. ;;;","11/May/22 09:38;taoran;[~thw] Hi, Thomas. Can u help me to review this pr ?;;;","08/Jul/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","12/Oct/22 09:28;taoran;[~thw]hi, Thomas, can u take a look.  I have post some JetBrains IDEA screenshot to illustrate this problem.   i think we need to change to equals method instead of '== 'when compare Integer object.;;;","12/Oct/22 09:33;taoran;[~renqs] Hi, Qingsheng, can u help to take a look? thanks.;;;","17/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a new configuration option 'compaction.rescale-bucket' for FileStore,FLINK-27528,13443546,13443540,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,06/May/22 07:46,07/May/22 07:55,04/Jun/24 20:51,06/May/22 13:33,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"This config key controls the behavior for {{{}ALTER TABLE ... COMPACT{}}}.

When {{compact.rescale-bucket}} is false, it indicates the compaction will rewrite data according to the bucket number, which is read from manifest meta. The commit will only add/delete files; o.w. it suggests the compaction will read bucket number from catalog meta, and the commit will overwrite the whole partition directory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 06 13:33:10 UTC 2022,,,,,,,,,,"0|z124g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/22 13:33;lzljs3620320;master: d0576e1bbd7f798d68db398593c8368376743203;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a file-based Upsert sink for testing internal components,FLINK-27527,13443544,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,alexanderpreuss,alexanderpreuss,alexanderpreuss,06/May/22 07:33,24/Nov/23 08:27,04/Jun/24 20:51,31/May/22 11:27,1.16.0,,,,,,,,,,,,,,1.16.0,,,,Connectors / Common,,,,,0,pull-request-available,,,,There are a bunch of tests that in order to ensure correctness of their tested component rely on a Sink providing upserts. These tests (e.g. test-sql-client.sh) mostly use the ElasticsearchSink which is a lot of overhead. We want to provide a simple file-based upsert sink for Flink developers to test their components against. The sink should be very simple and is not supposed to be used in production scenarios but rather just to facilitate easier testing.,,,,,,,,,,,,,FLINK-27142,,,,,,,,,,,,,,,,,,,,,FLINK-33608,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 11:27:23 UTC 2022,,,,,,,,,,"0|z124fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 11:27;mapohl;master: d0b7a03432dfb6f9318fc716c15d395cc469e26c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support scaling bucket number for FileStore,FLINK-27526,13443540,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,06/May/22 07:08,14/Jul/22 08:39,04/Jun/24 20:51,14/Jul/22 08:39,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,,,,,"Currently, TableStore does not support changing the number of the bucket (denoted by config option {{{}table-storage.bucket{}}}) once the managed table is created. The reason is that the LSM tree is built under bucket level, and thus writing with different bucket numbers will cause the same record to be hashed to another bucket and leads to data corruption. In the release-0.1, TableStore will detect this change and throw an exception when scanning. See FLINK-27316.

However, this is not flexible and user-friendly. To be more specific, if the bucket number remains unchanged, the number of files under each bucket will grow fast as time passes, slowing down the scan speed to restore the LSM tree and finally influencing read and write latency.

In this ticket, we aim to support changing the bucket number via {{INSERT OVERWRITE(...)}} to provide a way for users to reorganize the existing data layout.

 
{code:sql}
-- alter catalog metadata
ALTER TABLE table_identifier SET('bucket' = 'bucket-num');

-- rescale partition by overwrite
INSERT OVERWRITE table_identifier [PARTITION (partition_spec)] SELECT ...;

-- rescale whole table
INSERT OVERWRITE table_identifier SELECT ...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-06 07:08:09.0,,,,,,,,,,"0|z124ew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a demo for using kafka sink via Flink SQL,FLINK-27525,13443537,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,luoyuxia,luoyuxia,06/May/22 06:51,06/May/22 11:07,04/Jun/24 20:51,,,,,,,,,,,,,,,,,,,,Connectors / Kafka,Documentation,,,,0,,,,,"Currently, there's no demo for writing data to kafka using Flink SQL which is not convenient for newly user. We should add a demo in Flink website. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 06 11:07:27 UTC 2022,,,,,,,,,,"0|z124e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/22 11:07;martijnvisser;[~luoyuxia] Would this be more suitable for a blog post? We also don't have demo's or documentation for other integrations. I think blogs are better suited for this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce cache API to DataStream,FLINK-27524,13443533,13443524,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,xuannan,xuannan,xuannan,06/May/22 06:40,06/Aug/22 14:39,04/Jun/24 20:51,06/Aug/22 14:39,1.16.0,,,,,,,,,,,,,,1.16.0,,,,API / DataStream,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 06 14:39:13 UTC 2022,,,,,,,,,,"0|z124dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 14:39;gaoyunhaii;Merged on master via cf1a29d47a5bb4fb92e98a36934e525d74bae17b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runtime supports producing and consuming cached intermediate result,FLINK-27523,13443531,13443524,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,06/May/22 06:39,19/Sep/22 16:26,04/Jun/24 20:51,20/Jul/22 02:20,1.16.0,,,,,,,,,,,,,,,,,,API / DataStream,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29339,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-06 06:39:12.0,,,,,,,,,,"0|z124cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore max buffers per channel when allocate buffer,FLINK-27522,13443528,13443562,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,06/May/22 06:35,31/May/22 07:52,04/Jun/24 20:51,31/May/22 07:36,1.14.0,1.15.0,,,,,,,,,,,,,1.16.0,,,,Runtime / Checkpointing,Runtime / Network,,,,0,pull-request-available,,,,"This is first task of  [FLIP-227|https://cwiki.apache.org/confluence/display/FLINK/FLIP-227%3A+Support+overdraft+buffer]

The LocalBufferPool will be unavailable when the maxBuffersPerChannel is reached for this channel or availableMemorySegments.isEmpty.

If we request a memory segment from LocalBufferPool and the maxBuffersPerChannel is reached for this channel, we just ignore that and continue to allocate buffer while availableMemorySegments isn't empty in LocalBufferPool.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 07:52:24 UTC 2022,,,,,,,,,,"0|z124c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/May/22 07:36;pnowojski;merged commit 9146220 into apache:master;;;","31/May/22 07:52;fanrui;[~pnowojski] Thanks a lot for your review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-205: Support Cache in DataStream for Batch Processing,FLINK-27521,13443524,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,06/May/22 06:32,09/Aug/22 15:45,04/Jun/24 20:51,09/Aug/22 15:45,1.16.0,,,,,,,,,,,,,,1.16.0,,,,API / DataStream,,,,,0,,,,,"As the DataStream API now supports batch execution mode, we see users using the DataStream API to run batch jobs. Interactive programming is an important use case of Flink batch processing. And the ability to cache intermediate results of a DataStream is crucial to the interactive programming experience.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-06 06:32:37.0,,,,,,,,,,"0|z124bc:",9223372036854775807,"Supports caching the result of a transformation via DataStream#cache(). The cached intermediate result is generated lazily at the first time the intermediate result is computed so that the result can be reused by later jobs. If the cache is lost, it will be recomputed using the original transformations. Notes that currently only batch mode is supported. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use admission-controller-framework in Webhook,FLINK-27520,13443520,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,06/May/22 05:39,24/Nov/22 01:03,04/Jun/24 20:51,30/May/22 12:03,,,,,,,,,,,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,0,pull-request-available,,,,"Use the released [https://github.com/java-operator-sdk/admission-controller-framework]

instead of borrowed source codes in the Webhook module.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 30 12:03:03 UTC 2022,,,,,,,,,,"0|z124ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/22 15:21;morhidi;The released version doesn't appear to be healthy:

[https://repo1.maven.org/maven2/io/javaoperatorsdk/admission-controller-framework/0.1.0/] 

I'll talk to the developers to see what's going on.;;;","30/May/22 12:03;gyfora;merged to main 763c0f98f9c9888b1cdef85f990d1a935c3c5335;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix duplicates names when there are multiple levels of over window aggregate,FLINK-27519,13443519,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hackergin,hackergin,06/May/22 04:18,07/May/22 09:37,04/Jun/24 20:51,,1.15.0,,,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,0,,,,,"A similar  issue like [FLINK-22121|https://issues.apache.org/jira/browse/FLINK-22121]

And can be reproduced by adding this unit test 

org.apache.flink.table.planner.plan.stream.sql.agg.GroupWindowTest#testWindowAggregateWithAnotherWindowAggregate
{code:java}
//代码占位符
@Test
def testWindowAggregateWithAnotherWindowAggregate(): Unit = {
  val sql =
    """"""
      |SELECT CAST(pv AS INT) AS pv, CAST(uv AS INT) AS uv FROM (
      |  SELECT *, count(distinct(c)) over (partition by a order by b desc) AS uv
      |  FROM (
      |    SELECT *, count(*) over (partition by a, c order by b desc) AS pv
      |    FROM MyTable
      |  )
      |)
      |"""""".stripMargin
  util.verifyExecPlan(sql)
} {code}
The error message : 

 

 
{code:java}


//代码占位符
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [w0$o0]    at org.apache.flink.table.types.logical.RowType.validateFields(RowType.java:273)
    at org.apache.flink.table.types.logical.RowType.<init>(RowType.java:158)
    at org.apache.flink.table.types.logical.RowType.of(RowType.java:298)
    at org.apache.flink.table.types.logical.RowType.of(RowType.java:290)
    at org.apache.flink.table.planner.calcite.FlinkTypeFactory$.toLogicalRowType(FlinkTypeFactory.scala:663)
    at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamPhysicalOverAggregate.translateToExecNode(StreamPhysicalOverAggregate.scala:57)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:74)
    at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeGraphGenerator.generate(ExecNodeGraphGenerator.java:71)
 {code}
 

I think we can add come logical in  FlinkLogicalOverAggregate  to avoid duplicate names of  output rowType. 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 07 09:37:54 UTC 2022,,,,,,,,,,"0|z124a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/22 04:19;hackergin;[~jark]  [~TsReaper]  Can you help to confirm this issue. 

If this is a problem , I can help to fix this. ;;;","06/May/22 09:36;paul8263;Hi all,

It should be an issue of Calcite. Calcite will translate the logical field name of over window aggregation to w<window-index>$o<over-index>. Currently all aggregates must be computed on the same window. If the SQL is like:
{code:java}
select agg1(*) over (partition by a order by b), agg2(*) over (partition by a order by b){code}
The two logical fields would be translated to w0$o0 and w0$o1.

However, it seems that the field name replacement does not work well with nested over window SQL, which will result in duplication of field names.;;;","06/May/22 11:14;martijnvisser;Could it be that this issue is actually already resolved in Calcite (since we're not running on the latest version) ? ;;;","06/May/22 12:00;paul8263;Hi [~martijnvisser] ,

I am not sure about this, as in the latest version of Calcite I find some similar codes.  This problem is caused by ProjectToWindowRule. When we use HepPlanner and apply this rule from inner nested SQL to outter one, we will get duplicated field names for over aggregation fields.

 ;;;","07/May/22 08:02;paul8263;Hi [~hackergin] ，

FlinkLogicalOverAggregate  is in VolcanoPlanner. After debug I found the row names have already been  duplicated before applying `LOGICAL_CONVERTERS`。This issue is caused by rule sets in HepPlanner.

Correct me if I am wrong.;;;","07/May/22 08:08;hackergin;[~paul8263]  Yes,  there are already duplicate column names before the LOGICAL_CONVERTERS rule is applied,    I think the easiest way is to modify FlinkLogicalOverAggregate  if  it cannot be fixed in calcite currently. ;;;","07/May/22 09:37;paul8263;Hi [~hackergin] and [~TsReaper],

I created a demo using Calcite HepPlanner and ProjectToWindowRule.
{code:java}
SchemaPlus schemaPlus = Frameworks.createRootSchema(true);

schemaPlus.add(""T"", new ReflectiveSchema(new TestSchema()));
Frameworks.ConfigBuilder configBuilder = Frameworks.newConfigBuilder();

configBuilder.defaultSchema(schemaPlus);

FrameworkConfig frameworkConfig = configBuilder.build();

SqlParser.ConfigBuilder parserConfig = SqlParser.configBuilder(frameworkConfig.getParserConfig());

parserConfig.setCaseSensitive(false).setConfig(parserConfig.build());

Planner planner = Frameworks.getPlanner(frameworkConfig);

SqlNode sqlNode;
RelRoot relRoot = null;
try {

    sqlNode = planner.parse(
        ""SELECT *, count(distinct(\""z\"".\""o\"")) over (partition by \""z\"".\""s\"" order by \""z\"".\""p\"" desc) as uv"" +
        ""  FROM ("" +
        ""    SELECT *, count(*) over (partition by \""t\"".\""s\"", \""t\"".\""o\"" order by \""t\"".\""p\"" desc) as pv"" +
        ""    FROM \""T\"".\""rdf\"" \""t\"" "" +
        ""  ) as \""z\""""
    );

    planner.validate(sqlNode);

    relRoot = planner.rel(sqlNode);
} catch (Exception e) {
    e.printStackTrace();
}

RelNode relNode = relRoot.project();

System.out.print(RelOptUtil.toString(relNode));

HepProgram hepProgram = HepProgram.builder()
        .addRuleInstance(CoreRules.PROJECT_TO_LOGICAL_PROJECT_AND_WINDOW)
        .addMatchOrder(HepMatchOrder.BOTTOM_UP)
        .build();
HepPlanner hepPlanner = new HepPlanner(hepProgram);
hepPlanner.setRoot(relNode);
RelNode bestExp = hepPlanner.findBestExp();
System.out.println(RelOptUtil.toString(bestExp));
System.out.println(bestExp.getRowType().getFieldList());

{code}
The test schema is:
{code:java}
class Triple {
    public String s;
    public String p;
    public String o;

    public Triple(String s, String p, String o) {
        super();
        this.s = s;
        this.p = p;
        this.o = o;
    }
}
public static class TestSchema {
    public final Triple[] rdf = {new Triple(""s"", ""p"", ""o"")};
}
{code}
Then the field list of bestExp it printed was:
{code:java}
[#0: s JavaType(class java.lang.String), #1: p JavaType(class java.lang.String), #2: o JavaType(class java.lang.String), #3: PV BIGINT, #4: w0$o0 BIGINT]
{code}
The field names are not duplicated at all. Everything works well.

I guess the problem might be some rules that precedes ProjectToWindowRule transform the plan. We might need to check the rule set (especially some nested SQL transposition). I am not very faliliar with those rules.

Currenty as a workaround I think we can add the following inside class FlinkLogicalOverAggregate:
{code:scala}
  override def deriveRowType: RelDataType = {
    val typeFactory = cluster.getRexBuilder.getTypeFactory
    val typeBuilder = typeFactory.builder()
    input.getRowType.getFieldList.foreach(field => {
      if (typeBuilder.nameExists(field.getName)) {
        val newFieldName = RowTypeUtils.getUniqueName(field.getName, input.getRowType.getFieldNames)
        typeBuilder.add(
          new RelDataTypeFieldImpl(
            newFieldName,
            field.getIndex,
            field.getType))
      } else {
        typeBuilder.add(field)
      }
    })
    typeBuilder.build()
  }
{code}
Correct me if I am wrong.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Refactor migration tests to support version update automatically,FLINK-27518,13443518,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,06/May/22 04:12,12/Jul/23 07:04,04/Jun/24 20:51,12/May/23 07:02,1.16.0,,,,,,,,,,,,,,1.18.0,,,,Test Infrastructure,,,,,0,pull-request-available,,,,"Currently on releasing each version, we need to manually generate the snapshots for every migration tests and update the current versions. With more and more migration tests are added, this has been more and more intractable. It is better if we could make it happen automatically on cutting new branches. ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29485,,,,,,FLINK-32582,,,,FLINK-31567,FLINK-31593,,,,,,,,,FLINK-30944,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 12 07:01:52 UTC 2023,,,,,,,,,,"0|z124a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/22 11:04;mapohl;FYI: I closed the [PR draft for FLINK-29485|https://github.com/apache/flink/pull/20954]. Feel free to use code segments from there if it's of any help.;;;","25/Nov/22 08:00;mapohl;[~gaoyunhaii] any updates on this effort?;;;","30/Nov/22 06:28;gaoyunhaii;Hi [~mapohl] sorry I'm a bit bound in recent weeks and I'll try to finish this issue as soon as possible. ;;;","30/Nov/22 06:29;mapohl;That's alright. Let me know if you need help.;;;","02/Jan/23 12:34;mapohl;Any updates on that topic, [~gaoyunhaii]? It would be nice to have that in before 1.17 is released.;;;","03/Jan/23 06:54;gaoyunhaii;Got that, I'm now a bit freed and will improve the priority of this issue. I'll open the PR before early next week. ;;;","03/Jan/23 09:23;mapohl;Awesome, thanks~;;;","18/Jan/23 10:34;gaoyunhaii;Sorry for it takes more time than I thought to finish the PR, and I'll open the PR later today. ;;;","12/May/23 07:01;gaoyunhaii;Merged on master via 82fb74e23b1dfa46fa98c89a58d7f3126aeaec6c. ;;;",,,,,,,,,,,,,,,,,,,,,,,
Introduce rolling file writer to write one record each time for append-only table.,FLINK-27517,13443517,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,openinx,openinx,openinx,06/May/22 03:48,25/Aug/22 02:32,04/Jun/24 20:51,08/May/22 12:02,,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,pull-request-available,,,,"Currently,  we table store has introduced a `RollingFile` to write an iterator of rows into the underlying files.  That's suitable for the memory store flush processing, but for append-only table, it usually don't have any memory store to cache those branch of rows temporarily, the idea approach is writing one record into the columnar writer each time, and the columnar writer will cache them  and write into the underlying file in one batch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun May 08 12:02:39 UTC 2022,,,,,,,,,,"0|z1249s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/22 12:02;lzljs3620320;master: 1fedb6586ba292bedf00bb1b6922d5110a2f7fa8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The config execution.attached doesn't take effect when not set by Cli param,FLINK-27516,13443511,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Jiangang,Jiangang,06/May/22 03:03,14/Oct/22 09:03,04/Jun/24 20:51,,1.16.0,,,,,,,,,,,,,,,,,,Client / Job Submission,,,,,0,,,,,"The config execution.attached's default value is false. But no matter what value we set, it takes no effect. After digging in, we find that it is only affected by Cli param as following:
 # If we don't specify -d or -yd, the member detachedMode in ProgramOptions is set to false.
 # In method applyToConfiguration, the execution.attached is set true.
 # No matter what value is set to execution.attached, it take no effect.

If -d or -yd is not set, we should use the config execution.attached. Since the actual attach mode is using for a long time, we may need to change execution.attached's default value to true after the modification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 14 09:03:24 UTC 2022,,,,,,,,,,"0|z1248g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/22 09:03;Jiangang;[~gaoyunhaii] [~klouda] Can you help me verify the problem? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support 'ALTER TABLE ... COMPACT' in TableStore,FLINK-27515,13443507,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,06/May/22 02:32,15/Jul/22 02:46,04/Jun/24 20:51,15/Jul/22 02:46,table-store-0.2.0,,,,,,,,,,,,,,table-store-0.2.0,,,,Table Store,,,,,0,,,,,"Implement 'ALTER TABLE ... COMPACT'[1] in TableStore. 

[1] [https://cwiki.apache.org/confluence/display/FLINK/FLIP-188%3A+Introduce+Built-in+Dynamic+Table+Storage]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25520,,,,,,,,,,,,,FLINK-27652,FLINK-27705,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-05-06 02:32:50.0,,,,,,,,,,"0|z1247k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website links to up-to-date committer and PMC member lists,FLINK-27514,13443504,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xtsong,xtsong,xtsong,06/May/22 01:39,06/May/22 02:37,04/Jun/24 20:51,06/May/22 02:37,,,,,,,,,,,,,,,,,,,Project Website,,,,,0,pull-request-available,,,,"According to the [ML discussion|https://lists.apache.org/thread/679ds6lfqs8f4q8lnt7tnlofl58str4y], we are going to add a link to the up-to-date [committer and PMC member list|https://projects.apache.org/committee.html?flink] from the community page of our project website, as well as a notice about the current list could be outdated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 06 02:37:19 UTC 2022,,,,,,,,,,"0|z1246w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/22 02:37;xtsong;Done: 528cc5f44e918ccc8d055a4cb39b66eb34d8181b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
