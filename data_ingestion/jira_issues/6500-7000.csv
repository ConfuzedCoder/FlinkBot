Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
"Fail to use ""transform using""  when record reader is binary record reader in Hive dialect",FLINK-29013,13477218,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,17/Aug/22 10:35,05/Sep/22 09:34,04/Jun/24 20:41,05/Sep/22 09:34,1.16.0,,,,1.16.0,1.17.0,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"It'll cause NPE when using following code:

 
{code:java}
tableEnv.executeSql(
                ""INSERT OVERWRITE TABLE dest1\n""
                        + ""SELECT TRANSFORM(*)\n""
                        + ""  USING 'cat'\n""
                        + ""  AS mydata STRING\n""
                        + ""    ROW FORMAT SERDE\n""
                        + ""      'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n""
                        + ""    WITH SERDEPROPERTIES (\n""
                        + ""      'serialization.last.column.takes.rest'='true'\n""
                        + ""    )\n""
                        + ""    RECORDREADER 'org.apache.hadoop.hive.ql.exec.BinaryRecordReader'\n""
                        + ""FROM src""){code}
 

The NPE is thrown in

 
{code:java}
// HiveScriptTransformOutReadThread

recordReader.next(reusedWritableObject); {code}
 

For BinaryRecordReader, we should first call method  BinaryRecordReader#createRow to do initialization
{code:java}
// BinaryRecordReader
public Writable createRow() throws IOException {
  bytes = new BytesWritable();
  bytes.setCapacity(maxRecordLength);
  return bytes;
} {code}
otherwise it will throw NPE for the field `bytes` is null in the following code:
{code:java}
// BinaryRecordReader

public int next(Writable row) throws IOException {
  int recordLength = in.read(bytes.get(), 0, maxRecordLength);
  if (recordLength >= 0) {
    bytes.setSize(recordLength);
  }
  return recordLength;
} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 05 09:34:25 UTC 2022,,,,,,,,,,"0|z17tqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 03:33;luoyuxia;To fix it, we should first call RecordReader#createRow just like what Hive does.;;;","05/Sep/22 09:34;jark;Fixed in 
 - master: 8c2a2854eb0612d43c0d47e07d01fc539e227908
 - release-1.16: 3c6e11e08dde0f4d613928cfd14aecb0e7725adb;;;",,,,,,,,,,,,,,,,,,,
flink function doc is not correct,FLINK-29012,13477202,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,17/Aug/22 09:15,23/Aug/22 08:11,04/Jun/24 20:41,23/Aug/22 08:11,1.16.0,,,,1.16.0,,,,Documentation,,,,,,,0,pull-request-available,,,,,,!image-2022-08-17-17-15-39-702.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/22 09:15;jackylau;image-2022-08-17-17-15-39-702.png;https://issues.apache.org/jira/secure/attachment/13048230/image-2022-08-17-17-15-39-702.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 23 08:11:05 UTC 2022,,,,,,,,,,"0|z17tn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 08:11;godfreyhe;Fixed in master: b7c6116c98dc21f5dcce70d8849aa2fb6a7c6ed4;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer for Binarizer,FLINK-29011,13477198,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hotsuns,hotsuns,17/Aug/22 09:08,19/Apr/23 01:34,04/Jun/24 20:41,19/Apr/23 01:34,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 01:33:53 UTC 2023,,,,,,,,,,"0|z17tm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:33;lindong;Merged to apache/flink-ml master branch 5f99ce8687b00c0cd0392a67c677f56a4f121a91;;;",,,,,,,,,,,,,,,,,,,,
flink planner test is not correct,FLINK-29010,13477189,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,17/Aug/22 08:18,23/Aug/22 04:01,04/Jun/24 20:41,23/Aug/22 04:01,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"{code:java}
// code placeholder
private def prepareResult(seq: Seq[Row], isSorted: Boolean): Seq[String] = {
  if (!isSorted) seq.map(_.toString).sortBy(s => s) else seq.map(_.toString)
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-17 08:18:20.0,,,,,,,,,,"0|z17tk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-end-to-end-tests-sql compile failed in hadoop3,FLINK-29009,13477188,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,hxbks2ks,hxbks2ks,17/Aug/22 07:59,19/Aug/22 14:00,04/Jun/24 20:41,19/Aug/22 14:00,1.16.0,,,,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,test-stability,,,,,"{code:java}
2022-08-17T00:39:13.9082097Z Dependency convergence error for com.nimbusds:nimbus-jose-jwt:4.41.1 paths to dependency are:
2022-08-17T00:39:13.9082987Z +-org.apache.flink:flink-end-to-end-tests-sql:1.16-SNAPSHOT
2022-08-17T00:39:13.9083712Z   +-org.apache.hadoop:hadoop-common:3.1.3
2022-08-17T00:39:13.9084340Z     +-org.apache.hadoop:hadoop-auth:3.1.3
2022-08-17T00:39:13.9084963Z       +-com.nimbusds:nimbus-jose-jwt:4.41.1
2022-08-17T00:39:13.9085616Z and
2022-08-17T00:39:13.9086212Z +-org.apache.flink:flink-end-to-end-tests-sql:1.16-SNAPSHOT
2022-08-17T00:39:13.9086864Z   +-org.apache.hadoop:hadoop-common:3.1.3
2022-08-17T00:39:13.9087499Z     +-org.apache.kerby:kerb-simplekdc:1.0.1
2022-08-17T00:39:13.9088125Z       +-org.apache.kerby:kerb-client:1.0.1
2022-08-17T00:39:13.9088753Z         +-org.apache.kerby:token-provider:1.0.1
2022-08-17T00:39:13.9089381Z           +-com.nimbusds:nimbus-jose-jwt:3.10
2022-08-17T00:39:13.9089596Z 
2022-08-17T00:39:13.9090061Z [WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
2022-08-17T00:39:13.9090651Z Failed while enforcing releasability. See above detailed error message.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27790,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 14:00:43 UTC 2022,,,,,,,,,,"0|z17tk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 08:02;hxbks2ks;[~lsy] Could you help take a look? Thx.;;;","18/Aug/22 08:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40126&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691;;;","19/Aug/22 14:00;chesnay;master:
e9f8c791f9fc6c4090b649d2883e5411ccb3ae0f
d524e1dc3fd9f4d983605b0ceb1866342a7e922a;;;",,,,,,,,,,,,,,,,,,
SortMergeResultPartitionTest.testRelease failed with TestTimedOutException,FLINK-29008,13477186,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,hxbks2ks,hxbks2ks,17/Aug/22 07:47,02/Mar/23 09:14,04/Jun/24 20:41,27/Oct/22 07:58,1.16.0,,,,1.17.0,,,,Runtime / Network,,,,,,,0,test-stability,,,,,,"{code:java}
2022-08-17T02:58:01.8887337Z Aug 17 02:58:01 [ERROR] SortMergeResultPartitionTest.testRelease  Time elapsed: 60.843 s  <<< ERROR!
2022-08-17T02:58:01.8887851Z Aug 17 02:58:01 org.junit.runners.model.TestTimedOutException: test timed out after 60 seconds
2022-08-17T02:58:01.8891703Z Aug 17 02:58:01 	at org.apache.flink.runtime.io.network.partition.SortMergeResultPartitionTest.testRelease(SortMergeResultPartitionTest.java:374)
2022-08-17T02:58:01.8892705Z Aug 17 02:58:01 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T02:58:01.8893282Z Aug 17 02:58:01 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T02:58:01.8893966Z Aug 17 02:58:01 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T02:58:01.8898552Z Aug 17 02:58:01 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T02:58:01.8899210Z Aug 17 02:58:01 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T02:58:01.8899844Z Aug 17 02:58:01 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T02:58:01.8900581Z Aug 17 02:58:01 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T02:58:01.8901226Z Aug 17 02:58:01 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-17T02:58:01.8907420Z Aug 17 02:58:01 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-17T02:58:01.8908068Z Aug 17 02:58:01 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-17T02:58:01.8908735Z Aug 17 02:58:01 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2022-08-17T02:58:01.8909447Z Aug 17 02:58:01 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2022-08-17T02:58:01.8910255Z Aug 17 02:58:01 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-17T02:58:01.8915691Z Aug 17 02:58:01 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef
",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29884,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 07:57:45 UTC 2022,,,,,,,,,,"0|z17tjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 07:48;hxbks2ks;[~kevin.cyj] Could you help take a look? Thx.;;;","13/Sep/22 03:23;hxb;Given that it hasn't appeared for a month, I will downgrade the priority to major.;;;","27/Oct/22 07:57;kevin.cyj;I ran this test for more than 1000 times locally, the issue didn't reproduce. I also checked the code but found nothing. As it hasn't appeared for over two months, I am closing it. Feel free to reopen it if it still reproduce.;;;",,,,,,,,,,,,,,,,,,
UsingRemoteJarITCase failed with NPE in hadoop3,FLINK-29007,13477162,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,hxbks2ks,hxbks2ks,17/Aug/22 03:54,13/Feb/23 09:02,04/Jun/24 20:41,18/Aug/22 07:40,1.16.0,,,,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-08-17T03:01:45.9844224Z Aug 17 03:01:45 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar  Time elapsed: 1.319 s  <<< FAILURE!
2022-08-17T03:01:45.9844747Z Aug 17 03:01:45 org.opentest4j.MultipleFailuresError: 
2022-08-17T03:01:45.9845163Z Aug 17 03:01:45 Multiple Failures (2 failures)
2022-08-17T03:01:45.9845669Z Aug 17 03:01:45 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:45.9846199Z Aug 17 03:01:45 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:45.9846801Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:196)
2022-08-17T03:01:45.9847582Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:226)
2022-08-17T03:01:45.9848377Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:192)
2022-08-17T03:01:45.9849199Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:79)
2022-08-17T03:01:45.9849972Z Aug 17 03:01:45 	at org.junit.runner.notification.SynchronizedRunListener.testFinished(SynchronizedRunListener.java:87)
2022-08-17T03:01:45.9850715Z Aug 17 03:01:45 	at org.junit.runner.notification.RunNotifier$9.notifyListener(RunNotifier.java:225)
2022-08-17T03:01:45.9851413Z Aug 17 03:01:45 	at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)
2022-08-17T03:01:45.9852105Z Aug 17 03:01:45 	at org.junit.runner.notification.RunNotifier.fireTestFinished(RunNotifier.java:222)
2022-08-17T03:01:45.9852828Z Aug 17 03:01:45 	at org.junit.internal.runners.model.EachTestNotifier.fireTestFinished(EachTestNotifier.java:38)
2022-08-17T03:01:45.9853510Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:372)
2022-08-17T03:01:45.9854156Z Aug 17 03:01:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-17T03:01:45.9854864Z Aug 17 03:01:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-17T03:01:45.9855512Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-17T03:01:45.9856125Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-17T03:01:45.9856751Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-17T03:01:45.9857372Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-17T03:01:45.9857977Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-17T03:01:45.9858581Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-17T03:01:45.9859177Z Aug 17 03:01:45 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-08-17T03:01:45.9859726Z Aug 17 03:01:45 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-08-17T03:01:45.9860299Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-17T03:01:45.9860905Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-17T03:01:45.9861508Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-17T03:01:45.9862134Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-17T03:01:45.9862831Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-17T03:01:45.9863477Z Aug 17 03:01:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-17T03:01:45.9864128Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-17T03:01:45.9864729Z Aug 17 03:01:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-17T03:01:45.9865289Z Aug 17 03:01:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-17T03:01:45.9865851Z Aug 17 03:01:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-17T03:01:45.9866564Z Aug 17 03:01:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-17T03:01:45.9867293Z Aug 17 03:01:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-17T03:01:45.9868004Z Aug 17 03:01:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-17T03:01:45.9868759Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-17T03:01:45.9869577Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-17T03:01:45.9870401Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-17T03:01:45.9871369Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-17T03:01:45.9872223Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-17T03:01:45.9872988Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-17T03:01:45.9873698Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-17T03:01:45.9874486Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-17T03:01:45.9875313Z Aug 17 03:01:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-17T03:01:45.9876099Z Aug 17 03:01:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-17T03:01:45.9876922Z Aug 17 03:01:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-17T03:01:45.9877736Z Aug 17 03:01:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-08-17T03:01:45.9878501Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-17T03:01:45.9879214Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-17T03:01:45.9879881Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-17T03:01:45.9880543Z Aug 17 03:01:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-17T03:01:45.9881179Z Aug 17 03:01:45 	Suppressed: java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:45.9881755Z Aug 17 03:01:45 		at org.junit.Assert.fail(Assert.java:89)
2022-08-17T03:01:45.9882395Z Aug 17 03:01:45 		at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.createHDFS(UsingRemoteJarITCase.java:88)
2022-08-17T03:01:45.9883166Z Aug 17 03:01:45 		at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.before(UsingRemoteJarITCase.java:68)
2022-08-17T03:01:45.9883816Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T03:01:45.9884518Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T03:01:45.9885230Z Aug 17 03:01:45 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T03:01:45.9885853Z Aug 17 03:01:45 		at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T03:01:45.9886488Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T03:01:45.9887212Z Aug 17 03:01:45 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T03:01:45.9887921Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T03:01:45.9888632Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
2022-08-17T03:01:45.9889325Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
2022-08-17T03:01:45.9889993Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-17T03:01:45.9890671Z Aug 17 03:01:45 		at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-08-17T03:01:45.9891333Z Aug 17 03:01:45 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-17T03:01:45.9891997Z Aug 17 03:01:45 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-17T03:01:45.9892716Z Aug 17 03:01:45 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-17T03:01:45.9893333Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-17T03:01:45.9893995Z Aug 17 03:01:45 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-17T03:01:45.9894654Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-17T03:01:45.9895132Z Aug 17 03:01:45 		... 39 more
2022-08-17T03:01:45.9895540Z Aug 17 03:01:45 	Suppressed: java.lang.NullPointerException
2022-08-17T03:01:45.9896168Z Aug 17 03:01:45 		at org.apache.flink.table.sql.codegen.UsingRemoteJarITCase.destroyHDFS(UsingRemoteJarITCase.java:95)
2022-08-17T03:01:45.9896836Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T03:01:45.9897442Z Aug 17 03:01:45 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T03:01:45.9898150Z Aug 17 03:01:45 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T03:01:45.9898792Z Aug 17 03:01:45 		at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T03:01:45.9899423Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-17T03:01:45.9900141Z Aug 17 03:01:45 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-17T03:01:45.9900850Z Aug 17 03:01:45 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-17T03:01:45.9901552Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
2022-08-17T03:01:45.9902219Z Aug 17 03:01:45 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
2022-08-17T03:01:45.9902895Z Aug 17 03:01:45 		at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2022-08-17T03:01:45.9903554Z Aug 17 03:01:45 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-17T03:01:45.9904220Z Aug 17 03:01:45 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-17T03:01:45.9904859Z Aug 17 03:01:45 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-17T03:01:45.9905473Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-17T03:01:45.9906283Z Aug 17 03:01:45 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-17T03:01:45.9907052Z Aug 17 03:01:45 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-17T03:01:45.9907534Z Aug 17 03:01:45 		... 39 more
2022-08-17T03:01:45.9907852Z Aug 17 03:01:45 
2022-08-17T03:01:46.2525312Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2526850Z Aug 17 03:01:46 [INFO] Results:
2022-08-17T03:01:46.2527307Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2527708Z Aug 17 03:01:46 [ERROR] Failures: 
2022-08-17T03:01:46.2528297Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateCatalogFunctionUsingRemoteJar
2022-08-17T03:01:46.2528903Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2529519Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2551750Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2552413Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2553047Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2553670Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2554125Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2554658Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateTemporaryCatalogFunctionUsingRemoteJar
2022-08-17T03:01:46.2555270Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2555862Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2556665Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2557198Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2557800Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2558407Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2558865Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2559376Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testCreateTemporarySystemFunctionUsingRemoteJar
2022-08-17T03:01:46.2559989Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2560589Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2561194Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2561726Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2562317Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2562926Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2563375Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2563836Z Aug 17 03:01:46 [ERROR] UsingRemoteJarITCase.testUdfInRemoteJar
2022-08-17T03:01:46.2564373Z Aug 17 03:01:46 [ERROR]   Run 1: Multiple Failures (2 failures)
2022-08-17T03:01:46.2564979Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2565573Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2566104Z Aug 17 03:01:46 [ERROR]   Run 2: Multiple Failures (2 failures)
2022-08-17T03:01:46.2566702Z Aug 17 03:01:46 	java.lang.AssertionError: Test failed org/apache/hadoop/hdfs/HdfsConfiguration
2022-08-17T03:01:46.2567306Z Aug 17 03:01:46 	java.lang.NullPointerException: <no message>
2022-08-17T03:01:46.2567755Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2568104Z Aug 17 03:01:46 [INFO] 
2022-08-17T03:01:46.2568576Z Aug 17 03:01:46 [ERROR] Tests run: 6, Failures: 4, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40084&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28916,,FLINK-31033,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 18 07:40:04 UTC 2022,,,,,,,,,,"0|z17te8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 03:55;hxbks2ks;[~lsy] Could you help take a look? Thx;;;","17/Aug/22 04:11;lsy;ok, also cc [~Jiangang] ;;;","18/Aug/22 07:40;chesnay;master: 0be27a5e7b8d57c66cb809d563bbb594ce6abbad;;;",,,,,,,,,,,,,,,,,,
PulsarSourceITCase failed with Could not acquire the minimum required resources.,FLINK-29006,13477154,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,hxbks2ks,hxbks2ks,17/Aug/22 03:25,12/Oct/22 11:22,04/Jun/24 20:41,12/Oct/22 11:21,1.15.1,,,,,,,,Connectors / Pulsar,,,,,,,0,test-stability,,,,,,"{code:java}
2022-08-17T01:58:54.4397238Z Aug 17 01:58:54 [ERROR]   PulsarSourceITCase>SourceTestSuiteBase.testScaleDown:280->SourceTestSuiteBase.restartFromSavepoint:330->SourceTestSuiteBase.checkResultWithSemantic:744 
2022-08-17T01:58:54.4397969Z Aug 17 01:58:54 Expecting
2022-08-17T01:58:54.4398407Z Aug 17 01:58:54   <CompletableFuture[Failed with the following stack trace:
2022-08-17T01:58:54.4399009Z Aug 17 01:58:54 java.lang.RuntimeException: Failed to fetch next result
2022-08-17T01:58:54.4399720Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-08-17T01:58:54.4400608Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-08-17T01:58:54.4401505Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.compareWithExactlyOnceSemantic(CollectIteratorAssert.java:116)
2022-08-17T01:58:54.4402417Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.matchesRecordsFromSource(CollectIteratorAssert.java:71)
2022-08-17T01:58:54.4403459Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.lambda$checkResultWithSemantic$3(SourceTestSuiteBase.java:741)
2022-08-17T01:58:54.4404435Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2022-08-17T01:58:54.4405324Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1596)
2022-08-17T01:58:54.4406006Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-17T01:58:54.4406645Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-17T01:58:54.4407305Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-17T01:58:54.4407974Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-17T01:58:54.4408686Z Aug 17 01:58:54 Caused by: java.io.IOException: Failed to fetch job execution result
2022-08-17T01:58:54.4409432Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-08-17T01:58:54.4410300Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-08-17T01:58:54.4411158Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-08-17T01:58:54.4411842Z Aug 17 01:58:54 	... 10 more
2022-08-17T01:58:54.4412708Z Aug 17 01:58:54 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-08-17T01:58:54.4413686Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-08-17T01:58:54.4414572Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-08-17T01:58:54.4415394Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-08-17T01:58:54.4416024Z Aug 17 01:58:54 	... 12 more
2022-08-17T01:58:54.4416508Z Aug 17 01:58:54 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-08-17T01:58:54.4417327Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-08-17T01:58:54.4418138Z Aug 17 01:58:54 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-08-17T01:58:54.4419016Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-08-17T01:58:54.4419715Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-08-17T01:58:54.4420429Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4421102Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-17T01:58:54.4421861Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
2022-08-17T01:58:54.4422636Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-17T01:58:54.4423369Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-17T01:58:54.4424094Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4425089Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-17T01:58:54.4425789Z Aug 17 01:58:54 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-08-17T01:58:54.4426688Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-08-17T01:58:54.4427515Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-17T01:58:54.4428419Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-08-17T01:58:54.4429329Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-17T01:58:54.4430067Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-17T01:58:54.4430794Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4431489Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-17T01:58:54.4432210Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-08-17T01:58:54.4432881Z Aug 17 01:58:54 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-08-17T01:58:54.4433451Z Aug 17 01:58:54 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-08-17T01:58:54.4434037Z Aug 17 01:58:54 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-08-17T01:58:54.4434896Z Aug 17 01:58:54 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-08-17T01:58:54.4435505Z Aug 17 01:58:54 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-08-17T01:58:54.4436223Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-08-17T01:58:54.4436996Z Aug 17 01:58:54 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-08-17T01:58:54.4437709Z Aug 17 01:58:54 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-08-17T01:58:54.4438463Z Aug 17 01:58:54 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-08-17T01:58:54.4439247Z Aug 17 01:58:54 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-08-17T01:58:54.4439895Z Aug 17 01:58:54 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-08-17T01:58:54.4440623Z Aug 17 01:58:54 	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:118)
2022-08-17T01:58:54.4441277Z Aug 17 01:58:54 	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:1144)
2022-08-17T01:58:54.4441911Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-17T01:58:54.4442472Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-17T01:58:54.4443053Z Aug 17 01:58:54 	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:540)
2022-08-17T01:58:54.4443663Z Aug 17 01:58:54 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-17T01:58:54.4444342Z Aug 17 01:58:54 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-17T01:58:54.4445037Z Aug 17 01:58:54 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-17T01:58:54.4445606Z Aug 17 01:58:54 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-17T01:58:54.4446152Z Aug 17 01:58:54 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-17T01:58:54.4446593Z Aug 17 01:58:54 	... 4 more
2022-08-17T01:58:54.4447124Z Aug 17 01:58:54 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-08-17T01:58:54.4447945Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-08-17T01:58:54.4449004Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-08-17T01:58:54.4449973Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
2022-08-17T01:58:54.4450764Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
2022-08-17T01:58:54.4451597Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
2022-08-17T01:58:54.4452420Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
2022-08-17T01:58:54.4453338Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-08-17T01:58:54.4454496Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1536)
2022-08-17T01:58:54.4455459Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1118)
2022-08-17T01:58:54.4456158Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1058)
2022-08-17T01:58:54.4456871Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:897)
2022-08-17T01:58:54.4457615Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:466)
2022-08-17T01:58:54.4458441Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42)
2022-08-17T01:58:54.4459355Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:658)
2022-08-17T01:58:54.4460245Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:530)
2022-08-17T01:58:54.4461076Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-08-17T01:58:54.4461768Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-08-17T01:58:54.4462478Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4463316Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-08-17T01:58:54.4464075Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
2022-08-17T01:58:54.4465126Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:186)
2022-08-17T01:58:54.4466052Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408)
2022-08-17T01:58:54.4467009Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:397)
2022-08-17T01:58:54.4467882Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:859)
2022-08-17T01:58:54.4468633Z Aug 17 01:58:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T01:58:54.4469265Z Aug 17 01:58:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T01:58:54.4469982Z Aug 17 01:58:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T01:58:54.4470627Z Aug 17 01:58:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T01:58:54.4471311Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:296)
2022-08-17T01:58:54.4472257Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-17T01:58:54.4473044Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:295)
2022-08-17T01:58:54.4473804Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-08-17T01:58:54.4474895Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-08-17T01:58:54.4475685Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-08-17T01:58:54.4476361Z Aug 17 01:58:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-17T01:58:54.4476987Z Aug 17 01:58:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-17T01:58:54.4477602Z Aug 17 01:58:54 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-17T01:58:54.4478211Z Aug 17 01:58:54 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-17T01:58:54.4478937Z Aug 17 01:58:54 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-17T01:58:54.4479580Z Aug 17 01:58:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-17T01:58:54.4480219Z Aug 17 01:58:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-17T01:58:54.4480864Z Aug 17 01:58:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-17T01:58:54.4481462Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-17T01:58:54.4482006Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-17T01:58:54.4482603Z Aug 17 01:58:54 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-17T01:58:54.4483096Z Aug 17 01:58:54 	... 9 more
2022-08-17T01:58:54.4483848Z Aug 17 01:58:54 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-08-17T01:58:54.4484945Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:588)
2022-08-17T01:58:54.4485528Z Aug 17 01:58:54 	... 39 more
2022-08-17T01:58:54.4486186Z Aug 17 01:58:54 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-08-17T01:58:54.4487210Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-08-17T01:58:54.4487934Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-08-17T01:58:54.4488699Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-08-17T01:58:54.4489412Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-08-17T01:58:54.4489941Z Aug 17 01:58:54 	... 37 more
2022-08-17T01:58:54.4490537Z Aug 17 01:58:54 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-08-17T01:58:54.4491087Z Aug 17 01:58:54 ]>
2022-08-17T01:58:54.4491446Z Aug 17 01:58:54 to be completed within 2M.
2022-08-17T01:58:54.4491801Z Aug 17 01:58:54 
2022-08-17T01:58:54.4492400Z Aug 17 01:58:54 exception caught while trying to get the future result: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Failed to fetch next result
2022-08-17T01:58:54.4493183Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-08-17T01:58:54.4493858Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-08-17T01:58:54.4494768Z Aug 17 01:58:54 	at org.assertj.core.internal.Futures.assertSucceededWithin(Futures.java:109)
2022-08-17T01:58:54.4495691Z Aug 17 01:58:54 	at org.assertj.core.api.AbstractCompletableFutureAssert.internalSucceedsWithin(AbstractCompletableFutureAssert.java:400)
2022-08-17T01:58:54.4496558Z Aug 17 01:58:54 	at org.assertj.core.api.AbstractCompletableFutureAssert.succeedsWithin(AbstractCompletableFutureAssert.java:396)
2022-08-17T01:58:54.4497434Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.checkResultWithSemantic(SourceTestSuiteBase.java:744)
2022-08-17T01:58:54.4498337Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:330)
2022-08-17T01:58:54.4499276Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testScaleDown(SourceTestSuiteBase.java:280)
2022-08-17T01:58:54.4499981Z Aug 17 01:58:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T01:58:54.4500597Z Aug 17 01:58:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T01:58:54.4501310Z Aug 17 01:58:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T01:58:54.4501953Z Aug 17 01:58:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T01:58:54.4502601Z Aug 17 01:58:54 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-17T01:58:54.4503337Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-17T01:58:54.4504176Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-17T01:58:54.4505374Z Aug 17 01:58:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-17T01:58:54.4506142Z Aug 17 01:58:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-17T01:58:54.4506962Z Aug 17 01:58:54 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-08-17T01:58:54.4507840Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-17T01:58:54.4509177Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-17T01:58:54.4510181Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-17T01:58:54.4511081Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-17T01:58:54.4511926Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-17T01:58:54.4512761Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-17T01:58:54.4513547Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-17T01:58:54.4514429Z Aug 17 01:58:54 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-17T01:58:54.4515347Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-17T01:58:54.4516201Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4517026Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-17T01:58:54.4517862Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-17T01:58:54.4518883Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-17T01:58:54.4519704Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-17T01:58:54.4520533Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4521367Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-17T01:58:54.4522126Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-17T01:58:54.4522884Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-17T01:58:54.4523709Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4524617Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-17T01:58:54.4525381Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-17T01:58:54.4526265Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2022-08-17T01:58:54.4527227Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-08-17T01:58:54.4528108Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-08-17T01:58:54.4529042Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-08-17T01:58:54.4529898Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-08-17T01:58:54.4530671Z Aug 17 01:58:54 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-17T01:58:54.4531347Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-17T01:58:54.4532097Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-17T01:58:54.4532773Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-17T01:58:54.4533431Z Aug 17 01:58:54 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-17T01:58:54.4534102Z Aug 17 01:58:54 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-08-17T01:58:54.4535174Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-08-17T01:58:54.4535863Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-17T01:58:54.4536535Z Aug 17 01:58:54 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-17T01:58:54.4537218Z Aug 17 01:58:54 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-17T01:58:54.4537891Z Aug 17 01:58:54 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-17T01:58:54.4538645Z Aug 17 01:58:54 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-17T01:58:54.4539354Z Aug 17 01:58:54 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-17T01:58:54.4540036Z Aug 17 01:58:54 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-17T01:58:54.4540676Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-17T01:58:54.4541450Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-17T01:58:54.4542136Z Aug 17 01:58:54 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-17T01:58:54.4542815Z Aug 17 01:58:54 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-17T01:58:54.4543487Z Aug 17 01:58:54 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-17T01:58:54.4544183Z Aug 17 01:58:54 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-17T01:58:54.4545092Z Aug 17 01:58:54 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-17T01:58:54.4545773Z Aug 17 01:58:54 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-17T01:58:54.4546427Z Aug 17 01:58:54 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-17T01:58:54.4547180Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-08-17T01:58:54.4548007Z Aug 17 01:58:54 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-08-17T01:58:54.4548924Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-17T01:58:54.4549757Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4550826Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-17T01:58:54.4551571Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-17T01:58:54.4552332Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-17T01:58:54.4553165Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4553969Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-17T01:58:54.4555019Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-17T01:58:54.4555796Z Aug 17 01:58:54 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2022-08-17T01:58:54.4556597Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2022-08-17T01:58:54.4557528Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-17T01:58:54.4558352Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4559262Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-17T01:58:54.4560019Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-17T01:58:54.4560778Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-17T01:58:54.4561602Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4562401Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-17T01:58:54.4563159Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-17T01:58:54.4563810Z Aug 17 01:58:54 	at java.util.ArrayList.forEach(ArrayList.java:1259)
2022-08-17T01:58:54.4564797Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
2022-08-17T01:58:54.4565740Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-17T01:58:54.4566558Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4567384Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-17T01:58:54.4568138Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-17T01:58:54.4568968Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-17T01:58:54.4569770Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-17T01:58:54.4570573Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-17T01:58:54.4571344Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-17T01:58:54.4572234Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
2022-08-17T01:58:54.4573180Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
2022-08-17T01:58:54.4574028Z Aug 17 01:58:54 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
2022-08-17T01:58:54.4575150Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-17T01:58:54.4575968Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-17T01:58:54.4576813Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-17T01:58:54.4577692Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-17T01:58:54.4578720Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-17T01:58:54.4579499Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-17T01:58:54.4580215Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-17T01:58:54.4581008Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-17T01:58:54.4581848Z Aug 17 01:58:54 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-17T01:58:54.4582638Z Aug 17 01:58:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-17T01:58:54.4583461Z Aug 17 01:58:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-17T01:58:54.4584394Z Aug 17 01:58:54 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-17T01:58:54.4585246Z Aug 17 01:58:54 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-17T01:58:54.4585968Z Aug 17 01:58:54 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-17T01:58:54.4586645Z Aug 17 01:58:54 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-17T01:58:54.4587406Z Aug 17 01:58:54 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-17T01:58:54.4588007Z Aug 17 01:58:54 Caused by: java.lang.RuntimeException: Failed to fetch next result
2022-08-17T01:58:54.4588858Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2022-08-17T01:58:54.4589748Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2022-08-17T01:58:54.4590639Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.compareWithExactlyOnceSemantic(CollectIteratorAssert.java:116)
2022-08-17T01:58:54.4591555Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.utils.CollectIteratorAssert.matchesRecordsFromSource(CollectIteratorAssert.java:71)
2022-08-17T01:58:54.4592474Z Aug 17 01:58:54 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.lambda$checkResultWithSemantic$3(SourceTestSuiteBase.java:741)
2022-08-17T01:58:54.4593308Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
2022-08-17T01:58:54.4594015Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1596)
2022-08-17T01:58:54.4594913Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-17T01:58:54.4595582Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-17T01:58:54.4596247Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-17T01:58:54.4596912Z Aug 17 01:58:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-17T01:58:54.4597531Z Aug 17 01:58:54 Caused by: java.io.IOException: Failed to fetch job execution result
2022-08-17T01:58:54.4598270Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:184)
2022-08-17T01:58:54.4599199Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:121)
2022-08-17T01:58:54.4600068Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2022-08-17T01:58:54.4600798Z Aug 17 01:58:54 	... 10 more
2022-08-17T01:58:54.4601382Z Aug 17 01:58:54 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-08-17T01:58:54.4602126Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-08-17T01:58:54.4602796Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-08-17T01:58:54.4603567Z Aug 17 01:58:54 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:182)
2022-08-17T01:58:54.4604198Z Aug 17 01:58:54 	... 12 more
2022-08-17T01:58:54.4604783Z Aug 17 01:58:54 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-08-17T01:58:54.4605466Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-08-17T01:58:54.4606280Z Aug 17 01:58:54 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
2022-08-17T01:58:54.4607071Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2022-08-17T01:58:54.4607770Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-08-17T01:58:54.4608462Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4609214Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-17T01:58:54.4610055Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:259)
2022-08-17T01:58:54.4610826Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-17T01:58:54.4611558Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-17T01:58:54.4612285Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4612973Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-17T01:58:54.4613898Z Aug 17 01:58:54 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2022-08-17T01:58:54.4614875Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2022-08-17T01:58:54.4615725Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-17T01:58:54.4616627Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2022-08-17T01:58:54.4617628Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2022-08-17T01:58:54.4618562Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2022-08-17T01:58:54.4619292Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4619969Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2022-08-17T01:58:54.4620699Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2022-08-17T01:58:54.4621372Z Aug 17 01:58:54 	at akka.dispatch.OnComplete.internal(Future.scala:300)
2022-08-17T01:58:54.4621946Z Aug 17 01:58:54 	at akka.dispatch.OnComplete.internal(Future.scala:297)
2022-08-17T01:58:54.4622532Z Aug 17 01:58:54 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2022-08-17T01:58:54.4623130Z Aug 17 01:58:54 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2022-08-17T01:58:54.4623721Z Aug 17 01:58:54 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2022-08-17T01:58:54.4624832Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2022-08-17T01:58:54.4625616Z Aug 17 01:58:54 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2022-08-17T01:58:54.4626330Z Aug 17 01:58:54 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2022-08-17T01:58:54.4627084Z Aug 17 01:58:54 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2022-08-17T01:58:54.4627820Z Aug 17 01:58:54 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2022-08-17T01:58:54.4628461Z Aug 17 01:58:54 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2022-08-17T01:58:54.4629182Z Aug 17 01:58:54 	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:118)
2022-08-17T01:58:54.4629852Z Aug 17 01:58:54 	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:1144)
2022-08-17T01:58:54.4630482Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-17T01:58:54.4631039Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-17T01:58:54.4631621Z Aug 17 01:58:54 	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:540)
2022-08-17T01:58:54.4632226Z Aug 17 01:58:54 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-17T01:58:54.4632786Z Aug 17 01:58:54 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-17T01:58:54.4633467Z Aug 17 01:58:54 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-17T01:58:54.4634031Z Aug 17 01:58:54 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-17T01:58:54.4634837Z Aug 17 01:58:54 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-17T01:58:54.4635289Z Aug 17 01:58:54 	... 4 more
2022-08-17T01:58:54.4635804Z Aug 17 01:58:54 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-08-17T01:58:54.4636634Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2022-08-17T01:58:54.4637592Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2022-08-17T01:58:54.4638471Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
2022-08-17T01:58:54.4639331Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
2022-08-17T01:58:54.4640374Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
2022-08-17T01:58:54.4641202Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
2022-08-17T01:58:54.4642110Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
2022-08-17T01:58:54.4643090Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1536)
2022-08-17T01:58:54.4643943Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1118)
2022-08-17T01:58:54.4644765Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1058)
2022-08-17T01:58:54.4645485Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:897)
2022-08-17T01:58:54.4646221Z Aug 17 01:58:54 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:466)
2022-08-17T01:58:54.4647057Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42)
2022-08-17T01:58:54.4648066Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:658)
2022-08-17T01:58:54.4649023Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:530)
2022-08-17T01:58:54.4649856Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-08-17T01:58:54.4650558Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-08-17T01:58:54.4651277Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2022-08-17T01:58:54.4651999Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2022-08-17T01:58:54.4652760Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
2022-08-17T01:58:54.4653612Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:186)
2022-08-17T01:58:54.4654616Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408)
2022-08-17T01:58:54.4655540Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:397)
2022-08-17T01:58:54.4656515Z Aug 17 01:58:54 	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:859)
2022-08-17T01:58:54.4657185Z Aug 17 01:58:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-17T01:58:54.4657812Z Aug 17 01:58:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-17T01:58:54.4658593Z Aug 17 01:58:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-17T01:58:54.4659250Z Aug 17 01:58:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-17T01:58:54.4659915Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:296)
2022-08-17T01:58:54.4660742Z Aug 17 01:58:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-17T01:58:54.4661546Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:295)
2022-08-17T01:58:54.4662302Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2022-08-17T01:58:54.4663071Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2022-08-17T01:58:54.4663830Z Aug 17 01:58:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2022-08-17T01:58:54.4664771Z Aug 17 01:58:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-17T01:58:54.4665452Z Aug 17 01:58:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-17T01:58:54.4666073Z Aug 17 01:58:54 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-17T01:58:54.4666690Z Aug 17 01:58:54 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-17T01:58:54.4667320Z Aug 17 01:58:54 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-17T01:58:54.4667969Z Aug 17 01:58:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-17T01:58:54.4668696Z Aug 17 01:58:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-17T01:58:54.4669322Z Aug 17 01:58:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-17T01:58:54.4669921Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-17T01:58:54.4670574Z Aug 17 01:58:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-17T01:58:54.4671167Z Aug 17 01:58:54 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-17T01:58:54.4671658Z Aug 17 01:58:54 	... 9 more
2022-08-17T01:58:54.4672410Z Aug 17 01:58:54 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-08-17T01:58:54.4673402Z Aug 17 01:58:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:588)
2022-08-17T01:58:54.4673967Z Aug 17 01:58:54 	... 39 more
2022-08-17T01:58:54.4674726Z Aug 17 01:58:54 Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
2022-08-17T01:58:54.4675584Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2022-08-17T01:58:54.4676316Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2022-08-17T01:58:54.4677019Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2022-08-17T01:58:54.4677716Z Aug 17 01:58:54 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2022-08-17T01:58:54.4678246Z Aug 17 01:58:54 	... 37 more
2022-08-17T01:58:54.4678878Z Aug 17 01:58:54 Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40085&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=30373
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 12 11:22:50 UTC 2022,,,,,,,,,,"0|z17tcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 03:26;hxbks2ks;[~syhily] Could you help take a look? Thx.;;;","26/Aug/22 03:31;syhily;[~renqs] I have checked the log. It seems like the root cause is {{org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources}}.

Can you help me on this? I don't know why we meet this exception.

{code:java}
01:55:15,929 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger checkpoint for job 87a23d1ed0c3bcc081bc99e584c15b6e since Checkpoint triggering task Source: Tested Source (1/4) of job 87a23d1ed0c3bcc081bc99e584c15b6e is not being executed at the moment. Aborting checkpoint. Failure reason: Not all required tasks are currently running..
01:55:15,957 [flink-akka.actor.default-dispatcher-17] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 87a23d1ed0c3bcc081bc99e584c15b6e. Free slots: 0
01:55:15,958 [flink-akka.actor.default-dispatcher-17] WARN  org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge [] - Could not acquire the minimum required resources, failing slot requests. Acquired: [ResourceRequirement{resourceProfile=ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=13.333mb (13981013 bytes), networkMemory=10.667mb (11184810 bytes)}, numberOfRequiredSlots=2}]. Current slot pool status: Registered TMs: 1, registered slots: 2 free slots: 0
01:55:15,962 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (2/4) (b0e672230eb11a915c4a9e2107ae924f) switched from SCHEDULED to FAILED on [unassigned resource].
org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
01:55:15,968 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution b0e672230eb11a915c4a9e2107ae924f.
01:55:15,969 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task bc764cd8ddf7a0cff126f51c16239658_1.
01:55:15,969 [SourceCoordinator-Source: Tested Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Removing registered reader after failure for subtask 1 of source Source: Tested Source.
01:55:15,970 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 5 tasks should be restarted to recover the failed task bc764cd8ddf7a0cff126f51c16239658_1. 
01:55:15,973 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 4781 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1660701315973 for job 0769e011a959775468a874599b9a6f09.
01:55:15,973 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Restart Test (87a23d1ed0c3bcc081bc99e584c15b6e) switched from state RUNNING to FAILING.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1536) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1118) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1058) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:897) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:466) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:658) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:530) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:186) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:397) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:859) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:296) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:295) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:588) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 39 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_292]
	... 37 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
01:55:15,977 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (1/4) (f040702ad41cd88fae0f96b06e514cc1) switched from SCHEDULED to CANCELING.
01:55:15,977 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (1/4) (f040702ad41cd88fae0f96b06e514cc1) switched from CANCELING to CANCELED.
01:55:15,977 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution f040702ad41cd88fae0f96b06e514cc1.
01:55:15,978 [jobmanager-io-thread-12] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 4781 for job 0769e011a959775468a874599b9a6f09 (2807 bytes, checkpointDuration=5 ms, finalizationTime=0 ms).
01:55:15,978 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution f040702ad41cd88fae0f96b06e514cc1.
01:55:15,978 [SourceCoordinator-Source: Tested Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 4781 as completed for source Source: Tested Source.
01:55:15,978 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (3/4) (cfccc425f53da834665067f00def3ca2) switched from SCHEDULED to CANCELING.
01:55:15,978 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (3/4) (cfccc425f53da834665067f00def3ca2) switched from CANCELING to CANCELED.
01:55:15,978 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution cfccc425f53da834665067f00def3ca2.
01:55:15,978 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution cfccc425f53da834665067f00def3ca2.
01:55:15,978 [flink-akka.actor.default-dispatcher-29] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 87a23d1ed0c3bcc081bc99e584c15b6e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=3}]
01:55:15,979 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (4/4) (2d46baa77ad010d5f7ba937fdba6333a) switched from SCHEDULED to CANCELING.
01:55:15,979 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Tested Source (4/4) (2d46baa77ad010d5f7ba937fdba6333a) switched from CANCELING to CANCELED.
01:55:15,979 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 2d46baa77ad010d5f7ba937fdba6333a.
01:55:15,979 [flink-akka.actor.default-dispatcher-29] WARN  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Could not fulfill resource requirements of job 87a23d1ed0c3bcc081bc99e584c15b6e. Free slots: 0
01:55:15,979 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Data stream collect sink (1/1) (1b7bfdf3bef9c6cf69f7a0ca81747b30) switched from SCHEDULED to CANCELING.
01:55:15,979 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Sink: Data stream collect sink (1/1) (1b7bfdf3bef9c6cf69f7a0ca81747b30) switched from CANCELING to CANCELED.
01:55:15,979 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 1b7bfdf3bef9c6cf69f7a0ca81747b30.
01:55:15,979 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 87a23d1ed0c3bcc081bc99e584c15b6e: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=2}]
01:55:15,982 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Restart Test (87a23d1ed0c3bcc081bc99e584c15b6e) switched from state FAILING to FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1536) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1118) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1058) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:897) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:466) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:658) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:530) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:186) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:397) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:859) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:296) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:295) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163) ~[flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_462109a1-47d8-42e1-a010-a87419fdc33d.jar:1.15-SNAPSHOT]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:588) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
	... 39 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_292]
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_292]
	... 37 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
01:55:15,982 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 87a23d1ed0c3bcc081bc99e584c15b6e.
01:55:15,989 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Clearing resource requirements of job 87a23d1ed0c3bcc081bc99e584c15b6e
01:55:15,995 [flink-akka.actor.default-dispatcher-29] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 87a23d1ed0c3bcc081bc99e584c15b6e reached terminal state FAILED.
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1536)
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1118)
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1058)
	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:897)
	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:466)
	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:42)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:658)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignAllResourcesAndRegisterProducedPartitions$6(DefaultScheduler.java:530)
	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:186)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:408)
	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:397)
	at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:859)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$0(AkkaRpcActor.java:296)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:295)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResource$8(DefaultScheduler.java:588)
	... 39 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	... 37 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
01:55:15,996 [mini-cluster-io-thread-2] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 87a23d1ed0c3bcc081bc99e584c15b6e has been registered for cleanup in the JobResultStore after reaching a terminal state.
01:55:15,996 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job 'Restart Test' (87a23d1ed0c3bcc081bc99e584c15b6e).
01:55:15,998 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [87c97990e80f8864a5118945e53d9bd2].
01:55:15,998 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool [] - Releasing slot [ae2fd247ef273b8c211b7a5be489b735].
01:55:15,998 [flink-akka.actor.default-dispatcher-17] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 0c328f02b8c145b207e5cec7a3f6e6e3: Stopping JobMaster for job 'Restart Test' (87a23d1ed0c3bcc081bc99e584c15b6e).
01:55:15,997 [           Thread-12] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Closing SourceCoordinator for source Source: Tested Source.
01:55:16,000 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager b89d246bb6af7e8ff698a66e2d6940df@akka.tcp://flink@localhost:39292/user/rpc/jobmanager_6 for job 87a23d1ed0c3bcc081bc99e584c15b6e from the resource manager.
01:55:16,002 [mini-cluster-io-thread-1] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - JobManager for job 87a23d1ed0c3bcc081bc99e584c15b6e with leader id b89d246bb6af7e8ff698a66e2d6940df lost leadership.
01:55:16,002 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:1, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=13.333mb (13981013 bytes), networkMemory=10.667mb (11184810 bytes)}, allocationId: 87c97990e80f8864a5118945e53d9bd2, jobId: 87a23d1ed0c3bcc081bc99e584c15b6e).
01:55:16,003 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:2, state:ACTIVE, resource profile: ResourceProfile{taskHeapMemory=170.667gb (183251937962 bytes), taskOffHeapMemory=170.667gb (183251937962 bytes), managedMemory=13.333mb (13981013 bytes), networkMemory=10.667mb (11184810 bytes)}, allocationId: ae2fd247ef273b8c211b7a5be489b735, jobId: 87a23d1ed0c3bcc081bc99e584c15b6e).
01:55:16,003 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job 87a23d1ed0c3bcc081bc99e584c15b6e from job leader monitoring.
01:55:16,003 [flink-akka.actor.default-dispatcher-27] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job 87a23d1ed0c3bcc081bc99e584c15b6e.
01:55:16,054 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 4782 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1660701316024 for job 0769e011a959775468a874599b9a6f09.
01:55:16,054 [           Thread-12] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Source coordinator for source Source: Tested Source closed.
01:55:16,055 [                main] ERROR org.apache.flink.util.TestLoggerExtension                    [] - 
{code}
;;;","26/Aug/22 12:04;syhily;[~hxbks2ks] The root cause should be the bug on testing tools. Maybe you could ask others for help? ;;;","29/Aug/22 01:47;hxb;[~syhily] Thanks a lot for the investigation. [~renqs] Could you help take a look? Thx.;;;","12/Oct/22 06:00;syhily;[~hxb] I think we can close this issue if no new failed builds occur.;;;","12/Oct/22 11:22;hxb;[~syhily] Yes. Make sense. If the failed instance occur again, we can reopen the JIRA.;;;",,,,,,,,,,,,,,,
Parquet row type reader should not return null value when some child fields is null ,FLINK-29005,13477149,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yuchuanchen,lsy,lsy,17/Aug/22 02:48,31/Aug/22 04:09,04/Jun/24 20:41,31/Aug/22 04:09,1.16.0,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 31 04:09:39 UTC 2022,,,,,,,,,,"0|z17tbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 03:55;yuchuanchen;hi [~lsy] Could you assign this to me?;;;","17/Aug/22 04:10;lsy;cc [~jark] ;;;","30/Aug/22 07:45;martijnvisser;[~yuchuanchen] I've assigned it to you;;;","31/Aug/22 04:09;jark;Fixed in master: 00f585234f8db8fe1e2bfec5c6c323ca99d9b775;;;",,,,,,,,,,,,,,,,,
pyflink supports specifying index-url when installing third-party dependencies,FLINK-29004,13477140,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,long jiang,long jiang,17/Aug/22 01:27,23/Aug/22 08:08,04/Jun/24 20:41,17/Aug/22 01:50,,,,,,,,,API / Python,,,,,,,0,pull-request-available,,,,,,"When we use pyflink, if there is a third-party dependency, when using pip to install the package, it is retrieved and obtained from pypi.python.org by default, which is greatly affected by the network, slow download speed and easy to cause connection timeout. At this point, we think we can specify the index-url.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 01:50:29 UTC 2022,,,,,,,,,,"0|z17t9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 01:50;hxbks2ks;Hi [~long jiang], pyflink support specifying `requirements_cache` to install dependencies in the environment without network. For the details, you can refer to https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/python/dependency_management/#requirementstxt;;;",,,,,,,,,,,,,,,,,,,,
sqlServerDialect should support limit clause,FLINK-29003,13477127,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,16/Aug/22 21:34,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,1.20.0,,,,Connectors / JDBC,,,,,,,0,,,,,,,"SQL Server does not support limit natively, it supports by OFFSET syntax and requires ORDER BY to be valid, with the current structure.

This should not be only related to dialect, but also need to change or introduce a new {{pushdownXXX}} in the relevant pushdown mechanism.

Please see [https://github.com/apache/flink/pull/20235/files#r917362437] for details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-16 21:34:05.0,,,,,,,,,,"0|z17t6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Datadog reporter 'tags' option,FLINK-29002,13477117,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Aug/22 19:38,17/Aug/22 22:00,04/Jun/24 20:41,17/Aug/22 21:59,,,,,1.16.0,,,,Runtime / Metrics,,,,,,,0,,,,,,,The option us subsumed by the generic {{scope.variables.additional}} option.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 21:59:55 UTC 2022,,,,,,,,,,"0|z17t48:",9223372036854775807,The 'tags' option from the DatadogReporter has been deprecated in favor of the generic 'scope.variables.additional' option.,,,,,,,,,,,,,,,,,,,"17/Aug/22 21:59;chesnay;master: 3e6f47c9d3623c240bbb86845893f9a022dde03d;;;",,,,,,,,,,,,,,,,,,,,
Migrate datadog reporter to v2 metric submission API,FLINK-29001,13477062,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,chesnay,chesnay,16/Aug/22 12:58,11/Mar/24 12:44,04/Jun/24 20:41,,,,,,1.20.0,,,,Runtime / Metrics,,,,,,,0,,,,,,,"The current datadog API that we're using to submit metrics is deprecated.
https://docs.datadoghq.com/api/latest/metrics/#submit-metrics
Changes that I found so far:
* the {{host}} is part of the {{resources}} field
* metric types are now mapped to numbers (0-3)
* values must be doubles

We can optionally look into leveraging the datadog java client; shouldn't be difficult but from a quick experiment I couldn't figure out how to replicate the proxy host/port functionality.
https://github.com/DataDog/datadog-api-client-java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-16 12:58:02.0,,,,,,,,,,"0|z17ss0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support python UDF in the SQL Gateway,FLINK-29000,13477048,13478114,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,hxb,fsk119,fsk119,16/Aug/22 11:47,19/Jan/23 14:50,04/Jun/24 20:41,19/Jan/23 14:50,1.17.0,,,,1.17.0,,,,API / Python,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,"Currently Flink SQL Client supports python UDF, the Gateway should also support this feature if the SQL Client is able to submit SQL to the Gateway.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 19 14:50:06 UTC 2023,,,,,,,,,,"0|z17sow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 14:50;fsk119;Merged into master: 7b69e93b301445ca70e1bf2d87de49b13381870b;;;",,,,,,,,,,,,,,,,,,,,
OperatorCoordinatorHolder.checkpointCoordinatorInternal cannot mark for checkpoint ,FLINK-28999,13477037,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,hxbks2ks,hxbks2ks,16/Aug/22 10:53,16/Aug/22 11:06,04/Jun/24 20:41,16/Aug/22 11:06,1.16.0,,,,1.16.0,,,,Runtime / Checkpointing,,,,,,,0,test-stability,,,,,,"
{code:java}
2022-08-16T09:02:23.8660487Z Aug 16 09:02:23 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Trigger checkpoint failure.
2022-08-16T09:02:23.8661381Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-08-16T09:02:23.8662108Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2022-08-16T09:02:23.8663272Z Aug 16 09:02:23 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.restartFromSavepoint(SourceTestSuiteBase.java:343)
2022-08-16T09:02:23.8667137Z Aug 16 09:02:23 	at org.apache.flink.connector.testframe.testsuites.SourceTestSuiteBase.testSavepoint(SourceTestSuiteBase.java:236)
2022-08-16T09:02:23.8668444Z Aug 16 09:02:23 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-16T09:02:23.8669103Z Aug 16 09:02:23 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-16T09:02:23.8669997Z Aug 16 09:02:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-16T09:02:23.8671097Z Aug 16 09:02:23 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-16T09:02:23.8672030Z Aug 16 09:02:23 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-16T09:02:23.8672772Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-16T09:02:23.8673625Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-16T09:02:23.8674477Z Aug 16 09:02:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-16T09:02:23.8675549Z Aug 16 09:02:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-08-16T09:02:23.8676389Z Aug 16 09:02:23 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:92)
2022-08-16T09:02:23.8677556Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-16T09:02:23.8678483Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-16T09:02:23.8679356Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-16T09:02:23.8680249Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-16T09:02:23.8681482Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-16T09:02:23.8682365Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-16T09:02:23.8683409Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-16T09:02:23.8684177Z Aug 16 09:02:23 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-16T09:02:23.8684996Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-08-16T09:02:23.8685857Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8686699Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-08-16T09:02:23.8687639Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-08-16T09:02:23.8688458Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-08-16T09:02:23.8689301Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-16T09:02:23.8690233Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8691703Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8692970Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8694170Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8695490Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8697088Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8698124Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8699258Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8700344Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
2022-08-16T09:02:23.8701511Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
2022-08-16T09:02:23.8702766Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
2022-08-16T09:02:23.8704174Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:139)
2022-08-16T09:02:23.8705582Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:107)
2022-08-16T09:02:23.8706870Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-16T09:02:23.8707882Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-16T09:02:23.8708557Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
2022-08-16T09:02:23.8709200Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
2022-08-16T09:02:23.8709858Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
2022-08-16T09:02:23.8710514Z Aug 16 09:02:23 	at java.util.LinkedList$LLSpliterator.forEachRemaining(LinkedList.java:1235)
2022-08-16T09:02:23.8711547Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:647)
2022-08-16T09:02:23.8712593Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-16T09:02:23.8713777Z Aug 16 09:02:23 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-16T09:02:23.8714679Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-16T09:02:23.8715352Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-16T09:02:23.8716031Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-16T09:02:23.8716728Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-16T09:02:23.8717746Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-16T09:02:23.8718415Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-16T09:02:23.8719084Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
2022-08-16T09:02:23.8719949Z Aug 16 09:02:23 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
2022-08-16T09:02:23.8720750Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-16T09:02:23.8721724Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-16T09:02:23.8722860Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
2022-08-16T09:02:23.8723832Z Aug 16 09:02:23 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
2022-08-16T09:02:23.8724526Z Aug 16 09:02:23 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-16T09:02:23.8725164Z Aug 16 09:02:23 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
2022-08-16T09:02:23.8725923Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:107)
2022-08-16T09:02:23.8726970Z Aug 16 09:02:23 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:42)
2022-08-16T09:02:23.8728221Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-08-16T09:02:23.8729045Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8730141Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8731495Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8732605Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8733444Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8734303Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8735054Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8735974Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8737088Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-08-16T09:02:23.8738540Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-08-16T09:02:23.8739687Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-16T09:02:23.8740778Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8741589Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8742476Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8743624Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8744830Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8746096Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8747508Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8749049Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8750886Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-16T09:02:23.8752423Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-16T09:02:23.8753846Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8755201Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-16T09:02:23.8756198Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-16T09:02:23.8756954Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-16T09:02:23.8757972Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-16T09:02:23.8758769Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-16T09:02:23.8759538Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-16T09:02:23.8760457Z Aug 16 09:02:23 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-16T09:02:23.8761708Z Aug 16 09:02:23 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-16T09:02:23.8762365Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-16T09:02:23.8762997Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-16T09:02:23.8763654Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-16T09:02:23.8764657Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-16T09:02:23.8765392Z Aug 16 09:02:23 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Trigger checkpoint failure.
2022-08-16T09:02:23.8766097Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:550)
2022-08-16T09:02:23.8767045Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2096)
2022-08-16T09:02:23.8768208Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2083)
2022-08-16T09:02:23.8769060Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:956)
2022-08-16T09:02:23.8769940Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.onTriggerFailure(CheckpointCoordinator.java:929)
2022-08-16T09:02:23.8770864Z Aug 16 09:02:23 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$startTriggeringCheckpoint$7(CheckpointCoordinator.java:657)
2022-08-16T09:02:23.8771863Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2022-08-16T09:02:23.8773015Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2022-08-16T09:02:23.8774173Z Aug 16 09:02:23 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2022-08-16T09:02:23.8775251Z Aug 16 09:02:23 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-08-16T09:02:23.8776338Z Aug 16 09:02:23 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-16T09:02:23.8777649Z Aug 16 09:02:23 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2022-08-16T09:02:23.8779006Z Aug 16 09:02:23 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2022-08-16T09:02:23.8780292Z Aug 16 09:02:23 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-16T09:02:23.8781377Z Aug 16 09:02:23 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-16T09:02:23.8782051Z Aug 16 09:02:23 	at java.lang.Thread.run(Thread.java:748)
2022-08-16T09:02:23.8782658Z Aug 16 09:02:23 Caused by: java.lang.IllegalStateException: Cannot mark for checkpoint 211, already marked for checkpoint 210
2022-08-16T09:02:23.8783647Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.markForCheckpoint(SubtaskGatewayImpl.java:185)
2022-08-16T09:02:23.8784578Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinatorInternal$6(OperatorCoordinatorHolder.java:328)
2022-08-16T09:02:23.8785350Z Aug 16 09:02:23 	at java.util.HashMap.forEach(HashMap.java:1289)
2022-08-16T09:02:23.8786101Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.checkpointCoordinatorInternal(OperatorCoordinatorHolder.java:327)
2022-08-16T09:02:23.8787056Z Aug 16 09:02:23 	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinator$0(OperatorCoordinatorHolder.java:243)
2022-08-16T09:02:23.8788361Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453)
2022-08-16T09:02:23.8789659Z Aug 16 09:02:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2022-08-16T09:02:23.8791066Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453)
2022-08-16T09:02:23.8792253Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218)
2022-08-16T09:02:23.8793351Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-16T09:02:23.8794385Z Aug 16 09:02:23 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-16T09:02:23.8795385Z Aug 16 09:02:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-16T09:02:23.8796185Z Aug 16 09:02:23 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-16T09:02:23.8797442Z Aug 16 09:02:23 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-16T09:02:23.8798174Z Aug 16 09:02:23 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-16T09:02:23.8798819Z Aug 16 09:02:23 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-16T09:02:23.8799547Z Aug 16 09:02:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-16T09:02:23.8800690Z Aug 16 09:02:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T09:02:23.8801895Z Aug 16 09:02:23 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T09:02:23.8802879Z Aug 16 09:02:23 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-16T09:02:23.8803628Z Aug 16 09:02:23 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-16T09:02:23.8804614Z Aug 16 09:02:23 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-16T09:02:23.8805553Z Aug 16 09:02:23 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-16T09:02:23.8806571Z Aug 16 09:02:23 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-16T09:02:23.8807544Z Aug 16 09:02:23 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-16T09:02:23.8808366Z Aug 16 09:02:23 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-16T09:02:23.8809211Z Aug 16 09:02:23 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-16T09:02:23.8810075Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-16T09:02:23.8810819Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-16T09:02:23.8811484Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-16T09:02:23.8812143Z Aug 16 09:02:23 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40044&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28941,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 11:05:04 UTC 2022,,,,,,,,,,"0|z17smg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 10:55;hxbks2ks;[~yunfengzhou] Could you help take a look? Whether it is related to https://issues.apache.org/jira/browse/FLINK-28606?;;;","16/Aug/22 11:03;yunfengzhou;This problem is revealed and reported by the changes in FLINK-28606, but it requires changing the implementation of Flink's general checkpoint mechanism to solve this problem. I created a similar ticket (FLINK-28941) which includes my previous analysis of the cause of this problem.;;;","16/Aug/22 11:05;hxbks2ks;Thanks a lot for the investigation. [~yunfengzhou];;;",,,,,,,,,,,,,,,,,,
"Translate ""Fine-Grained Resource Management"" page into Chinese",FLINK-28998,13477023,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,MaoKing,MaoKing,MaoKing,16/Aug/22 09:24,29/Sep/22 08:04,04/Jun/24 20:41,29/Sep/22 08:04,1.15.1,,,,1.17.0,,,,,,,,,,,0,pull-request-available,,,,,,"Translate ""Fine-Grained Resource Management"" page into Chinese:https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/finegrained_resource/
The markdown file location: flink/docs/content.zh/docs/deployment/finegrained_resource.md, most of them have been translated, but some part has not been translated into Chinese. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 29 08:04:40 UTC 2022,,,,,,,,,,"0|z17sjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 09:28;MaoKing;[~yunta] [~jark] Hi, I am interested in this new feature called 'Fine-Grained Resource Management'. I think I could like to translate this page. Could you assign it to me ? I will complete the translation this week. Thank you very much! ;;;","16/Aug/22 09:35;yunta;Already assigned to you, please go ahead.;;;","29/Sep/22 08:04;martijnvisser;Fixed in master: c321c5c70f5211889a0d0694c2c398d548e14dbd;;;",,,,,,,,,,,,,,,,,,
Add a switch to datadog reporter to use logical scope,FLINK-28997,13477016,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Aug/22 09:14,17/Aug/22 22:00,04/Jun/24 20:41,17/Aug/22 22:00,,,,,1.16.0,,,,Runtime / Metrics,,,,,,,0,pull-request-available,,,,,,"For legacy reason the datadog reporter uses the metric identifier, while for tab-based reporters it usually makes more sense to use the logical scope.
Add a switch to control this behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 22:00:58 UTC 2022,,,,,,,,,,"0|z17shs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 22:00;chesnay;master: 7e7fd76078e6499a50edb2c69fff5088326390b6;;;",,,,,,,,,,,,,,,,,,,,
Move parameter parsing into Datadog reporter factory,FLINK-28996,13477014,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,16/Aug/22 09:13,17/Aug/22 22:02,04/Jun/24 20:41,17/Aug/22 22:02,,,,,1.16.0,,,,Runtime / Metrics,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 22:02:48 UTC 2022,,,,,,,,,,"0|z17shc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 22:02;chesnay;master: 56a7c847df16b70f321d5445e447ca4dc0d8bba4;;;",,,,,,,,,,,,,,,,,,,,
Fix NPE problem: createRowData will return null while encounter hive default partition,FLINK-28995,13477013,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,337361684@qq.com,337361684@qq.com,16/Aug/22 09:09,23/Aug/22 09:50,04/Jun/24 20:41,23/Aug/22 09:50,1.16.0,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"createRowData will return null while encounter hive default partition, which will make {{‘data.contains(partitionRow)’ throw NPE.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 23 09:50:49 UTC 2022,,,,,,,,,,"0|z17sh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 09:50;godfreyhe;Fixed in master: 2322791284e4e09895d8eb1bc45f4fad8287fcc0;;;",,,,,,,,,,,,,,,,,,,,
Enable withCredentials for Flink UI,FLINK-28994,13477012,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,airblader,airblader,airblader,16/Aug/22 08:59,16/Aug/22 13:22,04/Jun/24 20:41,16/Aug/22 13:22,,,,,1.15.2,1.16.0,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,,,"Some environments require cookies to authenticate the Flink UI. By enabling the withCredentials flag, Angular will send cookies along with the request.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 13:22:49 UTC 2022,,,,,,,,,,"0|z17sgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 13:22;twalthr;Fixed in master: bd274ca8a1b8249aa8a6a25514246ff79a89203b
Fixed in 1.15: 44c6496e95426a186dba7f775d3f65df3a00c979;;;",,,,,,,,,,,,,,,,,,,,
Fix adjusting join cost for dpp query pattern error,FLINK-28993,13477010,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,16/Aug/22 08:56,24/Aug/22 02:20,04/Jun/24 20:41,24/Aug/22 02:20,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"Now, adjusting join cost for dpp query pattern have two errors:

1. For these table which without table stats, we haven't plus the

dynamicPartitionPruningFactor.

2. For method #DynamicPartitionPruningUtils$visitFactSide, we have not judge whether candiateFields contain partition keys.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 24 02:20:30 UTC 2022,,,,,,,,,,"0|z17sgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 02:20;godfreyhe;Fixed in master: 1a0f591a59b59f0c6ce71f5af9e0660293c33fc1;;;",,,,,,,,,,,,,,,,,,,,
Change Ndv takes the max value instead of sum of all partitions when getting partition table column stats,FLINK-28992,13477006,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,16/Aug/22 08:42,22/Aug/22 02:24,04/Jun/24 20:41,22/Aug/22 02:24,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"Now, when we obtain column statistics Ndv of the partition table, we use the method of taking sum of Ndv of all partitions, which will cause the obtained stats to be far from the real stats. So now we take the max value instead of sum of all partitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 22 02:24:02 UTC 2022,,,,,,,,,,"0|z17sfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 02:24;godfreyhe;Fixed in master: 10e4c82902164f2651d73f018393f930dcef55a8;;;",,,,,,,,,,,,,,,,,,,,
Add documentation for lookup table caching feature,FLINK-28991,13477003,13470246,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,renqs,renqs,renqs,16/Aug/22 08:31,07/Mar/23 19:40,04/Jun/24 20:41,,1.16.0,,,,,,,,Documentation,,,,,,,0,,,,,,,We need a documentation to describe how to implement a lookup table based on the new caching framework,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 07 19:40:01 UTC 2023,,,,,,,,,,"0|z17sew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/22 07:12;mapohl;[~renqs] what's the status of this issue? It feels like we shouldn't have closed the parent issue if there's still an open subtask.;;;","07/Mar/23 19:40;elkhand;[~renqs] , +1 on documentation plan which describes how to benefit from caching in FlinkSQL.;;;",,,,,,,,,,,,,,,,,,,
Fix BatchPhysicalDynamicFilteringDataCollector with empty output type,FLINK-28990,13476998,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,16/Aug/22 08:15,19/Aug/22 03:36,04/Jun/24 20:41,19/Aug/22 03:36,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"When dpp fact side have calc node, and partition key index was changed in calc node, 
{code:java}
BatchPhysicalDynamicFilteringDataCollector  {code}
will be set with a empty output type, which will throw exception in 
{code:java}
HiveSourceDynamicFileEnumerator{code}
  while check argument:
{code:java}
Preconditions.checkArgument(rowType.getFieldCount() == dynamicFilterPartitionKeys.size()); {code}
in method 
{code:java}
setDynamicFilteringData {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 03:36:14 UTC 2022,,,,,,,,,,"0|z17sds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 03:36;godfreyhe;Fixed in master: 06e8b7fb30fee04ed61cdf99763fc7cd17401c8e;;;",,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-221 lookup table cache,FLINK-28989,13476992,13476068,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lsy,renqs,renqs,16/Aug/22 08:04,05/Sep/22 04:31,04/Jun/24 20:41,05/Sep/22 04:31,1.16.0,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,release-testing,,,,,,"The functionality of lookup table cache introduced in the FLIP-221 should be validated, including:

- Partial caching, which has been integrated by JDBC and HBase lookup table
- Full caching, only supported by test values lookup table",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/22 04:26;lsy;image-2022-09-05-12-26-22-263.png;https://issues.apache.org/jira/secure/attachment/13048940/image-2022-09-05-12-26-22-263.png","05/Sep/22 04:26;lsy;image-2022-09-05-12-26-51-427.png;https://issues.apache.org/jira/secure/attachment/13048941/image-2022-09-05-12-26-51-427.png","05/Sep/22 04:27;lsy;image-2022-09-05-12-27-23-259.png;https://issues.apache.org/jira/secure/attachment/13048942/image-2022-09-05-12-27-23-259.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 05 04:31:13 UTC 2022,,,,,,,,,,"0|z17scg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 02:08;hxb;Hi [~lsy]  any progress on the issue?;;;","30/Aug/22 02:18;lsy;Sorry, I haven't started it due to some other work, I will start it as soon as possible.;;;","05/Sep/22 04:31;lsy;Currently, only jdbc and hbase connectors implement the partial caching, so I've tested the jdbc cache through the standalone cluster. The test cases are as follows:
 # Set `lookup.partial-cache.expire-after-write = 10000s`, I found the hitCount metrics will +2 each time hit the cache. The phenomenon was not as expected.
 # Set `lookup.partial-cache.expire-after-write = 10s`,  after 10 expired, the numCachedRecord metrics didn't decrease to 0 right away,  the metrics decreased only after some new record arrived, so this may be confuse the user. In addition, even if one record had been loaded into the cache, after it expired, if the same join key record arrived again, the hitCount and missCount metrics increase simultaneously, I think only one of them should increase.

!image-2022-09-05-12-26-22-263.png!

 

 

!image-2022-09-05-12-27-23-259.png!

As summary, I found here exists two main problems: the first is the guava cache expiration mechanism, guava doesn't expire the cache data actively, it needs a data trigger, so the numCacheRecords metrics is not corrected; the second is  hitCount and missCount metrics increase simultaneously even if hit the cache.;;;",,,,,,,,,,,,,,,,,,
Incorrect result for filter after temporal join,FLINK-28988,13476990,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,xuannan,xuannan,16/Aug/22 07:58,10/Jan/23 19:53,04/Jun/24 20:41,05/Dec/22 12:32,1.15.1,,,,1.16.1,1.17.0,,,Table SQL / API,,,,,,,1,pull-request-available,,,,,,"The following code can reproduce the case

 
{code:java}
public class TemporalJoinSQLExample1 {

    public static void main(String[] args) throws Exception {

        // set up the Java DataStream API
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // set up the Java Table API
        final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

        final DataStreamSource<Tuple3<Integer, String, Instant>> ds =
                env.fromElements(
                        new Tuple3<>(0, ""online"", Instant.ofEpochMilli(0)),
                        new Tuple3<>(0, ""offline"", Instant.ofEpochMilli(10)),
                        new Tuple3<>(0, ""online"", Instant.ofEpochMilli(20)));

        final Table table =
                tableEnv.fromDataStream(
                                ds,
                                Schema.newBuilder()
                                        .column(""f0"", DataTypes.INT())
                                        .column(""f1"", DataTypes.STRING())
                                        .column(""f2"", DataTypes.TIMESTAMP_LTZ(3))
                                        .watermark(""f2"", ""f2 - INTERVAL '2' SECONDS"")
                                        .build())
                        .as(""id"", ""state"", ""ts"");
        tableEnv.createTemporaryView(""source_table"", table);
        final Table dedupeTable =
                tableEnv.sqlQuery(
                        ""SELECT * FROM (""
                                + "" SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY ts DESC) AS row_num FROM source_table""
                                + "") WHERE row_num = 1"");
        tableEnv.createTemporaryView(""versioned_table"", dedupeTable);

        DataStreamSource<Tuple2<Integer, Instant>> event =
                env.fromElements(
                        new Tuple2<>(0, Instant.ofEpochMilli(0)),
                        new Tuple2<>(0, Instant.ofEpochMilli(5)),
                        new Tuple2<>(0, Instant.ofEpochMilli(10)),
                        new Tuple2<>(0, Instant.ofEpochMilli(15)),
                        new Tuple2<>(0, Instant.ofEpochMilli(20)),
                        new Tuple2<>(0, Instant.ofEpochMilli(25)));

        final Table eventTable =
                tableEnv.fromDataStream(
                                event,
                                Schema.newBuilder()
                                        .column(""f0"", DataTypes.INT())
                                        .column(""f1"", DataTypes.TIMESTAMP_LTZ(3))
                                        .watermark(""f1"", ""f1 - INTERVAL '2' SECONDS"")
                                        .build())
                        .as(""id"", ""ts"");

        tableEnv.createTemporaryView(""event_table"", eventTable);

        final Table result =
                tableEnv.sqlQuery(
                        ""SELECT * FROM event_table""
                                + "" LEFT JOIN versioned_table FOR SYSTEM_TIME AS OF event_table.ts""
                                + "" ON event_table.id = versioned_table.id"");
        result.execute().print();

        result.filter($(""state"").isEqual(""online"")).execute().print();
    }
} {code}
 

The result of temporal join is the following:
|op|         id|                     ts|        id0|                         state|                    ts0|             row_num|
|+I|          0|1970-01-01 08:00:00.000|          0|                        online|1970-01-01 08:00:00.000|                   1|
|+I|          0|1970-01-01 08:00:00.005|          0|                        online|1970-01-01 08:00:00.000|                   1|
|+I|          0|1970-01-01 08:00:00.010|          0|                       offline|1970-01-01 08:00:00.010|                   1|
|+I|          0|1970-01-01 08:00:00.015|          0|                       offline|1970-01-01 08:00:00.010|                   1|
|+I|          0|1970-01-01 08:00:00.020|          0|                        online|1970-01-01 08:00:00.020|                   1|
|+I|          0|1970-01-01 08:00:00.025|          0|                        online|1970-01-01 08:00:00.020|                   1|

 

After filtering with predicate state = 'online', I expect only the two rows with state offline will be filtered out. But I got the following result:

|op|         id|                     ts|        id0|                         state|                    ts0|             row_num|
|+I|          0|1970-01-01 08:00:00.020|          0|                        online|1970-01-01 08:00:00.020|                   1|
|+I|          0|1970-01-01 08:00:00.025|          0|                        online|1970-01-01 08:00:00.020|                   1|

 
 
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Tue Jan 10 19:53:17 UTC 2023,,,,,,,,,,"0|z17sc0:",9223372036854775807,"After FLINK-28988 applied, the filter will not be pushed down into both inputs of the event time temporal join.
Note this may cause incompatible plan changes compare to 1.16.0, e.g., when left input is an upsert source(use upsert-kafka connector), the query plan will remove the ChangelogNormalize node from which appeared in 1.16.0.",,,,,,,,,,,,,,,,,,,"29/Aug/22 15:27;csq;[~xuannan] Thank you for reporting the issue. I have also reproduced the output with the code snippet you provided in the latest master branch.  I would like to have a deep dive into the issue.;;;","05/Sep/22 03:04;csq;TLDR: The filters in aboveFilter should not be pushed down into right table when it is a temporal join.

when there's no filter after temporal join, the query is explained as below:

{code:xml}
== Abstract Syntax Tree ==
LogicalProject(id=[$0], ts=[$1], id0=[$2], state=[$3], ts0=[$4], row_num=[$5])
+- LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0, 1}])
   :- LogicalProject(id=[AS($0, _UTF-16LE'id')], ts=[AS($1, _UTF-16LE'ts')])
   :  +- LogicalWatermarkAssigner(rowtime=[f1], watermark=[-($1, 2000:INTERVAL SECOND)])
   :     +- LogicalTableScan(table=[[*anonymous_datastream_source$2*]])
   +- LogicalFilter(condition=[=($cor0.id, $0)])
      +- LogicalSnapshot(period=[$cor0.ts])
         +- LogicalProject(id=[$0], state=[$1], ts=[$2], row_num=[$3])
            +- LogicalFilter(condition=[=($3, 1)])
               +- LogicalProject(id=[AS($0, _UTF-16LE'id')], state=[AS($1, _UTF-16LE'state')], ts=[AS($2, _UTF-16LE'ts')], row_num=[ROW_NUMBER() OVER (PARTITION BY AS($0, _UTF-16LE'id') ORDER BY AS($2, _UTF-16LE'ts') DESC NULLS LAST)])
                  +- LogicalWatermarkAssigner(rowtime=[f2], watermark=[-($2, 2000:INTERVAL SECOND)])
                     +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]])

== Optimized Physical Plan ==
Calc(select=[id, ts, id0, state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, row_num])
+- TemporalJoin(joinType=[LeftOuterJoin], where=[AND(=(id, id0), __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, state, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[-(f1, 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $1 AS state, $2 AS ts, 1:BIGINT AS row_num])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[-(f2, 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])

== Optimized Execution Plan ==
Calc(select=[id, ts, id0, state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, row_num])
+- TemporalJoin(joinType=[LeftOuterJoin], where=[((id = id0) AND __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, state, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[(f1 - 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $1 AS state, $2 AS ts, 1 AS row_num])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[(f2 - 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])
{code}

And after the FlinkChangelogModeInferenceProgram, the UpdateKindTrait of Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME]) will be come [ONLY_UPDATE_AFTER]. Therefore,  during  execution runtime, the rightSortedState in TemporalRowTimeJoinOperator contains the following rows:
[+I, 0, online, 1970-01-01 08:00:00.000]
[+I, 0, offline, 1970-01-01 08:00:00.010]
[+I, 0, online, 1970-01-01 08:00:00.020]

So we can get the expected temporal join result:
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0010,0,offline,1970-01-01 08:00:00.010,1]
[+I,0,1970-01-01 08:00:00.0015,0,offline,1970-01-01 08:00:00.010,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

However, if the filter was pushed down into the right table, the right sorted state will bocome:
[+I, 0, online, 1970-01-01 08:00:00.000]
[+I, 0, online, 1970-01-01 08:00:00.020]

and the temporal join result will become:
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0010,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0015,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

while the expected result is:
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1].

*We can find that if the record in right table is filtered before join, the temporal join result may be wrong.*

However, when we added a filter after temporal join, the actual output is:
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

this is not equal to 
[+I,0,1970-01-01 08:00:00.000,0,online,970-01-01 08:00:00.000 ,1]
[+I,0,1970-01-01 08:00:00.005,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0010,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.0015,0,online,1970-01-01 08:00:00.000,1]
[+I,0,1970-01-01 08:00:00.020,0,online,1970-01-01 08:00:00.020,1]
[+I,0,1970-01-01 08:00:00.025,0,online,1970-01-01 08:00:00.020,1]

*why?*
we notice that the query with filter is explained as below:

{code:xml}
== Abstract Syntax Tree ==
LogicalFilter(condition=[=($3, _UTF-16LE'online')])
+- LogicalProject(id=[$0], ts=[$1], id0=[$2], state=[$3], ts0=[$4], row_num=[$5])
   +- LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0, 1}])
      :- LogicalProject(id=[AS($0, _UTF-16LE'id')], ts=[AS($1, _UTF-16LE'ts')])
      :  +- LogicalWatermarkAssigner(rowtime=[f1], watermark=[-($1, 2000:INTERVAL SECOND)])
      :     +- LogicalTableScan(table=[[*anonymous_datastream_source$2*]])
      +- LogicalFilter(condition=[=($cor0.id, $0)])
         +- LogicalSnapshot(period=[$cor0.ts])
            +- LogicalProject(id=[$0], state=[$1], ts=[$2], row_num=[$3])
               +- LogicalFilter(condition=[=($3, 1)])
                  +- LogicalProject(id=[AS($0, _UTF-16LE'id')], state=[AS($1, _UTF-16LE'state')], ts=[AS($2, _UTF-16LE'ts')], row_num=[ROW_NUMBER() OVER (PARTITION BY AS($0, _UTF-16LE'id') ORDER BY AS($2, _UTF-16LE'ts') DESC NULLS LAST)])
                     +- LogicalWatermarkAssigner(rowtime=[f2], watermark=[-($2, 2000:INTERVAL SECOND)])
                        +- LogicalTableScan(table=[[*anonymous_datastream_source$1*]])

== Optimized Physical Plan ==
Calc(select=[id, ts, id0, CAST(_UTF-16LE'online':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, CAST(row_num AS BIGINT) AS row_num])
+- TemporalJoin(joinType=[InnerJoin], where=[AND(=(id, id0), __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[-(f1, 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $2 AS ts, 1:BIGINT AS row_num], where=[=($1, _UTF-16LE'online')])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[-(f2, 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])

== Optimized Execution Plan ==
Calc(select=[id, ts, id0, CAST('online' AS VARCHAR(2147483647)) AS state, CAST(ts0 AS TIMESTAMP_WITH_LOCAL_TIME_ZONE(3)) AS ts0, CAST(row_num AS BIGINT) AS row_num])
+- TemporalJoin(joinType=[InnerJoin], where=[((id = id0) AND __TEMPORAL_JOIN_CONDITION(ts, ts0, __TEMPORAL_JOIN_CONDITION_PRIMARY_KEY(id0), __TEMPORAL_JOIN_LEFT_KEY(id), __TEMPORAL_JOIN_RIGHT_KEY(id0)))], select=[id, ts, id0, ts0, row_num])
   :- Exchange(distribution=[hash[id]])
   :  +- Calc(select=[f0 AS id, f1 AS ts])
   :     +- WatermarkAssigner(rowtime=[f1], watermark=[(f1 - 2000:INTERVAL SECOND)])
   :        +- TableSourceScan(table=[[*anonymous_datastream_source$2*]], fields=[f0, f1])
   +- Exchange(distribution=[hash[id]])
      +- Calc(select=[$0 AS id, $2 AS ts, 1 AS row_num], where=[($1 = 'online')])
         +- Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME])
            +- Exchange(distribution=[hash[$0]])
               +- Calc(select=[f0 AS $0, f1 AS $1, f2 AS $2])
                  +- WatermarkAssigner(rowtime=[f2], watermark=[(f2 - 2000:INTERVAL SECOND)])
                     +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[f0, f1, f2])
{code}

There is a Calc(select=[$0 AS id, $2 AS ts, 1 AS row_num], where=[($1 = 'online')]) between Exchange(distribution=[hash[id]]) and Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME]). According to [FLINK-9528|https://issues.apache.org/jira/browse/FLINK-9528] and [FLINK-16887|https://issues.apache.org/jira/browse/FLINK-16887], after the FlinkChangelogModeInferenceProgram, the UpdateKindTrait of Deduplicate(keep=[LastRow], key=[$0], order=[ROWTIME]) will be come [BEFORE_AND_AFTER], that the Deduplicate operator will produce message with UPDATE_BEFORE message. As a result, the sortedRightState will become:
[-U, 0, online, 1970-01-01 08:00:00.000]
#[-U, 0, offline, 1970-01-01 08:00:00.010], it is filtered
[+U, 0, online, 1970-01-01 08:00:00.020]

Finally, only the [+U, 0, online, 1970-01-01 08:00:00.020] is joined, that's exactly equal to the actual output.

Overall, the solution would be not to push the filters in above filters into right table when it's a temporal join.


;;;","05/Sep/22 04:59;csq;Hi, [~jark], could you please help evaluate the solution and review the pr? Thanks!;;;","05/Dec/22 12:32;godfrey;Fixed in 1.17.0:

b2203eaef68364306dfcc27fb34ac82baefda3d3

2851fac9c4c052876c80440b6b0b637603de06ea

 

1.16.1:

17b42516ceb73fa342101aedf830df40a84d82bc

c14355243995bff7b03a527ed073a2bbaab70ce8;;;","10/Jan/23 19:53;jingge;[~csq] [~xuannan] Is there a typo in the description? 

After filtering with predicate state = 'online', I expect only the -two- {color:#FF0000}four{color} rows with state offline will be filtered.;;;",,,,,,,,,,,,,,,,
Incorrect async to sync lookup fallback path of LegacyTableSourceTable,FLINK-28987,13476981,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,16/Aug/22 07:38,19/Aug/22 14:25,04/Jun/24 20:41,19/Aug/22 14:25,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"FLINK-28779 introduces a new lookup hint, for LegacyTableSourceTable interface, it defines only one lookup capability of either sync or async is supported by 'isAsyncEnabled', but we add a unnecessary fallback path to fetch both async and sync lookup in `LookupJoinUtil`, this should be fixed.
{code:java}
if (tableSource.isAsyncEnabled()) {
asyncLookupFunction = tableSource.getAsyncLookupFunction(lookupFieldNamesInOrder);
}
syncLookupFunction = tableSource.getLookupFunction(lookupFieldNamesInOrder);

{code}
Once this was fixed, it can completely avoid create user lookup function instance to determine if async lookup is enabled for execution, this can make the exec lookup join node immutable and easier to maintain.

Also, since the new hint adds the capability of configuring async params on join level (different joins in same query may has different params), so the description of lookup join transformation should carry async params and retry strategy (which take effect in runtime) for easier debugging",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 14:25:46 UTC 2022,,,,,,,,,,"0|z17sa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 14:25;godfreyhe;Fixed in master:
d402fe255cdca37568ad6ac585ac6fbdf24b5e74
ef97c651f06dc835c36e8213687d66e78d80a0b6;;;",,,,,,,,,,,,,,,,,,,,
UNNEST function with nested filter fails to generate plan,FLINK-28986,13476959,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,16/Aug/22 06:23,18/Aug/22 10:24,04/Jun/24 20:41,18/Aug/22 10:24,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"h3. How to reproduce

add the following case to TableEnvironmentITCase
{code:scala}
@Test
def debug(): Unit = {
  tEnv.executeSql(
    s""""""
       |CREATE TEMPORARY TABLE source_kafka_wip_his_all (
       |  GUID varchar,
       |  OPERATION varchar,
       |  PRODUCTID varchar,
       |  LOTNO varchar,
       |  SERIALNO varchar,
       |  QUERYSERIALNO varchar,
       |  SERIALNO1 varchar,
       |  SERIALNO2 varchar,
       |  WIPORDERNO varchar,
       |  WIPORDERTYPE varchar,
       |  VIRTUALLOT varchar,
       |  PREOPERATION varchar,
       |  NORMALPREOPERATION varchar,
       |  PROCESSID varchar,
       |  EQUIPMENT varchar,
       |  INBOUNDDATE varchar,
       |  OUTBOUNDDATE varchar,
       |  REWORK varchar,
       |  REWORKPROCESSID varchar,
       |  CONTAINER varchar,
       |  WIPCONTENTCLASSID varchar,
       |  STATUSCODE varchar,
       |  WIPSTATUS varchar,
       |  TESTPROCESSID varchar,
       |  TESTORDERTYPE varchar,
       |  TESTORDER varchar,
       |  TEST varchar,
       |  SORTINGPROCESSID varchar,
       |  SORTINGORDERTYPE varchar,
       |  SORTINGORDER varchar,
       |  SORTING varchar,
       |  MINO varchar,
       |  GROUPCODE varchar,
       |  HIGHLOWGROUP varchar,
       |  PRODUCTNO varchar,
       |  FACILITY varchar,
       |  WIPLINE varchar,
       |  CHILDEQUCODE varchar,
       |  STATION varchar,
       |  QTY varchar,
       |  PASS_FLAG varchar,
       |  DEFECTCODELIST varchar,
       |  ISFIRST varchar,
       |  PARALIST ARRAY<ROW(GUID string,WIP_HIS_GUID string,QUERYSERIALNO string,OPERATION string,REWORKPROCESSID string,CHARACTERISTIC string,CHARACTERISTICREVISION string,CHARACTERISTICTYPE string,CHARACTERISTICCLASS string,UPPERCONTROLLIMIT string,TARGETVALUE string,LOWERCONTROLLIMIT string,TESTVALUE string,TESTATTRIBUTE string,TESTINGSTARTDATE string,TESTFINISHDATE string,UOMCODE string,DEFECTCODE string,SPECPARAMID string,STATION string,GP_TIME string,REFERENCEID string,LASTUPDATEON string,LASTUPDATEDBY string,CREATEDON string,CREATEDBY string,ACTIVE string,LASTDELETEON string,LASTDELETEDBY string,LASTREACTIVATEON string,LASTREACTIVATEDBY string,ARCHIVEID string,LASTARCHIVEON string,LASTARCHIVEDBY string,LASTRESTOREON string,LASTRESTOREDBY string,ROWVERSIONSTAMP string)>,
       |  REFERENCEID varchar,
       |  LASTUPDATEON varchar,
       |  LASTUPDATEDBY varchar,
       |  CREATEDON varchar,
       |  CREATEDBY varchar,
       |  ACTIVE varchar,
       |  LASTDELETEON varchar,
       |  LASTDELETEDBY varchar,
       |  LASTREACTIVATEON varchar,
       |  LASTREACTIVATEDBY varchar,
       |  ARCHIVEID varchar,
       |  LASTARCHIVEON varchar,
       |  LASTARCHIVEDBY varchar,
       |  LASTRESTOREON varchar,
       |  LASTRESTOREDBY varchar,
       |  ROWVERSIONSTAMP varchar,
       |  proctime as PROCTIME()
       |  ) with (
       |  'connector' = 'datagen'
       |)
       |"""""".stripMargin)

  tEnv.executeSql(
    s""""""
       |create TEMPORARY view transform_main_data as
       |select
       |      r.GUID as wip_his_guid,
       |      r.EQUIPMENT as equipment,
       |      r.WIPLINE as wipline,
       |      r.STATION as station,
       |      cast(r.PROCESSID as decimal) as processid,
       |      r.PRODUCTNO as productno,
       |      t.TESTFINISHDATE as testfinishdate,
       |      t.OPERATION as operation,
       |      t.CHARACTERISTIC as characteristic,
       |      t.LOWERCONTROLLIMIT as lowercontrollimit,
       |      t.UPPERCONTROLLIMIT as uppercontrollimit,
       |      t.TARGETVALUE as targetvalue,
       |      t.DEFECTCODE as defectcode,
       |      t.TESTVALUE as testvalue,
       |      t.CHARACTERISTICTYPE as characteristictype,
       |      proctime
       |  from
       |  (select
       |      GUID,
       |      EQUIPMENT,
       |      WIPLINE,
       |      STATION,
       |      PROCESSID,
       |      PRODUCTNO,
       |      PARALIST,
       |      proctime
       |  FROM source_kafka_wip_his_all) r
       |  cross join
       |  unnest(PARALIST) as t (GUID,WIP_HIS_GUID,QUERYSERIALNO,OPERATION,REWORKPROCESSID,CHARACTERISTIC,CHARACTERISTICREVISION,CHARACTERISTICTYPE,CHARACTERISTICCLASS,UPPERCONTROLLIMIT,TARGETVALUE,LOWERCONTROLLIMIT,TESTVALUE,TESTATTRIBUTE,TESTINGSTARTDATE,TESTFINISHDATE,UOMCODE,DEFECTCODE,SPECPARAMID,STATION,GP_TIME,REFERENCEID,LASTUPDATEON,LASTUPDATEDBY,CREATEDON,CREATEDBY,ACTIVE,LASTDELETEON,LASTDELETEDBY,LASTREACTIVATEON,LASTREACTIVATEDBY,ARCHIVEID,LASTARCHIVEON,LASTARCHIVEDBY,LASTRESTOREON,LASTRESTOREDBY,ROWVERSIONSTAMP)
       |  where t.CHARACTERISTICTYPE = '2'
       |"""""".stripMargin)

  tEnv.executeSql(
    s""""""
       |explain plan for
       |select * from transform_main_data
       |where operation not in ('G1208','G1209','G1211','G1213','G1206','G1207','G1214','G1215','G1282','G1292','G1216')
       |"""""".stripMargin).print()
} {code}
Stacktrace
{code:java}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: LogicalProject(inputs=[0..3], exprs=[[CAST($4):DECIMAL(10, 0), $5, $23, $11, $13, $19, $17, $18, $25, $20, $15, $7]])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{6}])
   :- LogicalProject(inputs=[0], exprs=[[$14, $36, $38, $13, $34, $43, PROCTIME()]])
   :  +- LogicalTableScan(table=[[default_catalog, default_database, source_kafka_wip_his_all]])
   +- LogicalFilter(condition=[AND(SEARCH($7, Sarg[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""]:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), SEARCH($3, Sarg[(-∞.._UTF-16LE'G1206'), (_UTF-16LE'G1206'.._UTF-16LE'G1207'), (_UTF-16LE'G1207'.._UTF-16LE'G1208'), (_UTF-16LE'G1208'.._UTF-16LE'G1209'), (_UTF-16LE'G1209'.._UTF-16LE'G1211'), (_UTF-16LE'G1211'.._UTF-16LE'G1213'), (_UTF-16LE'G1213'.._UTF-16LE'G1214'), (_UTF-16LE'G1214'.._UTF-16LE'G1215'), (_UTF-16LE'G1215'.._UTF-16LE'G1216'), (_UTF-16LE'G1216'.._UTF-16LE'G1282'), (_UTF-16LE'G1282'.._UTF-16LE'G1292'), (_UTF-16LE'G1292'..+∞)]:CHAR(5) CHARACTER SET ""UTF-16LE""))])
      +- Uncollect
         +- LogicalProject(exprs=[[$cor1.PARALIST]])
            +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

LogicalProject(inputs=[0..3], exprs=[[CAST($4):DECIMAL(10, 0), $5, $23, $11, $13, $19, $17, $18, $25, $20, $15, $7]])
+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{6}])
   :- LogicalProject(inputs=[0], exprs=[[$14, $36, $38, $13, $34, $43, PROCTIME()]])
   :  +- LogicalTableScan(table=[[default_catalog, default_database, source_kafka_wip_his_all]])
   +- LogicalFilter(condition=[AND(SEARCH($7, Sarg[_UTF-16LE'2':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""]:VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""), SEARCH($3, Sarg[(-∞.._UTF-16LE'G1206'), (_UTF-16LE'G1206'.._UTF-16LE'G1207'), (_UTF-16LE'G1207'.._UTF-16LE'G1208'), (_UTF-16LE'G1208'.._UTF-16LE'G1209'), (_UTF-16LE'G1209'.._UTF-16LE'G1211'), (_UTF-16LE'G1211'.._UTF-16LE'G1213'), (_UTF-16LE'G1213'.._UTF-16LE'G1214'), (_UTF-16LE'G1214'.._UTF-16LE'G1215'), (_UTF-16LE'G1215'.._UTF-16LE'G1216'), (_UTF-16LE'G1216'.._UTF-16LE'G1282'), (_UTF-16LE'G1282'.._UTF-16LE'G1292'), (_UTF-16LE'G1292'..+∞)]:CHAR(5) CHARACTER SET ""UTF-16LE""))])
      +- Uncollect
         +- LogicalProject(exprs=[[$cor1.PARALIST]])
            +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.

    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:527)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:96)
    at org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanner.scala:51)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:695)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1356)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:733)
    at org.apache.flink.table.api.TableEnvironmentITCase.debug(TableEnvironmentITCase.scala:695)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runners.Suite.runChild(Suite.java:128)
    at org.junit.runners.Suite.runChild(Suite.java:27)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=LOGICAL, FlinkRelDistributionTraitDef=any, MiniBatchIntervalTraitDef=None: 0, ModifyKindSetTraitDef=[NONE], UpdateKindTraitDef=[NONE].
Missing conversion is Uncollect[convention: NONE -> LOGICAL]
There is 1 empty subset: rel#485:RelSubset#4.LOGICAL.any.None: 0.[NONE].[NONE], the relevant part of the original plan is as follows
460:Uncollect
  458:LogicalProject(subset=[rel#459:RelSubset#3.NONE.any.None: 0.[NONE].[NONE]], PARALIST=[$cor1.PARALIST])
    17:LogicalValues(subset=[rel#457:RelSubset#2.NONE.any.None: 0.[NONE].[NONE]], tuples=[[{ 0 }]]){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:36;qingyue;image-2022-08-16-14-36-07-061.png;https://issues.apache.org/jira/secure/attachment/13048154/image-2022-08-16-14-36-07-061.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 18 10:24:56 UTC 2022,,,,,,,,,,"0|z17s54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:36;qingyue;The reason is that during the decorrelation rewrite, the top filter rel node was pushed down to form a nested filter pattern. Since the filter merge rule is not defined in the default rewrite rule sets, the nested filter rel nodes rendered the LogicalUnnestRule unmatched. This can be fixed by adding  CoreRules.FILTER_MERGE to FlinkStreamRuleSets. cc [~godfrey] 

!image-2022-08-16-14-36-07-061.png|width=1047,height=177!;;;","16/Aug/22 06:44;godfreyhe;[~qingyue] Thanks for reporting this, assign to you ;;;","18/Aug/22 10:24;godfreyhe;Fixed in master: bdffb6bd4ae3ea32bdcd6ec6bce6c6e0e8b92a11;;;",,,,,,,,,,,,,,,,,,
support create table like view,FLINK-28985,13476952,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,waywtdcc,waywtdcc,waywtdcc,16/Aug/22 05:29,14/Mar/24 07:36,04/Jun/24 20:41,,1.15.1,1.16.0,,,1.20.0,,,,Table SQL / API,,,,,,,0,pull-request-available,stale-assigned,,,,,"At present, to create a table based on table like, you can only use the table type table, not the view type.

 

create table <tableName> like <viewName>;

Only like table type can be used before. This is similar to create table as < querysql >, but some scenarios use views more flexibly and can reuse a single view in multiple places.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 10:35:11 UTC 2023,,,,,,,,,,"0|z17s3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 02:03;waywtdcc;嗨，[~jark]

 ;;;","18/Oct/22 01:10;waywtdcc;Please assign this to me.;;;","26/Oct/22 07:01;martijnvisser;[~godfreyhe] WDYT?;;;","04/Nov/22 03:28;waywtdcc;hello? [~godfreyhe] ;;;","27/Feb/23 14:43;jark;Hi [~waywtdcc], as this involves exposing a new SQL syntax, it requires a FLIP and discussion on dev ML. Besides, it would be better to investigate what syntax and behavior are in other systems. Currently, from the issue description, it's unclear what syntax and behavior and what problem this syntax solves. ;;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,
FsCheckpointStateOutputStream is not being released normally,FLINK-28984,13476950,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ChangjiGuo,ChangjiGuo,16/Aug/22 05:07,23/Aug/23 22:34,04/Jun/24 20:41,,1.11.6,1.15.1,,,,,,,Runtime / Checkpointing,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,"If the checkpoint is aborted, AsyncSnapshotCallable will close the snapshotCloseableRegistry when it is canceled. There may be two situations here:
 # The FSDataOutputStream has been created and closed while closing FsCheckpointStateOutputStream.
 # The FSDataOutputStream has not been created yet, but closed flag has been set to true. You can see this in log:
{code:java}
2022-08-16 12:55:44,161 WARN  org.apache.flink.core.fs.SafetyNetCloseableRegistry           - Closing unclosed resource via safety-net: ClosingFSDataOutputStream(org.apache.flink.runtime.fs.hdfs.HadoopDataOutputStream@4ebe8e64) : xxxxx/flink/checkpoint/state/9214a2e302904b14baf2dc1aacbc7933/ae157c5a05a8922a46a179cdb4c86b10/shared/9d8a1e92-2f69-4ab0-8ce9-c1beb149229a {code}

        The output stream will be automatically closed by the SafetyNetCloseableRegistry but the file will not be deleted.

The second case usually occurs when the storage system has high latency in creating files.

How to reproduce?

This is not easy to reproduce, but you can try to set a smaller checkpoint timeout and increase the parallelism of the flink job.
 ",,,,,,,,,,,,,,,,,,,,,,,,FLINK-28927,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/22 07:22;ChangjiGuo;log.png;https://issues.apache.org/jira/secure/attachment/13048156/log.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 23 22:34:56 UTC 2023,,,,,,,,,,"0|z17s34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:01;yunta;I think the second analysis looks a bit weird. If the FSDataOutputStream has not been created, it will not call {{FsCheckpointStateOutputStream#createStream}} , which means we will not get the generated random path.

The SafetyNetCloseableRegistry would help close the unreleased closable on JVM GC, it must be we forget to close them in somewhere.;;;","16/Aug/22 07:23;ChangjiGuo;[~yunta]  I'm sorry, maybe I didn't express it clearly. There is a prerequisite here that snapshotCloseableRegistry will be closed, and then the registered closabe will call the close method. Both _FsCheckpointStateOutputStream#createStream_ and _FsCheckpointStateOutputStream#close_ can be called at the same time. It is possible that the FSDataOutputStream has not been created when the close called(at this time, outStream is null).

In order to verify this conjecture, I printed the log at both setting closed = true and returning stream, as follows:

!log.png|width=741,height=304!
 ;;;","18/Aug/22 06:55;ChangjiGuo;Hi [~yunta], I have verified on Flink-1.15.1.

My fix is to check if _FsCheckpointStateOutputStream_ has been closed after creating the output stream and clean up(include closing stream and deleting file) if closed. It works well and without the above logs.

Can you take a look if you have time? Thx.;;;","13/Oct/22 08:55;Yanfei Lei;Hi [~ChangjiGuo], do you mean the {{FsCheckpointStateOutputStream#close()}} is called during 

{{{}FsCheckpointStateOutputStream#flushToFile(){}}}, I think the root cause is that there is no concurrency guarantee for checking {{close}} and {{FsCheckpointStateOutputStream#createStream}} in {{{}#flushToFile(){}}}, what do you think?

 ;;;","13/Oct/22 11:26;ChangjiGuo;Hi [~Yanfei Lei], thanks for your reply! Yes, that's what I want to express! 
 ;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","23/Aug/23 22:34;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,
using serviceaccount in FlinkDeployment not works when sink to aws s3,FLINK-28983,13476947,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,lichuan33,lichuan33,16/Aug/22 04:49,22/Sep/22 12:41,04/Jun/24 20:41,22/Sep/22 12:41,kubernetes-operator-1.1.0,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"I am deploying a Flink CDC job using sql-runner example from official examples(see [https://github.com/apache/flink-kubernetes-operator/tree/main/examples/flink-sql-runner-example).] 

The _flink_ service account has all s3 permissions (`s3:*` in iam policy) but the k8s pod keeps on restarting and there's too much errors on pod's log:

 
{code:java}
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
        at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
        at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
        at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
        at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
        at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
        at akka.actor.Actor.aroundReceive(Actor.scala:537)
        at akka.actor.Actor.aroundReceive$(Actor.scala:535)
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
        at akka.actor.ActorCell.invoke(ActorCell.scala:548)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
        at akka.dispatch.Mailbox.run(Mailbox.scala:231)
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
        ... 4 more
Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: doesBucketExist on nwlogs: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: B3ZNHK1DSM4JDJ39; S3 Extended Request ID: egjf79lHP0uHq2w4vGqe9yBNRE4exUVEYZ2EP093Aiz5H1YypS4SbcSfSVidbUTQeI/Zv0FmbIw=; Proxy: null), S3 Extended Request ID: egjf79lHP0uHq2w4vGqe9yBNRE4exUVEYZ2EP093Aiz5H1YypS4SbcSfSVidbUTQeI/Zv0FmbIw=:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: B3ZNHK1DSM4JDJ39; S3 Extended Request ID: egjf79lHP0uHq2w4vGqe9yBNRE4exUVEYZ2EP093Aiz5H1YypS4SbcSfSVidbUTQeI/Zv0FmbIw=; Proxy: null)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:224)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:111)
        at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)
        at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)
        at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:236)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:391)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:322)
        at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:127)
        at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508)
        at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
        at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink$BulkFormatBuilder.createBucketWriter(StreamingFileSink.java:428)
        at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink$BulkFormatBuilder.createBuckets(StreamingFileSink.java:438)
        at org.apache.flink.connector.file.table.stream.AbstractStreamingWriter.initializeState(AbstractStreamingWriter.java:96)
        at org.apache.flink.connector.file.table.stream.StreamingFileWriter.initializeState(StreamingFileWriter.java:81)
        at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:286)
        at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
        at java.lang.Thread.run(Thread.java:750)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: B3ZNHK1DSM4JDJ39; S3 Extended Request ID: egjf79lHP0uHq2w4vGqe9yBNRE4exUVEYZ2EP093Aiz5H1YypS4SbcSfSVidbUTQeI/Zv0FmbIw=; Proxy: null), S3 Extended Request ID: egjf79lHP0uHq2w4vGqe9yBNRE4exUVEYZ2EP093Aiz5H1YypS4SbcSfSVidbUTQeI/Zv0FmbIw=
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5259)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5206)
        at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1438)
        at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1374)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$verifyBucketExists$1(S3AFileSystem.java:392)
        at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
        ... 25 more{code}
 

I try to use AKSK(which is not recommended) to see if I am lucky. It occurs to me the k8s pod is in running state after setting `s3.access-key` and `s3.secret-key`. 

 

Here is my *FlinkDeployment* config file:

 
{code:java}
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: sql-example-stateful-s3
  namespace: flink
spec:
  image: 11122233344455.dkr.ecr.cn-northwest-1.amazonaws.com.cn/flink/flink-sql-runner-example:latest
  imagePullPolicy: Always
  flinkVersion: v1_15
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: ""1""
    state.savepoints.dir: s3://bucket/flink/flink-data/savepoints
    state.checkpoints.dir: s3://bucket/flink/flink-data/checkpoints
    high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory
    high-availability.storageDir: s3://bucket/flink/flink-data/ha
    execution.checkpointing.interval: ""10000""
    state.backend: filesystem
    fs.s3a.endpoint: s3.cn-northwest-1.amazonaws.com.cn
    env.java.opts: -Dcom.amazonaws.services.s3.enableV4
    s3.access-key: <AWS_ACCESS_KEY>
    s3.secret-key: <AWS_SECRET_ACCESS_KEY>
    s3a.endpoint: s3.cn-northwest-1.amazonaws.com.cn
  serviceAccount: flink
  jobManager:
    resource:
      memory: ""2048m""
      cpu: 1
  taskManager:
    resource:
      memory: ""2048m""
      cpu: 1
  job:
    jarURI: local:///opt/flink/usrlib/sql-runner.jar
    args: [""/opt/flink/usrlib/sql-scripts/orders.sql""]
    parallelism: 1
    upgradeMode: last-state{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 22 12:41:36 UTC 2022,,,,,,,,,,"0|z17s2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 08:08;gyfora;Could you successfully run any other jobs on this environment? These errors are usually some serviceaccount / permission related unfortunately.;;;","22/Sep/22 12:41;gyfora;This is not a bug in the operator, but it's related to Job configuration / dependency setup;;;",,,,,,,,,,,,,,,,,,,
Start TaskInterrupter when task switch from DEPLOYING to CANCELING,FLINK-28982,13476945,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,dangshazi,dangshazi,16/Aug/22 04:10,03/Feb/23 03:26,04/Jun/24 20:41,08/Dec/22 09:34,,,,,,,,,Runtime / Web Frontend,,,,,,,0,,,,,,,"Task will start TaskInterrupter only when `ExecutionState` is INITIALIZING or RUNNING in the function: org.apache.flink.runtime.taskmanager.Task#cancelOrFailAndCancelInvokableInternal

 

I got a dead lock in multi task which caused by Flink Remote Shuffle's sharing TCP connection bug and blocked tasks destruction when I use Flink Remote Shuffle.

stack as following:

!image-2022-08-16-12-10-43-894.png!

My question: Why not start the TaskInterrupter when cancel a deploying task?

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/22 04:10;dangshazi;image-2022-08-16-12-10-43-894.png;https://issues.apache.org/jira/secure/attachment/13048151/image-2022-08-16-12-10-43-894.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 03 03:26:35 UTC 2023,,,,,,,,,,"0|z17s20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/22 09:15;zhuzh;This is an intentional behavior because Flink does not want to interrupt actions originates from Flink framework, to avoid unexpected problems. The interruption only targets for user code. 
The task cancellation stuck in Flink code, this means a bug to fix.
For this specific case, to solve it, I think we should fix the problem of the Remote Shuffle Service.;;;","08/Dec/22 09:33;zhuzh;The related remote shuffle service problem is already fixed: https://github.com/flink-extended/flink-remote-shuffle/issues/77;;;","03/Feb/23 03:26;dangshazi;Got it. Thanks [~zhuzh] ;;;",,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-245 sources speculative execution,FLINK-28981,13476944,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,337361684@qq.com,zhuzh,zhuzh,16/Aug/22 03:53,02/Sep/22 13:42,04/Jun/24 20:41,02/Sep/22 13:42,,,,,1.16.0,,,,Connectors / Common,Runtime / Coordination,,,,,,0,release-testing,,,,,,"Speculative execution is introduced in Flink 1.16 to deal with temporary slow tasks caused by slow nodes. This feature currently consists of 4 FLIPs:
 - FLIP-168: Speculative Execution core part
 - FLIP-224: Blocklist Mechanism
 - FLIP-245: Source Supports Speculative Execution
 - FLIP-249: Flink Web UI Enhancement for Speculative Execution

This ticket aims for verifying FLIP-245, along with FLIP-168, FLIP-224 and FLIP-249.

More details about this feature and how to use it can be found in this [document|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/speculative_execution/].

To do the verification, the process can be:
 - Write Flink jobs which has some {{source}} subtasks running much slower than others. 3 kinds of sources should be verified, including
     -- [Source functions|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java]
     -- [InputFormat sources|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/io/InputFormat.java]
     -- [FLIP-27 new sources|https://github.com/apache/flink/blob/master//flink-core/src/main/java/org/apache/flink/api/connector/source/Source.java]
 - Modify Flink configuration file to enable speculative execution and tune the configuration as you like
 - Submit the job. Checking the web UI, logs, metrics and produced result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 02 13:42:25 UTC 2022,,,,,,,,,,"0|z17s1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 12:21;godfreyhe;Is there any progress on this issue ? [~337361684@qq.com];;;","25/Aug/22 01:42;337361684@qq.com;Yes, I will start the test today.;;;","30/Aug/22 01:52;hxb;Hi [~337361684@qq.com] , any progress on the issue?;;;","30/Aug/22 01:58;337361684@qq.com;[~hxb] , I have completed one-third of the work(InputFormat sources), and I will complete all the tests in the next two days.;;;","02/Sep/22 02:18;337361684@qq.com;Hi, [~hxb] [~zhuzh] [~godfreyhe] . I have finished tests on InputFormat sources. I modified datagen source‘s code. First, I let some tasks sleep when they process each records according to their subtask ID . and then gradually reduce the sleep time of these task according to the number of attempts. In this way, the functions mentioned in PR can be simulated, and the verification results are in line with expectations.

 

For me, I will continue to test FLIP-27 source if time is enough.;;;","02/Sep/22 09:59;337361684@qq.com;Hi, [~zhuzh], [~hxb], [~godfreyhe] . I have finished tests on FLIP-27 sources. I modified AbstractOrcFileInputFormat$readBatch() method's source code. I let some tasks sleep according to orcRowNumber, and run tasks on hive table source which reading data by orc reader. This test has achieved the expected results.

 

[~hxb] , I have finished these tests. Please closing this issue.;;;","02/Sep/22 13:40;godfrey;can this issue be closed ? [~zhuzh] [~337361684@qq.com] ;;;","02/Sep/22 13:42;zhuzh;Thanks for helping with this release testing! [~337361684@qq.com];;;",,,,,,,,,,,,,
Release Testing: Verify FLIP-168 speculative execution,FLINK-28980,13476940,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,SleePy,zhuzh,zhuzh,16/Aug/22 03:33,01/Sep/22 02:58,04/Jun/24 20:41,01/Sep/22 02:58,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,release-testing,,,,,,"Speculative execution is introduced in Flink 1.16 to deal with temporary slow tasks caused by slow nodes. This feature currently consists of 4 FLIPs:
 - FLIP-168: Speculative Execution core part
 - FLIP-224: Blocklist Mechanism
 - FLIP-245: Source Supports Speculative Execution
 - FLIP-249: Flink Web UI Enhancement for Speculative Execution

This ticket aims for verifying FLIP-168, along with FLIP-224 and FLIP-249.

More details about this feature and how to use it can be found in this [documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/speculative_execution/].

To do the verification, the process can be:
 - Write a Flink job which has a subtask running much slower than others (e.g. sleep indefinitely if it runs on a certain host, the hostname can be retrieved via InetAddress.getLocalHost().getHostName(), or if its (subtaskIndex + attemptNumer) % 2 == 0)
 - Modify Flink configuration file to enable speculative execution and tune the configuration as you like
 - Submit the job. Checking the web UI, logs, metrics and produced result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/22 15:01;SleePy;flink-root-standalonesession-0-VM_38_195_centos.log;https://issues.apache.org/jira/secure/attachment/13048820/flink-root-standalonesession-0-VM_38_195_centos.log","31/Aug/22 15:01;SleePy;flink-root-taskexecutor-0-VM_199_24_centos.log;https://issues.apache.org/jira/secure/attachment/13048822/flink-root-taskexecutor-0-VM_199_24_centos.log","31/Aug/22 15:01;SleePy;flink-root-taskexecutor-0-VM_38_195_centos.log;https://issues.apache.org/jira/secure/attachment/13048821/flink-root-taskexecutor-0-VM_38_195_centos.log","31/Aug/22 15:01;SleePy;screenshot1;https://issues.apache.org/jira/secure/attachment/13048818/screenshot1","31/Aug/22 15:01;SleePy;screenshot2;https://issues.apache.org/jira/secure/attachment/13048819/screenshot2","31/Aug/22 15:01;SleePy;stdout;https://issues.apache.org/jira/secure/attachment/13048823/stdout",,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 01 02:58:06 UTC 2022,,,,,,,,,,"0|z17s0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 12:22;godfreyhe;Is there any progress on this issue ? [~SleePy];;;","30/Aug/22 13:59;SleePy;[~godfreyhe] Sorry for late response, I missed the message notification. I'm working on it. It would be finished in today or tomorrow.;;;","31/Aug/22 14:59;SleePy;I've tested the scenario, and it looks good to me.

I started two TMs in different machines. And each of them has two slots. I used ""hostname checking"" with ""InetAddress.getLocalHost().getHostName()"" to make one task much slower than others (there are three subtasks of this operator). I set the ""slow-task-detector.execution-time.baseline-ratio"" to 0.5. The speculative task is launched as expected. I checked the web UI, metrics, logs and produced result. Everything works fine. There are some screenshots and log files in attachments.;;;","01/Sep/22 02:58;zhuzh;Thanks for helping with the release testing! [~SleePy];;;",,,,,,,,,,,,,,,,,
Add another owner into the JM deployment's owner references,FLINK-28979,13476930,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zezaeoh,haoxin,haoxin,16/Aug/22 02:47,09/Oct/22 15:18,04/Jun/24 20:41,09/Oct/22 15:17,,,,,kubernetes-operator-1.3.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"This action will link the resources into the FlinkDeployment

The JM deployment will look like
{code:java}
ownerReferences:  
- apiVersion: apps/v1    
  blockOwnerDeletion: true    
  controller: true    
  kind: Deployment    
  name: xxx    
- apiVersion: flink.apache.org/v1beta1   
  blockOwnerDeletion: true 
  controller: false 
  kind: FlinkDeployment 
  name: xxx{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/22 08:28;haoxin;image-2022-08-26-16-27-59-389.png;https://issues.apache.org/jira/secure/attachment/13048624/image-2022-08-26-16-27-59-389.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 09 15:17:06 UTC 2022,,,,,,,,,,"0|z17ryo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 08:07;gyfora;Why do you think we need this change? I am thinking whether there could be any unintended consequences;;;","26/Aug/22 08:30;haoxin;The reason is that I will see the linked resources in the Argo CD's Web UI.

Not something important, but will be a bit more user-friendly.

!image-2022-08-26-16-27-59-389.png|width=463,height=127!;;;","09/Oct/22 15:17;gyfora;merged to main 62eb68c812713378f7f66fde01cf14370a2252e4;;;",,,,,,,,,,,,,,,,,,
Kinesis connector doesn't work for new AWS regions,FLINK-28978,13476905,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liangtl,liangtl,liangtl,15/Aug/22 23:25,17/Aug/22 19:11,04/Jun/24 20:41,17/Aug/22 19:11,1.13.6,1.14.5,1.15.1,,1.14.6,1.15.2,1.16.0,,Connectors / Kinesis,,,,,,,0,pull-request-available,,,,,,"The current validation in the Kinesis connector checks that the AWS Region string specified is present in the {{Regions}} enum attached in the {{{}AWS SDK{}}}. This is not desirable because every time AWS launches a new region, we will have to update the AWS SDK shaded into the connector. 

We want to change it such that we validate the shape of the string, allowing for future AWS Regions. 

 

Current list of regions:

ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, ap-southeast-3, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 19:11:03 UTC 2022,,,,,,,,,,"0|z17rt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 10:40;dannycranmer;Merged commit [{{c90d0d5}}|https://github.com/apache/flink/commit/c90d0d537e662c2c0db1ff497a87da6a55b03bc4] into apache:release-1.15 ;;;","17/Aug/22 19:11;dannycranmer;* Merged commit [{{05bd634}}|https://github.com/apache/flink/commit/05bd63499358b589f2954b1822e68f43b5158cc1] into apache:release-1.14
 * Merged commit [{{a42e737}}|https://github.com/apache/flink/commit/a42e7373f79017ee4fffd1af5d61e25999b4038b] into apache:master;;;",,,,,,,,,,,,,,,,,,,
NullPointerException in HybridSourceSplitEnumerator.close,FLINK-28977,13476899,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Benenson,Benenson,Benenson,15/Aug/22 21:03,30/Aug/22 07:50,04/Jun/24 20:41,23/Aug/22 12:29,1.14.4,1.15.1,,,1.16.0,,,,Connectors / Common,,,,,,,0,pull-request-available,pull-requests-available,,,,,"HybridSource pipeline has an intermittent error when reading from s3, usually this error is fixed when pipeline restarts after recovering from checkpoint. But intermittently happens:


2022/08/02 22:26:51.435 INFO  o.a.f.runtime.jobmaster.JobMaster - Trying to recover from a global failure.
org.apache.flink.util.FlinkException: Global failure triggered by OperatorCoordinator for 'Source: hybrid-source -> decrypt -> map2Events -> filterOutNulls -> assignTimestampsAndWatermarks -> logRawJson' (operator fd9fbc680ee884c4eafd0b9c2d3d007f).
at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder$LazyInitializedCoordinatorContext.failJob(OperatorCoordinatorHolder.java:545)
at org.apache.flink.runtime.operators.coordination.RecreateOnResetOperatorCoordinator$DeferrableCoordinator.cleanAndFailJob(RecreateOnResetOperatorCoordinator.java:393)
...
Caused by: java.lang.NullPointerException: null
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.close(HybridSourceSplitEnumerator.java:246)
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.close(SourceCoordinator.java:151)
at org.apache.flink.runtime.operators.coordination.ComponentClosingUtils.lambda$closeAsyncWithTimeout$0(ComponentClosingUtils.java:70)
at java.lang.Thread.run(Thread.java:750)
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 07:49:57 UTC 2022,,,,,,,,,,"0|z17rrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 21:52;Benenson;I have verified the fix, the problem is intermittent, but reproducible in my environment.
[~thw] could you, please, assign this Jira to me & review PR for the fix?
Thank you.;;;","30/Aug/22 07:49;martijnvisser;Fixed in master: ee4d27411b3bf1cfcc4171aab777eb8bf08f1550;;;",,,,,,,,,,,,,,,,,,,
Changelog 1st materialization delayed unneccesarily,FLINK-28976,13476881,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,15/Aug/22 17:27,29/Aug/22 20:09,04/Jun/24 20:41,26/Aug/22 11:59,1.15.1,1.16.0,,,1.15.3,1.16.0,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,,,"In PeriodicMaterializationManager.start(), the 1st materialization is scheduled with a delay: materialization_interval + random_offset 

Here, random_offset is added to avoid thundering herd problem.
The next materialization will be scheduled with a delay of only materialization_interval.

That means that the 1st materialization will have to compact up to 2 times more state changes than the subsequent ones. 

Which in turn can cause FLINK--26590 or other problems.",,,,,,,,,,,,,,FLINK-25842,,,,,,,,,,,,,,,,,,,,FLINK-26590,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 26 11:59:53 UTC 2022,,,,,,,,,,"0|z17rns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 11:59;roman;Merged into master as 91e1291e942afc69779f09ead549352d5d357f22 .. a38b852bbbdb812aa404c226717a2fa3bdd89665,
into release-1.15 as 493a1aa8556038283e256efc5368bd319bd06d17 .. 258c3e35265bb3a966bd317340f2a5fe7cfd7364.
;;;",,,,,,,,,,,,,,,,,,,,
withIdleness marks all streams from FLIP-27 sources as idle,FLINK-28975,13476849,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,danderson,danderson,15/Aug/22 14:35,04/Nov/22 06:59,04/Jun/24 20:41,27/Sep/22 11:20,1.15.1,,,,1.15.3,1.16.0,,,API / DataStream,,,,,,,0,pull-request-available,,,,,,"Using withIdleness with a FLIP-27 source leads to all of the streams from the source being marked idle, which in turn leads to incorrect results, e.g., from joins that rely on watermarks.

Quoting from the user ML thread:

In org.apache.flink.streaming.api.operators.SourceOperator, there are separate instances of WatermarksWithIdleness created for each split output and the main output. There is multiplexing of watermarks between split outputs but no multiplexing between split output and main output.
 
For a source such as org.apache.flink.connector.kafka.source.KafkaSource, {color:#353833}there is only output from splits and no output from main. Hence the main output will (after an initial timeout) be marked as idle.{color}
{color:#353833} {color}
{color:#353833}The implementation of {color}WatermarksWithIdleness is such that once an output is idle, it will periodically re-mark the output as idle. Since there is no multiplexing between split outputs and main output, the idle marks coming from main output will repeatedly set the output to idle even though there are events from the splits. Result is that the entire source is repeatedly marked as idle.


See this ML thread for more details: [https://lists.apache.org/thread/bbokccohs16tzkdtybqtv1vx76gqkqj4]

This probably affects older versions of Flink as well, but that needs to be verified.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29048,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 13 03:24:28 UTC 2022,,,,,,,,,,"0|z17rgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 07:47;godfreyhe;[~renqs] could you have a look ?;;;","30/Aug/22 12:12;pnowojski;Why are we re-emitting idleness periodically? ;;;","31/Aug/22 09:46;renqs;[~pnowojski] There are two outputs in the source: per-split output and main output, and each of them has a copy of {{{}WatermarkGenerator{}}}:

[https://github.com/apache/flink/blob/c0f080762e7a1c1763942fed2e34420164a04bf3/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/source/ProgressiveTimestampsAndWatermarks.java#L102-L120]

{{WatermarksWithIdleness}} (which is an impl of {{{}WatermarkGenerator{}}}) periodically checks if there's any invocations of {{{}onEvent{}}}, and mark the watermark output as idle if the {{onEvent}} is never called with in the idleness timeout. For the source only uses per-split output instead of main output (like KafkaSource), the {{WatermarksWithIdleness}} on the main output is never touched so it keeps marking the watermark output as idle in {{{}onPeriodicEmit{}}}.

I'm not sure adding the main output to the watermark multiplexer in per-split output is a correct way to fix. From the current logic of {{ProgressiveTimestampsAndWatermarks}} the watermark from the main output should be directly sent to downstream instead of calculating min with watermarks from per-split output.

A possible solution in my mind is like we add another layer on WatermarkOutput of source that only mark idle if both main and per-split output are idle. WDYT?;;;","31/Aug/22 12:05;pnowojski;Thanks for the explanation
{quote}WatermarksWithIdleness (which is an impl of WatermarkGenerator) periodically checks if there's any invocations of onEvent, and mark the watermark output as idle if the onEvent is never called with in the idleness timeout.{quote}

Wouldn't it be more correct to not call {{output.markIdle();}} if previous {{onPeriodicEmit}} call has already called (i.e {{idlenessTimer.checkIfIdle()}} returned true in the previous call)?

{quote}A possible solution in my mind is like we add another layer on WatermarkOutput of source that only mark idle if both main and per-split output are idle. WDYT?{quote}
I think that sounds about right.

;;;","13/Sep/22 03:24;renqs;Fixed on master: d62df6899a1d7bd92ae998d3b610aa8b4de64505

release-1.16: a5be641a9a95de631dc697f3d2906e46cb126ee5

release-1.15: 2a99acde6cd9508da702474b69e3812805d17615;;;",,,,,,,,,,,,,,,,
Add doc for the API and Option of sql gateway rest endpoint,FLINK-28974,13476831,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,Wencong Liu,Wencong Liu,15/Aug/22 13:21,01/Sep/22 01:49,04/Jun/24 20:41,01/Sep/22 01:49,1.16.0,,,,1.16.0,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,,,Add document for the API and Option of sql gateway rest endpoint.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 01 01:49:57 UTC 2022,,,,,,,,,,"0|z17rco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 01:49;xtsong;master (1.16): 55902053a4cf6bf125f7d94c0cf397eaf915479b;;;",,,,,,,,,,,,,,,,,,,,
Extending /jars/:jarid/plan API to support setting Flink configs,FLINK-28973,13476825,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Zhanghao Chen,Zhanghao Chen,15/Aug/22 12:52,24/Nov/22 09:00,04/Jun/24 20:41,24/Nov/22 09:00,,,,,,,,,Runtime / REST,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27060,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 24 08:59:30 UTC 2022,,,,,,,,,,"0|z17rbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 08:03;gaoyunhaii;Hi [~Zhanghao Chen] may I have a double confirmation of the requirements: currently Flink contains two types of config options, namely the cluster-level and the job-level ones. The cluster level configurations are set on startup of the cluster and could not be changed on job submitting, while the job-level ones should be able to be changed via the programming APIs. ;;;","28/Oct/22 03:04;Zhanghao Chen;Hi [~gaoyunhaii]. The concern is totally valid, but as have been discussed in [FLIP-256: Support Job Dynamic Parameter With Flink Rest Api - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-256%3A+Support+Job+Dynamic+Parameter+With+Flink+Rest+Api], a pragmatic approach would just be allow to take both types of config options as input, and simply ignore the cluster-level ones, which is also consistent with how we handle it when submitting jobs.;;;","31/Oct/22 10:07;gaoyunhaii;Hi [~Zhanghao Chen] very thanks for the clarification! Is this issue a part of  https://issues.apache.org/jira/browse/FLINK-27060 ? If so we may make it to be a sub-issue of that FLINK-27060;;;","15/Nov/22 13:20;ConradJam;Hi [~gaoyunhaii] [~Zhanghao Chen] This fature is slove it in FLINK-27060;;;","24/Nov/22 08:59;gaoyunhaii;Very thanks [~ConradJam] for the update! [~Zhanghao Chen] I'll then first close this issue, If there are other issues let's reopen it. ;;;",,,,,,,,,,,,,,,,
Add methods of StartCursor and StopCursor to align the Java,FLINK-28972,13476817,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ana4,ana4,ana4,15/Aug/22 11:53,01/Sep/22 10:41,04/Jun/24 20:41,22/Aug/22 01:38,1.14.5,1.15.1,1.16.0,,1.16.0,,,,API / Python,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,,"Add fromPublishTime in the StartCursor class

Add afterEventTime and afterPublishTime in the StopCursor class",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27399,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 01 10:41:23 UTC 2022,,,,,,,,,,"0|z17r9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 11:54;ana4;I'll fix this soon;;;","22/Aug/22 01:39;dianfu;Merged to master via 77ea6638f150d1e261e6bd703a06282ebd2e9176;;;","01/Sep/22 10:41;syhily;[~ana4] We should also backport the PR to 1.15 release I think.;;;",,,,,,,,,,,,,,,,,,
Adds user documentation for the new LOOKUP hint,FLINK-28971,13476794,13474692,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,15/Aug/22 10:10,02/Sep/22 08:06,04/Jun/24 20:41,02/Sep/22 08:06,,,,,1.16.0,,,,Documentation,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 02 08:06:17 UTC 2022,,,,,,,,,,"0|z17r4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Sep/22 08:06;godfrey;Fixed in master: bbe969059b10f29567d9bed49c10fadeee8d0495;;;",,,,,,,,,,,,,,,,,,,,
Add the doc how to use Cython to optimize Python function,FLINK-28970,13476792,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hxbks2ks,hxbks2ks,15/Aug/22 09:53,15/Aug/22 09:53,04/Jun/24 20:41,,1.14.5,1.15.1,1.16.0,,,,,,API / Python,Documentation,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-15 09:53:57.0,,,,,,,,,,"0|z17r48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-229 Join Hints for Flink SQL Batch,FLINK-28969,13476791,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lincoln.86xy,xuyangzhong,xuyangzhong,15/Aug/22 09:52,02/Sep/22 13:52,04/Jun/24 20:41,02/Sep/22 13:52,1.16.0,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,,,"Join hints for Flink SQL batch is ready. Users can define the join hint they want to suggest the optimizer to use the customized join strategy.

I think we should verify:
 # Follow the doc to write a SQL with join hints and observe the final join strategy in the plan. The doc is preparing and you can refer this [pr|https://github.com/apache/flink/pull/20513] temporarily.
 # Test some unexpected behavior when writing the join hint (including but not limited to the conflicts on [doc|https://docs.google.com/document/d/19PL77ZggPIhz7FMZv2Yx7w_cnYyvmTc-mxUpJYv_934/edit#heading=h.uia01v8e78ov] or the  [pr|https://github.com/apache/flink/pull/20513]), and observe whether the join strategy in the plan is expected. For example:
 ** Multi same or different args in one join hint
 ** Multi same or different join hints
 ** Multi query blocks and join hints
 ** Unknown table names in join hints
 ** Complex SQL like union, minus, correlate, filter and etc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29113,FLINK-29119,FLINK-29120,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 29 01:59:14 UTC 2022,,,,,,,,,,"0|z17r40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 01:50;lincoln.86xy;[~xuyangzhong] related testing has been completed from my side, covering the syntax and supported hints mentioned by FLIP-229, plus testing the behavior on undefined CTE syntax, and jiras have been created for the found issues.;;;","29/Aug/22 01:59;xuyangzhong;Thanks for your testing,[~lincoln.86xy] . I'll try to fix them.;;;",,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-234 LOOKUP Hint,FLINK-28968,13476776,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xuyangzhong,lincoln.86xy,lincoln.86xy,15/Aug/22 08:20,02/Sep/22 14:04,04/Jun/24 20:41,02/Sep/22 14:04,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,release-testing,,,,,,"the user doc draft: [https://github.com/apache/flink/pull/20577]

The LOOKUP hint allows users to suggest the Flink optimizer to: 
1. use synchronous(sync) or asynchronous(async) lookup function
2. configure the async parameters
3. enable delayed retry strategy for lookup

we can verify each one and also some combinations of above features

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29112,FLINK-29111,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 02 14:04:07 UTC 2022,,,,,,,,,,"0|z17r0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 12:22;godfreyhe;Is there any progress on this issue ? [~xuyangzhong];;;","22/Aug/22 14:34;xuyangzhong;Hi, [~godfreyhe] , the testing is being carried out gradually and is expected to be completed no later than next week.;;;","26/Aug/22 06:24;xuyangzhong;Hi,[~lincoln.86xy]. The testing work is finished, and it seems that there are some confusion following:
 # Currently, only connector HBase support async lookup join(and it must add the argument ""lookup.async""). So I think it's better to note it in doc.
 # Would we better print the lookup join hint on the node `Correlate` in the origin RelNode tree? It can help the user to understand the behavior that the 'async' is missing in the Optimized RelNode tree because of planner ignores it when optimizing.;;;","26/Aug/22 07:21;lincoln.86xy;thanks [~xuyangzhong] for testing this!

> 1. Currently, only connector HBase support async lookup join(and it must add the argument ""lookup.async""). So I think it's better to note it in doc.

I checked the doc of hbase connector, the 'lookup.async' parameter is not conspicuous enough ([https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/hbase/#lookup-async),]  and It is not appropriate to specify the hbase connector params directly in the hint documentation. But as a followup work of FLIP-234, we plan to deprecate it later (in FLINK-29111)

> 2. Would we better print the lookup join hint on the node `Correlate` in the origin RelNode tree? It can help the user to understand the behavior that the 'async' is missing in the Optimized RelNode tree because of planner ignores it when optimizing.

good idea! it'll be more clear for user debuging, this will be done in FLINK-29112

 

thanks again!;;;","02/Sep/22 14:02;xuyangzhong;Hi, [~godfreyhe] . The test work is finished.;;;","02/Sep/22 14:04;godfrey;Thanks for your effort, I will close it.;;;",,,,,,,,,,,,,,,
Fix Hive multi-version support for Table Store,FLINK-28967,13476775,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,15/Aug/22 08:10,16/Aug/22 06:09,04/Jun/24 20:41,16/Aug/22 06:09,table-store-0.2.0,table-store-0.3.0,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,Currently table store Hive catalog and connector can only work for Hive 2.3. Support for Hive 2.1 and 2.2 should be fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 06:09:05 UTC 2022,,,,,,,,,,"0|z17r0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:09;lzljs3620320;master: 6652905c4c0ff7f2533353c2d7d3d6552eb978e3
release-0.2: cb5a24e4fc804df4ea746182ea28d03e1866971c;;;",,,,,,,,,,,,,,,,,,,,
Check backward compatibility against both 1.1.0 and 1.0.0 (all released versions),FLINK-28966,13476772,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,morhidi,gyfora,gyfora,15/Aug/22 07:52,24/Nov/22 01:02,04/Jun/24 20:41,22/Sep/22 20:39,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,We should not only check CRD compatibility against 1.0.0 but all released versions. We could also think of a way to add this check automatically when we release a new version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 22 20:39:28 UTC 2022,,,,,,,,,,"0|z17qzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/22 20:39;gyfora;merged to main f41025da98d21c4e8404b6554e9c3d6a6ce0a4fb;;;",,,,,,,,,,,,,,,,,,,,
Shouldn't create empty partition when it's for dynamic partition,FLINK-28965,13476759,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tartarus,luoyuxia,luoyuxia,15/Aug/22 06:24,19/Aug/22 04:06,04/Jun/24 20:41,19/Aug/22 04:06,1.16.0,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"Can be reproduced by the following code in HiveDialectTest:

 
{code:java}
tableEnv.executeSql(
        ""create table over1k_part_orc(\n""
                + ""           si smallint,\n""
                + ""           i int,\n""
                + ""           b bigint,\n""
                + ""           f float)\n""
                + ""       partitioned by (ds string, t tinyint) stored as orc"");
tableEnv.executeSql(
        ""create table over1k(\n""
                + ""           t tinyint,\n""
                + ""           si smallint,\n""
                + ""           i int,\n""
                + ""           b bigint,\n""
                + ""           f float,\n""
                + ""           d double,\n""
                + ""           bo boolean,\n""
                + ""           s string,\n""
                + ""           ts timestamp,\n""
                + ""           dec decimal(4,2),\n""
                + ""           bin binary)"");
tableEnv.executeSql(
                ""insert overwrite table over1k_part_orc partition(ds=\""foo\"", t)""
                        + "" select si,i,b,f,t from over1k where t is null or t=27 order by si"")
        .await(); {code}
 

Althogh it's for dynamic partition, the current code will try to create a partition for it (ds='foo') when there's no data wrotten to it , so the exception will be thrown since the partition spec (ds='foo') is not full path
{code:java}
Caused by: MetaException(message:Invalid partition key & values; keys [ds, t, ], values [foo, ]) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 04:06:16 UTC 2022,,,,,,,,,,"0|z17qww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 06:29;luoyuxia;[~tartarus] Could you please have a look?;;;","15/Aug/22 16:47;tartarus;[~luoyuxia]   thank for you raise this issue, I will fix it soon!;;;","19/Aug/22 04:06;jark;Fixed in master: e967a7b54da969c6d974a4ec6e3afcd1cc17a8be;;;",,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-205 Cache in DataStream for Batch Processing,FLINK-28964,13476758,13476068,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jessionX,xuannan,xuannan,15/Aug/22 06:15,05/Sep/22 01:53,04/Jun/24 20:41,31/Aug/22 01:53,,,,,1.16.0,,,,API / DataStream,,,,,,,0,release-testing,,,,,,"DataStream API provides the `cache` method to cache the result of a DataStream and reuse it in later jobs with batch execution mode.

I think we should verify:
 # Follow the doc to write a Flink job that produces cache and a job that consumes cache and submit it to a session cluster(standalone or yarn).
 # You can remove the source physically after the cache-producing job is finished to verify that the cache-consuming job is not reading from the source. For example, delete the file in the filesystem if you are using a file source. 
 # You can restart the TaskManager after the cache-producing job is finished to verify that the cache-consuming job will re-compute the result.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 05 01:53:49 UTC 2022,,,,,,,,,,"0|z17qwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 13:31;jessionX;Hi, I would like to take this release testing, please assign this ticket to me if you don't mind, thanks.;;;","16/Aug/22 02:39;hxbks2ks;Thanks [~jessionX] . I have assigned it to you. ;;;","22/Aug/22 12:23;godfreyhe;Is there any progress on this issue ? [~jessionX];;;","28/Aug/22 07:33;jessionX;I deployed a test code  on cluster  with standalone  and have a test, There is  no problems  util now;;;","30/Aug/22 02:08;xuannan;Thank [~jessionX] for your testing. I think you have run the test successfully. If there are no more problems till tomorrow, I'd like to close the ticket.;;;","05/Sep/22 01:53;jessionX;yes, I'm sure that you can close the ticket. [~xuannan] ;;;",,,,,,,,,,,,,,,
Add API compatibility test for sql gateway rest endpoint,FLINK-28963,13476757,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,Wencong Liu,Wencong Liu,15/Aug/22 05:42,18/Aug/22 13:18,04/Jun/24 20:41,18/Aug/22 13:18,1.16.0,,,,1.16.0,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,,,"Under the package {_}flink-runtime-web{_}, RestAPIStabilityTest performs compatibility checks on Rest API based on a series of CompatibilityRoutines. For Sql Gateway, its Rest Endpoint also needs to reuse the same rules to verify API compatibility, so as to ensure that all future modifications to the Sql Gateway Rest API are compatible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 18 13:18:51 UTC 2022,,,,,,,,,,"0|z17qwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 13:18;chesnay;master: b550e3f59d0e9cf06339bcf717050531aac5613d;;;",,,,,,,,,,,,,,,,,,,,
Constant memory usage increase,FLINK-28962,13476754,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhangdong7,zhangdong7,15/Aug/22 05:02,24/Apr/23 12:20,04/Jun/24 20:41,30/Aug/22 07:52,1.15.0,1.15.1,,,,,,,Connectors / Kafka,,,,,,,0,,,,,,,"Basically I'm running flink at the 1.15.1 version with docker  and often the application start to slow down because of OOM errors. 
It was observed that the memory continued to increase, and the number of threads continued to increase through the mertics data collected by Prometheus。
I tried to remove the sink kafka code and it looks normal,so I change the flink to 1.14.5 and it works fine.
Is this a bug?",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 25 03:26:25 UTC 2022,,,,,,,,,,"0|z17qvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 02:21;lixiang93;I have the same problem. When I replace the delivery guarantee EXACTLY_ONCE with AT_LEAST_ONCE, it looks normal. The speed of memory increase is related to the checkpoint interval and parallelism. ;;;","24/Aug/22 08:18;zhangdong7;here，you  can find answer  https://github.com/apache/flink/pull/20205;;;","25/Aug/22 03:26;lixiang93;thanks, I think this is the reason for the problem I'm having.:);;;",,,,,,,,,,,,,,,,,,
Add the doc how to support third party connector in Python DataStream API,FLINK-28961,13476751,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hxbks2ks,hxbks2ks,15/Aug/22 04:23,21/Aug/23 10:35,04/Jun/24 20:41,,1.14.5,1.15.1,1.16.0,,,,,,API / Python,Documentation,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 10:35:27 UTC 2023,,,,,,,,,,"0|z17qv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Pulsar throws java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement,FLINK-28960,13476746,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,syhily,syhily,syhily,15/Aug/22 03:49,18/Oct/22 13:16,04/Jun/24 20:41,18/Oct/22 13:16,1.14.6,1.15.1,,,1.14.7,1.15.3,1.16.1,1.17.0,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,,,"{code:java}
Unknown HK2 failure detected:
MultiException stack 1 of 2
java.lang.NoClassDefFoundError: javax/xml/bind/annotation/XmlElement
	at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.<init>(JaxbAnnotationIntrospector.java:137)
	at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.<init>(JaxbAnnotationIntrospector.java:124)
	at org.apache.pulsar.shade.com.fasterxml.jackson.module.jaxb.JaxbAnnotationIntrospector.<init>(JaxbAnnotationIntrospector.java:116)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at java.base/java.lang.Class.newInstance(Class.java:584)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 18 13:16:11 UTC 2022,,,,,,,,,,"0|z17qu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 13:16;tison;master via 40cfdda7895e9c70c37124bd9e1314d015fc1b78
1.16 via cbda786b5afb54f19c29ba2b4c95f4d80c3b35c3
1.15 via 962b6e041deb1da42090c3b3c22def744675fa8e
1.14 via 32326b6fb3053f5b0b57a33d69dfd19cae05ea8a;;;",,,,,,,,,,,,,,,,,,,,
504 gateway timeout when consume large number of topics using TopicPatten,FLINK-28959,13476744,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,15/Aug/22 03:46,16/Feb/23 10:15,04/Jun/24 20:41,16/Feb/23 09:46,pulsar-4.0.0,,,,pulsar-4.0.0,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,,,"Our situation is as follows:

* In a single namespace, more than 300 topics(partitioned-topic with a single partition) will report this error;
* Error still exists after resource expansion
* A flink client program consumes 30 to 50 topics per program. This error is bound to be reported after five consecutive programs","* flink-connector-pulsar: 1.15.0
* Pulsar 2.9.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 16 10:15:26 UTC 2023,,,,,,,,,,"0|z17qtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 09:45;Weijie Guo;main(4.0) via 64bc829c17b8686d35f66b274cb241cca51c7c1c;;;","16/Feb/23 09:49;Weijie Guo;[~martijnvisser] Sorry to bother you, but I want to ask whether we should using the flink-version or the connector-version for the affected version and the fixed version of the externalized connector in jira? I guess we should use the version of connector(i.e. 4.0) .;;;","16/Feb/23 10:05;martijnvisser;[~Weijie Guo] If it's an externalized connector, it should be the version of the connector indeed :);;;","16/Feb/23 10:15;Weijie Guo;Got it, thanks for your explanation~;;;",,,,,,,,,,,,,,,,,
flink dataStream to table  grows beyond 64 KB,FLINK-28958,13476743,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kcz,kcz,15/Aug/22 03:34,15/Aug/22 05:53,04/Jun/24 20:41,,1.14.4,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,,,"java entiy have 2000+ filed.i want ds to table.use sql query and insert into hive.

!image-2022-08-15-11-34-03-068.png!!image-2022-08-15-11-32-21-809.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 03:33;kcz;image-2022-08-15-11-32-21-809.png;https://issues.apache.org/jira/secure/attachment/13048093/image-2022-08-15-11-32-21-809.png","15/Aug/22 03:30;kcz;image-2022-08-15-11-34-03-068.png;https://issues.apache.org/jira/secure/attachment/13048092/image-2022-08-15-11-34-03-068.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 04:18:28 UTC 2022,,,,,,,,,,"0|z17qtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 04:18;lzljs3620320;The generated code in StructuredObjectConverter is not split by JavaCodeSplitter.split. We should add this.;;;",,,,,,,,,,,,,,,,,,,,
Optimize Python API doc,FLINK-28957,13476740,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxbks2ks,hxbks2ks,hxbks2ks,15/Aug/22 03:13,16/Nov/22 10:03,04/Jun/24 20:41,,1.14.5,1.15.1,1.16.0,,,,,,API / Python,Documentation,,,,,,0,,,,,,,"The overall style and structure of the current python api doc is not so convenient to new users to learn and use, we can use numpydoc or other document styles to optimize.",,,,,,,,,,,,,,,,FLINK-29966,FLINK-30040,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-15 03:13:55.0,,,,,,,,,,"0|z17qso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIx non-multi insert statement fall into multi insert logic  in Hive dialect,FLINK-28956,13476736,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,15/Aug/22 03:04,19/Aug/22 04:09,04/Jun/24 20:41,19/Aug/22 04:09,1.16.0,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"In [FLINK-27387|https://issues.apache.org/jira/browse/FLINK-27387], we support multi insert statment in Hive dialect,  but the check for  multi insert statment  is not strict which will result non-multi insert statement fall into such logic.

For example, 

 
{code:java}
with cte as (select t.a as a,t.a as b,t.a as c from t where t.b is null) select * from cte {code}
 

. The AST for it is [TOK_CTE, TOK_FROM, TOK_INSERT], then it 

will be mistaken as multi-insert statement

 
{code:java}
private boolean isMultiDestQuery(HiveParserASTNode astNode) {
    // Hive's multi dest insert will always be [FROM, INSERT+]
    // so, if it's children count is more than 2, it should be a multi-dest query
    return astNode.getChildCount() > 2;
} {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 04:09:22 UTC 2022,,,,,,,,,,"0|z17qrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 03:11;luoyuxia;To fix it, we need a more strict check.;;;","19/Aug/22 04:09;jark;Fixed in master: 682ec6a5d73222d5c88a86e9b4caa29ee47e0bc7;;;",,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase compile failed in hadoop3,FLINK-28955,13476732,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,hxbks2ks,hxbks2ks,15/Aug/22 02:27,16/Aug/22 08:20,04/Jun/24 20:41,16/Aug/22 08:20,1.16.0,,,,1.16.0,,,,Test Infrastructure,,,,,,,0,pull-request-available,test-stability,,,,,"It looks like we have not resolved the problem in hadoop3. This problem was not exposed before due to another test that failed to compile under hadoop3
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39951&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691

{code:java}
2022-08-14T00:36:43.9971481Z [ERROR] COMPILATION ERROR : 
2022-08-14T00:36:43.9972508Z [INFO] -------------------------------------------------------------
2022-08-14T00:36:43.9973460Z [ERROR] /__w/3/s/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNHighAvailabilityITCase.java:[53,31] package org.apache.curator.test does not exist
2022-08-14T00:36:43.9974493Z [ERROR] /__w/3/s/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YARNHighAvailabilityITCase.java:[106,20] cannot find symbol
2022-08-14T00:36:43.9975371Z   symbol:   class TestingServer
2022-08-14T00:36:43.9975818Z   location: class org.apache.flink.yarn.YARNHighAvailabilityITCase
2022-08-14T00:36:43.9976253Z [INFO] 2 errors 
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28713,,,,FLINK-28844,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 08:20:56 UTC 2022,,,,,,,,,,"0|z17qqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 02:29;hxbks2ks;[~chesnay] Could you help take a look?;;;","16/Aug/22 08:20;chesnay;master: a17ff2b9f65dafc6e58aae98a0937c78bf5fd492;;;",,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-223 HiveServer2 Endpoint,FLINK-28954,13476731,13476068,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Resolved,luoyuxia,fsk119,fsk119,15/Aug/22 02:21,06/Sep/22 10:10,04/Jun/24 20:41,06/Sep/22 10:10,1.16.0,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,release-testing,,,,,,"HiveServer2 Endpoint is ready to use in this version. I think we can verify:
 # We can start the SQL Gateway with HiveServer2 Endpoint
 # User is able to sumit SQL with Hive beeline
 # User is able to sumit SQL with DBeaver",,,,,,,,,,,,,,,,FLINK-29118,FLINK-29188,FLINK-29209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 06 10:10:21 UTC 2022,,,,,,,,,,"0|z17qqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 12:23;godfreyhe;Is there any progress on this issue ? [~luoyuxia];;;","30/Aug/22 02:02;luoyuxia;Just has done some simple tests. I think I still need some to test before close the issue. ;;;","06/Sep/22 10:08;luoyuxia;I has almost finished the verification for HiveServer2 Endpoint

1: start the sql gateway with HiveServer2 endpoint according to the documentation.

2: connect to HiveServer2 endpoint with beeline and run some sql randomly.

3: use Zeppelin to connect HiveServer2 endpoint and run some sql randomly.

4: use dolphinscheduler to schedule some sql jobs including create table/insert into table/query table via HiveServer2 endpoint 

5: use some BI tools to connect HiveServer2 endpoint. It works for Tableau/DataEase. But fail to HiveServer2 endpoint with SuperSet, FlineBI, Metabase, which is tracked by FLINK-29209;;;","06/Sep/22 10:10;luoyuxia;Closing it since the the verification has been finished.;;;",,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-91 SQL Gateway,FLINK-28953,13476730,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,junhe,fsk119,fsk119,15/Aug/22 02:18,07/Sep/22 06:47,04/Jun/24 20:41,07/Sep/22 06:47,1.16.0,,,,1.16.0,,,,Table SQL / Gateway,,,,,,,0,release-testing,,,,,,"SQL Gateway is ready to submit SQL with REST Endpoint. I think we should verify:
 # ./sql-gateway.sh start and stop the SQL Gateway and using REST API to submit a job
 # running SQL Gateway with standalone/yarn-session cluster
 # verify SELECT in batch/streaming mode:
 ## the RowKind is correct(+I/-UB/+UA/-D)
 ## fetch with token
 # verify add jar is ready in the gateway side, the downloaded resource will be cleaned after exits.",,,,,,,,,,,,,,,,FLINK-29184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 07 02:41:08 UTC 2022,,,,,,,,,,"0|z17qqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 03:09;Jun He;Hi, I have implemented an add jar syntax in our company's flink platform，so I'm very interested in researching the two different implementations . Can I take this ticket?;;;","22/Aug/22 10:07;godfreyhe;very glad to see you will take this issue. Is there any progress on this issue ? [~Jun He];;;","22/Aug/22 11:04;Jun He;hi，godfrey he, I'm waiting this ticket to be assinged.;;;","23/Aug/22 06:53;godfrey;[~junhe]  assign to you;;;","30/Aug/22 01:55;hxb;Hi [~junhe] ，any progress on the test ?;;;","30/Aug/22 02:05;fsk119;[~junhe] If  you have any questions, please connect to me. It's my pleasure to resolve your questions about the FLIP-91. 

 

BTW, I think we can communicate much quicker with the slack or dingtalk. Could you ping me when you are free?;;;","30/Aug/22 03:11;Jun He;Hi Xingbo Huang, about 30% progress.;;;","01/Sep/22 08:41;Jun He;I have tested the first three parts in standalone mode on my personal computer，and the test results are as expected，I'll test part four, namely add/remove jar later.;;;","01/Sep/22 09:04;Jun He;Hi, [~fsk119], when using BEGIN STATEMENT SET to execute multiple statements, it throws a *java.lang.IllegalArgumentException: only single statement supported* exception, does this is as expected? below is my rest request body:
{code:java}
curl -H ""Content-Type: application/json"" -XPOST 'http://localhost:8083/v1/sessions/07b330b0-33f6-4501-b2f7-98c407d229ff/statements' -d ""{\""statement\"": \""BEGIN STATEMENT SET;CREATE TABLE datagen (f_random_str STRING) WITH ('connector' = 'datagen','rows-per-second'='100','fields.f_random_str.length'='100');CREATE TABLE blackhole_table (f2 STRING) WITH ('connector' = 'blackhole');insert into blackhole_table select f_random_str as f2 from datagen; END\""}"" {code}
 ;;;","02/Sep/22 06:57;Jun He;[~fsk119] when I execute an ADD JAR statement, such as oss://bucket_name/xx.jar，the downloaded local jar is not deleted after the session closed, this conflicts with description in [Flip-91|https://cwiki.apache.org/confluence/display/FLINK/FLIP-91%3A+Support+SQL+Gateway];;;","02/Sep/22 09:04;fsk119;Thanks for your great work.

 

> BEGIN STATEMENT SET to execute multiple statements

We can add DML only in the `beginning statement set` now.

 

> the downloaded local jar is not deleted

Thanks for the report. It's a bug. I will open a ticket for this.

 

 ;;;","06/Sep/22 06:48;fsk119;[~junhe] hi. If we have no other problems, I think we can close this issue.;;;","07/Sep/22 02:41;Jun He;Hi, [~fsk119], there's no other problem found.;;;",,,,,,,,
Release Testing: Veify Hive dialect ,FLINK-28952,13476728,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Resolved,fsk119,luoyuxia,luoyuxia,15/Aug/22 01:35,08/Sep/22 12:30,04/Jun/24 20:41,06/Sep/22 06:48,1.16.0,,,,1.16.0,,,,Connectors / Hive,Table SQL / API,,,,,,0,release-testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29151,FLINK-29152,FLINK-29185,FLINK-29222,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 05 02:20:00 UTC 2022,,,,,,,,,,"0|z17qq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 10:06;godfreyhe;Is there any progress on this issue ?[~fsk119];;;","05/Sep/22 02:20;fsk119;I have tested the following cases:

1. CREATE table with location clause and using Avro format, insert/select/drop the table;
2. CREATE PARTITION TABLE and SHOW PARTITIONS statement.
3. CREATE FUNCTION using jar/add jar and CREATE temporary function/show functions. Use the registered functions in the QUERY.
4. SELECT with cube/rollup and compare the results with the hive
5. SELECT distribute by/sort by/cluster by and comparing the results with hive
6. CTE works
8. CREATE MACRO/USING MACRO/DROP MACRO works
9. LOAD DATA from the local system or hdfs and comparing the results ;;;",,,,,,,,,,,,,,,,,,,
Header in janino generated java files can merge with line numbers,FLINK-28951,13476726,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Aug/22 22:44,23/Aug/22 12:39,04/Jun/24 20:41,18/Aug/22 10:01,,,,,1.15.3,1.16.0,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,"Since Line numbers are generated only for debug output it should not be a big issue.
From the other side currently this behavior leads to not compiled code.
The suggestion is usage of one-line comments for header to prevent this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 18 10:01:07 UTC 2022,,,,,,,,,,"0|z17qpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 10:01;twalthr;Fixed in master: 22f574fc660e045cfde66be4e4e58bb5781a9ed7

Fixed in 1.15: 29ff78a898c476202c5c4d1d153e3aa3ac0b4855;;;",,,,,,,,,,,,,,,,,,,,
Disable HBase2 version check in hbase-default.xml,FLINK-28950,13476582,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xichaomin,xichaomin,12/Aug/22 14:10,19/Oct/23 22:04,04/Jun/24 20:41,,,,,,,,,,Connectors / HBase,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,"With the version check enabled, hbase connector can write to hbase 2.2.3 only. Disable the version check to support hbase 2.X. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 10:35:27 UTC 2023,,,,,,,,,,"0|z17ptk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Secondary job manager fails to retrieve savepoint,FLINK-28949,13476574,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tiagomsr,tiagomsr,12/Aug/22 13:21,01/Sep/22 09:37,04/Jun/24 20:41,,1.14.4,,,,,,,,Test Infrastructure,,,,,,,0,,,,,,,"After creating a savepoint, its information cannot be retrieved unless the API call is made to the active job manager.

Active job manager response:
{code:java}
>>> curl http://<active_ip>:8081/v1/jobs/<job_id>/savepoints/<savepoint_id> | jq 
{
  ""status"": {
    ""id"": ""COMPLETED""
  },
  ""operation"": {
    ""location"": ""file:/srv/flink/savepoints/<savepoint>""
  }
}{code}
Secondary job manager response:
{code:java}
>>> curl http://<secondary_ip>:8081/v1/jobs/<job_id>/savepoints/<savepoint_id> | jq 
{
  ""errors"": [
    ""org.apache.flink.runtime.rest.NotFoundException: Operation not found under key: org.apache.flink.runtime.rest.handler.job.AsynchronousJobOperationKey@e25522a4\n\tat org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers$StatusHandler.handleRequest(AbstractAsynchronousOperationHandlers.java:182)\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler.handleRequest(SavepointHandlers.java:219)\n\tat org.apache.flink.runtime.rest.handler.AbstractRestHandler.respondToRequest(AbstractRestHandler.java:83)\n\tat org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:195)\n\tat org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83)\n\tat java.base/java.util.Optional.ifPresent(Optional.java:183)\n\tat org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45)\n\tat org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80)\n\tat org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115)\n\tat org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94)\n\tat org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:238)\n\tat org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:71)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)\n\tat org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)\n\tat org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n\tat org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n\tat org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n\tat org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.flink.runtime.rest.handler.async.UnknownOperationKeyException: No ongoing operation for org.apache.flink.runtime.rest.handler.job.AsynchronousJobOperationKey@e25522a4\n\tat org.apache.flink.runtime.rest.handler.async.CompletedOperationCache.get(CompletedOperationCache.java:158)\n\tat org.apache.flink.runtime.rest.handler.async.AbstractAsynchronousOperationHandlers$StatusHandler.handleRequest(AbstractAsynchronousOperationHandlers.java:180)\n\t... 48 more\n""
  ]
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18312,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 01 09:37:10 UTC 2022,,,,,,,,,,"0|z17prs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 13:29;chesnay;This is fixed in 1.15.;;;","01/Sep/22 09:22;tiagomsr;Tried the same approach with a flink cluster on 1.15 but still got an error (new one) for the secondary job manager: 


{code:java}
❯❯ curl http://<secondary_ip>:8081/v1/jobs/<job_id>/savepoints/<savepoint_id>

{
  ""errors"": [
    ""org.apache.flink.runtime.rest.handler.RestHandlerException: Internal server error while retrieving status of savepoint operation with triggerId=8689c1f6c6cbe9fb5fef5962f4c808c2 for job <job_id>.\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.createInternalServerError(SavepointHandlers.java:352)\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers.access$000(SavepointHandlers.java:115)\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler.lambda$null$0(SavepointHandlers.java:311)\n\tat java.base/java.util.Optional.orElseGet(Optional.java:369)\n\tat org.apache.flink.runtime.rest.handler.job.savepoints.SavepointHandlers$SavepointStatusHandler.lambda$handleRequest$1(SavepointHandlers.java:309)\n\tat java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)\n\tat java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)\n\tat org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:252)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)\n\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1387)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n\tat org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n\tat java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)\n\tat org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)\n\tat akka.dispatch.OnComplete.internal(Future.scala:299)\n\tat akka.dispatch.OnComplete.internal(Future.scala:297)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)\n\tat org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)\n\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)\n\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)\n\tat akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)\n\tat akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:118)\n\tat akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:1144)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:537)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:535)\n\tat akka.remote.EndpointActor.aroundReceive(Endpoint.scala:540)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:548)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:231)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:243)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\nCaused by: org.apache.flink.runtime.rpc.akka.exceptions.AkkaRpcException: Failed to serialize the result for RPC call : getTriggeredSavepointStatus.\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.serializeRemoteResultAndVerifySize(AkkaRpcActor.java:405)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$sendAsyncResponse$2(AkkaRpcActor.java:361)\n\tat java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)\n\tat java.base/java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:946)\n\tat java.base/java.util.concurrent.CompletableFuture.handle(CompletableFuture.java:2266)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.sendAsyncResponse(AkkaRpcActor.java:353)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:320)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)\n\tat org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)\n\tat akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:123)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)\n\tat akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:537)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:535)\n\tat akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)\n\t... 10 more\nCaused by: java.io.NotSerializableException: org.apache.flink.runtime.rest.handler.async.OperationResult\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1185)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:349)\n\tat org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:632)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcSerializedValue.valueOf(AkkaRpcSerializedValue.java:66)\n\tat org.apache.flink.runtime.rpc.akka.AkkaRpcActor.serializeRemoteResultAndVerifySize(AkkaRpcActor.java:388)\n\t... 30 more\n""
  ]
}{code};;;","01/Sep/22 09:37;chesnay;If you're using 1.15.0 then you're running into FLINK-27933; in that case use 1.15.1 instead,;;;",,,,,,,,,,,,,,,,,,
Add more test coverage for lookup table full caching,FLINK-28948,13476549,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,smiralex,renqs,renqs,12/Aug/22 10:23,26/Sep/22 02:32,04/Jun/24 20:41,01/Sep/22 16:44,1.16.0,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,Currently there's only IT case for lookup table full caching. We need to add more test cases to guard the correctness of it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29405,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 01 16:44:17 UTC 2022,,,,,,,,,,"0|z17pm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 16:44;renqs;Merged to master: 20e00fd82846f7ed5129fcfe3862bc09cf2fdcc8;;;",,,,,,,,,,,,,,,,,,,,
Curator framework fails with NullPointerException,FLINK-28947,13476537,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Mynttinen,Mynttinen,12/Aug/22 09:42,19/Aug/22 10:51,04/Jun/24 20:41,19/Aug/22 10:51,1.15.1,,,,,,,,Runtime / Coordination,,,,,,,0,,,,,,,"I'm getting the following error in JobManager and as a result JobManager exits.
{code:java}
Aug 12 06:37:30 server_name java[173]: [2022-08-12 06:37:30,491] ERROR Background exception was not retry-able or retry gave up (org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl:733)
Aug 12 06:37:30 server_name java[173]: java.lang.NullPointerException: null
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.utils.Compatibility.getHostAddress(Compatibility.java:116) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:185) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:206) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:150) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:598) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]: [2022-08-12 06:37:30,493] ERROR Unhandled error in curator framework, error message: Background exception was not retry-able or retry gave up (org.apache.flink.runtime.util.ZooKeeperUtils:292)
Aug 12 06:37:30 server_name java[173]: java.lang.NullPointerException: null
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.utils.Compatibility.getHostAddress(Compatibility.java:116) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:185) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:206) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:150) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:598) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]: [2022-08-12 06:37:30,494] ERROR Fatal error occurred while executing the TaskManager. Shutting it down... (org.apache.flink.runtime.taskexecutor.TaskManagerRunner:427)
Aug 12 06:37:30 server_name java[173]: java.lang.NullPointerException: null
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.utils.Compatibility.getHostAddress(Compatibility.java:116) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString(EnsembleTracker.java:185) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.processConfigData(EnsembleTracker.java:206) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.access$300(EnsembleTracker.java:50) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker$2.processResult(EnsembleTracker.java:150) ~[flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult(GetConfigBuilderImpl.java:222) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:598) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
Aug 12 06:37:30 server_name java[173]:         at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510) [flink-shaded-zookeeper-3.5.9.jar:3.5.9-15.0]
{code}
Steps
 * Create three servers
 * Run Flink JobManager and TaskManager on all of them (let's call these A, B and C). Use ZooKeeper HA Services.
 * Everything works as expected
 * Add a new server (D).
 * Shutdown server C
 * This error can be seen on both servers A and D. I didn't check B and C.

This can be reproduced (apparently) with every execution.

I'm using Flink 1.15.1. Actually I'm migrating from 1.13.X to 1.15.X. I'm not totally sure whether this ever happens on 1.13.X, but it seems to _always_ happen on 1.15.1.

I looked using debugger what's going on in the JobManager:
{code:java}
main-EventThread[1] where
  [1] org.apache.flink.shaded.curator5.org.apache.curator.utils.Compatibility.getHostAddress (Compatibility.java:116)
  [2] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.configToConnectionString (EnsembleTracker.java:185)
  [3] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.processConfigData (EnsembleTracker.java:206)
  [4] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker.access$300 (EnsembleTracker.java:50)
  [5] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.EnsembleTracker$2.processResult (EnsembleTracker.java:150)
  [6] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback (CuratorFrameworkImpl.java:926)
  [7] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation (CuratorFrameworkImpl.java:683)
  [8] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation (WatcherRemovalFacade.java:152)
  [9] org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetConfigBuilderImpl$2.processResult (GetConfigBuilderImpl.java:222)
  [10] org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent (ClientCnxn.java:598)
  [11] org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run (ClientCnxn.java:510)
main-EventThread[1] dump address
 address = {
    holder: instance of java.net.InetSocketAddress$InetSocketAddressHolder(id=8302)
    serialVersionUID: 5076001401234631237
    serialPersistentFields: instance of java.io.ObjectStreamField[3] (id=8303)
    UNSAFE: instance of jdk.internal.misc.Unsafe(id=8304)
    FIELDS_OFFSET: 12
    java.net.SocketAddress.serialVersionUID: 5215720748342549866
}
main-EventThread[1] dump address.holder
 address.holder = {
    hostname: ""host_name_here""
    addr: null
    port: 2888
}
main-EventThread[1] print address.getAddress()
 address.getAddress() = null
{code}

(The hostname has been changed).

It can be seen that on line 116 of Compatibility.java (https://github.com/apache/curator/blob/d65669b64f003326c98843b32b997e3ffab1e442/curator-client/src/main/java/org/apache/curator/utils/Compatibility.java#L116) there's this

{code}
        return (address != null) ? address.getAddress().getHostAddress() : ""unknown"";
{code}

Here {{address.getAddress()}} returns {{null}} causing the crash.",,,,,,,,,,,,,,,,,,,,,,,,,,CURATOR-649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 10:51:08 UTC 2022,,,,,,,,,,"0|z17pjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 10:51;mapohl;Thanks for sharing this issue, [~Mynttinen]. This seems to be more of a curator/zookeeper issue rather than a Flink issue. I'm gonna close that one in favor of the linked CURATOR-649 for now.;;;",,,,,,,,,,,,,,,,,,,,
Remove RexSimplify together with calcite upgrade,FLINK-28946,13476533,13449408,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,12/Aug/22 09:18,17/Apr/23 21:47,04/Jun/24 20:41,02/Feb/23 09:09,,,,,,,,,Table SQL / API,,,,,,,0,,,,,,,"As mentioned in {{org.apache.calcite.rex.RexSimplify}} comment
{noformat}
* <p>Copied to fix Calcite 1.26 bugs, should be removed for the next Calcite upgrade.
 *
 * <p>Changes (line numbers are from the original RexSimplify file):
 *
 * <ol>
 *   <li>CALCITE-4364 & FLINK-19811: Line 1307, Line 1764, Line 2638 ~ Line 2656.
 *   <li>CALCITE-4446 & FLINK-22015: Line 2542 ~ Line 2548, Line 2614 ~ Line 2619.
 * </ol>
{noformat}
at https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/java/org/apache/calcite/rex/RexSimplify.java#L73-L78
both https://issues.apache.org/jira/browse/CALCITE-4364 and https://issues.apache.org/jira/browse/CALCITE-4446 are in 1.27+",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 02 09:09:01 UTC 2023,,,,,,,,,,"0|z17pio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:09;Sergey Nuyanzin;There is already a PR to fix it under https://issues.apache.org/jira/browse/FLINK-29237
so close it;;;",,,,,,,,,,,,,,,,,,,,
Several task IO metrics not displayed on web ui,FLINK-28945,13476515,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lincoln.86xy,lincoln.86xy,12/Aug/22 07:07,15/Aug/22 02:23,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,,,,,,,"There is feedback from the community user that neither the receive nor the sent metric is displayed when source sink tasks are chained together. 

!https://intranetproxy.alipay.com/skylark/lark/0/2022/png/13414/1658998667672-44c5f68e-27d7-4bc7-b187-97a7c58271b0.png!

Also, the received and sent metrics never displayed for source task

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/22 02:23;junhan;image-2022-08-15-10-22-59-964.png;https://issues.apache.org/jira/secure/attachment/13048091/image-2022-08-15-10-22-59-964.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 02:23:54 UTC 2022,,,,,,,,,,"0|z17peo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 02:23;junhan;From the web UI's perspective, the metrics displayed in the table is totally based on the REST API 'jobs/:jobId'. I suggest we should double-check the back-end computation logic behind this API. !image-2022-08-15-10-22-59-964.png|width=361,height=295!;;;",,,,,,,,,,,,,,,,,,,,
Optimize the Python Execution Mode Documentation,FLINK-28944,13476513,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,12/Aug/22 06:52,15/Aug/22 03:12,04/Jun/24 20:41,15/Aug/22 03:12,1.16.0,,,,1.16.0,,,,API / Python,Documentation,,,,,,0,pull-request-available,,,,,,https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/python_execution_mode/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 03:12:06 UTC 2022,,,,,,,,,,"0|z17pe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 03:12;dianfu;Merged to master via a62c5a35aa29014780bb5598facd0fbdf2db4426;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for MaxAbsScaler,FLINK-28943,13476512,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,12/Aug/22 06:48,24/Aug/22 03:14,04/Jun/24 20:41,23/Aug/22 06:34,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,Add Transformer and Estimator for MaxAbsScaler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 23 06:34:25 UTC 2022,,,,,,,,,,"0|z17pe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 06:34;zhangzp;Fixed on master via 3aa21751ddb1b561c44ec00ac5b43646ead6fcfd;;;",,,,,,,,,,,,,,,,,,,,
Deadlock may occurs when releasing readers for SortMergeResultPartition,FLINK-28942,13476490,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,12/Aug/22 04:26,12/Aug/22 09:53,04/Jun/24 20:41,12/Aug/22 09:52,1.16.0,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,"After adding the logic of recycling buffers in CompositeBuffer in https://issues.apache.org/jira/browse/FLINK-28373, when reading data and recycling buffers simultaneously, the deadlock between the lock of SortMergeResultPartition and the lock of SingleInputGate may occur.

In short, the deadlock may occur as follows.

1. SingleInputGate.getNextBufferOrEvent (SingleInputGate lock)

CompositeBuffer.getFullBufferData -> CompositeBuffer.recycleBuffer -> waiting for 

SortMergeResultPartition lock;

2. ResultPartitionManager.releasePartition (SortMergeResultPartition lock) -> 

SortMergeSubpartitionReader.notifyDataAvailable -> 

SingleInputGate.notifyChannelNonEmpty -> waiting for SingleInputGate lock.

The possibility of this deadlock is very small, but we should fix the bug as soon as possible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28373,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 09:52:33 UTC 2022,,,,,,,,,,"0|z17p94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 09:52;kevin.cyj;Merged into master via f2fb6b20ec493a3af3f19a6f69f25e26ed226dda;;;",,,,,,,,,,,,,,,,,,,,
Savepoint ignores MaxConcurrentCheckpoint limit in aligned checkpoint case,FLINK-28941,13476483,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,12/Aug/22 03:29,07/Sep/22 05:59,04/Jun/24 20:41,06/Sep/22 14:02,1.16.0,,,,1.16.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,,,"When the unaligned checkpoint is disabled, savepoints would be set as forced[1], which means they can ignore the maxConcurrentCheckpoint limit[2] and lead to the situation that there are more than maxConcurrentCheckpoint running simultaneously. 

This behavior is incompatible with OperatorCoordinatorHolder, which requires that there should be at most one pending checkpoint at a time. As a result, exceptions, as follows, might be thrown[3].


{code:java}
java.lang.IllegalStateException: Cannot mark for checkpoint 9, already marked for checkpoint 8
	at org.apache.flink.runtime.operators.coordination.SubtaskGatewayImpl.markForCheckpoint(SubtaskGatewayImpl.java:185) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinatorInternal$6(OperatorCoordinatorHolder.java:328) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at java.util.HashMap.forEach(HashMap.java:1289) ~[?:1.8.0_292]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.checkpointCoordinatorInternal(OperatorCoordinatorHolder.java:327) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.operators.coordination.OperatorCoordinatorHolder.lambda$checkpointCoordinator$0(OperatorCoordinatorHolder.java:243) ~[flink-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRunAsync$4(AkkaRpcActor.java:453) ~[classes/:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:453) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:218) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[classes/:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[classes/:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) [scala-library-2.12.7.jar:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) [scala-library-2.12.7.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [scala-library-2.12.7.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) [scala-library-2.12.7.jar:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [akka-actor_2.12-2.6.15.jar:2.6.15]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
{code}


[1] https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointRequestDecider.java#L160-L164
[2] https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L444-L449
[3] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39860&view=logs&j=219f6d90-20a2-5863-7c1b-c80377a1018f&t=20186858-1485-5059-c9c6-446952519524&s=ab6e269b-88b2-5ded-2544-4aa5b1124530",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28606,,FLINK-29217,,,,,,FLINK-28999,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 06 06:33:01 UTC 2022,,,,,,,,,,"0|z17p7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 11:06;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40044&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203
It is the failed case in connector test.;;;","16/Aug/22 11:07;hxbks2ks;Hi [~gaoyunhaii] , could you help take a look? Thx;;;","17/Aug/22 07:42;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40093&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37273;;;","25/Aug/22 03:00;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40340&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203]

 ;;;","25/Aug/22 03:41;gaoyunhaii;[~hxb] I'll have a look~;;;","30/Aug/22 07:53;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40505&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e;;;","01/Sep/22 01:50;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40556&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;","01/Sep/22 01:51;hxb;Hi [~gaoyunhaii] Is there any progress on this issue? ;;;","01/Sep/22 03:00;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40563&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=17015;;;","01/Sep/22 06:10;gaoyunhaii;Hi [~hxb] Sorry for the delay, I'm still confirming the issue, and I'll try to have it fixed inside this week. ;;;","02/Sep/22 08:46;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40616&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1];;;","02/Sep/22 09:05;gaoyunhaii;[~hxb] I have a double check with [~yunfengzhou] offline, that the modified exactly-once mechanism of the operator coordinator now relied on that no concurrent checkpoints, which is different from the previous implementation that only requires that no concurrent checkpoints in the trigger period. However, currently we could not ensure no concurrent checkpoints, since maxConcurrentCheckpoints is an open option to the users, even if we could have different thoughts for the forced checkpoints, it could not solve the issue if users have set maxConcurrentCheckpoints explicitly. Thus I think we may need to try to remove the dependency on the assumption that there is no concurrent checkpoints. ;;;","02/Sep/22 09:12;hxb;Thanks for the investigations [~gaoyunhaii] . +1 for removing the assumption that there is no concurrent checkpoints.;;;","06/Sep/22 06:33;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40713&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203;;;",,,,,,,
Release Testing: Verify FLIP-248 Dynamic Partition Prunning,FLINK-28940,13476479,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,zhuzh,godfreyhe,godfreyhe,12/Aug/22 03:08,30/Aug/22 05:11,04/Jun/24 20:41,29/Aug/22 09:14,1.16.0,,,,1.16.0,,,,Table SQL / Planner,Table SQL / Runtime,,,,,,0,release-testing,,,,,,"This issue aims to verify FLIP-248: https://cwiki.apache.org/confluence/display/FLINK/FLIP-248%3A+Introduce+dynamic+partition+pruning

We can verify it in SQL client after we build the flink-dist package.

1. create a partition table and a non-partition table (only hive connector is supported now, or we need write a new collector), and then insert some data
2. show the explain result for a join query, whose one side contains a partition table and other side is non-partition table with a filter, such as the example in the FLIP doc: select * from store_returns, date_dim where sr_returned_date_sk = d_date_sk and d_year = 2000. The explain result should contain `DynamicFilteringDataCollector` node.  We can also verify plan for the various variants of above query.
3. execute the above plan and verify the execution result. (the execution result should be same with the execution plan which disable dynamic filtering via table.optimizer.dynamic-filtering.enabled)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 05:11:40 UTC 2022,,,,,,,,,,"0|z17p6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 10:06;godfreyhe;Is there any progress on this issue ? [~zhuzh] ;;;","24/Aug/22 10:10;zhuzh;I'm working on it and it may need a bit more time.;;;","29/Aug/22 09:13;zhuzh;I have tested it and it looks good to me.
I used sql client to do the test, by connecting it to Hive. By running testing jobs, I can see that DPP is taking effect: the topology is modified to have a {{DynamicFilteringDataCollector}} and {{Order-Enforcer}}. By comparing the number of input records of the join operator, I can see that expected number of records are  truely filtered out in ahead. The job result is also as expected.
;;;","30/Aug/22 05:11;godfrey;[~zhuzh]  Thanks for your effort;;;",,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-241 ANALYZE TABLE,FLINK-28939,13476478,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,337361684@qq.com,godfreyhe,godfreyhe,12/Aug/22 03:08,30/Aug/22 02:04,04/Jun/24 20:41,30/Aug/22 02:04,1.16.0,,,,1.16.0,,,,Table SQL / API,Table SQL / Planner,,,,,,0,release-testing,,,,,,"This issue aims to verify FLIP-240: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217386481

We can verify it in SQL client after we build the flink-dist package. 

1. create a partition table and a non-partition table (with/without compute column/metadata column, with different columns), and then insert some data
2. verify the different statements, please refer to the FLIP doc examples
3. verify the result in catalog. Currently, {{describe extended}} statement does not support show the statistics in catalog, we should write some code to get the statistics from catalog, or we can use hive cli if the catalog is hive catalog
4. verify the unsupported cases,
4.1  analyze non-existed table
4.2 analyze view
4.3 analyze a partition table with non-existed partition
4.4. analyze a non-partition table with a partition
4.5. analyze a non-existed column
4.6. analyze a computed column
4.6. analyze a metadata column
",,,,,,,,,,HIVE-26492,,,,,,,,,FLINK-29059,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 26 03:40:39 UTC 2022,,,,,,,,,,"0|z17p6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/22 09:56;337361684@qq.com;Hi, [~godfreyhe] , I have tested `analyze table` based on the examples of [pr docs|[https://github.com/apache/flink/pull/20506].] Almost all cases met the expectations, but the following problems were founded:
 # When String/varchar type column `a` have `null` value, `Analyze table xxx FOR ALL COLUMNS/ COLUMNS a` may throw error:
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.thrift.protocol.TProtocolException: Required field 'maxColLen' is unset! Struct:StringColumnStatsData(maxColLen:0, avgColLen:0.0, numNulls:1, numDVs:0) {code}

 #  If there are three columns named `a, b, c` with column stats already exists,  I just analyze column `a` using `Analyze table xxx FOR COLUMNS a`, the existing column stats of `b, c` will be reset back to empty. (Is this in line with expectations ?)
 # For partition table, If I use hive catalog, after `Analyze table xxx  FOR ALL COLUMNS`, the result of hive statement `desc formatted orders amount;`(does't specify partition) is wrong. I think the reason is that we don't have the column stats merge logical in this FLIP like 'FlinkRecomputeStatisticsProgram' and write to catalog.
 # When an error of `1.` is thrown, I find that some of the column stats are successfully written to catalog and some of it is not. Is this in line with expectations? Will this generate incorrect column stats？

The above are the problems I found. Screenshots of some successful examples will be sent out by me after these problems are fixed. Thank you for your contribution [~godfreyhe] .;;;","22/Aug/22 10:04;godfreyhe;[~337361684@qq.com] good catch, would you like to fix it ? ;;;","22/Aug/22 10:16;337361684@qq.com;[~godfreyhe]  Ok, I will create the relevant Jira and fix it.;;;","26/Aug/22 03:40;337361684@qq.com;Hi, [~godfreyhe] , I have finished the tests, fixed the errors found and added link to the corresponding hive issue.;;;",,,,,,,,,,,,,,,,,
Fix HiveServer2 Endpoint can not set variable correctly,FLINK-28938,13476477,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,fsk119,fsk119,12/Aug/22 02:58,31/Aug/22 11:26,04/Jun/24 20:41,31/Aug/22 11:26,1.16.0,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,"Hive JDBC URL also supports Hive variable replacement. But the current implementation doesn't finish this.  

 

HiveServer2 Endoint should thorw exception to notify users if users tries to fetch logs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 31 11:26:39 UTC 2022,,,,,,,,,,"0|z17p6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Aug/22 11:26;fsk119;Merged into master:

549d4327cf4ae9646f74a1da561dcebecd3d47ff

6630ce7d6fcb118900a19bc0bb9143b3d388bf1a;;;",,,,,,,,,,,,,,,,,,,,
Add doc for SqlGateway,FLINK-28937,13476474,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Duplicate,,fsk119,fsk119,12/Aug/22 02:38,14/Sep/22 19:00,04/Jun/24 20:41,14/Sep/22 19:00,1.16.0,,,,1.16.0,,,,Documentation,Table SQL / Gateway,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-12 02:38:22.0,,,,,,,,,,"0|z17p5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix RSET endpoint can not serialize CHAR(0),FLINK-28936,13476472,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,fsk119,fsk119,12/Aug/22 02:33,25/Aug/22 03:35,04/Jun/24 20:41,25/Aug/22 03:35,1.16.0,,,,1.16.0,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,,,The current implementation doesn't align with the FLIP. We need to introduce a new serializer to fix this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 25 03:35:25 UTC 2022,,,,,,,,,,"0|z17p5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 03:35;fsk119;Merged into master: 64f11ee95499af94d9ca3c0ce86091ab9d0ba9fa;;;",,,,,,,,,,,,,,,,,,,,
"Calcite -test.jar are no longer published, use calcite-testkit instead",FLINK-28935,13476442,13449408,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,11/Aug/22 19:53,02/Feb/23 09:11,04/Jun/24 20:41,02/Feb/23 09:11,,,,,1.17.0,,,,,,,,,,,0,,,,,,,The change on Calcite's side was done in 1.28 within https://issues.apache.org/jira/browse/CALCITE-4821,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21239,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 02 09:11:30 UTC 2023,,,,,,,,,,"0|z17oyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:11;Sergey Nuyanzin;The problem was solved within https://issues.apache.org/jira/browse/FLINK-21239;;;",,,,,,,,,,,,,,,,,,,,
Pulsar Source put all the splits to only one parallelism when using Exclusive subscription,FLINK-28934,13476406,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,11/Aug/22 15:29,16/Sep/22 01:38,04/Jun/24 20:41,16/Sep/22 01:38,1.14.5,1.15.1,1.16.0,,1.14.6,1.15.3,1.16.0,,Connectors / Pulsar,,,,,,,1,bug,pull-request-available,stale-critical,test-stability,,," !image-2022-08-11-23-27-04-268.png|width=500px!

The image here shows if we start a Flink application with four parallelism and four splits. All the splits would be sent to the first added reader. This is because we don't assign splits by pre-divide splits according to the size of parallelism. The readers are added to the enumerator one by one in the first bootstrap.",,,,,,,,,,,,,,,,,,,,FLINK-27388,FLINK-27611,FLINK-27400,FLINK-28084,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/22 15:27;syhily;image-2022-08-11-23-27-04-268.png;https://issues.apache.org/jira/secure/attachment/13048023/image-2022-08-11-23-27-04-268.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 16 01:38:45 UTC 2022,,,,,,,,,,"0|z17oqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","16/Sep/22 01:38;tison;https://github.com/apache/flink/pull/20725;;;",,,,,,,,,,,,,,,,,,,
Support ON CONFLICT DO NOTHING in flink-jdbc-connector for Postgresql dialect,FLINK-28933,13476404,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,nir.tsruya,nir.tsruya,11/Aug/22 15:12,11/Aug/22 15:13,04/Jun/24 20:41,,1.15.1,,,,,,,,Connectors / JDBC,,,,,,,0,,,,,,,"As  a flink user I would like to have the option for the flink-jdbc-connector for the postgresql dialect to configure 
{code:java}
ON CONFLICT DO NOTHING{code}
Currently it is only possible to configure an update clause",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-11 15:12:36.0,,,,,,,,,,"0|z17oq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Builder#withConfiguration instead of deprecated EnvironmentSettings#fromConfiguration,FLINK-28932,13476395,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,11/Aug/22 14:19,12/Aug/22 13:09,04/Jun/24 20:41,12/Aug/22 13:09,,,,,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 13:09:15 UTC 2022,,,,,,,,,,"0|z17oo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 13:09;chesnay;master: ea3b44183a876e2154f9a3a1423dee688ba4eb0e;;;",,,,,,,,,,,,,,,,,,,,
BlockingPartitionBenchmark doesn't compile,FLINK-28931,13476389,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,11/Aug/22 13:20,12/Aug/22 13:05,04/Jun/24 20:41,12/Aug/22 08:11,1.16.0,,,,1.16.0,,,,Benchmarks,,,,,,,0,pull-request-available,,,,,,"{code}
10:15:12  [ERROR] /home/jenkins/workspace/flink-master-benchmarks-java8/flink-benchmarks/src/main/java/org/apache/flink/benchmark/BlockingPartitionBenchmark.java:117:50:  error: cannot find symbol
{code}

Caused by
https://github.com/apache/flink/commit/9f5d0c48f198ff69a175f630832687ba02cf4c3e#diff-f72e79ebd747b6fde91988d65de9121a5907c97e4630cb1e30ab65601b4d9753R79",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28781,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 13:05:21 UTC 2022,,,,,,,,,,"0|z17omw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 08:11;roman;Merged as 4726194e7e73a39a78b102a4ce2b4ece4d338ece.;;;","12/Aug/22 12:55;roman;I don't think this was a technical debt because a major performance regression in Flink would be a bug; and this issue prevented any  preformance checks.;;;","12/Aug/22 13:05;chesnay;To me a bug has to directly impact the user experience in some way, which this issue did not.
I will readily admit that I attribute a lot of things to technical debt; pretty much anything that users don't see, in an attempt to make the release notes more useful.

However, I'd argue that according to your definition anything that impacts the development can not be technical debt, but technical debt is usually defined as exactly that. It would reduce the category to tickets like ""removing unused method"", which can't be the goal, because it'd be rather pointless.
;;;",,,,,,,,,,,,,,,,,,
"Add ""What is Flink Table Store?"" link to flink website",FLINK-28930,13476384,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,11/Aug/22 12:41,15/Aug/22 03:33,04/Jun/24 20:41,12/Aug/22 02:47,,,,,,,,,Project Website,Table Store,,,,,,0,pull-request-available,,,,,,"Similar to statefun and ml projects we should also add a ""What is Flink Table Store?"" link to the menu pointing to the doc site. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 02:47:16 UTC 2022,,,,,,,,,,"0|z17ols:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 02:47;lzljs3620320;asf-site: 
8793e3e659635364153c617d957cf7cd9cd1270d
8ccc97cfc884c5ad71924eb4f242017bbe29f642;;;",,,,,,,,,,,,,,,,,,,,
Add built-in datediff function.,FLINK-28929,13476383,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,11/Aug/22 12:26,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,1.20.0,,,,Table SQL / API,,,,,,,0,pull-request-available,stale-assigned,,,,,"Syntax:
{code:java}
DATEDIFF(expr1,expr2){code}
Returns:

returns _{{expr1}}_ − _{{expr2}}_ expressed as a value in days from one date to the other. _{{expr1}}_ and _{{expr2}}_ are date or date-and-time expressions. Only the date parts of the values are used in the calculation.

This function returns {{NULL}} if _{{expr1}}_ or _{{expr2}}_ is {{{}NULL{}}}.

Examples:
{code:java}
> SELECT DATEDIFF('2007-12-31 23:59:59','2007-12-30');
        -> 1
> SELECT DATEDIFF('2010-11-30 23:59:59','2010-12-31');
        -> -31{code}
See more:
 * mysql: [https://dev.mysql.com/doc/refman/8.0/en/date-and-time-functions.html#function_datediff]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 10:35:12 UTC 2023,,,,,,,,,,"0|z17olk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Add E2E test for hybrid shuffle mode,FLINK-28928,13476380,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Aug/22 12:11,14/Sep/22 08:18,04/Jun/24 20:41,14/Sep/22 08:18,1.16.0,,,,1.16.0,,,,Tests,,,,,,,0,pull-request-available,,,,,,Add E2E IT cases for hybrid shuffle mode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 14 08:18:26 UTC 2022,,,,,,,,,,"0|z17okw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 08:18;xtsong;- master (1.17): 8de6f9208fc2780d9861941bad99f6e1f845540b
- release-1.16: 00855438f2ee7ef4027076553357552157eb452f;;;",,,,,,,,,,,,,,,,,,,,
Can not clean up the uploaded shared files when the checkpoint fails,FLINK-28927,13476366,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ChangjiGuo,ChangjiGuo,ChangjiGuo,11/Aug/22 10:28,13/Oct/22 11:19,04/Jun/24 20:41,,1.11.6,1.15.1,,,,,,,Runtime / State Backends,,,,,,,0,,,,,,,"If a checkpoint times out, the task will cancel all snapshot futures and do some cleanup work, including the following:
 * Cancel all AsyncSnapshotTasks.
 * If the future has finished, it will clean up all state object.
 * If the future has not completed, it will be interrupted(maybe).
 * Close snapshotCloseableRegistry.

In my case, the thread was interrupted while waiting for the file upload to complete, but the file was not cleaned up.

RocksDBStateUploader.java
{code:java}
FutureUtils.waitForAll(futures.values()).get();
{code}
It will wait for all files to be uploaded here. Although it has been interrupted, the uploaded files will not be cleaned up. The remaining files are mainly divided into:
 * Files that have finished uploading before the thread is canceled.
 * outputStream.closeAndGetHandle() is called, but snapshotCloseableRegistry has not been closed.

How to reproduce?
Shorten the checkpoint timeout time, making the checkpoint fail. Then check if there are any files in the shared directory.

I'm testing on Flink-1.11, but I found the code from the latest branch may have the same problem. I tried to fix it.
 
 ",Flink-1.11,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 13 11:19:11 UTC 2022,,,,,,,,,,"0|z17ohs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 12:49;ChangjiGuo;Can anyone take a look? :)
 ;;;","14/Aug/22 09:43;yunta;[~ChangjiGuo] could you share the design of how to solve this problem?;;;","15/Aug/22 03:53;ChangjiGuo;[~yunta] Thanks for your replay. In my first solution, I caught the exception while waiting for the upload to complete and set a callback for each futures. It could delete most of the remaining files. But I found that there are some threads that are not interrupted normally, the files will upload complete and the solution above doesn't work well. 

In this method([https://github.com/apache/flink/blob/master/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L265]), the completed variable will be set true, and uploaded file is also not cleaned up. I'm still testing for this.

In short, interrupting a thread has some unexpected result. What do you think, looking forward to your reply. [~yunta]
 ;;;","16/Aug/22 05:10;ChangjiGuo;I also found another problem when debugging the code.

See [here|https://issues.apache.org/jira/browse/FLINK-28984]

[~yunta] Can you assign these two issues to me? Thanks!
 
 ;;;","16/Aug/22 06:10;yunta;[~ChangjiGuo] It seems you did not give a clear idea of how to solve the problem of cleaning up interrupted uploaded files. I could assign the task to you first, and hope you could give clear design then.;;;","16/Aug/22 11:26;ChangjiGuo;[~yunta] Thanks for your reply! I sorted out my thoughts, we can clean up uploaded files in two ways:
 # For being interrupted or getting a unexpected exception while waiting for the future to complete, we can catch exception, and then set a callback for each future and discard the stream state handle(as mentioned above).
 # For uninterrupted case, that is, the files have been uploaded, but the AsyncSnapshotTask is still running when canceled. First, we need to collect all uploaded files, which can be cleaned up according to whether the AsyncSnapshotTask is canceled. But there is a uncertainty problem that we cannot ensure the order of getting value of {_}isCancelled(){_}(if true, we will delete these files) and calling _stateFuture.cancel(true)._ That is to say, we can't handle it in _FutureTask#run, and_ have to wait until the run method finishes. I have an idea that we can override the _FutureTask#set_ method and check if it can be discarded.
{code:java}
public class AsyncSnapshotTask extends FutureTask<T> {

    @Override
    protected void set(T t) {
        super.set(t);
        if (isCancelled()) {
            if (t instanceof SnapshotResult) {
                try {
                    ((SnapshotResult<?>) t).discardState();
                } catch (Exception e) {
                    LOG.warn(""clean this occured error"", e);
                }
            }
        } else {
            // super.set(t) has modified internal state, cancel will return false, StateUtil#discardStateFuture will clean it.
        }
    }
}{code}

I pasted part of the code of the _RocksDBIncrementalSnapshotOperation#get_ as follows:
{code:java}
@Override
public SnapshotResult<KeyedStateHandle> get(CloseableRegistry snapshotCloseableRegistry)
        throws Exception {

    boolean completed = false;
    ....

    final Map<StateHandleID, StreamStateHandle> sstFiles = new HashMap<>();
    // Handles to the misc files in the current snapshot will go here
    final Map<StateHandleID, StreamStateHandle> miscFiles = new HashMap<>();

    try {
        ....
        ....

        uploadSstFiles(sstFiles, miscFiles, snapshotCloseableRegistry);

        ....
        ....

        completed = true;

        return snapshotResult;
    } finally {
        if (!completed) {
            final List<StateObject> statesToDiscard =
                    new ArrayList<>(1 + miscFiles.size() + sstFiles.size());
            statesToDiscard.add(metaStateHandle);
            statesToDiscard.addAll(miscFiles.values());
            statesToDiscard.addAll(sstFiles.values());
            cleanupIncompleteSnapshot(statesToDiscard);
        }
    }{code}
Here are some cases I can think of:
 # Interrupted while uploading sst files, and no misc file uploaded yet, the first way can clean up uploaded sst files and delete the file while waiting for the unfinished future to complete.
 # Interrupted while uploading misc files, _sstFiles_ already contains all uploaded sst files and it will be cleaned because of completed is false. The first way also can clean up uploaded misc files and delete the file while waiting for the unfinished future to complete too.
 # Both sst files and misc files have been uploaded, but the AsyncSnapshotTasks is cancelled(can't actually be interrupted), the _FutureTask#run_ will execute normally. The second way can clean up all state files.

I'm Looking forward for your reply! Thx.
 
 ;;;","29/Aug/22 13:36;ChangjiGuo;Hi [~yunta]! I verified the above idea on Flink-1.15.1 and tested the case of continuous checkpoint times out, it can clean up the files that TMs have not reported to the JM, and the remaining files will not be cleaned up by JM because of issue: FLINK-24611.;;;","13/Oct/22 11:19;ChangjiGuo;Hi, [~yunta]! Can you help me review this idea in your free time? Thanks.;;;",,,,,,,,,,,,,
Release Testing: Verify flip-235 hybrid shuffle mode,FLINK-28926,13476365,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Yu Chen,Weijie Guo,Weijie Guo,11/Aug/22 10:19,05/Sep/22 08:19,04/Jun/24 20:41,05/Sep/22 08:19,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,release-testing,,,,,,"* Please refer to release note of  FLINK-27862 for a list of changes need to be verified.
 * Please refer to out document for more details [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/batch_shuffle|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/batch_shuffle/]
 * Hybrid shuffle have some known limitations: No support for Slot Sharing, Adaptive Batch Scheduler and Speculative Execution. Please make sure you do not using this features in testing.
 * The changes should be verified only in batch execution mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 05 08:18:50 UTC 2022,,,,,,,,,,"0|z17ohk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 03:06;Yu Chen;Hi, I would like to take this release testing, please assign this ticket to me if you don't mind, thanks.;;;","16/Aug/22 03:13;Weijie Guo;[~Yu Chen] Thank you for your interest. Yes, you can take over the test. It should be noted that there are still some known problems that have not been completely fixed. For example, compression is not supported at present. If you encounter problems in the test, feel free to contact me~;;;","29/Aug/22 03:28;hxb;Hi [~Yu Chen]  Any progress on the test? ;;;","30/Aug/22 03:16;Yu Chen;Hi [~hxb] [~Weijie Guo] , I have finished the setup work of the tests and the overall progress is about 30%. And there are no problems found yet, if I find any, I will contact you in time. FYI.;;;","05/Sep/22 08:06;Yu Chen;Hi, all.

According to the docs of [Batch Shuffle|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/batch_shuffle/], I have tested the feature in a standalone cluster with 1JM+2TM(2 slots/TM) locally and submitted the {{WordCount}} example job to the cluster in batch mode with the following command respectively：
{code:sh}
./bin/flink run -Dexecution.batch-shuffle-mode=ALL_EXCHANGES_BLOCKING -Dparallelism.default=2 --detached examples/streaming/WordCount.jar --input /tmp/wordcount.txt --output /tmp/wordcount_res
./bin/flink run -Dexecution.batch-shuffle-mode=ALL_EXCHANGES_HYBRID_FULL -Dparallelism.default=2 --detached examples/streaming/WordCount.jar --input /tmp/wordcount.txt --output /tmp/wordcount_res
./bin/flink run -Dexecution.batch-shuffle-mode=ALL_EXCHANGES_HYBRID_SELECTIVE -Dparallelism.default=2 --detached examples/streaming/WordCount.jar --input /tmp/wordcount.txt --output /tmp/wordcount_res 
{code}
Note that {{/tmp/wordcount.txt}} contains {{10,000,000}} random words generated by {{{}RandomStringUtils.randomAlphabetic(10){}}}.

Through the Flink WEB, I verified the following scenarios:
 # {{HYBRID SHUFFLE}} can utilize all four slots, while {{BLOCK SHUFFLE}} uses only two slots, which is consistent with the relevant description in the document.
 # By examining the timeline chart of the job, we can see that {{HYBRID SHUFFLE}} makes the upstream task and the downstream task in the {{WordCount}} job start simultaneously.
 # I also restart the cluster with the configuration `{{{}jobmanager.scheduler: AdaptiveBatch{}}}` in the {{{}flink-conf.yaml{}}}. The job configured with {{ALL_EXCHANGES_HYBRID_FULL}} and {{ALL_EXCHANGES_HYBRID_SELECTIVE}} produced an error consistent with the documentation's description of limitation.

Overall, I have not found any problems with this feature, please feel free to contact me if any other cases need to be tested.;;;","05/Sep/22 08:14;Weijie Guo;[~Yu Chen], Thank you for your timely and careful test, I have no other concerned.

cc [~hxb] ;;;","05/Sep/22 08:18;hxb;Thanks [~Yu Chen]  and [~Weijie Guo] for the great work.;;;",,,,,,,,,,,,,,
Fix the concurrency problem in hybrid shuffle,FLINK-28925,13476362,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Aug/22 10:07,17/Aug/22 03:08,04/Jun/24 20:41,17/Aug/22 03:08,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Through tpc-ds testing and code analysis, I found some thread unsafe problems in hybrid shuffle:
 # HsSubpartitionMemeoryDataManager#consumeBuffer should return a readOnlySlice buffer to downstream instead of original buffer: If the spilling thread is processing while  downstream task is consuming the same buffer, the amount of data written to the disk will be smaller than the actual value. To solve this, we should let the consuming thread and the spilling thread share the same data but not index.
 # HsSubpartitionMemoryDataManager#releaseSubpartitionBuffers should ignore the release decision if the buffer already removed from bufferIndexToContexts instead of throw an exception. It should be pointed out that although the actual release operation is synchronous, a double release can still happen. The reason is that non-global decisions do not need to be synchronized. In other words, the main task thread and the consumer thread may decide to release a buffer at the same time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 03:08:56 UTC 2022,,,,,,,,,,"0|z17ogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 06:03;godfreyhe;[~Weijie Guo] would you like to fix it ?;;;","16/Aug/22 06:58;Weijie Guo;[~godfreyhe], Yes I will fix this.;;;","17/Aug/22 03:08;xtsong;master (1.16): 7ed817f2054a13c3e2754c37f7681d8fbdba4b41;;;",,,,,,,,,,,,,,,,,,
"Translate ""LIMIT clause"" page into Chinese",FLINK-28924,13476359,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liu zhuang,liu zhuang,liu zhuang,11/Aug/22 09:54,17/Aug/22 09:07,04/Jun/24 20:41,17/Aug/22 09:07,1.15.1,,,,1.16.0,,,,chinese-translation,,,,,,,0,pull-request-available,,,,,,"Translate ""LIMIT clause"" page into Chinese: [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/limit/.|https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/limit/]

The markdown file is located in ""docs/content.zh/docs/dev/table/sql/queries/limit.md"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 09:07:53 UTC 2022,,,,,,,,,,"0|z17og8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 09:07;jark;Fixed in master: ae7f325a358574be6ce1e3c37aabff71214c857b;;;",,,,,,,,,,,,,,,,,,,,
Release Testing: Verify flip-241 completed jobs information enhancement ,FLINK-28923,13476349,13476068,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,Weijie Guo,xtsong,xtsong,11/Aug/22 09:21,28/Aug/22 05:07,04/Jun/24 20:41,28/Aug/22 05:07,1.16.0,,,,1.16.0,,,,Runtime / Web Frontend,,,,,,,0,release-testing,,,,,,"* Please refer to release note of FLINK-28307 for a list of changes to be verified
* The changes should be verified for both JobManager and HistoryServer
* I'd suggest to verify the history server functionality for various deployment modes. Previously, it didn't work with application mode (FLINK-19358).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 23 06:41:57 UTC 2022,,,,,,,,,,"0|z17oe0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 06:41;Weijie Guo;[~xtsong] Thank you for such a good job, I have manually tested all changes list in FLINK-28307 , everything look good to me.
In more detail, I verified both the running UI and the history server UI, and I also verified this in standalone, per job and application modes respectively (as you specifically pointed out, HA is enabled), Everything is working properly. ;;;",,,,,,,,,,,,,,,,,,,,
"Translate ""ORDER BY clause"" page into Chinese ",FLINK-28922,13476347,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liu zhuang,liu zhuang,liu zhuang,11/Aug/22 09:19,17/Aug/22 09:07,04/Jun/24 20:41,17/Aug/22 09:07,1.15.1,,,,1.16.0,,,,chinese-translation,Documentation,,,,,,0,pull-request-available,,,,,,"Translate ""ORDER BY clause"" page into Chinese.

The markdown file is located in ""docs/content.zh/docs/dev/table/sql/queries/orderby.md"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 09:07:12 UTC 2022,,,,,,,,,,"0|z17odk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 09:07;jark;Fixed in master: 3930014433915af0ea9d16714f21a6b55aa6dd61;;;",,,,,,,,,,,,,,,,,,,,
Optimize the Python DataStream Window Documentation,FLINK-28921,13476330,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,11/Aug/22 08:07,12/Aug/22 06:45,04/Jun/24 20:41,12/Aug/22 06:45,1.16.0,,,,1.16.0,,,,API / Python,Documentation,,,,,,0,pull-request-available,,,,,,"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 06:45:03 UTC 2022,,,,,,,,,,"0|z17o9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 06:45;hxbks2ks;Merged into master via 5fb8d24c1c369ab33ffe807d76e54a2080c56668;;;",,,,,,,,,,,,,,,,,,,,
Release Testing: Verify Python DataStream Window,FLINK-28920,13476321,13476068,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,ana4,hxbks2ks,hxbks2ks,11/Aug/22 07:37,16/Aug/22 05:53,04/Jun/24 20:41,16/Aug/22 05:53,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,release-testing,,,,,,"* Build flink source code and compile source code
{code:bash}
$ cd {flink-source-code}
$ mvn clean install -DskipTests
{code}

* Prepare a Python Virtual Environment

{code:bash}
$ cd flink-python/dev
$ ./lint-python.sh -s basic
$ source .conda/bin/activate
{code}

* Install PyFlink from source code. For more details, you can refer to the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/flinkdev/building/#build-pyflink]
{code:bash}
$ cd flink-python/apache-flink-libraries
$ python setup.py sdist
$ pip install dist/*.tar.gz
$ cd ..
$ pip install -r dev/dev-requirements.txt
$ python setpy.py sdist
$ pip install dist/*.tar.gz
{code}

h1. Test
* Write a python datastream window job  in thread mode. For details of Window, you can refer to the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/].

{code:python}
from typing import Tuple

from pyflink.common import Configuration
from pyflink.common.time import Time
from pyflink.common.typeinfo import Types
from pyflink.common.watermark_strategy import WatermarkStrategy, TimestampAssigner
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import AggregateFunction
from pyflink.datastream.window import EventTimeSessionWindows


class SecondColumnTimestampAssigner(TimestampAssigner):

    def extract_timestamp(self, value, record_timestamp) -> int:
        return int(value[1])


def main():
    config = Configuration()
    # thread mode
    config.set_string(""python.execution-mode"", ""thread"")
    # process mode
    # config.set_string(""python.execution-mode"", ""process"")
    env = StreamExecutionEnvironment.get_execution_environment(config)

    data_stream = env.from_collection([
        ('a', 1), ('a', 2), ('b', 3), ('a', 6), ('b', 8), ('b', 9), ('a', 15)],
        type_info=Types.TUPLE([Types.STRING(), Types.INT()]))  # type: DataStream
    watermark_strategy = WatermarkStrategy.for_monotonous_timestamps() \
        .with_timestamp_assigner(SecondColumnTimestampAssigner())

    class MyAggregateFunction(AggregateFunction):

        def create_accumulator(self) -> Tuple[int, str]:
            return 0, ''

        def add(self, value: Tuple[str, int], accumulator: Tuple[int, str]) -> Tuple[int, str]:
            return value[1] + accumulator[0], value[0]

        def get_result(self, accumulator: Tuple[str, int]):
            return accumulator[1], accumulator[0]

        def merge(self, acc_a: Tuple[int, str], acc_b: Tuple[int, str]):
            return acc_a[0] + acc_b[0], acc_a[1]

    ds = data_stream.assign_timestamps_and_watermarks(watermark_strategy) \
        .key_by(lambda x: x[0], key_type=Types.STRING()) \
        .window(EventTimeSessionWindows.with_gap(Time.milliseconds(2))) \
        .aggregate(MyAggregateFunction(),
                   accumulator_type=Types.TUPLE([Types.INT(), Types.STRING()]),
                   output_type=Types.TUPLE([Types.STRING(), Types.INT()]))

    ds.print()
    env.execute('test_window_aggregate_accumulator_type')


if __name__ == '__main__':
    main()
{code}

* run the python datastream window job and watch the result
{code:bash}
$ python demo.py
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 05:51:38 UTC 2022,,,,,,,,,,"0|z17o7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 03:40;ana4;I will test this on M1 Chip.;;;","16/Aug/22 03:45;hxbks2ks;Thanks [~ana4] for the test. I have assigned it to you. If you encountered any problem, feel free to contact me.;;;","16/Aug/22 05:42;ana4;This test can successfully run on an M1 chip. [~hxbks2ks];;;","16/Aug/22 05:51;hxbks2ks;[~ana4] Thanks a lot for the test.;;;",,,,,,,,,,,,,,,,,
Add built-in generate_series function.,FLINK-28919,13476318,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,11/Aug/22 07:29,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,1.20.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,," 

Syntax:
{code:java}
generate_series ( start numeric, stop numeric [, step numeric ] ) → setof numeric{code}
and it does n't support timestamp now, just like mysql

Returns:

When _{{step}}_ is positive, zero rows are returned if _{{start}}_ is greater than {_}{{stop}}{_}. Conversely, when _{{step}}_ is negative, zero rows are returned if _{{start}}_ is less than {_}{{stop}}{_}. Zero rows are also returned if any input is {{{}NULL{}}}. It is an error for _{{step}}_ to be zero. Some examples follow:

Examples:
{code:java}
SELECT * FROM generate_series(2,4);
 generate_series
-----------------
               2
               3
               4
(3 rows)

SELECT * FROM generate_series(5,1,-2);
 generate_series
-----------------
               5
               3
               1
(3 rows)

SELECT * FROM generate_series(4,3);
 generate_series
-----------------
(0 rows)

SELECT generate_series(1.1, 4, 1.3);
 generate_series
-----------------
             1.1
             2.4
             3.7{code}
See more:
 * pg: [https://www.postgresql.org/docs/current/functions-srf.html]
 * mysql: [https://docs.microsoft.com/en-us/sql/t-sql/functions/generate-series-transact-sql?view=sql-server-ver16]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-11 07:29:29.0,,,,,,,,,,"0|z17o74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release Testing: Verify FLIP-206 in Python DataStream API,FLINK-28918,13476316,13476068,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,ana4,hxbks2ks,hxbks2ks,11/Aug/22 07:17,16/Aug/22 05:53,04/Jun/24 20:41,16/Aug/22 05:53,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,release-testing,,,,,,"* Build flink source code and compile source code
{code:bash}
$ cd {flink-source-code}
$ mvn clean install -DskipTests
{code}

* Prepare a Python Virtual Environment

{code:bash}
$ cd flink-python/dev
$ ./lint-python.sh -s basic
$ source .conda/bin/activate
{code}

* Install PyFlink from source code. For more details, you can refer to the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/flinkdev/building/#build-pyflink]
{code:bash}
$ cd flink-python/apache-flink-libraries
$ python setup.py sdist
$ pip install dist/*.tar.gz
$ cd ..
$ pip install -r dev/dev-requirements.txt
$ python setpy.py sdist
$ pip install dist/*.tar.gz
{code}


h1. Test
* Write a python datastream job  in thread mode

{code:python}
from pyflink.common import Configuration
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment

def main():
    config = Configuration()
    config.set_string(""python.execution-mode"", ""thread"")
    env = StreamExecutionEnvironment.get_execution_environment(config)
    ds = env.from_collection(
    [(1, '9', 0), (1, '5', 1), (1, '6', 2), (5, '5', 0), (5, '3', 1)],
    type_info=Types.ROW_NAMED([""v1"", ""v2"", ""v3""],
                              [Types.INT(), Types.STRING(), Types.INT()]))


    def flat_map_func1(data):
        for i in data:
            yield int(i), 1

    def flat_map_func2(data):
        for i in data:
            yield i

    ds = ds.key_by(lambda x: x[0]) \
        .min_by(""v2"") \
        .map(lambda x: (x[0], x[1], x[2]),
            output_type=Types.TUPLE([Types.INT(), Types.STRING(), Types.INT()])) \
        .key_by(lambda x: x[2]) \
        .max_by(0) \
        .flat_map(flat_map_func1, output_type=Types.TUPLE([Types.INT(), Types.INT()])) \
        .key_by(lambda x: [1]) \
        .min_by() \
        .flat_map(flat_map_func2, output_type=Types.INT()) \
        .key_by(lambda x: x) \
        .max_by()

    ds.print()
    env.execute(""key_by_min_by_max_by_test_batch"")

if __name__ == '__main__':
    main()
{code}

* run the python datastream job and watch the result
{code:bash}
$ python demo.py
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 05:51:56 UTC 2022,,,,,,,,,,"0|z17o6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 03:40;ana4;I will test this on M1 Chip.;;;","16/Aug/22 03:45;hxbks2ks;Thanks [~ana4] for the test. I have assigned it to you. If you encountered any problem, feel free to contact me.;;;","16/Aug/22 05:42;ana4;This test can successfully run on an M1 chip. [~hxbks2ks];;;","16/Aug/22 05:51;hxbks2ks;[~ana4] Thanks a lot for the test.;;;",,,,,,,,,,,,,,,,,
Add sql test for adaptive hash join,FLINK-28917,13476308,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,11/Aug/22 06:48,19/Aug/22 04:07,04/Jun/24 20:41,19/Aug/22 04:07,1.16.0,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 19 04:07:31 UTC 2022,,,,,,,,,,"0|z17o4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/22 04:07;jark;Fixed in master: d9516238cefb03324b0468bd23fd8962166ee565;;;",,,,,,,,,,,,,,,,,,,,
Add e2e test for create function using jar syntax,FLINK-28916,13476307,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,11/Aug/22 06:47,17/Aug/22 03:54,04/Jun/24 20:41,15/Aug/22 07:28,1.16.0,,,,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,Add e2e test for create function using jar syntax that using remote hdfs jar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29007,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 07:28:15 UTC 2022,,,,,,,,,,"0|z17o4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 07:28;jark;Fixed in master: 7661af066f5830f8dc7fae62b67df31ac7fc2e45;;;",,,,,,,,,,,,,,,,,,,,
"Make application mode could support remote DFS schema(e.g. S3, OSS, HDFS, etc.)",FLINK-28915,13476291,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ferenc-csaky,hjw,hjw,11/Aug/22 03:43,25/Jan/24 08:57,04/Jun/24 20:41,25/Jan/24 08:55,,,,,1.19.0,,,,Deployment / Kubernetes,flink-contrib,,,,,,1,pull-request-available,stale-assigned,,,,,"As the Flink document show , local is the only supported scheme in Native k8s deployment.
Is there have a plan to support s3 filesystem? thx.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 25 08:55:21 UTC 2024,,,,,,,,,,"0|z17o14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Aug/22 01:50;Yu.an;I think this feature is valuable, we can avoid rebuilding the image for adding the user jar if having this support.

If the Kubernetes native mode wants to support remote files (s3, hdfs etc), some downloading mechanism should be implemented to download the remote files into Flink containers. Also, it will need to include some authentication parts. Let's see how others think about it.;;;","12/Aug/22 02:22;aitozi;I think it's reasonable feature, since currently we need to add the logic in the podTemplate to download jar to local first, it's not very convenient and can not cover the jar resource from different filesystem.

Can we support to download jar in the entrypoint and taskmanager runner to support this ? cc [~wangyang0918];;;","15/Aug/22 02:06;wangyang0918;Just like what we have done in the flink-kubernetes-operator, I think it makes sense to support the remote DFS schema(e.g. S3, OSS, HDFS, etc.) for user jar in application mode. Not only native K8s application mode, but also the Yarn application mode should also benefit from this.;;;","15/Aug/22 08:04;aitozi;Nice idea, I'd like to file a PR for this. Please help assign the ticket. [~wangyang0918];;;","15/Aug/22 09:00;hjw;I successful modify the Class KubernetesApplicationClusterEntryPoint.java to achieve the purpose that support S3 schema.

Here the modify logic:
1.read the jar localtion in s3 from pipeline.jars parameters. 
2.download the jar from s3 to local path of pod(jobmanager).
3.replace the local path (local schema) to pipeline.jars parameters.

I know such an implementation is not elegant and not compatible with other remote DFS schema.(OSS.HDFS .etc). I think the more elegant implementation is to use Flink filesystem to connect each DFS schema.

Howerver, I notice the fact that Flink filesystem is configred in Starting flink cluster. But the job PackagedProgram is inited in KubernetesApplicationClusterEntryPoint before the code ""ClusterEntrypoint.runclusterEntrypoint(KubernetesApplicationClusterEntryPoint)"".;;;","15/Aug/22 10:00;aitozi;[~hjw] Glad to see you already run through it. If we support to download resource from the filesystem. I think we need to adjust the order of the filesystem initialization;;;","15/Aug/22 14:23;hjw;[~aitozi] thank you. I'm also interested in implementing this feature,but I never commit a pr in the past.May I be involved in the implementation of this pr please？

Or is there something I can do in this pr?  I want to participate in the Flink community contribution.;;;","16/Aug/22 02:03;aitozi;[~hjw] Yes, you could work on this. Maybe you need to wait [~wangyang0918] to assign this ticket to you. You can start your coding first, since we already have a common sense in how to implement this in flink.

;;;","16/Aug/22 02:05;aitozi;Before, we have implement the similar feature in the flink k8s operator, you could take a look for it.

https://github.com/apache/flink-kubernetes-operator/pull/168;;;","16/Aug/22 03:10;wangyang0918;Thanks [~aitozi] and [~hjw] for the volunteering. I will assign this ticket to [~hjw] and I believe we could work together.;;;","16/Aug/22 15:07;hjw;Thanks  [~aitozi]  for the advice .

Thanks [~wangyang0918]   assign the ticket.;;;","18/Aug/22 16:40;hjw;[~wangyang0918] [~aitozi] Does the test case need to cover all Flink supported filesystems? From [https://github.com/apache/flink-kubernetes-operator/pull/168] , only http or https schema are covreed. Do other file systems require manual verification?;;;","19/Aug/22 02:42;wangyang0918;I think we need to cover all the flink-supported filesystems(hdfs, S3, OSS, GFS, etc.). The reason why the operator does not support is it does not have the credentials for accessing these filesystems. However, the fetcher in Flink could get the credential because it is usually already configured for checkpoint/HA usage.;;;","21/Aug/22 17:20;hjw;[~wangyang0918] Could you tell more about how to use the credential for filesystems(hdfs, S3, OSS, GFS, etc.) ？I have completed the basic development, and have completed the manual testing of part of the filesystem (OSS, s3, local, file). But other filesystems I need environment for testing.;;;","22/Aug/22 08:28;wangyang0918;I think you could use {{MinioTestContainer}} for the unit test.;;;","04/Aug/23 05:44;pegasas;Hi, [~hjw], [~wangyang0918],
What is the status of this thread?
Is there any plan to support ABFS https://hadoop.apache.org/docs/stable/hadoop-azure/abfs.html? ;;;","03/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","18/Oct/23 07:27;Yu.an;Hi [~hjw] and [~wangyang0918] , what is the status of this ticket? I saw the PRs were submitted long ago but are still not merged.;;;","05/Jan/24 10:42;mbalassi;Due to the [inactivity|https://github.com/apache/flink/pull/20779#issuecomment-1878422305] on the PRs [~ferenc-csaky] has offered to finish this implementation using the original commit of [~hjw] as a base. ;;;","25/Jan/24 08:55;mbalassi;[e63aa12|https://github.com/apache/flink/commit/e63aa12252843d0098a56f3091b28d48aff5b5af] and [a85bcb4|https://github.com/apache/flink/commit/a85bcb47d853a30b78cb5f17cd53f5c033110a60] in master.;;;",
Could not find any factories that implement,FLINK-28914,13476284,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,CoderDM,CoderDM,11/Aug/22 03:12,25/Oct/22 01:43,04/Jun/24 20:41,25/Oct/22 01:43,1.16.0,,,,1.17.0,,,,Table SQL / Gateway,,,,,,,0,,,,,,,"2022-08-11 11:09:53,135 ERROR org.apache.flink.table.gateway.SqlGateway                    [] - Failed to start the endpoints.

org.apache.flink.table.api.ValidationException: Could not find any factories that implement 'org.apache.flink.table.gateway.api.endpoint.SqlGatewayEndpointFactory' in the classpath.

-----------------------------------------------------

I packaged Flink-Master and tried to start sql-gateway, but some problems arise.

I found tow problem with Factory under resources of flink-sql-gateway module.

META-INF.services should not be a folder name, ti should be ... /META-INF/services/... 

The 

`` org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointFactory ``  in the org.apache.flink.table.factories.Factory file should be 

``  org.apache.flink.table.gateway.rest.util.SqlGatewayRestEndpointFactory `` . ",,,,,,,,,,,,,,,,,,,FLINK-27773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 24 07:24:43 UTC 2022,,,,,,,,,,"0|z17nzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:04;martijnvisser;[~fsk119] Is this something that's on your radar?;;;","24/Oct/22 02:07;fsk119;Yes. Thanks for the mention. I think it should be fixed during the [FLINK-27773|https://issues.apache.org/jira/browse/FLINK-27773];;;","24/Oct/22 07:24;martijnvisser;[~fsk119] Given that that ticket is already closed, should this ticket also be considered fixed?;;;",,,,,,,,,,,,,,,,,,
Fix fail to open HiveCatalog when it's for hive3,FLINK-28913,13476277,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,11/Aug/22 02:25,15/Aug/22 08:54,04/Jun/24 20:41,15/Aug/22 08:54,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"When use HiveCatalog for hive3, it will throw such exception:
{code:java}
java.lang.NoClassDefFoundError: org/apache/calcite/plan/RelOptRule
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:91)
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:79)
        at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:32)
        at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:306)
        at org.apache.flink.table.catalog.CatalogManager.registerCatalog(CatalogManager.java:211)
        at org.apache.flink.table.api.internal.TableEnvironmentImpl.registerCatalog(TableEnvironmentImpl.java:382) {code}
The failure is introduced by [FLINK-26413|https://issues.apache.org/jira/browse/FLINK-26413], which introduces `Hive.get(hiveConf);` in method `HiveMetastoreClientFactory.create` to support Hive's ""load data inpath` syntax.

But the class `Hive` will import class 'org.apache.calcite.plan.RelOptRule', then when try to load the class `Hive`, it'll throw class not found exception since this class is not in class path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 08:54:16 UTC 2022,,,,,,,,,,"0|z17ny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 03:00;luoyuxia;The workaround way is to swap `opt/flink-table-planner` and `lib/flink-table-planner-loader` as flink-table-planner contains calcite dependency.

But to fix it, I think we can lazy init the `Hive` class, only when we need to call method `loadTable` / `loadPartition`. 

I think it's fine for only in Hive dialect, do we need `Hive` class, and when user want to use Hive dialect, they need to swap `lib/flink-table-planner-loader` and `opt/flink-table-planner` so that the calcite will exist in class path.

 ;;;","15/Aug/22 08:54;jark;Fixed in master: cb4ead75d594e351dfe81af42e52db88faae356a;;;",,,,,,,,,,,,,,,,,,,
"Add Part of ""Who Use Flink"" In ReadMe file  and https://flink.apache.org/",FLINK-28912,13476173,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Information Provided,,zhouyao,zhouyao,10/Aug/22 12:19,16/Aug/22 06:30,04/Jun/24 20:41,14/Aug/22 09:52,,,,,,,,,Documentation,,,,,,,0,doc,,,,,,"May be ,we can learn from website of  [Apache Kylin|https://kylin.apache.org/], add part of  ""Who Use Flink""  in Readme or website.  This can make Flink more frendly

!image-2022-08-10-20-15-10-418.png|width=147,height=99!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/22 12:15;zhouyao;image-2022-08-10-20-15-10-418.png;https://issues.apache.org/jira/secure/attachment/13047974/image-2022-08-10-20-15-10-418.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 06:30:27 UTC 2022,,,,,,,,,,"0|z17nbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/22 09:52;yunta;I think the ""powered by flink"" page already shows this. https://flink.apache.org/poweredby.html;;;","16/Aug/22 06:30;zhouyao;Great~,thanks;;;",,,,,,,,,,,,,,,,,,,
Elasticsearch connector fails build,FLINK-28911,13476172,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,nielsbasjes,nielsbasjes,nielsbasjes,10/Aug/22 11:53,10/Feb/23 14:55,04/Jun/24 20:41,10/Feb/23 14:55,1.15.1,,,,,,,,Connectors / ElasticSearch,,,,,,,0,pull-request-available,,,,,,"When I run the `mvn clean verify` of the ES connector some if the integration tests fail.

Assesment so far: the SerializationSchema is not opened, triggering an NPE later on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-10 11:53:09.0,,,,,,,,,,"0|z17nbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDC From Mysql To Hbase Bugs,FLINK-28910,13476153,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,GanLuLu,GanLuLu,10/Aug/22 10:29,30/Jun/23 12:41,04/Jun/24 20:41,30/Jun/23 12:30,,,,,,,,,Connectors / HBase,,,,,,,0,pull-request-available,stale-blocker,,,,,"I use Flink for CDC from Mysql to Hbase.

The problem I encountered is that the Mysql record is updated (not deleted), but the record in hbase is deleted sometimes.

I tried to analyze the problem and found the reason as follows:

The update action of Mysql will be decomposed into delete + insert by Flink.
The Hbase connector uses a mutator to handle this set of actions.

However, if the order of these actions is not actively set, the processing of the mutator will not guarantee the order of execution.

Therefore, when the update of Mysql is triggered, it is possible that hbase actually performed the actions in the order of put + delete, resulting in the data being deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32139,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 30 12:30:45 UTC 2023,,,,,,,,,,"0|z17n74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","08/Sep/22 04:46;liyu;From my point of view, a better solution is to facilitate the atomic operations in HBase (Table#checkAndMutate or Table#mutateRow) for the update-alike change data ingestion. However, currently Flink's `HBaseSinkFunction` takes usage of HBase's `BufferedMutator` and `BufferedMutator` only supports `Mutation` (actually [currently|https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/client/BufferedMutator.java#L93-L99] only Put and Delete), and operations like `RowMutations` or `CheckAndMuate` are not supported. Let me check whether we could do something on the HBase side.

cc [~zhangduo];;;","27/Sep/22 10:16;martijnvisser;[~ferenc-csaky] Could you also have a look at this one?;;;","28/Sep/22 09:10;ferenc-csaky;Sure, I'll take a look.;;;","30/Sep/22 10:47;ferenc-csaky;My HBase knowledge is a bit rusty, so bear with me if any of my assumptions is not correct. As per my understanding, if we would like to move towards the atomic table operations way, we would lose the currently leveraged buffering functionality, because it is possible to define multiple mutations for 1 specific row with {{{}RowMutations{}}}, but that's it, the operations will be sent right away, which will probably affect performance.

According to the PR, it will do the job I think, would that be possible to handle the action itself smarter? I'm wondering about would it make sense to omit the delete op. in specific cases?;;;","30/Jun/23 12:30;martijnvisser;Fixed via FLINK-28910;;;",,,,,,,,,,,,,,,
Add ribbon filter policy option in RocksDBConfiguredOptions,FLINK-28909,13476151,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zlzhang0122,zlzhang0122,10/Aug/22 10:28,11/Mar/24 12:43,04/Jun/24 20:41,,1.14.2,1.15.1,,,1.20.0,,,,Runtime / State Backends,,,,,,,0,,,,,,,"Ribbon filter can efficiently enhance the read and reduce the disk and memory usage on RocksDB, it's supported by rocksdb since 6.15. (more details see [http://rocksdb.org/blog/2021/12/29/ribbon-filter.html|http://rocksdb.org/blog/2021/12/29/ribbon-filter.html])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 12:07:21 UTC 2022,,,,,,,,,,"0|z17n6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 03:37;Yanfei Lei;Hi [~zlzhang0122],  {{rocksjni}} does not support [Ribbon filter policy|https://github.com/facebook/rocksdb/blob/main/java/rocksjni/filter.cc] yet, we should let rocks-jni support it first, and then pick the commits to frocksdb.;;;","15/Aug/22 12:07;zlzhang0122;[~Yanfei Lei] Thanks for your reply and yes, you are right. There is still a lot of works to do to support Ribbon filter policy.;;;",,,,,,,,,,,,,,,,,,,
Coder for LIST type is incorrectly chosen is PyFlink,FLINK-28908,13476147,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/Aug/22 10:16,15/Aug/22 04:44,04/Jun/24 20:41,15/Aug/22 04:44,1.14.5,1.15.1,,,1.14.6,1.15.2,1.16.0,,API / Python,,,,,,,0,pull-request-available,,,,,,"Code to reproduce this bug, the result is `[None, None, None]`:
{code:python}
jvm = get_gateway().jvm
env = StreamExecutionEnvironment.get_execution_environment()
j_item = jvm.java.util.ArrayList()
j_item.add(1)
j_item.add(2)
j_item.add(3)
j_list = jvm.java.util.ArrayList()
j_list.add(j_item)
type_info = Types.LIST(Types.INT())
ds = DataStream(env._j_stream_execution_environment.fromCollection(j_list, type_info.get_java_type_info()))
ds.map(lambda e: print(e))
env.execute() {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 04:44:14 UTC 2022,,,,,,,,,,"0|z17n5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/22 04:44;dianfu;Fixed in:
- master via de8ff096a5344d85eb9be497902b99dd4b24e2a9 and 9b50ff584ecdb76256be8c35ee01b2ecd03d3dcb
- release-1.15 via 63c1c1b9f6ee9669305e081030768b43a267c0f5
- release-1.14 via f0545f42607655187b53381506e55d33beec4542;;;",,,,,,,,,,,,,,,,,,,,
Flink docs do not compile locally,FLINK-28907,13476140,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,zhuzh,zhuzh,10/Aug/22 09:51,31/Aug/22 08:25,04/Jun/24 20:41,31/Aug/22 08:25,1.16.0,,,,,,,,Documentation,,,,,,,0,,,,,,,"Flink docs fail to compile locally. The error is as below:

go: github.com/apache/flink-connector-elasticsearch/docs upgrade => v0.0.0-20220715033920-cbeb08187b3a
hugo: collected modules in 1832 ms
Start building sites …
ERROR 2022/08/10 17:48:29 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/elasticsearch"": ""/XXX/docs/content/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2022/08/10 17:48:29 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/elasticsearch"": ""/XXX/docs/content/docs/connectors/datastream/overview.md:44:20"": page not found
ERROR 2022/08/10 17:48:29 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/elasticsearch"": ""/XXX/docs/content/docs/connectors/table/overview.md:58:20"": page not found
WARN 2022/08/10 17:48:29 Expand shortcode is deprecated. Use 'details' instead.
ERROR 2022/08/10 17:48:32 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/elasticsearch"": ""/XXX/docs/content.zh/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2022/08/10 17:48:32 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/elasticsearch"": ""/XXX/docs/content.zh/docs/connectors/datastream/overview.md:43:20"": page not found
ERROR 2022/08/10 17:48:32 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/elasticsearch"": ""/XXX/docs/content.zh/docs/connectors/table/overview.md:58:20"": page not found
WARN 2022/08/10 17:48:32 Expand shortcode is deprecated. Use 'details' instead.
Built in 6415 ms
Error: Error building site: logged 6 error(s)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 31 08:25:18 UTC 2022,,,,,,,,,,"0|z17n48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 10:23;zhuzh;Tried building docs of latest Flink master on other dev machines, this problem does not happen.
So possibly this is not a common problem.;;;","30/Aug/22 09:06;martijnvisser;This happens when the documentation build system can't connect to the external connector repositories. ;;;","31/Aug/22 03:06;zhuzh;Thanks for looking into this! [~martijnvisser]

Do you mean the process below should happen, but when there is connection issue, it will not happen and will not report errors about the connection problem?
> go: downloading github.com/apache/flink-connector-elasticsearch ...




;;;","31/Aug/22 08:15;martijnvisser;I do think Go reports back on errors, but in case there's a connection issue, this could cause this error yes;;;","31/Aug/22 08:25;zhuzh;Got it! Thanks for the explanation.;;;",,,,,,,,,,,,,,,,
Add AlgoOperator for AgglomerativeClustering,FLINK-28906,13476135,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zhangzp,zhangzp,10/Aug/22 09:25,12/Oct/22 09:10,04/Jun/24 20:41,12/Oct/22 09:10,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-10 09:25:33.0,,,,,,,,,,"0|z17n34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HiveCatalog can't get statistic for Hive timestamp partition column,FLINK-28905,13476127,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,luoyuxia,luoyuxia,10/Aug/22 09:03,21/Aug/23 10:35,04/Jun/24 20:41,,,,,,,,,,Connectors / Hive,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,"Hive's timestamp type will be mapped to Flink's `TIMESTAMP_WITHOUT_TIME_ZONE`, but

in method `HiveStatsUtil#getPartitionColumnStats`, the `TIMESTAMP_WITH_LOCAL_TIME_ZONE` is mistaken as `TIMESTAMP_WITHOUT_TIME_ZONE`,  which will cause fail to get partition column's statistic when the partition column is 

So, we should change `TIMESTAMP_WITH_LOCAL_TIME_ZONE` to `TIMESTAMP_WITHOUT_TIME_ZONE`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 10:35:27 UTC 2023,,,,,,,,,,"0|z17n1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Add missing connector/format doc for PyFlink,FLINK-28904,13476124,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/Aug/22 08:42,10/Aug/22 11:47,04/Jun/24 20:41,10/Aug/22 11:46,,,,,1.16.0,,,,API / Python,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 11:46:06 UTC 2022,,,,,,,,,,"0|z17n0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 11:46;dianfu;Merged to master via 465db25502e6e2c59e466b05dfd3bdb28919a765;;;",,,,,,,,,,,,,,,,,,,,
flink-table-store-hive-catalog could not shade hive-shims-0.23,FLINK-28903,13476119,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,10/Aug/22 08:13,11/Aug/22 09:51,04/Jun/24 20:41,11/Aug/22 09:51,table-store-0.3.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"flink-table-store-hive-catalog could not shade hive-shims-0.23 because artifactSet doesn't include hive-shims-0.23 and the hive-shims-0.23 isn't specifically included with filters. The exception is as follows for setting hive.metastore.use.SSL or hive.metastore.sasl.enabled is true:
{code:java}
￼Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1708) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more
Caused by: java.lang.RuntimeException: Could not load shims in class org.apache.flink.table.store.shaded.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge23
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:132) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.getHadoopThriftAuthBridge(ShimLoader.java:108) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:414) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:247) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.store.shaded.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge23
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_181]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_181]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) ~[?:1.8.0_181]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_181]
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_181]
    at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:129) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.shims.ShimLoader.getHadoopThriftAuthBridge(ShimLoader.java:108) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:414) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:247) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181]
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181]
    at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:380) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:80) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:93) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:62) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.3-SNAPSHOT.jar:0.3-SNAPSHOT]
    at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:428) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1356) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1111) ~[flink-table-api-java-uber-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$executeOperation$3(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.context.ExecutionContext.wrapClassLoader(ExecutionContext.java:88) ~[flink-sql-client-1.15.1.jar:1.15.1]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:209) ~[flink-sql-client-1.15.1.jar:1.15.1]
    ... 10 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 09:51:47 UTC 2022,,,,,,,,,,"0|z17mzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 04:43;lzljs3620320;We can change this to `support hive 2.1&2.2`;;;","11/Aug/22 07:00;nicholasjiang;[~lzljs3620320] , when the value of hive.metastore.use.SSL or hive.metastore.sasl.enabled is true, there is above exception for Hive 2.x including Hive 2.3.;;;","11/Aug/22 09:51;lzljs3620320;master: 3b6e602cd64223a7861d30c57054d57b9557cf94
release-0.2: 3da1a91444bcf48a0345fbc102e9294eedaa0359;;;",,,,,,,,,,,,,,,,,,
FileSystemJobResultStoreTestInternal is a unit test that's not following the unit test naming convention,FLINK-28902,13476105,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tonyzhu,mapohl,mapohl,10/Aug/22 07:23,16/Sep/22 09:49,04/Jun/24 20:41,16/Sep/22 09:49,1.15.1,1.16.0,,,1.17.0,,,,Runtime / Coordination,Test Infrastructure,,,,,,0,pull-request-available,starter,,,,,"AzureCI still picks the test up as part of {{mvn verify}} (see an [example build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39780&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8539]).

Anyway, we should move the {{Internal}} suffix and move it somewhere inside of the test name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 16 09:49:45 UTC 2022,,,,,,,,,,"0|z17mwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/22 00:38;tonyzhu;Can I try this ticket?;;;","14/Sep/22 08:25;mapohl;Sure, thanks for offering your help. I assigned the ticket to you.;;;","15/Sep/22 05:37;tonyzhu;I created this PR [https://github.com/apache/flink/pull/20839]

Could you help review please;;;","16/Sep/22 03:07;tonyzhu;Thanks for review. I created another PR [#20845|https://github.com/apache/flink/pull/20845] .;;;","16/Sep/22 09:49;mapohl;master: 9d2ae5572897f3e2d9089414261a250cfc2a2ab8;;;",,,,,,,,,,,,,,,,
CassandraSinkBaseTest.testTimeoutExceptionOnInvoke failed with TestTimedOutException,FLINK-28901,13476094,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hxb,hxb,10/Aug/22 06:32,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,1.20.0,,,,Connectors / Cassandra,,,,,,,0,test-stability,,,,,,"
{code:java}
2022-08-10T03:39:22.6587394Z Aug 10 03:39:22 [ERROR] org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.testTimeoutExceptionOnInvoke  Time elapsed: 5.113 s  <<< ERROR!
2022-08-10T03:39:22.6588579Z Aug 10 03:39:22 org.junit.runners.model.TestTimedOutException: test timed out after 5000 milliseconds
2022-08-10T03:39:22.6589463Z Aug 10 03:39:22 	at java.util.zip.ZipFile.read(Native Method)
2022-08-10T03:39:22.6590286Z Aug 10 03:39:22 	at java.util.zip.ZipFile.access$1400(ZipFile.java:60)
2022-08-10T03:39:22.6591287Z Aug 10 03:39:22 	at java.util.zip.ZipFile$ZipFileInputStream.read(ZipFile.java:734)
2022-08-10T03:39:22.6592323Z Aug 10 03:39:22 	at java.util.zip.ZipFile$ZipFileInflaterInputStream.fill(ZipFile.java:434)
2022-08-10T03:39:22.6593673Z Aug 10 03:39:22 	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)
2022-08-10T03:39:22.6594638Z Aug 10 03:39:22 	at sun.misc.Resource.getBytes(Resource.java:124)
2022-08-10T03:39:22.6595535Z Aug 10 03:39:22 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:463)
2022-08-10T03:39:22.6596506Z Aug 10 03:39:22 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
2022-08-10T03:39:22.6597477Z Aug 10 03:39:22 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369)
2022-08-10T03:39:22.6598393Z Aug 10 03:39:22 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363)
2022-08-10T03:39:22.6599286Z Aug 10 03:39:22 	at java.security.AccessController.doPrivileged(Native Method)
2022-08-10T03:39:22.6600209Z Aug 10 03:39:22 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362)
2022-08-10T03:39:22.6601141Z Aug 10 03:39:22 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
2022-08-10T03:39:22.6602070Z Aug 10 03:39:22 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
2022-08-10T03:39:22.6603090Z Aug 10 03:39:22 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
2022-08-10T03:39:22.6604199Z Aug 10 03:39:22 	at net.bytebuddy.NamingStrategy$SuffixingRandom.<init>(NamingStrategy.java:153)
2022-08-10T03:39:22.6605188Z Aug 10 03:39:22 	at net.bytebuddy.ByteBuddy.<init>(ByteBuddy.java:196)
2022-08-10T03:39:22.6606063Z Aug 10 03:39:22 	at net.bytebuddy.ByteBuddy.<init>(ByteBuddy.java:187)
2022-08-10T03:39:22.6607141Z Aug 10 03:39:22 	at org.mockito.internal.creation.bytebuddy.InlineBytecodeGenerator.<init>(InlineBytecodeGenerator.java:81)
2022-08-10T03:39:22.6608423Z Aug 10 03:39:22 	at org.mockito.internal.creation.bytebuddy.InlineByteBuddyMockMaker.<init>(InlineByteBuddyMockMaker.java:224)
2022-08-10T03:39:22.6609844Z Aug 10 03:39:22 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-10T03:39:22.6610990Z Aug 10 03:39:22 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-10T03:39:22.6612246Z Aug 10 03:39:22 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-10T03:39:22.6613551Z Aug 10 03:39:22 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2022-08-10T03:39:22.6614482Z Aug 10 03:39:22 	at java.lang.Class.newInstance(Class.java:442)
2022-08-10T03:39:22.6615527Z Aug 10 03:39:22 	at org.mockito.internal.configuration.plugins.PluginInitializer.loadImpl(PluginInitializer.java:50)
2022-08-10T03:39:22.6616718Z Aug 10 03:39:22 	at org.mockito.internal.configuration.plugins.PluginLoader.loadPlugin(PluginLoader.java:63)
2022-08-10T03:39:22.6617900Z Aug 10 03:39:22 	at org.mockito.internal.configuration.plugins.PluginLoader.loadPlugin(PluginLoader.java:48)
2022-08-10T03:39:22.6619079Z Aug 10 03:39:22 	at org.mockito.internal.configuration.plugins.PluginRegistry.<init>(PluginRegistry.java:23)
2022-08-10T03:39:22.6620206Z Aug 10 03:39:22 	at org.mockito.internal.configuration.plugins.Plugins.<clinit>(Plugins.java:19)
2022-08-10T03:39:22.6621435Z Aug 10 03:39:22 	at org.mockito.internal.util.MockUtil.<clinit>(MockUtil.java:24)
2022-08-10T03:39:22.6622523Z Aug 10 03:39:22 	at org.mockito.internal.util.MockCreationValidator.validateType(MockCreationValidator.java:22)
2022-08-10T03:39:22.6623890Z Aug 10 03:39:22 	at org.mockito.internal.creation.MockSettingsImpl.validatedSettings(MockSettingsImpl.java:250)
2022-08-10T03:39:22.6625051Z Aug 10 03:39:22 	at org.mockito.internal.creation.MockSettingsImpl.build(MockSettingsImpl.java:232)
2022-08-10T03:39:22.6626085Z Aug 10 03:39:22 	at org.mockito.internal.MockitoCore.mock(MockitoCore.java:68)
2022-08-10T03:39:22.6626983Z Aug 10 03:39:22 	at org.mockito.Mockito.mock(Mockito.java:1933)
2022-08-10T03:39:22.6627829Z Aug 10 03:39:22 	at org.mockito.Mockito.mock(Mockito.java:1844)
2022-08-10T03:39:22.6628998Z Aug 10 03:39:22 	at org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest$TestCassandraSink.<clinit>(CassandraSinkBaseTest.java:379)
2022-08-10T03:39:22.6630495Z Aug 10 03:39:22 	at org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.createOpenedTestCassandraSink(CassandraSinkBaseTest.java:345)
2022-08-10T03:39:22.6631993Z Aug 10 03:39:22 	at org.apache.flink.streaming.connectors.cassandra.CassandraSinkBaseTest.testTimeoutExceptionOnInvoke(CassandraSinkBaseTest.java:314)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39799&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 13 03:23:57 UTC 2022,,,,,,,,,,"0|z17mu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/22 03:23;hxb;Given that it hasn't appeared for a month, I will downgrade the priority to major.;;;",,,,,,,,,,,,,,,,,,,,
RecreateOnResetOperatorCoordinatorTest compile failed in jdk11,FLINK-28900,13476093,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunfengzhou,hxb,hxb,10/Aug/22 06:19,10/Aug/22 16:29,04/Jun/24 20:41,10/Aug/22 16:29,1.16.0,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-08-10T00:19:25.3221073Z [ERROR] COMPILATION ERROR : 
2022-08-10T00:19:25.3221634Z [INFO] -------------------------------------------------------------
2022-08-10T00:19:25.3222878Z [ERROR] /__w/1/s/flink-runtime/src/test/java/org/apache/flink/runtime/operators/coordination/RecreateOnResetOperatorCoordinatorTest.java:[241,58] method containsExactly in class org.assertj.core.api.AbstractIterableAssert<SELF,ACTUAL,ELEMENT,ELEMENT_ASSERT> cannot be applied to given types;
2022-08-10T00:19:25.3223786Z   required: capture#1 of ? extends java.lang.Integer[]
2022-08-10T00:19:25.3224245Z   found: int
2022-08-10T00:19:25.3224684Z   reason: varargs mismatch; int cannot be converted to capture#1 of ? extends java.lang.Integer
2022-08-10T00:19:25.3225128Z [INFO] 1 error
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28606,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 06:21:57 UTC 2022,,,,,,,,,,"0|z17mts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 06:20;hxb;[~yunfengzhou] Could you help take a look?;;;","10/Aug/22 06:21;hxb;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=a7382ec4-87d2-5a9d-7c53-a2f93e317458]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=dffc2faa-5b48-5b4e-0797-dec1b1f74872]

 ;;;",,,,,,,,,,,,,,,,,,,
Fix LOOKUP hint with retry option on async lookup mode,FLINK-28899,13476092,13474692,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,10/Aug/22 06:14,16/Aug/22 02:23,04/Jun/24 20:41,16/Aug/22 02:23,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"Fix the broken path for async lookup with retry options in  LOOKUP hint.

Will use a `RetryableAsyncLookupFunctionDelegator` instead of `AsyncWaitOperator`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 02:23:54 UTC 2022,,,,,,,,,,"0|z17mtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 02:23;godfreyhe;Fixed in master: cd173e71162a2c4ceda28dff2667b621210f92ce;;;",,,,,,,,,,,,,,,,,,,,
ChangelogRecoverySwitchStateBackendITCase.testSwitchFromEnablingToDisablingWithRescalingOut failed,FLINK-28898,13476077,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,hxb,hxb,10/Aug/22 03:49,11/Jan/23 10:34,04/Jun/24 20:41,11/Jan/23 10:27,1.16.0,1.17.0,,,1.16.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,test-stability,,,,,"{code:java}
2022-08-10T02:48:19.5711924Z Aug 10 02:48:19 [ERROR] ChangelogRecoverySwitchStateBackendITCase.testSwitchFromEnablingToDisablingWithRescalingOut  Time elapsed: 6.064 s  <<< ERROR!
2022-08-10T02:48:19.5712815Z Aug 10 02:48:19 org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=0, backoffTimeMS=0)
2022-08-10T02:48:19.5714530Z Aug 10 02:48:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-10T02:48:19.5716211Z Aug 10 02:48:19 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-10T02:48:19.5717627Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-10T02:48:19.5718885Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-10T02:48:19.5720430Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-10T02:48:19.5721733Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-10T02:48:19.5722680Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-10T02:48:19.5723612Z Aug 10 02:48:19 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-08-10T02:48:19.5724389Z Aug 10 02:48:19 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-08-10T02:48:19.5725046Z Aug 10 02:48:19 	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
2022-08-10T02:48:19.5725708Z Aug 10 02:48:19 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-10T02:48:19.5726374Z Aug 10 02:48:19 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-10T02:48:19.5727065Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-10T02:48:19.5727932Z Aug 10 02:48:19 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-10T02:48:19.5729087Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-10T02:48:19.5730134Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-10T02:48:19.5731536Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-10T02:48:19.5732549Z Aug 10 02:48:19 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-10T02:48:19.5735018Z Aug 10 02:48:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-10T02:48:19.5735821Z Aug 10 02:48:19 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-10T02:48:19.5736465Z Aug 10 02:48:19 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-10T02:48:19.5737234Z Aug 10 02:48:19 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-10T02:48:19.5737895Z Aug 10 02:48:19 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-10T02:48:19.5738574Z Aug 10 02:48:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-10T02:48:19.5739276Z Aug 10 02:48:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-10T02:48:19.5740315Z Aug 10 02:48:19 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-10T02:48:19.5741211Z Aug 10 02:48:19 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-10T02:48:19.5741878Z Aug 10 02:48:19 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-10T02:48:19.5742497Z Aug 10 02:48:19 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-10T02:48:19.5743242Z Aug 10 02:48:19 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-10T02:48:19.5743977Z Aug 10 02:48:19 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-10T02:48:19.5744851Z Aug 10 02:48:19 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-10T02:48:19.5745438Z Aug 10 02:48:19 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-10T02:48:19.5746059Z Aug 10 02:48:19 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-10T02:48:19.5746885Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-10T02:48:19.5747571Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-10T02:48:19.5748236Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-10T02:48:19.5749314Z Aug 10 02:48:19 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-10T02:48:19.5750188Z Aug 10 02:48:19 Caused by: java.io.IOException: Could not parse external pointer as state base path
2022-08-10T02:48:19.5751178Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase$DeserializationContext.createExclusiveDirPath(MetadataV2V3SerializerBase.java:875)
2022-08-10T02:48:19.5752359Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase$DeserializationContext.getExclusiveDirPath(MetadataV2V3SerializerBase.java:865)
2022-08-10T02:48:19.5753888Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeStreamStateHandle(MetadataV2V3SerializerBase.java:727)
2022-08-10T02:48:19.5754918Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeOperatorStateHandle(MetadataV2V3SerializerBase.java:630)
2022-08-10T02:48:19.5755929Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeSubtaskState(MetadataV2V3SerializerBase.java:279)
2022-08-10T02:48:19.5756887Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.deserializeOperatorState(MetadataV3Serializer.java:184)
2022-08-10T02:48:19.5757815Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase.deserializeMetadata(MetadataV2V3SerializerBase.java:182)
2022-08-10T02:48:19.5758890Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV3Serializer.deserialize(MetadataV3Serializer.java:90)
2022-08-10T02:48:19.5759762Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV4Serializer.deserialize(MetadataV4Serializer.java:49)
2022-08-10T02:48:19.5760584Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.Checkpoints.loadCheckpointMetadata(Checkpoints.java:118)
2022-08-10T02:48:19.5761414Z Aug 10 02:48:19 	at org.apache.flink.test.util.TestUtils.loadCheckpointMetadata(TestUtils.java:103)
2022-08-10T02:48:19.5762243Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoveryITCaseBase.getAllStateHandleId(ChangelogRecoveryITCaseBase.java:233)
2022-08-10T02:48:19.5763361Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoverySwitchEnvTestBase$2.lambda$beforeElement$cdc83b0a$1(ChangelogRecoverySwitchEnvTestBase.java:120)
2022-08-10T02:48:19.5764755Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoveryITCaseBase$ControlledSource.waitWhile(ChangelogRecoveryITCaseBase.java:359)
2022-08-10T02:48:19.5765732Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoverySwitchEnvTestBase$2.beforeElement(ChangelogRecoverySwitchEnvTestBase.java:117)
2022-08-10T02:48:19.5766685Z Aug 10 02:48:19 	at org.apache.flink.test.checkpointing.ChangelogRecoveryITCaseBase$ControlledSource.run(ChangelogRecoveryITCaseBase.java:342)
2022-08-10T02:48:19.5767503Z Aug 10 02:48:19 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
2022-08-10T02:48:19.5768234Z Aug 10 02:48:19 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
2022-08-10T02:48:19.5769041Z Aug 10 02:48:19 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
2022-08-10T02:48:19.5771138Z Aug 10 02:48:19 Caused by: java.io.FileNotFoundException: Cannot find meta data file '_metadata' in directory 'file:/tmp/junit8987764363790519654/junit1534389645319798457/0d8e7372aaeb10c57565dae19d6f6ed6/chk-4'. Please try to load the checkpoint/savepoint directly from the metadata file instead of the directory.
2022-08-10T02:48:19.5772448Z Aug 10 02:48:19 	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:290)
2022-08-10T02:48:19.5773761Z Aug 10 02:48:19 	at org.apache.flink.runtime.checkpoint.metadata.MetadataV2V3SerializerBase$DeserializationContext.createExclusiveDirPath(MetadataV2V3SerializerBase.java:872)
2022-08-10T02:48:19.5774662Z Aug 10 02:48:19 	... 18 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,,,,,,,,,FLINK-30561,,,,FLINK-28766,FLINK-30107,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 11 10:27:26 UTC 2023,,,,,,,,,,"0|z17mq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 03:49;hxb;[~Yanfei Lei] Could you help take a look?;;;","10/Aug/22 16:08;roman;Merged as 3268ec6a7ce0e060eb401917f7d169969334d07d.;;;","16/Aug/22 08:01;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40021&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7 new instance, but the stack is a little different.

{code:java}
2022-08-16T02:33:54.7104899Z Aug 16 02:33:54 [ERROR] ChangelogRecoverySwitchStateBackendITCase.testSwitchFromEnablingToDisabling  Time elapsed: 1.777 s  <<< ERROR!
2022-08-16T02:33:54.7106371Z Aug 16 02:33:54 org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=0, backoffTimeMS=0)
2022-08-16T02:33:54.7108148Z Aug 16 02:33:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-08-16T02:33:54.7109843Z Aug 16 02:33:54 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-08-16T02:33:54.7111540Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:256)
2022-08-16T02:33:54.7112933Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:247)
2022-08-16T02:33:54.7114297Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-08-16T02:33:54.7115692Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:738)
2022-08-16T02:33:54.7117495Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:715)
2022-08-16T02:33:54.7118891Z Aug 16 02:33:54 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-08-16T02:33:54.7120184Z Aug 16 02:33:54 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-08-16T02:33:54.7121454Z Aug 16 02:33:54 	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
2022-08-16T02:33:54.7122591Z Aug 16 02:33:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-16T02:33:54.7123720Z Aug 16 02:33:54 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-16T02:33:54.7124919Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-08-16T02:33:54.7126454Z Aug 16 02:33:54 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-08-16T02:33:54.7127962Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-08-16T02:33:54.7129466Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-08-16T02:33:54.7130872Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-08-16T02:33:54.7132172Z Aug 16 02:33:54 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-08-16T02:33:54.7133358Z Aug 16 02:33:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-08-16T02:33:54.7134409Z Aug 16 02:33:54 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-08-16T02:33:54.7135475Z Aug 16 02:33:54 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-08-16T02:33:54.7136530Z Aug 16 02:33:54 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-08-16T02:33:54.7137793Z Aug 16 02:33:54 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-08-16T02:33:54.7138882Z Aug 16 02:33:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-08-16T02:33:54.7139851Z Aug 16 02:33:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T02:33:54.7141054Z Aug 16 02:33:54 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-08-16T02:33:54.7142108Z Aug 16 02:33:54 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-08-16T02:33:54.7143090Z Aug 16 02:33:54 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-08-16T02:33:54.7144138Z Aug 16 02:33:54 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-08-16T02:33:54.7145216Z Aug 16 02:33:54 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-08-16T02:33:54.7146262Z Aug 16 02:33:54 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-08-16T02:33:54.7147460Z Aug 16 02:33:54 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-08-16T02:33:54.7148504Z Aug 16 02:33:54 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-08-16T02:33:54.7149483Z Aug 16 02:33:54 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-08-16T02:33:54.7150525Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-16T02:33:54.7151788Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-16T02:33:54.7152997Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-16T02:33:54.7154220Z Aug 16 02:33:54 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-16T02:33:54.7155427Z Aug 16 02:33:54 Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
2022-08-16T02:33:54.7156924Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
2022-08-16T02:33:54.7158987Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
2022-08-16T02:33:54.7160411Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
2022-08-16T02:33:54.7161891Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:723)
2022-08-16T02:33:54.7163204Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
2022-08-16T02:33:54.7164520Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699)
2022-08-16T02:33:54.7165749Z Aug 16 02:33:54 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:666)
2022-08-16T02:33:54.7166969Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-08-16T02:33:54.7168473Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
2022-08-16T02:33:54.7169553Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-08-16T02:33:54.7170693Z Aug 16 02:33:54 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-08-16T02:33:54.7171631Z Aug 16 02:33:54 	at java.lang.Thread.run(Thread.java:748)
2022-08-16T02:33:54.7172910Z Aug 16 02:33:54 Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_08a489791a4e7fcd83ae029ef13928c6_(4/4) from any of the 1 provided restore options.
2022-08-16T02:33:54.7174517Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
2022-08-16T02:33:54.7175999Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
2022-08-16T02:33:54.7177669Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
2022-08-16T02:33:54.7178709Z Aug 16 02:33:54 	... 11 more
2022-08-16T02:33:54.7181176Z Aug 16 02:33:54 Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/junit5354320522396578132/junit5523751389985007360/8145786fe678df6770655845be5f9dc9/dstl/95c98d3b-a010-44e8-813c-3f5d0e4af9a0 (No such file or directory)
2022-08-16T02:33:54.7182986Z Aug 16 02:33:54 	at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
2022-08-16T02:33:54.7184444Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
2022-08-16T02:33:54.7186197Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
2022-08-16T02:33:54.7188080Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
2022-08-16T02:33:54.7189740Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
2022-08-16T02:33:54.7191428Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.DeactivatedChangelogStateBackend.restore(DeactivatedChangelogStateBackend.java:70)
2022-08-16T02:33:54.7193056Z Aug 16 02:33:54 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
2022-08-16T02:33:54.7194769Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
2022-08-16T02:33:54.7196452Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
2022-08-16T02:33:54.7198411Z Aug 16 02:33:54 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
2022-08-16T02:33:54.7199429Z Aug 16 02:33:54 	... 13 more
2022-08-16T02:33:54.7201525Z Aug 16 02:33:54 Caused by: java.io.FileNotFoundException: /tmp/junit5354320522396578132/junit5523751389985007360/8145786fe678df6770655845be5f9dc9/dstl/95c98d3b-a010-44e8-813c-3f5d0e4af9a0 (No such file or directory)
2022-08-16T02:33:54.7202933Z Aug 16 02:33:54 	at java.io.FileInputStream.open0(Native Method)
2022-08-16T02:33:54.7203887Z Aug 16 02:33:54 	at java.io.FileInputStream.open(FileInputStream.java:195)
2022-08-16T02:33:54.7204911Z Aug 16 02:33:54 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
2022-08-16T02:33:54.7206087Z Aug 16 02:33:54 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
2022-08-16T02:33:54.7207582Z Aug 16 02:33:54 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
2022-08-16T02:33:54.7209065Z Aug 16 02:33:54 	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
2022-08-16T02:33:54.7210405Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
2022-08-16T02:33:54.7212048Z Aug 16 02:33:54 	at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89)
2022-08-16T02:33:54.7213573Z Aug 16 02:33:54 	at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
2022-08-16T02:33:54.7215163Z Aug 16 02:33:54 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
2022-08-16T02:33:54.7216361Z Aug 16 02:33:54 	... 21 more
{code}

[~masteryhx] Could you help take another look? Thx;;;","13/Sep/22 03:24;hxb;Given that it hasn't appeared for a month, I will downgrade the priority to major.;;;","26/Sep/22 02:47;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41318&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798;;;","25/Nov/22 03:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43448&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10007;;;","25/Nov/22 04:39;mapohl;[~masteryhx] may you have another look?;;;","30/Nov/22 13:43;mapohl;FLINK-28898 and FLINK-28440 both are caused by some temporary file not being found anymore.;;;","30/Nov/22 13:57;mapohl;FLINK-28440 appeared to be a duplicate of this issue based on the stacktrace. Therefore, the cause of this issue also affects the stability of {{EventTimeWindowCheckpointingITCase.testSlidingTimeWindow}};;;","30/Nov/22 15:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43615&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8171;;;","09/Jan/23 08:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44558&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=7616;;;","11/Jan/23 10:27;mapohl;I took a look at the stacktraces: Only the issue description has a stacktrace pointing to a missing metafile. All the other build failures were caused by an Exception in while creating the {{StreamOperatorStateContext}}, which will be handled in FLINK-28898. Therefore, reopening this issue was a mistake.;;;",,,,,,,,,
Fail to use udf in added jar when enabling checkpoint,FLINK-28897,13476071,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,lsy,Jiangang,Jiangang,10/Aug/22 03:38,11/Mar/24 13:26,04/Jun/24 20:41,,1.16.0,,,,1.20.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,stale-assigned,,,,,"The problem can be reproduced when enabling checkpoint for that StreamingJobGraphGenerator.preValidate is called actually in this case. Maybe this is a classloader problem.

The reproduced steps are as following:
{code:java}
// Enable checkpoint first and execute the command in sql client.
ADD JAR  '~/flink/flink-end-to-end-tests/flink-sql-client-test/target/SqlToolbox.jar';
create function func1 as 'org.apache.flink.table.toolbox.StringRegexReplaceFunction' LANGUAGE JAVA;
SELECT id, func1(str, 'World', 'Flink') FROM (VALUES (1, 'Hello World')) AS T(id, str); {code}
The output is as following:
{code:java}
/* 1 */
/* 2 */      public class StreamExecCalc$11 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$4;
/* 7 */
/* 8 */        private final org.apache.flink.table.data.binary.BinaryStringData str$6 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""World"");
/* 9 */
/* 10 */
/* 11 */        private final org.apache.flink.table.data.binary.BinaryStringData str$7 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""Flink"");
/* 12 */
/* 13 */        private transient org.apache.flink.table.toolbox.StringRegexReplaceFunction function_org$apache$flink$table$toolbox$StringRegexReplaceFunction;
/* 14 */        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$8;
/* 15 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(2);
/* 16 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 17 */
/* 18 */        public StreamExecCalc$11(
/* 19 */            Object[] references,
/* 20 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 21 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 22 */            org.apache.flink.streaming.api.operators.Output output,
/* 23 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 24 */          this.references = references;
/* 25 */          typeSerializer$4 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
/* 26 */          function_org$apache$flink$table$toolbox$StringRegexReplaceFunction = (((org.apache.flink.table.toolbox.StringRegexReplaceFunction) references[1]));
/* 27 */          converter$8 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[2]));
/* 28 */          this.setup(task, config, output);
/* 29 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 30 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 31 */              .setProcessingTimeService(processingTimeService);
/* 32 */          }
/* 33 */        }
/* 34 */
/* 35 */        @Override
/* 36 */        public void open() throws Exception {
/* 37 */          super.open();
/* 38 */
/* 39 */          function_org$apache$flink$table$toolbox$StringRegexReplaceFunction.open(new org.apache.flink.table.functions.FunctionContext(getRuntimeContext()));
/* 40 */
/* 41 */
/* 42 */          converter$8.open(getRuntimeContext().getUserCodeClassLoader());
/* 43 */
/* 44 */        }
/* 45 */
/* 46 */        @Override
/* 47 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 48 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 49 */
/* 50 */          int field$2;
/* 51 */          boolean isNull$2;
/* 52 */          org.apache.flink.table.data.binary.BinaryStringData field$3;
/* 53 */          boolean isNull$3;
/* 54 */          org.apache.flink.table.data.binary.BinaryStringData field$5;
/* 55 */          java.lang.String externalResult$9;
/* 56 */          org.apache.flink.table.data.binary.BinaryStringData result$10;
/* 57 */          boolean isNull$10;
/* 58 */
/* 59 */
/* 60 */          isNull$2 = in1.isNullAt(0);
/* 61 */          field$2 = -1;
/* 62 */          if (!isNull$2) {
/* 63 */            field$2 = in1.getInt(0);
/* 64 */          }
/* 65 */
/* 66 */          isNull$3 = in1.isNullAt(1);
/* 67 */          field$3 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 68 */          if (!isNull$3) {
/* 69 */            field$3 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1));
/* 70 */          }
/* 71 */          field$5 = field$3;
/* 72 */          if (!isNull$3) {
/* 73 */            field$5 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$4.copy(field$5));
/* 74 */          }
/* 75 */
/* 76 */
/* 77 */          out.setRowKind(in1.getRowKind());
/* 78 */
/* 79 */
/* 80 */
/* 81 */
/* 82 */          out.setInt(0, field$2);
/* 83 */
/* 84 */
/* 85 */
/* 86 */
/* 87 */
/* 88 */
/* 89 */
/* 90 */          externalResult$9 = (java.lang.String) function_org$apache$flink$table$toolbox$StringRegexReplaceFunction
/* 91 */            .eval(isNull$3 ? null : ((java.lang.String) converter$8.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$5)), false ? null : ((java.lang.String) converter$8.toExternal((org.apache.flink.table.data.binary.BinaryStringData) ((org.apache.flink.table.data.binary.BinaryStringData) str$6))), false ? null : ((java.lang.String) converter$8.toExternal((org.apache.flink.table.data.binary.BinaryStringData) ((org.apache.flink.table.data.binary.BinaryStringData) str$7))));
/* 92 */
/* 93 */          isNull$10 = externalResult$9 == null;
/* 94 */          result$10 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 95 */          if (!isNull$10) {
/* 96 */            result$10 = (org.apache.flink.table.data.binary.BinaryStringData) converter$8.toInternalOrNull((java.lang.String) externalResult$9);
/* 97 */          }
/* 98 */
/* 99 */          if (isNull$10) {
/* 100 */            out.setNullAt(1);
/* 101 */          } else {
/* 102 */            out.setNonPrimitiveValue(1, result$10);
/* 103 */          }
/* 104 */
/* 105 */
/* 106 */          output.collect(outElement.replace(out));
/* 107 */
/* 108 */
/* 109 */        }
/* 110 */
/* 111 */
/* 112 */
/* 113 */        @Override
/* 114 */        public void close() throws Exception {
/* 115 */           super.close();
/* 116 */
/* 117 */          function_org$apache$flink$table$toolbox$StringRegexReplaceFunction.close();
/* 118 */
/* 119 */        }
/* 120 */
/* 121 */
/* 122 */      }
/* 123 *//* 1 */
/* 2 */      public class StreamExecCalc$11 extends org.apache.flink.table.runtime.operators.TableStreamOperator
/* 3 */          implements org.apache.flink.streaming.api.operators.OneInputStreamOperator {
/* 4 */
/* 5 */        private final Object[] references;
/* 6 */        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$4;
/* 7 */
/* 8 */        private final org.apache.flink.table.data.binary.BinaryStringData str$6 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""World"");
/* 9 */
/* 10 */
/* 11 */        private final org.apache.flink.table.data.binary.BinaryStringData str$7 = org.apache.flink.table.data.binary.BinaryStringData.fromString(""Flink"");
/* 12 */
/* 13 */        private transient org.apache.flink.table.toolbox.StringRegexReplaceFunction function_org$apache$flink$table$toolbox$StringRegexReplaceFunction;
/* 14 */        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$8;
/* 15 */        org.apache.flink.table.data.BoxedWrapperRowData out = new org.apache.flink.table.data.BoxedWrapperRowData(2);
/* 16 */        private final org.apache.flink.streaming.runtime.streamrecord.StreamRecord outElement = new org.apache.flink.streaming.runtime.streamrecord.StreamRecord(null);
/* 17 */
/* 18 */        public StreamExecCalc$11(
/* 19 */            Object[] references,
/* 20 */            org.apache.flink.streaming.runtime.tasks.StreamTask task,
/* 21 */            org.apache.flink.streaming.api.graph.StreamConfig config,
/* 22 */            org.apache.flink.streaming.api.operators.Output output,
/* 23 */            org.apache.flink.streaming.runtime.tasks.ProcessingTimeService processingTimeService) throws Exception {
/* 24 */          this.references = references;
/* 25 */          typeSerializer$4 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
/* 26 */          function_org$apache$flink$table$toolbox$StringRegexReplaceFunction = (((org.apache.flink.table.toolbox.StringRegexReplaceFunction) references[1]));
/* 27 */          converter$8 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[2]));
/* 28 */          this.setup(task, config, output);
/* 29 */          if (this instanceof org.apache.flink.streaming.api.operators.AbstractStreamOperator) {
/* 30 */            ((org.apache.flink.streaming.api.operators.AbstractStreamOperator) this)
/* 31 */              .setProcessingTimeService(processingTimeService);
/* 32 */          }
/* 33 */        }
/* 34 */
/* 35 */        @Override
/* 36 */        public void open() throws Exception {
/* 37 */          super.open();
/* 38 */
/* 39 */          function_org$apache$flink$table$toolbox$StringRegexReplaceFunction.open(new org.apache.flink.table.functions.FunctionContext(getRuntimeContext()));
/* 40 */
/* 41 */
/* 42 */          converter$8.open(getRuntimeContext().getUserCodeClassLoader());
/* 43 */
/* 44 */        }
/* 45 */
/* 46 */        @Override
/* 47 */        public void processElement(org.apache.flink.streaming.runtime.streamrecord.StreamRecord element) throws Exception {
/* 48 */          org.apache.flink.table.data.RowData in1 = (org.apache.flink.table.data.RowData) element.getValue();
/* 49 */
/* 50 */          int field$2;
/* 51 */          boolean isNull$2;
/* 52 */          org.apache.flink.table.data.binary.BinaryStringData field$3;
/* 53 */          boolean isNull$3;
/* 54 */          org.apache.flink.table.data.binary.BinaryStringData field$5;
/* 55 */          java.lang.String externalResult$9;
/* 56 */          org.apache.flink.table.data.binary.BinaryStringData result$10;
/* 57 */          boolean isNull$10;
/* 58 */
/* 59 */
/* 60 */          isNull$2 = in1.isNullAt(0);
/* 61 */          field$2 = -1;
/* 62 */          if (!isNull$2) {
/* 63 */            field$2 = in1.getInt(0);
/* 64 */          }
/* 65 */
/* 66 */          isNull$3 = in1.isNullAt(1);
/* 67 */          field$3 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 68 */          if (!isNull$3) {
/* 69 */            field$3 = ((org.apache.flink.table.data.binary.BinaryStringData) in1.getString(1));
/* 70 */          }
/* 71 */          field$5 = field$3;
/* 72 */          if (!isNull$3) {
/* 73 */            field$5 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$4.copy(field$5));
/* 74 */          }
/* 75 */
/* 76 */
/* 77 */          out.setRowKind(in1.getRowKind());
/* 78 */
/* 79 */
/* 80 */
/* 81 */
/* 82 */          out.setInt(0, field$2);
/* 83 */
/* 84 */
/* 85 */
/* 86 */
/* 87 */
/* 88 */
/* 89 */
/* 90 */          externalResult$9 = (java.lang.String) function_org$apache$flink$table$toolbox$StringRegexReplaceFunction
/* 91 */            .eval(isNull$3 ? null : ((java.lang.String) converter$8.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$5)), false ? null : ((java.lang.String) converter$8.toExternal((org.apache.flink.table.data.binary.BinaryStringData) ((org.apache.flink.table.data.binary.BinaryStringData) str$6))), false ? null : ((java.lang.String) converter$8.toExternal((org.apache.flink.table.data.binary.BinaryStringData) ((org.apache.flink.table.data.binary.BinaryStringData) str$7))));
/* 92 */
/* 93 */          isNull$10 = externalResult$9 == null;
/* 94 */          result$10 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 95 */          if (!isNull$10) {
/* 96 */            result$10 = (org.apache.flink.table.data.binary.BinaryStringData) converter$8.toInternalOrNull((java.lang.String) externalResult$9);
/* 97 */          }
/* 98 */
/* 99 */          if (isNull$10) {
/* 100 */            out.setNullAt(1);
/* 101 */          } else {
/* 102 */            out.setNonPrimitiveValue(1, result$10);
/* 103 */          }
/* 104 */
/* 105 */
/* 106 */          output.collect(outElement.replace(out));
/* 107 */
/* 108 */
/* 109 */        }
/* 110 */
/* 111 */
/* 112 */
/* 113 */        @Override
/* 114 */        public void close() throws Exception {
/* 115 */           super.close();
/* 116 */
/* 117 */          function_org$apache$flink$table$toolbox$StringRegexReplaceFunction.close();
/* 118 */
/* 119 */        }
/* 120 */
/* 121 */
/* 122 */      }
/* 123 */[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 13, Column 30: Cannot determine simple type name ""org"" {code}
The log stack is as following:
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Could not execute SQL statement.
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:208) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQuery(LocalExecutor.java:228) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:537) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.callOperation(CliClient.java:444) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeOperation(CliClient.java:371) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:328) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:279) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:227) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
Caused by: org.apache.flink.table.api.TableException: Failed to execute sql
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:896) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 11 more
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:120) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:51) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:462) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:205) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:145) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1016) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.execute(AbstractSessionClusterExecutor.java:71) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2199) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:877) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 11 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:120) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:51) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:462) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:205) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:145) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1016) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.execute(AbstractSessionClusterExecutor.java:71) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2199) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:877) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 11 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:120) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:51) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:462) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:205) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:145) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1016) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.execute(AbstractSessionClusterExecutor.java:71) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2199) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:877) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 11 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 13, Column 30: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$1300(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$25.getType(UnitCompiler.java:8271) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6873) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$14400(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6499) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitFieldAccess(UnitCompiler.java:6494) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$FieldAccess.accept(Java.java:4310) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6855) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$14200(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6497) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2$1.visitAmbiguousName(UnitCompiler.java:6494) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:4224) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6494) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22$2.visitLvalue(UnitCompiler.java:6490) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$Lvalue.accept(Java.java:4148) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9026) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.generated.GeneratedClass.getClass(GeneratedClass.java:120) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.getStreamOperatorClass(CodeGenOperatorFactory.java:51) ~[flink-table-runtime-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:462) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:205) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:145) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:1016) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.client.deployment.executors.AbstractSessionClusterExecutor.execute(AbstractSessionClusterExecutor.java:71) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2199) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95) ~[?:?]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:877) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375) ~[flink-table-api-java-uber-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
    ... 11 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 11 13:26:01 UTC 2024,,,,,,,,,,"0|z17mow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 04:14;lsy;[~Jiangang] Thanks for report it, I will take a look.;;;","10/Aug/22 06:46;Jiangang;A related issue is https://issues.apache.org/jira/browse/FLINK-16662;;;","06/Sep/22 10:04;jark;Fixed in 
 - master: 97f5a45cd035fbae37a7468c6f771451ddb4a0a4
 - release-1.16: f07df6864e5da21473bf7396774b71ee5482c290;;;","17/Jan/23 18:45;charles-tan;[~jark] [~lsy] Can we re-open this issue? I believe this issue still persists in Flink 1.16.0. When testing UDF in the following example, I noticed that when checkpoints are disabled the test passes, but when checkpoints are enabled the same test fails.
{code:java}
    /**
     * This one works on Flink 1.16.0
     * @throws Exception
     */
    @Test
    public void shouldGenerateFlinkJobForInteractiveQueryWithUDFSuccessfully() throws Exception {
        final Path jarPath = Paths.get(TEST_FUNCTIONS_LOCATION, ""user-functions.jar"");
        final String jarPathString = String.format(""%s%s"", ""file://"", jarPath.toAbsolutePath());
        final Configuration config = new Configuration();
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);
        final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);        final String functionClassName = ""util.LowerCase"";        tEnv.executeSql(String.format(""create temporary system function LowerCase as '%s' using jar '%s'"", functionClassName, jarPathString));        final Table table = tEnv.fromValues(
            DataTypes.ROW(
                DataTypes.FIELD(""id"", DataTypes.DECIMAL(10, 2)),
                DataTypes.FIELD(""name"", DataTypes.STRING())
            ),
            row(1, ""ABC""),
            row(2 L, ""ABCDE"")
        );

        final CloseableIterator < Row > iter = tEnv.sqlQuery(""SELECT LowerCase(name) as name FROM "" + table).execute().collect();
        final List < Row > list = new ArrayList < > ();
        iter.forEachRemaining(list::add);

        System.out.println("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>"");
        System.out.println(list);
    }

    /**
     * Same as test above but with checkpointing enabled.
     * This one does not work on Flink 1.16.0 but should, enabling checkpointing somehow breaks UDFs.
     * @throws Exception
     */
    @Test
    public void shouldGenerateFlinkJobForInteractiveQueryWithUDFSuccessfullyWithCheckpointing() throws Exception {
        final Path jarPath = Paths.get(TEST_FUNCTIONS_LOCATION, ""user-functions.jar"");
        final String jarPathString = String.format(""%s%s"", ""file://"", jarPath.toAbsolutePath());
        final Configuration config = new Configuration();
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);
        env.enableCheckpointing(1000 L);
        final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);        final String functionClassName = ""util.LowerCase"";        tEnv.executeSql(String.format(""create temporary system function LowerCase as '%s' using jar '%s'"", functionClassName, jarPathString));        final Table table = tEnv.fromValues(
            DataTypes.ROW(
                DataTypes.FIELD(""id"", DataTypes.DECIMAL(10, 2)),
                DataTypes.FIELD(""name"", DataTypes.STRING())
            ),
            row(1, ""ABC""),
            row(2 L, ""ABCDE"")
        );

        final CloseableIterator < Row > iter = tEnv.sqlQuery(""SELECT LowerCase(name) as name FROM "" + table).execute().collect();
        final List < Row > list = new ArrayList < > ();
        iter.forEachRemaining(list::add);

        System.out.println("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>"");
        System.out.println(list);
    } {code}
Code repo: [https://github.com/charles-tan/udfs-flink-1.16/blob/main/src/test/java/com/example/UDFTest.java] 

Related Issue: https://issues.apache.org/jira/browse/FLINK-29890 ;;;","20/Jan/23 15:50;martijnvisser;Re-opened as explained on the mailing list due to the report of [~charles-tan]. I'll leave it up to the other to decide if this ultimately needs to become a new ticket or not. ;;;","28/Jan/23 10:53;jark;I reproduced this bug on the master branch. I think this bug was fixed for pure SQL but not completely fixed for the Table API case. Could you have a look again [~lsy]? ;;;","30/Jan/23 11:31;lsy;As [~jark] says, we only fix the bug for pure SQL cases, but for table API case, we need the runtime to support it. I've created one issue early to track this bug for table API, see https://issues.apache.org/jira/browse/FLINK-29240 for detail, cc [~zhuzh] , can you help to take a look?;;;","23/Feb/23 12:34;martijnvisser;[~lsy] What is the user impact of this bug? Does it mean that we have a piece of broken functionality compared to older Flink versions?;;;","24/Feb/23 09:33;lsy;[~martijnvisser] It only impacts users who use the Table API and udf which is added by the `add jar ` or `create function ... using jar` syntax, and enabling the checkpoint simultaneously in streaming mode. I don't think this is a 
broken functionality compared to older Flink versions. Before Flink 1.16, we don't support `ADD JAR` syntax at Table API level, just support it on SqlClient Side, but from 1.17, we extend this feature to be more generic. For users of the Table API, this is a new feature, except that there are some problems that we need to fix. For the problem https://issues.apache.org/jira/browse/FLINK-29240 , we need to work together with runtime module related experts to provide a more general classloader.;;;","24/Feb/23 10:35;martijnvisser;[~lsy] Thans for the clarification. Should we include a release note when 1.17 is released about this functionality not yet working properly for Table API users? ;;;","27/Feb/23 14:31;jark;A release note guides users on how to upgrade from an old version. Considering this is a new feature, I think we may need to add a note after the feature introduction in the documentation.;;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","11/Mar/24 13:26;lincoln.86xy;[~lsy] Any new thoughts on this issue?;;;",,,,,,,,
[Umbrella] Test Flink Release 1.16,FLINK-28896,13476068,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,hxbks2ks,hxbks2ks,10/Aug/22 03:25,08/Sep/22 02:07,04/Jun/24 20:41,08/Sep/22 02:07,1.16.0,,,,1.16.0,,,,Tests,,,,,,,0,release-testing,,,,,,"This is an umbrella ticket for the Flink 1.16 testing efforts. Please prepare for the release testing by creating child testing tasks for the new features. Tickets should be opened with Priority Blocker, FixVersion 1.16.0 and Label release-testing
(testing tasks only).
At the meantime, you need to update column value of `X-team verified` in the
https://cwiki.apache.org/confluence/display/FLINK/1.16+Release#id-1.16Release-List.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-10 03:25:39.0,,,,,,,,,,"0|z17mo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eliminate RowRowConverter between RowData source and sink,FLINK-28895,13476049,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/Aug/22 01:59,10/Aug/22 11:14,04/Jun/24 20:41,10/Aug/22 11:14,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 11:14:25 UTC 2022,,,,,,,,,,"0|z17mk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 11:14;dianfu;Merged to master via 6844be2128dd4c6ec62fe6e0b5230d885393e1ce;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer for Interaction,FLINK-28894,13475966,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,09/Aug/22 12:30,17/Aug/22 06:13,04/Jun/24 20:41,17/Aug/22 06:13,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,Add Transformer for Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 06:13:03 UTC 2022,,,,,,,,,,"0|z17m20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 06:13;zhangzp;fixed on master via c33e51d57eb2db9436e4b9bd11dd728834620289;;;",,,,,,,,,,,,,,,,,,,,
Add conv function supported in SQL & Table API,FLINK-28893,13475954,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,jackylau,jackylau,09/Aug/22 12:05,11/Mar/24 12:44,04/Jun/24 20:41,,1.17.0,,,,1.20.0,,,,Table SQL / API,,,,,,,0,pull-request-available,stale-assigned,,,,,"Returns true if {{array}} contains {{{}value{}}}.

Syntax:
{code:java}
conv(number, fromBase, toBase) {code}
Arguments:
 * number: INTEGER_NUMERIC or CHARACTER_STRING family.

 * fromBase: INTEGER_NUMERIC family.
 * toBase: INTEGER_NUMERIC family.

Returns:

Converts numbers between different number bases. Returns a string representation of the number {_}{{N}}{_}, converted from base _{{from_base}}_ to base {_}{{to_base}}{_}. Returns {{NULL}} if any argument is {{{}NULL{}}}. The argument _{{N}}_ is interpreted as an integer, but may be specified as an integer or a string. The minimum base is {{2}} and the maximum base is {{{}36{}}}. If _{{from_base}}_ is a negative number, _{{N}}_ is regarded as a signed number. Otherwise, _{{N}}_ is treated as unsigned. [{{CONV()}}|https://dev.mysql.com/doc/refman/8.0/en/mathematical-functions.html#function_conv] works with 64-bit precision.

Examples:
{code:java}
> SELECT CONV(100, 10, -8);
 ""144""
> SELECT CONV(1111, 2, 10);
 ""15""
> SELECT CONV(100, 2, NULL);
 NULL {code}
See more:
 * pg: [https://dev.mysql.com/doc/refman/8.0/en/mathematical-functions.html#function_conv]
 * mariadb: [https://mariadb.com/kb/en/conv/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 10:35:13 UTC 2023,,,,,,,,,,"0|z17lzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Update NOTICE for flink-kubernetes-operator 1.1.1,FLINK-28892,13475943,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,ConradJam,gyfora,gyfora,09/Aug/22 11:08,15/Aug/22 12:13,04/Jun/24 20:41,15/Aug/22 12:13,kubernetes-operator-1.1.1,,,,kubernetes-operator-1.1.1,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"We should update the NOTICE file in the flink-kubernetes-operator subproject as the okhttp changes introduced a few things.

To get the list of dependencies:
{code:java}
mvn clean install -DskipTests -pl flink-kubernetes-operator | grep Including | grep -v org.apache.flink | cut -d' ' -f3 | sort{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 12:13:30 UTC 2022,,,,,,,,,,"0|z17lww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 08:39;ConradJam;[~gyfora] I want to take this ticket;;;","15/Aug/22 12:13;gyfora;merged to release-1.1: c94ba1701ca84682bc97a7fe46ac954800dfaa11;;;",,,,,,,,,,,,,,,,,,,
Upgrade google-cloud-libraries-bom version to 25.0.0,FLINK-28891,13475939,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bilna123,bilna123,09/Aug/22 10:41,16/Sep/22 07:29,04/Jun/24 20:41,,,,,,,,,,Connectors / Google Cloud PubSub,,,,,,,0,,,,,,,"*CVE-2022-25647*
In flink-connector-gcp-pubsub, the google-cloud-pubsub version is pulled from
google-cloud-bom (loaded via the libraries-bom) and libraries-bom version in 1.13.6 is 8.1.0. The the google-cloud-pubsub version pulled thorigh this is 1.108.0
https://mvnrepository.com/artifact/com.google.cloud/libraries-bom/8.1.0
 
The dependecny google-cloud-pubsub:1.108.0 has com.google.code.gson:gson:jar:2.8.6 which is vulnerable
https://search.maven.org/artifact/com.google.cloud/google-cloud-pubsub/1.108.0/jar
 
The google-cloud-pubsub:1.116.0 onwards the gson version is 2.9.0.
https://search.maven.org/artifact/com.google.cloud/google-cloud-pubsub/1.116.0/jar
 
So in order to resolve the vulnerability,  google-cloud-libraries-bom version needs to be upgraded to 25.0.0 or higher",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 15 15:44:43 UTC 2022,,,,,,,,,,"0|z17lw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:08;martijnvisser;[~bilna123] Do you want to want to work on a fix for this?;;;","15/Sep/22 15:44;bilna123;[~martijnvisser] I would love to contribute. But currently I am occupied with the sprint cycle.;;;",,,,,,,,,,,,,,,,,,,
Incorrect semantic of latestLoadTime in CachingLookupFunction and CachingAsyncLookupFunction,FLINK-28890,13475937,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,renqs,renqs,09/Aug/22 10:32,23/Sep/22 14:35,04/Jun/24 20:41,23/Sep/22 14:35,1.16.0,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,"The semantic of latestLoadTime in CachingLookupFunction and CachingAsyncLookupFunction is not correct, which should be the time spent for the latest load operation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 23 06:19:22 UTC 2022,,,,,,,,,,"0|z17lvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/22 06:19;renqs;Fixed on master: 24c685a58ef72db4c64c90e37056a07eb562be15

release-1.16: 0830c2ac819fd33628d11315b2485a12c23af5e6;;;",,,,,,,,,,,,,,,,,,,,
Hybrid shuffle should supports multiple consumer,FLINK-28889,13475930,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Done,Weijie Guo,Weijie Guo,Weijie Guo,09/Aug/22 09:49,31/Oct/22 03:33,04/Jun/24 20:41,31/Oct/22 03:33,1.16.0,,,,1.17.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Hybrid shuffle does not support multiple consumer for single subpartition data. This will bring some defects, such as the inability to support partition reuse, speculative execution. In particular, it cannot support broadcast optimization, that is, hybrid shuffle writes multiple copies of broadcast data. This will cause a waste of memory and disk space and affect the performance of shuffle write phase. Ideally, for the full spilling strategy, any broadcast data (record or event) should only write one piece of data in the memory, and the same is true for the disk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 31 03:33:59 UTC 2022,,,,,,,,,,"0|z17lu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 03:33;xtsong;master (1.17): d11940c4a78c71548b5a06af50da2e5f9cb68918;;;",,,,,,,,,,,,,,,,,,,,
The statistics of HsResultPartition are not updated correctly,FLINK-28888,13475928,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,09/Aug/22 09:44,11/Aug/22 10:19,04/Jun/24 20:41,11/Aug/22 10:19,1.16.0,,,,1.16.0,,,,Runtime / Metrics,,,,,,,0,pull-request-available,,,,,,"The statistics of HsResultPartition are not updated correctly, such as numBuffersOut, numBytesProduced and numBytesOut. This will cause bytes sent not to be displayed correctly on the Web UI, and will also affect the subsequent support for adaptive BatchScheduler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 10:19:16 UTC 2022,,,,,,,,,,"0|z17ltk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 10:19;xtsong;master (1.16): e39fb24ecd77306b57130d01226608aea8ce5d96;;;",,,,,,,,,,,,,,,,,,,,
Fix the bug of custom metrics in Thread Mode,FLINK-28887,13475925,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,09/Aug/22 09:31,11/Aug/22 12:39,04/Jun/24 20:41,11/Aug/22 12:39,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 12:39:52 UTC 2022,,,,,,,,,,"0|z17lsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 12:39;hxbks2ks;Merged into master via 4ebb787ff354e5c54ea4c55d712bab6220d9ed55;;;",,,,,,,,,,,,,,,,,,,,
Support HybridSource in Python DataStream API,FLINK-28886,13475924,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,09/Aug/22 09:30,09/Aug/22 15:56,04/Jun/24 20:41,09/Aug/22 15:56,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 15:56:31 UTC 2022,,,,,,,,,,"0|z17lso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 15:56;dianfu;Merged to master via fda2bdd05c75587b81205e72ca010d0022a44616;;;",,,,,,,,,,,,,,,,,,,,
Improve LookupFunction to support different kinds of RowData as key row,FLINK-28885,13475917,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,09/Aug/22 09:17,09/Aug/22 09:17,04/Jun/24 20:41,,,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,,,Currently LookupFunction uses {{GenericRowData}} as a wrapper of key fields in key row by using {{GenericRowData#of}} . We could improve the implementation here to use other type like BinaryRowData and so forth. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-09 09:17:18.0,,,,,,,,,,"0|z17lr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downstream task may never be notified of data available in hybrid shuffle when number of credits is zero.,FLINK-28884,13475894,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,09/Aug/22 07:36,11/Aug/22 09:48,04/Jun/24 20:41,11/Aug/22 09:48,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Downstream task may never be notified of data available in hybrid shuffle when number of credits is zero.

There are two potential problems:
 # HsSubpartitionView should be initialized to a notifiable state, there may be a problem of never consuming otherwise. Imagine the following situation: Downstream task has no initial credit(i.e. exclusive buffers is configured to zero), if there is no data output in the upstream, it will feedback a zero backlog to downstream input channel. All subsequent data available notifications will be intercepted as needNotify is false.
 # HsSubpartitionView should be notifiable when downstream get a zero backlog. Generally speaking, if the backlog is zero, when data become available, even if there is no credit, the backlog information will be notified also. However, in the hybrid shuffle, the notification will be ignored. This behavior is incorrect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 09:48:55 UTC 2022,,,,,,,,,,"0|z17lm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 09:48;xtsong;master (1.16): 0b5aa420180729d8792c5d6feecfeb14382f72ee;;;",,,,,,,,,,,,,,,,,,,,
Fix HiveTableSink failed to report metrics to hive metastore,FLINK-28883,13475892,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,Jiangang,Jiangang,09/Aug/22 07:24,26/Aug/22 12:20,04/Jun/24 20:41,26/Aug/22 12:20,1.16.0,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"Currently, HiveTableSink is failed to report metrics to metastores, like file number, total line number and total size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 26 12:20:00 UTC 2022,,,,,,,,,,"0|z17llk:",9223372036854775807,"In batch mode, Hive sink now will report statistics for written tables and partitions to Hive metastore by default. This might be time-consuming when there are many written files. You can disable this feature by setting `table.exec.hive.sink.statistic-auto-gather.enable` to `false`.",,,,,,,,,,,,,,,,,,,"11/Aug/22 04:27;luoyuxia;I'll try to fix it. [~jark] Could you please assign it to me.;;;","26/Aug/22 12:20;jark;Fixed in master: 4399b3fc40d11c2083197b6a505c23c4fcfec6df;;;",,,,,,,,,,,,,,,,,,,
ENCODE return error,FLINK-28882,13475843,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,csq,ana4,ana4,09/Aug/22 03:57,16/Aug/23 10:35,04/Jun/24 20:41,,1.15.0,,,,,,,,Table SQL / API,Table SQL / Planner,,,,,,0,pull-request-available,stale-assigned,,,,,"Run the following in SQL Client, it will return 'k' rather than 'kyuubi' but it returns 'kyuubi' in the 1.14 version.
{code:java}
select encode('kyuubi', 'UTF-8') {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/22 02:44;csq;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13048979/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 10:35:13 UTC 2023,,,,,,,,,,"0|z17lao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/22 02:53;csq;I have encountered the same problem in latest master branch (1.17-SNAPSHOT).
 !screenshot-1.png! 
We can see that the result type of Encode('kyuubi', 'UTF-8') call is BINARY(1). However, the type of RexLiteral `X'6b7975756269'` is BINARY(6)). According to [FLINK-24419 |https://issues.apache.org/jira/browse/FLINK-24419]

{code:java}
    /* Example generated code for BINARY(2):

    // legacy behavior
    ((byte[])(inputValue))

    // new behavior
    ((((byte[])(inputValue)).length == 2) ? (((byte[])(inputValue))) : (java.util.Arrays.copyOf(((byte[])(inputValue)), 2)))

    */
{code}

The BINARY(6) is trimmed to BINARY(1) in BinaryToBinaryCastRule during the generation of CAST call.

There are two optional solutions:
1. Apply the legacy cast behaviour with LegacyCastBehaviour.ENABLED in TableConfig when it is ENCODE call;
2. Make the output type of  `CAST(X'6b7975756269':BINARY(6)):BINARY(1)` to be BINARY(6) that equals to the type of RexLiteral `X'6b7975756269'`.
;;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
PubSubConsumingTest.testStoppingConnectorWhenDeserializationSchemaIndicatesEndOfStream test failure,FLINK-28881,13475841,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,leonard,leonard,09/Aug/22 03:52,10/Sep/23 22:35,04/Jun/24 20:41,,1.16.0,gcp-pubsub-3.0.0,,,,,,,Connectors / Google Cloud PubSub,,,,,,,0,auto-deprioritized-major,test-stability,,,,,"
{code:java}
2022-08-09T03:14:22.0113691Z Aug 09 03:14:22 [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.PubSubConsumingTest.testStoppingConnectorWhenDeserializationSchemaIndicatesEndOfStream  Time elapsed: 1.867 s  <<< FAILURE!
2022-08-09T03:14:22.0114504Z Aug 09 03:14:22 java.lang.AssertionError: 
2022-08-09T03:14:22.0114903Z Aug 09 03:14:22 
2022-08-09T03:14:22.0115263Z Aug 09 03:14:22 Expected: <[1, 2, 3]>
2022-08-09T03:14:22.0115679Z Aug 09 03:14:22      but: was <[1, 2]>
2022-08-09T03:14:22.0116232Z Aug 09 03:14:22 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
2022-08-09T03:14:22.0116871Z Aug 09 03:14:22 	at org.junit.Assert.assertThat(Assert.java:964)
2022-08-09T03:14:22.0117580Z Aug 09 03:14:22 	at org.junit.Assert.assertThat(Assert.java:930)
2022-08-09T03:14:22.0118460Z Aug 09 03:14:22 	at org.apache.flink.streaming.connectors.gcp.pubsub.PubSubConsumingTest.testStoppingConnectorWhenDeserializationSchemaIndicatesEndOfStream(PubSubConsumingTest.java:119)
{code}

CI link: https://dev.azure.com/leonardBang/Azure_CI/_build/results?buildId=713&view=logs&j=3796201e-ea88-5776-0ea8-9ccca648a70c&t=8ca54b76-085e-5cf1-8060-2c500a258258",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 10 22:35:06 UTC 2023,,,,,,,,,,"0|z17la8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/22 08:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43484&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38637;;;","05/Jan/23 07:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44443&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38702;;;","23/Jan/23 07:59;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45137&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38677;;;","16/Mar/23 15:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38460;;;","04/Jul/23 08:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50866&view=logs&j=fc7981dc-d266-55b0-5fff-f0d0a2294e36&t=1a9b228a-3e0e-598f-fc81-c321539dfdbf&l=38464;;;","02/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,
Fix CEP doc with wrong result of strict contiguity of looping patterns,FLINK-28880,13475834,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,Juntao Hu,Juntao Hu,09/Aug/22 03:08,09/Aug/22 03:39,04/Jun/24 20:41,09/Aug/22 03:35,1.15.1,,,,1.14.6,1.15.2,1.16.0,,Library / CEP,,,,,,,0,pull-request-available,,,,,,"https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/libs/cep/#contiguity-within-looping-patterns
The result of strict contiguity should be {a b1 c}, {a b2 c}, {a b3 c}, since b is *followed by* c.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 03:35:30 UTC 2022,,,,,,,,,,"0|z17l8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 03:35;hxbks2ks;Merged into master via a08b050eb9fbea319275771fd9e95bbb025e2737
Merged into release-1.15 via 6eced8aa39c9e16b9918bd8b05fed1a1e17b6fe7
Merged into release-1.14 via c5108f6ab078ffbf1cbc483ad469197988e5553b;;;",,,,,,,,,,,,,,,,,,,,
New File Sink s3 end-to-end test failed with Output hash mismatch,FLINK-28879,13475828,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hxbks2ks,hxbks2ks,09/Aug/22 02:05,01/Oct/23 22:35,04/Jun/24 20:41,,1.16.0,1.17.2,,,,,,,API / DataStream,Connectors / FileSystem,Tests,,,,,0,auto-deprioritized-major,test-stability,,,,,"
{code:java}
2022-08-09T00:50:02.8229585Z Aug 09 00:50:02 FAIL File Streaming Sink: Output hash mismatch.  Got 6037b01ca0ffc61a95c12cb475c661a8, expected 6727342fdd3aae2129e61fc8f433fb6f.
2022-08-09T00:50:02.8230700Z Aug 09 00:50:02 head hexdump of actual:
2022-08-09T00:50:02.8477319Z Aug 09 00:50:02 0000000   E   r   r   o   r       e   x   e   c   u   t   i   n   g    
2022-08-09T00:50:02.8478206Z Aug 09 00:50:02 0000010   a   w   s       c   o   m   m   a   n   d   :       s   3    
2022-08-09T00:50:02.8479475Z Aug 09 00:50:02 0000020   c   p       -   -   q   u   i   e   t       s   3   :   /   /
2022-08-09T00:50:02.8480205Z Aug 09 00:50:02 0000030   f   l   i   n   k   -   i   n   t   e   g   r   a   t   i   o
2022-08-09T00:50:02.8480924Z Aug 09 00:50:02 0000040   n   -   t   e   s   t   s   /   t   e   m   p   /   t   e   s
2022-08-09T00:50:02.8481612Z Aug 09 00:50:02 0000050   t   _   f   i   l   e   _   s   i   n   k   -   1   d   3   d
2022-08-09T00:50:02.8483048Z Aug 09 00:50:02 0000060   4   0   0   8   -   b   0   b   f   -   4   2   6   5   -   b
2022-08-09T00:50:02.8483618Z Aug 09 00:50:02 0000070   e   0   e   -   3   b   9   f   7   8   2   c   5   5   2   d
2022-08-09T00:50:02.8484222Z Aug 09 00:50:02 0000080       /   h   o   s   t   d   i   r   /   /   t   e   m   p   -
2022-08-09T00:50:02.8484831Z Aug 09 00:50:02 0000090   t   e   s   t   -   d   i   r   e   c   t   o   r   y   -   2
2022-08-09T00:50:02.8485719Z Aug 09 00:50:02 00000a0   3   9   3   7   7   8   2   6   8   0   /   t   e   m   p   /
2022-08-09T00:50:02.8486427Z Aug 09 00:50:02 00000b0   t   e   s   t   _   f   i   l   e   _   s   i   n   k   -   1
2022-08-09T00:50:02.8487134Z Aug 09 00:50:02 00000c0   d   3   d   4   0   0   8   -   b   0   b   f   -   4   2   6
2022-08-09T00:50:02.8487826Z Aug 09 00:50:02 00000d0   5   -   b   e   0   e   -   3   b   9   f   7   8   2   c   5
2022-08-09T00:50:02.8488511Z Aug 09 00:50:02 00000e0   5   2   d       -   -   e   x   c   l   u   d   e       '   *
2022-08-09T00:50:02.8489202Z Aug 09 00:50:02 00000f0   '       -   -   i   n   c   l   u   d   e       '   *   /   p
2022-08-09T00:50:02.8489891Z Aug 09 00:50:02 0000100   a   r   t   -   [   !   /   ]   *   '       -   -   r   e   c
2022-08-09T00:50:02.8490385Z Aug 09 00:50:02 0000110   u   r   s   i   v   e  \n                                    
2022-08-09T00:50:02.8490822Z Aug 09 00:50:02 0000117
2022-08-09T00:50:02.8502212Z Aug 09 00:50:02 Stopping job timeout watchdog (with pid=141134)
2022-08-09T00:50:06.8430959Z rm: cannot remove '/home/vsts/work/1/s/flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT/lib/flink-shaded-netty-tcnative-static-*.jar': No such file or directory
2022-08-09T00:50:06.9278248Z Aug 09 00:50:06 5ccfeb22307c2a88625a38b9537acc79001d1b29094ef40fd70692ce11407502
2022-08-09T00:50:06.9618147Z Aug 09 00:50:06 5ccfeb22307c2a88625a38b9537acc79001d1b29094ef40fd70692ce11407502
2022-08-09T00:50:06.9645077Z Aug 09 00:50:06 [FAIL] Test script contains errors.
2022-08-09T00:50:06.9666227Z Aug 09 00:50:06 Checking of logs skipped.
2022-08-09T00:50:06.9671891Z Aug 09 00:50:06 
2022-08-09T00:50:06.9673050Z Aug 09 00:50:06 [FAIL] 'New File Sink s3 end-to-end test' failed after 3 minutes and 42 seconds! Test exited with exit code 1
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39667&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=4136",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 01 22:35:16 UTC 2023,,,,,,,,,,"0|z17l7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/22 07:43;xtsong;This has never been reproduced since reported 2.5 months ago. Closing for now. Feel free to re-open if it happen again.;;;","24/Jul/23 23:34;Sergey Nuyanzin;reopen since it is reproduced https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51629&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=efbee0b1-38ac-597d-6466-1ea8fc908c50&l=4192;;;","23/Sep/23 22:34;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","01/Oct/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,
PipelinedRegionSchedulingITCase.testRecoverFromPartitionException failed with AssertionError,FLINK-28878,13475827,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,hxbks2ks,hxbks2ks,09/Aug/22 01:53,15/Aug/22 03:44,04/Jun/24 20:41,15/Aug/22 03:42,1.14.5,1.15.1,1.16.0,,1.15.2,1.16.0,,,Tests,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-08-08T20:38:43.3934646Z Aug 08 20:38:43 [ERROR] org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.testRecoverFromPartitionException  Time elapsed: 20.288 s  <<< FAILURE!
2022-08-08T20:38:43.3935309Z Aug 08 20:38:43 java.lang.AssertionError: 
2022-08-08T20:38:43.3937070Z Aug 08 20:38:43 
2022-08-08T20:38:43.3938015Z Aug 08 20:38:43 Expected: is <false>
2022-08-08T20:38:43.3940277Z Aug 08 20:38:43      but: was <true>
2022-08-08T20:38:43.3940927Z Aug 08 20:38:43 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
2022-08-08T20:38:43.3941571Z Aug 08 20:38:43 	at org.junit.Assert.assertThat(Assert.java:964)
2022-08-08T20:38:43.3942120Z Aug 08 20:38:43 	at org.junit.Assert.assertThat(Assert.java:930)
2022-08-08T20:38:43.3943202Z Aug 08 20:38:43 	at org.apache.flink.test.scheduling.PipelinedRegionSchedulingITCase.testRecoverFromPartitionException(PipelinedRegionSchedulingITCase.java:98)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39652&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9994",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 03:42:08 UTC 2022,,,,,,,,,,"0|z17l74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 01:55;hxbks2ks;cc [~zhuzh];;;","12/Aug/22 04:19;zhuzh;Thanks for reporting it! [~hxbks2ks]
The test fails due to an unexpected slowness of test running (may be due to an environment slowness). The slowness resulted in a slow request timeout and triggered a failover. This made the job failover number to be 2 instead of the expected 1.
Will increase the slot request timeout to make the tests more stable.;;;","15/Aug/22 03:42;zhuzh;Fixed via 
master: 5f8f387cba774a2c3900ea38e8a3dad017cf1790
release-1.15: ded03b750f46d8636d6744d4e094943d04f787dd;;;",,,,,,,,,,,,,,,,,,
Elasticsearch7DynamicSinkITCase.testWritingDocumentsNoPrimaryKey case failed,FLINK-28877,13475826,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lincoln.86xy,lincoln.86xy,09/Aug/22 01:27,09/Aug/22 02:23,04/Jun/24 20:41,09/Aug/22 02:23,,,,,1.16.0,,,,,,,,,,,0,,,,,,,"{code}
Aug 08 16:00:39 Caused by: java.lang.RuntimeException: An error occurred in ElasticsearchSink. 
Aug 08 16:00:39 at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:426) 
Aug 08 16:00:39 at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:365) 
Aug 08 16:00:39 at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41) 
Aug 08 16:00:39 at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.close(AbstractUdfStreamOperator.java:114) 
Aug 08 16:00:39 at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163) 
Aug 08 16:00:39 at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125) 
Aug 08 16:00:39 at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:1022) 
Aug 08 16:00:39 at org.apache.flink.streaming.runtime.tasks.StreamTask.afterInvoke(StreamTask.java:900) 
Aug 08 16:00:39 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:783) 
Aug 08 16:00:39 at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) 
Aug 08 16:00:39 at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) 
Aug 08 16:00:39 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) 
Aug 08 16:00:39 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) 
Aug 08 16:00:39 at java.lang.Thread.run(Thread.java:748) 
Aug 08 16:00:39 Caused by: java.net.SocketTimeoutException: 30,000 milliseconds timeout on connection http-outgoing-0 [ACTIVE] 
Aug 08 16:00:39 at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387) 
Aug 08 16:00:39 at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92) 
Aug 08 16:00:39 at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) 
Aug 08 16:00:39 at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591)
{code}

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39617&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24095,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-09 01:27:49.0,,,,,,,,,,"0|z17l6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support writing RowData to Orc files in PyFlink,FLINK-28876,13475770,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,08/Aug/22 15:14,10/Aug/22 01:17,04/Jun/24 20:41,10/Aug/22 01:17,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 01:17:09 UTC 2022,,,,,,,,,,"0|z17kug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 01:17;dianfu;Merged to master via cf3beff586a088c20833d8a8fc72bbc37d87a2b1;;;",,,,,,,,,,,,,,,,,,,,
Add FlinkSessionJobControllerTest,FLINK-28875,13475759,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,gyfora,gyfora,08/Aug/22 14:08,19/Dec/22 14:14,04/Jun/24 20:41,15/Dec/22 15:14,,,,,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"There are currently no tests for the FlinkSessionJobController, only unit tests for the individual components of it.

We should add a larger integration style test similar to the FlinkDeploymentControllerTest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 19 14:14:23 UTC 2022,,,,,,,,,,"0|z17ks0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 14:25;aitozi;Agree, I would like to take this ticket;;;","22/Sep/22 12:48;gyfora;are you still working on this [~aitozi] ?;;;","30/Sep/22 03:40;aitozi;Sorry [~gyfora], I have not start this work yet, I will try to push a PR this week;;;","30/Sep/22 07:03;gyfora;No worries :) We are not going to include it in the 1.2.0 release as the RCs are already being prepared. So no rush.;;;","15/Dec/22 15:14;gyfora;merged to main c8ed11dfde1e0fc57d8fa45b0121ecec362bd03d;;;","19/Dec/22 14:14;gyfora;merged to release-1.3 69f7e94808cbded27f2238bf3039b9d693009f4a;;;",,,,,,,,,,,,,,,
Python tests fail locally with flink-connector-hive_2.12 is missing jhyde pentaho-aggdesigner-algorithm:jar:5.1.5-jhyde ,FLINK-28874,13475748,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Bug,,Sergey Nuyanzin,Sergey Nuyanzin,08/Aug/22 12:54,09/Aug/22 13:31,04/Jun/24 20:41,09/Aug/22 06:59,,,,,,,,,Test Infrastructure,Tests,,,,,,0,pull-request-available,,,,,,"this command fails  {{./tools/ci/test_controller.sh python}}

The reason is a newer maven version blocks http repositories.

There were similar issues e.g. https://issues.apache.org/jira/browse/FLINK-27640

The suggestion is to use maven wrapper in {{run_mvn}} in {{tools/ci/maven-utils.sh}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 06:59:12 UTC 2022,,,,,,,,,,"0|z17kpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 17:14;chesnay;You should never run that script locally outside of a flink-ci docker image.

If you want to run the python tests locally, call {{flink-python/dev/lint-python.sh}}.;;;","09/Aug/22 06:59;Sergey Nuyanzin;Ok, thank you for clarification;;;",,,,,,,,,,,,,,,,,,,
"Make ""jobmanager.scheduler"" visible in documentation",FLINK-28873,13475745,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,08/Aug/22 12:48,09/Aug/22 14:25,04/Jun/24 20:41,09/Aug/22 14:25,,,,,1.16.0,,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,,,"Currently, the option {{jobmanager.scheduler}} is still excluded from documentation. But in fact, this option is already used as a public interface (this option needs to be configured by users when using AdaptiveScheduler and AdaptiveBatchScheduler).

We should remove the {{ExcludeFromDocumentation}} to make it visible in the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 14:25:28 UTC 2022,,,,,,,,,,"0|z17kow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 13:44;wanglijie;cc [~zhuzh] ;;;","09/Aug/22 14:25;zhuzh;Fixed via 15f87d4a470e9bf29fd18874c26c4506ea57c09f;;;",,,,,,,,,,,,,,,,,,,
UnalignedCheckpointStressITCase fails with NoSuchFileException,FLINK-28872,13475731,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,frozen stone,frozen stone,08/Aug/22 11:41,08/Aug/22 11:46,04/Jun/24 20:41,08/Aug/22 11:46,1.16.0,,,,,,,,Test Infrastructure,,,,,,,0,,,,,,,"UnalignedCheckpointStressITCase fails occasionally.

From the logs from one failed attempt, random configuration was set to :
 *  false for taskmanager.network.memory.buffer-debloat.enabled
 *  false for execution.checkpointing.unaligned
 *  PT0.1S for execution.checkpointing.alignment-timeout
 *  false for state.backend.changelog.enabled

It failed when tried to fetch latest retained checkpoint.
{code:java}
Caused by: java.nio.file.NoSuchFileException: /tmp/junit933601030800674266/ea67eac6fd3f8192f43aae35952a64e7/chk-6/6e234f61-ba86-4fc6-9739-980ae3edb682 {code}
It tried to read chk-6, but chk-7 completed before job finished, but this test case did not realize that chk-7 was the latest retained checkpoint.
{code:java}
04:59:34,077 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1659848374076 for job ea67eac6fd3f8192f43aae35952a64e7.
04:59:35,284 [jobmanager-io-thread-11] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job ea67eac6fd3f8192f43aae35952a64e7 (15635521 bytes, checkpointDuration=1208 ms, finalizationTime=0 ms).
04:59:35,285 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1659848375285 for job ea67eac6fd3f8192f43aae35952a64e7.
04:59:35,950 [jobmanager-io-thread-10] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job ea67eac6fd3f8192f43aae35952a64e7 (15775934 bytes, checkpointDuration=665 ms, finalizationTime=0 ms).
04:59:35,951 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 8 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1659848375951 for job ea67eac6fd3f8192f43aae35952a64e7.
04:59:35,952 [Channel state writer Map -> Map (1/1)#0] INFO  org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl [] - Map -> Map (1/1)#0 discarding 0 drained requests04:59:35,953 [  Map -> Map (1/1)#0] WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Map -> Map (1/1)#0 (8e004c2ca00f22588638cc354972c8be_624c2fac4e5e1bf52e83f8a978720139_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.runtime.operators.testutils.ExpectedTestException: Record(sourceId=7, payload.length=4096, value=741) {code}
this is a failed pipeline (not master branch),but we reproduce this bug on master code locally.
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39472&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28766,,,,,,,,,,,,,,,,,,,"08/Aug/22 11:36;frozen stone;mvn-3-1.log;https://issues.apache.org/jira/secure/attachment/13047865/mvn-3-1.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-08 11:41:24.0,,,,,,,,,,"0|z17kls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make DPP also works if batch shuffle mode is not ALL_BLOCKING,FLINK-28871,13475725,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaoyunhaii,gaoyunhaii,gaoyunhaii,08/Aug/22 11:10,09/Aug/22 15:40,04/Jun/24 20:41,09/Aug/22 15:40,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"Currently dpp only works when all edges is blocking. Otherwise if the dynamic filtering data collector is located in the same region with the fact source, the fact source would not be started after the data collector task.

To fix this issue, we'll force the collector task's output edges to be blocking. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 15:40:43 UTC 2022,,,,,,,,,,"0|z17kkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 15:40;gaoyunhaii;Merged on master via 1719ff85b8ed3b90330bd5770a5e4abf2cd39d17;;;",,,,,,,,,,,,,,,,,,,,
Pulsar Source hangs on the small incoming message rates,FLINK-28870,13475723,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,xyhmnb,xyhmnb,08/Aug/22 10:57,10/Jan/23 12:20,04/Jun/24 20:41,10/Jan/23 08:40,,,,,pulsar-4.0.0,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,,,"When using Pulsar Source to consume data, if the data rate is small, e.g. 2 msg/s, there will be long periods of time when no messages are consumed. This is caused by the default PulsarSourceOptions.PULSAR_MAX_FETCH_TIME and PulsarSourceOptions.PULSAR_MAX_FETCH_RECORDS. Pulsar Source will try to pull messages until any one of the condition exceed.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 10 08:40:29 UTC 2023,,,,,,,,,,"0|z17kk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 08:40;tison;master via https://github.com/apache/flink-connector-pulsar/pull/15;;;",,,,,,,,,,,,,,,,,,,,
Emit warning event for ClusterDeploymentException,FLINK-28869,13475712,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,haoxin,haoxin,haoxin,08/Aug/22 09:58,10/Aug/22 10:32,04/Jun/24 20:41,10/Aug/22 10:32,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"We should emit a warning event for ClusterDeploymentException also.

We only send the waning events for DeploymentFailedException currently.

I met this when I have some invalid K8s spec (ConfigMap) in the PodTemplate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink-kubernetes-operator/pull/341,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 10:32:24 UTC 2022,,,,,,,,,,"0|z17khk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 10:32;gyfora;merged to main 0267c9166b3523edbb209112012af9f92a94fb16;;;",,,,,,,,,,,,,,,,,,,,
Migrate HBase table connector to the new LookupFunction interface,FLINK-28868,13475704,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,renqs,renqs,renqs,08/Aug/22 09:42,09/Aug/22 23:15,04/Jun/24 20:41,09/Aug/22 23:14,1.16.0,,,,1.16.0,,,,Connectors / HBase,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 23:15:54 UTC 2022,,,,,,,,,,"0|z17kfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 23:15;renqs;Merged to master: bf81768ff564c5bf4a57cb33c6f5126b83b28fb5;;;",,,,,,,,,,,,,,,,,,,,
Parquet reader support nested type in array/map type,FLINK-28867,13475702,13444611,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lsy,lsy,08/Aug/22 09:33,11/May/24 06:14,04/Jun/24 20:41,,1.16.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/22 02:50;Benenson;ReadParquetArray1.java;https://issues.apache.org/jira/secure/attachment/13052247/ReadParquetArray1.java","16/Nov/22 02:50;Benenson;part-00121.parquet;https://issues.apache.org/jira/secure/attachment/13052246/part-00121.parquet",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 11 06:14:24 UTC 2024,,,,,,,,,,"0|z17kfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 02:48;Benenson;I’m using Flink 1.16.0, and I would like to read Parquet file parquet file part-00121.parquet (attached), that has schema [1].

I could read this file with Spark, but when I try to read it with Flink 1.16.0 (program ReadParquetArray1.java attached) using schema [2]

I got IndexOutOfBoundsException [3]

code ReadParquetArray1.java, and parquet file part-00121.parquet are attached.

[1]: Parquet Schema
{code:java}
root
|-- amount: decimal(38,9) (nullable = true)
|-- connectionAccountId: string (nullable = true)
|-- sourceEntity: struct (nullable = true)
|    |-- extendedProperties: array (nullable = true)
|    |    |-- element: struct (containsNull = true)
|    |    |    |-- key: string (nullable = true)
|    |    |    |-- value: string (nullable = true)
|    |-- sourceAccountId: string (nullable = true)
|    |-- sourceEntityId: string (nullable = true)
|    |-- sourceEntityType: string (nullable = true)
|    |-- sourceSystem: string (nullable = true)
{code}
[2]: Schema used in Flink:
{code:java}
static RowType getSchema()
{
RowType elementType = RowType.of(
new LogicalType[]

{ new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH) }

,
new String[]

{ ""key"", ""value"" }

);

RowType element = RowType.of(
new LogicalType[]

{ elementType }

,
new String[]

{ ""element"" }

);

RowType sourceEntity = RowType.of(
new LogicalType[]

{ new ArrayType(element), new VarCharType(), new VarCharType(), new VarCharType(), new VarCharType(), }

,
new String[]

{ ""extendedProperties"", ""sourceAccountId"", ""sourceEntityId"", ""sourceEntityType"", ""sourceSystem"" }

);

return RowType.of(
new LogicalType[]

{ new DecimalType(), new VarCharType(), sourceEntity }

,
new String[]

{ ""amount"", ""connectionAccountId"", ""sourceEntity"", }

);
}
{code}
[3]: Execution Exception:

2022/11/15 11:39:58.657 ERROR o.a.f.c.b.s.r.f.SplitFetcherManager - Received uncaught exception.
java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150)
...
Caused by: java.lang.IndexOutOfBoundsException: Index 1 out of bounds for length 1
at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
at java.base/java.util.Objects.checkIndex(Objects.java:372)
at java.base/java.util.ArrayList.get(ArrayList.java:459)
at org.apache.parquet.schema.GroupType.getType(GroupType.java:216)
at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector(ParquetSplitReaderUtil.java:536)
at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector(ParquetSplitReaderUtil.java:533)
at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector(ParquetSplitReaderUtil.java:503)
at org.apache.flink.formats.parquet.vector.ParquetSplitReaderUtil.createWritableColumnVector(ParquetSplitReaderUtil.java:533)
at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createWritableVectors(ParquetVectorizedInputFormat.java:281)
at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createReaderBatch(ParquetVectorizedInputFormat.java:270)
at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createPoolOfBatches(ParquetVectorizedInputFormat.java:260)
at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:143)
at org.apache.flink.formats.parquet.ParquetVectorizedInputFormat.createReader(ParquetVectorizedInputFormat.java:77)
at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.checkSplitOrStartNext(FileSourceSplitReader.java:112)
at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:65)
at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142)
... 6 common frames omitted;;;","11/May/24 06:14;xccui;Hey [~jark], any plan to improve this in the near future? I feel that this is a blocker for Flink OLAP despite the data lake projects having their data readers/writers. Sometimes users would like to use Flink to process some raw parquet files.;;;",,,,,,,,,,,,,,,,,,,
Use DDL instead of legacy method to register the test source in JoinITCase,FLINK-28866,13475697,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xu_shuai_,godfreyhe,godfreyhe,08/Aug/22 09:16,24/Aug/23 13:17,04/Jun/24 20:41,24/Aug/23 08:18,,,,,1.19.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 24 08:18:45 UTC 2023,,,,,,,,,,"0|z17ke8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/23 01:29;xu_shuai_;Hi, I would like to fix this issue. Could it be assigned to me?;;;","23/Aug/23 01:18;lincoln.86xy;[~xu_shuai_] assigned to you.;;;","24/Aug/23 08:18;lincoln.86xy;fixed in master:d5b114f8156f34271ff47f42e5f91763b2aaa90d;;;",,,,,,,,,,,,,,,,,,
Add updated Print sink for new interfaces,FLINK-28865,13475695,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,08/Aug/22 09:15,08/Aug/22 23:04,04/Jun/24 20:41,08/Aug/22 23:04,,,,,1.16.0,,,,API / DataStream,,,,,,,0,pull-request-available,,,,,,The built-in print sink still uses the old sink interfaces. Add a new implementation for the new sink interfaces.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 23:04:36 UTC 2022,,,,,,,,,,"0|z17kds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 23:04;chesnay;master: aaea1adc155122f066736a4e2a4a287a40a77969;;;",,,,,,,,,,,,,,,,,,,,
DynamicPartitionPruningRule#isNewSource should check if the source used by the DataStreamScanProvider is actually a new sourc,FLINK-28864,13475692,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pltbkd,pltbkd,08/Aug/22 09:05,08/Aug/22 09:05,04/Jun/24 20:41,,,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,,,"DynamicPartitionPruningRule#isNewSource supposes DataStreamScanProvider that supports dynamic filtering will use new source as its source, but it's not reliable. For better compatibility, the method should acquire the source transformation from the translated DataStream and check if the source is actually a new source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-08 09:05:35.0,,,,,,,,,,"0|z17kd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot result of RocksDB native savepoint should have empty shared-state,FLINK-28863,13475688,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lijinzhong,yunta,yunta,08/Aug/22 08:37,26/Dec/22 02:16,04/Jun/24 20:41,23/Dec/22 02:26,,,,,1.15.4,1.16.1,1.17.0,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,,,,,,"The current snapshot result of RocksDB native savepoint has non-empty shared state, which is obviously not correct as all snapshot artifacts already stay in the exclusive checkpoint scope folder.

This does not bring real harmful result due to we would not register the snapshot results of RocksDB native savepoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 26 02:16:29 UTC 2022,,,,,,,,,,"0|z17kc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/22 03:12;lijinzhong;In my opinion, for Rocksdb native savepoint SnapshotResult, we should put [sstFiles|https://github.com/apache/flink/blob/bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L302] into privateState Map of IncrementalRemoteKeyedStateHandle to fix this issue.

This change has no effect on restore, which [downloads both the priavateStates and shareStates|[https://github.com/apache/flink/blob/35c5f674041bcefea93e1de459cea0d1789f98e0/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBStateDownloader.java#L53].]

 

[~yunta] WDYT?  If my understanding is correct, I can fix the issue.;;;","31/Oct/22 03:58;yunta;[~lijinzhong] I think this idea should be correct, already assigned to you.;;;","02/Nov/22 06:41;lijinzhong;[~yunta] 

1. I found that the changes I metioned above would change native savepoint's behavior in CLAIM mode.

Now, when job restore from native savepoint in CLAIM mode, the first checkpoint will be incremental based on savepoint sstFiles.
This means although native savepoint sstFiles stay in the exclusive scope folder, the checkpoints of the job which restore from the savepoint can still share sstFiles with it.
If we put [sstFiles|https://github.com/apache/flink/blob/bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L302] into privateState Map of IncrementalRemoteKeyedStateHandle for native savepoint, the above behavior will be changed.

2. Back to the issue itself, i think, if the semantic of ""[sharedState|https://github.com/apache/flink/blob/bb9f2525e6e16d00ef2f0739d9cb96c2e47e35e7/flink-runtime/src/main/java/org/apache/flink/runtime/state/IncrementalRemoteKeyedStateHandle.java#L80]"" in IncrementalRemoteKeyedStateHandle is that state files can be shared by other checkpoints (include checkpoints after restore), *current behavior is by design.*   In other words,  the snapshot artifacts in the exclusive scope folder can still share sstFiles with checkpoints after restore in CLAIM mode.

[~yunta]  WDYT? ;;;","24/Nov/22 06:45;yunta;[~lijinzhong] Thanks for the analysis, do you mean if we put the ""sstFiles"" into the privateState map, the next restored checkpoint in CLAIM mode would be a full checkpoint? If so, I think we can keep the logic as it was now to not break the behavior. However, the logic is still not so straightforward, and we could add some comments in related code for developers.;;;","16/Dec/22 04:17;lijinzhong;[~yunta]  Yes, that is. And +1 for keeping the logic as it was now. 
Thanks for the advice. I‘ve added some comments in related code. Could you please help review it?;;;","22/Dec/22 14:08;yunta;merged in master: 287929c407d26541f1af1c7eaed24507d691725d;;;","26/Dec/22 02:16;yunta;merged in release-1.16: bb17c3baab727c23059db978ca48adadbbd0e897
merged in release-1.15: beaa0f698b0551a04cc751292cddeb4ba39c01e2;;;",,,,,,,,,,,,,,
Support writing Parquet files in PyFlink,FLINK-28862,13475685,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,08/Aug/22 08:23,09/Aug/22 15:48,04/Jun/24 20:41,09/Aug/22 15:48,1.15.1,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 15:48:20 UTC 2022,,,,,,,,,,"0|z17kbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 15:48;dianfu;Merged to master via de8ff096a5344d85eb9be497902b99dd4b24e2a9;;;",,,,,,,,,,,,,,,,,,,,
Non-deterministic UID generation might cause issues during restore,FLINK-28861,13475673,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,twalthr,colinsmetz,colinsmetz,08/Aug/22 07:11,17/Aug/22 07:55,04/Jun/24 20:41,16/Aug/22 07:41,1.15.1,,,,1.15.2,1.16.0,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,,,"I want to use the savepoint mechanism to move existing jobs from one version of Flink to another, by:
 # Stopping a job with a savepoint
 # Creating a new job from the savepoint, on the new version.

In Flink 1.15.1, it fails, even when going from 1.15.1 to 1.15.1. I get this error, meaning that it could not map the state from the previous job to the new one because of one operator:
{quote}{{Failed to rollback to checkpoint/savepoint hdfs://hdfs-name:8020/flink-savepoints/savepoint-046708-238e921f5e78. Cannot map checkpoint/savepoint state for operator d14a399e92154660771a806b90515d4c to the new program, because the operator is not available in the new program.}}
{quote}
After investigation, the problematic operator corresponds to a {{ChangelogNormalize}} operator, that I do not explicitly create. It is generated because I use [{{tableEnv.fromChangelogStream(stream, schema, ChangelogMode.upsert())}}|https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.html#fromChangelogStream-org.apache.flink.streaming.api.datastream.DataStream-org.apache.flink.table.api.Schema-org.apache.flink.table.connector.ChangelogMode-] (the upsert mode is important, other modes do not fail). The table created is passed to an SQL query using the SQL API, which generates something like:
{quote}{{ChangelogNormalize[8] -> Calc[9] -> TableToDataSteam -> [my_sql_transformation] -> [my_sink]}}
{quote}
In previous versions of Flink it seems this operator was always given the same uid so the state could match when starting from the savepoint. In Flink 1.15.1, I see that a different uid is generated every time. I could not find a reliable way to set that uid manually. The only way I found was by going backwards from the transformation:
{quote}{{dataStream.getTransformation().getInputs().get(0).getInputs().get(0).getInputs().get(0).setUid(""the_user_defined_id"");}}
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 07:33:11 UTC 2022,,,,,,,,,,"0|z17k94:",9223372036854775807,1.15.0 and 1.15.1 generated non-deterministic UIDs for operators that make it difficult/impossible to restore state or upgrade to next patch version. A new table.exec.uid.generation config option (with correct default behavior) disables setting a UID for new pipelines from non-compiled plans. Existing pipelines can set table.exec.uid.generation=ALWAYS if the 1.15.0/1 behavior was acceptable.,,,,,,,,,,,,,,,,,,,"08/Aug/22 08:51;twalthr;This is clearly a bug that needs investigation. We introduced stable UIDs for all operators that the planner ingests. The default before was to set no UIDs at all, which resulted in autogenerated UIDs based on the topology and previous operators. ;;;","08/Aug/22 13:24;twalthr;After looking into this topic, I have a first guess what the problem might be. Could you share the UID of the affected operator with us after multiple tries? There might be a bug in the design. A UID is currently set as {{1_stream-exec-table-source-scan_1_external-datastream}}, however, the first component is generated by an {{AtomicCounter}} that is started per JVM. Which means that multiple runs of the same main() in the same JVM might generated different UIDs. How do you submit your jobs?

In any case, there is a workaround, you can set {{table.exec.legacy-transformation-uids}} to {{true}}. Let me know if this solves your problem for 1.15 upgrades or simple pipelines from previous versions? However, keep in mind that we don't support stateful upgrades between Flink versions for Flink SQL yet. [FLIP-190|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489] aims to fix this.;;;","08/Aug/22 14:07;colinsmetz;{quote}Could you share the UID of the affected operator with us after multiple tries?
{quote}
I only have what seems to be a hash of the uid (obtained via [getGeneratedOperatorID|https://nightlies.apache.org/flink/flink-docs-release-1.12/api/java/org/apache/flink/runtime/OperatorIDPair.html#getGeneratedOperatorID--]), but here's what I get for three successive submissions:
 * 3c217b755850a1fb331af4b7f67946f5
 * 2ea21244917b900c533f89ff25291e8d
 * 48f2f08e9f1663064e1369bd518990a1

Does that help?
{quote}How do you submit your jobs?
{quote}
When we first noticed the problem, we were submitting the job with the REST API (POST /jars/:jarid/run). But we've since reproduced it in our tests using [PackagedProgramUtils.createJobGraph|[https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/client/program/PackagedProgramUtils.html]]
{quote}In any case, there is a workaround, you can set {{table.exec.legacy-transformation-uids}} to {{{}true{}}}. Let me know if this solves your problem for 1.15 upgrades or simple pipelines from previous versions? 
{quote}
It works indeed, thanks! At least from 1.15.1 to 1.15.1, I'll check more complex cases later.
{quote}However, keep in mind that we don't support stateful upgrades between Flink versions for Flink SQL yet. [FLIP-190|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336489] aims to fix this.
{quote}
We are aware of that but since it does seem to work at least sometimes, it's still better than nothing. Good to see there is a FLIP to support that.;;;","08/Aug/22 14:39;twalthr;Thanks for the response! The generated UID hashes were Flink 1.14 behavior, we replaced them with explicit and human readable UIDs as shown above. So 1.15.1 to 1.15.1 should not show you a hash. But nevertheless, the bug that will result in different UIDs when executing the same JAR twice needs to be fixed.;;;","09/Aug/22 10:18;twalthr;Let me summarize the issue: The UID generation using a static AtomicInteger as counter makes sense for plan compilation. There, the generated UID will be immediately persisted in JSON and thus remains static.

However, it should not be enabled by default for regular Table API jobs that potentially connect to DataStream API. The current counter approach for UID generation causes issues when the same JVM translates multiple SQL/Table API pipelines. It is not easily possible to ensure uniqueness as one DataStream API job could potentially consist of multiple SQL pipelines. The previous approach in 1.14 (viewing the entire StreamGraph and assigning the UIDs in a last step at the end) makes more sense if the pipeline is not constructed from a compiled plan.

I would suggest to revert the change made in 1.15. Not all pipelines are affected by this bug. It works nicely in containerized environments that start a new JVM per job. I suggest to introduce a new config option:
{{table.exec.uid-generation}} of type enum with values {{PLAN_ONLY}} (default) and {{ALWAYS}} (for 1.15.0, 1.15.1 behavior and expert users that know what they are doing). {{PLAN_ONLY}} means that we use the behavior of {{table.exec.legacy-transformation-uids}} if the translation does not happen for a compiled plan.

CC [~godfrey] [~jark];;;","15/Aug/22 13:27;twalthr;Fixed in master:
commit b142860352721fe65b8fe7e4106cbcd2059714e5
{code}
[FLINK-28861][table] Make UID generation behavior configurable and plan-only by default

Before this commit, due to changes for FLIP-190, every operator generated by the planner
got a UID assigned. However, the UID is based on a static counter that might return different
results depending on the environment. Thus, UIDs are not deterministic and make stateful
restores impossible e.g. when going from 1.15.0 -> 1.15.1. This PR restores the old pre-1.15
behavior for regular Table API. It only adds UIDs if the operator has been created from a
compiled plan. A compiled plan makes the UIDs static and thus deterministic.

table.exec.uid.generation=ALWAYS exists for backwards compatibility and could make stateful
upgrades possible even with invalid UIDs on best effort basis.
{code}

commit 881b2bf046e510b1b6dddddf8c15af45926397f1
{code}
[FLINK-28861][table] Fix bug in UID format for future migrations and make it configurable

Before this commit, the UID format was not future-proof for migrations. The ExecNode version
should not be in the UID, otherwise, operator migration won't be possible once plan migration
is executed. See the FLIP-190 example that drops a version in the plan, once operator migration
has been performed. Given that the plan feature is marked as @Experimental, this change should
still be possible without providing backwards compatibility.

However, the config option table.exec.uid.format allows for restoring the old format and solves
other UID related issues on the way.
{code};;;","16/Aug/22 07:33;twalthr;Fixed in 1.15.2:

commit 105d7c911bd0c5d8634417c22164547651abf07b
{code}
[FLINK-28861][table] Make UID generation behavior configurable and plan-only by default

Before this commit, due to changes for FLIP-190, every operator generated by the planner
got a UID assigned. However, the UID is based on a static counter that might return different
results depending on the environment. Thus, UIDs are not deterministic and make stateful
restores impossible e.g. when going from 1.15.0 -> 1.15.1. This PR restores the old pre-1.15
behavior for regular Table API. It only adds UIDs if the operator has been created from a
compiled plan. A compiled plan makes the UIDs static and thus deterministic.

table.exec.uid.generation=ALWAYS exists for backwards compatibility and could make stateful
upgrades possible even with invalid UIDs on best effort basis.
{code};;;",,,,,,,,,,,,,,
CacheITCase.testBatchProduceCacheStreamConsume failed,FLINK-28860,13475670,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xuannan,hxbks2ks,hxbks2ks,08/Aug/22 06:58,08/Sep/22 03:26,04/Jun/24 20:41,08/Sep/22 03:26,1.16.0,,,,1.16.0,,,,API / DataStream,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-08-08T03:27:22.1988575Z Aug 08 03:27:22 [ERROR] org.apache.flink.test.streaming.runtime.CacheITCase.testBatchProduceCacheStreamConsume(Path)  Time elapsed: 0.593 s  <<< ERROR!
2022-08-08T03:27:22.1989338Z Aug 08 03:27:22 java.lang.RuntimeException: Producing cache IntermediateResult is not supported in streaming mode
2022-08-08T03:27:22.1990401Z Aug 08 03:27:22 	at org.apache.flink.streaming.runtime.translators.CacheTransformationTranslator.translateForStreamingInternal(CacheTransformationTranslator.java:75)
2022-08-08T03:27:22.1991511Z Aug 08 03:27:22 	at org.apache.flink.streaming.runtime.translators.CacheTransformationTranslator.translateForStreamingInternal(CacheTransformationTranslator.java:42)
2022-08-08T03:27:22.1993671Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.translateForStreaming(SimpleTransformationTranslator.java:62)
2022-08-08T03:27:22.1994900Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:830)
2022-08-08T03:27:22.1995748Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:560)
2022-08-08T03:27:22.1996932Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.getParentInputIds(StreamGraphGenerator.java:851)
2022-08-08T03:27:22.1998562Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:809)
2022-08-08T03:27:22.1999581Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:560)
2022-08-08T03:27:22.2000376Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.generate(StreamGraphGenerator.java:319)
2022-08-08T03:27:22.2001359Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2250)
2022-08-08T03:27:22.2002767Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2241)
2022-08-08T03:27:22.2004121Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2227)
2022-08-08T03:27:22.2005059Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2178)
2022-08-08T03:27:22.2005939Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollectWithClient(DataStream.java:1469)
2022-08-08T03:27:22.2006735Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1334)
2022-08-08T03:27:22.2007500Z Aug 08 03:27:22 	at org.apache.flink.streaming.api.datastream.DataStream.executeAndCollect(DataStream.java:1320)
2022-08-08T03:27:22.2008315Z Aug 08 03:27:22 	at org.apache.flink.test.streaming.runtime.CacheITCase.testBatchProduceCacheStreamConsume(CacheITCase.java:190)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39518&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 08 03:26:25 UTC 2022,,,,,,,,,,"0|z17k8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 06:59;hxbks2ks;[~xuannan] Could you help take a look?;;;","09/Aug/22 01:59;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39618&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","09/Aug/22 02:20;xuannan;[~hxbks2ks]I will take a look at it. Could you assign the ticket to me?;;;","09/Aug/22 15:46;gaoyunhaii;Temporarily disabled on master via 208f08b406a7fd48890cda16d317a30ee892a2e7;;;","08/Sep/22 03:26;gaoyunhaii;Merged on master via 208f08b406a7fd48890cda16d317a30ee892a2e7^..b7dd42617a46fcecfffbea3409391e204a40b9b1.

Merged on 1.16 via 38088230bb57486f81bb089a96aa3fa1e3f414f7^..f599a8c444ab44660824a7b3e0a08a635c22d3f4;;;",,,,,,,,,,,,,,,,
FAILED pyflink/datastream/connectors/tests/test_file_system.py::FileSinkCsvBulkWriterTests::test_csv_customize_quote_char_write,FLINK-28859,13475649,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,godfreyhe,godfreyhe,08/Aug/22 04:29,08/Aug/22 11:55,04/Jun/24 20:41,08/Aug/22 11:55,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,test-stability,,,,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39522&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901


Aug 08 03:45:08 =================================== FAILURES ===================================
Aug 08 03:45:08 ________ FileSinkCsvBulkWriterTests.test_csv_customize_quote_char_write ________
Aug 08 03:45:08 
Aug 08 03:45:08 self = <pyflink.datastream.connectors.tests.test_file_system.FileSinkCsvBulkWriterTests testMethod=test_csv_customize_quote_char_write>
Aug 08 03:45:08 
Aug 08 03:45:08     def test_csv_customize_quote_char_write(self):
Aug 08 03:45:08         schema, lines = _create_csv_customize_quote_char_schema_lines()
Aug 08 03:45:08         self._build_csv_job(schema, lines)
Aug 08 03:45:08 >       self.env.execute('test_csv_customize_quote_char_write')
Aug 08 03:45:08 
Aug 08 03:45:08 pyflink/datastream/connectors/tests/test_file_system.py:463: 
Aug 08 03:45:08 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 11:55:17 UTC 2022,,,,,,,,,,"0|z17k4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 04:30;godfreyhe;cc [~hxbks2ks];;;","08/Aug/22 04:49;hxbks2ks;Thanks for reporting this. [~Juntao Hu] is helping looking into this problem.;;;","08/Aug/22 11:55;hxbks2ks;Merged into master via 4cf0b81d4f492b49d12aa317f3dfbc6e50d72ec8;;;",,,,,,,,,,,,,,,,,,
Add document to describe join hints for batch sql,FLINK-28858,13475648,13447716,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,08/Aug/22 04:00,04/Sep/22 13:27,04/Jun/24 20:41,04/Sep/22 13:27,,,,,1.16.0,,,,Documentation,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 04 13:27:54 UTC 2022,,,,,,,,,,"0|z17k4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 09:19;337361684@qq.com;Hi, [~godfreyhe], Could you assign this to me, Thanks!;;;","04/Sep/22 13:27;godfrey;Fixed in master: 72f0cb6fc7cd635c872de6fcbac9aec4c5beaf69;;;",,,,,,,,,,,,,,,,,,,
Add Document for DataStream Cache API,FLINK-28857,13475645,13443524,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuannan,xuannan,xuannan,08/Aug/22 03:31,10/Aug/22 09:35,04/Jun/24 20:41,09/Aug/22 15:43,,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 15:42:49 UTC 2022,,,,,,,,,,"0|z17k3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 15:42;gaoyunhaii;Merged on master via 21712c932cdbcc6be26f46a201412646c56f9650;;;",,,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase tests failed with NoSuchMethodError,FLINK-28856,13475643,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,hxbks2ks,hxbks2ks,08/Aug/22 03:01,12/Aug/22 12:28,04/Jun/24 20:41,08/Aug/22 03:08,1.16.0,,,,,,,,Build System / Azure Pipelines,Tests,,,,,,0,test-stability,,,,,,"
{code:java}
2022-08-07T15:54:12.7203154Z Aug 07 15:54:12 [ERROR] org.apache.flink.yarn.YARNHighAvailabilityITCase  Time elapsed: 4.606 s  <<< ERROR!
2022-08-07T15:54:12.7203828Z Aug 07 15:54:12 java.lang.NoSuchMethodError: org.apache.curator.test.InstanceSpec.getHostname()Ljava/lang/String;
2022-08-07T15:54:12.7204675Z Aug 07 15:54:12 	at org.apache.flink.runtime.testutils.ZooKeeperTestUtils.getZookeeperInstanceSpecWithIncreasedSessionTimeout(ZooKeeperTestUtils.java:71)
2022-08-07T15:54:12.7205582Z Aug 07 15:54:12 	at org.apache.flink.runtime.testutils.ZooKeeperTestUtils.createAndStartZookeeperTestingServer(ZooKeeperTestUtils.java:49)
2022-08-07T15:54:12.7206508Z Aug 07 15:54:12 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.setup(YARNHighAvailabilityITCase.java:114)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39502&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28713,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-08 03:01:21.0,,,,,,,,,,"0|z17k3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftObjectConversions compile failed,FLINK-28855,13475638,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yzl,hxbks2ks,hxbks2ks,08/Aug/22 02:45,13/Aug/22 02:56,04/Jun/24 20:41,13/Aug/22 02:56,1.16.0,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-08-08T00:32:45.5104326Z [ERROR] /home/vsts/work/1/s/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/endpoint/hive/util/ThriftObjectConversions.java:[615,31] cannot find symbol
2022-08-08T00:32:45.5105191Z   symbol:   variable INDEX_TABLE
2022-08-08T00:32:45.5107273Z   location: class org.apache.hadoop.hive.metastore.TableType
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39514&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=4632ba9d-f1f2-5ad2-13fc-828d0e28bac4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28633,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 13 02:56:09 UTC 2022,,,,,,,,,,"0|z17k28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 02:46;hxbks2ks;cc [~fsk119] [~yzl];;;","08/Aug/22 03:33;fsk119;Thanks for pointing out! [~yzl] will help to fix this soon.;;;","09/Aug/22 02:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39675&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691&l=7350;;;","10/Aug/22 03:54;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39795&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691;;;","13/Aug/22 02:56;fsk119;Merged into master: 1232629c80cbb64eb4ca9f6c95d6c5c1a2e8e82d;;;",,,,,,,,,,,,,,,,
Migrate JDBC table connector to the new LookupFunction interface,FLINK-28854,13475637,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,renqs,renqs,renqs,08/Aug/22 02:41,09/Aug/22 14:34,04/Jun/24 20:41,09/Aug/22 14:34,1.16.0,,,,1.16.0,,,,Connectors / JDBC,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 14:33:53 UTC 2022,,,,,,,,,,"0|z17k20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 14:33;renqs;Merged to master: 3c8402a97a26cf9c9d42c2842818d6dfadf432fa;;;",,,,,,,,,,,,,,,,,,,,
FLIP-217 Support watermark alignment of source splits,FLINK-28853,13475610,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,smattheis,smattheis,07/Aug/22 17:53,06/Jun/23 08:28,04/Jun/24 20:41,14/Sep/22 12:58,1.16.0,,,,1.17.0,,,,Connectors / Common,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,,"This improvement implements [FLIP-217|https://cwiki.apache.org/confluence/display/FLINK/FLIP-217+Support+watermark+alignment+of+source+splits] to support watermark alignment of source splits.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27117,,,,,,FLINK-31324,,FLINK-27117,,,,FLINK-12675,FLINK-31519,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 06 08:27:56 UTC 2023,,,,,,,,,,"0|z17jw0:",9223372036854775807,"Added support for watermark alignment of source splits. 

Since Flink 1.17, source connectors have to implement watermark alignment of source split in order to use the watermark alignment feature. The required methods to implement are: `SourceReader#pauseOrResumeSplits` and `SplitReader#pauseOrResumeSplits`. If you are migrating from Flink <= 1.16.x, and you were using watermark alignment, but at the same time you are not able to upgrade/modify your connector, you can disable per split alignment via setting `pipeline.watermark-alignment.allow-unaligned-source-splits` to true. Note that by doing so, watermark alignment will be working properly only when your number of splits equals to the parallelism of the source operator.",,,,,,,,,,,,,,,,,,,"14/Sep/22 12:58;mxm;Implemented as of 0612a997ddcc791ee54f500fbf1299ce04987679.;;;","23/Nov/22 17:44;yunta;[~mxm] Can we say the  FLIP-217 has been totally implemented?;;;","25/Nov/22 13:09;pnowojski;I think yes, but it looks like [the documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/event-time/generating_watermarks/#watermark-alignment-_beta_] hasn't been updated?;;;","28/Nov/22 09:09;mxm;Thanks for updating the release note. We will update the docs.;;;","04/Mar/23 14:47;yunta;[~mxm] It seems the docs are still not updated?;;;","07/Mar/23 08:59;leonard;[~mxm] We're preparing the release of Flink 1.17, could you add the feature introduction into the release announcement [1]?

[1] https://docs.google.com/document/d/1aao4ATNcDBlDNdZ7VFFfrjrTGfUDcSTsvVzZrSSked4

 ;;;","13/Mar/23 11:15;mxm;Thanks [~leonard]! I see there was already a paragraph added. Thank you!;;;","27/May/23 07:26;mxm;Documentation is now fixed. See  https://github.com/apache/flink/pull/22531. Thanks to [~mason6345]!;;;","30/May/23 12:05;asardaes;I guess FLINK-31519 can be closed now.;;;","30/May/23 13:49;pnowojski;Thanks for pointing this out [~asardaes]. Closed.;;;","06/Jun/23 08:27;martijnvisser;Docs sync PR:

master: 43be40f5b5a1ac0220f0acbeb2a8ba243dcc9b00
release-1.17: todo;;;",,,,,,,,,,
Closed metrics could be found in metric dashboard from WebUI,FLINK-28852,13475601,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,stupid_pig,stupid_pig,07/Aug/22 13:28,31/Aug/22 08:54,04/Jun/24 20:41,31/Aug/22 08:54,1.14.0,,,,,,,,Runtime / Metrics,,,,,,,0,,,,,,,"When I close  metric group, the related metrics would be unregistered from metric-reporter, however the closed metrics could be found in metric dashboad from WebUI.

This would leak to memory leak.",Flink 1.11.3/1.14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 31 08:54:42 UTC 2022,,,,,,,,,,"0|z17ju0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/22 13:12;stupid_pig;I'd like to take this ticket!

 

I think the problem comes from *MetricStore.class,* beause it always add metrics but  not clear them when refreshing to query metrics . As the code shown below:
{code:java}
synchronized void addAll(List<MetricDump> metricDumps) {
    for (MetricDump metric : metricDumps) {
        add(metric);
    }
}
{code}
If we can clear the previous metrics before adding the latest metrics ，the problem could be solved.

 

I am planning to add a method named  *clearMetrics* which could be implemented by  {*}ComponentMetricStore/JobMetricStore/TaskMetricStore{*}, and then we could change the code above to:
{code:java}
synchronized void addAll(List<MetricDump> metricDumps) {
    jobManager.clearMetrics();
    taskManagers.values().forEach(ComponentMetricStore::clearMetrics);
    jobs.values().forEach(JobMetricStore::clearMetrics);
    for (MetricDump metric : metricDumps) {
        add(metric);
    }
} {code}
I had tested the changed code in our unit test and flink cluster.

 

 ;;;","31/Aug/22 08:54;chesnay;This fundamentally conflicts with FLINK-28588.

Unless you are creating/closing thousands of groups (which TBH is a bad idea anyway) you are unlikely to run into issues.;;;",,,,,,,,,,,,,,,,,,,
Allow to GetTypeInfo in the HiveServer2 Endpoint,FLINK-28851,13475586,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ZhaoWeiNan,fsk119,fsk119,07/Aug/22 09:30,08/Aug/22 02:28,04/Jun/24 20:41,08/Aug/22 02:28,,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 02:28:34 UTC 2022,,,,,,,,,,"0|z17jqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 02:21;ZhaoWeiNan;hi [~fsk119] ,please assgin for me.

thanks.;;;","08/Aug/22 02:28;fsk119;Merged into master:

3aaa160bd80a5e89447118fa1bc9cfea6bd368d1

29ca0ad8e33ff6391450c5d9b29f484373ce824a;;;",,,,,,,,,,,,,,,,,,,
Support table alias in LOOKUP hint,FLINK-28850,13475585,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,07/Aug/22 09:26,29/Dec/22 03:14,04/Jun/24 20:41,29/Dec/22 03:14,,,,,1.17.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"currently calcite's LogicalSnapshot is not Hintable, to support table alias in LOOKUP hint relies on it, so we need create a ticket in calcite community and also do the change in Flink first.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 29 03:14:54 UTC 2022,,,,,,,,,,"0|z17jqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 09:50;lincoln.86xy;The pre-step issue: CALCITE-5251, once this was fixed, we should pick the change to flink first before upgrading to the corresponding new version of calcite.;;;","29/Dec/22 03:14;godfrey;Fixed in master:

be6b1c94ef3f552c753746863cb0a4e7dd86d2fc

f32ba645246a8180a02182650fb51392facfcc09

62a9d837e24150461790e7689659c50f15197ebc;;;",,,,,,,,,,,,,,,,,,,
Add it cases with lookup hint using new lookup function api for lookup join,FLINK-28849,13475584,13474692,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln.86xy,lincoln.86xy,07/Aug/22 09:21,10/Aug/22 06:07,04/Jun/24 20:41,10/Aug/22 06:07,,,,,1.16.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,FLINK-28420,,,,,,,,,,,,,,,,,,,FLINK-28848,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 06:07:15 UTC 2022,,,,,,,,,,"0|z17jq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 09:24;lincoln.86xy;FLINK-28420 will update the test utility 'TestValuesTableFactory' to cover the new lookup function api.

This will also be used in this change;;;","10/Aug/22 06:07;lincoln.86xy;fixed in c5b5d4368437fc98b2f5ca31b1f1c6cf3e4ce263  #20482;;;",,,,,,,,,,,,,,,,,,,
Introduces LOOKUP join hint to support delayed retry for lookup join (table alias unsupported in hint) ,FLINK-28848,13475583,13474692,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,07/Aug/22 09:20,10/Aug/22 06:07,04/Jun/24 20:41,10/Aug/22 06:05,,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,main part of flip234,,,,,,,,,,,,,,,,,,,,FLINK-27625,,,,,,,,,,,FLINK-28849,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 06:05:51 UTC 2022,,,,,,,,,,"0|z17jq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 06:05;godfreyhe;Fixed in master: 
8b25b969d4168d070e5e78ef07a6a573c9b63d95
fa6d62dd6bbaa3876656a97ff519e37f9a3f0730
3a2fc5ef34f563c906473cbe4bdd79a9d7eec48e
c5b5d4368437fc98b2f5ca31b1f1c6cf3e4ce263;;;",,,,,,,,,,,,,,,,,,,,
Typo in FileSinkProgram.java in file-sink-file-test module,FLINK-28847,13475570,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,XinWen,XinWen,07/Aug/22 03:43,18/Aug/23 22:35,04/Jun/24 20:41,,1.15.1,,,,,,,,Tests,,,,,,,0,auto-deprioritized-minor,easyfix,,,,,"There is a redundant semicolon in [FileSinkProgram.java|https://github.com/apache/flink/blob/b0859789e7733c73a21e600ec0d595ead730c59d/flink-end-to-end-tests/flink-file-sink-test/src/main/java/org/apache/flink/connector/file/sink/FileSinkProgram.java#L57] which will confuse the users.

!image-2022-08-07-11-42-25-221.png|width=700,height=290!",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/22 03:43;XinWen;image-2022-08-07-11-42-25-221.png;https://issues.apache.org/jira/secure/attachment/13047811/image-2022-08-07-11-42-25-221.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Fri Aug 18 22:35:04 UTC 2023,,,,,,,,,,"0|z17jn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Trigger event on validation error,FLINK-28846,13475559,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,06/Aug/22 20:20,09/Aug/22 07:24,04/Jun/24 20:41,09/Aug/22 07:24,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,Validation errors currently do not trigger any event which is inconsistent with other error handling.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 07:24:47 UTC 2022,,,,,,,,,,"0|z17jko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 07:24;gyfora;merged to main: f5b1d4a62155b11d4d4b5483df41e3215be8106a;;;",,,,,,,,,,,,,,,,,,,,
InitialSavepointPath might be ignored if first deployment fails,FLINK-28845,13475545,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,06/Aug/22 13:54,09/Aug/22 09:49,04/Jun/24 20:41,09/Aug/22 09:49,kubernetes-operator-1.0.0,kubernetes-operator-1.1.0,,,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,If the first deployment completely fails (and the cluster wasn't even created) the initialSavepointPath is subsequently ignored by the operator.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 09:49:21 UTC 2022,,,,,,,,,,"0|z17jhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 13:54;gyfora;cc [~jeesmon] ;;;","09/Aug/22 09:49;gyfora;merged:
main ac21bc8fe148f6dc803988791224f792a66875ce
release-1.1 d0c9f6ba714d5265ec55154213f3b2c383831cdc;;;",,,,,,,,,,,,,,,,,,,
YARNHighAvailabilityITCase fails with NoSuchMethod of org.apache.curator,FLINK-28844,13475535,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,jark,jark,06/Aug/22 12:44,15/Aug/22 02:27,04/Jun/24 20:41,08/Aug/22 08:03,1.16.0,,,,1.16.0,,,,Test Infrastructure,,,,,,,0,,,,,,,"This is keep failing on master since the commit of https://github.com/flink-ci/flink-mirror/commit/6335b573863af2b30a6541f910be96ddf61f9c84 which removes curator-test dependency from the flink-test-utils module. 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39394&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461

{code}
2022-08-05T18:31:47.0438160Z Aug 05 18:31:47 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.237 s <<< FAILURE! - in org.apache.flink.yarn.YARNHighAvailabilityITCase
2022-08-05T18:31:47.0439564Z Aug 05 18:31:47 [ERROR] org.apache.flink.yarn.YARNHighAvailabilityITCase  Time elapsed: 5.237 s  <<< ERROR!
2022-08-05T18:31:47.0440370Z Aug 05 18:31:47 java.lang.NoSuchMethodError: org.apache.curator.test.InstanceSpec.getHostname()Ljava/lang/String;
2022-08-05T18:31:47.0441582Z Aug 05 18:31:47 	at org.apache.flink.runtime.testutils.ZooKeeperTestUtils.getZookeeperInstanceSpecWithIncreasedSessionTimeout(ZooKeeperTestUtils.java:71)
2022-08-05T18:31:47.0442643Z Aug 05 18:31:47 	at org.apache.flink.runtime.testutils.ZooKeeperTestUtils.createAndStartZookeeperTestingServer(ZooKeeperTestUtils.java:49)
2022-08-05T18:31:47.0443461Z Aug 05 18:31:47 	at org.apache.flink.yarn.YARNHighAvailabilityITCase.setup(YARNHighAvailabilityITCase.java:114)
2022-08-05T18:31:47.0444094Z Aug 05 18:31:47 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-05T18:31:47.0444717Z Aug 05 18:31:47 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-05T18:31:47.0445424Z Aug 05 18:31:47 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-05T18:31:47.0446063Z Aug 05 18:31:47 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-05T18:31:47.0446818Z Aug 05 18:31:47 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-08-05T18:31:47.0447822Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-08-05T18:31:47.0448657Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-08-05T18:31:47.0449692Z Aug 05 18:31:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-08-05T18:31:47.0450637Z Aug 05 18:31:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:126)
2022-08-05T18:31:47.0451443Z Aug 05 18:31:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptBeforeAllMethod(TimeoutExtension.java:68)
2022-08-05T18:31:47.0452304Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-08-05T18:31:47.0453162Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-08-05T18:31:47.0454013Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-08-05T18:31:47.0454882Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-08-05T18:31:47.0455716Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-08-05T18:31:47.0456525Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-08-05T18:31:47.0457512Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-08-05T18:31:47.0458637Z Aug 05 18:31:47 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-08-05T18:31:47.0459742Z Aug 05 18:31:47 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllMethods$11(ClassBasedTestDescriptor.java:397)
2022-08-05T18:31:47.0460871Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-05T18:31:47.0461714Z Aug 05 18:31:47 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllMethods(ClassBasedTestDescriptor.java:395)
2022-08-05T18:31:47.0462554Z Aug 05 18:31:47 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:209)
2022-08-05T18:31:47.0463333Z Aug 05 18:31:47 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
2022-08-05T18:31:47.0464145Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
2022-08-05T18:31:47.0465135Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-05T18:31:47.0465947Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-05T18:31:47.0466902Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-05T18:31:47.0467820Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-05T18:31:47.0468643Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-05T18:31:47.0469431Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-05T18:31:47.0470177Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-05T18:31:47.0471348Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-05T18:31:47.0472453Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-08-05T18:31:47.0473397Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-08-05T18:31:47.0474219Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-05T18:31:47.0475031Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-08-05T18:31:47.0475788Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-08-05T18:31:47.0476541Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-08-05T18:31:47.0477599Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-08-05T18:31:47.0478388Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-08-05T18:31:47.0479152Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-08-05T18:31:47.0480065Z Aug 05 18:31:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-08-05T18:31:47.0480948Z Aug 05 18:31:47 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-08-05T18:31:47.0481670Z Aug 05 18:31:47 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-08-05T18:31:47.0482493Z Aug 05 18:31:47 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-08-05T18:31:47.0483153Z Aug 05 18:31:47 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-08-05T18:31:47.0483818Z Aug 05 18:31:47 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-08-05T18:31:47.0484313Z Aug 05 18:31:47 
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28955,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 08:03:30 UTC 2022,,,,,,,,,,"0|z17jfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 12:45;jark;[~chesnay], could you help to take a look?;;;","06/Aug/22 15:50;lincoln.86xy;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39443&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","07/Aug/22 09:16;fsk119;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39466&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","07/Aug/22 11:46;chesnay;No clue why this happens but it should be easy to fix at least.;;;","08/Aug/22 02:05;xuyangzhong;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39494&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","08/Aug/22 02:07;godfreyhe;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39483&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","08/Aug/22 02:11;godfreyhe;[~chesnay] Could you fix it with the highest priority?;;;","08/Aug/22 02:31;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39490&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461;;;","08/Aug/22 05:53;tanyuxin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39534&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=29842;;;","08/Aug/22 08:03;chesnay;master: caef5b7a5c1b980cff926fec5dce3a34ad1b354f;;;",,,,,,,,,,,
Fail to find incremental handle when restoring from changelog checkpoint in claim mode,FLINK-28843,13475516,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,frozen stone,frozen stone,frozen stone,06/Aug/22 03:09,09/Aug/22 03:10,04/Jun/24 20:41,09/Aug/22 03:10,1.15.0,1.15.1,,,1.16.0,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,,,"# When native checkpoint is enabled and incremental checkpointing is enabled in rocksdb statebackend，if state data is greater than state.storage.fs.memory-threshold，it will be stored in a data file (FileStateHandle，RelativeFileStateHandle, etc) rather than stored with ByteStreamStateHandle in checkpoint metadata, like base-path1/chk-1/file1.
 # Then restore the job from base-path1/chk-1 in claim mode，using changelog statebackend，and the checkpoint path is set to base-path2, then new checkpoint will be saved in base-path2/chk-2, previous checkpoint file (base-path1/chk-1/file1) is needed.
 # Then restore the job from base-path2/chk-2 in changelog statebackend, flink will try to read base-path2/chk-2/file1, rather than the actual file location base-path1/chk-1/file1, which leads to FileNotFoundException and job failed.

 
How to reproduce?
 # Set state.storage.fs.memory-threshold to a small value, like '20b'.
 # {{run org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase#testSwitchFromDisablingToEnablingInClaimMode}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25872,FLINK-28699,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 03:10:06 UTC 2022,,,,,,,,,,"0|z17jb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 06:24;yunta;Thanks for reporting this bug!

The root cause is that the native savepoint could contain the relative file state handles (all files under {{chk-x}} folder would be {{{}RelativeFileStateHandle{}}}), and the snapshot on changelog state-backend might not trigger the materialization part, which leads to the newly created {{chk-y}} folder does not contain previous snapshots. Thus, once restoring from {{{}chk-y{}}}, relocatable {{chk-x/file-1}} would be transferred to {{{}chk-y/file-1{}}}, resulting in the file not found exception.

Since we already give docs that native savepoint is relocatable (refer to [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/ops/state/checkpoints_vs_savepoints/#capabilities-and-limitations] ), we might have to let changelog state-backend trigger materialization on the 1st checkpoint if restored snapshot containing relative file state handles. cc [~roman]  [~ym] [~Yanfei Lei] ;;;","07/Aug/22 15:21;frozen stone;I think the root cause is that IncrementalKeyedStateHandle  is not handled properly, only KeyGroupsStateHandle will be cast to absolute path during restore, maybe we could fix this by casting  IncrementalRemoteKeyedStateHandle in the same way. ;;;","08/Aug/22 02:22;yunta;[~frozen stone] I noticed that FLINK-25872 has considered such cases via introducing {{{}ChangelogStateBackendHandle#castToAbsolutePath{}}}, unfortunately, it just forget to cover the native savepoint of RocksDB state-backend. ;;;","08/Aug/22 02:36;Yanfei Lei;[~frozen stone] Thanks for reporting this bug! I think your analysis is correct, I had assumed that all  IncrementalKeyedStateHandle were located in /shared, and forgot the private handles of IncrementalKeyedStateHandle are located in /chk-x. ;;;","09/Aug/22 03:10;yunta;merged in master: 7f708d0ba42f727b3f8c3d77cef2108206cad2de;;;",,,,,,,,,,,,,,,,
Add client.id.prefix for the KafkaSink,FLINK-28842,13475501,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,sap1ens,sap1ens,sap1ens,05/Aug/22 20:59,11/Oct/23 18:53,04/Jun/24 20:41,,1.15.1,,,,,,,,Connectors / Kafka,,,,,,,0,pull-request-available,stale-assigned,,,,,"Currently, KafkaSink doesn't provide a way to configure a client.id.prefix like KafkaSource does. client.id is as important for Kafka Producers, so it makes sense to implement the missing logic for the KafkaSink. 

A similar implementation that leverages subtaskId for uniqueness can be used here.",,,,,,,,,,,,,,,,,,,,FLINK-8093,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 10:35:13 UTC 2023,,,,,,,,,,"0|z17j7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 21:07;sap1ens;I believe this will also fix https://issues.apache.org/jira/browse/FLINK-8093;;;","13/May/23 18:25;syamka256;Is there any progress on this issue ? As I understood, PR was not accepted because realizaion doesn't guarantee uniqueness of producer's client.id in any case ?

I would be happy to offer another aproach: 
 * client.id can be built exactly in the constructor of [FlinkKafkaInternalProducer|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/FlinkKafkaInternalProducer.java]
 * to support uniqueness of client.id it can be built with static counter (not subtaskId) - same as it was [previously done|https://github.com/apache/kafka/blob/2.4/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L443] in kafka-clients with default ""producer-n"" value 

I have already checked this approach in the local patch of the connector, I suppose it works well.

 ;;;","14/May/23 19:45;sap1ens;Hi Valentina, unfortunately I didn’t have a chance to address feedback, so feel free to provide an alternative PR.;;;","16/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
Document dynamic property support for startup scripts,FLINK-28841,13475483,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Aug/22 17:32,25/Aug/22 07:24,04/Jun/24 20:41,25/Aug/22 07:24,,,,,1.16.0,,,,Deployment / Scripts,Documentation,,,,,,0,pull-request-available,,,,,,The support for dynamic properties in startup scripts isn't documented anywhere.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 25 07:24:17 UTC 2022,,,,,,,,,,"0|z17j3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Aug/22 07:24;chesnay;master: 58c4be49001b37ebe1b972b4fb91116a2bb1fb62;;;",,,,,,,,,,,,,,,,,,,,
Introduce roadmap document of Flink Table Store,FLINK-28840,13475412,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,05/Aug/22 13:06,11/Aug/22 04:50,04/Jun/24 20:41,11/Aug/22 04:50,,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,The Flink Table Store subproject needs its own roadmap document to present an overview of the general direction.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 04:50:52 UTC 2022,,,,,,,,,,"0|z17io0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 13:09;nicholasjiang;[~lzljs3620320] , could you please help to assign this ticket to me?;;;","11/Aug/22 04:50;lzljs3620320;master: 26786fad4df2fb889bc339cf93e5ce08e3ee8652;;;",,,,,,,,,,,,,,,,,,,
Incorrect english sentence in docs/content/docs/dev/dataset/iterations.md,FLINK-28839,13475395,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,bisvarup,bisvarup,05/Aug/22 12:24,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,Documentation,,,,,,,0,auto-deprioritized-minor,easy-fix,starter,,,,"There is this line in the[ iteration documentation|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/dataset/iterations/#example-propagate-minimum-in-graph] that reads like

_""it can be skipped it in the next workset"",_ 

This line does not look like properly formatted English, it should have been
_""it can be skipped in the next workset""_

 

!Screenshot 2022-08-05 at 5.56.52 PM.png|width=878,height=320!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/22 12:27;bisvarup;Screenshot 2022-08-05 at 5.56.52 PM.png;https://issues.apache.org/jira/secure/attachment/13047781/Screenshot+2022-08-05+at+5.56.52+PM.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:04 UTC 2023,,,,,,,,,,"0|z17ik8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 12:27;bisvarup;Hi folks I would be happy to take up this change.;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
Avoid to notify the elementQueue consumer when the fetch result is empty,FLINK-28838,13475334,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,aitozi,aitozi,05/Aug/22 08:53,11/Mar/24 12:43,04/Jun/24 20:41,,1.15.0,1.15.1,,,1.20.0,,,,Connectors / Common,,,,,,,0,pull-request-available,,,,,,"When using the new source api, I found that if the source has no data, it still brings high cpu usage. 
The reason behind this is that it will always return the {{RecordsWithSplitIds}} from the {{splitReader.fetch}} in FetchTask and it will be added to the elementQueue. It will make the consumer be notified to wake up frequently.
This causes the thread to keep busy to run and wake up, which leads to the high sys and user cpu usage.

I think not all the SplitReader#fetch will block until there is data, if it returns immediately when there is no data, then this problem will happen

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/22 08:55;aitozi;20220805165441.jpg;https://issues.apache.org/jira/secure/attachment/13047765/20220805165441.jpg",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 20 10:35:35 UTC 2022,,,,,,,,,,"0|z17i6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 09:07;aitozi;I think the fetch task can be improved from two aspects:

- check the {{RecordsWithSplitIds}} whether isEmpty before put it in the element queue
- add the default mechanism to cool down the fetch task, if there is no data. eg: add a SleepTask to taskQueue when its fetch result is empty {{RecordsWithSplitIds}} for several times


I have made a simple implementation for it, looking forward some thoughts from the community :);;;","08/Aug/22 14:43;aitozi;cc [~renqs] could you help take a look ?;;;","09/Aug/22 03:30;renqs;Thanks for the ticket [~aitozi]! Yeah we can definitely make some improvement as not all source implementations works as expected. 

I think your first proposal make sense to me. We can drop empty records earlier before putting into elementsQueue. I have some concerns about the second one (adding SleepTask) as we can hardly decide the length of sleep considering source implementations differ a lot. For example KafkaConsumer itself has ability to block the thread if no data is available for polling so it doesn't need the SleepTask at all. I prefer to leave it to split reader implementation itself as the doc of {{SplitReader#fetch}} is quite clear that it could be a blocking call. WDYT?

BTW which source has this issue? We can check its implementation too. ;;;","09/Aug/22 13:45;aitozi;[~renqs] Thanks for your inputs. Actually, it's observed in our internal source connector, I think the blocking mode do not conflict with the cool down mechanism, because if kafka block until there is data then it will never have the chance to trigger the SleepTask I think. 
Besides, I lean to provide an inner cool down manager which will be disabled by default, and if the {{fetch}} is non-blocking mode, user can enable it by setting a suitable threshold and sleep interval.
Otherwise, the current work flow will rely on a blocking {{fetch}} to avoid the busy loop in this case.;;;","20/Aug/22 10:35;aitozi;Hi [~renqs] do you have any further comments? If no objection, I'd like to open a PR for this.;;;",,,,,,,,,,,,,,,,
"Translate ""Hybrid Source"" page of ""DataStream Connectors"" into Chinese",FLINK-28837,13475331,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,JasonLee,JasonLee,05/Aug/22 08:33,18/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,chinese-translation,,,,,,,0,auto-deprioritized-minor,pull-request-available,,,,,"The page url is [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/datastream/hybridsource/]

The markdown file is located in docs/content.zh/docs/connectors/datastream/hybridsource.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:04 UTC 2023,,,,,,,,,,"0|z17i68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 08:29;ChunJi;Hi,[~JasonLee] ,i want to do this work,please assign it to me,thank you!;;;","29/Aug/22 11:05;JasonLee;hi [~ChunJi] ,I have submitted a PR for a while, but no one has reviewed it yet. If you are interested, there are many documents that need to be translated on the official website. You can choose the page you like to create the corresponding jira.;;;","30/Aug/22 09:40;ChunJi;ok;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,
Support broadcast in Thread Mode,FLINK-28836,13475330,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,05/Aug/22 08:33,08/Aug/22 10:00,04/Jun/24 20:41,08/Aug/22 10:00,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-25724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 10:00:16 UTC 2022,,,,,,,,,,"0|z17i60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 10:00;hxbks2ks;Merged into master via ff91aa53cfc9327ed591cbe0c1b3dcf9116dc3a5;;;",,,,,,,,,,,,,,,,,,,,
Savepoint and checkpoint capabilities and limitations table is incorrect,FLINK-28835,13475324,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,pnowojski,pnowojski,05/Aug/22 08:21,05/Aug/22 12:12,04/Jun/24 20:41,05/Aug/22 12:12,1.15.1,1.16.0,,,1.15.2,1.16.0,,,Documentation,,,,,,,0,pull-request-available,,,,,,"https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints_vs_savepoints/

is inconsistent with https://cwiki.apache.org/confluence/display/FLINK/FLIP-203%3A+Incremental+savepoints#FLIP203:Incrementalsavepoints-Proposal. ""Non-arbitrary job upgrade"" for unaligned checkpoints should be officially supported. 

It looks like a typo in the original PR FLINK-26134",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26134,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 12:12:18 UTC 2022,,,,,,,,,,"0|z17i4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 12:12;pnowojski;fixed on release-1.15 as 39a737f31be
merged commit b1c40bd into apache:master;;;",,,,,,,,,,,,,,,,,,,,
Add TemporalJoin example and ITCase,FLINK-28834,13475318,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qinjunjerry,qinjunjerry,qinjunjerry,05/Aug/22 07:49,06/Aug/22 16:10,04/Jun/24 20:41,06/Aug/22 16:10,,,,,1.16.0,,,,Examples,Table SQL / API,,,,,,0,pull-request-available,,,,,,"A temporal join example is useful to show users how to use temporary join and how it works. The corresponding ITCase also helps to verify the temporal join functionality in Flink SQL and to show users how to do Flink SQL testing.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 06 16:10:18 UTC 2022,,,,,,,,,,"0|z17i3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 16:10;jark;Fixed in master: 62786320eb555e36fe9fb82168fe97855dc54056
;;;",,,,,,,,,,,,,,,,,,,,
[5] Support customized scheduler options as a hashmap for each scheduler.,FLINK-28833,13475317,13432972,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bzhaoop,bzhaoop,bzhaoop,05/Aug/22 07:41,09/Sep/22 04:19,04/Jun/24 20:41,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,,,"-D{*}kubernetes.scheduler.<scheduler-name>.config=priorityclass:hight-priority{*}{*},X:Y…{*}

The option value is a map, and will be loaded by the specified customized scheduler.

Additional K8S customized configuration proposal, such as PodGroup in Volcano:
|*Key*|*Category*|*Related Config Options*|*Description*|
|config|Defined by user|kubernetes.scheduler.<scheduler-name>.config|The customized Pod Group definition for JobManager and TaskManager. Such as:
mincpu:1, minmember:2, minmemory:1000Mi, priorityclass: <CLASSNAME>, the customized scheduler will get the message it is concerned, ignoring others. |

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-05 07:41:22.0,,,,,,,,,,"0|z17i34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[4] Upgrade Fabric8 to the latest version,FLINK-28832,13475316,13432972,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,bzhaoop,bzhaoop,05/Aug/22 07:39,09/Sep/22 04:17,04/Jun/24 20:41,21/Aug/22 15:00,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,,,Due to the customized scheduler resources were introduced to Fabric8 in newer version. So We'd better to upgrade it to the latest version as much.,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28481,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-05 07:39:10.0,,,,,,,,,,"0|z17i2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[3] Support pluginable decorators mechanism,FLINK-28831,13475314,13432972,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bzhaoop,bzhaoop,bzhaoop,05/Aug/22 07:37,16/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,stale-assigned,,,,,"Introduce a pluginable decorators mechanism into Flink JobManager and TaskManager. Currently, all decorators are hard-coded in the JobManager deployment and TaskManager pod K8S creation.

 

We propose using SPI or Flink plugins mechanism to load the external decorators. In this way, we can make Flink more flexible towards supporting the decorators of customized K8S schedulers.

We propose a new plugin mechanism for supporting load the jar package of a single customized scheduler. The driver jar package should contain the specific K8sStepDecorator and its dependencies. Taking an example, the jar package should be packaged into the ‘opt’ directory, and contains 2 major things:
 # A K8sStepDecorator implemented by Customized K8S SchedulerA.
 # The all dependencies from the introduced K8sStepDecorator.

when users want to use the customized scheduler A, he/she need to create a new directory which named as the customized scheduler’s name A, then copy & paste the said jar into the new directory, that could be loaded when user specific the related Flink K8S configuration options and enable the functionality in Flink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:14 UTC 2023,,,,,,,,,,"0|z17i2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 07:44;gyfora;While working on this we should also allow an easy way to exclude any of the built in step decorators. Sometimes they can be very inconvenient :) ;;;","20/Aug/22 12:01;bzhaoop;[~gyfora] Hi, thanks for the advice. And sorry that I might miss the comment at that moment. I hope I can follow it now.

I think that would be another Issue to trace. ;).;;;","22/Aug/22 10:13;chesnay;Can you expand on what you're trying to do? Do you just want to exclude /re-order some steps, or also add completely custom ones?;;;","22/Aug/22 18:16;gyfora;[~chesnay] I think this ticket is about add completely new custom decorators. There is a ticket for excluding existing decorators by classname;;;","23/Aug/22 09:26;bzhaoop;https://issues.apache.org/jira/browse/FLINK-29055

I think we are discussing about this one.

From my personal thought, I think we'd better not re-order for avoiding change the behavior, just exclude. How about your thoughts?;;;","01/May/23 10:17;gyfora;[~bzhaoop] are you still working on this or can [~darenwkt] take over this work?;;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,
new stack udtf doesn't support atomic type ,FLINK-28830,13475313,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,05/Aug/22 07:35,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,1.20.0,,,,Table SQL / Planner,,,,,,,0,,,,,,,"{code:java}
// code placeholder
public class GenerateSeriesFunction extends BuiltInTableFunction<Long> {

    private static final long serialVersionUID = 1L;

    public GenerateSeriesFunction(SpecializedContext specializedContext) {
        super(BuiltInFunctionDefinitions.GENERATE_SERIES, specializedContext);
    }

    public void eval(long start, long stop) {
        eval(start, stop, 1);
    }
    
    public void eval(long start, long stop, long step) {
        long s = start;
        while (s <= stop) {
            collect(s);
            s += step;
        }
    }
}


public static final BuiltInFunctionDefinition GENERATE_SERIES =
        BuiltInFunctionDefinition.newBuilder()
                .name(""GENERATE_SERIES"")
                .kind(TABLE)
                .inputTypeStrategy(
                        or(
                                sequence(
                                        logical(LogicalTypeFamily.NUMERIC),
                                        logical(LogicalTypeFamily.NUMERIC)),
                                sequence(
                                        logical(LogicalTypeFamily.NUMERIC),
                                        logical(LogicalTypeFamily.NUMERIC),
                                        logical(LogicalTypeFamily.NUMERIC))))
                .outputTypeStrategy(explicit(DataTypes.BIGINT()))
                .runtimeClass(
                        ""org.apache.flink.table.runtime.functions.table.GenerateSeriesFunction"")
                .build(); {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 07:36:46 UTC 2022,,,,,,,,,,"0|z17i28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 07:36;jackylau;hi [~twalthr] 

val functionResultType = FlinkTypeFactory.toLogicalRowType(rexCall.getType) cause the function return type rowtype, and the code gen below cause it not translate from long to row data
{code:java}
// code placeholder
// code for wrapping atomic types
val collectorCode = if (!isCompositeType(outputType)) {
  val resultGenerator = new ExprCodeGenerator(collectorCtx, outputType.isNullable)
    .bindInput(outputType, externalResultTerm)
  val wrappedResult = resultGenerator.generateConverterResultExpression(
    returnType.asInstanceOf[RowType],
    classOf[GenericRowData])
  s""""""
     |${wrappedResult.code}
     |outputResult(${wrappedResult.resultTerm});
     |"""""".stripMargin
} else {
  s""""""
     |if ($externalResultTerm != null) {
     |  outputResult($externalResultTerm);
     |}
     |"""""".stripMargin
} {code};;;",,,,,,,,,,,,,,,,,,,,
[2] Need the ability of preparing K8S resource before JobManager creation,FLINK-28829,13475312,13432972,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bzhaoop,bzhaoop,bzhaoop,05/Aug/22 07:35,16/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,stale-assigned,,,,,"For applying the customized schedulers functionality or making Flink K8S more flexible, we need to make Flink to provide an ability to prepare the K8S resource before we actually set up the Flink components.

In current Flink K8S design, it provides buildAccompanyingKubernetesResources interface to add the resources from decorators, the mentioned AccompanyingKubernetesResources will be created after the JobManager Deployment creation. But some K8S Pod-related customized schedulers might need an extension K8S resource to control the real scheduling before the target Pods setup. On the other hand, we only make an example about the customized scheduler which only worked before the Pod creation, we can not imagine all customized schedulers works like that, so we can make the existing JobManager deployment create processing more common.

 

New interface should be provided, and the decorators can create the K8S resource for associate them with the following Flink Pods, then using buildAccompanyingKubernetesResources to refresh the owner reference for keeping the same behavior like other AccompanyingKubernetesResources.

public interface KubernetesStepDecorator {

    FlinkPod decorateFlinkPod(FlinkPod flinkPod);

    List<HasMetadata> buildAccompanyingKubernetesResources() throws IOException;

    List<HasMetadata> [Pre K8S resource creation Interface]();

}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:15 UTC 2023,,,,,,,,,,"0|z17i20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Sorting all unfinished readers in batches at one time in SortMergeResultPartitionReadScheduler,FLINK-28828,13475310,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,05/Aug/22 07:22,08/Aug/22 07:18,04/Jun/24 20:41,07/Aug/22 12:53,1.16.0,,,,,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Currently, when reading data in SortMergeResultPartitionReadScheduler, the reader is added to the priority queue immediately. However, the data read from this reader may not have been consumed, which will cause this reader to be ranked later in the queue, which is unfavorable to sequential reading.

To solve the issue, After reading the data, we should sort all unfinished readers in batches at one time, that is, add all unfinished readers to the priority queue, which is more conducive to sequential reading.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 12:53:11 UTC 2022,,,,,,,,,,"0|z17i1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 12:53;kevin.cyj;Merged into master via 07a309ad93444097049aa65944e2ca4fbff37e42;;;",,,,,,,,,,,,,,,,,,,,
Complete DataType support in DataStream API for PyFlink,FLINK-28827,13475309,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,05/Aug/22 07:18,17/Oct/22 10:44,04/Jun/24 20:41,08/Aug/22 09:19,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,"DataTypes are used for some FileSource formats originated from Table API, e.g. ParquetColumnarRowInput, but some types doesn't work in current PyFlink implementations, e.g. TIMESTAMP_LTZ.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29658,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 09:19:23 UTC 2022,,,,,,,,,,"0|z17i1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 09:19;dianfu;Merged to master via 1c40dc15fbc29e8e5e514565109a3fb05b47a83f;;;",,,,,,,,,,,,,,,,,,,,
Avoid notifying too frequently when recycling buffers for BatchShuffleReadBufferPool,FLINK-28826,13475305,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,05/Aug/22 07:04,07/Aug/22 12:37,04/Jun/24 20:41,07/Aug/22 12:37,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"When recycling buffers in BatchShuffleReadBufferPool, the number of buffers may be larger than numBuffersPerRequest, which may cause too frequent notifications that the buffers are already available.

So we should modify the condition of notifications that the buffer is available to solve this problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 12:37:14 UTC 2022,,,,,,,,,,"0|z17i0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 12:37;kevin.cyj;Merged into master via 05de0a295efe06ff342a211071fc3810fdd53a2e.;;;",,,,,,,,,,,,,,,,,,,,
[1] Support specify K8S pod scheduler,FLINK-28825,13475304,13432972,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bzhaoop,bzhaoop,bzhaoop,05/Aug/22 07:03,16/Aug/23 22:35,04/Jun/24 20:41,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,pull-request-available,stale-assigned,,,,,"Need support 3 options of K8S Pod Scheduler.

-D{*}kubernetes.jobmanager.scheduler-name{*}=[scheduler name]

-D{*}kubernetes.taskmanager.scheduler-name{*}=[scheduler name]

-D{*}kubernetes.scheduler-name={*}[scheduler name]",,,,,,,,,,FLINK-31188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:15 UTC 2023,,,,,,,,,,"0|z17i08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
StreamingJobGraphGenerator#triggerSerializationAndReturnFuture might swallow exception ,FLINK-28824,13475300,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Weijie Guo,gaoyunhaii,gaoyunhaii,05/Aug/22 06:36,28/Sep/22 07:38,04/Jun/24 20:41,28/Sep/22 07:38,1.16.0,,,,,,,,API / DataStream,,,,,,,0,,,,,,,"Currently the `triggerSerializationAndReturnFuture` is executed in a separate thread. Currently when this method throws exception (for example, users may pass an UDF that is not serializable), then the main thread would be blocked.

We'd better fail the future for any exception thrown in this thread. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 28 07:38:17 UTC 2022,,,,,,,,,,"0|z17hzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/22 04:24;Weijie Guo;This ticket is duplicated to FLINK-29431.

 ;;;","28/Sep/22 07:38;gaoyunhaii;Thanks [~Weijie Guo] for looking at this issue!;;;",,,,,,,,,,,,,,,,,,,
Enlarge the max requested buffers for SortMergeResultPartitionReadScheduler,FLINK-28823,13475298,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,05/Aug/22 06:33,08/Aug/22 07:27,04/Jun/24 20:41,08/Aug/22 07:27,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"The num of the max requested buffers in SortMergeResultPartitionReadScheduler is determined by the num of subpartitions, which may be insufficient in some scenarios, such as speculative execution and partition reuse.

Therefore, we should increase the max requested buffers to make full use of available memory when TM memory is sufficient.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 07:27:20 UTC 2022,,,,,,,,,,"0|z17hyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 07:27;kevin.cyj;Merged into master via ca45a28205b424c2c77a21366fb29a67457672ff;;;",,,,,,,,,,,,,,,,,,,,
Avoid create VectorizedColumnBatch for each read in ArrowReader,FLINK-28822,13475293,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,05/Aug/22 06:21,08/Aug/22 01:22,04/Jun/24 20:41,08/Aug/22 01:22,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,"When checking the ArrowReader's logic, I found that it's not necessary to {{new VectorizedColumnBatch}} during each read. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 01:22:05 UTC 2022,,,,,,,,,,"0|z17hxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 06:41;aitozi;cc [~dianfu] could you help take a look on this ticket?;;;","08/Aug/22 01:22;dianfu;Merged to master via e47cb6eeb4ceac13db4b8b357a0885b77651826c;;;",,,,,,,,,,,,,,,,,,,
Adjust join cost for dpp query pattern which could help more plans use dpp,FLINK-28821,13475265,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,05/Aug/22 02:33,10/Aug/22 02:33,04/Jun/24 20:41,10/Aug/22 02:33,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 02:33:15 UTC 2022,,,,,,,,,,"0|z17hrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 02:34;337361684@qq.com;Hi, [~godfreyhe] , can you assign this to me. Thanks!;;;","10/Aug/22 02:33;godfreyhe;Fixed in master: 320f4cd5e5f3e851ad14f9d1e504959e58106157;;;",,,,,,,,,,,,,,,,,,,
Pulsar Connector PulsarSink performance issue when delivery guarantee is not NONE,FLINK-28820,13475263,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,simonss,simonss,05/Aug/22 02:29,24/Jul/23 06:09,04/Jun/24 20:41,18/Oct/22 13:34,1.15.0,,,,1.17.0,,,,Connectors / Pulsar,,,,,,,0,pull-request-available,,,,,,"Pulsar Sink writes at a speed of a dozen messages per second when At-Least-Once and Exactly-Once are enabled. When None is used, the message write speed reaches the Pulsar write bottleneck.",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32645,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 18 13:35:16 UTC 2022,,,,,,,,,,"0|z17hr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/22 13:34;tison;master via 3f863b1e280a232c70b594d70035d34642d8a367;;;","18/Oct/22 13:35;tison;[~simonss] does including this improvement starting from 1.17.0 work for you?;;;",,,,,,,,,,,,,,,,,,,
The natural time of day TTL,FLINK-28819,13475261,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,kcz,kcz,05/Aug/22 01:59,18/Aug/23 02:22,04/Jun/24 20:41,,1.15.1,,,,,,,,API / DataStream,Runtime / State Backends,,,,,,0,,,,,,,"When new energy vehicles are monitored for parts, in order to reduce the push of abnormal information, for the same abnormal information, only push the first time a day, and the current community's statettl does not support emptying the state according to the natural day time. Hopefully, the community will support the emptying of the state according to the natural day time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 02:22:45 UTC 2023,,,,,,,,,,"0|z17hqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 10:09;Yanfei Lei;Hi [~kcz], could you explain what ""natural time of day"" means, is it processing time or user-specified time？:)

If “natural time of day” represent user-specified time, how would you like to pass it into Flink? ;;;","06/Aug/22 03:09;kcz;For example: I want to empty all states in the early morning every day, the current statettl can not be implemented, need to use onTimer to achieve, I hope that the framework can support the emptying of the state in the early morning of each day, or how much time between;;;","30/Aug/22 09:16;martijnvisser;I think ""natural time of day"" also implies that we need to take timezones and those type of things into account. I think Flink needs to stay away from that. I think there are mechanism in Flink already to support these type of use cases;;;","18/Aug/23 02:22;masteryhx;Hi，[~kcz] 

Could you also share your specific job logic about why you want to emptying states periodically instead of using current processing time TTL ?;;;",,,,,,,,,,,,,,,,,
KafkaSinkITCase.testWriteRecordsToKafkaWithExactlyOnceGuarantee failed with AssertionFailedError,FLINK-28818,13475260,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,hxbks2ks,hxbks2ks,05/Aug/22 01:50,16/Oct/23 12:14,04/Jun/24 20:41,16/Oct/23 12:14,1.16.0,1.17.0,,,,,,,Connectors / Kafka,,,,,,,0,auto-deprioritized-major,test-stability,,,,,"{code:java}
2022-08-04T13:31:52.8933185Z Aug 04 13:31:52 [ERROR] org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testWriteRecordsToKafkaWithExactlyOnceGuarantee  Time elapsed: 4.146 s  <<< FAILURE!
2022-08-04T13:31:52.8933887Z Aug 04 13:31:52 org.opentest4j.AssertionFailedError: 
2022-08-04T13:31:52.8936215Z Aug 04 13:31:52 
2022-08-04T13:31:52.8936766Z Aug 04 13:31:52 expected: 1664L
2022-08-04T13:31:52.8937362Z Aug 04 13:31:52  but was: 1858L
2022-08-04T13:31:52.8937955Z Aug 04 13:31:52 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-08-04T13:31:52.8938814Z Aug 04 13:31:52 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-08-04T13:31:52.8939622Z Aug 04 13:31:52 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-08-04T13:31:52.8940522Z Aug 04 13:31:52 	at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.writeRecordsToKafka(KafkaSinkITCase.java:399)
2022-08-04T13:31:52.8941489Z Aug 04 13:31:52 	at org.apache.flink.connector.kafka.sink.KafkaSinkITCase.testWriteRecordsToKafkaWithExactlyOnceGuarantee(KafkaSinkITCase.java:213)
2022-08-04T13:31:52.8942418Z Aug 04 13:31:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-04T13:31:52.8943081Z Aug 04 13:31:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-04T13:31:52.8944125Z Aug 04 13:31:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-04T13:31:52.8944851Z Aug 04 13:31:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-04T13:31:52.8945519Z Aug 04 13:31:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-04T13:31:52.8946254Z Aug 04 13:31:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-04T13:31:52.8947307Z Aug 04 13:31:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-04T13:31:52.8948048Z Aug 04 13:31:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-04T13:31:52.8948777Z Aug 04 13:31:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-04T13:31:52.8949511Z Aug 04 13:31:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-04T13:31:52.8950206Z Aug 04 13:31:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-04T13:31:52.8950885Z Aug 04 13:31:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-04T13:31:52.8951656Z Aug 04 13:31:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-04T13:31:52.8952421Z Aug 04 13:31:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-04T13:31:52.8953092Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-04T13:31:52.8953802Z Aug 04 13:31:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-04T13:31:52.8954499Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-04T13:31:52.8955195Z Aug 04 13:31:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-04T13:31:52.8955939Z Aug 04 13:31:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-04T13:31:52.8956600Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-04T13:31:52.8957229Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-04T13:31:52.8957880Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-04T13:31:52.8958531Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-04T13:31:52.8959336Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-04T13:31:52.8960024Z Aug 04 13:31:52 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-04T13:31:52.8960715Z Aug 04 13:31:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-04T13:31:52.8961581Z Aug 04 13:31:52 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-08-04T13:31:52.8962406Z Aug 04 13:31:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-08-04T13:31:52.8963013Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-04T13:31:52.8963655Z Aug 04 13:31:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-04T13:31:52.8964256Z Aug 04 13:31:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-04T13:31:52.8964848Z Aug 04 13:31:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-04T13:31:52.8965481Z Aug 04 13:31:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-04T13:31:52.8966234Z Aug 04 13:31:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-04T13:31:52.8966971Z Aug 04 13:31:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-04T13:31:52.8967762Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-04T13:31:52.8968624Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-04T13:31:52.8969491Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-04T13:31:52.8970399Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-04T13:31:52.8971372Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-04T13:31:52.8972292Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-04T13:31:52.8973024Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-04T13:31:52.8973836Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-04T13:31:52.8974714Z Aug 04 13:31:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-04T13:31:52.8975541Z Aug 04 13:31:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-04T13:31:52.8976400Z Aug 04 13:31:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-04T13:31:52.8977227Z Aug 04 13:31:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-04T13:31:52.8978026Z Aug 04 13:31:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-04T13:31:52.8978766Z Aug 04 13:31:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-04T13:31:52.8979454Z Aug 04 13:31:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-04T13:31:52.8980140Z Aug 04 13:31:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39289&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36783
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 10:35:27 UTC 2023,,,,,,,,,,"0|z17hqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 01:51;hxbks2ks;cc [~renqs];;;","06/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44525&view=logs&j=fa307d6d-91b1-5ab6-d460-ef50f552b1fe&t=21eae189-b04c-5c04-662b-17dc80ffc83a&l=37447;;;","06/Mar/23 08:49;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46753&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=37306;;;","07/Mar/23 16:50;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46915&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203&l=36626;;;","13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,
NullPointerException in HybridSource when restoring from checkpoint,FLINK-28817,13475253,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhongqishang,Benenson,Benenson,05/Aug/22 00:31,16/Aug/22 20:31,04/Jun/24 20:41,12/Aug/22 23:55,1.14.4,1.15.1,,,1.15.2,1.16.0,,,Connectors / Common,,,,,,,1,pull-request-available,,,,,,"Scenario:
 # CheckpointCoordinator - Completed checkpoint 14 for job 00000000000000000000000000000000
 # HybridSource successfully completed processing a few SourceFactories, that reads from s3
 # HybridSourceSplitEnumerator.switchEnumerator failed with com.amazonaws.SdkClientException: Unable to execute HTTP request: Read timed out. This is intermittent error, it is usually fixed, when Flink recover from checkpoint & repeat the operation.
 # Flink starts recovering from checkpoint, 
 # HybridSourceSplitEnumerator receives SourceReaderFinishedEvent\{sourceIndex=-1}
 # Processing this event cause 

2022/08/08 08:39:34.862 ERROR o.a.f.r.s.c.SourceCoordinator - Uncaught exception in the SplitEnumerator for Source Source: hybrid-source while handling operator event SourceEventWrapper[SourceReaderFinishedEvent

{sourceIndex=-1}

] from subtask 6. Triggering job failover.
java.lang.NullPointerException: Source for index=0 is not available from sources: \{788=org.apache.flink.connector.file.src.SppFileSource@5a3803f3}
at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104)
at org.apache.flink.connector.base.source.hybridspp.SwitchedSources.sourceOf(SwitchedSources.java:36)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.sendSwitchSourceEvent(HybridSourceSplitEnumerator.java:152)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.handleSourceEvent(HybridSourceSplitEnumerator.java:226)
...

I'm running my version of the Hybrid Sources with additional logging, so line numbers & some names could be different from Flink Github.

My Observation: the problem is intermittent, sometimes it works ok, i.e. SourceReaderFinishedEvent comes with correct sourceIndex. As I see from my log, it happens if my SourceFactory.create()  is executed BEFORE HybridSourceSplitEnumerator - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=-1}.
If  HybridSourceSplitEnumerator - handleSourceEvent is executed before my SourceFactory.create(), then sourceIndex=-1 in SourceReaderFinishedEvent

Preconditions-checkNotNull-error log from JobMgr is attached",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/22 22:19;Benenson;Preconditions-checkNotNull-error.zip;https://issues.apache.org/jira/secure/attachment/13047897/Preconditions-checkNotNull-error.zip","05/Aug/22 00:30;Benenson;bf-29-JM-err-analysis.log;https://issues.apache.org/jira/secure/attachment/13047737/bf-29-JM-err-analysis.log",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 20:31:20 UTC 2022,,,,,,,,,,"0|z17how:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 09:49;zhongqishang;I encountered the similar problem.

[~thw] Please take a look the following case.

 

org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumeratorTest
{code:java}
@Test
public void testRestoreEnumeratorWith2ndSource() throws Exception {
    setupEnumeratorAndTriggerSourceSwitch();
    HybridSourceEnumeratorState enumeratorState = enumerator.snapshotState(0);
    MockSplitEnumerator underlyingEnumerator = getCurrentEnumerator(enumerator);
    assertThat(
            (List<MockSourceSplit>)
                    Whitebox.getInternalState(underlyingEnumerator, ""splits""))
            .hasSize(0);
    enumerator =
            (HybridSourceSplitEnumerator) source.restoreEnumerator(context, enumeratorState);
    enumerator.start();
    enumerator.handleSourceEvent(SUBTASK0, new SourceReaderFinishedEvent(-1));
    underlyingEnumerator = getCurrentEnumerator(enumerator);
    assertThat(
            (List<MockSourceSplit>)
                    Whitebox.getInternalState(underlyingEnumerator, ""splits""))
            .hasSize(0);
}
 {code}
 ;;;","08/Aug/22 22:17;Benenson;Clarification for the problem:

1. --  HybridSourceSplitEnumerator.switchEnumerator failed with 
com.amazonaws.SdkClientException: Unable to execute HTTP request: Read timed out
Caused by: java.net.SocketTimeoutException: Read timed out
This is intermittent error, it is usually fixed, when Flink recover from checkpoint & repeat the operation

2. --  Flink starts recovering from checkpoint:
CheckpointCoordinator - Restoring job 00000000000000000000000000000000 from Checkpoint
SourceCoordinator - Closing SourceCoordinator for source Source: hybrid-source.
SourceCoordinator - Restoring SplitEnumerator of source Source: hybrid-source from checkpoint.
SourceCoordinator - Starting split enumerator for source Source: hybrid-source.
HybridSourceSplitEnumerator - Restoring enumerator for sourceIndex=788
HybridSourceSplitEnumerator - Starting enumerator for sourceIndex=788

3. --  HybridSourceSplitEnumerator receives SourceReaderFinishedEvent\{sourceIndex=-1}
HybridSourceSplitEnumerator - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=-1} subtask=6

4. --  Processing this event cause 
2022/08/08 08:39:34.862 ERROR o.a.f.r.s.c.SourceCoordinator - Uncaught exception in the SplitEnumerator for Source Source: hybrid-source while handling operator event SourceEventWrapper[SourceReaderFinishedEvent\{sourceIndex=-1}] from subtask 6. Triggering job failover.
java.lang.NullPointerException: Source for index=0 is not available from sources: \{788=org.apache.flink.connector.file.src.SppFileSource@5a3803f3}
at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104)
at org.apache.flink.connector.base.source.hybridspp.SwitchedSources.sourceOf(SwitchedSources.java:36)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.sendSwitchSourceEvent(HybridSourceSplitEnumerator.java:152)
at org.apache.flink.connector.base.source.hybridspp.HybridSourceSplitEnumerator.handleSourceEvent(HybridSourceSplitEnumerator.java:226)
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$handleEventFromOperator$1(SourceCoordinator.java:182)
at org.apache.flink.runtime.source.coordinator.SourceCoordinator.lambda$runInEventLoop$8(SourceCoordinator.java:344)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

I'm running my version of the Hybrid Sources with additional logging, so line numbers & some names could be different from Flink Github.

My Observation: the problem is intermittent, sometimes it works ok, i.e. SourceReaderFinishedEvent comes with correct sourceIndex. As I see from my log, it happens if my SourceFactory.create()  is executed BEFORE HybridSourceSplitEnumerator - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=-1}.
If  HybridSourceSplitEnumerator - handleSourceEvent is executed before my SourceFactory.create(), then sourceIndex=-1 in SourceReaderFinishedEvent

[~thw] , [~mason6345] could you, please, look at this issue?

Preconditions-checkNotNull-error log from JobMgr is attached;;;","10/Aug/22 07:11;zhongqishang;[~thw] 

I tried to open a PR for this issue.

[~Benenson] has already test for this PR.

 ;;;","11/Aug/22 14:28;thw;[~Benenson] thanks for investigating this issue. I think that has to do with the reader not having any restored splits (most likely because none were previously assigned) and therefore reporting -1 back to the enumerator. Let me check what the correct fix for this is.;;;","12/Aug/22 09:55;nicholasjiang;[~thw], IMO, this pull request could also fixed the bug of FLINK-26938 , right?

cc [~zhongqishang] ;;;","12/Aug/22 23:54;thw;[~nicholasjiang] I believe it does. Can you please verify and close FLINK-26938 if so?;;;","12/Aug/22 23:56;thw;[~Benenson] thank you for the thorough investigation!;;;","16/Aug/22 20:31;dannycranmer;* Merged commit [{{6e80d90}}|https://github.com/apache/flink/commit/6e80d90b1611499375cf74b45a0828db383aac7d] into apache:master
 * Merged commit [{{e5570e3}}|https://github.com/apache/flink/commit/e5570e3e33ac33fd1b31d38c86ac6a291e7bc47e] into apache:release-1.15 ;;;",,,,,,,,,,,,,
Include some metrics for the pod created in operator,FLINK-28816,13475252,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,aitozi,aitozi,05/Aug/22 00:21,07/Aug/22 02:09,04/Jun/24 20:41,07/Aug/22 02:09,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"Currently, the metrics are around the operator self operation. In our use case, we also want to measure the metric especially about the flink pod's create time cost, pod create failure rate metrics, I think the operator is the best place to put/collect these metrics.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 02:09:17 UTC 2022,,,,,,,,,,"0|z17hoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 00:23;aitozi;cc [~gyfora] [~wangyang0918] Do you think it's suitable to add this in the operator ?;;;","05/Aug/22 11:49;wangyang0918;TBH, I hesitate to add pod related metrics in the flink-kubernetes-operator. In my opinion, many companies have already build their own monitoring system based on the promethus, which should include the pod creation metrics. The monitoring is not only for Flink workloads, but also the online systems, spark workloads, etc.

Moreover, I do not think the operator has enough information to calculate the pod creation cost/failure-rate metrics except for querying from K8s.;;;","07/Aug/22 02:09;aitozi;[~wangyang0918] Thanks for your valuable insight, will check the internal monitoring system for these metrics.;;;",,,,,,,,,,,,,,,,,,
"Translate the ""Real Time Reporting with the Table API"" page into Chinese",FLINK-28815,13475192,13213873,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,zyxing,zyxing,zyxing,04/Aug/22 15:03,24/Aug/22 07:54,04/Jun/24 20:41,24/Aug/22 07:54,,,,,1.16.0,,,,chinese-translation,Documentation,,,,,,0,pull-request-available,,,,,,"Page ""Real Time Reporting with the Table API"" need to be translated in Chinese.

I will be pleasure to take this PR.

PS:

Also willing fix a typo on page ""Fraud Detection with the DataStream API""  in chinese with the same PR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 24 07:54:21 UTC 2022,,,,,,,,,,"0|z17hbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 07:54;hxb;Merged into master via 221d70d9930f72147422ea24b399f006ebbfb8d7;;;",,,,,,,,,,,,,,,,,,,,
Update postgres driver because of CVE-2022-31197,FLINK-28814,13475175,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,04/Aug/22 13:15,30/Aug/22 09:17,04/Jun/24 20:41,30/Aug/22 09:17,,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,More details about CVE at pgjdbc repo page https://github.com/pgjdbc/pgjdbc/security/advisories/GHSA-r38f-c4h4-hqq2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 09:17:28 UTC 2022,,,,,,,,,,"0|z17h7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:17;martijnvisser;Fixed in master: 7669daffdc5adc7cb719a47266e3cc47af83f8dd;;;",,,,,,,,,,,,,,,,,,,,
new stach builtinfunction  validateClassForRuntime it not correct,FLINK-28813,13475150,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,04/Aug/22 12:18,11/Mar/24 12:44,04/Jun/24 20:41,,1.16.0,,,,1.20.0,,,,Table SQL / Planner,,,,,,,0,,,,,,,"{code:java}
// code placeholder

public static final BuiltInFunctionDefinition CONV =
        BuiltInFunctionDefinition.newBuilder()
                .name(""CONV"")
                .kind(SCALAR)
                .inputTypeStrategy(
                        or(
                                sequence(
                                        logical(LogicalTypeFamily.INTEGER_NUMERIC),
                                        logical(LogicalTypeFamily.INTEGER_NUMERIC),
                                        logical(LogicalTypeFamily.INTEGER_NUMERIC)),
                                sequence(
                                        logical(LogicalTypeFamily.CHARACTER_STRING),
                                        logical(LogicalTypeFamily.INTEGER_NUMERIC),
                                        logical(LogicalTypeFamily.INTEGER_NUMERIC))))
                .outputTypeStrategy(nullableIfArgs(explicit(DataTypes.STRING())))
                .runtimeClass(""org.apache.flink.table.runtime.functions.scalar.ConvFunction"")
                .build();{code}
{code:java}

// code placeholder
public class ConvFunction extends BuiltInScalarFunction {

    public ConvFunction(SpecializedFunction.SpecializedContext context) {
        super(BuiltInFunctionDefinitions.CONV, context);
    }

    public static StringData eval(StringData input, Integer fromBase, Integer toBase) {
        if (input == null || fromBase == null || toBase == null) {
            return null;
        }
        return StringData.fromString(BaseConversionUtils.conv(input.toBytes(), fromBase, toBase));
    }

    public static StringData eval(long input, Integer fromBase, Integer toBase) {
        return eval(StringData.fromString(String.valueOf(input)), fromBase, toBase);
    }
}


@Test
public void testRowScalarFunction1() throws Exception {
    tEnv().executeSql(
                    ""CREATE TABLE TestTable(s STRING) "" + ""WITH ('connector' = 'COLLECTION')"");

    // the names of the function input and r differ
    tEnv().executeSql(""INSERT INTO TestTable select conv(3, cast(1 AS TINYINT), 4)"").await();

} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/22 12:23;jackylau;image-2022-08-04-20-23-57-269.png;https://issues.apache.org/jira/secure/attachment/13047713/image-2022-08-04-20-23-57-269.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 12:26:51 UTC 2022,,,,,,,,,,"0|z17h20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 12:21;jackylau;org.apache.flink.table.api.ValidationException: Could not find an implementation method 'eval' in class 'org.apache.flink.table.runtime.functions.scalar.ConvFunction' for function 'CONV' that matches the following signature: org.apache.flink.table.data.StringData eval(org.apache.flink.table.data.StringData, java.lang.Byte, java.lang.Integer)  at org.apache.flink.table.functions.UserDefinedFunctionHelper.validateClassForRuntime(UserDefinedFunctionHelper.java:319) ;;;","04/Aug/22 12:24;jackylau;the eval method args type can be Integer/int, which depends on uses. the conversions class will not match

 

!image-2022-08-04-20-23-57-269.png!;;;","04/Aug/22 12:26;jackylau;so i think we should easy check instead of strict check. what do you think [~twalthr]  [~jark] ;;;",,,,,,,,,,,,,,,,,,
Add read.compacted to read compacted snapshot only,FLINK-28812,13475138,13475067,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,lzljs3620320,lzljs3620320,04/Aug/22 11:38,24/Nov/22 06:55,04/Jun/24 20:41,24/Nov/22 06:55,,,,,table-store-0.3.0,,,,,,,,,,,0,pull-request-available,,,,,,"If we separate the compaction from the writing, perhaps a large number of level0 files will be generated, which may lead to an OOM of merging too many files when reading.
We can introduce `read.compacted` to avoid reading uncompacted snapshot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 24 03:28:17 UTC 2022,,,,,,,,,,"0|z17gzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/22 03:28;lzljs3620320;master:
638c34b39ab0b57b14cef9b050c749de7b1bf7d3
255c4f7a02a619217169df4c12d351d04c5dad35;;;",,,,,,,,,,,,,,,,,,,,
Enable Java 11 tests for Hbase 2,FLINK-28811,13475097,13416492,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Aug/22 10:04,09/Aug/22 08:44,04/Jun/24 20:41,09/Aug/22 08:44,,,,,1.16.0,,,,Connectors / HBase,,,,,,,0,,,,,,,AFAICT the hbase 2.2 unit/it cases success on java 11.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 08:44:39 UTC 2022,,,,,,,,,,"0|z17gq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 08:44;chesnay;master: 5fb135e23e3251bcacf19d8db8cf358ddda76d6e;;;",,,,,,,,,,,,,,,,,,,,
Introduce write.skip-compaction to skip compaction on write,FLINK-28810,13475083,13475067,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Aug/22 09:32,19/Sep/22 11:02,04/Jun/24 20:41,19/Sep/22 11:02,,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"We want to separate the compaction from the writing, first is support writing without compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 19 11:02:23 UTC 2022,,,,,,,,,,"0|z17gn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 11:02;lzljs3620320;master: fc73a0bc4427741b41cb4b15e75c2dba363b97fb;;;",,,,,,,,,,,,,,,,,,,,
Concurrent write support for table store,FLINK-28809,13475067,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Aug/22 09:22,09/Jan/23 03:03,04/Jun/24 20:41,09/Jan/23 03:03,,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,,,,,,,"Streaming Concurrent Writes, to support this, we need to separate the compaction from the writing.
And we should provide an independent compaction job support, both streaming and batch mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-04 09:22:02.0,,,,,,,,,,"0|z17gjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsvFileFormatFactory#createEncodingFormat should create ConverterContext on server-side,FLINK-28808,13475055,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Aug/22 09:04,05/Aug/22 17:17,04/Jun/24 20:41,05/Aug/22 17:17,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,"Since we shouldn't assume that jackson mappers are serializable (due to non-serializable extension) the RowDataToCsvFormatConverterContext also shouldn't be serializable.

With that in mind the CsvFileFormatFactory should create the context when the writer is created, not beforehand.",,,,,,,,,,,,,,FLINK-28621,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 17:17:09 UTC 2022,,,,,,,,,,"0|z17ggw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 17:17;chesnay;master: e0f4a0ca9271f0e9e31011578dbdea4b5a8d30fe;;;",,,,,,,,,,,,,,,,,,,,
Various components don't respect schema lifecycle,FLINK-28807,13475053,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Aug/22 08:57,13/Sep/22 08:59,04/Jun/24 20:41,06/Aug/22 20:42,1.15.0,,,,1.16.0,elasticsearch-3.0.0,,,API / Python,Connectors / ElasticSearch,Connectors / Kafka,Connectors / Kinesis,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Tests,,0,pull-request-available,,,,,,A surprising number of components never call \{{(De)SerializationSchema#open}} making life very difficult for people who want to make use of said method.,,,,,,,,,,,,,,FLINK-28621,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 06 20:42:37 UTC 2022,,,,,,,,,,"0|z17ggg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 20:42;chesnay;master: fb95798b1c301152b912c4b8ec4a737ea16d8641
elasticsearch-main: c9ec08a7bafa343c4a74e2579d13b0a00b6317b5;;;",,,,,,,,,,,,,,,,,,,,
Add Estimator and Transformer for IDF,FLINK-28806,13475051,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,04/Aug/22 08:53,19/Apr/23 01:35,04/Jun/24 20:41,19/Apr/23 01:35,ml-2.2.0,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 19 01:34:46 UTC 2023,,,,,,,,,,"0|z17gg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:34;lindong;Merged to apache/flink-ml master branch d224f565dd7adfa59f10ae208c49159abf106635;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer for HashingTF,FLINK-28805,13475050,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,04/Aug/22 08:52,24/Aug/22 03:12,04/Jun/24 20:41,24/Aug/22 03:12,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-04 08:52:32.0,,,,,,,,,,"0|z17gfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use proper stand-ins for missing metrics groups,FLINK-28804,13475049,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,04/Aug/22 08:51,04/Aug/22 21:27,04/Jun/24 20:41,04/Aug/22 21:27,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,"A few classes call the open() method of schemas with a custom initialization context that either throws an exception or returns null when the metricgroup is accessed.

The correct approach is to return an unregistered metrics group.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 21:27:50 UTC 2022,,,,,,,,,,"0|z17gfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 21:27;chesnay;master: fd26e088a43a9426ad5f0b94237495d50d78656b;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for KBinsDiscretizer,FLINK-28803,13475048,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,04/Aug/22 08:51,24/Aug/22 03:15,04/Jun/24 20:41,17/Aug/22 06:29,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 06:29:25 UTC 2022,,,,,,,,,,"0|z17gfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 06:29;zhangzp;fixed on master via be546ad47887bdf2bbcf66b78d33bbbc5cdf643b;;;",,,,,,,,,,,,,,,,,,,,
EOFException when recovering from a checkpoint,FLINK-28802,13475038,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,YordanPavlov,YordanPavlov,04/Aug/22 08:24,04/Aug/22 08:24,04/Jun/24 20:41,,,,,,,,,,,,,,,,,0,,,,,,,"Flink version: 1.14.2

When recovering from a Savepoint, the TaskManager would throw an EOFException at this point:

[https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer.java#L1681]

It looks like the transactional id is missing on deserialization. As an interesting observation, before trying to recover from the mentioned checkpoint the job failed with:

 
{code:java}
 Aug 2, 2022 @ 16:50:40.107    Caused by: org.apache.kafka.common.errors.OutOfOrderSequenceException: The broker received an out of order sequence number.
    Aug 2, 2022 @ 16:50:40.106    2022-08-02 13:50:40.106 [flink-akka.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph  - Time correct sink UTXO Balances -> Sink Save to Kafka UTXO Balances bch-balances-v4 (1/1) (d2847bd31137561e3ba075a7cf70ee7c) switched from RUNNING to FAILED on 10.42.238.71:37353-fc3e48 @ bch-balances-v4-flink-taskmanager-0.bch-balances-v4-flink-taskmanager.flink.svc.cluster.local (dataPort=42553).
    Aug 2, 2022 @ 16:50:40.106    org.apache.flink.util.FlinkRuntimeException: Failed to send data to Kafka bch-balances-v4-0@-1 with FlinkKafkaInternalProducer{transactionalId='bch-balances-v4-0-690507', inTransaction=true, closed=false} {code}
 

Could it be that Flink actually failed to construct the checkpoint but still marked it as completed. What would be the way to check this? Is there something like a checksum byte that is checked on recovery?
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-04 08:24:37.0,,,,,,,,,,"0|z17gd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade OSS SDK Version,FLINK-28801,13475036,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wujinhu,wujinhu,wujinhu,04/Aug/22 08:16,05/Aug/22 03:26,04/Jun/24 20:41,05/Aug/22 03:26,1.15.1,,,,1.16.0,,,,Connectors / FileSystem,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 03:26:21 UTC 2022,,,,,,,,,,"0|z17gco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 03:26;lzljs3620320;master: d9e6721ac724409b2d4276374959db89287e90c5;;;",,,,,,,,,,,,,,,,,,,,
HsFileDataManager should avoid busy-loop when fileReader has not data to read,FLINK-28800,13475028,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,04/Aug/22 07:26,18/Aug/22 15:10,04/Jun/24 20:41,18/Aug/22 15:10,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"When fileReader has no data to read, for example, most of data is consumed from memory. HsFileDataManager will encounter busy-loop problem, which will lead to a meaningless surge in CPU utilization and seriously affect performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 18 15:10:33 UTC 2022,,,,,,,,,,"0|z17gaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Aug/22 15:10;xtsong;master (1.16): 5d13403429db27d63fdd6932c65c23ed5b90ef96;;;",,,,,,,,,,,,,,,,,,,,
Hybrid shuffle can't schedule graph contains blocking edge,FLINK-28799,13475025,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,04/Aug/22 07:19,09/Aug/22 02:10,04/Jun/24 20:41,09/Aug/22 02:10,1.16.0,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,"Based on TPC-DS test, we found that hybrid shuffle can't schedule graph contains blocking edge. The reason is that some batch operators will forcibly set the exchange mode to blocking, which breaks ALL_ EDGE_HYBRID‘s constraint makes the scheduling deadlock.

We should think of a better way to support the scheduling the graph of all kinds of edges, including hybrid edge.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 02:10:38 UTC 2022,,,,,,,,,,"0|z17ga8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 02:10;xtsong;master (1.16): 4deaf6edc152d06488f8738386b6a8b7544fe5e9;;;",,,,,,,,,,,,,,,,,,,,
Upgrade JDOM version to 2.0.6.1 in order to resolve CVE-2021-33813 ,FLINK-28798,13475024,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,bilna123,bilna123,04/Aug/22 07:18,30/Aug/22 12:14,04/Jun/24 20:41,30/Aug/22 12:14,1.13.6,,,,,,,,FileSystems,,,,,,,0,,,,,,,The flink-oss-fs-hadoop module(flink/flink-filesystems/flink-oss-fs-hadoop/pom.xml) has aliyun-sdk-oss:3.4.1 as dependency. The version of jdom in aliyun-sdk-oss:3.4.1 is 1.1 which is vulnerable. The aliyun-sdk-oss:3.14.1 has jdom:2.0.6.1. Even the flink:1.15 has aliyun-sdk-oss:3.4.1 only.  Please upgrade  aliyun-sdk-oss to 3.14.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 12:07:57 UTC 2022,,,,,,,,,,"0|z17ga0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Aug/22 09:20;martijnvisser;[~bilna123] Since you've worked on FLINK-28801 can you elaborate if this upgrade should (still) happen?;;;","30/Aug/22 10:05;bilna123;Thanks for the response [~martijnvisser].

I saw the changes in [#FLINK-28801] where aliyun-sdk-oss is upgraded to 3.13.2. The JDOM version in aliyun-sdk-oss:3.13.2 is 2.0.6. We were looking for jdom:2.0.6.1. Since it is minor version change, we don't need to upgrade.;;;","30/Aug/22 10:51;martijnvisser;[~bilna123] So we can close this ticket?;;;","30/Aug/22 12:07;bilna123;sure..;;;",,,,,,,,,,,,,,,,,
Hive source should also support verctor reading for complex data type when with parquet format,FLINK-28797,13475017,13444738,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Aug/22 06:56,01/Dec/22 03:15,04/Jun/24 20:41,09/Aug/22 16:04,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-24614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 16:04:49 UTC 2022,,,,,,,,,,"0|z17g8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 07:00;luoyuxia;Can move on after https://issues.apache.org/jira/browse/FLINK-24614;;;","09/Aug/22 16:04;jark;Fixed in master: 43eda5d3fe23de1aa95e73475dfb4792d36d4490 to 9237e63bd8a671cbf3bf133b89d6a037d198de1b;;;",,,,,,,,,,,,,,,,,,,
Add Statement Completement API for sql gateway rest endpoint,FLINK-28796,13475010,13500273,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,Wencong Liu,Wencong Liu,04/Aug/22 05:45,03/Apr/23 13:04,04/Jun/24 20:41,09/Jan/23 04:14,,,,,1.17.0,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,,,"SQL Gateway supports various clients: sql client, rest, hiveserver2, etc. Given the 1.16 feature freeze date, we won't be able to finish all the endpoints. Thus, we'd exclude one of the rest apis (tracked by this ticket) from [FLINK-28163] Introduce the statement related API for REST endpoint - ASF JIRA (apache.org)], which is only needed by the sql client, and still try to complete the remaining of them.

In other words, we'd expect the sql gateway to support rest & hiveserver2 apis in 1.16, and sql client in 1.17.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31711,,FLINK-29742,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 09 04:14:53 UTC 2023,,,,,,,,,,"0|z17g6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 04:14;fsk119;Merged into master: 6301cca7a6924cebd1107fcd39c063b7a091551d;;;",,,,,,,,,,,,,,,,,,,,
Unstable test for HiveDialectQueryITCase#testInsertDirectory,FLINK-28795,13474986,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Aug/22 04:23,15/Aug/22 07:06,04/Jun/24 20:41,15/Aug/22 07:06,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"The HiveDialectQueryITCase#testInsertDirectory is unstable. The failure link [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39233&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461]

we need to wait the insert to finish before select from the table",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 07:06:58 UTC 2022,,,,,,,,,,"0|z17g1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 01:52;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39278&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=25284;;;","05/Aug/22 02:11;fsk119;Merged into master:

5f620d29787abad2036a28373e80429026e29f21

 ;;;","15/Aug/22 01:53;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39941&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461
Different failed case HiveDialectQueryITCase.testBoolComparison, but the stack is looks like same:

{code:java}
2022-08-13T01:29:51.5076876Z Aug 13 01:29:51 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-08-13T01:29:51.5084747Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-08-13T01:29:51.5086194Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:174)
2022-08-13T01:29:51.5087528Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:140)
2022-08-13T01:29:51.5088973Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:141)
2022-08-13T01:29:51.5090646Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:101)
2022-08-13T01:29:51.5092431Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-08-13T01:29:51.5093827Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
2022-08-13T01:29:51.5095525Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-08-13T01:29:51.5096916Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-08-13T01:29:51.5098430Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:257)
2022-08-13T01:29:51.5099909Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-08-13T01:29:51.5101362Z Aug 13 01:29:51 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-08-13T01:29:51.5103082Z Aug 13 01:29:51 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:93)
2022-08-13T01:29:51.5104386Z Aug 13 01:29:51 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-08-13T01:29:51.5105516Z Aug 13 01:29:51 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-08-13T01:29:51.5106544Z Aug 13 01:29:51 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-08-13T01:29:51.5107651Z Aug 13 01:29:51 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-08-13T01:29:51.5108783Z Aug 13 01:29:51 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-08-13T01:29:51.5109907Z Aug 13 01:29:51 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-08-13T01:29:51.5111008Z Aug 13 01:29:51 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-08-13T01:29:51.5112618Z Aug 13 01:29:51 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-08-13T01:29:51.5113726Z Aug 13 01:29:51 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-08-13T01:29:51.5114888Z Aug 13 01:29:51 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-08-13T01:29:51.5116119Z Aug 13 01:29:51 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:92)
2022-08-13T01:29:51.5117429Z Aug 13 01:29:51 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:197)
2022-08-13T01:29:51.5118842Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1730)
2022-08-13T01:29:51.5120271Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:865)
2022-08-13T01:29:51.5122896Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1375)
2022-08-13T01:29:51.5124442Z Aug 13 01:29:51 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:733)
2022-08-13T01:29:51.5125932Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveDialectQueryITCase.testBoolComparison(HiveDialectQueryITCase.java:737)
2022-08-13T01:29:51.5127189Z Aug 13 01:29:51 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-13T01:29:51.5128360Z Aug 13 01:29:51 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-13T01:29:51.5129674Z Aug 13 01:29:51 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-13T01:29:51.5130773Z Aug 13 01:29:51 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-13T01:29:51.5132093Z Aug 13 01:29:51 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-13T01:29:51.5133411Z Aug 13 01:29:51 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-13T01:29:51.5134642Z Aug 13 01:29:51 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-13T01:29:51.5135847Z Aug 13 01:29:51 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-13T01:29:51.5137276Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-13T01:29:51.5138524Z Aug 13 01:29:51 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-13T01:29:51.5139886Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-13T01:29:51.5141112Z Aug 13 01:29:51 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-13T01:29:51.5142586Z Aug 13 01:29:51 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-13T01:29:51.5143709Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-13T01:29:51.5144859Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-13T01:29:51.5145912Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-13T01:29:51.5147026Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-13T01:29:51.5148157Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-13T01:29:51.5149260Z Aug 13 01:29:51 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-13T01:29:51.5150528Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-13T01:29:51.5151516Z Aug 13 01:29:51 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-13T01:29:51.5152655Z Aug 13 01:29:51 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-13T01:29:51.5153642Z Aug 13 01:29:51 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-13T01:29:51.5154710Z Aug 13 01:29:51 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-13T01:29:51.5155983Z Aug 13 01:29:51 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-13T01:29:51.5157176Z Aug 13 01:29:51 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-13T01:29:51.5158402Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-13T01:29:51.5159780Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-13T01:29:51.5161226Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-13T01:29:51.5162982Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-13T01:29:51.5164495Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-13T01:29:51.5165816Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-13T01:29:51.5167072Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-13T01:29:51.5168449Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-13T01:29:51.5169857Z Aug 13 01:29:51 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-13T01:29:51.5171273Z Aug 13 01:29:51 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-13T01:29:51.5174304Z Aug 13 01:29:51 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-13T01:29:51.5175677Z Aug 13 01:29:51 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-13T01:29:51.5176937Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-13T01:29:51.5178382Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-13T01:29:51.5179568Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-13T01:29:51.5180846Z Aug 13 01:29:51 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-13T01:29:51.5181990Z Aug 13 01:29:51 Caused by: java.io.IOException: Fail to create input splits.
2022-08-13T01:29:51.5183187Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-08-13T01:29:51.5184564Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:91)
2022-08-13T01:29:51.5185825Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:179)
2022-08-13T01:29:51.5187083Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-08-13T01:29:51.5188427Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-08-13T01:29:51.5189395Z Aug 13 01:29:51 	... 71 more
2022-08-13T01:29:51.5192250Z Aug 13 01:29:51 Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error while running command to get file permissions : ExitCodeException exitCode=2: /bin/ls: cannot access '/tmp/junit2768738167464506810/hive_warehouse/tbool/.staging_1660353747721': No such file or directory
2022-08-13T01:29:51.5193692Z Aug 13 01:29:51 
2022-08-13T01:29:51.5194437Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:585)
2022-08-13T01:29:51.5195378Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.run(Shell.java:482)
2022-08-13T01:29:51.5196379Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
2022-08-13T01:29:51.5197392Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
2022-08-13T01:29:51.5198326Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
2022-08-13T01:29:51.5199250Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)
2022-08-13T01:29:51.5200401Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)
2022-08-13T01:29:51.5201738Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)
2022-08-13T01:29:51.5203113Z Aug 13 01:29:51 	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)
2022-08-13T01:29:51.5204108Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1732)
2022-08-13T01:29:51.5205029Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1712)
2022-08-13T01:29:51.5206102Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)
2022-08-13T01:29:51.5207168Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
2022-08-13T01:29:51.5208228Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
2022-08-13T01:29:51.5209334Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-08-13T01:29:51.5210467Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-08-13T01:29:51.5211483Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-13T01:29:51.5212737Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-13T01:29:51.5213768Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-13T01:29:51.5214947Z Aug 13 01:29:51 	at java.lang.Thread.run(Thread.java:748)
2022-08-13T01:29:51.5215549Z Aug 13 01:29:51 
2022-08-13T01:29:51.5216245Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-08-13T01:29:51.5217136Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-08-13T01:29:51.5218306Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-08-13T01:29:51.5219143Z Aug 13 01:29:51 	... 75 more
2022-08-13T01:29:51.5221273Z Aug 13 01:29:51 Caused by: java.lang.RuntimeException: Error while running command to get file permissions : ExitCodeException exitCode=2: /bin/ls: cannot access '/tmp/junit2768738167464506810/hive_warehouse/tbool/.staging_1660353747721': No such file or directory
2022-08-13T01:29:51.5222712Z Aug 13 01:29:51 
2022-08-13T01:29:51.5223401Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:585)
2022-08-13T01:29:51.5224297Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.run(Shell.java:482)
2022-08-13T01:29:51.5225223Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)
2022-08-13T01:29:51.5226163Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:869)
2022-08-13T01:29:51.5227017Z Aug 13 01:29:51 	at org.apache.hadoop.util.Shell.execCommand(Shell.java:852)
2022-08-13T01:29:51.5227854Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:1097)
2022-08-13T01:29:51.5228897Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:659)
2022-08-13T01:29:51.5230278Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)
2022-08-13T01:29:51.5231349Z Aug 13 01:29:51 	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)
2022-08-13T01:29:51.5232455Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1732)
2022-08-13T01:29:51.5233314Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1712)
2022-08-13T01:29:51.5234312Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)
2022-08-13T01:29:51.5235451Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
2022-08-13T01:29:51.5236537Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
2022-08-13T01:29:51.5237636Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-08-13T01:29:51.5238665Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-08-13T01:29:51.5239613Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-13T01:29:51.5240709Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-13T01:29:51.5242167Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-13T01:29:51.5243205Z Aug 13 01:29:51 	at java.lang.Thread.run(Thread.java:748)
2022-08-13T01:29:51.5243905Z Aug 13 01:29:51 
2022-08-13T01:29:51.5244958Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:699)
2022-08-13T01:29:51.5246461Z Aug 13 01:29:51 	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:634)
2022-08-13T01:29:51.5247749Z Aug 13 01:29:51 	at org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:49)
2022-08-13T01:29:51.5248814Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1732)
2022-08-13T01:29:51.5249773Z Aug 13 01:29:51 	at org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:1712)
2022-08-13T01:29:51.5251083Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:270)
2022-08-13T01:29:51.5252423Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
2022-08-13T01:29:51.5253513Z Aug 13 01:29:51 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
2022-08-13T01:29:51.5254822Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-08-13T01:29:51.5256129Z Aug 13 01:29:51 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-08-13T01:29:51.5257367Z Aug 13 01:29:51 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-08-13T01:29:51.5258501Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-08-13T01:29:51.5259618Z Aug 13 01:29:51 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-08-13T01:29:51.5260578Z Aug 13 01:29:51 	at java.lang.Thread.run(Thread.java:748)
2022-08-13T01:29:51.5261222Z Aug 13 01:29:51 
2022-08-13T01:29:53.8083054Z Aug 13 01:29:53 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-08-13T01:29:55.9454104Z Aug 13 01:29:55 [INFO] Running org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase
2022-08-13T01:30:39.9002693Z Aug 13 01:30:39 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 43.859 s - in org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReaderITCase
2022-08-13T01:34:21.8856413Z Aug 13 01:34:21 == Abstract Syntax Tree ==
2022-08-13T01:34:21.8860211Z Aug 13 01:34:21 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
2022-08-13T01:34:21.8862493Z Aug 13 01:34:21 +- LogicalProject(a=[$0], b=[$1], c=[$2], p=[$3], x=[$4], y=[$5])
2022-08-13T01:34:21.8864021Z Aug 13 01:34:21    +- LogicalFilter(condition=[AND(=($4, $3), =($6, 1))])
2022-08-13T01:34:21.8865411Z Aug 13 01:34:21       +- LogicalJoin(condition=[true], joinType=[inner])
2022-08-13T01:34:21.8866858Z Aug 13 01:34:21          :- LogicalTableScan(table=[[test-catalog, default, fact]])
2022-08-13T01:34:21.8868511Z Aug 13 01:34:21          +- LogicalTableScan(table=[[test-catalog, default, dim]])
2022-08-13T01:34:21.8869569Z Aug 13 01:34:21 
2022-08-13T01:34:21.8870398Z Aug 13 01:34:21 == Optimized Physical Plan ==
2022-08-13T01:34:21.8871284Z Aug 13 01:34:21 Sort(orderBy=[a ASC])
2022-08-13T01:34:21.8872787Z Aug 13 01:34:21 +- Exchange(distribution=[single])
2022-08-13T01:34:21.8874290Z Aug 13 01:34:21    +- HashJoin(joinType=[InnerJoin], where=[=(x, p)], select=[a, b, c, p, x, y], build=[right])
2022-08-13T01:34:21.8875683Z Aug 13 01:34:21       :- Exchange(distribution=[hash[p]])
2022-08-13T01:34:21.8877169Z Aug 13 01:34:21       :  +- DynamicFilteringTableSourceScan(table=[[test-catalog, default, fact]], fields=[a, b, c, p])
2022-08-13T01:34:21.8878630Z Aug 13 01:34:21       :     +- DynamicFilteringDataCollector(fields=[x])
2022-08-13T01:34:21.8879936Z Aug 13 01:34:21       :        +- Calc(select=[x, y], where=[=(z, 1)])
2022-08-13T01:34:21.8881333Z Aug 13 01:34:21       :           +- TableSourceScan(table=[[test-catalog, default, dim]], fields=[x, y, z])
2022-08-13T01:34:21.8883005Z Aug 13 01:34:21       +- Exchange(distribution=[hash[x]])
2022-08-13T01:34:21.8885064Z Aug 13 01:34:21          +- Calc(select=[x, y], where=[=(z, 1)])
2022-08-13T01:34:21.8886532Z Aug 13 01:34:21             +- TableSourceScan(table=[[test-catalog, default, dim]], fields=[x, y, z])
2022-08-13T01:34:21.8887394Z Aug 13 01:34:21 
2022-08-13T01:34:21.8888901Z Aug 13 01:34:21 == Optimized Execution Plan ==
2022-08-13T01:34:21.8889775Z Aug 13 01:34:21 Sort(orderBy=[a ASC])
2022-08-13T01:34:21.8891058Z Aug 13 01:34:21 +- Exchange(distribution=[single])
2022-08-13T01:34:21.8892592Z Aug 13 01:34:21    +- HashJoin(joinType=[InnerJoin], where=[(x = p)], select=[a, b, c, p, x, y], build=[right])
2022-08-13T01:34:21.8893794Z Aug 13 01:34:21       :- Exchange(distribution=[hash[p]])
2022-08-13T01:34:21.8895346Z Aug 13 01:34:21       :  +- DynamicFilteringTableSourceScan(table=[[test-catalog, default, fact]], fields=[a, b, c, p])
2022-08-13T01:34:21.8896728Z Aug 13 01:34:21       :     +- DynamicFilteringDataCollector(fields=[x])
2022-08-13T01:34:21.8897969Z Aug 13 01:34:21       :        +- Calc(select=[x, y], where=[(z = 1)])(reuse_id=[1])
2022-08-13T01:34:21.8899462Z Aug 13 01:34:21       :           +- TableSourceScan(table=[[test-catalog, default, dim]], fields=[x, y, z])
2022-08-13T01:34:21.8900691Z Aug 13 01:34:21       +- Exchange(distribution=[hash[x]])
2022-08-13T01:34:21.8901761Z Aug 13 01:34:21          +- Reused(reference_id=[1])
{code}
;;;","15/Aug/22 07:06;hxbks2ks;Merged into master via 662f9de97ffadb7c178a94d556ffebfec7d97817;;;",,,,,,,,,,,,,,,,,
Publish flink-table-store snapshot artifacts,FLINK-28794,13474979,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Aug/22 03:13,11/Aug/22 04:44,04/Jun/24 20:41,11/Aug/22 04:44,,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"It is better to publish the Maven artifacts, so that downstream Java projects can use this.
See FLINK-26639",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 04:44:24 UTC 2022,,,,,,,,,,"0|z17g00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 04:44;lzljs3620320;master: 49491377b93223e914f5656d8d2c2f7ade7999bb;;;",,,,,,,,,,,,,,,,,,,,
Allow to GetInfo for HiveServer2 Endpoint,FLINK-28793,13474975,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ZhaoWeiNan,fsk119,fsk119,04/Aug/22 02:25,08/Aug/22 12:14,04/Jun/24 20:41,08/Aug/22 12:14,,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 12:14:27 UTC 2022,,,,,,,,,,"0|z17fz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 02:39;ZhaoWeiNan;hi, [~fsk119]. please assign to me . thanks.;;;","08/Aug/22 12:14;fsk119;Merged into master: ac0bbe608f56fc9514191e7a9da6dc76d54047b6;;;",,,,,,,,,,,,,,,,,,,
Upgrade to JOSDK 3.1.0,FLINK-28792,13474892,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,03/Aug/22 12:56,05/Aug/22 07:42,04/Jun/24 20:41,05/Aug/22 07:42,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 07:42:17 UTC 2022,,,,,,,,,,"0|z17fh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 07:42;gyfora;merged to main 4eed42b6e70268b2fb3eaf94bbcff0a16efebb3f;;;",,,,,,,,,,,,,,,,,,,,
flink-sql-gateway-test does not compile on Java 11,FLINK-28791,13474875,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,chesnay,chesnay,03/Aug/22 11:35,03/Aug/22 13:21,04/Jun/24 20:41,03/Aug/22 13:21,1.16.0,,,,1.16.0,,,,Table SQL / Gateway,Tests,,,,,,0,pull-request-available,,,,,,{{Could not find artifact jdk.tools:jdk.tools:jar:1.7}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27770,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 13:21:51 UTC 2022,,,,,,,,,,"0|z17fdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 13:21;chesnay;master: 9efd97e05717181ee9b5489cddc27b6351127e38;;;",,,,,,,,,,,,,,,,,,,,
Incorrect KafkaProducer metrics initialization,FLINK-28790,13474866,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,syamka256,syamka256,03/Aug/22 11:06,31/Oct/22 17:38,04/Jun/24 20:41,,1.14.4,1.15.1,,,,,,,Connectors / Kafka,,,,,,,0,,,,,,,"Problem

KafkaProducer Flink metrics have unpredictable behavior because of concurrent initialization of broker's and topic's metrics.

Reproducing

Firstly we found the problem with our Flink cluster: metric KafkaProducer.outgoing-byte-rate was periodically missing (was equals zero or near zero) on several subtasks, in the same time other subtasks was fine with this metric. Actual outgoing rate was the same on different subtasks, it was clear from, for example, KafkaProducer.records-send-rate metric, which was ok on every subtask, problem 100% was with metric itself.

After long investigation we found the root-cause of this behavior:
 
* KafkaWriter creates an instance of FlinkKafkaInternalProducer and then [initializes|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L327-L330] metric wrappers over existing KafkaProducer metrics (gauges)
* KafkaProducer itself in the constructor [creates Sender|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L458-L460] to access brokers, starts a thread (kafka-producer-network-thread) and run Sender in this separate thread
* After starting the Sender, metrics connected with topics and brokers register for some time. If they register quickly, KafkaWriter will see them before the end of initialization and these metrics will be wrapped as flink gauges. Otherwise, they will not.
* [Some KafkaProducer metrics|https://docs.confluent.io/platform/current/kafka/monitoring.html#producer-metrics] from producer and from broker has same names - for example, outgoing-byte-rate
* In case if two metrics has same name, Flink KafkaWriter [rewrites|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L359-L360] metric in wrapper

So, to reproduce this bug it's enough to run any job with Kafka Sink and to look at the KafkaProducer metrics, some of them will be absent (broker's or topic's) or some of them will be rewritten (like outgoing-byte-rate in the example).

I suppose there is at least two ways to fix it:
1. Add tag (producer-metric, producer-node-metric, etc.) to Flinks metrics name
2. Use only metrics with tag=producer-metrics, ignore any another tags - without considering broker's and topic's metrics

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 31 17:38:21 UTC 2022,,,,,,,,,,"0|z17fbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 11:08;syamka256;Is there any workarounds to prevent missing producer metrics ? I can use my own patched version of flink-kafka-connector, but I actually don't want :);;;","31/Oct/22 07:17;rmetzger;Thanks for reporting this issue. 
Correct me if I'm wrong, but my gut feeling is that fix 1. is nicer, as it exposes all metrics, however, it will be a breaking change in the next Flink release, because some metrics will have a new name? (or am I wrong because these are just additional tags?)

;;;","31/Oct/22 08:14;syamka256;You're right, I don't think there is some silver bullet solution.
Look: kafka has different types of metrics - producer-metrics and producer-node-metrics. Flink has metrics with only prefix KafkaProducer.\{metric_name} for all metrics of every kafka type.

So, fix 1 is to change flink metric prefix to something like KafkaProducer.producer-metrics.\{metric_name} and KafkaProducer.producer-node-metrics.\{metric_name} - this approach has no backward compability for sure (

Fix 2 is to totally ignore metrics with type=producer-node-metrics - I think this solution is good for most of users, but this will drop information about brokers metrics if somebody use it.;;;","31/Oct/22 08:27;rmetzger;[~chesnay] Do you have any advise here?
I don't think we can break existing metric names for the Kafka connector.

Other ideas:
- Introduce a config parameter to switch the behavior
- add the `producer-metrics` and `producer-node-metrics` prefixes in addition to the current behavior: this will introduce more and duplicate metrics though.;;;","31/Oct/22 17:38;sharonxr55;I would add the `producer-metrics` and `producer-node-metrics` prefixes to be consistent with Kafka, and make the existing `KafkaProducer.{metric_name}` to use `tag=producer-metrics` and mark them deprecated. 

If we really don't want to introduce more and duplicate metrics, we could just introduce the `producer-node-metrics` prefix, and map `producer-metrics` to `KafkaProducer.{metric_name}`. ;;;",,,,,,,,,,,,,,,,
 TPC-DS tests failed  due to release input gate for task failure,FLINK-28789,13474856,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanyuxin,leonard,leonard,03/Aug/22 09:40,08/Aug/22 03:16,04/Jun/24 20:41,08/Aug/22 03:16,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,test-stability,,,,,,"

{code:java}
switched from CANCELING to CANCELED.
2022-08-03 08:03:02,776 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for MultipleInput[2212] -> Calc[2191] -> HashAggregate[2192] (8/8)#1 (cf5f33b100f0efb21b9ff8d27a78cd8e_d806bb3f5ea308ac3f1df304a96163b4_7_1).
2022-08-03 08:03:02,776 ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Failed to release input gate for task MultipleInput[2212] -> Calc[2191] -> HashAggregate[2192] (8/8)#1.
org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.toLiveRealRefCnt(ReferenceCountUpdater.java:74) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.release(ReferenceCountUpdater.java:138) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.ReadOnlySlicedNetworkBuffer.recycleBuffer(ReadOnlySlicedNetworkBuffer.java:123) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.buffer.CompositeBuffer.recycleBuffer(CompositeBuffer.java:70) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_332]
	at org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.releaseInternal(SortMergeSubpartitionReader.java:181) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.SortMergeSubpartitionReader.releaseAllResources(SortMergeSubpartitionReader.java:163) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.releaseAllResources(LocalInputChannel.java:341) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:667) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:140) ~[flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.closeAllInputGates(Task.java:1010) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.releaseResources(Task.java:975) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:820) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) [flink-dist-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332]
2022-08-03 08:03:02,778 WARN  org.apache.flink.metrics.MetricGroup     
{code}


The failed CI link: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39152&view=results",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28373,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 03:15:47 UTC 2022,,,,,,,,,,"0|z17f94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 11:30;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39176&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Aug/22 11:31;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39179&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Aug/22 11:31;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39185&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","03/Aug/22 11:50;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39192&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","04/Aug/22 06:03;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39200&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39202&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39205&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39209&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=11492
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39220&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=11684
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=f8e16326-dc75-5ba0-3e95-6178dd55bf6c&t=15c1d318-5ca8-529f-77a2-d113a700ec34
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=fb37c667-81b7-5c22-dd91-846535e99a97&t=39a035c3-c65e-573c-fb66-104c66c28912;;;","04/Aug/22 08:27;fsk119;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39233&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=860bfb5d-81b0-5968-f128-2a8b5362110d;;;","04/Aug/22 08:36;luoyuxia;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39212&view=logs&s=7d4e458d-e0e0-5f89-c72d-7371ef61b09b&j=fce28b06-a2d8-5021-adb0-154438f48362];;;","04/Aug/22 16:46;kevin.cyj;Though still not know the root cause, by reverting FLINK-28373 and testing multiple times on my own azure account, the issue seems resolved. For CI stability, I am reverting FLINK-28373, let's see if that solves the problem.;;;","06/Aug/22 15:33;jark;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39439&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a;;;","08/Aug/22 02:57;tanyuxin;The commit has been reverted on Aug 05, and the issue has been resolved.;;;","08/Aug/22 03:15;kevin.cyj;Closing this issue, feel free to reopen it if the problem still exists.;;;",,,,,,,,,,
Support SideOutput in Thread Mode,FLINK-28788,13474854,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,hxb,hxb,03/Aug/22 09:37,08/Aug/22 12:59,04/Jun/24 20:41,08/Aug/22 12:59,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-25724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 12:59:34 UTC 2022,,,,,,,,,,"0|z17f8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 12:59;hxbks2ks;Merged into master via 7047916a487ed68c8e01ad005384b787501345cb;;;",,,,,,,,,,,,,,,,,,,,
rename getUniqueKeys to getUpsertKeys in CommonPhysicalJoin,FLINK-28787,13474852,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,godfreyhe,godfreyhe,03/Aug/22 09:35,09/Mar/23 09:46,04/Jun/24 20:41,09/Sep/22 02:48,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 09 09:44:14 UTC 2023,,,,,,,,,,"0|z17f88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Sep/22 02:48;godfreyhe;Fixed in master: 347316ea6394b24c4471aa8616f2632e126f733d
in 1.16: 77574b7e1ee5f514c32304c071732a11050baede;;;","09/Mar/23 09:44;twalthr;[~godfreyhe] Gently reminder that we need to be very careful with changes to compiled plans. So far this feature is still experimental, but we need to constrain ourselves to not break compatibility. This change silently broke compatibility and should have bumped up the ExecNode version.;;;",,,,,,,,,,,,,,,,,,,
Cannot run PyFlink 1.16 on MacOS with M1 chip,FLINK-28786,13474822,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,taoran,taoran,03/Aug/22 06:48,05/May/23 02:26,04/Jun/24 20:41,27/Apr/23 10:04,1.16.0,,,,1.16.2,1.17.1,,,API / Python,,,,,,,0,pull-request-available,,,,,,"I have tested it with 2 m1 machines. i will reproduce the bug 100%.

1.m1 machine
macos bigsur 11.5.1 & jdk8 * & jdk11 & python 3.8 & python 3.9
1.m1 machine
macos monterey 12.1 & jdk8 * & jdk11 & python 3.8 & python 3.9

reproduce step:
1.python -m pip install -r flink-python/dev/dev-requirements.txt
2.cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..;
3.python -m pip install apache-flink-libraries/dist/*.tar.gz
4.python -m pip install dist/*.whl

when run [word_count.py|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/table_api_tutorial/] it will cause


{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}


",,,,,,,,,,,,,,,,,,,,FLINK-29796,,,,,,FLINK-31968,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 27 03:55:10 UTC 2023,,,,,,,,,,"0|z17f1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 07:15;ana4;I can't reproduce this problem in macOS Monterey 12.4 & M1Chip & JDK8 & Python 3.9.;;;","23/Sep/22 08:20;dianfu;[~lemonjing] Hi, could you provide more information? Otherwise, I'm tending to close this issue as ""Cannot Produced"";;;","26/Sep/22 07:07;grimsby;Hi,

I've got this same issue, but thats because I'm using protobuf >3.18 (3.19.5). The reason is this:
{code:java}
// pyflink/fn_execution/flink_fn_execution_pb2.py
...
DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'...')
... {code}
DESCRIPTOR will always be None, since AddSerializedFile in protbuf <3.18 will never return anything:
{code:java}
def AddSerializedFile(self, serialized_file_desc_proto):
    """"""Adds the FileDescriptorProto and its types to this pool.
    Args:
      serialized_file_desc_proto (bytes): A bytes string, serialization of the
        :class:`FileDescriptorProto` to add.
    """"""

    # pylint: disable=g-import-not-at-top
    from google.protobuf import descriptor_pb2
    file_desc_proto = descriptor_pb2.FileDescriptorProto.FromString(
        serialized_file_desc_proto)
    self.Add(file_desc_proto) {code}
Am I missing something, or has pull request 20685 introduced this bug?

{{}};;;","26/Sep/22 09:13;grimsby;This is due to packages from pip is used instead of conda. Adding the dev-requirements.txt from conda instead of pip will resolce this issue.;;;","12/Oct/22 03:39;taoran;[~grimsby] Yes, you are right. when i change to conda, it works well.  thanks.;;;","18/Nov/22 07:58;hxbks2ks;Merged into master via e5762a558f3697294cd73da4247a741fc6f73456
Merged into release-1.16 via 4f0ffa0ddfeffbe25435595d08825cada713ac44;;;","07/Feb/23 09:09;hxbks2ks;If you encounter the following stack trace in m1

{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}

you can execute the following command to solve the problem

{code:java}
pip install cython==0.29.24
brew install gcc
pip uninstall apache-flink
pip uninstall apache-beam
pip install apache-flink
{code}

;;;","14/Feb/23 11:32;hxbks2ks;Merged into master via 1e8d528e293da7ab990f1406fcd209ff4fb177b3
Merged into release-1.17 via 556a3faae9f147edf1c263e57a6661fc88b8f7ab
Merged into release-1.16 via 1e1461945d633486ee41f7137743837d0756e9b6;;;","26/Apr/23 14:40;dianfu;Reopen it as one user reported an issue related to Mac M1 on 1.17.0 (https://apache-flink.slack.com/archives/C03G7LJTS2G/p1679904702297129):

{code}
Traceback (most recent call last):
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 287, in _execute
    response = task()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 360, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 597, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 634, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1004, in process_bundle
    element.data)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 226, in process_encoded
    input_stream, True)
  File ""apache_beam/coders/coder_impl.py"", line 1519, in apache_beam.coders.coder_impl.ParamWindowedValueCoderImpl.decode_from_stream
  File ""apache_beam/coders/coder_impl.py"", line 1520, in apache_beam.coders.coder_impl.ParamWindowedValueCoderImpl.decode_from_stream
  File ""apache_beam/coders/coder_impl.py"", line 135, in apache_beam.coders.coder_impl.CoderImpl.decode_from_stream
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 34, in decode_from_stream
    return self._value_coder.decode_from_stream(in_stream, nested)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/fn_execution/beam/beam_coder_impl_slow.py"", line 58, in decode_from_stream
    return self._value_coder.decode_from_stream(data_input_stream)
TypeError: Argument 'input_stream' has incorrect type (expected pyflink.fn_execution.stream_fast.LengthPrefixInputStream, got BeamInputStream)
{code};;;","27/Apr/23 03:55;dianfu;Fixed in:
 - master via a3368635e3d06f764d144f8c8e2e06e499e79665
 - release-1.17 via 52c9742eed7128284278b07c40785bf1c4e30139
 - release-1.16 via ae998dda4ee60e2268a8c4f8bfdbbd46cc1a0746;;;",,,,,,,,,,,
Hybrid shuffle consumer thread and upstream thread may have dead lock ,FLINK-28785,13474817,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,03/Aug/22 06:40,09/Aug/22 08:34,04/Jun/24 20:41,09/Aug/22 08:34,1.16.0,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"In hybrid shuffle mode, subpartition view lock will be acquired by consumer thread, and further wait the read lock of MemoryDataManager. But MemoryDataManager may acquire write lock to make a global spilling decision, and then wait subpartition view lock to get consuming offset. In this case, deadlock will occurs.

consumer thread : acqurie subpartition lock -> wait read lock.

upstream thread  : acquire write lock -> wait subpartition lock.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 08:34:09 UTC 2022,,,,,,,,,,"0|z17f0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 08:34;xtsong;master (1.16): b442394c65b1e924c4b5710a9453e3c7eacf05b5;;;",,,,,,,,,,,,,,,,,,,,
ParameterTool support read and parse program arguments from yml file,FLINK-28784,13474800,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Won't Do,,Aiden Gong,Aiden Gong,03/Aug/22 03:34,27/Oct/22 07:47,04/Jun/24 20:41,27/Oct/22 07:47,1.15.1,,,,,,,,API / Core,,,,,,,0,,,,,,,ParameterTool support read and parse program arguments from different sources. But don't support `yml` file. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 07:47:15 UTC 2022,,,,,,,,,,"0|z17ewo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 03:48;Aiden Gong;[~Leonard] What do you think about this ? If need , I'm willing to open a pr for this.;;;","03/Aug/22 08:17;chesnay;I would be against this. Arguably Flink should have never offered a tool to parse command-line arguments in the first place and should have left that to other dedicated projects.;;;","27/Oct/22 07:47;zhuzh;Close this ticket because it is inactive.;;;",,,,,,,,,,,,,,,,,,
Class ConfigOptions's example code have error.,FLINK-28783,13474786,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Aiden Gong,Aiden Gong,Aiden Gong,03/Aug/22 02:19,11/Aug/22 08:20,04/Jun/24 20:41,11/Aug/22 08:20,1.15.1,,,,1.16.0,,,,API / Core,,,,,,,0,pull-request-available,,,,,,!image-2022-08-03-10-18-54-965.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/22 02:18;Aiden Gong;image-2022-08-03-10-18-54-965.png;https://issues.apache.org/jira/secure/attachment/13047566/image-2022-08-03-10-18-54-965.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 08:20:18 UTC 2022,,,,,,,,,,"0|z17etk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 08:20;leonard;Fixed in master(1.16): 3143526637ef772b43ab413e1cf0a63e566e6dab;;;",,,,,,,,,,,,,,,,,,,,
Support FileSink compaction in PyFlink,FLINK-28782,13474784,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,03/Aug/22 02:02,07/Aug/22 13:31,04/Jun/24 20:41,07/Aug/22 13:31,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 13:31:47 UTC 2022,,,,,,,,,,"0|z17et4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 13:31;dianfu;Merged to master via b6cc5b24c0e23f08d4191e2c6bae5d8af8729c22;;;",,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle should support compression,FLINK-28781,13474734,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,02/Aug/22 16:25,12/Aug/22 12:29,04/Jun/24 20:41,05/Aug/22 05:05,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Compression is a useful feature for batch jobs, which can significantly reduce disk load and the amount of data transferred over the network. Hybrid shuffle should also support the compression of spilled data, especially under the full spilling strategy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28931,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 05:05:07 UTC 2022,,,,,,,,,,"0|z17ei0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 05:05;xtsong;master (1.16): 2edc43c6e469a3e16ee01a1373cf52523eb96b01;;;",,,,,,,,,,,,,,,,,,,,
function docs of dayofmonth is not correct,FLINK-28780,13474694,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,02/Aug/22 12:32,17/Aug/22 09:06,04/Jun/24 20:41,17/Aug/22 09:06,1.16.0,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,!image-2022-08-02-20-32-22-309.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/22 12:32;jackylau;image-2022-08-02-20-32-22-309.png;https://issues.apache.org/jira/secure/attachment/13047549/image-2022-08-02-20-32-22-309.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 09:06:02 UTC 2022,,,,,,,,,,"0|z17e94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Aug/22 09:06;jark;FIxed in master: 59843ae275ceb64d8429aea1426c829e1d562c26;;;",,,,,,,,,,,,,,,,,,,,
Support Retryable Lookup Join To Solve Delayed Updates Issue In External Systems,FLINK-28779,13474692,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln.86xy,lincoln.86xy,02/Aug/22 12:29,13/Sep/22 13:40,04/Jun/24 20:41,02/Sep/22 08:07,,,,,1.16.0,,,,Table SQL / Planner,Table SQL / Runtime,,,,,,0,,,,,,,"FLIP-234: Support Retryable Lookup Join To Solve Delayed Updates Issue In External Systems

https://cwiki.apache.org/confluence/display/FLINK/FLIP-234%3A+Support+Retryable+Lookup+Join+To+Solve+Delayed+Updates+Issue+In+External+Systems",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-02 12:29:27.0,,,,,,,,,,"0|z17e8o:",9223372036854775807,Adds retryable lookup join to support both async and sync lookups in order to solve the delayed updates issue in external systems.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bulk fetch of table and column statistics for given partitions,FLINK-28778,13474681,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,02/Aug/22 10:51,22/Nov/22 02:18,04/Jun/24 20:41,09/Aug/22 15:57,1.15.1,,,,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25605,FLINK-19103,,,,,,,FLINK-28306,FLINK-27597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 15:57:16 UTC 2022,,,,,,,,,,"0|z17e68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 15:57;godfreyhe;Fixed in master: 6fb49b1d708137605e4cd90175be78389290cea5;;;",,,,,,,,,,,,,,,,,,,,
Add configure session API for sql gateway rest endpoint,FLINK-28777,13474671,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Wencong Liu,Wencong Liu,02/Aug/22 10:07,11/Mar/24 12:43,04/Jun/24 20:41,,,,,,1.20.0,,,,Table SQL / Gateway,,,,,,,0,,,,,,,"In the development of version 1.16, we will temporarily skip the development of configure session api in sql gateway rest endpoint. Considering the workload and sql gateway temporarily does not need to be compatible with sql client, so the relevant development work will be carried out in the development work of version 1.17.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 11:55:13 UTC 2022,,,,,,,,,,"0|z17e40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 11:02;martijnvisser;What is the user impact of this ticket? Why doesn't the SQL Gateway need to be compatible with the SQL client according to you? We should make sure that we still follow the time-based releases plan from https://cwiki.apache.org/confluence/display/FLINK/Time-based+releases, specifically on the part of 'what happens if features don't complete' ;;;","02/Aug/22 11:14;xtsong;[~martijnvisser],

I think this is exactly an outcome of following the time-based release plan.

SQL Gateway supports various clients: sql client, rest, hiveserver2, etc. Given the 1.16 feature freeze date, we won't be able to finish all the endpoints. Thus, we'd exclude one of the rest apis (tracked by this ticket) from FLINK-28161, which is only needed by the sql client, and still try to complete the remaining of them.

In other words, we'd expect the sql gateway to support rest & hiveserver2 apis in 1.16, and sql client in 1.17.;;;","02/Aug/22 11:55;martijnvisser;[~xtsong] That's exactly the clarification I was looking for, thanks. The ""not compatible"" implicated for me that SQL Client would not be usable anymore. ;;;",,,,,,,,,,,,,,,,,,
RowTimeMiniBatchAssginerOperator doesn't need separate chain with upstream WatermarkAssignerOperator,FLINK-28776,13474651,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,02/Aug/22 08:17,03/Aug/22 12:36,04/Jun/24 20:41,03/Aug/22 12:36,1.16.0,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 12:36:13 UTC 2022,,,,,,,,,,"0|z17dzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 12:36;jark;Fixed in master: 2a7fe42d5069a7f9d0d7c531011aaf622893064e;;;",,,,,,,,,,,,,,,,,,,,
Unclean shade in flink-table-store-dist,FLINK-28775,13474643,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,02/Aug/22 07:26,02/Aug/22 10:49,04/Jun/24 20:41,02/Aug/22 10:49,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"java.lang.NoSuchFieldError: callback
  at org.apache.flink.table.store.kafka.KafkaSinkFunction.open (KafkaSinkFunction.java:71)

Error when using table store with kafka sql-jar.

We should shade more for flink-connector-kafka.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 10:49:53 UTC 2022,,,,,,,,,,"0|z17dxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 10:49;lzljs3620320;master: dfaa44cbc1b97ef00cb653569e60ae20280fd0ac
release-0.2: 5862320788f323ca1ae46c2fa6ab6cb4e4a433e7;;;",,,,,,,,,,,,,,,,,,,,
Allow user to configure whether to enable sort or not when it's for dynamic parition writing for HiveSource,FLINK-28774,13474642,13421719,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,02/Aug/22 07:16,10/Aug/22 05:26,04/Jun/24 20:41,10/Aug/22 05:26,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"In HiveSource, when it's for inserting into dynamic parition, it'll always add a sort node which may be time consuming.

It'll be better to allow users  to configure whether to add a sort or not in the case for inserting into dynamic parition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 05:26:43 UTC 2022,,,,,,,,,,"0|z17dxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 05:26;jark;Fixed in master: 22dba69f16d69e8235ec873d90e66af0558b120d
;;;",,,,,,,,,,,,,,,,,,,,
Fix Hive sink not write a success file after finish writing in batch mode,FLINK-28773,13474640,13421719,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,luoyuxia,luoyuxia,02/Aug/22 07:07,13/Apr/23 02:34,04/Jun/24 20:41,16/Aug/22 04:09,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"Currently, in stream mode, we allow user to configure commit policy, but in batch mode, we don't provide such way and it will always commit to metastore. 

But user expect other commit policy such like write a success file in batch mode. So, it'll be better to support write a success file after finish writing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31790,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 04:09:08 UTC 2022,,,,,,,,,,"0|z17dx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/22 04:09;jark;Fixed in master: 00cded670753e2795c774c159facb801bbeb9927 to b66680bb658dfb550e60a5e7440ee904c87219c6;;;",,,,,,,,,,,,,,,,,,,,
Hive dialect supports add jar ,FLINK-28772,13474637,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,02/Aug/22 06:48,06/Aug/22 15:35,04/Jun/24 20:41,06/Aug/22 15:35,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"Flink dialect provides AddJarOperation, and Hive dialect will delegate to Flink's parser for ""add jar xxx"". But the behavior  between Flink and Hive for add command is different. The main difference is Flink dialect requires quotation but Hive dialect doesn't.

For better compatibility,  we need to port it to Hive dialect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 06 15:35:41 UTC 2022,,,,,,,,,,"0|z17dwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 15:35;jark;Fixed in master: 6d06cdaa1633035c127483af1dc4b45e57cbc035 and 1a094eb3619e9696759eaf1a5a0dad4e6852bb12;;;",,,,,,,,,,,,,,,,,,,,
Assign speculative execution attempt with correct CREATED timestamp,FLINK-28771,13474618,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,02/Aug/22 03:48,03/Aug/22 02:43,04/Jun/24 20:41,03/Aug/22 02:43,1.16.0,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,"Currently, newly created speculative execution attempt is assigned with a wrong CREATED timestamp in SpeculativeScheduler. We need to fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 02:43:06 UTC 2022,,,,,,,,,,"0|z17ds8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 02:43;zhuzh;Fixed via e1a74df4427e99f4b0f3aaa4e8f4f5ff7cbd044e;;;",,,,,,,,,,,,,,,,,,,,
CREATE TABLE AS SELECT supports explain,FLINK-28770,13474608,13436816,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tartarus,tartarus,02/Aug/22 02:54,02/Aug/22 02:54,04/Jun/24 20:41,,,,,,,,,,Table SQL / Planner,,,,,,,0,,,,,,,Unsupported operation: org.apache.flink.table.operations.ddl.CreateTableASOperation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-02 02:54:10.0,,,,,,,,,,"0|z17dq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink History Server show wrong name of batch jobs,FLINK-28769,13474557,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,bgeng777,bgeng777,01/Aug/22 16:49,27/Oct/22 07:51,04/Jun/24 20:41,27/Oct/22 07:51,,,,,,,,,API / DataSet,,,,,,,0,,,,,,,"When running {{examples/batch/WordCount.jar}} using flink1.15 and 1.16 together with history server started, the history server shows default name(e.g. Flink Java Job at Tue Aug 02.. ) of the batch job instead of the name( ""WordCount Example"" ) specified in the java code.
But for {{examples/streaming/WordCount.jar}}, the job name in history server is correct.

It looks like that {{org.apache.flink.api.java.ExecutionEnvironment#executeAsync(java.lang.String)}} does not set job name as what {{org.apache.flink.streaming.api.environment.StreamExecutionEnvironment#execute(java.lang.String)}} does(e.g. streamGraph.setJobName(jobName); ).


!image-2022-08-02-00-41-51-815.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/22 16:41;bgeng777;image-2022-08-02-00-41-51-815.png;https://issues.apache.org/jira/secure/attachment/13047496/image-2022-08-02-00-41-51-815.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 07:51:22 UTC 2022,,,,,,,,,,"0|z17deo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 02:55;zhuzh;Loos to me the the specified name does work after checking the code of {{ExecutionEnvironment#executeAsync(java.lang.String)}}.
I guess your batch job is not set with a ""output"" param so that the job is created via {{counts.print()}} instead of {{env.execute(...)}}. This is a known limitation of DataSet and it's less likely we will change the interface of {{counts.print()}} because {{DataSet}} will be deprecated soon.;;;","27/Oct/22 07:51;xtsong;Closing as we are no longer actively maintaining the deprecated DataSet API.;;;",,,,,,,,,,,,,,,,,,,
Update JUnit5 to v5.9.1,FLINK-28768,13474555,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,01/Aug/22 16:27,05/Apr/23 07:47,04/Jun/24 20:41,12/Oct/22 11:04,,,,,1.17.0,,,,Test Infrastructure,Tests,,,,,,0,pull-request-available,stale-assigned,,,,,"Junit 5.9.0 is released

with release notes https://junit.org/junit5/docs/current/release-notes/#release-notes-5.9.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 12 11:04:18 UTC 2022,,,,,,,,,,"0|z17de8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","12/Oct/22 11:04;chesnay;master: 51fc20db30d001a95de95b3b9993eeb06f558f6c;;;",,,,,,,,,,,,,,,,,,,
SqlGatewayServiceITCase.testCancelOperation failed with AssertionFailedError,FLINK-28767,13474509,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,hxbks2ks,hxbks2ks,01/Aug/22 11:45,12/Aug/22 02:43,04/Jun/24 20:41,12/Aug/22 02:43,1.16.0,,,,1.16.0,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-07-31T03:32:09.3148584Z Jul 31 03:32:09 [ERROR] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.268 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceITCase
2022-07-31T03:32:09.3205977Z Jul 31 03:32:09 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCancelOperation  Time elapsed: 0.008 s  <<< FAILURE!
2022-07-31T03:32:09.3207151Z Jul 31 03:32:09 org.opentest4j.AssertionFailedError: expected: <org.apache.flink.table.gateway.api.results.OperationInfo@ab4fed9f> but was: <org.apache.flink.table.gateway.api.results.OperationInfo@ea9e78d5>
2022-07-31T03:32:09.3207956Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55)
2022-07-31T03:32:09.3211582Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertionUtils.failNotEqual(AssertionUtils.java:62)
2022-07-31T03:32:09.3212267Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:182)
2022-07-31T03:32:09.3212945Z Jul 31 03:32:09 	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:177)
2022-07-31T03:32:09.3213607Z Jul 31 03:32:09 	at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:1141)
2022-07-31T03:32:09.3216761Z Jul 31 03:32:09 	at org.apache.flink.table.gateway.service.SqlGatewayServiceITCase.testCancelOperation(SqlGatewayServiceITCase.java:245)
2022-07-31T03:32:09.3217645Z Jul 31 03:32:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-31T03:32:09.3218243Z Jul 31 03:32:09 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-31T03:32:09.3218971Z Jul 31 03:32:09 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-31T03:32:09.3219622Z Jul 31 03:32:09 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-31T03:32:09.3227901Z Jul 31 03:32:09 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-07-31T03:32:09.3228714Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-07-31T03:32:09.3229561Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-07-31T03:32:09.3230353Z Jul 31 03:32:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-07-31T03:32:09.3231409Z Jul 31 03:32:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-07-31T03:32:09.3235059Z Jul 31 03:32:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-07-31T03:32:09.3236100Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-07-31T03:32:09.3236978Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-07-31T03:32:09.3237841Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-07-31T03:32:09.3241384Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-07-31T03:32:09.3242557Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-07-31T03:32:09.3243372Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-07-31T03:32:09.3244155Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-07-31T03:32:09.3244899Z Jul 31 03:32:09 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-07-31T03:32:09.3249715Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-07-31T03:32:09.3250594Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3251432Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-07-31T03:32:09.3252291Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-07-31T03:32:09.3253055Z Jul 31 03:32:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-07-31T03:32:09.3256625Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-07-31T03:32:09.3257465Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3258297Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-31T03:32:09.3259062Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-31T03:32:09.3262851Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-31T03:32:09.3263723Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3264528Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-31T03:32:09.3265259Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-31T03:32:09.3266191Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-31T03:32:09.3270216Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-07-31T03:32:09.3271316Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-07-31T03:32:09.3272431Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-07-31T03:32:09.3273258Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3277605Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-31T03:32:09.3278454Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-31T03:32:09.3279180Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-31T03:32:09.3280019Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3280980Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-31T03:32:09.3287071Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-31T03:32:09.3288036Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-31T03:32:09.3289128Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-07-31T03:32:09.3290078Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-07-31T03:32:09.3294254Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3295128Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-31T03:32:09.3295912Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-31T03:32:09.3296679Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-31T03:32:09.3297504Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-31T03:32:09.3305416Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-31T03:32:09.3306251Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-31T03:32:09.3307191Z Jul 31 03:32:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-31T03:32:09.3308099Z Jul 31 03:32:09 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-07-31T03:32:09.3308702Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-07-31T03:32:09.3315595Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-07-31T03:32:09.3316450Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-07-31T03:32:09.3317133Z Jul 31 03:32:09 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38962&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 02:43:15 UTC 2022,,,,,,,,,,"0|z17d40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 11:45;hxbks2ks;cc [~fsk119];;;","04/Aug/22 06:13;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39229&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be;;;","08/Aug/22 08:16;hxbks2ks;https://dev.azure.com/hxbks2ks/FLINK-TEST/_build/results?buildId=1963&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f;;;","10/Aug/22 05:20;luoyuxia;[https://dev.azure.com/leonardBang/Azure_CI/_build/results?buildId=719&view=logs&j=43a593e7-535d-554b-08cc-244368da36b4&t=82d122c0-8bbf-56f3-4c0d-8e3d69630d0f];;;","10/Aug/22 07:52;fsk119;After investigation, I find the main cause is when the test thread cancel the execution, the SqlGatewayService doesn't wait for the execution thread exits. When the execution thread is interrupted, it will record the error. Here the test cancels the operation and then fetch the OperationStatus. It means it's possible to get the InterruptedException or not.

 

I think we don't need to record the exception if the execution is interrupted by the users. Users knows what happens. ;;;","12/Aug/22 02:43;fsk119;Merged into master: 8813a5ac914835ce76eeeeb39f9e0bf0e1760af3;;;",,,,,,,,,,,,,,,
UnalignedCheckpointStressITCase.runStressTest failed with NoSuchFileException,FLINK-28766,13474505,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalashnikov,hxbks2ks,hxbks2ks,01/Aug/22 11:40,07/Dec/22 12:29,04/Jun/24 20:41,07/Dec/22 12:29,1.16.0,1.17.0,,,1.16.1,1.17.0,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-08-01T01:36:16.0563880Z Aug 01 01:36:16 [ERROR] org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runStressTest  Time elapsed: 12.579 s  <<< ERROR!
2022-08-01T01:36:16.0565407Z Aug 01 01:36:16 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit1058240190382532303/f0f99754a53d2c4633fed75011da58dd/chk-7/61092e4a-5b9a-4f56-83f7-d9960c53ed3e
2022-08-01T01:36:16.0566296Z Aug 01 01:36:16 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
2022-08-01T01:36:16.0566972Z Aug 01 01:36:16 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
2022-08-01T01:36:16.0567600Z Aug 01 01:36:16 	at java.util.Iterator.forEachRemaining(Iterator.java:115)
2022-08-01T01:36:16.0568290Z Aug 01 01:36:16 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
2022-08-01T01:36:16.0569172Z Aug 01 01:36:16 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
2022-08-01T01:36:16.0569877Z Aug 01 01:36:16 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
2022-08-01T01:36:16.0570554Z Aug 01 01:36:16 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
2022-08-01T01:36:16.0571371Z Aug 01 01:36:16 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
2022-08-01T01:36:16.0572417Z Aug 01 01:36:16 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
2022-08-01T01:36:16.0573618Z Aug 01 01:36:16 	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.discoverRetainedCheckpoint(UnalignedCheckpointStressITCase.java:289)
2022-08-01T01:36:16.0575187Z Aug 01 01:36:16 	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runAndTakeExternalCheckpoint(UnalignedCheckpointStressITCase.java:262)
2022-08-01T01:36:16.0576540Z Aug 01 01:36:16 	at org.apache.flink.test.checkpointing.UnalignedCheckpointStressITCase.runStressTest(UnalignedCheckpointStressITCase.java:158)
2022-08-01T01:36:16.0577684Z Aug 01 01:36:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-08-01T01:36:16.0578546Z Aug 01 01:36:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-08-01T01:36:16.0579374Z Aug 01 01:36:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-08-01T01:36:16.0580298Z Aug 01 01:36:16 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-08-01T01:36:16.0581243Z Aug 01 01:36:16 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-08-01T01:36:16.0582029Z Aug 01 01:36:16 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-08-01T01:36:16.0582766Z Aug 01 01:36:16 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-08-01T01:36:16.0583488Z Aug 01 01:36:16 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-08-01T01:36:16.0584203Z Aug 01 01:36:16 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-08-01T01:36:16.0585087Z Aug 01 01:36:16 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-08-01T01:36:16.0585778Z Aug 01 01:36:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-01T01:36:16.0586482Z Aug 01 01:36:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-01T01:36:16.0587155Z Aug 01 01:36:16 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-08-01T01:36:16.0587809Z Aug 01 01:36:16 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-08-01T01:36:16.0588434Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-01T01:36:16.0589203Z Aug 01 01:36:16 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-08-01T01:36:16.0589867Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-08-01T01:36:16.0590672Z Aug 01 01:36:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-08-01T01:36:16.0591534Z Aug 01 01:36:16 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-08-01T01:36:16.0592209Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-08-01T01:36:16.0592832Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-08-01T01:36:16.0593462Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-08-01T01:36:16.0594097Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-08-01T01:36:16.0594951Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-08-01T01:36:16.0595605Z Aug 01 01:36:16 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-08-01T01:36:16.0596215Z Aug 01 01:36:16 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-08-01T01:36:16.0596820Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-08-01T01:36:16.0597438Z Aug 01 01:36:16 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-08-01T01:36:16.0598029Z Aug 01 01:36:16 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-08-01T01:36:16.0598579Z Aug 01 01:36:16 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-08-01T01:36:16.0599296Z Aug 01 01:36:16 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-08-01T01:36:16.0600030Z Aug 01 01:36:16 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-08-01T01:36:16.0600750Z Aug 01 01:36:16 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-08-01T01:36:16.0601515Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-08-01T01:36:16.0602352Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-08-01T01:36:16.0603185Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-08-01T01:36:16.0604176Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-08-01T01:36:16.0605378Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-08-01T01:36:16.0606160Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-08-01T01:36:16.0606871Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-08-01T01:36:16.0607666Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-08-01T01:36:16.0608513Z Aug 01 01:36:16 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-08-01T01:36:16.0609407Z Aug 01 01:36:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-08-01T01:36:16.0610227Z Aug 01 01:36:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-08-01T01:36:16.0611053Z Aug 01 01:36:16 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-08-01T01:36:16.0611831Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-08-01T01:36:16.0612546Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-08-01T01:36:16.0613229Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-08-01T01:36:16.0614034Z Aug 01 01:36:16 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-08-01T01:36:16.0615450Z Aug 01 01:36:16 Caused by: java.nio.file.NoSuchFileException: /tmp/junit1058240190382532303/f0f99754a53d2c4633fed75011da58dd/chk-7/61092e4a-5b9a-4f56-83f7-d9960c53ed3e
2022-08-01T01:36:16.0616258Z Aug 01 01:36:16 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
2022-08-01T01:36:16.0616928Z Aug 01 01:36:16 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
2022-08-01T01:36:16.0617591Z Aug 01 01:36:16 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
2022-08-01T01:36:16.0618293Z Aug 01 01:36:16 	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
2022-08-01T01:36:16.0619099Z Aug 01 01:36:16 	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
2022-08-01T01:36:16.0619817Z Aug 01 01:36:16 	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
2022-08-01T01:36:16.0620450Z Aug 01 01:36:16 	at java.nio.file.Files.readAttributes(Files.java:1737)
2022-08-01T01:36:16.0621066Z Aug 01 01:36:16 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219)
2022-08-01T01:36:16.0621704Z Aug 01 01:36:16 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276)
2022-08-01T01:36:16.0622316Z Aug 01 01:36:16 	at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
2022-08-01T01:36:16.0622968Z Aug 01 01:36:16 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:84)
2022-08-01T01:36:16.0623482Z Aug 01 01:36:16 	... 60 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38984&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9
",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28872,,,,,,,,,,FLINK-28898,,,,FLINK-30107,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 07 12:29:18 UTC 2022,,,,,,,,,,"0|z17d34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 01:53;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39120&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","08/Aug/22 02:48;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39369&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=7942;;;","10/Aug/22 03:32;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39779&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","12/Aug/22 09:32;zhuzh;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39908&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","05/Sep/22 02:00;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40645&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9;;;","13/Sep/22 07:15;martijnvisser;[~pnowojski] Any idea who could help out with this test instability? ;;;","19/Sep/22 12:35;akalashnikov;[~martijnvisser], I will take a look;;;","19/Sep/22 12:48;martijnvisser;Thanks [~akalashnikov];;;","21/Sep/22 09:58;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41188&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10741;;;","10/Oct/22 03:47;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41753&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","21/Oct/22 03:41;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42272&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10622;;;","01/Nov/22 14:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42701&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10538;;;","21/Nov/22 09:29;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43254&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=f2c100be-250b-5e85-7bbe-176f68fcddc5;;;","22/Nov/22 07:45;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43367&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","25/Nov/22 03:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43451&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10267;;;","25/Nov/22 03:53;mapohl;[~akalashnikov] any updates on that issue. FYI: We're seeing other tests failing with a similar issue where temporary files are not present anymore (e.g. FLINK-28898);;;","28/Nov/22 10:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43521&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8710;;;","04/Dec/22 13:58;pnowojski;Merged to master as 1ddaa2a460d and 9e1cefe2509;;;","05/Dec/22 13:06;mapohl;I created the missing [1.16 backport PR|https://github.com/apache/flink/pull/21456]. May one of you approve that PR, [~akalashnikov]/[~pnowojski]? Or was there a reason why you didn't proceed with the backport? We observed the test failure in both {{master}} and {{release-1.16}}.;;;","07/Dec/22 09:20;akalashnikov;[~mapohl] , thanks for creating the backport. There are no reasons to have it. I just missed this part. I've approved your PR.;;;","07/Dec/22 12:29;mapohl;Cool, thanks for approving the PR. Here's the summary of the ticket resolution:

master: 
* 1ddaa2a460dd8def96742d0d94b1a86df4ae9c81
* 9e1cefe250972b213ac69322531a4de69ca0e1a6

1.16: 
* 98647fba9a0c04151135a2bbfe611bba9956aea6
* 56273f307c6e5e1d25d712a931ffc352d0c9021e;;;"
Create a doc for protobuf format,FLINK-28765,13474491,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,maosuhan,maosuhan,maosuhan,01/Aug/22 10:36,10/Sep/22 01:04,04/Jun/24 20:41,03/Aug/22 03:13,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,"After this feature has been done https://issues.apache.org/jira/browse/FLINK-18202, we should write a doc to introduce how to use the protobuf format in SQL.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18202,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Sep 10 01:01:59 UTC 2022,,,,,,,,,,"0|z17d00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 03:12;libenchao;fixed via 92ec61a54df2a66183633c57944eac774dd48906 (master);;;","05/Aug/22 00:09;hui.yang;[~maosuhan] when is this protobuf support ready to use? I found the pull request [https://github.com/apache/flink/pull/14376|https://github.com/apache/flink/pull/14376/files] and [https://github.com/apache/flink/pull/20408] has been merged. Is it packaged with latest version(1.16-snapshot)? If I want to use it with version 1.15, can I import the flink-protobuf as dependency(I didn't find this dependency yet on maven repository). Thanks!;;;","06/Aug/22 12:57;maosuhan;[~hui.yang] It probably can work, you could try.;;;","08/Aug/22 23:46;hui.yang;[~maosuhan] can you guide me how I can use the flink-protobuf now with the flink version 1.15?;;;","17/Aug/22 06:08;hui.yang;[~maosuhan] want to follow up with you to see if you have any guidance to use the flink-protobuf feature? thanks;;;","10/Sep/22 01:01;hui.yang;[~maosuhan] does this change support accessing the nested object in Protobuf? thanks

Proto object for example:

message Demand {
repeated MinuteDemand minuteDemands = 1 [packed = false];
}

message MinuteDemand {
int64 met = 1;
int64 unmet = 2;
}

message Supply {
int64 met = 1;
int64 unmet = 2;
}

message SupplyDemandTracker {
int64 timestampMs = 1;
int64 durationMs = 2;
string geohash = 3;
int64 vehicleID = 4;
Demand demand = 5 [packed = false];
Supply supply = 6 [packed = false];
};;;",,,,,,,,,,,,,,,
Support more than 64 distinct aggregate function calls in one aggregate SQL query,FLINK-28764,13474475,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhongwei,zhongwei,zhongwei,01/Aug/22 09:05,11/Mar/24 12:44,04/Jun/24 20:41,,1.13.6,1.14.5,1.15.1,,1.20.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,stale-assigned,,,,,"Currently Flink SQL does not support more than 64 distinct aggregate function calls in one aggregate SQL query. We encountered this problem while migrating batch jobs from spark to flink. The spark job has 79 distinct aggregate function calls in one aggregate SQL query.

Reproduce code:
{code:java}
public class Test64Distinct {
    public static void main(String[] args) {
        TableEnvironment tableEnv = TableEnvironment.create(EnvironmentSettings.inBatchMode());
        tableEnv.executeSql(""create table datagen_source(id BIGINT, val BIGINT) with "" +
                ""('connector'='datagen', 'number-of-rows'='1000')"");
        tableEnv.executeSql(""select "" +
                ""count(distinct val * 1), "" +
                ""count(distinct val * 2), "" +
                ""count(distinct val * 3), "" +
                ""count(distinct val * 4), "" +
                ""count(distinct val * 5), "" +
                ""count(distinct val * 6), "" +
                ""count(distinct val * 7), "" +
                ""count(distinct val * 8), "" +
                ""count(distinct val * 9), "" +
                ""count(distinct val * 10), "" +
                ""count(distinct val * 11), "" +
                ""count(distinct val * 12), "" +
                ""count(distinct val * 13), "" +
                ""count(distinct val * 14), "" +
                ""count(distinct val * 15), "" +
                ""count(distinct val * 16), "" +
                ""count(distinct val * 17), "" +
                ""count(distinct val * 18), "" +
                ""count(distinct val * 19), "" +
                ""count(distinct val * 20), "" +
                ""count(distinct val * 21), "" +
                ""count(distinct val * 22), "" +
                ""count(distinct val * 23), "" +
                ""count(distinct val * 24), "" +
                ""count(distinct val * 25), "" +
                ""count(distinct val * 26), "" +
                ""count(distinct val * 27), "" +
                ""count(distinct val * 28), "" +
                ""count(distinct val * 29), "" +
                ""count(distinct val * 30), "" +
                ""count(distinct val * 31), "" +
                ""count(distinct val * 32), "" +
                ""count(distinct val * 33), "" +
                ""count(distinct val * 34), "" +
                ""count(distinct val * 35), "" +
                ""count(distinct val * 36), "" +
                ""count(distinct val * 37), "" +
                ""count(distinct val * 38), "" +
                ""count(distinct val * 39), "" +
                ""count(distinct val * 40), "" +
                ""count(distinct val * 41), "" +
                ""count(distinct val * 42), "" +
                ""count(distinct val * 43), "" +
                ""count(distinct val * 44), "" +
                ""count(distinct val * 45), "" +
                ""count(distinct val * 46), "" +
                ""count(distinct val * 47), "" +
                ""count(distinct val * 48), "" +
                ""count(distinct val * 49), "" +
                ""count(distinct val * 50), "" +
                ""count(distinct val * 51), "" +
                ""count(distinct val * 52), "" +
                ""count(distinct val * 53), "" +
                ""count(distinct val * 54), "" +
                ""count(distinct val * 55), "" +
                ""count(distinct val * 56), "" +
                ""count(distinct val * 57), "" +
                ""count(distinct val * 58), "" +
                ""count(distinct val * 59), "" +
                ""count(distinct val * 60), "" +
                ""count(distinct val * 61), "" +
                ""count(distinct val * 62), "" +
                ""count(distinct val * 63), "" +
                ""count(distinct val * 64), "" +
                ""count(distinct val * 65) from datagen_source"").print();
    }
} {code}
Exception:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.TableException: Sql optimization: Cannot generate a valid execution plan for the given query: LogicalSink(table=[*anonymous_collect$1*], fields=[EXPR$0, EXPR$1, EXPR$2, EXPR$3, EXPR$4, EXPR$5, EXPR$6, EXPR$7, EXPR$8, EXPR$9, EXPR$10, EXPR$11, EXPR$12, EXPR$13, EXPR$14, EXPR$15, EXPR$16, EXPR$17, EXPR$18, EXPR$19, EXPR$20, EXPR$21, EXPR$22, EXPR$23, EXPR$24, EXPR$25, EXPR$26, EXPR$27, EXPR$28, EXPR$29, EXPR$30, EXPR$31, EXPR$32, EXPR$33, EXPR$34, EXPR$35, EXPR$36, EXPR$37, EXPR$38, EXPR$39, EXPR$40, EXPR$41, EXPR$42, EXPR$43, EXPR$44, EXPR$45, EXPR$46, EXPR$47, EXPR$48, EXPR$49, EXPR$50, EXPR$51, EXPR$52, EXPR$53, EXPR$54, EXPR$55, EXPR$56, EXPR$57, EXPR$58, EXPR$59, EXPR$60, EXPR$61, EXPR$62, EXPR$63, EXPR$64])
+- LogicalAggregate(group=[{}], EXPR$0=[COUNT(DISTINCT $0)], EXPR$1=[COUNT(DISTINCT $1)], EXPR$2=[COUNT(DISTINCT $2)], EXPR$3=[COUNT(DISTINCT $3)], EXPR$4=[COUNT(DISTINCT $4)], EXPR$5=[COUNT(DISTINCT $5)], EXPR$6=[COUNT(DISTINCT $6)], EXPR$7=[COUNT(DISTINCT $7)], EXPR$8=[COUNT(DISTINCT $8)], EXPR$9=[COUNT(DISTINCT $9)], EXPR$10=[COUNT(DISTINCT $10)], EXPR$11=[COUNT(DISTINCT $11)], EXPR$12=[COUNT(DISTINCT $12)], EXPR$13=[COUNT(DISTINCT $13)], EXPR$14=[COUNT(DISTINCT $14)], EXPR$15=[COUNT(DISTINCT $15)], EXPR$16=[COUNT(DISTINCT $16)], EXPR$17=[COUNT(DISTINCT $17)], EXPR$18=[COUNT(DISTINCT $18)], EXPR$19=[COUNT(DISTINCT $19)], EXPR$20=[COUNT(DISTINCT $20)], EXPR$21=[COUNT(DISTINCT $21)], EXPR$22=[COUNT(DISTINCT $22)], EXPR$23=[COUNT(DISTINCT $23)], EXPR$24=[COUNT(DISTINCT $24)], EXPR$25=[COUNT(DISTINCT $25)], EXPR$26=[COUNT(DISTINCT $26)], EXPR$27=[COUNT(DISTINCT $27)], EXPR$28=[COUNT(DISTINCT $28)], EXPR$29=[COUNT(DISTINCT $29)], EXPR$30=[COUNT(DISTINCT $30)], EXPR$31=[COUNT(DISTINCT $31)], EXPR$32=[COUNT(DISTINCT $32)], EXPR$33=[COUNT(DISTINCT $33)], EXPR$34=[COUNT(DISTINCT $34)], EXPR$35=[COUNT(DISTINCT $35)], EXPR$36=[COUNT(DISTINCT $36)], EXPR$37=[COUNT(DISTINCT $37)], EXPR$38=[COUNT(DISTINCT $38)], EXPR$39=[COUNT(DISTINCT $39)], EXPR$40=[COUNT(DISTINCT $40)], EXPR$41=[COUNT(DISTINCT $41)], EXPR$42=[COUNT(DISTINCT $42)], EXPR$43=[COUNT(DISTINCT $43)], EXPR$44=[COUNT(DISTINCT $44)], EXPR$45=[COUNT(DISTINCT $45)], EXPR$46=[COUNT(DISTINCT $46)], EXPR$47=[COUNT(DISTINCT $47)], EXPR$48=[COUNT(DISTINCT $48)], EXPR$49=[COUNT(DISTINCT $49)], EXPR$50=[COUNT(DISTINCT $50)], EXPR$51=[COUNT(DISTINCT $51)], EXPR$52=[COUNT(DISTINCT $52)], EXPR$53=[COUNT(DISTINCT $53)], EXPR$54=[COUNT(DISTINCT $54)], EXPR$55=[COUNT(DISTINCT $55)], EXPR$56=[COUNT(DISTINCT $56)], EXPR$57=[COUNT(DISTINCT $57)], EXPR$58=[COUNT(DISTINCT $58)], EXPR$59=[COUNT(DISTINCT $59)], EXPR$60=[COUNT(DISTINCT $60)], EXPR$61=[COUNT(DISTINCT $61)], EXPR$62=[COUNT(DISTINCT $62)], EXPR$63=[COUNT(DISTINCT $63)], EXPR$64=[COUNT(DISTINCT $64)])
   +- LogicalProject(exprs=[[*($1, 1), *($1, 2), *($1, 3), *($1, 4), *($1, 5), *($1, 6), *($1, 7), *($1, 8), *($1, 9), *($1, 10), *($1, 11), *($1, 12), *($1, 13), *($1, 14), *($1, 15), *($1, 16), *($1, 17), *($1, 18), *($1, 19), *($1, 20), *($1, 21), *($1, 22), *($1, 23), *($1, 24), *($1, 25), *($1, 26), *($1, 27), *($1, 28), *($1, 29), *($1, 30), *($1, 31), *($1, 32), *($1, 33), *($1, 34), *($1, 35), *($1, 36), *($1, 37), *($1, 38), *($1, 39), *($1, 40), *($1, 41), *($1, 42), *($1, 43), *($1, 44), *($1, 45), *($1, 46), *($1, 47), *($1, 48), *($1, 49), *($1, 50), *($1, 51), *($1, 52), *($1, 53), *($1, 54), *($1, 55), *($1, 56), *($1, 57), *($1, 58), *($1, 59), *($1, 60), *($1, 61), *($1, 62), *($1, 63), *($1, 64), *($1, 65)]])
      +- LogicalTableScan(table=[[default_catalog, default_database, datagen_source]])group count must be less than 64.
Please check the documentation for the set of currently supported SQL features.
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:86)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
    at scala.collection.Iterator.foreach(Iterator.scala:937)
    at scala.collection.Iterator.foreach$(Iterator.scala:937)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
    at scala.collection.IterableLike.foreach(IterableLike.scala:70)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:92)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:44)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:44)
    at scala.collection.immutable.List.foreach(List.scala:388)
    at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:44)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:78)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:312)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:192)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1688)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:840)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1342)
    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:708)
    at com.shopee.di.Test64Distinct.main(Test64Distinct.java:11)
Caused by: org.apache.flink.table.api.TableException: group count must be less than 64.
    at org.apache.flink.table.planner.plan.rules.logical.DecomposeGroupingSetsRule.onMatch(DecomposeGroupingSetsRule.scala:177)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
    at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
    ... 27 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:16 UTC 2023,,,,,,,,,,"0|z17cwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 09:06;zhongwei;cc [~jark];;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
Make savepoint format configurable for upgrades/savepoint operations,FLINK-28763,13474466,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,gyfora,gyfora,01/Aug/22 08:42,09/Aug/22 10:36,04/Jun/24 20:41,09/Aug/22 10:36,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"We should make the savepoint format configurable for Flink 1.15+ so users can select canonical/native. This should be configured in flinkConfiguration directly.

[https://stackoverflow.com/questions/73169474/apache-flink-k8s-operator-and-native-savepoint-format] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 10:36:25 UTC 2022,,,,,,,,,,"0|z17cug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 10:36;gyfora;merged to main 70bb74c06280985065f0ce56236186a146d4436f;;;",,,,,,,,,,,,,,,,,,,,
Support AvroParquetWriters in PyFlink,FLINK-28762,13474450,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,01/Aug/22 07:28,02/Aug/22 07:36,04/Jun/24 20:41,02/Aug/22 07:36,1.15.1,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 07:36:54 UTC 2022,,,,,,,,,,"0|z17cqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 07:36;hxbks2ks;Merged into master via b434c8ec8c8907eac2f5aa60a46d9bf50b27960d;;;",,,,,,,,,,,,,,,,,,,,
BinaryClassificationEvaluator cannot work with unaligned checkpoint,FLINK-28761,13474447,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,yunfengzhou,yunfengzhou,01/Aug/22 07:06,10/Jan/23 04:13,04/Jun/24 20:41,10/Jan/23 04:13,ml-2.1.0,,,,,,,,Library / Machine Learning,,,,,,,0,,,,,,,"If we make  {{BinaryClassificationEvaluatorTest}} extend  {{AbstractTestBase}}, this test class would throw the following exceptions during execution:
 
{code:java}
org.apache.flink.table.api.TableException: Failed to execute sql

	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:854)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:605)
	at org.apache.flink.ml.evaluation.BinaryClassificationEvaluatorTest.testEvaluateWithWeight(BinaryClassificationEvaluatorTest.java:305)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.UnsupportedOperationException: Unaligned checkpoints are currently not supported for custom partitioners, as rescaling is not guaranteed to work correctly.
The user can force Unaligned Checkpoints by using 'execution.checkpointing.unaligned.forced'
	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.preValidate(StreamingJobGraphGenerator.java:390)
	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:166)
	at org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.createJobGraph(StreamingJobGraphGenerator.java:121)
	at org.apache.flink.streaming.api.graph.StreamGraph.getJobGraph(StreamGraph.java:993)
	at org.apache.flink.client.StreamGraphTranslator.translateToJobGraph(StreamGraphTranslator.java:50)
	at org.apache.flink.client.FlinkPipelineTranslationUtil.getJobGraph(FlinkPipelineTranslationUtil.java:39)
	at org.apache.flink.client.deployment.executors.PipelineExecutorUtils.getJobGraph(PipelineExecutorUtils.java:56)
	at org.apache.flink.test.util.MiniClusterPipelineExecutorServiceLoader$MiniClusterExecutor.execute(MiniClusterPipelineExecutorServiceLoader.java:156)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2095)
	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:835)
	... 38 more
{code}


This error message shows that {{BinaryClassificationEvaluator}} cannot work correctly when unaligned checkpoint is enabled, and it is probably caused by the {{partitionCustom}} method in this algorithm's implementation. 

This problem needs to be fixed and we need to make {{BinaryClassificationEvaluatorTest}} extend {{AbstractTestBase}} after that to reduce the time to execute CI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-08-01 07:06:48.0,,,,,,,,,,"0|z17cq8:",9223372036854775807,We decided that Flink ML stages do not need to support unaligned checkpoint and rescaling for now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive table store support can't be fully enabled by ADD JAR statement,FLINK-28760,13474439,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,01/Aug/22 06:21,02/Aug/22 02:28,04/Jun/24 20:41,02/Aug/22 02:28,table-store-0.2.0,table-store-0.3.0,,,table-store-0.2.0,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Current table store Hive document states that, to enable support in Hive, we should use ADD JAR statement to add hive connector jar.

However some types of queries, for example join statements in MR engine, may still throw ClassNotFound exception. The correct way to enable table store support is to create an auxlib directory under the root directory of Hive and copy jar into that directory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 02:28:57 UTC 2022,,,,,,,,,,"0|z17cog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 02:28;lzljs3620320;master: adcf39744c7a2c7b3ec143d1d217eaa02efa10a6
release-0.2: da30ce533b81dc4c8ddd8b37624924e469f7a39a;;;",,,,,,,,,,,,,,,,,,,,
Enable speculative execution for in AdaptiveBatchScheduler TPC-DS e2e tests,FLINK-28759,13474438,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,zhuzh,zhuzh,01/Aug/22 06:16,02/Aug/22 10:18,04/Jun/24 20:41,02/Aug/22 10:18,,,,,1.16.0,,,,Runtime / Coordination,Tests,,,,,,0,pull-request-available,,,,,,"To verify the correctness of speculative execution, we can enabled it in AdaptiveBatchScheduler TPC-DS e2e tests, which runs a lot of different batch jobs and verifies the result.
Note that we need to disable the blocklist (by setting block duration to 0) in such single machine e2e tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 10:18:29 UTC 2022,,,,,,,,,,"0|z17co8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 06:25;JunRuiLi;[~zhuzh] I want to take this task. Would you assign it to me?;;;","01/Aug/22 06:28;zhuzh;[~JunRuiLi] I have assign you the ticket. Go a head to open a PR for it.;;;","02/Aug/22 10:18;zhuzh;Done via 16b0cc1117d4bda11b89440e962646024b2ff6c5;;;",,,,,,,,,,,,,,,,,,
FlinkKafkaConsumer fails to stop with savepoint ,FLINK-28758,13474432,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pnowojski,hjw,hjw,01/Aug/22 05:54,13/Nov/23 01:42,04/Jun/24 20:41,15/Sep/23 09:28,1.15.0,1.16.1,1.17.0,,kafka-3.0.1,kafka-3.1.0,,,Connectors / Kafka,,,,,,,1,pull-request-available,,,,,,"I post a stop with savepoint request to Flink Job throught rest api.
A Error happened in Kafka connector close.
The job will enter restarting .
It is successful to use savepoint command alone.
{code:java}
13:33:42.857 [Kafka Fetcher for Source: nlp-kafka-source -> nlp-clean (1/1)#0] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-hjw-3, groupId=hjw] Kafka consumer has been closed
13:33:42.857 [Kafka Fetcher for Source: cpp-kafka-source -> cpp-clean (1/1)#0] INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-hjw-4 unregistered
13:33:42.857 [Kafka Fetcher for Source: cpp-kafka-source -> cpp-clean (1/1)#0] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=consumer-hjw-4, groupId=hjw] Kafka consumer has been closed
13:33:42.860 [Source: nlp-kafka-source -> nlp-clean (1/1)#0] DEBUG org.apache.flink.streaming.runtime.tasks.StreamTask - Cleanup StreamTask (operators closed: false, cancelled: false)
13:33:42.860 [jobmanager-io-thread-4] INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator - Decline checkpoint 5 by task eeefcb27475446241861ad8db3f33144 of job d6fed247feab1c0bcc1b0dcc2cfb4736 at 79edfa88-ccc3-4140-b0e1-7ce8a7f8669f @ 127.0.0.1 (dataPort=-1).
org.apache.flink.util.SerializedThrowable: Task name with subtask : Source: nlp-kafka-source -> nlp-clean (1/1)#0 Failure reason: Task has failed.
 at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1388)
 at org.apache.flink.runtime.taskmanager.Task.lambda$triggerCheckpointBarrier$3(Task.java:1331)
 at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)
 at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:343)
Caused by: org.apache.flink.util.SerializedThrowable: org.apache.flink.streaming.connectors.kafka.internals.Handover$ClosedException
 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
 at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
 at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)
 at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)
 ... 3 common frames omitted
Caused by: org.apache.flink.util.SerializedThrowable: null
 at org.apache.flink.streaming.connectors.kafka.internals.Handover.close(Handover.java:177)
 at org.apache.flink.streaming.connectors.kafka.internals.KafkaFetcher.cancel(KafkaFetcher.java:164)
 at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:945)
 at org.apache.flink.streaming.api.operators.StreamSource.stop(StreamSource.java:128)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.stopOperatorForStopWithSavepoint(SourceStreamTask.java:305)
 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.lambda$triggerStopWithSavepointAsync$1(SourceStreamTask.java:285)
 at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
 at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
 at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
 at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
 at java.lang.Thread.run(Thread.java:748)
13:34:00.925 [flink-akka.actor.default-dispatcher-21] DEBUG org.apache.flink.runtime.jobmaster.JobMaster - Trigger heartbeat request.
13:34:00.926 [flink-akka.actor.default-dispatcher-22] DEBUG org.apache.flink.runtime.resourcemanager.StandaloneResourceManager - Trigger heartbeat request.
13:34:00.926 [flink-akka.actor.default-dispatcher-21] DEBUG org.apache.flink.runtime.taskexecutor.TaskExecutor - Received heartbeat request from 85d44174e17281984d28699c42e3eed6.
{code}","Flink version:1.15.0
deploy mode :K8s applicaiton Mode.   local mini cluster also have this problem.
Kafka Connector : use Kafka SourceFunction . No new Api.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23532,,,,FLINK-23527,,,,,,,,,,,"13/Oct/22 11:48;hjw;image-2022-10-13-19-47-56-635.png;https://issues.apache.org/jira/secure/attachment/13050901/image-2022-10-13-19-47-56-635.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 09:28:08 UTC 2023,,,,,,,,,,"0|z17cmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/22 10:44;Yanfei Lei;Hi [~hjw] , from the log, this is because the source failed when you posted the savepoint request.

And ""It is successful to use savepoint command alone."" is because the job is running when you send the request. I think this is expected behavior, the implementation of stop-with-savepoint behind REST and CLI is the same. ;;;","13/Oct/22 11:02;hjw;Hi [~Yanfei Lei] . Sorry, I made a mistake in my description.

The right description is I posted the ""stop with savepoint"" request.

I have corrected the error description.;;;","13/Oct/22 11:48;hjw;Other people also encounter similar problems posted by Chinese  Mailing Lists.

!image-2022-10-13-19-47-56-635.png!;;;","14/Oct/22 06:16;Yanfei Lei;Thanks for this information, I also reproduce this issue in my local environment when using {{{}FlinkKafkaConsumer{}}}. After

https://issues.apache.org/jira/browse/FLINK-24055, {{FlinkKafkaConsumer}} had been deprecated[1], so I also test ""stop-with-checkpoint"" by {{{}KafkaSource{}}}, which can work well.  

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/#kafka-sourcefunction;;;","18/Oct/22 06:23;Yanfei Lei;Hi [~renqs] , would you like to take a look? Is it necessary for FlinkKafkaConsumer(deprecated) to support stop-with-savepoint?;;;","20/Oct/22 09:51;ym;[~Yanfei Lei] and [~hjw] 

{{Since FlinkKafkaConsumer is deprecated, I do not think we have plans to support stop-with-savepoint on top of it.}}

If stop-with-savepoint is needed, you could migrate to use the new source.;;;","14/Mar/23 11:55;dwysakowicz;I think it would be nice to actually fix it. As long as we have the consumer in the code base we should ensure its stability does not decline. This `stop-with-savepoint` has been broken by the rework of `stop-with-savepoint`, so I think it makes sense to fix it, especially as I believe it's not too hard to do so.

As far as I can tell, the problem is that {{FlinkKafkaConsumer}} does not fully respect the contract of {{SourceFunction#cancel}} (https://issues.apache.org/jira/browse/FLINK-23527), because it does not finish gracefully, but always throws {{ClosedException}}.

I believe we could fix it, by adjusting the {{KafkaFetcher#runFetchLoop}}:
{code}
    @Override
    public void runFetchLoop() throws Exception {
        try {
            // kick off the actual Kafka consumer
            consumerThread.start();

            while (running) {
                // this blocks until we get the next records
                // it automatically re-throws exceptions encountered in the consumer thread
                final ConsumerRecords<byte[], byte[]> records = handover.pollNext();
                 .....
                }
            }
        } catch (Handover.ClosedException ex) {

            // WE SHOULD ADD THIS CODE

            if (running) {
                // rethrow, only if we are running, if fetcher is not running we should not throw
                // the ClosedException, as we are stopping gracefully
                ExceptionUtils.rethrowException(ex);
            }
        } finally {
            // this signals the consumer thread that no more work is to be done
            consumerThread.shutdown();
        }
{code};;;","14/Mar/23 13:07;pnowojski;+1 to what [~dwysakowicz] wrote.

This might be caused by FLINK-23532.

It's a bit pitty that the stack trace is pretty mangled and partially hidden, due to the usage of `ExceptionUtils.rethrowException` in `org.apache.flink.streaming.connectors.kafka.internals.Handover#pollNext` instead of just wrapping the `error` directly.;;;","14/Mar/23 15:58;markcho;[~dwysakowicz] [~pnowojski] Thanks for the pointers. This is something that we can take a look at, as it's currently impacting our Flink jobs.;;;","13/Sep/23 06:28;kination;[~dwysakowicz] [~markcho] [~pnowojski] thanks for suggestion.
I'm also facing same issue, and temporary suggestions above worked well on my side.

(yet not sure whether savepoint keeps the data correctly...)

 

Is there a plan to apply this change?

 ;;;","13/Sep/23 13:25;pnowojski;I will try to take care of that;;;","15/Sep/23 09:28;pnowojski;merged commit 27a5465 into main
merged commit 20c854c into v3.0;;;",,,,,,,,,
Increase precision of TIME fields in JSON Format,FLINK-28757,13474417,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kyledong,kyledong,01/Aug/22 03:26,01/Aug/22 13:01,04/Jun/24 20:41,,1.13.6,1.14.5,1.15.1,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,,,"Currently, TIME fields in JSON Format could only support a precision with ZERO decimal places, like '12:33:16', regardless of the precision of the original data ingested from the sources.

However, since Flink internally uses Integer to store the values of TIME fields, the precision could be easily extended to 3 decimal places, like '12:33:16.235', which has been confirmed by switching to other formats or modifying _org.apache.flink.formats.json.JsonToRowDataConverters#convertToTime_

Hence I propose the extend the precision of TIME fields in JSON Format from 0 to 3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 13:01:25 UTC 2022,,,,,,,,,,"0|z17cjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 03:36;kyledong;Related to FLINK-17525 and FLINK-19872, but still not fully resolved.;;;","01/Aug/22 13:01;martijnvisser;[~kyledong] Isn't this just a duplicate of FLINK-17525? I don't see what's really the difference. ;;;",,,,,,,,,,,,,,,,,,,
Can't write nested object to parquet flink 1.15.1,FLINK-28756,13474389,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,JonathanDiamant,JonathanDiamant,31/Jul/22 12:25,21/Aug/23 10:35,04/Jun/24 20:41,,1.15.1,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / API,,,,,,0,auto-deprioritized-major,Flink,flink-formats,Flink-parquet,Flink-table-api,TableAPI,"Hey, I'm trying to write nested objects to parquet files with Table api (by specifying the schema). I did manage to write flattened objects to parquet files. The objects I want to write contain array of objects. The schema I used is 

 
{code:java}
 DataTypes. ARRAY(DataTypes.ROW({code}
{code:java}
DataTypes.FIELD(...),...)){code}
 

When I tried to run it locally I got the exception:

 

empty fields are illegal, the field should be ommited completely instead

 

And when I debugged the code I saw that the method 
{code:java}
 Public void write(Array Data array Data, int ordinal) {code}
 

 

In the class RowWriter in ParquetRowDataWriter is not implemented and thus the content of the array cannot be written.

 

I have wondered if the method can be implemented or what it takes to implement it. 

(I have thought that this can be solved by call the write(RowData row) method of the same class with the correct parameters from the arrayData).",Local running and Flink on docker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,java,JAVA,Mon Aug 21 10:35:28 UTC 2023,,,,,,,,,,"0|z17cdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 10:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 10:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
Error when switching from stateless to savepoint upgrade mode,FLINK-28755,13474373,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gyfora,gyfora,31/Jul/22 09:13,21/Sep/22 15:07,04/Jun/24 20:41,21/Sep/22 15:07,kubernetes-operator-1.1.0,kubernetes-operator-1.2.0,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"When using the savepoint upgrade mode the state.savepoints.dir currently comes from the currently deployed spec / config.

This causes a nullpointer exception when switching to savepoint upgrade mode from stateless if state.savepoints.dir was previously undefined: 


{noformat}
org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:59)
org.apache.flink.kubernetes.operator.service.AbstractFlinkService.cancelJob(AbstractFlinkService.java:279)
org.apache.flink.kubernetes.operator.service.NativeFlinkService.cancelJob(NativeFlinkService.java:93)
org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.cancelJob(ApplicationReconciler.java:172)
org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.cancelJob(ApplicationReconciler.java:52)
org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:108)
org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:148)
org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:56)
org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:115){noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 21 15:07:53 UTC 2022,,,,,,,,,,"0|z17ca0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/22 15:07;gyfora;merged to main 1eca2c5e6a62bd9f1c9e752191f8a7477903d73c;;;",,,,,,,,,,,,,,,,,,,,
Document that Java 8 is required to build table store,FLINK-28754,13474345,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,danderson,danderson,30/Jul/22 14:32,11/Aug/22 04:41,04/Jun/24 20:41,11/Aug/22 04:41,,,,,table-store-0.2.0,,,,Documentation,Table Store,,,,,,0,pull-request-available,,,,,,"The table store can not be built with Java 11, but the ""build from source"" instructions don't mention this restriction.

https://nightlies.apache.org/flink/flink-table-store-docs-master/docs/engines/build/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 04:41:25 UTC 2022,,,,,,,,,,"0|z17c40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Aug/22 04:41;lzljs3620320;master: d29f41a2e4cabe37083ee0faf0bbd776db7fd9d8
release-0.2: 3f3f7ece3563a19654882db066a4c79e44c67dfe;;;",,,,,,,,,,,,,,,,,,,,
Improve FilterIntoJoinRule which could push some predicate to another side,FLINK-28753,13474323,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,30/Jul/22 10:20,09/Aug/22 02:32,04/Jun/24 20:41,09/Aug/22 02:32,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"for sql: SELECT * FROM MyTable1 join MyTable2 ON a1 = a2 AND a1 = 2

{{a1 = 2}} can be pushed into both left side and right side. but currently only left side will be pushed by FilterIntoJoinRule.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 02:32:08 UTC 2022,,,,,,,,,,"0|z17bz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 02:32;godfreyhe;Fixed in master: 0e6e4198ad84227c20e2c61c2dd8b0616324aa31;;;",,,,,,,,,,,,,,,,,,,,
Add the json plan support in Python UDFs,FLINK-28752,13474314,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,30/Jul/22 08:31,01/Aug/22 11:10,04/Jun/24 20:41,01/Aug/22 11:10,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,"In release-1.15, we removed the json plan support in https://issues.apache.org/jira/browse/FLINK-26060. Since we have updated PyFlink to use the new type system in https://issues.apache.org/jira/browse/FLINK-25231, we need to add the json plan support again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 11:10:16 UTC 2022,,,,,,,,,,"0|z17bx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 11:10;hxbks2ks;Merged into master via 6ff6978c0eb56a9cd036a6bafdd049d0d8de9ba8;;;",,,,,,,,,,,,,,,,,,,,
Poor performance of the built in json_value function,FLINK-28751,13474313,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,aitozi,aitozi,30/Jul/22 07:47,30/Aug/22 12:31,04/Jun/24 20:41,30/Aug/22 12:31,,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,"When I use the JSON_VALUE function, I found the performance is very poor. It's mainly affected by the heavy lock operation in jsonpath inner LRUCache which is also observed by other systems, eg: [https://github.com/apache/pinot/pull/7409|https://github.com/apache/pinot/pull/7409]

!image-2022-07-30-15-47-34-788.png|width=500,height=250!

So I proposal to use a different cache to replace the current one for better performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/22 07:47;aitozi;image-2022-07-30-15-47-34-788.png;https://issues.apache.org/jira/secure/attachment/13047423/image-2022-07-30-15-47-34-788.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 12:31:08 UTC 2022,,,,,,,,,,"0|z17bww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/22 09:30;aitozi;I opened a PR for this. cc [~jark] [~libenchao] ;;;","30/Aug/22 12:31;jark;Fixed in master: 2220f24925ab5146d5771c3782ed8c0837bb0bc4;;;",,,,,,,,,,,,,,,,,,,
Whether to add field comment for hive table,FLINK-28750,13474310,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,hehuiyuan,hehuiyuan,30/Jul/22 07:23,18/Aug/23 22:35,04/Jun/24 20:41,,1.14.5,,,,,,,,Connectors / Hive,,,,,,,0,auto-deprioritized-minor,pull-request-available,,,,,"Currently,  I have a hive ddl，as follows
{code:java}
""set table.sql-dialect=hive;\n"" +
""CREATE TABLE IF NOT EXISTS myhive.dev.shipu3_test_1125 (\n"" +
""   `id` int COMMENT 'iadddd',\n"" +
""   `cartdid` bigint COMMENT 'aaa',\n"" +
""   `customer` string COMMENT 'vvvv',\n"" +
""   `product` string COMMENT 'cccc',\n"" +
""   `price` double COMMENT '',\n"" +
""   `dt` STRING COMMENT ''\n"" +
"") PARTITIONED BY (dt STRING) STORED AS TEXTFILE TBLPROPERTIES (\n"" +
""  'streaming-source.enable' = 'false',\n"" +
""  'streaming-source.partition.include' = 'all',\n"" +
""  'lookup.join.cache.ttl' = '12 h'\n"" +
"")""; {code}
It is parsed as SqlCreateHiveTable by hive dialect parser. But the field commet is lost.

 

 

!image-2022-07-30-16-36-37-032.png|width=777,height=526!

 

 

 

 

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/22 07:56;hehuiyuan;image-2022-07-30-15-53-03-754.png;https://issues.apache.org/jira/secure/attachment/13047424/image-2022-07-30-15-53-03-754.png","30/Jul/22 08:37;hehuiyuan;image-2022-07-30-16-36-37-032.png;https://issues.apache.org/jira/secure/attachment/13047425/image-2022-07-30-16-36-37-032.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 18 22:35:04 UTC 2023,,,,,,,,,,"0|z17bw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/22 11:42;luoyuxia;We can move on it after finish FLINK-18958;;;","10/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Minor but is unassigned and neither itself nor its Sub-Tasks have been updated for 180 days. I have gone ahead and marked it ""stale-minor"". If this ticket is still Minor, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","18/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-minor"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Minor, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
Add resource type to KubernetesResourceNamespaceMetricGroup scope,FLINK-28749,13474305,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,30/Jul/22 06:48,05/Aug/22 13:11,04/Jun/24 20:41,05/Aug/22 13:11,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"Currently the KubernetesResourceMetricGroup and KubernetesResourceNamespaceMetricGroup doesn't have information about the resource type and different metrics add this differently.

We should unify this and always have the managed resource type FlinkDeployment/FlinkSessionJob in the scope.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 13:11:09 UTC 2022,,,,,,,,,,"0|z17bv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 13:11;gyfora;Merged to main c91ebbd1ae989749948dc6de69255be70c5b7557;;;",,,,,,,,,,,,,,,,,,,,
"Translate ""SELECT DISTINCT"" page into Chinese",FLINK-28748,13474298,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,MaoKing,MaoKing,MaoKing,30/Jul/22 05:16,01/Aug/22 10:03,04/Jun/24 20:41,01/Aug/22 10:03,1.15.1,,,,1.16.0,,,,Documentation,,,,,,,0,pull-request-available,,,,,,"Translate ""{*}SELECT DISTINCT{*}"" page into Chinese:[https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/sql/queries/select-distinct/|http://example.com]
The markdown file location: flink/docs/content.zh/docs/dev/table/sql/queries/select-distinct.md, most of them have been translated, but some part has not been translated into Chinese. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 10:03:35 UTC 2022,,,,,,,,,,"0|z17btk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/22 05:21;MaoKing;[~yunta] [~jark], Hi,I think I could like to translate this page. Could you assign it to me ? I will complete it in today. Thank you very much!;;;","01/Aug/22 10:03;martijnvisser;Fixed in master: ab9e195678a37ad4dc281aeaf28e1dc0ec6fb7f8;;;",,,,,,,,,,,,,,,,,,,
"""target_id can not be missing"" in HTTP statefun request",FLINK-28747,13474284,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stepweiwu,stepweiwu,29/Jul/22 23:24,09/Feb/24 15:48,04/Jun/24 20:41,,statefun-3.0.0,statefun-3.1.1,statefun-3.2.0,,,,,,Stateful Functions,,,,,,,0,,,,,,,"Hi all,

We've suddenly started to see the following exception in our HTTP statefun functions endpoints:

{code}Traceback (most recent call last):
  File ""/src/.venv/lib/python3.9/site-packages/uvicorn/protocols/http/h11_impl.py"", line 403, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""/src/.venv/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py"", line 78, in __call__
    return await self.app(scope, receive, send)
  File ""/src/worker/baseplate_asgi/asgi/baseplate_asgi_middleware.py"", line 37, in __call__
    await span_processor.execute()
  File ""/src/worker/baseplate_asgi/asgi/asgi_http_span_processor.py"", line 61, in execute
    raise e
  File ""/src/worker/baseplate_asgi/asgi/asgi_http_span_processor.py"", line 57, in execute
    await self.app(self.scope, self.receive, self.send)
  File ""/src/.venv/lib/python3.9/site-packages/starlette/applications.py"", line 124, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""/src/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py"", line 184, in __call__
    raise exc
  File ""/src/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py"", line 162, in __call__
    await self.app(scope, receive, _send)
  File ""/src/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py"", line 75, in __call__
    raise exc
  File ""/src/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py"", line 64, in __call__
    await self.app(scope, receive, sender)
  File ""/src/.venv/lib/python3.9/site-packages/starlette/routing.py"", line 680, in __call__
    await route.handle(scope, receive, send)
  File ""/src/.venv/lib/python3.9/site-packages/starlette/routing.py"", line 275, in handle
    await self.app(scope, receive, send)
  File ""/src/.venv/lib/python3.9/site-packages/starlette/routing.py"", line 65, in app
    response = await func(request)
  File ""/src/worker/baseplate_statefun/server/asgi/make_statefun_handler.py"", line 25, in statefun_handler
    result = await handler.handle_async(request_body)
  File ""/src/.venv/lib/python3.9/site-packages/statefun/request_reply_v3.py"", line 262, in handle_async
    msg = Message(target_typename=sdk_address.typename, target_id=sdk_address.id,
  File ""/src/.venv/lib/python3.9/site-packages/statefun/messages.py"", line 42, in __init__
    raise ValueError(""target_id can not be missing""){code}

Interestingly, this has started to happen in three separate Flink deployments at the very same time. The only thing in common between the three deployments is that they consume the same Kafka topics.

No deployments have happened when the issue started happening which was on July 28th 3:05PM. We have since been continuously seeing the error.

We were also able to extract the request that Flink sends to the HTTP statefun endpoint:



{code}{'invocation': {'target': {'namespace': 'com.x.dummy', 'type': 'dummy'}, 'invocations': [{'argument': {'typename': 'type.googleapis.com/v2_event.Event', 'has_value': True, 'value': '-redicated-'}}]}}
{code}

As you can see, no `id` field is present in the `invocation.target` object or the `target_id` was an empty string.

 

This is our module.yaml from one of the Flink deployments:

 
{code}
version: ""3.0""
module:
meta:
type: remote
spec:
endpoints:
 - endpoint:
meta:
kind: io.statefun.endpoints.v1/http
spec:
functions: com.x.dummy/dummy
urlPathTemplate: [http://x-worker-dummy.x-functions:9090/statefun]
timeouts:
call: 2 min
read: 2 min
write: 2 min
maxNumBatchRequests: 100
ingresses:
 - ingress:
meta:
type: io.statefun.kafka/ingress
id: com.x/ingress
spec:
address: x-kafka-0.x.ue1.x.net:9092
consumerGroupId: x-worker-dummy
topics:
 - topic: v2_post_events
valueType: type.googleapis.com/v2_event.Event
targets:
 - com.x.dummy/dummy
startupPosition:
type: group-offsets
autoOffsetResetPosition: earliest
{code}

 

Can you please help us investigate as this is critically impacting our prod setup?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 09 15:48:31 UTC 2024,,,,,,,,,,"0|z17bqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 23:20;stepweiwu;I've tested this issue in the following versions and they all behave the same:
 * 3.2.0
 * 3.1.1
 * 3.0.0

 ;;;","02/Aug/22 14:54;trohrmann;Thanks for reporting this issue [~stepweiwu]. Without knowing the exact details of your StateFun job, I suspect that it could be caused by Kafka messages that only have an empty string as a key specified. This could also explain why you are seeing the problem appear across different services consuming from the same topic.

For some context, StateFun uses the key of a Kafka message as the target id for the StatefulFunction invocation. The target id identifies an instance of the StatefulFunction so that the runtime can associate state with it.

Empty strings should be valid keys, so I believe that this is a bug in the Python SDK where we check {{if not target_id: raise ValueError}} that is executed if the key is empty (https://github.com/apache/flink-statefun/blob/master/statefun-sdk-python/statefun/messages.py#L41).

As a workaround (if you can control it), you could require your Kafka message to require a non-empty key. The proper fix would be to fix the Python SDK.;;;","02/Aug/22 17:19;stepweiwu;Hey [~trohrmann],

thank you very much for taking a look! Yes, you are right, I was able to confirm that we do have empty keys in the Kafka topic (this shouldn't happen but that's a different story).

 The {{ToFunction}} message that I've pasted in the description does not contain {{target.id}}. Is this expected? If the key in Kafka was indeed an empty string, shouldn't Flink send an empty string to the function as well? I would expect {{'id': ''}} to be in the {{target}} object. In this case a check for an empty string would still cause the error.;;;","03/Aug/22 14:52;trohrmann;I think this is Protobuf's behaviour. If Protobuf sees that a field has the default value assigned, then it won't serialize this field. On the receiving end where the message is deserialized, Protobuf will return the default value for a missing field (for a string type it is the empty string).;;;","04/Aug/22 03:08;stepweiwu;Yes, you're right, that seems like protobuf behaviour. 

What's the correct fix here in the python sdk? Remove [the check|https://github.com/apache/flink-statefun/blob/master/statefun-sdk-python/statefun/messages.py#L41] and/or set `target_id` to a random or fixed value?

Thanks again for looking into this!;;;","19/Aug/22 00:47;groot;Thanks [~stepweiwu] for reporting this issues. Do you like to fix the issue or I can take it up to fix it;;;","19/Aug/22 16:14;stepweiwu;[~groot], what's the correct fix here? Allow `None` values for `target_id` or should we set a fixed / random value for `target_id`? I don't know what the implications are of either solution.

If the fix is trivial, I'm happy to do it - I'm just not very familiar with the code base.;;;","20/Sep/22 16:40;stepweiwu;[~groot] / [~trohrmann]quick ping. I don't have enough knowledge on what's the right fix here. Would one of you mind taking a look and getting this fixed? Thank you in advance!;;;","09/Feb/24 15:48;nathantalewis;In Protobuf 3, there is an `optional` label. An unset field could then be distinguished from a field that was set to the default value.

Would adding {{optional}} to https://github.com/apache/flink-statefun/blob/accd75ea0109845c4b4c0ddd74021147af1439d4/statefun-sdk-protos/src/main/protobuf/io/kafka-egress.proto#L28 be enough to provide the SDKs with a way to distinguish between a valid empty string key vs. an invalid unset key? I'm guessing there would have to be other changes elsewhere since that file is for the egress and I don't see any equivalent protobuf file for kafka ingress messages.;;;",,,,,,,,,,,,
Add test for sequential read,FLINK-28746,13474220,13440330,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,29/Jul/22 14:01,01/Aug/22 07:25,04/Jun/24 20:41,01/Aug/22 07:25,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,,,,,,,Add test for sequential read within each bucket,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 07:25:00 UTC 2022,,,,,,,,,,"0|z17bc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 07:25;lzljs3620320;master: f89bcb386d410c8cc3edb771c74a27320be059c2
release-0.2: aad53aba9e683c9ae4c95b13b33025bd7fd94548;;;",,,,,,,,,,,,,,,,,,,,
Support DataStream PythonCoProcessOperator and PythonKeyedCoProcessOperator in Thread Mode,FLINK-28745,13474190,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,29/Jul/22 10:58,02/Aug/22 11:20,04/Jun/24 20:41,02/Aug/22 11:20,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-25724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 11:20:08 UTC 2022,,,,,,,,,,"0|z17b5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 11:20;hxbks2ks;Merged into master via d966c10eaa3312303e5eb8239d8c002179724a10;;;",,,,,,,,,,,,,,,,,,,,
Upgrade Calcite version to 1.31,FLINK-28744,13474166,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,29/Jul/22 09:40,29/Jun/23 22:23,04/Jun/24 20:41,29/Jun/23 16:30,,,,,1.18.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,We should upgrade to Calcite 1.31 so we can benefit from https://issues.apache.org/jira/browse/CALCITE-4865,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27998,,,,FLINK-29319,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 29 16:30:59 UTC 2023,,,,,,,,,,"0|z17b08:",9223372036854775807,"Due to CALCITE-4861 (Optimization of chained CAST calls can lead to unexpected behavior), also Flink's casting behavior has slightly changed. Some corner cases might behave differently now: For example, casting from FLOAT/DOUBLE 9234567891.12 to INT/BIGINT has now Java behavior for overflows.",,,,,,,,,,,,,,,,,,,"01/Aug/22 02:10;xuyangzhong;Just for reminding, the following files should be removed from the Flink code base when upgrading calcite to 1.31

in `org.apahce.calcite.rel.core`:
 * Correlate
 * Filter
 * Intersect
 * Minus
 * SetOp
 * Sort
 * Union
 * Values
 * Window

in `org.apahce.calcite.rel.hint`:
 * HintPredicates
 * NodeTypeHintPredicate

in `org.apahce.calcite.rel.logical`:
 * LogicalCorrelate
 * LogicalFilter
 * LogicalIntersect
 * LogicalMinus
 * LogicalSort
 * LogicalUnion
 * LogicalValues
 * LogicalWindow;;;","18/Apr/23 09:19;Sergey Nuyanzin;Since 1.30.0 Calcite Upgrade (FLINK-27998) has been approved and will be merged the next one should be this.
[~jingzhang] since you are an assignee here I would like to ask if you are ok that I continue here with this update (in my local branch I have update till last commit in Apache Calcite)?
Or will you work on this? I can help anyway if help is required 
;;;","20/Apr/23 11:06;Sergey Nuyanzin;After a short sync offline (thanks to [~jark] ) it was agreed that I can continue with this issue myself
For that reason I reassign it to myself;;;","08/May/23 14:01;Sergey Nuyanzin;regarding {{NodeTypeHintPredicate}} 
it can not be removed in 1.31.0 upgrade since in FLINK-28850 there was a cherry pick from CALCITE-5251 (Calcite 1.32.0);;;","29/Jun/23 16:30;twalthr;Fixed in master: ab6a81118b45c06e822f3c77468dd7d0afb66b9e;;;",,,,,,,,,,,,,,,,
Support validating the determinism for StreamPhysicalMatchRecognize,FLINK-28743,13474149,13447630,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,29/Jul/22 08:21,15/Jul/23 09:16,04/Jun/24 20:42,14/Jul/23 02:41,,,,,1.18.0,,,,,,,,,,,0,pull-request-available,,,,,,"MatchRecognize has complex expressions and is not commonly used in traditional SQLs, so mark this as a minor issue (for 1.16)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 14 02:41:45 UTC 2023,,,,,,,,,,"0|z17awg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jul/23 14:49;twalthr;[~lincoln.86xy] I'm wondering whether we should still improve this for 1.18. At least we could avoid throwing an exception for all cases. The option is called ""TRY_RESOLVE"" so it should be ok to not fully meet the determinism requirements.;;;","11/Jul/23 09:25;twalthr;From looking at the code, it seems the behavior would be similar to an OVER window. Only insert only + some aggregation.;;;","11/Jul/23 12:44;lincoln.86xy;[~twalthr] Sorry for the late reply. Yes, I also confirmed with [~dianfu] offline that the MatchRecognize won't support updating inputs in the foreseeable future either, so the analytic support for this operator can be relatively simple, and I'm working on a pr that should be ready in time for the 1.18 release.;;;","14/Jul/23 02:41;lincoln.86xy;fixed in master: 8829b5e60a71b871462d1d2bb849c926b0de9b80;;;",,,,,,,,,,,,,,,,,
"Table.to_pandas fails with lit(""xxx"")",FLINK-28742,13474147,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,29/Jul/22 08:11,10/Jan/23 01:43,04/Jun/24 20:42,10/Jan/23 01:42,1.15.0,,,,1.15.4,1.16.1,1.17.0,,API / Python,,,,,,,0,pull-request-available,,,,,,"Table.to_pandas method throws the following exception when the table contains lit(""anyString"").

 
{code:none}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
: java.lang.UnsupportedOperationException: Python vectorized UDF doesn't support logical type CHAR(3) NOT NULL currently.
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:743)
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:617)
    at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:62)
    at org.apache.flink.table.types.logical.CharType.accept(CharType.java:148)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowField(ArrowUtils.java:189)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.lambda$toArrowSchema$0(ArrowUtils.java:180)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowSchema(ArrowUtils.java:181)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:483)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)
 {code}
 

The code to reproduce the problem
{code:python}
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

src_table = t_env.from_data_stream(
    env.from_collection([1, 2], type_info=BasicTypeInfo.INT_TYPE_INFO())
)

table = src_table.select(expr.lit(""123""))
# table.execute().print()
print(table.to_pandas()){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 10 01:42:58 UTC 2023,,,,,,,,,,"0|z17aw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 01:42;dianfu;Fixed in:
- master via 0f14f13122e2df5e4a2396871936a5b6ff47cd12
- release-1.16 via 2b77c837636e1df2d527f0097726481ce198362b
- release-1.15 via 310b8fb4206fe665e9c70149df37ffacf9e20b91

 ;;;",,,,,,,,,,,,,,,,,,,,
Unexpected result if insert 'false' to boolean column,FLINK-28741,13474143,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jingzhang,jingzhang,29/Jul/22 07:43,21/Aug/23 22:35,04/Jun/24 20:42,,1.15.0,1.15.1,,,,,,,Connectors / Hive,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,"Using hive dialect to insert a string 'false' to boolean column, the result is true. It seems to treat all non-empty string as true.

The error could be reproduced in the following ITCase.
{code:java}
@Test
public void testUnExpectedResult() throws ExecutionException, InterruptedException {
    HiveModule hiveModule = new HiveModule(hiveCatalog.getHiveVersion());
    CoreModule coreModule = CoreModule.INSTANCE;
    for (String loaded : tableEnv.listModules()) {
        tableEnv.unloadModule(loaded);
    }
    tableEnv.loadModule(""hive"", hiveModule);
    tableEnv.loadModule(""core"", coreModule);
// create source table
    tableEnv.executeSql(
            ""CREATE TABLE test_table (params string) PARTITIONED BY (`p_date` string)"");
// prepare a data which value is 'false'
    tableEnv.executeSql(""insert overwrite test_table partition(p_date = '20220612') values ('false')"")
            .await();
// create target table which only contain one boolean column 
    tableEnv.executeSql(
            ""CREATE TABLE target_table (flag boolean) PARTITIONED BY (`p_date` string)"");
// 
    tableEnv.executeSql(
            ""insert overwrite table target_table partition(p_date = '20220724') ""
                    + ""SELECT params FROM test_table WHERE p_date='20220612'"").await();
    TableImpl flinkTable =
            (TableImpl) tableEnv.sqlQuery(""select flag from target_table where p_date = '20220724'"");
   List<Row> results = CollectionUtil.iteratorToList(flinkTable.execute().collect());
    assertEquals(
            ""[false]"", results.toString());
} {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:20 UTC 2023,,,,,,,,,,"0|z17av4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 09:20;luoyuxia;Thanks for reporting. Yes, you're right. It'll  treat all non-empty string as true and it's the Hive2's behavior. To verify it, I test with my hive 2.3.2, and execute the sql you post. The result does be ""{*}true"".{*}

The behavior is not stardard and Hive commutity also notices that  with a fix https://issues.apache.org/jira/browse/HIVE-15939. But the fix is only merge into Hive3.

It's simple to fix it in Flink. But I'm wondering is there some  Hive2  users may expect the string *""false""*  to be cast to the boolean value *true* since that what Hive2's behavior.

 

 

 

 ;;;","02/Aug/22 03:41;luoyuxia;[~jingzhang] Just another quick question. Should the empty string *""""* be cast to *false* or *null* or {*}throw exception{*}?  I'm wondering whether we should follow Hive's style or other style.;;;","22/Nov/22 11:07;jingzhang;[~luoyuxia] Good question.

I test this behavior in spark, the result is different with hive 2.X

 cast('XXXX' as boolean): null
 cast ('' as boolean): null

 cast('false' as boolean) : false
 cast ('true' as boolean) : true;;;","22/Nov/22 11:08;jingzhang;>  I'm wondering is there some  Hive2  users may expect the string *""false""*  to be cast to the boolean value *true* since that what Hive2's behavior.

I'm afraid it is not what the user expected that cast ('false' to boolean) returns true.

However there might different voices for cast('' to boolean) or cast ('XXX' to boolean).;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,
Support CsvBulkWriter in PyFlink,FLINK-28740,13474139,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,29/Jul/22 07:28,03/Aug/22 11:35,04/Jun/24 20:42,03/Aug/22 11:35,,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,"Java users could just use a lambda to construct a BulkWriterFactory for CsvBulkWriter, but for Python users, they still need a direct-to-use method to create a factory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 11:35:44 UTC 2022,,,,,,,,,,"0|z17au8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 11:35;dianfu;Merged to master via 17e1920bcd2780e12f5ad318faa0db68c4f7fec0;;;",,,,,,,,,,,,,,,,,,,,
Illegal state for checkpoint in LogisticRegressionTest.test_get_model_data ,FLINK-28739,13474131,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,gaoyunhaii,zhangzp,zhangzp,29/Jul/22 07:08,24/Aug/22 08:31,04/Jun/24 20:42,24/Aug/22 08:31,ml-2.1.0,,,,ml-2.1.0,ml-2.2.0,,,,,,,,,,0,,,,,,,"Caused by: java.lang.IllegalStateException: Should be blocked by checkpoint. 
[157|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:158]E at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
[158|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:159]E at org.apache.flink.runtime.io.network.partition.PipelinedSubpartition.resumeConsumption(PipelinedSubpartition.java:381) 
[159|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:160]E at org.apache.flink.runtime.io.network.partition.PipelinedSubpartitionView.resumeConsumption(PipelinedSubpartitionView.java:79) 
[160|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:161]E at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.resumeConsumption(LocalInputChannel.java:286) 
[161|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:162]E at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.resumeConsumption(SingleInputGate.java:941) 
[162|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:163]E at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.resumeConsumption(InputGateWithMetrics.java:60) 
[163|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:164]E at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:223) 
[164|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:165]E at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerUnfinishedChannelsCheckpoint(StreamTask.java:1172) 
[165|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:166]E at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$11(StreamTask.java:1089) 
[166|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:167]E at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) 
[167|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:168]E at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) 
[168|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:169]E at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.yield(MailboxExecutorImpl.java:86) 
[169|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:170]E at org.apache.flink.iteration.operator.HeadOperator.endInput(HeadOperator.java:407) 
[170|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:171]E at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96) 
[171|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:172]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97) 
[172|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:173]E at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68) 
[173|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:174]E at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519) 
[174|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:175]E at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) 
[175|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:176]E at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804) 
[176|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:177]E at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753) 
[177|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:178]E at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) 
[178|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:179]E at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) 
[179|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:180]E at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) 
[180|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:181]E at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) 
[181|https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true#step:9:182]E at java.lang.Thread.run(Thread.java:750)
 
 
https://github.com/apache/flink-ml/runs/7351668847?check_suite_focus=true",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 24 08:31:56 UTC 2022,,,,,,,,,,"0|z17asg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Aug/22 08:31;zhangzp;This is a duplicate of https://issues.apache.org/jira/browse/FLINK-25148;;;",,,,,,,,,,,,,,,,,,,,
[doc] Add a user doc about the correctness for non-deterministic updates,FLINK-28738,13474124,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln.86xy,lincoln.86xy,29/Jul/22 06:41,19/Sep/22 02:56,04/Jun/24 20:42,19/Sep/22 02:56,,,,,1.16.0,,,,Documentation,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 19 02:56:00 UTC 2022,,,,,,,,,,"0|z17aqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/22 02:56;godfreyhe;Fixed in master: a02b2c232ea2fb1b22bd0e5c290e4c6f0217549b
in 1.16.0: be73c9695d01e3d3164b6c89342a1e41fa4ea450;;;",,,,,,,,,,,,,,,,,,,,
FlinkRelMdUpsertKeys should consider the case when the condition of a calc node is non-deterministic,FLINK-28737,13474117,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lincoln.86xy,lincoln.86xy,29/Jul/22 05:55,11/Mar/24 12:43,04/Jun/24 20:42,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,,0,,,,,,,"Currently FlinkRelMdUpsertKeys does consider the case when the condition of a calc node is non-deterministic, this should be fixed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-29 05:55:12.0,,,,,,,,,,"0|z17apc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add APPROX_PERCENTILE function,FLINK-28736,13474042,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jmahonin,jmahonin,28/Jul/22 15:52,29/Jul/22 14:26,04/Jun/24 20:42,,1.15.1,,,,,,,,Table SQL / API,,,,,,,0,,,,,,,"We have an {{APPROX_PERCENTILE}} UDF that we believe may be useful to the broader community. It's a rather simple implementation that wraps TDigest to return approximate quantile/percentile data in both batch and streaming mode.

I'm somewhat torn as to how to properly contribute this. Following Calcite conventions, I believe this would qualify as an {{APPROXIMATE PERCENTILE_DISC}} function, although that would require Calcite >= 1.28 (FLINK-21239, FLINK-27998). 

Alternatively, perhaps this could simply be dropped in as a new function that's not backed by Calcite, although it's not immediately clear to me how to proceed with that, though I'm happy to take guidance here.

This is a gist of the implementation:
https://gist.github.com/jmahonin/d75150999af30bc78bdf00c7b0ecbd4f",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 14:26:33 UTC 2022,,,,,,,,,,"0|z17a8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 05:57;martijnvisser;I would prefer to wait with this implementation to the release after Flink 1.16, since a) the release branch for that one will be cut in two weeks and b) in the release after that one, we'll update Calcite to 1.31. ;;;","29/Jul/22 14:26;jmahonin;That seems reasonable. I'll revisit this after Flink 1.16.;;;",,,,,,,,,,,,,,,,,,,
Deprecate host/web-ui-port parameter of jobmanager.sh,FLINK-28735,13474026,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,28/Jul/22 14:39,29/Sep/22 14:18,04/Jun/24 20:42,24/Aug/22 07:36,,,,,1.16.0,,,,Deployment / Scripts,,,,,,,0,pull-request-available,,,,,,"If we fix FLINK-28733 we could while we're at it deprecate these 2 parameters, since you can then also control them via dynamic properties.

This would also subsume FLINK-21038.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21038,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 24 07:36:29 UTC 2022,,,,,,,,,,"0|z17a54:",9223372036854775807,The host/web-ui-port parameters of the jobmanager.sh script have been deprecated. These can (and should) be specified with the corresponding options as dynamic properties.,,,,,,,,,,,,,,,,,,,"24/Aug/22 07:36;chesnay;master: cb5076513685ba7b84b9738d51644b6071fc8585;;;",,,,,,,,,,,,,,,,,,,,
Configurable role(binding) names in the helm chart,FLINK-28734,13474023,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,mbalassi,mbalassi,28/Jul/22 14:37,29/Aug/22 10:11,04/Jun/24 20:42,29/Aug/22 10:11,kubernetes-operator-1.2.0,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"The names of the roles and rolebindings in the helm chart are not yet configurable, we should improve this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 29 10:11:02 UTC 2022,,,,,,,,,,"0|z17a4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Aug/22 10:11;gyfora;merged to main
8edfc0592c962dcc538fe3d1f0acd4c3f716e537;;;",,,,,,,,,,,,,,,,,,,,
jobmanager.sh should support dynamic properties,FLINK-28733,13474007,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,28/Jul/22 13:36,29/Sep/22 13:01,04/Jun/24 20:42,05/Aug/22 21:47,,,,,1.15.3,1.16.0,,,Deployment / Scripts,,,,,,,0,pull-request-available,,,,,,"{{jobmanager.sh}} throws away all arguments after the host/webui-port settings, in contrast to other scripts like {{taskmanager.sh}},{{ historyserver.sh}} or {{standalone-job.sh}}.

This prevents users from using dynamic properties.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 21:47:49 UTC 2022,,,,,,,,,,"0|z17a0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 21:47;chesnay;master: ff2f4cb624ab84c43f2fd0daea2daa8ba74b4169
1.15: dbfdebaa1c0eac5346871e023a02662b8301c295;;;",,,,,,,,,,,,,,,,,,,,
Deprecate ambiguous StateTTLConfig#cleanFullSnapshot API,FLINK-28732,13473998,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,yunta,yunta,yunta,28/Jul/22 13:00,11/Mar/24 12:43,04/Jun/24 20:42,,1.15.0,1.15.1,,,1.20.0,,,,,,,,,,,0,pull-request-available,stale-assigned,,,,,"After we introduce the native savepoint in flink-1.15, the semantics of the full snapshot changed. ""Full snapshot"" mainly refers to the newly ""Canonical Savepoint"", and the previous API of {{StateTTLConfig.Builder#cleanupFullSnapshot}} ([https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/datastream/fault-tolerance/state/#cleanup-in-full-snapshot)] becomes ambiguous. Even we trigger a native savepoint, we cannot cleanup expired data in the generated snapshot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:16 UTC 2023,,,,,,,,,,"0|z179yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Logging of global config should take dynamic properties into account,FLINK-28731,13473987,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,28/Jul/22 12:06,09/Aug/22 19:02,04/Jun/24 20:42,09/Aug/22 19:02,,,,,1.16.0,,,,Runtime / Configuration,,,,,,,0,pull-request-available,,,,,,"When a Flink process is started they first thing they do is load the global configuration from the flink-conf.yaml and log what was read.

When additional options were set by the user via dynamic properties, then they are not reflected in the logging of the global configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 19:02:19 UTC 2022,,,,,,,,,,"0|z179wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 19:02;chesnay;master: 6b20433e7f007a97a1a958ef28e1c8f58277d940;;;",,,,,,,,,,,,,,,,,,,,
Add Tez execution engine test for Table Store Hive connector,FLINK-28730,13473986,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,28/Jul/22 11:56,29/Jul/22 02:36,04/Jun/24 20:42,29/Jul/22 02:36,table-store-0.2.0,table-store-0.3.0,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,Table Store Hive connector is supposed to support both MR and Tez engine. However current tests only runs on MR. We need to add Tez tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 02:36:09 UTC 2022,,,,,,,,,,"0|z179w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 02:36;lzljs3620320;master: 476b1834d5765043d08f2f7421afda1486e77160
release-0.2: 1bbbe2e81d9e269f62700a9749eb2c8a7e984f86;;;",,,,,,,,,,,,,,,,,,,,
flink hive catalog don't support jdk11,FLINK-28729,13473953,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,jeff-zou,jeff-zou,28/Jul/22 09:06,14/Nov/22 03:58,04/Jun/24 20:42,14/Nov/22 03:58,1.15.1,,,,,,,,Connectors / Hive,,,,,,,0,,,,,,,"when I upgraded jdk to 11,I got the following error:
{code:java}
<dependency>
         <groupId>org.apache.flink</groupId>
         <artifactId>flink-sql-connector-hive-3.1.2_2.12</artifactId>
         <version>1.15.1</version>
     </dependency> {code}
{code:java}
// error
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1654)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:80)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:115)
    ... 84 more
Caused by: java.lang.reflect.InvocationTargetException
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)
    ... 87 more
Caused by: MetaException(message:Got exception: java.lang.ClassCastException class [Ljava.lang.Object; cannot be cast to class [Ljava.net.URI; ([Ljava.lang.Object; and [Ljava.net.URI; are in module java.base of loader 'bootstrap'))
    at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1342)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:278)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:210)
    ... 92 more
Process finished with exit code -1
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-21508,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 13:39:12 UTC 2022,,,,,,,,,,"0|z179ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 09:11;martijnvisser;Isn't this because Hive doesn't support Java 11 at all? ;;;","28/Jul/22 13:39;luoyuxia;[~jeff-zou]  There's a fix for it in Hive community https://issues.apache.org/jira/browse/HIVE-21508.

But it's a pity that for Hive3, seems the fixed version  haven't been released, maybe we have to wait Hive 3.2.0 to be released.

 ;;;",,,,,,,,,,,,,,,,,,,
Support to set the end offset for streaming connector,FLINK-28728,13473937,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Incomplete,,tanjialiang,tanjialiang,28/Jul/22 07:55,02/Aug/22 08:11,04/Jun/24 20:42,02/Aug/22 08:11,,,,,,,,,,,,,,,,0,,,,,,,"Like MQ or CDC connector, it support set the startup mode for reading, but not support set the end offset/position. When the MQ's or CDC's data is not frequently,  consume their data in period can cut lots of kubernetes/YARN resource, such as startup from earliest, and end to the current offset, save current offset in savepoint, and using the savepoint to startup in the next period time, in a loop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 08:11:07 UTC 2022,,,,,,,,,,"0|z179lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 13:05;martijnvisser;[~tanjialiang] Which connector are you talking about? Keep in mind that specifying the end offset/position means that the connector will run in bounded/batch mode and no longer in streaming mode. ;;;","02/Aug/22 08:05;tanjialiang;[~martijnvisser] Such as kafka, when kafka's data is not frequently, i would like to consume it's data by scheduing rather than a long-term running job, because a long-term running job will wasting the cluster resource. I think this kind of job running in batch is ok, but i don't known if the job running in batch, can it do savepoint when the job batch finished? So that i can restore the job from savepoint in the next schedule.;;;","02/Aug/22 08:11;martijnvisser;[~tanjialiang] The Flink Kafka Datastream connector can already be used for specifying the offsets and therefore running in batch mode. If you do a savepoint, then your job will restart next time with the previously defined offsets. I do think that it makes sense that some connectors can be run in batch mode, but there are already Jira tickets for that. For your use case, I see more value in minimising cluster resources in case you have low volume. There are already solutions for that in Flink and I'm sure that more improvements are planned.

I'm closing the Jira ticket because it lacks clarity and more detailed/relevant tickets already exist. ;;;",,,,,,,,,,,,,,,,,,
Flink Source supports SupportsLimitPushDown,FLINK-28727,13473925,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,28/Jul/22 07:12,29/Jul/22 10:27,04/Jun/24 20:42,29/Jul/22 10:26,,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 10:26:17 UTC 2022,,,,,,,,,,"0|z179io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 02:26;nicholasjiang;[~lzljs3620320], could you please assign this ticket to me? I will push a PR for this support.;;;","29/Jul/22 02:44;lzljs3620320;[~nicholasjiang] Thanks, assigned;;;","29/Jul/22 10:26;lzljs3620320;master: 779afb51190ca30ede9e4027972f5596523c409b;;;",,,,,,,,,,,,,,,,,,
CompileException: BoundedOverAggregateHelper must implement method AggsHandeFunction.setWindowSize(int),FLINK-28726,13473921,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,godfreyhe,godfreyhe,28/Jul/22 06:52,11/Mar/24 12:44,04/Jun/24 20:42,,1.16.0,,,,1.20.0,,,,Table SQL / Runtime,,,,,,,0,,,,,,, !image-2022-07-28-14-52-08-729.png! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/22 06:52;godfreyhe;image-2022-07-28-14-52-08-729.png;https://issues.apache.org/jira/secure/attachment/13047314/image-2022-07-28-14-52-08-729.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 06:52:50 UTC 2022,,,,,,,,,,"0|z179hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 06:52;godfreyhe;This bug is introduced in [FLINK-27618] cc [~luoyuxia];;;",,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator taskManager:     replicas: 2 error,FLINK-28725,13473905,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,lizu18xz,lizu18xz,28/Jul/22 05:06,28/Jul/22 06:51,04/Jun/24 20:42,28/Jul/22 06:24,,,,,,,,,,,,,,,,0,,,,,,,"version:v1.1.0

 

taskManager:
    replicas: 2
    resource:
      memory: ""1024m""
      cpu: 1

 

error validating data: ValidationError(FlinkDeployment.spec.taskManager): unknown field ""replicas"" in org.apache.flink.v1beta1.FlinkDeployment.spec.taskManager; if you choose to ignore these errors, turn validation off with --validate=false",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 06:09:46 UTC 2022,,,,,,,,,,"0|z179e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 06:09;wangyang0918;You need to upgrade the CRD if you want to use the replicas.

 

https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.1/docs/operations/upgrade/#1-upgrading-the-crd;;;",,,,,,,,,,,,,,,,,,,,
Test,FLINK-28724,13473901,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Done,,Rufy666,Rufy666,28/Jul/22 04:00,28/Jul/22 04:06,04/Jun/24 20:42,28/Jul/22 04:06,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-28 04:00:49.0,,,,,,,,,,"0|z179dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix json format failed to serialize the MapData when its key is not STRING ,FLINK-28723,13473888,13451461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,28/Jul/22 01:46,09/Aug/22 15:58,04/Jun/24 20:42,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,,,"Currently, the JSON format only supports serializing the Map when its key is STRING. We may convert the key to a JSON string. For example, we can convert the `MAP<ARRAY<INT>, ARRAY<INT>>` to the following string.
{code:java}
{
  ""[1, 2, 3]"": [
    1,
    2,
    3
  ]
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-28 01:46:58.0,,,,,,,,,,"0|z179ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Source should use .equals() for Integer comparison,FLINK-28722,13473868,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mason6345,mason6345,27/Jul/22 21:59,16/Aug/22 18:38,04/Jun/24 20:42,27/Jul/22 22:11,1.15.1,,,,,,,,Connectors / Common,,,,,,,0,,,,,,,"HybridSource should use .equals() for Integer comparison in filtering out the underlying sources. This causes the HybridSource to stop working when it hits the 128th source (would not work for anything past 127 sources).

https://github.com/apache/flink/blob/release-1.14.3-rc1/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/hybrid/HybridSourceSplitEnumerator.java#L358 

A user reported this issue here: https://lists.apache.org/thread/7h2rblsdt7rjf85q9mhfht77bghtbswh",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27529,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 22:12:18 UTC 2022,,,,,,,,,,"0|z17960:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 22:03;mason6345;[~thw] [~martijnvisser] mind if I take a look?;;;","27/Jul/22 22:12;thw;[~mason6345] please take a look at the PR in linked JIRA.;;;",,,,,,,,,,,,,,,,,,,
Support Protobuf in DataStream API,FLINK-28721,13473845,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,27/Jul/22 18:48,27/Jul/22 18:48,04/Jun/24 20:42,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,,,"With FLINK-18202 merged and planned to be released in Flink 1.16, Flink will have support for Protobuf for Table API and SQL applications. Flink should also have support for Protobuf for DataStream API users, like already exists for CSV and Avro. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18202,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-27 18:48:01.0,,,,,,,,,,"0|z1790w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Hive partition when flink has no data to write,FLINK-28720,13473775,13421719,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tartarus,tartarus,tartarus,27/Jul/22 12:01,04/Aug/22 14:30,04/Jun/24 20:42,04/Aug/22 14:30,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"When writing data to a specified partition (static partition) of a Hive table with Flink SQL, the partition should be added just like Hive/Spark regardless of whether there is data written or not.

we should also ensure that Insert into and insert overwrite semantics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 14:30:27 UTC 2022,,,,,,,,,,"0|z178lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 14:30;jark;Fixed in master: 6fe0d1dbfc58b80393972855da9d58c237f29376;;;",,,,,,,,,,,,,,,,,,,,
Mapping a data source before window aggregation causes Flink to stop handle late events correctly.,FLINK-28719,13473773,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Information Provided,,Mykeo,Mykeo,27/Jul/22 11:55,31/Aug/22 09:09,04/Jun/24 20:42,31/Aug/22 09:09,1.15.1,,,,,,,,API / DataStream,,,,,,,0,,,,,,,"I have created a [repository|https://github.com/mykytamykhailenko/flink-map-with-issue] where I describe this issue in detail. 

I have provided a few tests and source code so that you can reproduce the issue on your own machine. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 09:42:12 UTC 2022,,,,,,,,,,"0|z178kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 15:00;chesnay;Your test only works when the elements arrive at the window function in the same order as in the original sequence.

But this is not the case since you are not setting a parallelism and the MiniClusterWithClientResource isn't used properly; it will use some parallelism > 1, causing the {{mapWith}} to effectively shuffle values.

This results in the record/watermark streams being interlaced, resulting in non-deterministic output.;;;","01/Aug/22 15:22;chesnay;So let's dive into what is happening. The source runs with a parallelism of 1, and let's say for simplicity that the map runs with p=7 (one subtask for each record).
The sink send one record to one map subtask, and then the watermark to all map subtasks. These just forward what they receive to the window operator.

We can thus visualize the inputs to the window function as a series of stacks, one for each map subtask.

(X is an element where the timestamp is X, WY is a watermark with timestamp Y).
||op1||op2||op3||op4||op5||op6||op7||
|W7|W7|W7|W7|W7|W7|W7|
| | | | | | |7|
|W4|W4|W4|W4|W4|W4|W4|
| | | | | |4| |
|W3|W3|W3|W3|W3|W3|W3|
| | | | |3| | |
|W6|W6|W6|W6|W6|W6|W6|
| | | |6| | | |
|W5|W5|W5|W5|W5|W5|W5|
| | |5| | | | |
|W2|W2|W2|W2|W2|W2|W2|
| |2| | | | | |
|W1|W1|W1|W1|W1|W1|W1|
|1| | | | | | |

Now consume records in an arbitrary order (i.e., pull data from the bottom of a stack until you reach the value). Whenever the watermarks from each input is greater or equal T, then the window considers T as the current time.

Let's say we consume the input of op1 and op2 completely, and from all other inputs consume all watermarks until we hit a record in each.

The inputs then only contain this:
||op1||op2||op3||op4||op5||op6||op7||
| | |W7|W7|W7|W7|W7|
| | | | | | |7|
| | |W4|W4|W4|W4| |
| | | | | |4| |
| | |W3|W3|W3| | |
| | | | |3| | |
| | |W6|W6| | | |
| | | |6| | | |
| | |W5| | | | |
| | |5| | | | |

Whereas the current watermark for each input are this:
|W7|W7|W2|W5|W6|W6|W6|

Since 2 is the current watermark we can fire the first window (from timestamp 0-2).

This is the point where things get interesting. The next window to be fired ranges from 3-5.

If we now consume an element from op3, then we read the element 5 and the watermark 5.

Our watermarks would then look like this:
|W7|W7|W5|W5|W6|W6|W6|

So we fire the next window, only containing element 5.

However, let's revert back, and instead first read records from other streams, like element 4 and 3.
These still fit into the current window (3-5)
||op1||op2||op3||op4||op5||op6||op7||
| | |W7|W7| | |W7|
| | | | | | |7|
| | |W4|W4| | | |
| | | | | | | |
| | |W3|W3| | | |
| | | | | | | |
| | |W6|W6| | | |
| | | |6| | | |
| | |W5| | | | |
| | |5| | | | |
Now let's look at the watermarks:
|W7|W7|W2|W5|W7|W7|W6|

Since we haven't read from op3 the current time is still 2, so these elements are now not considered late.
Now we read 5, bumping the time to 5, firing the window, containing 3,4 and 5.;;;","08/Aug/22 12:29;Mykeo;Sorry I haven't answered you earlier [~chesnay]. I have read through your explanation and that helped a lot, thank you!  ;;;","12/Aug/22 08:12;Mykeo;Actually, I think I hurried a little. Could you answer a few more questions, [~chesnay]?

_Let's say we consume the input of op1 and op2 completely, and from all other inputs consume all watermarks until we hit a record in each._

Could you elaborate on this point? 
 * As far as I understand, op1 and op2 should have watermark 1 and 2 respectively, because those subtasks don't have any events in them (besides 1 and 2, of course, which create those watermarks). Then, why do they get the maximum watermark of 7 after first step?
 * Also, why after op1 and op2, op4 and op5 get consumed? Is there any strategy that dictates in which order to process subtasks?
 * Also, why op3 to op7 have such watermarks: W2, W5, W6, W6,  W6, after first step? I thought those subtasks should have watermark of Long.MinValue, because there were no elements before?;;;","12/Aug/22 09:42;chesnay;??As far as I understand, op1 and op2 should have watermark 1 and 2 respectively, because those subtasks don't have any events in them (besides 1 and 2, of course, which create those watermarks). Then, why do they get the maximum watermark of 7 after first step???

Watermarks are broadcasted to all downstream operators. So, if a source emits watermark 7, then all map subtasks get it as an input.

??Also, why after op1 and op2, op4 and op5 get consumed? Is there any strategy that dictates in which order to process subtasks???

Whichever arrives first at the downstream operator. You have 7 map subtasks all sending data to the window operators at the same time, and the order in which they arrive is not deterministic. So they _may_ arrive in a perfect round-robin pattern, or sequentially, or in any other pattern. The only guarantee is that the elements from a particular map subtask arrive in the same order that they were sent in.

??Also, why op3 to op7 have such watermarks: W2, W5, W6, W6,  W6, after first step? I thought those subtasks should have watermark of Long.MinValue, because there were no elements before???

The above 2 comments should answer it; all watermarks are sent to all map subtasks, and are consumed by the window operator in any order.;;;",,,,,,,,,,,,,,,,
SinkSavepointITCase.testRecoverFromSavepoint is unstable,FLINK-28718,13473771,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,27/Jul/22 11:35,19/Mar/23 05:47,04/Jun/24 20:42,19/Mar/23 05:47,,,,,table-store-0.4.0,,,,Table Store,,,,,,,0,,,,,,,"https://github.com/apache/flink-table-store/runs/7537817210?check_suite_focus=true


{code:java}
Error:  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 185.274 s <<< FAILURE! - in org.apache.flink.table.store.connector.sink.SinkSavepointITCase
Error:  testRecoverFromSavepoint  Time elapsed: 180.157 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 180000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.table.store.connector.sink.SinkSavepointITCase.testRecoverFromSavepoint(SinkSavepointITCase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-27 11:35:30.0,,,,,,,,,,"0|z178kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive connector will throw exception if primary key fields are not selected,FLINK-28717,13473770,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,27/Jul/22 11:24,28/Jul/22 02:31,04/Jun/24 20:42,28/Jul/22 02:31,table-store-0.2.0,table-store-0.3.0,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Table Store Hive connector implement projection pushdown by reading desired fields and setting other unread fields to null. However the nullability of primary key fields are not null, so `RowData.FieldGetter` will not check for null values for these types. This may cause exception when primary key fields are not selected and are set to null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 02:31:44 UTC 2022,,,,,,,,,,"0|z178k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 02:31;lzljs3620320;master: 334c6159e546ba54aa1c38154882122758604d3d
release-0.2: ceab29fda7512b550a2de4d9cff0a90a8cd1d3c8;;;",,,,,,,,,,,,,,,,,,,,
 uploading multiple files/form datas fail randomly when use rest api,FLINK-28716,13473757,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hehuiyuan,hehuiyuan,27/Jul/22 10:29,27/Jul/22 12:02,04/Jun/24 20:42,27/Jul/22 11:29,1.14.0,,,,1.15.0,,,,,,,,,,,0,,,,,,,"It can happen error randomly when use `jars/upload` rest api.

[https://github.com/eclipse-vertx/vert.x/issues/3949]

Can you help me to look at this question?
{code:java}
java.lang.IndexOutOfBoundsException: index: 1804, length: 1 (expected: range(0, 1804))
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkRangeBounds(AbstractByteBuf.java:1390)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkIndex0(AbstractByteBuf.java:1397)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1384)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1379)
    at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.getByte(AbstractByteBuf.java:355)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostBodyUtil.findDelimiter(HttpPostBodyUtil.java:238)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.loadDataMultipartOptimized(HttpPostMultipartRequestDecoder.java:1172)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.getFileUpload(HttpPostMultipartRequestDecoder.java:926)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.decodeMultipart(HttpPostMultipartRequestDecoder.java:572)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.findMultipartDisposition(HttpPostMultipartRequestDecoder.java:797)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.decodeMultipart(HttpPostMultipartRequestDecoder.java:511)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.findMultipartDelimiter(HttpPostMultipartRequestDecoder.java:663)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.decodeMultipart(HttpPostMultipartRequestDecoder.java:498)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.parseBodyMultipart(HttpPostMultipartRequestDecoder.java:463)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.parseBody(HttpPostMultipartRequestDecoder.java:432)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.offer(HttpPostMultipartRequestDecoder.java:347)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostMultipartRequestDecoder.offer(HttpPostMultipartRequestDecoder.java:54)
    at org.apache.flink.shaded.netty4.io.netty.handler.codec.http.multipart.HttpPostRequestDecoder.offer(HttpPostRequestDecoder.java:223)
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:176)
    at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:71) {code}",,,,,,,,,,,,,,,,,,,FLINK-25016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 11:28:06 UTC 2022,,,,,,,,,,"0|z178hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 11:18;chesnay;Which Flink version did you use? According to the issue you linked this was a Netty bug that has been fixed in 4.1.66, and thus should not occur in Flink 1.15.0.;;;","27/Jul/22 11:22;hehuiyuan;Hi [~chesnay] , flink1.14.

Yes, it is fixed in 4.1.66 version

https://github.com/netty/netty/pull/11335;;;","27/Jul/22 11:28;chesnay;Then you will need to upgrade to 1.15.0.;;;",,,,,,,,,,,,,,,,,,
Throw better exception when file not found in reading,FLINK-28715,13473756,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Jul/22 10:23,29/Jul/22 05:53,04/Jun/24 20:42,29/Jul/22 05:53,,,,,table-store-0.2.0,,,,Table Store,,,,,,,1,pull-request-available,,,,,,"When reading a file, if it is found that the file does not exist, it directly throws a file not found exception, which is often difficult for users to understand.
We can make it more clear in the exception message, e.g.
The file cannot be found, this may be because the read is too slow and the previous snapshot expired, you can configure a larger snapshot.time-retained or speed up your read.


Caused by: java.io.FileNotFoundException: File does not exist: 
at org.apache.flink.table.store.file.utils.FileUtils.getFileSize(FileUtils.java:94) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]
at org.apache.flink.table.store.file.data.DataFileReader$DataFileRecordReader.<init>(DataFileReader.java:86) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 05:53:49 UTC 2022,,,,,,,,,,"0|z178h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 05:53;lzljs3620320;master: 487b05e6d9162d65c1d83d42286d735061864dc1
release-0.2: 82d35ea7cff686967d0c7d63af2c7c24ee6dd16d;;;",,,,,,,,,,,,,,,,,,,,
Resolve CVEs from beam-vendor-grpc-1_26_0-0.3,FLINK-28714,13473754,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,bilna123,bilna123,27/Jul/22 10:19,27/Jul/22 11:26,04/Jun/24 20:42,27/Jul/22 11:26,1.13.6,,,,,,,,API / Python,,,,,,,0,,,,,,,"The following CVEs comes from the transient dependency, BouncyCastle:1.54 through Apache Beam dependency in flink-python.
CVE-2018-1000180, 
CVE-2016-1000352,
CVE-2016-1000344, 
CVE-2016-1000340, 
CVE-2016-1000342, 
CVE-2016-1000343, 
CVE-2016-1000338

The issue comes from beam-vendor-grpc-1_26_0-0.3. 
 
The latest Flink uses apache beam 2.38.0 and its BouncyCastle version is 1.67. BouncyCastle should be of version 1.7 or greater

grpc-Java:1.48.0 has removed BouncyCastle dependency.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27713,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-27 10:19:41.0,,,,,,,,,,"0|z178go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused curator-test dependency from flink-test-utils,FLINK-28713,13473750,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,27/Jul/22 10:03,15/Aug/22 02:28,04/Jun/24 20:42,05/Aug/22 17:18,,,,,1.16.0,,,,Build System,Tests,,,,,,0,pull-request-available,,,,,,Remove an unused dependency that also pulls in log4j1 into user projects.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28856,FLINK-28955,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 06:09:54 UTC 2022,,,,,,,,,,"0|z178fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 17:18;chesnay;master: 6335b573863af2b30a6541f910be96ddf61f9c84;;;","08/Aug/22 06:09;xtsong;Fix CI in master (1.16): caef5b7a5c1b980cff926fec5dce3a34ad1b354f;;;",,,,,,,,,,,,,,,,,,,
Default Changelog all when changelog producer is input,FLINK-28712,13473748,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Jul/22 09:58,28/Jul/22 04:52,04/Jun/24 20:42,28/Jul/22 04:52,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"When changelog producer is input, It is implied that the file already contains all the changelogs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 04:52:15 UTC 2022,,,,,,,,,,"0|z178fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 04:52;lzljs3620320;master: be5fc4a3e54d56b2a7d2fa67781474763ea8fe2e
release-0.2: 6de3067aca83cbab81557cf2c43e2b82969d89fc;;;",,,,,,,,,,,,,,,,,,,,
Hive connector implements SupportsDynamicFiltering interface,FLINK-28711,13473745,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,godfreyhe,godfreyhe,27/Jul/22 09:36,09/Aug/22 09:41,04/Jun/24 20:42,09/Aug/22 09:41,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 09:41:27 UTC 2022,,,,,,,,,,"0|z178eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 09:41;godfreyhe;Fixed in 1.16: 0dc8890f1b8164bc37bd8b209e6d4eef37386087;;;",,,,,,,,,,,,,,,,,,,,
Supports dynamic filtering execution,FLINK-28710,13473744,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,27/Jul/22 09:35,07/Aug/22 13:53,04/Jun/24 20:42,07/Aug/22 13:53,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 13:53:02 UTC 2022,,,,,,,,,,"0|z178eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 13:53;godfreyhe;Fixed in master:
2176ec0aa131cd7dbbf4f19ac02640084b6db021
2176ec0aa131cd7dbbf4f19ac02640084b6db021;;;",,,,,,,,,,,,,,,,,,,,
Implement dynamic filtering operators,FLINK-28709,13473741,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,godfreyhe,godfreyhe,27/Jul/22 09:29,07/Aug/22 02:34,04/Jun/24 20:42,07/Aug/22 02:34,,,,,1.16.0,,,,Runtime / Coordination,Table SQL / Runtime,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 02:34:57 UTC 2022,,,,,,,,,,"0|z178ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 02:34;godfreyhe;Fixed in master:
9bda67795628576aa4f161df6cb976ba71c3936d
3a8e71e286d1d097fe6cfc8b7ff2125a5108f334
b0859789e7733c73a21e600ec0d595ead730c59d;;;",,,,,,,,,,,,,,,,,,,,
Introduce planner rules to optimize dpp pattern,FLINK-28708,13473738,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,godfreyhe,godfreyhe,27/Jul/22 09:27,04/Aug/22 02:51,04/Jun/24 20:42,04/Aug/22 02:51,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 02:51:55 UTC 2022,,,,,,,,,,"0|z178d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 02:51;godfreyhe;Fixed in master: ae186e8650f137d624812c79f1b1111c33325e72;;;",,,,,,,,,,,,,,,,,,,,
Introduce interface SupportsDynamicPartitionPruning,FLINK-28707,13473737,13473735,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,27/Jul/22 09:21,30/Jul/22 10:15,04/Jun/24 20:42,30/Jul/22 10:15,1.16.0,,,,1.16.0,,,,Table SQL / API,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jul 30 10:15:20 UTC 2022,,,,,,,,,,"0|z178cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jul/22 10:15;godfreyhe;Fixed in master: c9d34ed773e311bcb0e1c54ed1b3ca31973e8853;;;",,,,,,,,,,,,,,,,,,,,
FLIP-248: Introduce dynamic partition pruning,FLINK-28706,13473735,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfreyhe,godfreyhe,godfreyhe,27/Jul/22 09:19,27/Jun/23 03:41,04/Jun/24 20:42,10/Aug/22 02:34,1.16.0,,,,1.16.0,,,,Connectors / Hive,Runtime / Coordination,Table SQL / Planner,Table SQL / Runtime,,,,0,,,,,,,Please refer to https://cwiki.apache.org/confluence/display/FLINK/FLIP-248%3A+Introduce+dynamic+partition+pruning for more details,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 02:34:09 UTC 2022,,,,,,,,,,"0|z178cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 02:34;godfreyhe;Fixed in master;;;",,,,,,,,,,,,,,,,,,,,
Update copyright year to 2014-2022 in NOTICE files,FLINK-28705,13473729,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,27/Jul/22 08:53,27/Jul/22 10:10,04/Jun/24 20:42,27/Jul/22 10:09,table-store-0.3.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,Copyright year of the NOTICE files in Flink Table Store should be '2014-2022'.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 10:09:47 UTC 2022,,,,,,,,,,"0|z178b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 10:07;nicholasjiang;[~lzljs3620320], please help to assign this ticket to me and close this ticket.;;;","27/Jul/22 10:09;lzljs3620320;master: 22416a5d73af201abd849a4b4b4c7fd46cd2e213
release-0.2: e410f4ffa79bdc7b8c5280459d658e67c36e64f2;;;",,,,,,,,,,,,,,,,,,,
Document change-log mode cannot support map and multiset data type,FLINK-28704,13473720,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,27/Jul/22 08:28,28/Jul/22 11:25,04/Jun/24 20:42,28/Jul/22 11:25,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,Blocked by GenerateUtils#generateCompare,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28552,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 11:25:52 UTC 2022,,,,,,,,,,"0|z17894:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 11:25;lzljs3620320;master: 5f6c707a050f4e4e3d44a2acaf949b5f3cffad08
release-0.2: 54fe5f0fa8d37681de2c2dbd2b56d0fd799db956;;;",,,,,,,,,,,,,,,,,,,,
Describe partitioned spark table lost partition info,FLINK-28703,13473716,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,27/Jul/22 08:14,27/Jul/22 09:01,04/Jun/24 20:42,27/Jul/22 09:00,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"SparkTable should override #partitioning

!image-2022-07-27-16-17-14-240.png|width=740,height=364!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/22 08:17;qingyue;image-2022-07-27-16-17-14-240.png;https://issues.apache.org/jira/secure/attachment/13047278/image-2022-07-27-16-17-14-240.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 09:00:46 UTC 2022,,,,,,,,,,"0|z17888:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 09:00;lzljs3620320;master: a251033ece720a5fc571f36f5335c83715dc07de
release-0.2: 625dbabbd75737399777760b9e0fa6cdda2b7286;;;",,,,,,,,,,,,,,,,,,,,
"Why can't ""scan.incremental.snapshot.enabled"" be set when using datastream source in MySQL CDC connector",FLINK-28702,13473696,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,liujian,liujian,27/Jul/22 07:32,27/Jul/22 08:14,04/Jun/24 20:42,27/Jul/22 08:14,,,,,,,,,API / DataStream,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 08:14:38 UTC 2022,,,,,,,,,,"0|z1783s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 08:14;martijnvisser;Please ask questions related to CDC connectors at https://github.com/ververica/flink-cdc-connectors;;;",,,,,,,,,,,,,,,,,,,,
Minimize the explosion range during failover for hybrid shuffle,FLINK-28701,13473679,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Jul/22 06:05,10/Aug/22 01:35,04/Jun/24 20:42,10/Aug/22 01:35,1.16.0,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,"In hybrid shuffle mode, there are currently two strategies to control spilling. For the full spilling strategy, the data is guaranteed to be persistent to the disk after task finished. When a failover occurs, if the upstream has been finished, the data should be recovered directly from the disk file without re-compute. For selective spilling strategy, the entire topology must be restarted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 01:35:21 UTC 2022,,,,,,,,,,"0|z17800:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 01:35;xtsong;master (1.16): 56e91700ca56f5535f677314167b94b476eb6c3f;;;",,,,,,,,,,,,,,,,,,,,
Table store sink fails to commit for Flink 1.14 batch job,FLINK-28700,13473678,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,27/Jul/22 05:58,28/Jul/22 04:54,04/Jun/24 20:42,28/Jul/22 04:54,table-store-0.2.0,table-store-0.3.0,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,We can't get configurations from DummyStreamExecutionEnvironment in Flink 1.14 (this is fixed by FLINK-26709 in Flink 1.15) so we have to use Java reflection to check if this execution environment is for batch job or for streaming job.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 04:54:02 UTC 2022,,,,,,,,,,"0|z177zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 04:54;lzljs3620320;master: bc451951a0cfbdd5f7e4472df53430b36f3a9df7
release-0.2: 77230ddf70f809c578ab54c8e2881be774da044c;;;",,,,,,,,,,,,,,,,,,,,
Native rocksdb full snapshot in non-incremental checkpointing,FLINK-28699,13473665,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,frozen stone,frozen stone,frozen stone,27/Jul/22 03:32,09/Aug/22 03:26,04/Jun/24 20:42,09/Aug/22 03:26,1.14.5,1.15.1,,,1.16.0,,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,,,"When rocksdb statebackend is used and state.backend.incremental enabled, flink will figure out newly created sst files generated by rocksdb during checkpoint, and read all the states from rocksdb and write to files during savepoint [1].

When state.backend.incremental disabled, flink will read all the states from rocksdb and generate state files in checkpoint and savepoint [2]. This makes sense in savepoint, cause user can take a savepoint with rocksdb statebackend and then restore it using another statebackend, but in checkpoint, deserialisation and serialisation of state results in performance loss.

If the native rocksdb snapshot is introduced in full snapshot, theoretically better performance can be achieved. At the same time, savepoint remains the same as before.

 
 # https://github.com/apache/flink/blob/master/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java
 # https://github.com/apache/flink/blob/master/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksFullSnapshotStrategy.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28843,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 03:26:26 UTC 2022,,,,,,,,,,"0|z177ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 08:46;yunta;I think this is doable and bring performance benefits for users with default configurations, especially considering that we cannot make `state.backend.incremental` as true in the coming flink-1.16.

[~frozen stone] Please make sure all uploaded sst files in each checkpoint should only stay in the exclusive scoped checkpoint folder and do not touch the shared state registry in the new full checkpoint strategy.

I think this change would not impact the current changelog state-backend design and implementations. cc [~roman] ;;;","29/Jul/22 09:21;masteryhx;+1, I also think it could improve performance using native full snapshot.
Currently, we could implement it easily by adding NO_SHARING flag into RocksIncrementalSnapshotStrategy after the pr of FLINK-25745 when 'state.backend.incremental' is false.

With that, Rocksdb could also support native sp when 'state.backend.incremental' is false.;;;","01/Aug/22 09:14;yunta;I think adding {{NO_SHARING}} flag to RocksIncrementalSnapshotStrategy might not be a good idea as the current RocksIncrementalSnapshotStrategy could handle different sharing files strategies according to the passed checkpoint options. And I believe the code change should not be big as we can share most of the code for the newly {{RocksNativeFullSnapshotStrategy}} with the existing {{{}RocksIncrementalSnapshotStrategy{}}}.;;;","09/Aug/22 03:26;yunta;merged in master: d9a067e5e1c8672930b0ea7d76400a1d3020a1e2;;;",,,,,,,,,,,,,,,,,
The display order of aggregated metrics should follow the order of task state transitions,FLINK-28698,13473654,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,junhan,wanglijie,wanglijie,27/Jul/22 02:00,28/Jul/22 02:48,04/Jun/24 20:42,28/Jul/22 02:48,1.16.0,,,,1.16.0,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,,,"!image-2022-07-27-10-00-49-345.png|width=921,height=382!

 Currently, the display order of task state duration is INITIALIZING, CREATED, SCHEDULED, RUNNING, DEPLOYING. I think it would be more reasonable to change to CAEATED, SCHEDULED, DEPLOYING, INITIALIZING, RUNNING, which is follow the order of task state transitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/22 02:00;wanglijie;image-2022-07-27-10-00-49-345.png;https://issues.apache.org/jira/secure/attachment/13047258/image-2022-07-27-10-00-49-345.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 02:48:07 UTC 2022,,,,,,,,,,"0|z177ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 02:05;wanglijie;cc [~xtsong] ;;;","27/Jul/22 03:19;xtsong;Thanks [~wanglijie95], that makes sense to me.

[~junhany], could you please take a look?;;;","28/Jul/22 02:48;xtsong;master (1.16): 1e180475cd5dd68db527b5481411da1ff6ca0822;;;",,,,,,,,,,,,,,,,,,
MapDataSerializer doesn't declare a serialVersionUID,FLINK-28697,13473588,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jmahonin,jmahonin,26/Jul/22 15:10,21/Aug/23 22:35,04/Jun/24 20:42,,1.15.1,,,,,,,,API / Type Serialization System,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,"MapDataSerializer doesn't declare a serialVersionUID, which can manifest as a InvalidClassException when attempting to serialize with different JREs for compilation / runtime.
{code:java}
Caused by: java.io.InvalidClassException: org.apache.flink.table.runtime.typeutils.MapDataSerializer; local class incompatible: stream classdesc serialVersionUID = 2533002123505507000, local class serialVersionUID = 1622156938509929811 {code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:20 UTC 2023,,,,,,,,,,"0|z177fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 03:26;yunta;[~jmahonin] Did you use different flink versions between the client and server side? Or could you share the full exception stack trace?;;;","27/Jul/22 13:13;jmahonin;Hi [~yunta] 

Unfortunately the issue is a few months old, and as such the logs have expired from our aggregator.

We were using the same Flink version, but there was a mismatch in JVM versions between the compiled JAR, and what was being used at runtime. As well, I believe the JVM versions were different between client and server. I'm not certain exactly scenario caused the issue, but specifically setting the {{serialVersionUID}} resolved it. Note that it's the only class in that package without one defined.;;;","27/Oct/22 09:05;zhuzh;Thanks for reporting this problem and opening a PR to fix it. [~jmahonin]
I think not only `MapDataSerializer`, but also some other serializable classes may encounter this kind of problems when using different java versions at different sides. This means the pull request does not fully fix this kind of problems. Also, developing and running jobs with different java versions may introduce other potential incompatible problems.
I think we should use the same java version to develop, compile and run Flink jobs. And, with this assumption, the reported problem is not needed to be fixed.;;;","27/Oct/22 11:13;jmahonin;Agreed, the use of mismatched JVM versions is not ideal. In this case the major versions were the same (16), but due to some ideosyncrasies in container packaging, they were not the exact same JVM version.

The other classes in that package all have a serialVersionUID set (last I checked).

I’ll also note the Flink style guide says all serializable classes must declare a serialVersionUID
[https://flink.apache.org/contributing/code-style-and-quality-java.html#java-serialization];;;","31/Oct/22 09:05;zhuzh;My major concern is that whether the change is possible to break the compatibility of existed external checkpoints.
Although looks to me it will not. I still hope some other experts like [~yunta] can help to double confirm it.;;;","01/Nov/22 07:40;yunta;For checkpoints, it will use [MapDataSerializerSnapshot |https://github.com/apache/flink/blob/4e860b6a3e2b1cd2e36ca0dcb549a93c92430e1d/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/typeutils/MapDataSerializer.java#L250] to restore the MapDataSerializer instead of {{deserializeObject}}. Thus it would be safe for checkpoints.
However, we might have other places to deserialize, you can refer to FLINK-13910.;;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,
Notify all newlyAdded/Merged blocked nodes to BlocklistListener,FLINK-28696,13473577,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,26/Jul/22 14:04,27/Jul/22 05:42,04/Jun/24 20:42,27/Jul/22 05:42,,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,"This bug was introduced by FLINK-28660. Our newly added logic results in that blocklist listener will not be notified when there are no newly added nodes (only merge nodes) 。

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 05:42:25 UTC 2022,,,,,,,,,,"0|z177dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 05:42;zhuzh;Fixed via bbaeb628f48a4bc4c324bfc4afd06ddf34f546f0;;;",,,,,,,,,,,,,,,,,,,,
Fail to send partition request to restarted taskmanager,FLINK-28695,13473576,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fanrui,simonas.gelazevicius@vinted.com,simonas.gelazevicius@vinted.com,26/Jul/22 14:00,25/Oct/23 20:47,04/Jun/24 20:42,21/Nov/22 18:51,1.15.0,1.15.1,,,1.15.4,1.16.1,1.17.0,,Deployment / Kubernetes,Runtime / Network,,,,,,0,pull-request-available,,,,,,"After upgrade to *1.15.1* we started getting error while running JOB

 
{code:java}
org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: Sending the partition request to '/XXX.XXX.XX.32:6121 (#0)' failed.    at org.apache.flink.runtime.io.network.netty.NettyPartitionRequestClient$1.operationComplete(NettyPartitionRequestClient.java:145)    .... {code}
{code:java}
Caused by: org.apache.flink.shaded.netty4.io.netty.channel.StacklessClosedChannelException atrg.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source){code}
After investigation we managed narrow it down to the exact behavior then this issue happens:
 # Deploying JOB on fresh kubernetes session cluster with multiple TaskManagers: TM1 and TM2 is successful. Job has multiple partitions running on both TM1 and TM2.
 # One TaskManager TM2 (XXX.XXX.XX.32) fails for unrelated issue. For example OOM exception.
 # Kubernetes POD with mentioned TaskManager TM2 is restarted. POD retains same IP address as before.
 # JobManager is able to pickup the restarted TM2 (XXX.XXX.XX.32)
 # JOB is restarted because it was running on the failed TaskManager TM2
 # TM1 data channel to TM2 is closed and we get LocalTransportException: Sending the partition request to '/XXX.XXX.XX.32:6121 (#0)' failed during JOB running stage.  
 # When we explicitly delete pod with TM2 it creates new POD with different IP address and JOB is able to start again.

Important to note that we didn't encountered this issue with previous *1.14.4* version and TaskManager restarts didn't cause such error.

Please note attached kubernetes deployments and reduced logs from JobManager. TaskManager logs did show errors before error, but doesn't show anything significant after restart.

EDIT:
{quote}
Setting {{taskmanager.network.max-num-tcp-connections}} to a very high number workarounds the problem
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15455,,,,,,,,,,,,,,,"26/Jul/22 13:43;simonas.gelazevicius@vinted.com;deployment.txt;https://issues.apache.org/jira/secure/attachment/13047246/deployment.txt","25/Oct/23 20:47;kibebr;image-1.png;https://issues.apache.org/jira/secure/attachment/13063871/image-1.png","20/Nov/22 08:16;fanrui;image-2022-11-20-16-16-45-705.png;https://issues.apache.org/jira/secure/attachment/13052899/image-2022-11-20-16-16-45-705.png","21/Nov/22 09:16;fanrui;image-2022-11-21-17-15-58-749.png;https://issues.apache.org/jira/secure/attachment/13052921/image-2022-11-21-17-15-58-749.png","25/Oct/23 20:47;kibebr;image.png;https://issues.apache.org/jira/secure/attachment/13063870/image.png","26/Jul/22 13:55;simonas.gelazevicius@vinted.com;job_log.txt;https://issues.apache.org/jira/secure/attachment/13047241/job_log.txt","26/Jul/22 13:49;simonas.gelazevicius@vinted.com;jobmanager_config.txt;https://issues.apache.org/jira/secure/attachment/13047244/jobmanager_config.txt","26/Jul/22 13:54;simonas.gelazevicius@vinted.com;jobmanager_logs.txt;https://issues.apache.org/jira/secure/attachment/13047242/jobmanager_logs.txt","26/Jul/22 13:53;simonas.gelazevicius@vinted.com;pod_restart.txt;https://issues.apache.org/jira/secure/attachment/13047243/pod_restart.txt","26/Jul/22 13:49;simonas.gelazevicius@vinted.com;taskmanager_config.txt;https://issues.apache.org/jira/secure/attachment/13047245/taskmanager_config.txt",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 21 18:51:18 UTC 2022,,,,,,,,,,"0|z177d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Nov/22 09:38;Brian Zhou;We have experienced this issue on Flink 1.15, is there anyone that can help on this issue?;;;","07/Nov/22 02:47;Brian Zhou;From investigating the code, looks like it is related to FLINK-22643. While reusing the TCP connections in taskmanagers, it also brings the risk that the reusing the dead connection if the IP address does not change under a Kubernetes session cluster.

Ping the author [~fanrui] for further discussion and help on this issue.;;;","07/Nov/22 02:52;fanrui;Hi [~Brian Zhou] , thanks for your feedback, I will take a look this week. cc [~pnowojski] [~kevin.cyj] ;;;","07/Nov/22 04:04;Weijie Guo;[~Brian Zhou] Just to confirm, if you do not manually delete the POD of TM2, what is the status of job, can this job recover from the exception thrown in 6 or never recover? If so, is the job constantly reporting this errors or is it stuck?;;;","07/Nov/22 05:33;Brian Zhou;Not sure if the original reporter has the same, but from our testing, the job is constantly reporting with this error and keeps restarting.;;;","07/Nov/22 06:17;Weijie Guo;[~Brian Zhou] If you suspect that the problem is caused by connection reuse, you can try to set `taskmanager.network.max-num-tcp-connections` to a large value.What I'm puzzled about is that even if the TCP connection between two TMs is reused, the K8S should theoretically be able to correctly deliver the message to the new pod. And It sounds like you can reproduce this problem, could you provide me with a minimum reproducible approach to do further investigation.;;;","07/Nov/22 07:16;Brian Zhou;Thanks [~Weijie Guo] for your suggestion, we can try that.

For reproduction, in our case, we are experiencing this issue on a Kubernetes node reboot case. In other words, the other steps are the same as the description, except Step 2, we have a node reboot. 

However, unfortunately, we have tested several times during the weekend with the same steps, but cannot reproduce the problem.

 ;;;","17/Nov/22 10:51;nobleyd;We also encounter this problem in production, and task failures will continue to restart because of this problem.;;;","18/Nov/22 16:22;pnowojski;I've assigned the ticket to you [~fanrui]. Please ping me/request a review from me once you have some fix ready.

In the meantime can someone, who can reproduce this issue, try out if setting {{`taskmanager.network.max-num-tcp-connections`}} to a very high number work arounds the problem?;;;","20/Nov/22 08:21;fanrui;Thank you all for your feedback.

After analysis, I think the root cause is as follows:

`CreditBasedPartitionRequestClientHandler#channelInactive` doesn't call the `notifyAllChannelsOfErrorAndClose` when  the `inputChannels.isEmpty()`.

code link: https://github.com/apache/flink/blob/d745f5b3f7a64445854c735668afa9b72edb3fee/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/CreditBasedPartitionRequestClientHandler.java#L126

!image-2022-11-20-16-16-45-705.png!


FLINK-15455 enables TCP connection reuse across multiple jobs. That means `CreditBasedPartitionRequestClientHandler#channelInactive` doesn't call the `notifyAllChannelsOfErrorAndClose` when  after old job stop and before new job start. If the tcp connection is disconnected during this period, the new job should create a new tcp connection. However, the `PartitionRequestClientFactory` doesn't know that the tcp connection is broken, so the new job reuses the failed connection.

Please correct me if there is any mistake. cc [~Weijie Guo]  [~pnowojski]  [~guoyangze];;;","21/Nov/22 02:15;nobleyd;[~pnowojski] Hi, we have the same problem, and adjusting `taskmanager.network.max-num-tcp-connections` words fine.;;;","21/Nov/22 08:54;pnowojski;[~fanrui] What's your proposed solution? If the {{inputChannels}} is empty how would calling {{notifyAllChannelsOfErrorAndClose}} help with anything? Would it help, because it would set {{channelError}} for any future use? ;;;","21/Nov/22 09:20;fanrui;> Would it help, because it would set {{channelError}} for any future use?

[~pnowojski] Yes, you're right. From the notifyAllChannelsOfErrorAndClose code, we can see if inputChannels is empty, nothing to to do. It just set channelError and close ctx.

The channelError will be used within {_}PartitionRequestClientFactory#createPartitionRequestClient{_}. When create the new client, factory will think the old client has error, so create a new client.

If channelError is null, factory will think old client is well, so don't create the new client, and reuse the old client.

 

!image-2022-11-21-17-15-58-749.png|width=1273,height=696!;;;","21/Nov/22 09:37;pnowojski;Thanks for the explanation!;;;","21/Nov/22 18:51;pnowojski;merged commit 6b56505 into apache:release-1.15 
merged commit ecede7a into apache:release-1.16 
merged e7854193816^^^..e7854193816 into apache:master

Thanks [~fanrui] for fixing, and others for reporting/analyzing and confirming the bug and workaround.;;;",,,,,,
Set pipeline.name to resource name by default for application deployments,FLINK-28694,13473575,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,gyfora,gyfora,26/Jul/22 13:53,10/Aug/22 15:27,04/Jun/24 20:42,10/Aug/22 15:27,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,Starter,,,,,I think it would be nice to set pipeline.name by default to the resource name for application deployments.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 15:27:49 UTC 2022,,,,,,,,,,"0|z177cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 13:54;gyfora;[~wangyang0918] do you see any downsides?;;;","27/Jul/22 11:28;wangyang0918;Since {{env.execute(""job name"")}} has a higher priority over config options, I do not have any downsides on this ticket.;;;","10/Aug/22 15:27;gyfora;Merged to main 0a1a68c9a88ad3055dcf6630e23ce89df7d23a97;;;",,,,,,,,,,,,,,,,,,
Codegen failed if the watermark is defined on a columnByExpression,FLINK-28693,13473562,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,liuhb86,liuhb86,26/Jul/22 12:54,18/Apr/24 02:03,04/Jun/24 20:42,03/Apr/24 10:25,1.15.1,,,,1.18.2,1.19.1,1.20.0,,Table SQL / Runtime,,,,,,,1,pull-request-available,,,,,,"The following code will throw an exception:

 
{code:java}
Table program cannot be compiled. This is a bug. Please file an issue.
 ...
 Caused by: org.codehaus.commons.compiler.CompileException: Line 29, Column 54: Cannot determine simple type name ""org"" {code}
{color:#000000}Code:{color}
{code:java}
public class TestUdf extends  ScalarFunction {
    @DataTypeHint(""TIMESTAMP(3)"")
    public LocalDateTime eval(String strDate) {
       return LocalDateTime.now();
    }
}

public class FlinkTest {
    @Test
    void testUdf() throws Exception {
        //var env = StreamExecutionEnvironment.createLocalEnvironment();
        // run `gradlew shadowJar` first to generate the uber jar.
        // It contains the kafka connector and a dummy UDF function.

        var env = StreamExecutionEnvironment.createRemoteEnvironment(""localhost"", 8081,
                ""build/libs/flink-test-all.jar"");
        env.setParallelism(1);
        var tableEnv = StreamTableEnvironment.create(env);
        tableEnv.createTemporarySystemFunction(""TEST_UDF"", TestUdf.class);

        var testTable = tableEnv.from(TableDescriptor.forConnector(""kafka"")
                .schema(Schema.newBuilder()
                        .column(""time_stamp"", DataTypes.STRING())
                        .columnByExpression(""udf_ts"", ""TEST_UDF(time_stamp)"")
                        .watermark(""udf_ts"", ""udf_ts - INTERVAL '1' second"")
                        .build())
                // the kafka server doesn't need to exist. It fails in the compile stage before fetching data.
                .option(""properties.bootstrap.servers"", ""localhost:9092"")
                .option(""topic"", ""test_topic"")
                .option(""format"", ""json"")
                .option(""scan.startup.mode"", ""latest-offset"")
                .build());
        testTable.printSchema();
        tableEnv.createTemporaryView(""test"", testTable );

        var query = tableEnv.sqlQuery(""select * from test"");
        var tableResult = query.executeInsert(TableDescriptor.forConnector(""print"").build());
        tableResult.await();
    }
}{code}
What does the code do?
 # read a stream from Kakfa
 # create a derived column using an UDF expression
 # define the watermark based on the derived column

The full callstack:

 
{code:java}
org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:97) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.createWatermarkGenerator(GeneratedWatermarkGeneratorSupplier.java:62) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.createMainOutput(ProgressiveTimestampsAndWatermarks.java:104) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.api.operators.SourceOperator.initializeMainOutput(SourceOperator.java:426) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:402) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:387) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) [flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) [flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) [flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) [flink-dist-1.15.1.jar:1.15.1]
    at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    ... 18 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    ... 18 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 29, Column 54: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7121) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.access$17000(UnitCompiler.java:215) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6529) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6490) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$NewClassInstance.accept(Java.java:5190) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9237) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859) ~[flink-dist-1.15.1.jar:1.15.1]
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92) ~[flink-table-runtime-1.15.1.jar:1.15.1]
    ... 18 more
2022-07-26 09:53:12,770 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Closing Source Reader.
2022-07-26 09:53:12,772 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Source: *anonymous_kafka$1*[1] -> Calc[2] -> Sink: *anonymous_print$2*[3] (1/1)#0 (5283ea1dc807419920579ac3f255a090) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: Could not instantiate generated class 'WatermarkGenerator$0'
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74)
    at org.apache.flink.table.runtime.generated.GeneratedWatermarkGeneratorSupplier.createWatermarkGenerator(GeneratedWatermarkGeneratorSupplier.java:62)
    at org.apache.flink.streaming.api.operators.source.ProgressiveTimestampsAndWatermarks.createMainOutput(ProgressiveTimestampsAndWatermarks.java:104)
    at org.apache.flink.streaming.api.operators.SourceOperator.initializeMainOutput(SourceOperator.java:426)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:402)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:387)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:94)
    at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:101)
    at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:68)
    ... 16 more
Caused by: org.apache.flink.shaded.guava30.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2051)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache.get(LocalCache.java:3962)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4859)
    at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:92)
    ... 18 more
Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:107)
    at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$0(CompileUtils.java:92)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4864)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)
    at org.apache.flink.shaded.guava30.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)
    ... 21 more
Caused by: org.codehaus.commons.compiler.CompileException: Line 29, Column 54: Cannot determine simple type name ""org""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12211)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6833)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6594)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:6607)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:6573)
    at org.codehaus.janino.UnitCompiler.access$13900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6481)
    at org.codehaus.janino.UnitCompiler$22$1.visitReferenceType(UnitCompiler.java:6476)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3928)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6476)
    at org.codehaus.janino.UnitCompiler$22.visitType(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$ReferenceType.accept(Java.java:3927)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:7121)
    at org.codehaus.janino.UnitCompiler.access$17000(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6529)
    at org.codehaus.janino.UnitCompiler$22$2.visitNewClassInstance(UnitCompiler.java:6490)
    at org.codehaus.janino.Java$NewClassInstance.accept(Java.java:5190)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6490)
    at org.codehaus.janino.UnitCompiler$22.visitRvalue(UnitCompiler.java:6469)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4116)
    at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:6469)
    at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9237)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9123)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9025)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5062)
    at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4423)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4396)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4396)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5662)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3783)
    at org.codehaus.janino.UnitCompiler.access$5900(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3762)
    at org.codehaus.janino.UnitCompiler$13.visitMethodInvocation(UnitCompiler.java:3734)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5073)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3734)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2360)
    at org.codehaus.janino.UnitCompiler.access$1800(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1494)
    at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1487)
    at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2874)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1487)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1567)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3388)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1357)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1330)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:822)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:432)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:215)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:411)
    at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:406)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1414)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:406)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:378)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:237)
    at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:465)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:216)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:207)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:75)
    at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:104)
    ... 27 more {code}
The generated class:
{code:java}
/* 1 */
/* 2 */      public final class WatermarkGenerator$0
/* 3 */          extends org.apache.flink.table.runtime.generated.WatermarkGenerator {
/* 4 */
/* 5 */        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$2;
/* 6 */        private transient com.flinktest.TestUdf function_com$flinktest$TestUdf;
/* 7 */        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$4;
/* 8 */        private transient org.apache.flink.table.data.conversion.TimestampLocalDateTimeConverter converter$6;
/* 9 */        
/* 10 */        private transient org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context
/* 11 */        context;
/* 12 */        
/* 13 */
/* 14 */        public WatermarkGenerator$0(Object[] references) throws Exception {
/* 15 */          typeSerializer$2 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
/* 16 */          function_com$flinktest$TestUdf = (((com.flinktest.TestUdf) references[1]));
/* 17 */          converter$4 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[2]));
/* 18 */          converter$6 = (((org.apache.flink.table.data.conversion.TimestampLocalDateTimeConverter) references[3]));
/* 19 */          
/* 20 */          int len = references.length;
/* 21 */          context =
/* 22 */          (org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context) references[len-1];
/* 23 */          
/* 24 */        }
/* 25 */
/* 26 */        @Override
/* 27 */        public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {
/* 28 */          
/* 29 */          function_com$flinktest$TestUdf.open(new org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGeneratorFunctionContextWrapper(context));
/* 30 */                 
/* 31 */          
/* 32 */          converter$4.open(this.getClass().getClassLoader());
/* 33 */                     
/* 34 */          
/* 35 */          converter$6.open(this.getClass().getClassLoader());
/* 36 */                     
/* 37 */        }
/* 38 */
/* 39 */        @Override
/* 40 */        public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
/* 41 */          
/* 42 */          org.apache.flink.table.data.binary.BinaryStringData field$1;
/* 43 */          boolean isNull$1;
/* 44 */          org.apache.flink.table.data.binary.BinaryStringData field$3;
/* 45 */          java.time.LocalDateTime externalResult$5;
/* 46 */          org.apache.flink.table.data.TimestampData result$7;
/* 47 */          boolean isNull$7;
/* 48 */          boolean isNull$8;
/* 49 */          org.apache.flink.table.data.TimestampData result$9;
/* 50 */          
/* 51 */          isNull$1 = row.isNullAt(0);
/* 52 */          field$1 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 53 */          if (!isNull$1) {
/* 54 */            field$1 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(0));
/* 55 */          }
/* 56 */          field$3 = field$1;
/* 57 */          if (!isNull$1) {
/* 58 */            field$3 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$2.copy(field$3));
/* 59 */          }
/* 60 */                  
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          
/* 65 */          externalResult$5 = (java.time.LocalDateTime) function_com$flinktest$TestUdf
/* 66 */            .eval(isNull$1 ? null : ((java.lang.String) converter$4.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$3)));
/* 67 */          
/* 68 */          isNull$7 = externalResult$5 == null;
/* 69 */          result$7 = null;
/* 70 */          if (!isNull$7) {
/* 71 */            result$7 = (org.apache.flink.table.data.TimestampData) converter$6.toInternalOrNull((java.time.LocalDateTime) externalResult$5);
/* 72 */          }
/* 73 */          
/* 74 */          
/* 75 */          isNull$8 = isNull$7 || false;
/* 76 */          result$9 = null;
/* 77 */          if (!isNull$8) {
/* 78 */            
/* 79 */          
/* 80 */          result$9 = org.apache.flink.table.data.TimestampData.fromEpochMillis(result$7.getMillisecond() - ((long) 1000L), result$7.getNanoOfMillisecond());
/* 81 */          
/* 82 */            
/* 83 */          }
/* 84 */          
/* 85 */          if (isNull$8) {
/* 86 */            return null;
/* 87 */          } else {
/* 88 */            return result$9.getMillisecond();
/* 89 */          }
/* 90 */        }
/* 91 */
/* 92 */        @Override
/* 93 */        public void close() throws Exception {
/* 94 */          
/* 95 */          function_com$flinktest$TestUdf.close();
/* 96 */                 
/* 97 */        }
/* 98 */      }
/* 99 */    /* 1 */
/* 2 */      public final class WatermarkGenerator$0
/* 3 */          extends org.apache.flink.table.runtime.generated.WatermarkGenerator {
/* 4 */
/* 5 */        private transient org.apache.flink.table.runtime.typeutils.StringDataSerializer typeSerializer$2;
/* 6 */        private transient com.flinktest.TestUdf function_com$flinktest$TestUdf;
/* 7 */        private transient org.apache.flink.table.data.conversion.StringStringConverter converter$4;
/* 8 */        private transient org.apache.flink.table.data.conversion.TimestampLocalDateTimeConverter converter$6;
/* 9 */        
/* 10 */        private transient org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context
/* 11 */        context;
/* 12 */        
/* 13 */
/* 14 */        public WatermarkGenerator$0(Object[] references) throws Exception {
/* 15 */          typeSerializer$2 = (((org.apache.flink.table.runtime.typeutils.StringDataSerializer) references[0]));
/* 16 */          function_com$flinktest$TestUdf = (((com.flinktest.TestUdf) references[1]));
/* 17 */          converter$4 = (((org.apache.flink.table.data.conversion.StringStringConverter) references[2]));
/* 18 */          converter$6 = (((org.apache.flink.table.data.conversion.TimestampLocalDateTimeConverter) references[3]));
/* 19 */          
/* 20 */          int len = references.length;
/* 21 */          context =
/* 22 */          (org.apache.flink.api.common.eventtime.WatermarkGeneratorSupplier.Context) references[len-1];
/* 23 */          
/* 24 */        }
/* 25 */
/* 26 */        @Override
/* 27 */        public void open(org.apache.flink.configuration.Configuration parameters) throws Exception {
/* 28 */          
/* 29 */          function_com$flinktest$TestUdf.open(new org.apache.flink.table.planner.codegen.WatermarkGeneratorCodeGeneratorFunctionContextWrapper(context));
/* 30 */                 
/* 31 */          
/* 32 */          converter$4.open(this.getClass().getClassLoader());
/* 33 */                     
/* 34 */          
/* 35 */          converter$6.open(this.getClass().getClassLoader());
/* 36 */                     
/* 37 */        }
/* 38 */
/* 39 */        @Override
/* 40 */        public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
/* 41 */          
/* 42 */          org.apache.flink.table.data.binary.BinaryStringData field$1;
/* 43 */          boolean isNull$1;
/* 44 */          org.apache.flink.table.data.binary.BinaryStringData field$3;
/* 45 */          java.time.LocalDateTime externalResult$5;
/* 46 */          org.apache.flink.table.data.TimestampData result$7;
/* 47 */          boolean isNull$7;
/* 48 */          boolean isNull$8;
/* 49 */          org.apache.flink.table.data.TimestampData result$9;
/* 50 */          
/* 51 */          isNull$1 = row.isNullAt(0);
/* 52 */          field$1 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
/* 53 */          if (!isNull$1) {
/* 54 */            field$1 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(0));
/* 55 */          }
/* 56 */          field$3 = field$1;
/* 57 */          if (!isNull$1) {
/* 58 */            field$3 = (org.apache.flink.table.data.binary.BinaryStringData) (typeSerializer$2.copy(field$3));
/* 59 */          }
/* 60 */                  
/* 61 */          
/* 62 */          
/* 63 */          
/* 64 */          
/* 65 */          externalResult$5 = (java.time.LocalDateTime) function_com$flinktest$TestUdf
/* 66 */            .eval(isNull$1 ? null : ((java.lang.String) converter$4.toExternal((org.apache.flink.table.data.binary.BinaryStringData) field$3)));
/* 67 */          
/* 68 */          isNull$7 = externalResult$5 == null;
/* 69 */          result$7 = null;
/* 70 */          if (!isNull$7) {
/* 71 */            result$7 = (org.apache.flink.table.data.TimestampData) converter$6.toInternalOrNull((java.time.LocalDateTime) externalResult$5);
/* 72 */          }
/* 73 */          
/* 74 */          
/* 75 */          isNull$8 = isNull$7 || false;
/* 76 */          result$9 = null;
/* 77 */          if (!isNull$8) {
/* 78 */            
/* 79 */          
/* 80 */          result$9 = org.apache.flink.table.data.TimestampData.fromEpochMillis(result$7.getMillisecond() - ((long) 1000L), result$7.getNanoOfMillisecond());
/* 81 */          
/* 82 */            
/* 83 */          }
/* 84 */          
/* 85 */          if (isNull$8) {
/* 86 */            return null;
/* 87 */          } else {
/* 88 */            return result$9.getMillisecond();
/* 89 */          }
/* 90 */        }
/* 91 */
/* 92 */        @Override
/* 93 */        public void close() throws Exception {
/* 94 */          
/* 95 */          function_com$flinktest$TestUdf.close();
/* 96 */                 
/* 97 */        }
/* 98 */      }
/* 99 */    {code}
Addtional information:
 # This is a regression from 1.14. The issue doesn't happen in 1.14
 # It needs to be run in a flink cluster. It doesn't happen in a LocalEnvironment.
 # It doen't happen when using `datagen` source instead of Kafka.

Reproduce steps:
 # download flink-1.15.1-bin-scala_2.12.tgz and run bin/start-cluster
 # clone the test code from `git clone -b FLINK-28693 [https://github.com/liuhb86/flink-test.git]`
 # run `gradlew shadowJar` to generate the jar files with the UDF function.
 # run `gradlew test`. The exception occurs in about 10 seconds.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34016,FLINK-35081,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 09 15:11:14 UTC 2024,,,,,,,,,,"0|z177a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 13:57;liuhb86;Is it caused by change in the codegen logic or a classloader problem because of the new module `{{{}flink-table-planner-loader{}}}`?;;;","26/Jul/22 15:07;liuhb86;Swaping flink-table-planner-loader with flink-table-planer in the opt/ folder can solve the problem.

Is there any drawback to the swap? (If not, why use flink-table-planner-loader as the default, as I saw others also reported problems about this lib).;;;","18/Aug/23 02:05;luca.yang;watermark with column by udf may cause a error. when i replace json_value(build-in function) with myUDF, no error.;;;","08/Jan/24 07:55;wczhu;same problem [FLINK-34016|https://issues.apache.org/jira/browse/FLINK-34016];;;","07/Feb/24 07:26;xuyangzhong;This bug is caused by that the code generated by codegen references the class in the table-planner package, but the class in the table-planner package is hidden by table-planner-loader, so classloader cannot find it. 

I'll try to fix it.;;;","09/Feb/24 08:00;seb-pereira;We are facing similar issue in 1.18 - our analyses lead to this:

Once deployed, the job fails with: *_Could not instantiate generated class 'WatermarkGenerator$0'_*
{code:java}
org.apache.flink.runtime.taskmanager.Task                    [] - Source: source_1___TABLE[1] -> Calc[2] -> KafkaSinkTable6[3]: Writer -> KafkaSinkTable6[3]: Committer (1/1)#0 (b7ae8e7fdeab754fd21c02a17b7736aa_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from RUNNING to FAILED with failure cause:
2024-02-08T16:25:56.613700525Z java.lang.RuntimeException: Could not instantiate generated class 'WatermarkGenerator$0'
2024-02-08T16:25:56.613703025Z     at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:74) ~[flink-table-runtime-1.18.0.jar:1.18.0]
[...]
Caused by: org.apache.flink.shaded.guava31.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.{code}
To reproduce, the source table should meet these conditions:
 * the watermark definition is based on a computed column,
 * the computed column relies on a UDF,
 * uses the `kafka` connector

 
{code:java}
CREATE TABLE `source_1___TABLE`
(
  `ts` STRING,
  `ts___EVENT_TIME` AS CAST (TO_TIMESTAMP_UDF(`ts`) AS TIMESTAMP(3)),
  WATERMARK FOR `ts___EVENT_TIME` AS `ts___EVENT_TIME` - INTERVAL '1' MINUTE
){code}
Note that with the `filesystem` connector, such declaration deploys and consume events as expected.

Behavior is consistent when the job is created and deployed directly through the table API or through the SQL client using SQL statement definition.;;;","03/Apr/24 10:25;Sergey Nuyanzin;Merged as [0acf92f1c8a90dcb3eb2c1038c1cda3344b7b988|https://github.com/apache/flink/commit/0acf92f1c8a90dcb3eb2c1038c1cda3344b7b988];;;","09/Apr/24 15:11;Sergey Nuyanzin;1.18: [f5c62abf7475ea8bc976de2a2079b1a9e29b79df|https://github.com/apache/flink/commit/f5c62abf7475ea8bc976de2a2079b1a9e29b79df]
1.19: [b1165a89edb9857754e283c6afd7983a34acd465|https://github.com/apache/flink/commit/b1165a89edb9857754e283c6afd7983a34acd465];;;",,,,,,,,,,,,,
Check warehouse path in CatalogFactory,FLINK-28692,13473527,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 10:48,27/Jul/22 01:35,04/Jun/24 20:42,27/Jul/22 01:35,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"* Not exist, automatic creating the directory.
* Exist but it is not directory, throw exception.
* Exist and it is a directory, pass...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 01:35:46 UTC 2022,,,,,,,,,,"0|z17728:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 01:35;lzljs3620320;master: d30e7998b136dec0f9aded8376a516a76b467f2e
release-0.2: f149a4f349fa2ced986bec42b3d1f0cd27004343;;;",,,,,,,,,,,,,,,,,,,,
Improve cache hit rate of generated class,FLINK-28691,13473525,13417633,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,FrankZou,FrankZou,FrankZou,26/Jul/22 10:34,22/Dec/23 07:45,04/Jun/24 20:42,,,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,,,"In OLAP scenarios, compiling generated classes is very frequent, it will consume a lot of CPU and large amount of generated classes will also takes up a lot of space in metaspace, which will lead to frequent Full GC.

As we use a self-incrementing counter in CodeGenUtils#newName, it means we could not get the same generated class between two queries even when they are exactly the same. Maybe we could share the same generated class between different queries if they has the same logic, it will be good for job latency and resource consumption. ",,,,,,,,,,,FLINK-33792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 11 08:26:17 UTC 2023,,,,,,,,,,"0|z1771s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 02:59;zjureel;Hi [~jark] We found the metaspace fullgc problem in codegen and [~FrankZou] fixed it our internal branch. This may involve codegen and planner, what do you think of it? I found that other users also reported the same problem such as https://issues.apache.org/jira/browse/FLINK-31308;;;","11/Dec/23 07:02;FrankZou;[~libenchao] Hi, I would like to take this ticket, could you please assign it to me?;;;","11/Dec/23 08:26;libenchao;[~FrankZou] Assigned to you.;;;",,,,,,,,,,,,,,,,,,
UpdateSchema#fromCatalogTable lost column comment,FLINK-28690,13473522,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,qingyue,qingyue,qingyue,26/Jul/22 09:59,19/Mar/23 05:47,04/Jun/24 20:42,19/Mar/23 05:47,table-store-0.2.0,,,,table-store-0.4.0,,,,Table Store,,,,,,,0,,,,,,,"The reason is that org.apache.flink.table.api.TableSchema#toPhysicalRowDataType lost column comments, which leads to comparison failure in AbstractTableStoreFactory#buildFileStoreTable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28686,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 09 03:09:31 UTC 2023,,,,,,,,,,"0|z17714:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 03:09;lzljs3620320;wait for Flink 1.17;;;",,,,,,,,,,,,,,,,,,,,
Optimize Spark documentation to Catalog and Dataset,FLINK-28689,13473511,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 09:06,26/Jul/22 10:07,04/Jun/24 20:42,26/Jul/22 10:07,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"* Introduce Dataset API.
* Unify table_store and tablestore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 10:07:54 UTC 2022,,,,,,,,,,"0|z176yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 10:07;lzljs3620320;master: c62d721865e08d4f119b118461829261c8f4819b
release-0.2: fbb5807127d7d190c5631f3e006ea6845395897a;;;",,,,,,,,,,,,,,,,,,,,
Support DataStream PythonWindowOperator in Thread Mode,FLINK-28688,13473496,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,26/Jul/22 07:57,08/Aug/22 03:29,04/Jun/24 20:42,08/Aug/22 03:29,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-25724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 03:29:45 UTC 2022,,,,,,,,,,"0|z176vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 03:29;hxbks2ks;Merged into master via d137c12877fa2b21688acd3a89bb947975b0553a;;;",,,,,,,,,,,,,,,,,,,,
BucketSelector is wrong when hashcode is negative,FLINK-28687,13473492,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 07:50,26/Jul/22 08:44,04/Jun/24 20:42,26/Jul/22 08:44,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,The calculation of bucket should be `Math.abs(hashcode % numBucket)` instead of `hashcode % numBucket`.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 08:44:18 UTC 2022,,,,,,,,,,"0|z176ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 08:44;lzljs3620320;master: ff697ba12a80f51702a5412c2deb6d4f89a966ff
release-0.2: 0fc2c32db3cf3aab4a0887f24f22eed5b415fb27;;;",,,,,,,,,,,,,,,,,,,,
Spark table with column comment cannot be read/write by Flink,FLINK-28686,13473491,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,qingyue,qingyue,26/Jul/22 07:48,26/Jul/22 11:54,04/Jun/24 20:42,26/Jul/22 11:54,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"The reason is that org.apache.flink.table.api.TableSchema#toPhysicalRowDataType lost column comments, which leads to comparison failure in AbstractTableStoreFactory#buildFileStoreTable.


!screenshot-1.png|width=662,height=253!

 

The reason why Flink does not encounter this problem is due to the column is lost before persisting the schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28690,,,,,,,,,,,,,,,"26/Jul/22 07:51;qingyue;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13047220/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 11:54:12 UTC 2022,,,,,,,,,,"0|z176u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 11:54;lzljs3620320;master: 0dcf21dc1b6c17057083c144f5bda9ec7dbf0324
release-0.2: 3c76a3916e55271eb08e10d03b4124fb8325d6b8;;;",,,,,,,,,,,,,,,,,,,,
Optimize CatalogManager getTable implementation,FLINK-28685,13473457,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Paul Lin,Paul Lin,26/Jul/22 04:24,26/Jul/22 06:07,04/Jun/24 20:42,,1.16.0,,,,,,,,Table SQL / API,,,,,,,0,,,,,,,"Currently, Catalog#getTable(ObjectPath) might be abused in CatalogManager. CatalogManager gets an ObjectPath directly, instead of checking the existence of the table first, which could surprise Catalog developers and may get unexpected exceptions other than TableNotFoundExceptions.
{code:java}
private Optional<CatalogBaseTable> getUnresolvedTable(ObjectIdentifier objectIdentifier) {
Catalog currentCatalog = catalogs.get(objectIdentifier.getCatalogName());
ObjectPath objectPath = objectIdentifier.toObjectPath();
if (currentCatalog != null) {
try
{ final CatalogBaseTable table = currentCatalog.getTable(objectPath); return Optional.of(table); }
catch (TableNotExistException e)
{ // Ignore. }
}
return Optional.empty();
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 04:26:30 UTC 2022,,,,,,,,,,"0|z176mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 04:26;Paul Lin;Please assign this issue to me. I have fixed it in our in-house branch. Thanks!;;;",,,,,,,,,,,,,,,,,,,,
NullPointerException at OneHotEncoder.GenerateModelDataOperator.snapshot,FLINK-28684,13473455,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,zhangzp,zhangzp,26/Jul/22 04:13,24/Aug/22 03:17,04/Jun/24 20:42,26/Jul/22 08:06,ml-2.1.0,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,"E Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator GenerateModelDataOperator -> *anonymous_datastream_source$215*[229] -> ConstraintEnforcer[230] -> TableToDataSteam (1/1)#0. Failure reason: Checkpoint was declined. 
[193|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:194]E at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269) 
[194|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:195]E at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173) 
[195|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:196]E at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:348) 
[196|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:197]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:227) 
[197|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:198]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:212) 
[198|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:199]E at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192) 
[199|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:200]E at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:647) 
[200|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:201]E at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:320) 
[201|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:202]E at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$12(StreamTask.java:1253) 
[202|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:203]E at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) 
[203|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:204]E at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1241) 
[204|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:205]E at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1198) 
[205|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:206]E ... 22 more 
[206|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:207]E Caused by: java.lang.NullPointerException 
[207|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:208]E at org.apache.flink.api.common.typeutils.base.GenericArraySerializer.copy(GenericArraySerializer.java:92) 
[208|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:209]E at org.apache.flink.api.common.typeutils.base.GenericArraySerializer.copy(GenericArraySerializer.java:37) 
[209|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:210]E at org.apache.flink.runtime.state.ArrayListSerializer.copy(ArrayListSerializer.java:75) 
[210|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:211]E at org.apache.flink.runtime.state.PartitionableListState.<init>(PartitionableListState.java:65) 
[211|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:212]E at org.apache.flink.runtime.state.PartitionableListState.deepCopy(PartitionableListState.java:79) 
[212|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:213]E at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:77) 
[213|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:214]E at org.apache.flink.runtime.state.DefaultOperatorStateBackendSnapshotStrategy.syncPrepareResources(DefaultOperatorStateBackendSnapshotStrategy.java:36) 
[214|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:215]E at org.apache.flink.runtime.state.SnapshotStrategyRunner.snapshot(SnapshotStrategyRunner.java:77) 
[215|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:216]E at org.apache.flink.runtime.state.DefaultOperatorStateBackend.snapshot(DefaultOperatorStateBackend.java:230) 
[216|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:217]E at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:230) 
[217|https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true#step:9:218]E ... 33 more
 

 

https://github.com/apache/flink-ml/runs/7512316012?check_suite_focus=true",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 08:06:18 UTC 2022,,,,,,,,,,"0|z176m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 06:31;yunfengzhou;This problem occurs when a checkpoint barrier reaches a `GenerateModelDataOperator` before the first `StreamRecord` arrives at the operator. In this case, the `maxIndices` variable is not initialized and thus a `null` value is attempted to be stored in the snapshot. The solution is to provide a proper initial value for this variable.;;;","26/Jul/22 06:36;yunfengzhou;It is not found that this problem exists in other variables to be saved in snapshots. They have either adopted the solution above or only update the state when there is a not-null value, as follows.
{code:java}
if (x != null){
    xState.update(x);
}
{code}
Thus it is enough to just add fixes in OneHotEncoder.;;;","26/Jul/22 08:06;zhangzp;fixed on (1) master: 8df38791a0243870d9b11f03ee5ae24cae880d89 (2) release-2.1: ccee3a319f731d29d26049955880ae8fe077573f;;;",,,,,,,,,,,,,,,,,,
Modify changelog-file to changelog-producer,FLINK-28683,13473453,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,26/Jul/22 04:10,26/Jul/22 09:16,04/Jun/24 20:42,26/Jul/22 09:16,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"The changelog-file is limited, we can have more changelog producer types.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 09:16:41 UTC 2022,,,,,,,,,,"0|z176ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 09:16;lzljs3620320;master: f85ba571fb3aa87d1e619b091c17a8657b9a2d16
release-0.2: bfc02fd8287119a9686f29a11c4a19d3e672c963;;;",,,,,,,,,,,,,,,,,,,,
support join hint in flink batch,FLINK-28682,13473450,13447716,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,26/Jul/22 03:53,08/Aug/22 04:00,04/Jun/24 20:42,08/Aug/22 04:00,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,Add logic about join hint in batch rules.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 04:00:11 UTC 2022,,,,,,,,,,"0|z176l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 04:00;godfreyhe;Fixed in master: 
53f1a66f67ca1414e9b8f833f79466aa6b2110f5
f794d72903c9c76e89731273dbb60bbee75e3166;;;",,,,,,,,,,,,,,,,,,,,
"The number of windows allocated by sliding windows is inaccurate, when the window length is irregular",FLINK-28681,13473442,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,nyingping,nyingping,26/Jul/22 03:25,19/Apr/23 01:53,04/Jun/24 20:42,19/Apr/23 01:53,1.15.1,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,,,"In assignWindows, the initial length of the list of Windows is determined by '(int) (size/slide)'.
{code:java}
List<TimeWindow> windows = new ArrayList<>((int) (size / slide)) {code}
This calculation is not accurate when the sliding window length is irregular.

For example, if the size is 10 and the slide is 3, '(int) (size/slide)' gives 3, but the final number of Windows is 4.

 

Although this does not affect functionality.But I think it would be better.
{code:java}
int len = (int) Math.ceil((double) size / slide);
List windows = new ArrayList<>(len); {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-26 03:25:08.0,,,,,,,,,,"0|z176jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No space left on device on Azure e2e_2_ci tests,FLINK-28680,13473435,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,rmetzger,hxbks2ks,hxbks2ks,26/Jul/22 02:25,04/Aug/22 10:23,04/Jun/24 20:42,26/Jul/22 12:30,1.14.5,1.16.0,,,1.14.6,1.15.2,1.16.0,,Build System / Azure Pipelines,,,,,,,0,pull-request-available,test-stability,,,,,"Many e2e tests failed due to no enough space. We previously cleaned up the space by cleaning up the flink-e2e directory, but at the moment this is not enough to solve the problem. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24434,FLINK-28305,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 14:38:18 UTC 2022,,,,,,,,,,"0|z176hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:30;hxbks2ks;TPC-DS end-to-end test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38644&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c
;;;","26/Jul/22 02:30;hxbks2ks;1.14 SQL Client end-to-end test: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38644&view=logs&j=739e6eac-8312-5d31-d437-294c4d26fced&t=2a8cc459-df7a-5e6f-12bf-96efcc369aa9;;;","26/Jul/22 02:35;hxbks2ks;KinesisStreamsTableApiIT: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38660&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=5846934b-7a4f-545b-e5b0-eb4d8bda32e1;;;","26/Jul/22 07:44;rmetzger;The cleanup script which runs before each test is cleaning up the disk space from 17G available space to 24G. The script used to free up 28G. I'm looking into ways of freeing up more ;) ;;;","26/Jul/22 08:58;rmetzger;I'm able to make 53G available. Opening a PR.;;;","26/Jul/22 11:23;hxbks2ks;[~rmetzger] Great Job. Thanks a lot for the improvement.(y);;;","26/Jul/22 12:30;rmetzger;Thanks a lot!

Merged to master (1.16) in https://github.com/apache/flink/commit/6dd75e1d39597d6482eb920a610b1ebc55f39458
Merged to release-1.15 in 28bb487c99b4551a4dea63441c71dc4fd68898b1;;;","26/Jul/22 12:56;chesnay;Why no backport to 1.14?;;;","26/Jul/22 14:38;rmetzger;Sorry, I wasn't sure whether 1.14 is still maintained.

merged to release-1.14 in 274040dafad472cfe2a88e1e3038258545f1a2ef;;;",,,,,,,,,,,,
HiveServer2 Endpoint supports to build with Hive3 ,FLINK-28679,13473430,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,hxbks2ks,hxbks2ks,26/Jul/22 02:08,29/Jul/22 10:30,04/Jun/24 20:42,29/Jul/22 10:30,1.16.0,,,,1.16.0,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,"
{code:java}
2022-07-26T00:28:38.9402708Z [ERROR] COMPILATION ERROR : 
2022-07-26T00:28:38.9403248Z [INFO] -------------------------------------------------------------
2022-07-26T00:28:38.9404706Z [ERROR] /home/vsts/work/1/s/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/endpoint/hive/HiveServer2Endpoint.java:[116,8] org.apache.flink.table.endpoint.hive.HiveServer2Endpoint is not abstract and does not override abstract method SetClientInfo(org.apache.hive.service.rpc.thrift.TSetClientInfoReq) in org.apache.hive.service.rpc.thrift.TCLIService.Iface
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38660&view=logs&j=b1fcf054-9138-5463-c73c-a49979b9ac2a&t=9291ac46-dd95-5135-b799-3839e65a8691
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28150,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 10:30:09 UTC 2022,,,,,,,,,,"0|z176go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:08;hxbks2ks;[~fsk119] Could you help take a look?;;;","29/Jul/22 10:30;jark;Fixed in master: aa624025dce378da3c11fc3ab6e8cf91fc94c30e;;;",,,,,,,,,,,,,,,,,,,
"Support SQL hint for Filter, SetOp, Sort, Window, Values",FLINK-28678,13473428,13447716,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,26/Jul/22 01:53,08/Aug/22 03:59,04/Jun/24 20:42,08/Aug/22 03:58,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,,,,,,,"Ref CALCITE-5107 . Mainly cherry pick it.

The following files should be removed from the Flink code base when upgrading calcite to 1.31

in `org.apahce.calcite.rel.core`:
 * Correlate
 * Filter
 * Intersect
 * Minus
 * SetOp
 * Sort
 * Union
 * Values
 * Window

in `org.apahce.calcite.rel.hint`:
 * HintPredicates
 * NodeTypeHintPredicate

in `org.apahce.calcite.rel.logical`:
 * LogicalCorrelate
 * LogicalFilter
 * LogicalIntersect
 * LogicalMinus
 * LogicalSort
 * LogicalUnion
 * LogicalValues
 * LogicalWindow",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27998,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 03:58:24 UTC 2022,,,,,,,,,,"0|z176g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 07:39;martijnvisser;[~xuyangzhong] Please make sure that you also create the necessary info in the tickets in Flink that when the upgrade to Calcite 1.31 in Flink is done (scheduled for 1.17) that it's known what needs to be done. See the description of FLINK-21239 as an example;;;","28/Jul/22 07:20;xuyangzhong;Hi, thanks for your reminder, [~martijnvisser] . What about the new description? ;;;","29/Jul/22 09:40;martijnvisser;[~xuyangzhong] You should add the steps that need to be taken to FLINK-28744, not in this ticket. ;;;","01/Aug/22 02:11;xuyangzhong;[~martijnvisser] I copied these steps to FLINK-28744;;;","01/Aug/22 09:59;martijnvisser;Thanks!;;;","08/Aug/22 03:58;godfreyhe;Fixed in master: 
d17dcbd43b287c58efc726f8a0a9b78c5ddf7f60
4e46602342f3e3e45716f3adf8e29f7531cb9ca5;;;",,,,,,,,,,,,,,,
Primary key should not be null when creating Spark table,FLINK-28677,13473348,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,qingyue,qingyue,25/Jul/22 14:54,26/Jul/22 07:35,04/Jun/24 20:42,26/Jul/22 07:34,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,Should add a null check on pk fields,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 07:34:33 UTC 2022,,,,,,,,,,"0|z175yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 07:34;lzljs3620320;master: ac950a299c487416b9f558f9067df0b0c3606304
release-0.2: 746f1e28695d74bf29649f93b4cdb5cc127a6586;;;",,,,,,,,,,,,,,,,,,,,
Update copyright year to 2014-2022 in NOTICE files,FLINK-28676,13473344,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,libenchao,libenchao,libenchao,25/Jul/22 14:31,23/Aug/22 07:01,04/Jun/24 20:42,23/Aug/22 07:01,1.15.1,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,"In the review of [https://github.com/apache/flink/pull/14376,] I found that many NOTICE file's copyright year is still '2014-2021'. We should update it to '2014-2022'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 23 07:01:53 UTC 2022,,,,,,,,,,"0|z175xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Aug/22 07:01;chesnay;master: 803027a152781230f743a90054f97d333ba8cabb;;;",,,,,,,,,,,,,,,,,,,,
Avro Schemas should eagerly validate that class is SpecificRecord,FLINK-28675,13473321,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,chesnay,chesnay,25/Jul/22 13:06,08/Aug/22 10:35,04/Jun/24 20:42,08/Aug/22 10:35,1.15.1,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,,,,,,,"The AvroDeserializationSchema supports both generic and specific records, with dedicated factory methods.

It does however not validate in any way whether the classes passed to the factories methods are actually generic/specific records respectively, which can result in Flink attempting to read generic records (and failing with an NPE) even though the user told us to read specific records.

We should validate this eagerly and fail early.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 10:35:47 UTC 2022,,,,,,,,,,"0|z175sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 10:35;chesnay;This is already enforced by generics.;;;",,,,,,,,,,,,,,,,,,,,
EqualiserCodeGenerator generates wrong equaliser for Timestamp fields in BinaryRowData,FLINK-28674,13473319,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,kyledong,kyledong,25/Jul/22 13:02,25/Jul/22 13:28,04/Jun/24 20:42,25/Jul/22 13:22,1.13.6,1.14.5,1.15.1,,,,,,Table SQL / Runtime,,,,,,,0,,,,,,,UPDATE: Not a problem,Flink 1.13.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 13:22:29 UTC 2022,,,,,,,,,,"0|z175s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 13:22;kyledong;Actually caused by missing timestamp settings in CDC connector options, not a problem in Equaliser.;;;",,,,,,,,,,,,,,,,,,,,
Migrate Flink ML to Flink 1.15.1,FLINK-28673,13473292,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,25/Jul/22 11:54,24/Aug/22 03:17,04/Jun/24 20:42,26/Jul/22 02:32,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,Update Flink ML's Flink dependency to 1.15.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 02:32:04 UTC 2022,,,,,,,,,,"0|z175m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:32;zhangzp;fixed via d9bac9a44812d6eca145112f4901ceaf484ba014;;;",,,,,,,,,,,,,,,,,,,,
properties.allow.auto.create.topics disable,FLINK-28672,13473279,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,songwenbin,songwenbin,25/Jul/22 10:55,18/Nov/22 01:02,04/Jun/24 20:42,01/Aug/22 13:09,1.13.6,,,,,,,,Table SQL / Runtime,,,,,,,0,,,,,,,"# 配置了'properties.allow.auto.create.topics' = 'false'，但job执行后都会创建出mocktest27的topic

CREATE TABLE interval_source7 (
    item               STRING,
    val                DOUBLE,
    is_energy          BIGINT,
    ts                 BIGINT,
    event_time      AS TO_TIMESTAMP_LTZ(ts, 3),
    process_time    AS PROCTIME(),
    WATERMARK FOR event_time AS event_time)
   WITH ('connector' = 'kafka', 'topic' = 'daily-consumption', 'properties.bootstrap.servers' = 'broker:9092','properties.group.id' = 'l3_daily-consumption_mocktest','value.format' = 'avro-confluent','value.avro-confluent.schema-registry.url' = 'http://schema-registry:8081','value.fields-include' = 'EXCEPT_KEY', 'scan.startup.mode' = 'earliest-offset');

CREATE TABLE interval_consumption_sink7 (
        item STRING,
        val DOUBLE,
        ts  BIGINT,
        isEnergy   BIGINT,
        nodeId     INTEGER,
        siteId     INTEGER,
        shiftId    INTEGER,
        shiftTime  BIGINT,     
        PRIMARY KEY (item, ts) NOT ENFORCED)
    WITH ('connector' = 'upsert-kafka','topic' = 'mocktest27','properties.allow.auto.create.topics' = 'false' ,'properties.bootstrap.servers' = 'broker:9092', 'key.format' = 'avro-confluent', 'key.avro-confluent.schema-registry.url' = 'http://schema-registry:8081','value.format' = 'avro-confluent',
'value.avro-confluent.schema-registry.url' = 'http://schema-registry:8081');


INSERT INTO interval_consumption_sink7
    SELECT 
        item,
        val,
        ts,
        1        AS is_energy,
        2,
        3,
        4,
        10000
    FROM interval_source7 ;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 13:09:02 UTC 2022,,,,,,,,,,"0|z175j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 13:09;martijnvisser;Please provide Jira tickets in English language. ;;;",,,,,,,,,,,,,,,,,,,,
Update Spark3 to create/drop table in Compatibility Matrix ,FLINK-28671,13473266,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zjureel,lzljs3620320,lzljs3620320,25/Jul/22 10:17,16/Nov/22 08:37,04/Jun/24 20:42,16/Nov/22 08:37,,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,https://nightlies.apache.org/flink/flink-table-store-docs-master/docs/engines/overview/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 16 08:37:03 UTC 2022,,,,,,,,,,"0|z175g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/22 08:37;lzljs3620320;master: 7ed6818d822f882c61f17856a6dd7333132bab47;;;",,,,,,,,,,,,,,,,,,,,
Documentation of spark2 is wrong,FLINK-28670,13473261,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,25/Jul/22 10:01,25/Jul/22 10:23,04/Jun/24 20:42,25/Jul/22 10:23,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"DDL is not supported. Spark2 just support:
```
spark.read().format(""tablestore"").load(""file:/tmp/warehouse/default.db/myTable"")
```",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 10:23:42 UTC 2022,,,,,,,,,,"0|z175f4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 10:23;lzljs3620320;master: 
37106b0039d721d492fd802b695d79afde6dd311
b5fdf24b9fca476286055770f1a3c123afefa694
release-0.2: 
ae4fb439bc2a569557a12876d500d87711992134
f6811b8ff0534696d8af56eb3313855646b8c095;;;",,,,,,,,,,,,,,,,,,,,
FileStoreExpireTest.testExpireWithTime failed,FLINK-28669,13473258,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,25/Jul/22 09:56,26/Jul/22 08:58,04/Jun/24 20:42,26/Jul/22 08:58,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,https://github.com/apache/flink-table-store/runs/7496947558?check_suite_focus=true,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 08:58:35 UTC 2022,,,,,,,,,,"0|z175eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 08:58;lzljs3620320;master: 11053c16b3fd9fc75287f386f2baf19bca112db1
release-0.2: 72c6330470cd994aebefdaedc30f14a40ee6718e;;;",,,,,,,,,,,,,,,,,,,,
Fail to create Spark table without primary key,FLINK-28668,13473256,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,qingyue,qingyue,qingyue,25/Jul/22 09:51,26/Jul/22 02:38,04/Jun/24 20:42,26/Jul/22 02:00,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,The primary key list should be empty when pk is not specified.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 02:00:27 UTC 2022,,,,,,,,,,"0|z175e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:00;lzljs3620320;master: e48d9b00ecf29d89aee0c1765a69ca55db86f2f4
release-0.2: a8559f9e159ee821982e93bd7ca7fbd80ff76936;;;",,,,,,,,,,,,,,,,,,,,
"""evictor"" operations of ""window_all"" are supported in Python datastream API",FLINK-28667,13473240,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,cun8cun8,cun8cun8,cun8cun8,25/Jul/22 08:40,17/Oct/22 01:46,04/Jun/24 20:42,17/Oct/22 01:46,,,,,,,,,API / DataStream,API / Python,,,,,,0,,,,,,,"The window allocator based on ""window_all"" adds ""evictor"" operations to align the evictor already supported in the Java API",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-25 08:40:03.0,,,,,,,,,,"0|z175ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alter spark table's primary key should throw exception,FLINK-28666,13473238,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,25/Jul/22 08:35,25/Jul/22 09:22,04/Jun/24 20:42,25/Jul/22 09:22,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 09:22:30 UTC 2022,,,,,,,,,,"0|z175a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 09:22;lzljs3620320;master: 73e20888c528ac31562ce67a5f203e148455e43a
release-0.2: eb1bb9a2f89d0b01a8fdb6809bb95bc521204ad6;;;",,,,,,,,,,,,,,,,,,,,
flink-table-store-benchmark-jar-with-dependencies.jar NOTICE file is wrong,FLINK-28665,13473229,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,25/Jul/22 07:52,01/Aug/22 07:17,04/Jun/24 20:42,01/Aug/22 07:17,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 07:17:29 UTC 2022,,,,,,,,,,"0|z17580:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 07:17;lzljs3620320;master: 567bf755edc00b5e27e6804a4a775380a615966f
release-0.2: ddbf6c8683c7356ffbaa095532687be077ca19fd;;;",,,,,,,,,,,,,,,,,,,,
Support AvroWriters in PyFlink,FLINK-28664,13473212,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,25/Jul/22 06:58,01/Aug/22 04:47,04/Jun/24 20:42,01/Aug/22 04:47,1.15.1,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 04:47:37 UTC 2022,,,,,,,,,,"0|z17548:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 04:47;dianfu;Merged to master via f8f2ebb34f18540f4bfdf2b5db591539199b65e7;;;",,,,,,,,,,,,,,,,,,,,
Allow multiple downstream consumer job vertices sharing the same intermediate dataset at scheduler side,FLINK-28663,13473206,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,25/Jul/22 06:37,08/Aug/22 15:00,04/Jun/24 20:42,08/Aug/22 15:00,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,"Currently, one intermediate dataset can only be consumed by one downstream consumer vertex. If there are multiple consumer vertices consuming the same output of the same upstream vertex, multiple intermediate datasets will be produced. We can optimize this behavior to produce only one intermediate dataset which can be shared by multiple consumer vertices. As the first step, we should allow multiple downstream consumer job vertices sharing the same intermediate dataset at scheduler side. (Note that this optimization only works for blocking shuffle because pipelined shuffle result partition can not be consumed multiple times)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 15:00:15 UTC 2022,,,,,,,,,,"0|z1752w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 15:00;kevin.cyj;Merged into master via 72405361610f1576e0f57ea4e957fafbbbaf0910;;;",,,,,,,,,,,,,,,,,,,,
Compaction may block job cancelling,FLINK-28662,13473158,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,25/Jul/22 02:51,25/Jul/22 06:56,04/Jun/24 20:42,25/Jul/22 06:55,table-store-0.2.0,table-store-0.3.0,,,table-store-0.2.0,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Currently when cancelling a job, we have to wait for current compaction thread to finish. If the compaction takes too long, the task manager may fail due to job cancelling takes more than 180 seconds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 06:56:06 UTC 2022,,,,,,,,,,"0|z174s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 06:56;lzljs3620320;master: 7c2bf3453c68eabd2935f4a8f0275933779f1dd5
release-0.2: ebe5cfa5887085ab9a66b0a2b1b61fc90d418ff4;;;",,,,,,,,,,,,,,,,,,,,
Introduce generic mode for table store catalog,FLINK-28661,13473152,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,25/Jul/22 02:08,29/Mar/23 01:54,04/Jun/24 20:42,29/Mar/23 01:54,,,,,table-store-0.4.0,,,,Table Store,,,,,,,0,,,,,,,"- Introduce a generic option for catalog.
- Table Store catalog can store Flink generic tables when generic is true.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-25 02:08:46.0,,,,,,,,,,"0|z174qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify logs of blocklist,FLINK-28660,13473124,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,24/Jul/22 14:28,26/Jul/22 05:44,04/Jun/24 20:42,26/Jul/22 05:44,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 05:44:47 UTC 2022,,,,,,,,,,"0|z174kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 14:35;wanglijie;[~zhuzh] please assign the ticket to me, thanks :)
 ;;;","26/Jul/22 05:44;zhuzh;Fixed via 64ad6709f412e80c5e48d24127e7a558bed99e8a;;;",,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-java,FLINK-28659,13473120,13417682,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,RocMarshal,RocMarshal,24/Jul/22 13:20,06/Sep/22 08:56,04/Jun/24 20:42,06/Sep/22 08:56,1.16.0,,,,1.17.0,,,,Tests,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 06 08:55:56 UTC 2022,,,,,,,,,,"0|z174k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 13:30;RocMarshal;HI, [~shengkai] , Could you help to assign this ticket to me ? Thx;;;","06/Sep/22 08:55;gaoyunhaii;Merged on master via 9955c85a53d10c85fbca57da33777b940f2e69a9.;;;",,,,,,,,,,,,,,,,,,,
Add document for job lifecycle statements,FLINK-28658,13473096,13441036,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Paul Lin,Paul Lin,Paul Lin,24/Jul/22 02:39,15/Feb/23 02:51,04/Jun/24 20:42,15/Feb/23 02:51,,,,,1.17.0,1.18.0,,,Documentation,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 15 02:51:30 UTC 2023,,,,,,,,,,"0|z174eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 02:51;fsk119;Merged into master: 00e1ef38f98d176b2ed00aed3176b66d9b75ec9c

Merged into release-1.17: e68bac9cce9a1e3a68c49847b4b56bb47603f257;;;",,,,,,,,,,,,,,,,,,,,
Support drop savepoint statement in SqlGatewayService,FLINK-28657,13473095,13441036,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Paul Lin,Paul Lin,24/Jul/22 02:38,24/Jul/22 02:40,04/Jun/24 20:42,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-24 02:38:45.0,,,,,,,,,,"0|z174eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support savepoint statements in SqlGatewayService,FLINK-28656,13473094,13441036,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Paul Lin,Paul Lin,24/Jul/22 02:37,24/Jul/22 02:39,04/Jun/24 20:42,,,,,,,,,,Table SQL / API,Table SQL / Gateway,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-24 02:37:55.0,,,,,,,,,,"0|z174e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support show jobs statement in SqlGatewayService,FLINK-28655,13473093,13441036,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,Paul Lin,Paul Lin,Paul Lin,24/Jul/22 02:36,22/Feb/23 20:40,04/Jun/24 20:42,12/Jan/23 06:27,,,,,1.17.0,,,,Table SQL / API,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 22 20:40:55 UTC 2023,,,,,,,,,,"0|z174e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 06:27;fsk119;Merged into master: f1770892c5b8bce408093fc4c2fa52aafd42d7c6;;;","22/Feb/23 20:40;AlexeyLV;a full link of the PR for simpler future reference: [https://github.com/apache/flink/commit/f1770892c5b8bce408093fc4c2fa52aafd42d7c6#diff-644eb6aba1d893820137572d644e50ec2fe973f99c3a322d1e04139094241030L65]

 ;;;",,,,,,,,,,,,,,,,,,,
Operator includes JMX metric reporter,FLINK-28654,13473074,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dylanmei,dylanmei,dylanmei,23/Jul/22 17:10,25/Jul/22 17:01,04/Jun/24 20:42,25/Jul/22 17:01,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"Documentation states ""well known Metric Reporters are shipped in the operator image and are ready to use,"" but JMX reporter is not included.

When it is included as a plugin it enables local developer to view metrics with simple console tools. JMX also facilitates integration with collectors such as Jolokia and New Relic Java agent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 17:01:16 UTC 2022,,,,,,,,,,"0|z1749s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/22 18:14;gyfora;[~dylanmei] would you like to open a PR for this ? ;;;","23/Jul/22 19:03;dylanmei;Thanks for asking, [~gyfora]. Yes, I will do it.;;;","25/Jul/22 17:01;gyfora;merged to main 4cfda374a44fa1ad5ad96a4fc60f42be91865000;;;",,,,,,,,,,,,,,,,,,
State Schema Evolution does not work - Flink defaults to Kryo serialization even for POJOs and Avro SpecificRecords,FLINK-28653,13473051,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Feedback Received,,peleg68,peleg68,23/Jul/22 08:53,07/Sep/22 18:46,04/Jun/24 20:42,07/Sep/22 18:46,1.14.3,1.15.0,,,,,,,API / Type Serialization System,Runtime / State Backends,,,,,,0,avro,KryoSerializer,pojo,schema-evolution,State,,"I am trying to do a POC of Flink State Schema Evolution. I am using Flink 1.15.0 and Java 11 but also tested on Flink 1.14.3.
I tried to create 3 data classes - one for each serialization type:
1. `io.peleg.kryo.User` - Uses `java.time.Instant` class which I know is not supported for POJO serialization in Flink.
2. `io.peleg.pojo.User` - Uses only classic wrapped primitives - `Integer`, `Long`, `String`. The getters, setters and constructors are generated using Lombok.
3. `io.peleg.avro.User` - Generated from Avro schema using Avro Maven Plugin.

For each class I wrote a stream job that uses a time window to buffer elements and turn them into a list.

For each class I tried to do the following:
1. Run a job
2. Stop with savepoint
3. Add a field to the data class
4. Submit using savepoint

For all data classes the submit with savepoint failed with this exception:
{code:java}
java.lang.Exception: Exception while creating StreamOperatorStateContext.
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_3983d6bb2f0a45b638461bc99138f22f_(2/2) from any of the 1 provided restore options.
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
    ... 11 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:172)
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:106)
    at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:143)
    at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:74)
    at org.apache.flink.runtime.state.StateBackend.createKeyedStateBackend(StateBackend.java:140)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
    ... 13 more
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IndexOutOfBoundsException: Index 83 out of bounds for length 3
Serialization trace:
favoriteColor (io.peleg.avro.User)
    at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
    at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
    at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116)
    at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)
    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:402)
    at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.readKVStateData(HeapSavepointRestoreOperation.java:219)
    at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.readKeyGroupStateData(HeapSavepointRestoreOperation.java:149)
    at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.restore(HeapSavepointRestoreOperation.java:125)
    at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.restore(HeapSavepointRestoreOperation.java:57)
    at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:169)
    ... 20 more
Caused by: java.lang.IndexOutOfBoundsException: Index 83 out of bounds for length 3
    at java.base/jdk.internal.util.Preconditions.outOfBounds(Unknown Source)
    at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Unknown Source)
    at java.base/jdk.internal.util.Preconditions.checkIndex(Unknown Source)
    at java.base/java.util.Objects.checkIndex(Unknown Source)
    at java.base/java.util.ArrayList.get(Unknown Source)
    at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
    at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)
    at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:728)
    at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113)
    ... 31 more{code}
 

I expected this exception would be thrown for io.peleg.kryo.User since Flink does not support state schema evolution for Kryo serialized classes.

But it seems to me like for all classes it ended up using the Kryo serializer instead of the POJO or Avro serializers.

My entire code is public on GitHub [here|http://github.com/peleg68/flink-state-schema-evolution] .

What I would like to achieve is succusfuly running a job from a savepoint of an older version of a POJO with less/more fields.
 
 ","I ran the job on a Flink cluster I spun up using docker compose:
```
version: ""2.2""
services:
  jobmanager:
    image: flink:latest
    ports:
      - ""8081:8081""
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager

  taskmanager:
    image: flink:latest
    depends_on:
      - jobmanager
    command: taskmanager
    scale: 1
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
```

 My machine is a MacBook Pro (14-inch, 2021) with the Apple M1 Pro chip.

I'm running macOS Monterey Version 12.4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,English,,Wed Sep 07 18:11:42 UTC 2022,,,,,,,,,,"0|z1744o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 00:38;kkrugler;What happens if you explicitly define the no-args constructor and getters/setters for User.java, without relying on Lombok?;;;","27/Jul/22 03:28;masteryhx;Hi, Maybe you could try to set

{{env.getConfig().enableForceAvro() to force avro}}

{{or }}

{{{}{}}}{{{}env.getConfig().disableGenericTypes() to disable kryo{}}}

and then see whether it works.;;;","29/Jul/22 11:00;peleg68;[~kkrugler] I don't think the problem is with lombok since the Avro generated class does not use Lombok and it doesn't work as well.

[~masteryhx] I tried using both {{env.getConfig().enableForceAvro() and env.getConfig().disableGenericTypes(). }}It didn't work.

What else can I do?

Is it possible the problem is related to the fact I'm testing this on an ARM processor?

Can anybody maybe try to recreate the problem on their machine?

Unfortunately I don't have access to a machine with an Intel processor.

 ;;;","29/Jul/22 12:11;chesnay;[~peleg68] Please run {{TypeInformation.of(User.class);}} for the lombok POJO with INFO logging enabled and check if the TypeExtractor rejects it as a Pojo.;;;","29/Jul/22 12:46;chesnay;-FYI, these lines are _not_ required for the pojo serializer to be used:-

{code:java}
        env.getConfig().registerPojoType(User.class);
        env.getConfig().disableForceKryo();
{code}

Dang they are actually necessary for state?

[~peleg68] your JobRunner ignores the ExecutionEnvironment argument.
https://github.com/peleg68/flink-state-schema-evolution/blob/main/src/main/java/io/peleg/JobRunner.java#L17
;;;","29/Jul/22 13:16;chesnay;You're also using too many generic parameters; Flink can't infer the type that the functions consume, so Kryo is/should used for everything.

Because the JobRunner has a parameter T, Flink doesn't know anything about the actual type (because at runtime it's just {{Object}}). In contrast, if JobRunner used the type {{User}} Flink could infer more about the data (e.g., that it's a POJO).;;;","30/Jul/22 10:35;peleg68;[~chesnay] Great comments!

I fixed the ignnored environment argument.

I also added a type hint so that Flink could recognize the type even though JobRunner is generic like so:
{code:java}
env.addSource(sourceFunction).returns(clazz){code}
So it helped because now we getting a different error when I restore from a version with a missing field in Avro:
 
{noformat}
2022-07-30 13:26:38 Path in schema: --> endTime at org.apache.avro.generic.GenericData.getDefaultValue(GenericData.java:1176) at org.apache.avro.data.RecordBuilderBase.defaultValue(RecordBuilderBase.java:138) at io.peleg.avro.User$Builder.build(User.java:549) at io.peleg.avro.RandomUserSourceFunction.randomUser(RandomUserSourceFunction.java:45) at io.peleg.avro.RandomUserSourceFunction.run(RandomUserSourceFunction.java:23) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110) at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332){noformat}
 
endTime is the field I added to the Avro schema to test the state schema evolution feature.
I made it nullable so it is weird that Avro throws this org.apache.avro.AvroMissingFieldException.
 
Why do you think this happans?;;;","30/Jul/22 11:29;peleg68;Okay I fixed the AvroMissingFieldException by adding a default value to the endTime field in the Avro schema.

I also added another type hint after the aggregate:
{code:java}
env.addSource(sourceFunction).returns(clazz)
    .keyBy(user -> 0)
    .window(SlidingProcessingTimeWindows.of(
        Time.seconds(3L),
        Time.seconds(1L)
    ))
    .aggregate(new Buffer<T>()).returns(windowOutput) {code}
But this error still occurs:
{noformat}
2022-07-30 14:16:59
java.lang.Exception: Exception while creating StreamOperatorStateContext.
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)
   at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
   at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
   at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
   at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
   at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
   at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_3983d6bb2f0a45b638461bc99138f22f_(2/2) from any of the 1 provided restore options.
   at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
   ... 11 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
   at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:172)
   at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:106)
   at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:143)
   at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:74)
   at org.apache.flink.runtime.state.StateBackend.createKeyedStateBackend(StateBackend.java:140)
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
   at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
   at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
   ... 13 more
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IndexOutOfBoundsException: Index 80 out of bounds for length 3
Serialization trace:
favoriteColor (io.peleg.avro.User)
   at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
   at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
   at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
   at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116)
   at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)
   at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
   at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:402)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.readKVStateData(HeapSavepointRestoreOperation.java:219)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.readKeyGroupStateData(HeapSavepointRestoreOperation.java:149)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.restore(HeapSavepointRestoreOperation.java:125)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.restore(HeapSavepointRestoreOperation.java:57)
   at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:169)
   ... 20 more
Caused by: java.lang.IndexOutOfBoundsException: Index 80 out of bounds for length 3
   at java.base/jdk.internal.util.Preconditions.outOfBounds(Unknown Source)
   at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Unknown Source)
   at java.base/jdk.internal.util.Preconditions.checkIndex(Unknown Source)
   at java.base/java.util.Objects.checkIndex(Unknown Source)
   at java.base/java.util.ArrayList.get(Unknown Source)
   at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
   at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)
   at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:728)
   at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113)
   ... 31 more{noformat}
Even though now I am sure at some parts of the pipeline Flink recognizes io.peleg.avro.User as an Avro class I am also sure the window state is serialized with Kryo.;;;","02/Aug/22 14:20;chesnay;The issue could be that your aggregate function internally uses a List<T>, and there's no way for Flink to know what type T is. The returns only locks in the output type of the AggregateFunction but not the accumulator type. AFAICT there's also nothing the API that allows this to be set.

IOW, you may have remove the generic parameter from the buffer, or (not sure if this works) use a similar trick as we do for OutputTags and create an anonymous class.
{{.aggregate(new Buffer<T>(){})}};;;","27/Aug/22 11:43;peleg68;[~chesnay]

I removed all generic parameters from my code, and I still get this exception when restoring:
{noformat}
2022-08-27 14:29:16
java.lang.Exception: Exception while creating StreamOperatorStateContext.
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:255)
   at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
   at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:700)
   at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:676)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:643)
   at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
   at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
   at java.base/java.lang.Thread.run(Unknown Source)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_3983d6bb2f0a45b638461bc99138f22f_(2/2) from any of the 1 provided restore options.
   at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:346)
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:164)
   ... 11 more
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
   at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:172)
   at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:106)
   at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:143)
   at org.apache.flink.runtime.state.hashmap.HashMapStateBackend.createKeyedStateBackend(HashMapStateBackend.java:74)
   at org.apache.flink.runtime.state.StateBackend.createKeyedStateBackend(StateBackend.java:140)
   at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:329)
   at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
   at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
   ... 13 more
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IndexOutOfBoundsException: Index 194 out of bounds for length 3
Serialization trace:
favoriteColor (io.peleg.avro.User)
   at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
   at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
   at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
   at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116)
   at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)
   at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:761)
   at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:402)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.readKVStateData(HeapSavepointRestoreOperation.java:219)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.readKeyGroupStateData(HeapSavepointRestoreOperation.java:149)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.restore(HeapSavepointRestoreOperation.java:125)
   at org.apache.flink.runtime.state.heap.HeapSavepointRestoreOperation.restore(HeapSavepointRestoreOperation.java:57)
   at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.restoreState(HeapKeyedStateBackendBuilder.java:169)
   ... 20 more
Caused by: java.lang.IndexOutOfBoundsException: Index 194 out of bounds for length 3
   at java.base/jdk.internal.util.Preconditions.outOfBounds(Unknown Source)
   at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Unknown Source)
   at java.base/jdk.internal.util.Preconditions.checkIndex(Unknown Source)
   at java.base/java.util.Objects.checkIndex(Unknown Source)
   at java.base/java.util.ArrayList.get(Unknown Source)
   at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
   at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:805)
   at com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:728)
   at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113)
   ... 31 more{noformat}
What do you think the problem might be?

 ;;;","31/Aug/22 09:10;chesnay;Did you create a new savepoint after removing the generic parameters, or are you trying to restore a previously created savepoint?;;;","02/Sep/22 16:36;peleg68;[~chesnay] I created a new savepoint after removing the generic parameters.;;;","06/Sep/22 13:17;chesnay;My bad, the problem is actually quite obvious and I should have seen it sooner.

Your accumulator is a List, and those are serialized with Kryo.
As a result you can't evolve the contained POJO/avro type, since they never actually go through our POJO/Avro serialization stack.

I'd suggest to use the {{aggregate()}} variant that also accepts a {{TypeInformation}} for the accumulator type, and then pass {{new ListTypeInfo(TypeInformation.of(User.class))}}.

Flink will then use the built-in list serializer which will internally leverage the pojo serializer.;;;","07/Sep/22 18:11;peleg68;[~chesnay] Thanks you so much!

It finally worked with this:
{code:java}
ListTypeInfo<User> userListTypeInfo = new ListTypeInfo<User>(TypeInformation.of(User.class));

env.addSource(sourceFunction).returns(User.class)
        .keyBy(user -> 0)
        .window(SlidingProcessingTimeWindows.of(
                Time.seconds(3L),
                Time.seconds(1L)
        ))
        .aggregate(aggregateFunction, userListTypeInfo, userListTypeInfo)
        .uid(""buffer"")
        .print()
        .uid(""sink-print""); {code};;;",,,,,,,
Amazon Linux 2 based Docker container for Flink,FLINK-28652,13472961,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,apetrenko,apetrenko,22/Jul/22 17:38,22/Jul/22 17:38,04/Jun/24 20:42,,,,,,,,,,flink-docker,,,,,,,0,,,,,,,"Amazon Linux 2 is the Linux distribution which is widely used by projects running on AWS.
It would be nice to have Amazon Linux 2 based container for Flink in such cases so it's possible to standardize the AWS running product on a single Linux distribution.

I currently have a prototype of such a container which we're verifying to work.
And I'm ready to donate the code as soon as it's completed.
It requires just minor changes related to packages installation and jemalloc library location in the docker-entrypoint.sh",AWS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-22 17:38:09.0,,,,,,,,,,"0|z173ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JarRunHandler ignores restore mode set in the configuration,FLINK-28651,13472948,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nuafonso,nuafonso,nuafonso,22/Jul/22 16:44,15/Nov/22 09:09,04/Jun/24 20:42,15/Nov/22 09:09,1.15.0,1.15.1,,,1.17.0,,,,Runtime / REST,,,,,,,0,pull-request-available,stale-assigned,,,,,"Hello all,

 

I started a Flink cluster with execution.savepoint-restore-mode set to CLAIM. Then, I submitted a job through the REST API without specifying restoreMode in the request. I would expect Flink to use CLAIM, but the job used NO_CLAIM as restore mode.

 

After looking into the source code, I believe the issue comes from the way JarRunHandler picks the default for restoreMode. It directly gets it from the default of execution.savepoint-restore-mode instead of looking into the existing configuration: [https://github.com/apache/flink/blob/release-1.15.1/flink-runtime-web/src/main/java/org/apache/flink/runtime/webmonitor/handlers/JarRunHandler.java#L150]

 

I think the fix can be achieved by getting execution.savepoint-restore-mode from the configuration.

 

Looking forward to hearing your feedback.",,,,,,,,,,,,,,,,,,,,,,,,FLINK-29543,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 28 22:38:02 UTC 2022,,,,,,,,,,"0|z173hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 09:37;chesnay;Sounds good, do you want to open a PR [~nuafonso] ?;;;","26/Jul/22 09:25;nuafonso;Hi [~chesnay],

Yes, I'm planning to open a PR soon.;;;","28/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,
Flink SQL Parsing bug for METADATA,FLINK-28650,13472925,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,qinjunjerry,qinjunjerry,22/Jul/22 14:38,18/Oct/23 01:20,04/Jun/24 20:42,18/Oct/23 01:20,1.14.4,,,,,,,,Table SQL / API,,,,,,,0,,,,,,,"With the following source/sink tables:
{code:sql}
CREATE TABLE sourceTable ( 
`key` INT, 
`time` TIMESTAMP(3),
`value` STRING NOT NULL, 
id INT 
) 
WITH ( 
'connector' = 'datagen', 
'rows-per-second'='10', 
'fields.id.kind'='sequence', 
'fields.id.start'='1', 
'fields.id.end'='100' 
);

CREATE TABLE sinkTable1 ( 
`time` TIMESTAMP(3) METADATA FROM 'timestamp', 
`value` STRING NOT NULL
) 
WITH ( 
  'connector' = 'kafka',
...
)
CREATE TABLE sinkTable2 ( 
`time` TIMESTAMP(3),    -- without METADATA
`value` STRING NOT NULL
) 
WITH ( 
  'connector' = 'kafka',
...
)
{code}
the following three pass the validation:
{code:sql}
INSERT INTO sinkTable1
SELECT 
`time`, 
`value`
FROM sourceTable;

INSERT INTO sinkTable2
SELECT 
`time`, 
`value`
FROM sourceTable;

INSERT INTO sinkTable2 (`time`,`value`)
SELECT 
`time`, 
`value`
FROM sourceTable;
{code}
but this one does not:
{code:sql}
INSERT INTO sinkTable1 (`time`,`value`)
SELECT 
`time`, 
`value`
FROM sourceTable;
{code}
It failed with 

{code:java}
Unknown target column 'time'
{code}

It seems when providing column names in INSERT, the METADATA have an undesired effect. ",,,,,,,,,,,,,,,,,,,FLINK-30922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 01:19:45 UTC 2023,,,,,,,,,,"0|z173co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 15:19;JasonLee;hi [~qinjunjerry] ,maybe u can try the following
{code:java}
// code placeholder
INSERT INTO sinkTable1
SELECT 
`time`, 
`value`
FROM sourceTable; {code};;;","22/Jul/22 16:55;qinjunjerry;Thanks [~JasonLee]. Your example indeed works, and it is included in my description above. The JIRA is to fix the issue when column names are explicitly specified. 
[~jark] FYI;;;","25/Jul/22 09:44;jark;Yes. I think this is a bug in partial insert (insert with specified columns).
;;;","30/Aug/22 06:57;qinjunjerry;Is anyone interested in fixing this?;;;","16/Oct/23 07:58;xu_shuai_;This bug  is fixed in FLINK-30922.;;;","18/Oct/23 01:19;lincoln.86xy;[~xu_shuai_] Thanks for helping to confirm this! ;;;",,,,,,,,,,,,,,,
Allow adopting unmanaged jobs as FlinkSessionJob resource,FLINK-28649,13472918,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,gyfora,gyfora,22/Jul/22 14:22,25/Oct/22 11:39,04/Jun/24 20:42,25/Oct/22 11:39,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"We should allow users to create a FlinkSessionJob resource for already existing jobs deployed to a managed session cluster.

We should add a configuration option to specify the jobId of the running job, and then the operator would assume that it is running according to the initial spec provided.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 25 11:39:02 UTC 2022,,,,,,,,,,"0|z173b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 14:22;gyfora;cc [~jeesmon] ;;;","25/Oct/22 11:39;gyfora;This feature doesn't bring any real benefit to the user compared to manually stopping the old job with a savepoint and creating a FlinkSessionJob CR afterwards.;;;",,,,,,,,,,,,,,,,,,,
Allow session deletion to block on any running job,FLINK-28648,13472916,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bvarga,gyfora,gyfora,22/Jul/22 14:19,22/Feb/23 13:05,04/Jun/24 20:42,,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"Currently session FlinkDeployment deletion blocks on existing FlinkSessionJob-s for that cluster.

We could add the option to block on any running job in case it is an unmanaged job deployed through the Flink CLI directly",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 14:19:14 UTC 2022,,,,,,,,,,"0|z173ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 14:19;gyfora;cc [~jeesmon] ;;;",,,,,,,,,,,,,,,,,,,,
Remove separate error handling and adjust documentation for CLAIM mode + RocksDB native savepoint,FLINK-28647,13472897,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,22/Jul/22 12:17,05/Aug/22 06:40,04/Jun/24 20:42,05/Aug/22 06:40,1.16.0,,,,1.16.0,,,,Documentation,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,,"After FLINK-25872, checkpoint folder deletion is not performed as long as there is some state from that checkpoint used by other checkpoints.
Therefore, the following changes could be reverted/adjusted:
* FLINK-25745 e8bcbfd5a48fd8d3ca48ef7803867569214e0dbc Do not log exception
* FLINK-25745 c1f5c5320150402fc0cb4fbf3a31f9a27b1e4d9a Document incremental savepoints in CLAIM mode limitation

cc: [~Yanfei Lei], [~dwysakowicz]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25745,FLINK-25872,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 06:40:23 UTC 2022,,,,,,,,,,"0|z1736g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Aug/22 06:40;roman;Merged as 17a782c202c93343b8884cb52f4562f9c4ba593f.;;;",,,,,,,,,,,,,,,,,,,,
Handle scaling operation separately in reconciler/service ,FLINK-28646,13472865,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,gyfora,gyfora,22/Jul/22 09:09,25/Apr/23 15:55,04/Jun/24 20:42,26/Aug/22 09:01,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"The standalone integration for session clusters and application clusters (with reactive mode), allows for more efficient scaling operations when only the parallelism changes.

We should distinguish this opration in the reconciler/service and implement this for standalone mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 26 09:01:36 UTC 2022,,,,,,,,,,"0|z172zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Aug/22 09:01;gyfora;merged to main 0a346ec7c6e8b770c1c10aa01686ee68c9dd4f27;;;",,,,,,,,,,,,,,,,,,,,
Clean up logging in FlinkService / Reconciler,FLINK-28645,13472863,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,ZhenqiuHuang,gyfora,gyfora,22/Jul/22 09:07,19/Feb/24 15:28,04/Jun/24 20:42,,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"With the introduction of standalone mode, logging in the service implementation / reconcilers became a bit chatotic and often redundant.

We should ensure that we log consistently around cluster operations such as cancellation, deletion, submission etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-22 09:07:59.0,,,,,,,,,,"0|z172yw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support collecting arbitrary number of streams,FLINK-28644,13472861,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,22/Jul/22 09:07,26/Jul/22 11:00,04/Jun/24 20:42,26/Jul/22 11:00,,,,,1.16.0,,,,API / DataStream,,,,,,,0,pull-request-available,,,,,,"Extend the collect api to support collecting multiple streams, as outlined in [FLIP-251|https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=217390264&draftShareId=55e665b7-8973-4765-b3b5-54756e3a9e4c&].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 11:00:01 UTC 2022,,,,,,,,,,"0|z172yg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 11:00;chesnay;master:
1b8e776a915fd243e9088fb05be603b446cc663d
d788f0a2af3e13ba2b56f6abd88531332cf62593;;;",,,,,,,,,,,,,,,,,,,,
Job archive file contains too much Job json file on Flink HistoryServer ,FLINK-28643,13472860,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dangshazi,dangshazi,dangshazi,22/Jul/22 09:05,16/Aug/23 22:35,04/Jun/24 20:42,,1.15.1,,,,,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,stale-assigned,,,,,"h1. Problem

In History Server, 'HistoryServerArchiveFetcher' fetch job archived file,and pass it into a job dir.

There are more than 1w json files for big parallism job which run out of inodes

!image-2022-10-13-16-39-46-486.png!
h1. Suggestion
 # Generate a big json file for each job instead of a lot of small json file
 # Support lazy fetcher/unzip of archivedJobFile 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/22 09:04;dangshazi;image-2022-07-22-17-04-55-046.png;https://issues.apache.org/jira/secure/attachment/13047116/image-2022-07-22-17-04-55-046.png","30/Sep/22 06:45;dangshazi;image-2022-09-30-14-45-10-846.png;https://issues.apache.org/jira/secure/attachment/13049968/image-2022-09-30-14-45-10-846.png","13/Oct/22 08:39;dangshazi;image-2022-10-13-16-39-25-887.png;https://issues.apache.org/jira/secure/attachment/13050877/image-2022-10-13-16-39-25-887.png","13/Oct/22 08:39;dangshazi;image-2022-10-13-16-39-46-486.png;https://issues.apache.org/jira/secure/attachment/13050878/image-2022-10-13-16-39-46-486.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:17 UTC 2023,,,,,,,,,,"0|z172y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 16:39;bgeng777;I run into this issue as well when running jobs with large parallism. One possible remedy is to limit the number of the archieved jobs. But a better way may be add limit in flink side so that it would not break down the whole hdfs system.;;;","30/Sep/22 06:51;dangshazi;Hdfs system is safe. History files of each job is archived on HDFS.

So I think json files should be merged to reduce number of json files

!image-2022-09-30-14-45-10-846.png!;;;","17/Oct/22 08:59;lsy;[~xtsong] Can you help to take a glance at this ticket?;;;","19/Oct/22 02:10;dangshazi;[~lsy] I'm making an improvement by add a lazy unzip of archivedJobFile.
May I take this issue ?;;;","25/Oct/22 09:37;xtsong;That sounds like a nice improvement. Thanks [~dangshazi]. You are assigned.;;;","08/Feb/23 10:24;dangshazi;Design doc: [History Server support lazy unzip|https://docs.google.com/document/d/1o7YgXhHJxsObkduHLsr4YSwS8T-mo-tzLWwpsRceMNc/edit?usp=sharing];;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,
Cannot create sink for Temporary table in table store catalog,FLINK-28642,13472856,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,22/Jul/22 08:53,23/Jul/22 07:15,04/Jun/24 20:42,23/Jul/22 07:15,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Using Flink 1.14. Temporary table in table store catalog will use the Factory provided by catalog.
We need to do some delegate work like HiveDynamicTableFactory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jul 23 07:15:07 UTC 2022,,,,,,,,,,"0|z172xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jul/22 07:15;lzljs3620320;master: f8f0f64c3f6ecaf81374659b2d2d0da895766e86
release-0.2: 8fa384217f974f69356f301ee5801220146b8e95;;;",,,,,,,,,,,,,,,,,,,,
Remove StreamingFileSink,FLINK-28641,13472851,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,martijnvisser,martijnvisser,22/Jul/22 08:32,28/Apr/23 14:04,04/Jun/24 20:42,,,,,,,,,,Connectors / FileSystem,,,,,,,0,,,,,,,"With the StreamingFileSink being marked as deprecated via FLINK-27188, we should be able to remove the StreamingFileSink implementation completely after 1.16 is released. ",,,,,,,,,,,FLINK-30627,FLINK-30166,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-22 08:32:42.0,,,,,,,,,,"0|z172w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let BlocklistDeclarativeSlotPool accept duplicate slot offers,FLINK-28640,13472833,13450987,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,22/Jul/22 06:20,22/Jul/22 12:57,04/Jun/24 20:42,22/Jul/22 12:57,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,"BlocklistSlotPool should accept a duplicate (already accepted) slot offer, even if it is from a currently blocked task manager",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 12:57:18 UTC 2022,,,,,,,,,,"0|z172s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 07:31;wanglijie;[~zhuzh] please assign the ticket to me, thanks :);;;","22/Jul/22 12:57;zhuzh;Done via f6a22eaf99d4ba2ef03445bae54a0da7c39c4d1a;;;",,,,,,,,,,,,,,,,,,,
Preserve distributed consistency of OperatorEvents from subtasks to OperatorCoordinator,FLINK-28639,13472832,13427369,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,yunfengzhou,yunfengzhou,yunfengzhou,22/Jul/22 05:57,16/Aug/23 22:35,04/Jun/24 20:42,,1.14.3,,,,,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,stale-assigned,,,,,"This is the second step to solving the consistency issue of OC communications. In this step, we would also guarantee the consistency of operator events sent from subtasks to OCs. Combined with the other subtask to preserve the consistency of communications in the reverse direction, all communications between OC and subtasks would be consistent across checkpoints and global failovers.

To achieve the goal of this step, we need to add closing/reopening functions to the subtasks' gateways and make the subtasks aware of a checkpoint before they receive the checkpoint barriers. The general process would be as follows.

1. When the OC starts checkpoint, it notifies all subtasks about this information.
2. After being notified about the ongoing checkpoint in OC, a subtask sends a special operator event to its OC, which is the last operator event the OC could receive from the subtask before the subtask completes the checkpoint. Then the subtask closes its gateway.
3. After receiving this special event from all subtasks, the OC finishes its checkpoint and closes its gateway. Then the checkpoint coordinator sends checkpoint barriers to the sources.
4. If the subtask or the OC generate any event to send to each other, they buffer the events locally.
5. When a subtask starts checkpointing, it also stores the buffered events in the checkpoint.
6. After the subtask completes the checkpoint, communications in both directions are recovered and the buffered events are sent out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 16 22:35:17 UTC 2023,,,,,,,,,,"0|z172s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Restrict ALTER TABLE from setting write-mode,FLINK-28638,13472822,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,22/Jul/22 03:43,22/Jul/22 04:54,04/Jun/24 20:42,22/Jul/22 04:54,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,Restrict conversion between changelog table and append-only table,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 04:54:33 UTC 2022,,,,,,,,,,"0|z172ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 04:54;lzljs3620320;master: af6cee84076476b73d677d5a15f74852565e94c4
release-0.2: bf34ec1ecbf8a244293a37a0517f555b49bb5420;;;",,,,,,,,,,,,,,,,,,,,
High vulnerability in flink-kubernetes-operator-1.1.0-shaded.jar,FLINK-28637,13472724,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeesmon,jbusche,jbusche,21/Jul/22 16:18,25/Jul/22 15:45,04/Jun/24 20:42,25/Jul/22 15:45,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"I noticed a high vulnerability in the flink-kubernetes-operator-1.1.0-shaded.jar file.

=======

cvss: 7.5

riskFactors: Has fix,High severity

cve: PRISMA-2022-0239    

link: https://github.com/square/okhttp/issues/6738

status: fixed in 4.9.2

packagePath: /flink-kubernetes-operator/flink-kubernetes-operator-1.1.0-shaded.jar

description: com.squareup.okhttp3_okhttp packages prior to version 4.9.2 are vulnerable for sensitive information disclosure. An illegal character in a header value will cause IllegalArgumentException which will include full header value. This applies to Authorization, Cookie, Proxy-Authorization and Set-Cookie headers. 

=======

It looks like we're using version 3.12.12, and there's no plans to provide this fix for the 3.x version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 15:45:27 UTC 2022,,,,,,,,,,"0|z17240:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 16:23;gyfora;We need to make sure that fabric8 and java-operator-sdk use a version of this library that has this fix and then bump those versions.

Based on the description this should not really affect the operator as the http client is not exposed to the user.;;;","21/Jul/22 16:25;gyfora;I think it would be risky for us to manually upgrade this transitive dependency to a new major version as it is likely to cause some unintended behaviour in the fabric8 client.;;;","21/Jul/22 18:14;jbusche;Thanks Gyula - makes complete since as it could require some extensive testing.;;;","21/Jul/22 19:23;mbalassi;For reference at the moment this is the transitive dependency hierarchy that we are pulling this through:

|  +- io.fabric8:kubernetes-client:jar:5.12.2:provided

|  |  +- com.squareup.okhttp3:okhttp:jar:3.12.12:provided

As [~gyfora] suggested we best keep the fabric8 version in synch with our JOSDK dependency which at the currently used 3.0.3 version also uses 5.12.2 of the fabric8 client.

It seems that the fabric8 community is currently working on their 6.0.0 release:

[https://github.com/fabric8io/kubernetes-client/releases/tag/v6.0.0-RC1]

But this still has the same okhttp version as listed above:

[https://github.com/fabric8io/kubernetes-client/blob/v6.0.0-RC1/pom.xml#L84]

Looking through their open issues and PRs I have not found an issue for bumping the okhttp version, but found this relevant:

[https://github.com/fabric8io/kubernetes-client/issues/2764]

[~jbusche] would you mind opening an issue on the fabric8 client to report this issue and ask their community whether they think this is relevant and if they would bump the version given this or they will rather merge the PR that tries to make the HTTP client agnostic that I linked above.;;;","21/Jul/22 20:57;jbusche;Sure [~mbalassi] I've opened [issue 4290|https://github.com/fabric8io/kubernetes-client/issues/4290], thanks! ;;;","22/Jul/22 01:45;wangyang0918;It seems that we also have this vulnerability in Flink project since we are using 3.14.9 there. So I am in favor of not making this ticket as a blocker.;;;","22/Jul/22 09:39;mbalassi;Thanks, [~jbusche].

fyi [~wangyang0918], [~gyfora]:

[https://github.com/fabric8io/kubernetes-client/issues/4290#issuecomment-1192194532]

I think we are fine for the 1.1 operator release as is, but it might make sense to explore swapping the dependency version later. What do you think?;;;","22/Jul/22 09:55;gyfora;We also need to check with the JOSDK team if they plan on migrating to fabric8 6.0.0 soon, in that case we could get rid of okhttp completely instead of swapping the dependency.;;;","22/Jul/22 12:59;jeesmon;There is a new PR against OSDK to bump up okhttp version: https://github.com/fabric8io/kubernetes-client/issues/4290#issuecomment-1192511844;;;","22/Jul/22 14:49;mbalassi;Fortunately both the Fabric8 and the JOSDK community was very responsive, this gives a path for fixing this. However given the following:
 
1. The HTTP client is internal to the operator, this vulnerability is very unlikely to affect it,
2. We also need to bump the dependency within the Flink native k8s integration,
3. We need extensive testing to make sure the new dependency version behaves properly,
 
My suggestion is to release 1.1.0 with this as a known issue and fix it in 1.1.1. That said we can merge a fix for it to the release-1.1 as soon as possible, so folks who are prohibited to use the 1.1.0 version can roll their own image from source.
 
The JOSDK folks offered to produce a new patch release that we can depend on in 1.1.1.;;;","22/Jul/22 15:48;jeesmon;[~mbalassi] Like JOSDK explicitly set okhttp version, can we use the same approach in 1.1.0 until we can upgrade JOSDK? That way we don't need to ship a new version with vulnerability. I'm using this approach locally in 1.0.1 and happy to create a PR. All e2e tests are passing with okhttp version upgrade.

Just adding the diff here so we can refer in case we decide not to fix it now and someone need it to satisfy their internal security requirements.
{code:java}
diff --git a/flink-kubernetes-operator/pom.xml b/flink-kubernetes-operator/pom.xml
index 6e85b8c..e82a5e9 100644
--- a/flink-kubernetes-operator/pom.xml
+++ b/flink-kubernetes-operator/pom.xml
@@ -143,6 +143,28 @@ under the License.
             <version>${junit.jupiter.version}</version>
             <scope>test</scope>
         </dependency>
+
+        <!--
+            regarding the okhttp explicit version
+            see https://github.com/fabric8io/kubernetes-client/issues/4290
+            and https://issues.apache.org/jira/browse/FLINK-28637
+            -->
+        <dependency>
+            <groupId>com.squareup.okhttp3</groupId>
+            <artifactId>okhttp</artifactId>
+            <version>${okhttp.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.squareup.okhttp3</groupId>
+            <artifactId>logging-interceptor</artifactId>
+            <version>${okhttp.version}</version>
+        </dependency>
+        <dependency>
+            <groupId>com.squareup.okhttp3</groupId>
+            <artifactId>mockwebserver</artifactId>
+            <version>${okhttp.version}</version>
+            <scope>test</scope>
+        </dependency>
     </dependencies>

     <build>
diff --git a/pom.xml b/pom.xml
index 279f0b5..9f04d01 100644
--- a/pom.xml
+++ b/pom.xml
@@ -79,6 +79,8 @@ under the License.

         <spotless.version>2.4.2</spotless.version>
         <it.skip>true</it.skip>
+
+        <okhttp.version>4.10.0</okhttp.version>
     </properties>

     <dependencyManagement>
{code};;;","22/Jul/22 15:52;gyfora;Personally I am not extremely confident in simply swapping out a HttpClient implementation and releasing it with only minimal testing. The current JOSDK, fabric8, okhttp clients have been tested in various cloud environments for weeks/months.

It would be a real shame to introduce instability or any other problems for fixing a vulnaribility that cannot reasonable surface for the operator. Especially by doing a last minute change like that.

Please open a PR, we can merge this for the main/release-1.1 branches and release a patch release after 1-2 weeks of testing.;;;","25/Jul/22 15:45;gyfora;Merged

main: f8f8b96273eae68b10bb24eff01c5d44db5b10f0
release-1.1: 651a165f542bb1a491e0e23fbd3ca98eccacde79;;;",,,,,,,,
Add utility to test POJO compliance,FLINK-28636,13472697,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Jul/22 13:44,26/Jul/22 08:35,04/Jun/24 20:42,26/Jul/22 08:35,,,,,1.16.0,,,,API / DataStream,Tests,,,,,,0,pull-request-available,,,,,,"Users should be encouraged to eagerly verify that their POJOs satisfy all the requirements that Flink imposes, however we provide no convenient way to test that.

They currently have to resort to something like below, which isn't obvious at all:
{code:java}
TypeSerializer<Event> eventSerializer =
            TypeInformation.of(Event.class).createSerializer(new ExecutionConfig());
assertThat(eventSerializer).isInstanceOf(PojoSerializer.class);{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 08:35:15 UTC 2022,,,,,,,,,,"0|z171y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 08:35;chesnay;master: 94c3daac4a96a68009a22825fb1fd4ac0bb92195;;;",,,,,,,,,,,,,,,,,,,,
HybridSourceReader availabilityFuture always completed,FLINK-28635,13472692,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,litzhf,litzhf,21/Jul/22 13:24,10/Aug/22 00:09,04/Jun/24 20:42,,1.15.0,1.15.1,,,,,,,Connectors / Common,,,,,,,0,,,,,,,"HybridSourceReader may contains multi SourceReader.

AvailabilityFuture is variable in some reader like kafka, but AvailabilityFuture  is not always changing in HybridSourceReader.

This leads to a problem: 
 * task busy time is calculated based on idle time
 * idle time is based on operator AvailabilityFuture
 * When the idle time is constant, it will always be busy

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 00:09:43 UTC 2022,,,,,,,,,,"0|z171ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 00:09;mason6345;I think this is a duplicate of https://issues.apache.org/jira/browse/FLINK-27479;;;",,,,,,,,,,,,,,,,,,,,
Add a simple Json (De) SerializationSchema,FLINK-28634,13472691,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Jul/22 13:10,28/Jul/22 12:29,04/Jun/24 20:42,28/Jul/22 12:29,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,"Add a basic schema to read/write JSON.

This is so common that users shouldn't have to implement that themselves.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 12:29:07 UTC 2022,,,,,,,,,,"0|z171wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 12:29;chesnay;master: ae45bd3c50abf8b2621a5de31410ae381f7ffa04;;;",,,,,,,,,,,,,,,,,,,,
Allow to GetTables in the HiveServer2 Endpoint,FLINK-28633,13472688,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,fsk119,fsk119,21/Jul/22 12:50,08/Aug/22 02:45,04/Jun/24 20:42,07/Aug/22 14:33,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28855,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 07 14:33:18 UTC 2022,,,,,,,,,,"0|z171w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Aug/22 14:33;fsk119;Merged into the master: 

782982fc99b076d1b6317bd6ece420961e3e391a;;;",,,,,,,,,,,,,,,,,,,,
Allow to GetColumns/GetTableTypes/GetPrimaryKeys in the HiveServer2 Endpoint,FLINK-28632,13472687,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,fsk119,fsk119,21/Jul/22 12:48,10/Aug/22 06:12,04/Jun/24 20:42,10/Aug/22 05:59,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 10 05:59:03 UTC 2022,,,,,,,,,,"0|z171vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Aug/22 05:59;fsk119;Merged into master: d0a5023f9896663552e4b8df3b12efb1c8a8c814;;;",,,,,,,,,,,,,,,,,,,,
Allow to GetFunctions in the HiveServer2 Endpoint,FLINK-28631,13472686,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,21/Jul/22 12:47,09/Aug/22 02:10,04/Jun/24 20:42,09/Aug/22 02:10,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 02:10:39 UTC 2022,,,,,,,,,,"0|z171vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 02:10;fsk119;Merged into master: def2f5438090779fa23862027de9b1c4db36b21f;;;",,,,,,,,,,,,,,,,,,,,
Allow to GetSchemas in the HiveServer2 Endpoint,FLINK-28630,13472685,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,yzl,fsk119,fsk119,21/Jul/22 12:47,05/Aug/22 14:35,04/Jun/24 20:42,05/Aug/22 14:35,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 05 14:35:12 UTC 2022,,,,,,,,,,"0|z171vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 02:43;yzl;please assign to me.;;;","05/Aug/22 14:35;fsk119;Merged into master:
82417ab9a5ad6935a73f9b7d1bc6bd4f47ce4f61;;;",,,,,,,,,,,,,,,,,,,
Allow to GetCatalogs in the HiveServer2 Endpoint,FLINK-28629,13472683,13451461,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,fsk119,fsk119,fsk119,21/Jul/22 12:40,04/Aug/22 12:36,04/Jun/24 20:42,04/Aug/22 12:36,1.16.0,,,,,,,,Connectors / Hive,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,Implement to getCatalogs API in the HiveServer2 Endpoint.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 12:36:57 UTC 2022,,,,,,,,,,"0|z171uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 12:36;fsk119;Merged into master: 

e3b26c283ca3ad266978b719d21e2da72ff225da

4b73437524fe9718a50feed2ce47c9e17c27ed98;;;",,,,,,,,,,,,,,,,,,,,
Introduce operation execution plugin,FLINK-28628,13472675,13430553,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,21/Jul/22 11:57,26/Jul/22 08:45,04/Jun/24 20:42,26/Jul/22 08:45,,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,,,,,,"Hive dialect may has his own operation execution logic for some operations. So, it'll be better introduce a operation excution plugin to delegate Hive or other dialects's execution logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 08:45:41 UTC 2022,,,,,,,,,,"0|z171t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 08:45;jark;Fixed in master: c7af66be7cb5c8148dd8bd258267bb7d2550f10e;;;",,,,,,,,,,,,,,,,,,,,
KafkaITCase.testBrokerFailure failed with TestTimedOutException: test timed out after 60000 milliseconds,FLINK-28627,13472674,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Cannot Reproduce,,hxbks2ks,hxbks2ks,21/Jul/22 11:47,13/Sep/22 03:26,04/Jun/24 20:42,13/Sep/22 03:26,1.16.0,,,,1.16.0,,,,Connectors / Kafka,,,,,,,0,test-stability,,,,,,"
{code:java}
2022-07-21T04:34:59.2437774Z Jul 21 04:34:59 org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds
2022-07-21T04:34:59.2440379Z Jul 21 04:34:59 	at sun.misc.Unsafe.park(Native Method)
2022-07-21T04:34:59.2441037Z Jul 21 04:34:59 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2022-07-21T04:34:59.2442035Z Jul 21 04:34:59 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2022-07-21T04:34:59.2442762Z Jul 21 04:34:59 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2022-07-21T04:34:59.2443457Z Jul 21 04:34:59 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2022-07-21T04:34:59.2444437Z Jul 21 04:34:59 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-07-21T04:34:59.2445085Z Jul 21 04:34:59 	at org.apache.flink.test.util.TestUtils.tryExecute(TestUtils.java:67)
2022-07-21T04:34:59.2445878Z Jul 21 04:34:59 	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runBrokerFailureTest(KafkaConsumerTestBase.java:1506)
2022-07-21T04:34:59.2446794Z Jul 21 04:34:59 	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testBrokerFailure(KafkaITCase.java:112)
2022-07-21T04:34:59.2447512Z Jul 21 04:34:59 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-21T04:34:59.2448170Z Jul 21 04:34:59 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-21T04:34:59.2448908Z Jul 21 04:34:59 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-21T04:34:59.2449574Z Jul 21 04:34:59 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-21T04:34:59.2450323Z Jul 21 04:34:59 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-21T04:34:59.2451516Z Jul 21 04:34:59 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-21T04:34:59.2452471Z Jul 21 04:34:59 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-21T04:34:59.2453408Z Jul 21 04:34:59 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-21T04:34:59.2454188Z Jul 21 04:34:59 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
2022-07-21T04:34:59.2454996Z Jul 21 04:34:59 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
2022-07-21T04:34:59.2455687Z Jul 21 04:34:59 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-21T04:34:59.2456240Z Jul 21 04:34:59 	at java.lang.Thread.run(Thread.java:748)
2022-07-21T04:34:59.2456674Z Jul 21 04:34:59 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38507&view=logs&j=aa18c3f6-13b8-5f58-86bb-c1cffb239496&t=502fb6c0-30a2-5e49-c5c2-a00fa3acb203",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 13 03:26:09 UTC 2022,,,,,,,,,,"0|z171sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 11:47;hxbks2ks;cc [~renqs] ;;;","13/Sep/22 03:26;hxb;Given that it hasn't appeared for two months, I will close this issue. Please reopen the ticket if the case appears again. ;;;",,,,,,,,,,,,,,,,,,,
RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState failed with FileNotFoundException,FLINK-28626,13472672,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,hxbks2ks,hxbks2ks,21/Jul/22 11:39,12/Aug/22 08:06,04/Jun/24 20:42,12/Aug/22 08:06,1.16.0,,,,1.16.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-07-21T04:14:20.8815245Z Jul 21 04:14:20 [ERROR] org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState  Time elapsed: 17.495 s  <<< ERROR!
2022-07-21T04:14:20.8816943Z Jul 21 04:14:20 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2022-07-21T04:14:20.8818144Z Jul 21 04:14:20 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2022-07-21T04:14:20.8819625Z Jul 21 04:14:20 	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:93)
2022-07-21T04:14:20.8821068Z Jul 21 04:14:20 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.restoreAndAssert(RescaleCheckpointManuallyITCase.java:229)
2022-07-21T04:14:20.8822767Z Jul 21 04:14:20 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingKeyedState(RescaleCheckpointManuallyITCase.java:147)
2022-07-21T04:14:20.8824888Z Jul 21 04:14:20 	at org.apache.flink.test.checkpointing.RescaleCheckpointManuallyITCase.testCheckpointRescalingInKeyedState(RescaleCheckpointManuallyITCase.java:115)
2022-07-21T04:14:20.8826253Z Jul 21 04:14:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-21T04:14:20.8827387Z Jul 21 04:14:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-21T04:14:20.8828651Z Jul 21 04:14:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-21T04:14:20.8829804Z Jul 21 04:14:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-21T04:14:20.8830909Z Jul 21 04:14:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-21T04:14:20.8832209Z Jul 21 04:14:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-21T04:14:20.8833685Z Jul 21 04:14:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-21T04:14:20.8834929Z Jul 21 04:14:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-21T04:14:20.8836083Z Jul 21 04:14:20 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-21T04:14:20.8837322Z Jul 21 04:14:20 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-21T04:14:20.8838517Z Jul 21 04:14:20 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-21T04:14:20.8839670Z Jul 21 04:14:20 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-21T04:14:20.8840778Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-21T04:14:20.8841995Z Jul 21 04:14:20 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-21T04:14:20.8843136Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-21T04:14:20.8844590Z Jul 21 04:14:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-21T04:14:20.8845750Z Jul 21 04:14:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-21T04:14:20.8846777Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-21T04:14:20.8863771Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-21T04:14:20.8865160Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-21T04:14:20.8866042Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-21T04:14:20.8867111Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-21T04:14:20.8868244Z Jul 21 04:14:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-21T04:14:20.8869232Z Jul 21 04:14:20 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-21T04:14:20.8870076Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-21T04:14:20.8870708Z Jul 21 04:14:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-21T04:14:20.8871304Z Jul 21 04:14:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-21T04:14:20.8872096Z Jul 21 04:14:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-21T04:14:20.8872756Z Jul 21 04:14:20 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-21T04:14:20.8874193Z Jul 21 04:14:20 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-21T04:14:20.8875381Z Jul 21 04:14:20 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-21T04:14:20.8876305Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-21T04:14:20.8877492Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-21T04:14:20.8878537Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-21T04:14:20.8880053Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-21T04:14:20.8881201Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-21T04:14:20.8882004Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-21T04:14:20.8882763Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-21T04:14:20.8884157Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-21T04:14:20.8885538Z Jul 21 04:14:20 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-21T04:14:20.8887119Z Jul 21 04:14:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-21T04:14:20.8888580Z Jul 21 04:14:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-21T04:14:20.8889928Z Jul 21 04:14:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-21T04:14:20.8891021Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-21T04:14:20.8891768Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-21T04:14:20.8892448Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-21T04:14:20.8893141Z Jul 21 04:14:20 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-21T04:14:20.8894148Z Jul 21 04:14:20 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2022-07-21T04:14:20.8895611Z Jul 21 04:14:20 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:139)
2022-07-21T04:14:20.8897152Z Jul 21 04:14:20 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:83)
2022-07-21T04:14:20.8898332Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:257)
2022-07-21T04:14:20.8899810Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:248)
2022-07-21T04:14:20.8901228Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:240)
2022-07-21T04:14:20.8902622Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:736)
2022-07-21T04:14:20.8904411Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:713)
2022-07-21T04:14:20.8906179Z Jul 21 04:14:20 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
2022-07-21T04:14:20.8907708Z Jul 21 04:14:20 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:477)
2022-07-21T04:14:20.8908979Z Jul 21 04:14:20 	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
2022-07-21T04:14:20.8910204Z Jul 21 04:14:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-21T04:14:20.8911365Z Jul 21 04:14:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-21T04:14:20.8912807Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)
2022-07-21T04:14:20.8914590Z Jul 21 04:14:20 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2022-07-21T04:14:20.8916179Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)
2022-07-21T04:14:20.8917591Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)
2022-07-21T04:14:20.8919097Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)
2022-07-21T04:14:20.8920536Z Jul 21 04:14:20 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)
2022-07-21T04:14:20.8921840Z Jul 21 04:14:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2022-07-21T04:14:20.8923032Z Jul 21 04:14:20 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2022-07-21T04:14:20.8924524Z Jul 21 04:14:20 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2022-07-21T04:14:20.8925603Z Jul 21 04:14:20 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2022-07-21T04:14:20.8926835Z Jul 21 04:14:20 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2022-07-21T04:14:20.8927999Z Jul 21 04:14:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2022-07-21T04:14:20.8929267Z Jul 21 04:14:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-07-21T04:14:20.8930424Z Jul 21 04:14:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2022-07-21T04:14:20.8931618Z Jul 21 04:14:20 	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2022-07-21T04:14:20.8932624Z Jul 21 04:14:20 	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2022-07-21T04:14:20.8933999Z Jul 21 04:14:20 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2022-07-21T04:14:20.8935237Z Jul 21 04:14:20 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2022-07-21T04:14:20.8936275Z Jul 21 04:14:20 	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2022-07-21T04:14:20.8937399Z Jul 21 04:14:20 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2022-07-21T04:14:20.8938443Z Jul 21 04:14:20 	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2022-07-21T04:14:20.8939444Z Jul 21 04:14:20 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2022-07-21T04:14:20.8940598Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-07-21T04:14:20.8941795Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-07-21T04:14:20.8943108Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-07-21T04:14:20.8944484Z Jul 21 04:14:20 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-07-21T04:14:20.8945754Z Jul 21 04:14:20 Caused by: java.lang.Exception: Exception while creating StreamOperatorStateContext.
2022-07-21T04:14:20.8947259Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
2022-07-21T04:14:20.8949110Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:268)
2022-07-21T04:14:20.8950972Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
2022-07-21T04:14:20.8952682Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:723)
2022-07-21T04:14:20.8954586Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
2022-07-21T04:14:20.8955961Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699)
2022-07-21T04:14:20.8957056Z Jul 21 04:14:20 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:666)
2022-07-21T04:14:20.8958332Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
2022-07-21T04:14:20.8959729Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
2022-07-21T04:14:20.8960812Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
2022-07-21T04:14:20.8961814Z Jul 21 04:14:20 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
2022-07-21T04:14:20.8962645Z Jul 21 04:14:20 	at java.lang.Thread.run(Thread.java:748)
2022-07-21T04:14:20.8963814Z Jul 21 04:14:20 Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for StreamFlatMap_20ba6b65f97481d5570070de90e4e791_(1/3) from any of the 1 provided restore options.
2022-07-21T04:14:20.8965034Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
2022-07-21T04:14:20.8965992Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
2022-07-21T04:14:20.8967579Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
2022-07-21T04:14:20.8968966Z Jul 21 04:14:20 	... 11 more
2022-07-21T04:14:20.8969810Z Jul 21 04:14:20 Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
2022-07-21T04:14:20.8971107Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:405)
2022-07-21T04:14:20.8972946Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:503)
2022-07-21T04:14:20.8974883Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:98)
2022-07-21T04:14:20.8976443Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.lambda$createKeyedStateBackend$1(AbstractChangelogStateBackend.java:145)
2022-07-21T04:14:20.8977963Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:69)
2022-07-21T04:14:20.8979280Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:92)
2022-07-21T04:14:20.8980302Z Jul 21 04:14:20 	at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
2022-07-21T04:14:20.8981411Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
2022-07-21T04:14:20.8982399Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
2022-07-21T04:14:20.8983579Z Jul 21 04:14:20 	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
2022-07-21T04:14:20.8984609Z Jul 21 04:14:20 	... 13 more
2022-07-21T04:14:20.8986340Z Jul 21 04:14:20 Caused by: java.io.FileNotFoundException: /tmp/junit2530256108929796473/junit8041656567883419966/87c3128889b96de94a21d4f7e1fdf2a2/taskowned/03c0e5d4-a423-4c9f-b9fb-b5aed08dda7a (No such file or directory)
2022-07-21T04:14:20.8987775Z Jul 21 04:14:20 	at java.io.FileInputStream.open0(Native Method)
2022-07-21T04:14:20.8988345Z Jul 21 04:14:20 	at java.io.FileInputStream.open(FileInputStream.java:195)
2022-07-21T04:14:20.8988969Z Jul 21 04:14:20 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
2022-07-21T04:14:20.8989679Z Jul 21 04:14:20 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
2022-07-21T04:14:20.8990401Z Jul 21 04:14:20 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
2022-07-21T04:14:20.8991271Z Jul 21 04:14:20 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
2022-07-21T04:14:20.8992295Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.downloadDataForStateHandle(RocksDBStateDownloader.java:127)
2022-07-21T04:14:20.8993367Z Jul 21 04:14:20 	at org.apache.flink.contrib.streaming.state.RocksDBStateDownloader.lambda$createDownloadRunnables$0(RocksDBStateDownloader.java:110)
2022-07-21T04:14:20.8994408Z Jul 21 04:14:20 	at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:49)
2022-07-21T04:14:20.8995182Z Jul 21 04:14:20 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
2022-07-21T04:14:20.8995914Z Jul 21 04:14:20 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-21T04:14:20.8996630Z Jul 21 04:14:20 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-21T04:14:20.8997142Z Jul 21 04:14:20 	... 1 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38504&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 12 08:06:08 UTC 2022,,,,,,,,,,"0|z171sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 08:37;Yanfei Lei;Hi [~hxbks2ks], could you please assign it to me?;;;","01/Aug/22 08:39;hxbks2ks;Thanks [~Yanfei Lei]. I have assigned it to you.;;;","01/Aug/22 11:30;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38992&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10133;;;","03/Aug/22 11:33;hxb;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39185&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","12/Aug/22 08:06;yunta;merged in master: 7cf71585a603866822ed0aee3978eea8d8587eee;;;",,,,,,,,,,,,,,,,
Guard against changing target session deployment for sessionjob,FLINK-28625,13472671,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,gyfora,gyfora,21/Jul/22 11:39,03/Aug/22 09:53,04/Jun/24 20:42,03/Aug/22 09:53,kubernetes-operator-1.0.1,kubernetes-operator-1.1.0,,,kubernetes-operator-1.1.1,kubernetes-operator-1.2.0,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,We should guard against changing the target session cluster as this can lead to an inconsistent state within the operator and make it difficult to delete the resources.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 09:53:37 UTC 2022,,,,,,,,,,"0|z171s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 11:39;gyfora;cc [~aitozi] ;;;","31/Jul/22 05:19;aitozi;Make sense, I will push a PR for this.;;;","03/Aug/22 07:33;gyfora;Merged to main: 0cba89cdea5e19b92b1000c7e19e325b6d7625a2;;;","03/Aug/22 09:53;gyfora;merged to release-1.1: 67f0770c04714aa17fa86ca53b01bac2467a98c5;;;",,,,,,,,,,,,,,,,,
CsvReaderFormat should accept CsvMapper factory,FLINK-28624,13472654,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Jul/22 10:00,26/Jul/22 10:51,04/Jun/24 20:42,26/Jul/22 10:51,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,"The CsvReaderFormat allows the user to pass a {{CsvMapper}} via the factory methods.

This is a flawed approach because we generally cannot assume that the mapper is actually serializable.

For example adding the JavaTimeModule breaks serialization because it internally uses the DateTimeFormatter.",,,,,,,,,,,,,,FLINK-28621,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 10:51:29 UTC 2022,,,,,,,,,,"0|z171og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 10:51;chesnay;master: 1dee35a9966065593aa5dfc618f5fec17fb80890;;;",,,,,,,,,,,,,,,,,,,,
Optimize the use of off heap memory by blocking and hybrid shuffle reader,FLINK-28623,13472652,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,21/Jul/22 09:53,09/Aug/22 15:47,04/Jun/24 20:42,09/Aug/22 15:47,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Currently, each FileReader(PartitionFileReader or HsSubpartitionFileReaderImpl) will internally allocate a headerbuffer with the size of 8B. Beside, PartitionFileReader also has a 12B indexEntryBuf. Because FileReader is of subpartition granularity, If the parallelism becomes very big, and there are many slots on each TM, the memory occupation will even reach the MB level. In fact, all FileReader of the same ResultPartition read data in a single thread, so we only need to allocate a headerbuffer to a ResultPartition to optimize this phenomenon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 15:47:58 UTC 2022,,,,,,,,,,"0|z171o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 15:47;kevin.cyj;Merged into master via 87d4f70e49255b551d0106117978b1aa0747358c;;;",,,,,,,,,,,,,,,,,,,,
Can't restore a flink job that uses Table API and Kafka connector with savepoint,FLINK-28622,13472651,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,norris3n,norris3n,21/Jul/22 09:52,21/Jul/22 12:05,04/Jun/24 20:42,,1.15.0,,,,,,,,Table SQL / API,,,,,,,0,,,,,,,"I canceled a flink job with a savepoint, then tried to restore the job with the savepoint (just using the same jar file) but it said it cannot map savepoint state. I was just using the same jar file so I think the execution plan and generated operator ID should be the same? (Flink version has not been changed)
 
Related errors:
{code:java}
used by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/root/flink-savepoints/savepoint-5f285c-c2749410db07. Cannot map checkpoint/savepoint state for operator dd5fc1f28f42d777f818e2e8ea18c331 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.

used by: java.lang.IllegalStateException: Failed to rollback to checkpoint/savepoint file:/root/flink-savepoints/savepoint-5f285c-c2749410db07. Cannot map checkpoint/savepoint state for operator dd5fc1f28f42d777f818e2e8ea18c331 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI. {code}
My code:
{code:java}
public final class FlinkJob {

    public static void main(String[] args) {

        final String JOB_NAME = ""FlinkJob"";

        final EnvironmentSettings settings = EnvironmentSettings.inStreamingMode();
        final TableEnvironment tEnv = TableEnvironment.create(settings);
        tEnv.getConfig().set(""pipeline.name"", JOB_NAME);
        tEnv.getConfig().setLocalTimeZone(ZoneId.of(""UTC""));

        tEnv.executeSql(""CREATE TEMPORARY TABLE ApiLog ("" +
                ""  `_timestamp` TIMESTAMP(3) METADATA FROM 'timestamp' VIRTUAL,"" +
                ""  `_partition` INT METADATA FROM 'partition' VIRTUAL,"" +
                ""  `_offset` BIGINT METADATA FROM 'offset' VIRTUAL,"" +
                ""  `Data` STRING,"" +
                ""  `Action` STRING,"" +
                ""  `ProduceDateTime` TIMESTAMP_LTZ(6),"" +
                ""  `OffSet` INT"" +
                "") WITH ("" +
                ""  'connector' = 'kafka',"" +
                ""  'topic' = 'api.log',"" +
                ""  'properties.group.id' = 'flink',"" +
                ""  'properties.bootstrap.servers' = '<mykafkahost...>',"" +
                ""  'format' = 'json',"" +
                ""  'json.timestamp-format.standard' = 'ISO-8601'"" +
                "")"");

        tEnv.executeSql(""CREATE TABLE print_table ("" +
                "" `_timestamp` TIMESTAMP(3),"" +
                "" `_partition` INT,"" +
                "" `_offset` BIGINT,"" +
                "" `Data` STRING,"" +
                "" `Action` STRING,"" +
                "" `ProduceDateTime` TIMESTAMP(6),"" +
                "" `OffSet` INT"" +
                "") WITH ('connector' = 'print')"");

        tEnv.executeSql(""INSERT INTO print_table"" +
                "" SELECT * FROM ApiLog"");

    }

} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://stackoverflow.com/questions/73018059/cant-restore-a-flink-job-that-uses-table-api-and-kafka-connector-with-savepoint,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 12:05:44 UTC 2022,,,,,,,,,,"0|z171ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 12:05;martijnvisser;[~renqs] Any idea why this would fail? I'm under the impression that if the Flink version, the SQL statement and nothing else changes, savepoints should be supported?;;;",,,,,,,,,,,,,,,,,,,,
Register Java 8 modules in all internal object mappers,FLINK-28621,13472644,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,21/Jul/22 09:15,08/Aug/22 17:08,04/Jun/24 20:42,08/Aug/22 17:08,,,,,1.16.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,"In FLINK-25588 we extended flink-shaded-jackson to also bundle the jackson extensions for handling Java 8 time / optional classes, but barely any of the internal object mappers were adjusted to register said module.

We can improve the user experience by always registering this module (in cases where users can provide a mapper), and solve some incompatibilities in others (like the JsonNodeDeserializationSchema).

 ",,,,,,,,,,,FLINK-28624,FLINK-28808,FLINK-28807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 08 17:08:44 UTC 2022,,,,,,,,,,"0|z171m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Aug/22 17:08;chesnay;master:
9a967c010a58e0b2277516068256ef45ee711edc
c0bf0ac3fb1fe4814bff09807ed2040bb13da052
328007f0b9a3e4da31b20e75b94d9c339b168af0;;;",,,,,,,,,,,,,,,,,,,,
SQL Client doesn't properly print values of INTERVAL type,FLINK-28620,13472629,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,jark,jark,21/Jul/22 08:50,25/Jun/23 07:52,04/Jun/24 20:42,25/Jun/23 07:52,,,,,,,,,Table SQL / Client,,,,,,,0,,,,,,,"The display of values of interval type should follow the CAST rules. However, currently, SQL Client prints it using {{Period.toString()}} and {{Duration.toString()}} which is not SQL standard compliant. 

{code}
Flink SQL> select interval '9-11' year to month;
+--------+
| EXPR$0 |
+--------+
|  P119M |
+--------+
1 row in set

Flink SQL> select cast(interval '9-11' year to month as varchar);
+--------+
| EXPR$0 |
+--------+
|  +9-11 |
+--------+
1 row in set

Flink SQL> select interval '2 1:2:3' day to second;
+-----------+
|    EXPR$0 |
+-----------+
| PT49H2M3S |
+-----------+
1 row in set

Flink SQL> select cast(interval '2 1:2:3' day to second as varchar);
+-----------------+
|          EXPR$0 |
+-----------------+
| +2 01:02:03.000 |
+-----------------+
1 row in set
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 25 07:51:55 UTC 2023,,,,,,,,,,"0|z171iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jun/23 11:36;Sergey Nuyanzin;[~jark] it looks like it was fixed at some  point of time
currently i can not reproduce it...
should we still have it opened?;;;","25/Jun/23 07:51;jark;[~Sergey Nuyanzin] Thank you for the validation. I will close it then. ;;;",,,,,,,,,,,,,,,,,,,
flink sql window aggregation using early fire will produce empty data,FLINK-28619,13472578,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,simen,simen,21/Jul/22 03:27,21/Jul/22 06:16,04/Jun/24 20:42,,1.15.0,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,,,,0,,,,,,,"sql is as follows:

 
{code:java}
set table.exec.emit.early-fire.enabled=true;
set table.exec.emit.early-fire.delay=1s;
set table.exec.resource.default-parallelism = 1;
CREATE TABLE source_table
(
    id   int,
    name VARCHAR,
    age  int,
    proc_time AS PROCTIME()
) WITH (
      'connector' = 'datagen'
      ,'rows-per-second' = '3'
      ,'number-of-rows' = '1000'
      ,'fields.id.min' = '1'
      ,'fields.id.max' = '1'
      ,'fields.age.min' = '1'
      ,'fields.age.max' = '150'
      ,'fields.name.length' = '3'
      );
CREATE TABLE sink_table
(
    id     int,
    name   VARCHAR,
    ageAgg int,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
      'connector' = 'upsert-kafka'
      ,'properties.bootstrap.servers' = 'localhost:9092'
      ,'topic' = 'aaa'
      ,'key.format' = 'json'
      ,'value.format' = 'json'
      ,'value.fields-include' = 'ALL'
      );
INSERT
INTO sink_table
select id,
       last_value(name) as name,
       sum(age)         as ageAgg
from source_table
group by tumble(proc_time, interval '1' day), id;
{code}
Result received in kafka:

 
{code:java}
{""id"":1,""name"":""efe"",""ageAgg"":455}
null
{""id"":1,""name"":""96a"",""ageAgg"":701}
null
{""id"":1,""name"":""d71"",""ageAgg"":1289}
null
{""id"":1,""name"":""89c"",""ageAgg"":1515}{code}
 

 

Is the extra null normal?

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 03:56:10 UTC 2022,,,,,,,,,,"0|z1717k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:30;simen;[~Zsigner] [~Leo Zhou] [~jark] can you take a look at it？;;;","21/Jul/22 03:56;Leo Zhou;when using upsert-kafka as a sink, it  will write DELETE data as Kafka messages with null values. So，result received in kafka is ok. For more infomation, you can see [Upsert Kafka|https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/upsert-kafka/];;;",,,,,,,,,,,,,,,,,,,
Cannot use hive.dialect on master ,FLINK-28618,13472575,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,liubox123,liubox123,21/Jul/22 03:07,12/Oct/22 04:19,04/Jun/24 20:42,12/Oct/22 04:19,,,,,,,,,Connectors / Hive,,,,,,,0,,,,,,,"I build the newest master flink and copy \{hive-exec-2.3.9.jar;flink-sql-connector-hive-2.3.9_2.12-1.16-SNAPSHOT.jar;flink-connector-hive_2.12-1.16-SNAPSHOT.jar} to $FLINK_HOME/lib 。

 

then ， i got faild in sql-client  !image-2022-07-21-11-01-12-395.png!

and after copy opt/flink-table-planner_2.12-1.16-SNAPSHOT.jar 

even cannot open the sql-client 

!image-2022-07-21-11-04-12-552.png!

 

so , what's wronge? ","hadoop_class  2.10

openjdk11",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:01;liubox123;image-2022-07-21-11-01-12-395.png;https://issues.apache.org/jira/secure/attachment/13047052/image-2022-07-21-11-01-12-395.png","21/Jul/22 03:04;liubox123;image-2022-07-21-11-04-12-552.png;https://issues.apache.org/jira/secure/attachment/13047051/image-2022-07-21-11-04-12-552.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 12 04:19:35 UTC 2022,,,,,,,,,,"0|z1716w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 08:05;chesnay;Did you remove the flink-table-planner-loader jar from lib?;;;","22/Jul/22 06:45;liubox123;already did that  ""Did you remove the flink-table-planner-loader jar from lib?""  

 

and sql-client can open  if i remove link-sql-connector-hive* out from lib  。

it's so strange

 ;;;","19/Aug/22 12:03;luoyuxia;[~liubox123] I think the current master has fixed this problem. Could you please try it again?;;;","12/Oct/22 04:19;luoyuxia;Close it since I haven't reproduced this problem using  master branch. Feel free to open it if the problem still exists.;;;",,,,,,,,,,,,,,,,,
Support stop job statement in SqlGatewayService,FLINK-28617,13472571,13441036,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,Paul Lin,Paul Lin,Paul Lin,21/Jul/22 02:44,18/Dec/22 03:54,04/Jun/24 20:42,18/Dec/22 03:54,,,,,,,,,Table SQL / Gateway,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 18 03:54:33 UTC 2022,,,,,,,,,,"0|z17160:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/22 03:54;fsk119;Merged into master: 934edd37dee44fc7c8708f09d7b715cd5e8b3404;;;",,,,,,,,,,,,,,,,,,,,
Operator quickstart and examples docs should use configured stable version,FLINK-28616,13472508,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,gyfora,gyfora,20/Jul/22 17:00,21/Jul/22 13:39,04/Jun/24 20:42,21/Jul/22 13:39,,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"The Kubernetes Operator documentation contains several parts that refer to the current stable versions.  A good example would be a quickstart which uses helm repo link for the last stable release. 

We should change the logic so that this always points to the configured stable release (part of config doc). This way we would avoid the need for upgrading this as part of every release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 13:39:13 UTC 2022,,,,,,,,,,"0|z170s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 13:39;gyfora;merged to main 7ced741f51a99f2093ce8a45c8c92879a247f836;;;",,,,,,,,,,,,,,,,,,,,
"Add K8s recommend labels to the JM deployments, JM services created by operator",FLINK-28615,13472507,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,haoxin,haoxin,20/Jul/22 16:55,16/Aug/22 02:43,04/Jun/24 20:42,16/Aug/22 02:43,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"I'm using the Flink operator with Argo CD, it will be nice if we can add the K8s recommend labels to the deployments and services. Such as:
{code:java}
labels:    
  app.kubernetes.io/managed-by: apache-flink-operator    
  app.kubernetes.io/part-of: flink-session-cluster-a {code}
With this, the users can see all the resources created by FlinkDeployment in the Argo CD Web UI

 

See also:

[https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/#labels]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 24 11:47:22 UTC 2022,,,,,,,,,,"0|z170rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jul/22 07:32;ConradJam;I think we can can add this  label in helm flink-operator.yaml to delopy flink k8s operator,for example

 

 
{code:java}
metadata:
  labels:
    app.kubernetes.io/name: flink-k8s-operator
    app.kubernetes.io/instance: flink-k8s-operator
    app.kubernetes.io/version: ""1.1.0""
    app.kubernetes.io/component: operator
    app.kubernetes.io/part-of: flink
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/created-by: controller-manager {code}
 

 

[~haoxin] [~gyfora] what do you think ？if assign this suggest, will edit flink-operator.yaml add this labels;;;","24/Jul/22 11:45;haoxin;Sorry for the unclear description, I'm saying the Flink cluster deployments, not the operator itself.;;;","24/Jul/22 11:47;haoxin;For the operator itself, it's also better to have these labels ~;;;",,,,,,,,,,,,,,,,,,
Empty local state folders not cleanup on retrieving local state,FLINK-28614,13472371,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,Yanfei Lei,Yanfei Lei,20/Jul/22 05:04,12/Aug/22 04:34,04/Jun/24 20:42,12/Aug/22 04:34,1.15.0,1.15.1,1.16.0,,1.16.0,,,,Runtime / Coordination,,,,,,,0,,,,,,,"It would create a checkpoint directory when trying to load {{TaskStateSnapshot}} from the disk. The local checkpoint directory is not deleted on exit {{tryLoadTaskStateSnapshotFromDisk() }}even though {{TaskStateSnapshot}} doesn't exist. 

 
{code:java}
File getTaskStateSnapshotFile(long checkpointId) {
    final File checkpointDirectory =
            localRecoveryConfig
                    .getLocalStateDirectoryProvider()
                    .orElseThrow(
                            () -> new IllegalStateException(""Local recovery must be enabled.""))
                    .subtaskSpecificCheckpointDirectory(checkpointId);

    if (!checkpointDirectory.exists() && !checkpointDirectory.mkdirs()) {
        throw new FlinkRuntimeException(
                String.format(
                        ""Could not create the checkpoint directory '%s'"", checkpointDirectory));
    }

    return new File(checkpointDirectory, TASK_STATE_SNAPSHOT_FILENAME);
} {code}
 

 

This will cause the folder in /{{{}localState{}}} to remain after failover. Here is an example: 
{code:java}
41854 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Restoring job 35644df535ca04613d6a6116dcfcfd59 from Checkpoint 2 @ 1658292943408 for 35644df535ca04613d6a6116dcfcfd59 located at file:/var/folders/4n/q3r37vws2f910rt_f469kwg00000gn/T/junit1426665332205293555/junit63847204117629783/35644df535ca04613d6a6116dcfcfd59/chk-2.

_______________________________________
directory of localState
_______________________________________ 
tm_2
    │   ├── blobStorage
    │   ├── localState
    │   │   └── aid_6df21e53ca06ea69ee0643d25d27dbee
    │   │       └── jid_35644df535ca04613d6a6116dcfcfd59
    │   │           └── vtx_0a448493b4782967b150582570326227_sti_1
    │   │               ├── chk_2
    │   │               └── chk_5
    │   │                   ├── _task_state_snapshot
    │   │                   ├── edab98058083464a9ca29b6d7a950c68
    │   │                   │   ├── 000014.sst
    │   │                   │   ├── 000015.sst
    │   │                   │   ├── 000022.sst
    │   │                   │   ├── 000023.sst
    │   │                   │   ├── CURRENT
    │   │                   │   ├── MANIFEST-000018
    │   │                   │   └── OPTIONS-000021
    │   │                   └── f3724ae6-fd24-4e9a-80a8-02aa34bca0f0 {code}
cc: [~trohrmann] , [~masteryhx] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28581,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 07:21:24 UTC 2022,,,,,,,,,,"0|z16zxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 07:21;Yanfei Lei;Solved in aef75be34d99c737f5c565703a971027ac44f855..52519a8eb695c9523c546439c66910b15f19be20.

See [this discussion|https://github.com/apache/flink/pull/19907#discussion_r926200002].;;;",,,,,,,,,,,,,,,,,,,,
PyFlink 1.15 unable to start in Application Mode in k8s,FLINK-28613,13472285,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Workaround,,clivewongts,clivewongts,19/Jul/22 13:30,19/Jul/22 18:52,04/Jun/24 20:42,19/Jul/22 18:52,1.15.1,,,,,,,,Client / Job Submission,,,,,,,0,,,,,,,"I recently bumped my PyFlink job from 1.14 to 1.15, and the job is failing with build 1.15 in k8s.

The error is due to NetUtils not able to getAvailablePort. I suspect this is related to the version bump of py4j from 0.10.8.1 to 0.10.9.3 in required by apache-flink 1.15 in python.

The error stack is:
{code:java}
2022-07-19 11:17:06,225 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.
2022-07-19 11:17:06,226 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Starting resource manager service.
2022-07-19 11:17:06,227 INFO  org.apache.flink.runtime.resourcemanager.ResourceManagerServiceImpl [] - Resource manager service is granted leadership with session id 00000000-0000-0000-0000-000000000000.
2022-07-19 11:17:06,229 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs that are not finished, yet.
2022-07-19 11:17:06,229 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.
2022-07-19 11:17:06,306 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_0 .
2022-07-19 11:17:06,309 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka://flink/user/rpc/resourcemanager_1 .
2022-07-19 11:17:06,317 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
2022-07-19 11:17:06,401 INFO  org.apache.flink.client.ClientUtils                          [] - Starting program (detached: true)
2022-07-19 11:17:06,500 WARN  org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap [] - Application failed unexpectedly: 
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.
    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346) ~[?:?]
    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063) ~[?:?]
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) ~[?:?]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:323) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:244) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171) ~[flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41) ~[flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) [flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) [?:?]
    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) [?:?]
    at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) [?:?]
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) [?:?]
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183) [?:?]
Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.
    ... 14 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.lang.RuntimeException: Could not find a free permitted port on the machine.
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:291) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    ... 13 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not find a free permitted port on the machine.
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]
    at org.apache.flink.client.python.PythonEnvUtils.startGatewayServer(PythonEnvUtils.java:387) ~[?:?]
    at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:75) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
    at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:291) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    ... 13 more
Caused by: java.lang.RuntimeException: Could not find a free permitted port on the machine.
    at org.apache.flink.util.NetUtils.getAvailablePort(NetUtils.java:177) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.python.PythonEnvUtils.lambda$startGatewayServer$3(PythonEnvUtils.java:365) ~[?:?]
    at java.lang.Thread.run(Thread.java:834) ~[?:?]
2022-07-19 11:17:06,505 ERROR org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Fatal error occurred in the cluster entrypoint.
java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.
    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346) ~[?:?]
    at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063) ~[?:?]
    at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) ~[?:?]
    at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) ~[?:?]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:323) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$2(ApplicationDispatcherBootstrap.java:244) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
    at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:171) ~[flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68) ~[flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$withContextClassLoader$0(ClassLoadingUtils.java:41) ~[flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) [flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_73d9230b-9d22-4143-8bbc-2ab5d539166f.jar:1.15.0-stream1]
    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) [?:?]
    at java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) [?:?]
    at java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) [?:?]
    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) [?:?]
    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183) [?:?]
Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could not execute application.
    ... 14 more
Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.lang.RuntimeException: Could not find a free permitted port on the machine.
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:291) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    ... 13 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not find a free permitted port on the machine.
    at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) ~[?:?]
    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) ~[?:?]
    at org.apache.flink.client.python.PythonEnvUtils.startGatewayServer(PythonEnvUtils.java:387) ~[?:?]
    at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:75) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]
    at java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:291) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    ... 13 more
Caused by: java.lang.RuntimeException: Could not find a free permitted port on the machine.
    at org.apache.flink.util.NetUtils.getAvailablePort(NetUtils.java:177) ~[flink-dist-1.15.0-stream1.jar:1.15.0-stream1]
    at org.apache.flink.client.python.PythonEnvUtils.lambda$startGatewayServer$3(PythonEnvUtils.java:365) ~[?:?]
    at java.lang.Thread.run(Thread.java:834) ~[?:?]
2022-07-19 11:17:06,508 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting StandaloneApplicationClusterEntryPoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2022-07-19 11:17:06,509 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124 {code}
It's the same with Python3.7 & Python3.8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 18:51:49 UTC 2022,,,,,,,,,,"0|z16zeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 18:51;clivewongts;Turns out it's because of this change:
[https://github.com/apache/flink/blob/adbf09fb941c8f793df6d322ed95df87bc4254f3/flink-core/src/main/java/org/apache/flink/util/NetUtils.java#L166]

that attempts to write to a path that flink doesn't have access to. We fixed it by chmod the path it tries to write (flink bin path) in the container.

I'd recommend giving option as an env variable so that FileLock can be created a different directory.;;;",,,,,,,,,,,,,,,,,,,,
Cancel pending slot allocation after canceling executions,FLINK-28612,13472264,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuzh,zhuzh,zhuzh,19/Jul/22 12:35,20/Jul/22 08:28,04/Jun/24 20:42,20/Jul/22 08:28,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,"Canceling pending slot allocation before canceling executions will result in execution failures  and pollute the logs. It will also result in an execution to be FAILED even if the execution vertex has FINISHED, which breaks the assumption of SpeculativeScheduler#isExecutionVertexPossibleToFinish().",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 20 08:28:03 UTC 2022,,,,,,,,,,"0|z16z9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 08:28;zhuzh;Fixed via 3278995372d1ea27b6fd86806e9a860a644694c7;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer for ElementwiseProduct,FLINK-28611,13472252,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,19/Jul/22 11:30,03/Aug/22 04:10,04/Jun/24 20:42,03/Aug/22 04:10,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,Support ElementwiseProduct in FlinkML.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 04:10:32 UTC 2022,,,,,,,,,,"0|z16z74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 04:10;zhangzp;fixed via 0f1ad10b0760abea9c3551f9985f9fdd2a077f54;;;",,,,,,,,,,,,,,,,,,,,
Enable speculative execution of sources,FLINK-28610,13472251,13470079,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,19/Jul/22 11:17,26/Jul/22 06:45,04/Jun/24 20:42,26/Jul/22 06:45,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,Currently speculative execution of sources is disabled. It can be enabled with the improvement done to support InputFormat sources and new sources to work correctly with speculative execution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 06:45:48 UTC 2022,,,,,,,,,,"0|z16z6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 06:45;zhuzh;Done via 9192446847b6fb29beb8d36d49d6900de1e61685;;;",,,,,,,,,,,,,,,,,,,,
Flink-Pulsar connector fails on larger schemas,FLINK-28609,13472248,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,jacek_wislicki,jacek_wislicki,19/Jul/22 11:05,30/Aug/22 02:33,04/Jun/24 20:42,30/Aug/22 02:33,1.14.3,1.14.4,1.14.5,1.15.1,1.14.6,1.15.3,1.16.0,,Connectors / Pulsar,,,,,,,1,pull-request-available,,,,,,"When a model results in a larger schema (this seems to be related to its byte array representation), the number of expected bytes to read is different than the number of actually read bytes: [^exception.txt]. The ""read"" is such a case is always 1018 while the expected ""byteLen"" gives a greater value. For smaller schemata, the numbers are equal (less than 1018) and no issue occurs.

The problem reproduction is on [GitHub|https://github.com/JacekWislicki/vp-test2]. There are 2 simple jobs (SimpleJob1 and SimpleJob2) using basic models for the Pulsar source definition (PulsarMessage1 and PulsarMessage2, respectively). Each of the corresponding schemata is properly serialised and deserialised, unless an effective byte array length becomes excessive (marked with ""the problem begins"" in model classes). The fail condition can be achieved by a number of fields (PulsarMessage1) or just longer field names (PulsarMessage2). The problem occurs on either Avro or a JSON schema set in the Pulsar source definition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/22 14:43;syhily;[FLINK-28609][Connector_Pulsar]_PulsarSchema_didn't_get_properly_serialized_.patch;https://issues.apache.org/jira/secure/attachment/13047081/%5BFLINK-28609%5D%5BConnector_Pulsar%5D_PulsarSchema_didn%27t_get_properly_serialized_.patch","19/Jul/22 10:50;jacek_wislicki;exception.txt;https://issues.apache.org/jira/secure/attachment/13046970/exception.txt",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 02:33:59 UTC 2022,,,,,,,,,,"0|z16z68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 13:30;syhily;Thanks for your report.;;;","21/Jul/22 14:43;syhily;This bug is easy to fix. I'll submit a PR tonight.;;;","21/Jul/22 14:54;jacek_wislicki;Great, thank you, Yufan!;;;","21/Jul/22 19:26;syhily;[~jacek_wislicki] The PR is ready. But I don't know when we can merge it to master and backport to 1.14 and 1.15.;;;","21/Jul/22 20:20;jacek_wislicki;[~syhily] - it would be perfect to have it in any of the current releases. Still, if this is not going to be possible, we will need to wait for 1.16. Do you know when we could expect this one? ;;;","22/Jul/22 06:42;syhily;[~jacek_wislicki] You can use streamnative self-maintained connector. https://github.com/streamnative/flink/releases;;;","25/Jul/22 07:31;jacek_wislicki;Thank you, but unfortunately we are constrained to used only official releases that can be pulled from public Maven repos. For development purposes, we will be using the old (deprecated) connector version from StreamNative/Pulsar.

 

Do you know a time plan from Flink 1.16 to be released? I see that there is a lot of work done but still some issues are open and being discussed.  ;;;","25/Jul/22 08:17;martijnvisser;The Flink 1.16 release branch will be cut in two weeks, see https://cwiki.apache.org/confluence/display/FLINK/1.16+Release;;;","25/Jul/22 08:52;jacek_wislicki;It's perfect, thank you, [~martijnvisser]!;;;","10/Aug/22 12:52;AleksandraSarna;Hi [~martijnvisser] ,

Could you please confirm that the fix for this issue is still in scope of 1.16 release?

Regards,
Aleksandra;;;","11/Aug/22 18:57;syhily;[~AleksandraSarna] We will get this merged soon after the FLINK-27399;;;","12/Aug/22 07:04;AleksandraSarna;Thanks for the information [~syhily].;;;","30/Aug/22 02:33;tison;master via d9bcbffc006481c09a8d2e04aa05cc92cd5c80d2
1.15 via cbd6c8d47b43f67c15d3a7d7bfb9b42b3e7d033b
1.14 via 55cf3b37bc6c864cc6ce5b7e9913de2de00b95cb;;;","30/Aug/22 02:33;tison;Thank you for reporting and taking care of this issue!;;;",,,,,,,
Make Hadoop FS token renewer configurable,FLINK-28608,13472209,13355999,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,19/Jul/22 07:45,17/Apr/24 02:39,04/Jun/24 20:42,21/Jul/22 13:11,1.16.0,,,,1.16.0,,,,,,,,,,,0,pull-request-available,,,,,,Please see issue in gist: https://gist.github.com/JackWangCS/0b1ec2c1137c686ab874124569063234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 13:11:32 UTC 2022,,,,,,,,,,"0|z16yxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 13:11;gaborgsomogyi;0c5108c in master.;;;",,,,,,,,,,,,,,,,,,,,
Table-API print connector encoding issue,FLINK-28607,13472204,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,sarahhhc,sarahhhc,sarahhhc,19/Jul/22 07:27,19/Jul/22 09:32,04/Jun/24 20:42,,,,,,,,,,Connectors / Common,Table SQL / Ecosystem,,,,,,0,,,,,,,"I found out Table API's print connector has an encoding issue.

 

[https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/common/functions/util/PrintSinkOutputWriter.java#L54]

You can set PrintStream with encoding like this.

```java

PrintStream stream = new PrintStream(System.out, true, StandardCharsets.UTF_8);

```

 

Can I make a PR for this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 09:26:31 UTC 2022,,,,,,,,,,"0|z16ywg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 07:49;martijnvisser;[~sarahhhc] Do you want to make on a PR to fix this?;;;","19/Jul/22 09:26;sarahhhc;[~martijnvisser] Yes, I'll mention you after work!;;;",,,,,,,,,,,,,,,,,,,
Preserve distributed consistency of OperatorEvents from OperatorCoordinator to subtasks,FLINK-28606,13472202,13427369,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,yunfengzhou,yunfengzhou,19/Jul/22 07:20,02/Sep/22 08:59,04/Jun/24 20:42,09/Aug/22 12:33,1.14.3,,,,1.16.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,,,"This is a component of our solution to the consistency issue in the operator coordinator mechanism. In this step, we would guarantee the consistency of all communications in one direction, from OC to subtasks. This would need less workload and should unblock the implementation of the CEP coordinator in FLIP-200.

Roughly, we would need to implement the following process in this step.
 # Let the OC finish processing all the incoming OperatorEvents before the snapshot.
 # Closes the gateway that sends operator events to its subtasks when the OC completes the snapshot.
 # Wait until all the outgoing OperatorEvents created before the snapshot are sent and acked.
 # Send checkpoint barriers to the Source operators.
 # Open the corresponding gateway of a subtask when the OC learned that the subtask has completed the checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28900,FLINK-28941,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-19 07:20:50.0,,,,,,,,,,"0|z16yw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception intentionally when new snapshots are committed during restore,FLINK-28605,13472200,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,19/Jul/22 07:11,19/Jul/22 08:09,04/Jun/24 20:42,19/Jul/22 08:09,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Currently snapshots are committed in {{notifyCheckpointComplete}}. If the job fails between a successful checkpoint and the call of {{notifyCheckpointComplete}}, these snapshots will be committed after job restarts.

However when the writer starts they also need to read from the latest snapshot (to build the latest structure of LSM tree). These two steps may happen concurrently and what the writers see may not be the latest snapshot.

To fix this problem, we can throw exception intentionally after new snapshots are committed during restore. In this way the job will be forcefully restarted and it is very likely that the writers can see the latest snapshot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 08:09:03 UTC 2022,,,,,,,,,,"0|z16yvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 08:09;lzljs3620320;master: d92a7f37dc072a6d3178b28c2d6040667b9f96b1;;;",,,,,,,,,,,,,,,,,,,,
job failover and not restore from checkpoint in zookeeper HA mode,FLINK-28604,13472196,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zouyunhe,zouyunhe,19/Jul/22 06:32,25/Jul/22 02:24,04/Jun/24 20:42,19/Jul/22 10:20,1.14.2,,,,,,,,Runtime / Checkpointing,,,,,,,0,,,,,,,"Run a job with flink 1.14.2 by configure the zookeeper ha 
{code:java}
high-availability.storageDir: hdfs://testcluster/app/ha
high-availability: zookeeper
high-availability.zookeeper.quorum: *****
high-availability.zookeeper.path.root: /flink{code}
when the zookeeper node restart, I see the JM failover with log ""Close and clean up all data for  ZookeeperHaServices"",  So the ha data was cleaned when the first JM shutdown. 

when the second JM was started,  the log was ""No checkpoint found during restore"", and no checkpoint to restored  .

From debug, I find when job failover, it would goto the `ClusterEntryPoint.java` line 285

!image-2022-07-19-14-30-27-198.png!

and will set the `cleanupHaData` as true.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/22 06:30;zouyunhe;image-2022-07-19-14-30-27-198.png;https://issues.apache.org/jira/secure/attachment/13046939/image-2022-07-19-14-30-27-198.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 02:24:30 UTC 2022,,,,,,,,,,"0|z16yuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 07:49;martijnvisser;[~zouyunhe] Can you verify this please with the latest Flink 1.14 release to make sure this is not already fixed? ;;;","19/Jul/22 10:19;zouyunhe;[~martijnvisser] this problem not exist in flink 1.14.5, it should be fixed. I will close this issue.;;;","25/Jul/22 02:24;yunta;BTW, [~zouyunhe] could this problem be easily reproduced in flink-1.14.2, and it will disappear once we bump to flink-1.14.5?;;;",,,,,,,,,,,,,,,,,,
Flink runtime.rpc.RpcServiceUtils code style,FLINK-28603,13472191,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,wuqingzhi,wuqingzhi,19/Jul/22 05:52,19/Jul/22 07:37,04/Jun/24 20:42,19/Jul/22 07:37,,,,,,,,,Runtime / RPC,,,,,,,0,pull-request-available,,,,,,"Hello, I found a code style problem in , which is located in

RpcServiceUtils, where nextNameOffset should be capitalized and separated by an underscore.

eg: 

private static final AtomicLong nextNameOffset = new AtomicLong(0L);",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 07:37:11 UTC 2022,,,,,,,,,,"0|z16ytk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 07:37;chesnay;If it's not detected as a codestyle violation by our checks, then it's not a codestyle violation.;;;",,,,,,,,,,,,,,,,,,,,
StateChangeFsUploader cannot close stream normally while enabling compression,FLINK-28602,13472188,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,masteryhx,masteryhx,masteryhx,19/Jul/22 05:32,29/Jul/22 07:05,04/Jun/24 20:42,29/Jul/22 07:05,1.15.1,1.16.0,,,1.15.2,1.16.0,,,Runtime / State Backends,,,,,,,0,pull-request-available,,,,,,"While enabling compression, Changelog part will wrap output stream using   

StreamCompressionDecorator#decorateWithCompression.

As the comment said, ""IMPORTANT: For streams returned by this method, \{@link OutputStream#close()} is not propagated to the inner stream. The inner stream must be closed separately."".

But StateChangeFsUploader will not close inner stream if wrapped stream has been closed.

So the upload may not complete when enabling compression even if it returns success.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28581,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 07:05:01 UTC 2022,,,,,,,,,,"0|z16ysw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 07:05;roman;Merged into master as e77d3a226bab6d06272976a742c225811dc3ca36,
into 1.15 as d5dc354dce663230b38973b285ff31b943da8fff.;;;",,,,,,,,,,,,,,,,,,,,
Add Transformer for FeatureHasher,FLINK-28601,13472180,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,19/Jul/22 04:05,24/Aug/22 03:16,04/Jun/24 20:42,24/Aug/22 03:16,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,Support FeatureHasher in FlinkML.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-19 04:05:53.0,,,,,,,,,,"0|z16yr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support FilterPushDown in flink-connector-jdbc,FLINK-28600,13472178,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,wanghailin,wanghailin,19/Jul/22 03:54,29/Nov/22 14:57,04/Jun/24 20:42,20/Jul/22 08:49,,,,,,,,,Connectors / JDBC,,,,,,,0,pull-request-available,,,,,,Support FilterPushDown in flink-connector-jdbc,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 20 08:49:44 UTC 2022,,,,,,,,,,"0|z16yqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 08:49;martijnvisser;[~wanghailin] This is a duplicate of FLINK-16024 - I have to close this ticket and your PR too. Please verify together with the contributor on the original ticket if you can help out with that PR. ;;;",,,,,,,,,,,,,,,,,,,,
Adding FlinkJoinToMultiJoinRule to support translating  left/right outer join to multi join,FLINK-28599,13472171,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,19/Jul/22 02:43,31/Aug/22 12:31,04/Jun/24 20:42,08/Aug/22 10:14,1.16.0,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"Now, Flink use Calcite's rule 
{code:java}
JOIN_TO_MULTI_JOIN{code}
 to convert multiple joins into a join set, which can be used by join reorder. However, calcite's rule can not adapte to all outer joins. For left or right outer join, if they meet certain conditions, it can also be converted to multi join. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 31 12:31:54 UTC 2022,,,,,,,,,,"0|z16yp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 02:44;337361684@qq.com;Hi, [~godfreyhe] , could you assign this to me ! Thanks !;;;","19/Jul/22 07:51;martijnvisser;[~337361684@qq.com] Do you know if this is addressed in a later version of Calcite already? If not, it probably makes sense to also raise this issue with the Calcite community?;;;","08/Aug/22 10:14;godfreyhe;Fixed in master: 6886e444c2be97984aa95606120080b8db532feb;;;","08/Aug/22 10:15;godfreyhe;[~martijnvisser], I think we can port the improvements to Calcite in the future;;;","31/Aug/22 12:31;martijnvisser;[~godfreyhe] Does it make sense to already create a Calcite ticket and link it to this one? ;;;",,,,,,,,,,,,,,,,
ClusterEntryPoint can't get the real exit reason when shutting down,FLINK-28598,13472170,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zlzhang0122,zlzhang0122,19/Jul/22 02:42,03/Nov/22 07:52,04/Jun/24 20:42,,1.14.2,1.15.1,,,,,,,Deployment / YARN,Runtime / Task,,,,,,0,,,,,,,"When the cluster is starting and some error occurs, the ClusterEntryPoint will shutDown the cluster asynchronous, but if it can't get a Throwable, the shutDown reason will be null, but actually if it's a user code problem and this may happen. 

I think we can get the real exit reason caused by user code and pass it to the diagnostics parameter, this may help users a lot.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 03 07:52:47 UTC 2022,,,,,,,,,,"0|z16yow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/22 07:52;gaoyunhaii;[~zlzhang0122] May I have a double confirmation of the issue that which deployment mode you are using? Since I'm not quite sure which part the user code would be executed during starting. ;;;",,,,,,,,,,,,,,,,,,,,
Empty checkpoint folders not deleted on job cancellation if their shared state is still in use,FLINK-28597,13472158,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,roman,roman,18/Jul/22 23:41,03/Aug/22 11:00,04/Jun/24 20:42,03/Aug/22 11:00,1.16.0,,,,1.16.0,,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,,,,,,"After FLINK-25872, SharedStateRegistry registers all state handles, including private ones.
Once the state isn't use AND the checkpoint is subsumed, it will actually be discarded.

This is done to prevent premature deletion when recovering in CLAIM mode:
1. RocksDB native savepoint folder (shared state is stored in chk-xx folder so it might fail the deletion)
2. Initial non-changelog checkpoint when switching to changelog-based checkpoints (private state of the initial checkpoint might be included into later checkpoints and its deletion would invalidate them)

Additionally, checkpoint folders are not deleted for a longer time which might be confusing.
In case of a crash, more folders will remain.

cc: [~Yanfei Lei], [~ym]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25872,,,,FLINK-28581,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 11:00:15 UTC 2022,,,,,,,,,,"0|z16ym8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Aug/22 11:00;roman;Merged as d8a4304b892412eab4c5c19b5deb84166943d3bb.;;;",,,,,,,,,,,,,,,,,,,,
Support writing arrays to postgres array columns in Flink SQL JDBC connector,FLINK-28596,13472147,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bobbyrlg,bobbyrlg,18/Jul/22 21:21,30/Jan/23 01:16,04/Jun/24 20:42,,1.15.0,,,,,,,,Connectors / JDBC,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 30 01:16:18 UTC 2023,,,,,,,,,,"0|z16yjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 21:24;bobbyrlg;Looking at the code it looks like the PostgresRowConverter supports reading arrays from postgres, but not writing them. Looks simple enough to add by calling setArray on the preparedstatement from the PostgresRowConverter.  Any thoughts [~martijnvisser] ?;;;","01/Aug/22 13:16;martijnvisser;I'm not too familiar with this code but if you can open a PR, we most certainly can find someone who can review it :);;;","02/Aug/22 19:58;bobbyrlg;I have this working in my fork. Will send a PR soon :);;;","26/Jan/23 16:46;martijnvisser;[~bobbyrlg] If you have a PR, looking forward to it :);;;","30/Jan/23 01:16;bobbyrlg;[~martijnvisser] Sorry I completely forgot about this, working on some tests now. Will send something this week!;;;",,,,,,,,,,,,,,,,
KafkaSource should not read metadata of unmatched regex topics,FLINK-28595,13472124,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,JasonLee,amazhar1,amazhar1,18/Jul/22 17:50,11/Oct/23 18:54,04/Jun/24 20:42,,,,,,,,,,Connectors / Kafka,,,,,,,0,pull-request-available,stale-assigned,,,,,"When we use a regex to subscribe to topics, the current connector gets a list of all topics, then runs describe against all of them, and finally filters by the regex pattern. This is not performant, as well as could possibly trigger audit alarms against sensitive topics that do not match the regex.

Proposed fix: move the regex filtering from the TopicPatternSubscriber's set() down into KafkaSubscriberUtils getAllTopicMetadata(). Get the list of topics, filter by pattern (if any), then get metadata. Create appropriate tests.",,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Wed Aug 16 22:35:17 UTC 2023,,,,,,,,,,"0|z16yf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 08:07;JasonLee;I think Afzal Mazhar is right that we should put the filter forward and I'm happy to fix it

Can someone assign this to me,thanks;;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
Add metrics for FlinkService,FLINK-28594,13472069,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,morhidi,,18/Jul/22 12:21,31/Aug/23 15:45,04/Jun/24 20:42,,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,We would need some metrics for the `FlinkService` to be able to tell how long does it take to perform most of the blocking operations we have in this service,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 12:18:44 UTC 2022,,,,,,,,,,"0|z16y2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jul/22 06:31;nicholasjiang;[~matyas] , are you working for this?;;;","28/Jul/22 06:34;morhidi;no, feel free to grab it.;;;","04/Aug/22 12:12;nicholasjiang;[~matyas] , what are the most of the blocking operations? I'm adding the Histogram metrics for the operations like triggerSavepoint etc, except the querying operations like listJobs.;;;","04/Aug/22 12:18;morhidi;I guess we need all of them, especially the listJobs. This method is called the most out of all.;;;",,,,,,,,,,,,,,,,,
Introduce default CR templates at operator level,FLINK-28593,13472067,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,morhidi,,18/Jul/22 12:11,31/Aug/23 15:44,04/Jun/24 20:42,,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"Ingress templates are currently [defined at CR level|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/ingress/] for example, but these rules can be enabled globally at operator level too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 12:27:22 UTC 2022,,,,,,,,,,"0|z16y2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 12:15;gyfora;I think instead of making this specifc to ingresses I would like to allow FlinkDeployment / FlinkSessionJob spec defaults on the operator level. It's not completely clear to me yet how would it be best to introduce these so we should discuss on the mailing list.

Some approaches:
 - Global spec defaults
 - Per namespace spec defaults
 - Dynamic logic with some pluggable mechanism;;;","18/Jul/22 12:27;morhidi;Agree this could be generalised the way you described, updated the Jiradescription;;;",,,,,,,,,,,,,,,,,,,
Custom resource counter metrics improvements,FLINK-28592,13472066,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,morhidi,,18/Jul/22 11:58,24/Nov/22 01:03,04/Jun/24 20:42,30/Jul/22 06:42,,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"* add counters at global level
 * add option to turn on/off the metrics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jul 30 06:42:50 UTC 2022,,,,,,,,,,"0|z16y28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 14:40;morhidi;started working on this;;;","30/Jul/22 06:42;gyfora;merged to main 4d3b08f0a890c84e2a5131e08efd8925bc41ddad;;;",,,,,,,,,,,,,,,,,,,
Array<Row<...>> is not serialized correctly when BigInt is present,FLINK-28591,13472062,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,swtwsk,swtwsk,18/Jul/22 11:39,18/Sep/23 16:51,04/Jun/24 20:42,,1.15.0,,,,,,,,Table SQL / API,Table SQL / Planner,,,,,,1,,,,,,,"When using Table API to insert data into array of rows, the data apparently is incorrectly serialized internally, which leads to incorrect serialization at the connectors. It happens when one of the table fields is a BIGINT (and does not happen, when it is INT).

E.g., a following table:
{code:java}
CREATE TABLE wrongArray (
    foo bigint,
    bar ARRAY<ROW<`foo1` STRING, `foo2` STRING>>
) WITH (
  'connector' = 'filesystem',
  'path' = 'file://path/to/somewhere',
  'format' = 'json'
) {code}
along with the following insert:
{code:java}
insert into wrongArray (
    SELECT
        1,
        array[
            ('Field1', 'Value1'),
            ('Field2', 'Value2')
        ]
    FROM (VALUES(1))
) {code}
gets serialized into: 
{code:java}
{
  ""foo"":1,
  ""bar"":[
    {
      ""foo1"":""Field2"",
      ""foo2"":""Value2""
    },
    {
      ""foo1"":""Field2"",
      ""foo2"":""Value2""
    }
  ]
}{code}
It is easy to spot that `bar` (an Array of Rows with two Strings) consists of duplicates of the last row in the array.

On the other hand, when `foo` is of type `int` instead of `bigint`:
{code:java}
CREATE TABLE wrongArray (
    foo int,
    bar ARRAY<ROW<`foo1` STRING, `foo2` STRING>>
) WITH (
  'connector' = 'filesystem',
  'path' = 'file://path/to/somewhere',
  'format' = 'json'
) {code}
the previous insert yields correct value: 
{code:java}
{
  ""foo"":1,
  ""bar"":[
    {
      ""foo1"":""Field1"",
      ""foo2"":""Value1""
    },
    {
      ""foo1"":""Field2"",
      ""foo2"":""Value2""
    }
  ]
}{code}
Bug reproduced in the Flink project: [https://github.com/swtwsk/flink-array-row-bug]
----
It is not an error connected with either a specific connector or format. I have done a bit of debugging when trying to implement my own format and it seems that `BinaryArrayData` holding the row values has wrong data saved in its `MemorySegment`, i.e. calling: 
{code:java}
for (var i = 0; i < array.size(); i++) {
  Object element = arrayDataElementGetter.getElementOrNull(array, i);
}{code}
correctly calculates offsets but yields the same result as the data is malformed in the array's `MemorySegment`. Such a call can be, e.g., found in `flink-json` — to be more specific in {color:#e8912d}org.apache.flink.formats.json.RowDataToJsonConverters::createArrayConverter {color}(line 241 in 1.15.0 version)",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33110,,,,,,,,,,,,,,,,,,,"19/Jul/22 11:51;KristoffSC;image-2022-07-19-13-51-45-254.png;https://issues.apache.org/jira/secure/attachment/13046971/image-2022-07-19-13-51-45-254.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 11:34:45 UTC 2022,,,,,,,,,,"0|z16y1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 09:26;swtwsk;I have created a simple reproducer of this bug to look into: [https://github.com/swtwsk/flink-array-row-bug];;;","19/Jul/22 11:07;swtwsk;Update: it turns out that the bug is caused by having a BigInt field in the table. When I have changed type of `foo` from `bigint` to `int`, the array of rows gets serialized correctly.;;;","19/Jul/22 11:34;KristoffSC;The potential issue might be in _CopyingChainingOutput.class_ line 82 where we call 

input.processElement(copy);
The type of input is ""StreamExecCalc"" but i do not see processElement method on this type.. and when I try to go inside with IntelliJ debug, I actually dont see anything..

Anyways,
for case with bigint, 
{code:java}
input.processElement(copy);{code}
 leads us to {_}GenericArrayData{_}, where for case with int, we dont have this object created.

Unfortunately I dont know what is happening inside 
{code:java}
input.processElement(copy);{code}
Any hint about how to debug this place would help.
Currently I see this:
!image-2022-07-19-13-51-45-254.png!;;;",,,,,,,,,,,,,,,,,,
flink on yarn checkpoint exception,FLINK-28590,13472045,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,muWang,muWang,18/Jul/22 09:53,25/Aug/22 06:06,04/Jun/24 20:42,25/Aug/22 06:06,1.15.0,1.15.1,,,1.15.0,1.15.1,,,Deployment / YARN,Runtime / Checkpointing,,,,,,0,,,,,,,"When I submit using flink on yarn on cdh6.2.1, the jobmanager log prints the following exception：

!image-2022-07-18-17-55-54-408.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/22 09:55;muWang;image-2022-07-18-17-55-54-408.png;https://issues.apache.org/jira/secure/attachment/13046902/image-2022-07-18-17-55-54-408.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 25 06:06:32 UTC 2022,,,,,,,,,,"0|z16xxk:",9223372036854775807,Dependency conflict,,,,,,,,,,,,,,,,,,,"20/Jul/22 13:27;muWang;[~gaoyunhaii] [~jark] 

Can you help solve this problem, thanks!

hadoop version：3.0.0    flink version：1.15.1

 ;;;","25/Aug/22 06:06;muWang;Dependency conflict;;;",,,,,,,,,,,,,,,,,,,
Enhance Web UI for Speculative Execution,FLINK-28589,13472044,13472042,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,junhan,pltbkd,pltbkd,18/Jul/22 09:41,02/Aug/22 05:21,04/Jun/24 20:42,02/Aug/22 05:21,1.16.0,,,,1.16.0,,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 02 05:21:19 UTC 2022,,,,,,,,,,"0|z16xxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Aug/22 05:21;zhuzh;Done via 
4af75011fe32506a70076a2fe9e847cef39587bb
57990c332f3d87e4bcc1824973ac5ed2bcafec85
f6c5dc1b32ad6e6b524e549eff2b7d9d2b7d9970;;;",,,,,,,,,,,,,,,,,,,,
Enhance REST API for Speculative Execution,FLINK-28588,13472043,13472042,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pltbkd,pltbkd,pltbkd,18/Jul/22 09:41,31/Aug/22 08:25,04/Jun/24 20:42,01/Aug/22 10:32,1.16.0,,,,1.16.0,,,,Runtime / REST,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29132,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 10:32:14 UTC 2022,,,,,,,,,,"0|z16xx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 10:32;gaoyunhaii;Merged on master via aae0462cb398f5c58c46a769b25b06a7fdd15d1d^..f436b20429b55ada2a9e8936a5e80fc672a397de;;;",,,,,,,,,,,,,,,,,,,,
FLIP-249: Flink Web UI Enhancement for Speculative Execution,FLINK-28587,13472042,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,pltbkd,pltbkd,pltbkd,18/Jul/22 09:39,16/Aug/22 03:56,04/Jun/24 20:42,16/Aug/22 03:56,1.16.0,,,,1.16.0,,,,Runtime / REST,Runtime / Web Frontend,,,,,,0,,,,,,,"As a follow-up step of FLIP-168 and FLIP-224, the Flink Web UI needs to be enhanced to display the related information if the speculative execution mechanism is enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28131,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-18 09:39:34.0,,,,,,,,,,"0|z16xww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative execution for new sources,FLINK-28586,13472018,13470079,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,18/Jul/22 08:19,22/Jul/22 12:08,04/Jun/24 20:42,22/Jul/22 12:08,,,,,1.16.0,,,,Connectors / Common,Runtime / Coordination,,,,,,0,pull-request-available,,,,,,This task enables new sources(FLIP-27) for speculative execution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 12:08:54 UTC 2022,,,,,,,,,,"0|z16xrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 12:08;zhuzh;Done via
79d93f2512f6826baefb14c8dc9b59d419d7df0a
9af271f3108ce8af6b6972fabf5420b99e55fc71
bedcc3f7b5c0fc184953d3c1a969f03887db2cae
7129c2ee09ce7eb3959ce88383b5d8ea0987fcf5
863222e926df26fde4caa470c58b261174181719;;;",,,,,,,,,,,,,,,,,,,,
Speculative execution for InputFormat sources,FLINK-28585,13472017,13470079,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhuzh,zhuzh,zhuzh,18/Jul/22 08:18,25/Jul/22 13:09,04/Jun/24 20:42,25/Jul/22 13:09,,,,,1.16.0,,,,Runtime / Coordination,,,,,,,0,pull-request-available,,,,,,This task enables InputFormat sources for speculative execution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 13:09:10 UTC 2022,,,,,,,,,,"0|z16xrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jul/22 13:09;zhuzh;Done via 7611928d0f1a7bb20ec5b0538e0fbe9102a07023;;;",,,,,,,,,,,,,,,,,,,,
Add an option to ConfigMap that can be set to immutable,FLINK-28584,13472008,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,spoon-lz,spoon-lz,18/Jul/22 07:56,25/Jul/22 10:04,04/Jun/24 20:42,,,,,,,,,,Deployment / Kubernetes,,,,,,,0,,,,,,,"When a job is started in the kubernetes environment, multiple configmaps are usually created to mount data (eg: flink-config, hadoop-config, etc.). If a cluster runs too many jobss, the configmap will become a performance bottleneck, and occasionally An exception of file mount failure occurs, resulting in slower pod startup time


According to kubernetes' description of configmap, if the immutable parameter is enabled, it will greatly reduce the pressure on kube-apiserver and improve cluster performance.

[configmap-immutable|https://kubernetes.io/zh-cn/docs/concepts/configuration/configmap/#configmap-immutable]


In my understanding, parameter information such as flink-config, hadoop-config is loaded at startup, and even if it is subsequently modified, it cannot affect the running of the job. Should we provide a control switch to choose whether to set the configmap to immutable?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 25 10:04:12 UTC 2022,,,,,,,,,,"0|z16xpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 12:23;wangyang0918;Given that we are also restoring the log4j properties in the ConfigMap, I am not sure whether we could simply make the whole ConfigMap immutable. Please note that dynamically changing the log level is a useful feature when debugging some problems.;;;","25/Jul/22 07:15;spoon-lz;[~wangyang0918] Should a configuration switch be added to control this setting (the default is the current mode), because we found in actual use that when a cluster runs 2000+ jobs, the Pod often has a timeout error of the Mount configmap file.;;;","25/Jul/22 10:04;wangyang0918;Just like the doc said, immutable ConfigMap could reduce the APIServer load by closing watches. Since the flink configuration ConfigMap does not have any watches, I am not pretty sure whether making it immutable is really helpful.;;;",,,,,,,,,,,,,,,,,,
make flink dist log4j dependency simple and clear,FLINK-28583,13472005,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,18/Jul/22 07:48,11/Mar/24 12:44,04/Jun/24 20:42,,1.16.0,,,,1.20.0,,,,Deployment / Scripts,,,,,,,0,pull-request-available,,,,,,"flink don't shade the log4j and want to put it flink/lib using shade exclusion like this 
{code:java}
<excludes>
   <!-- log4j 2 is bundled separately from the flink-dist jar -->
   <exclude>org.apache.logging.log4j:*</exclude> {code}
and add this to put them to flink/lib
{code:java}
<includes>
   <include>org.apache.logging.log4j:log4j-api</include>
   <include>org.apache.logging.log4j:log4j-core</include>
   <include>org.apache.logging.log4j:log4j-slf4j-impl</include>
   <include>org.apache.logging.log4j:log4j-1.2-api</include>
</includes> {code}
i suggest to make the log4j to provided. it will make it clear and simple

 

the assemble dependencySet scope just like filter ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 07:49:25 UTC 2022,,,,,,,,,,"0|z16xoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 07:49;jackylau;hi [~chesnay] , what do you think ?;;;",,,,,,,,,,,,,,,,,,,,
LSM tree structure may be incorrect when multiple jobs are committing into the same bucket,FLINK-28582,13471993,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,TsReaper,TsReaper,18/Jul/22 07:20,19/Jul/22 06:48,04/Jun/24 20:42,19/Jul/22 06:48,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Currently `FileStoreCommitImpl` only checks for conflicts by checking the files we're going to delete (due to compaction) are still there.

However, consider two jobs committing into the same LSM tree at the same time. For their first compaction no conflict is detected because they'll only delete their own level 0 files. But they will both produce level 1 files and the key ranges of these files may overlap. This is not correct for our LSM tree structure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 06:48:37 UTC 2022,,,,,,,,,,"0|z16xm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 06:48;lzljs3620320;master: f34be4ba159341d52eced8f7d0b0a5afc97ff91f;;;",,,,,,,,,,,,,,,,,,,,
Test Changelog StateBackend V2 Manually,FLINK-28581,13471963,13425108,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,masteryhx,masteryhx,masteryhx,18/Jul/22 03:51,23/Jul/23 10:07,04/Jun/24 20:42,31/Oct/22 02:52,1.16.0,,,,1.16.0,,,,Runtime / State Backends,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28614,,,,FLINK-28602,FLINK-28597,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-18 03:51:27.0,,,,,,,,,,"0|z16xfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Predicate supports unknown stats,FLINK-28580,13471960,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,18/Jul/22 03:39,19/Jul/22 06:58,04/Jun/24 20:42,19/Jul/22 06:58,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Now there will be a NPE if minValue or maxValue of FieldStats is null.
We can know the stats is unknown in LeafPredicate.test(long rowCount, FieldStats[] fieldStats), and return true directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 06:58:29 UTC 2022,,,,,,,,,,"0|z16xeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 06:58;lzljs3620320;master: d761f210c5e6dd9dc3e8cf1a7e46c1bf3a32ef66;;;",,,,,,,,,,,,,,,,,,,,
Supports predicate testing for new columns,FLINK-28579,13471959,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,18/Jul/22 03:38,19/Jul/22 04:16,04/Jun/24 20:42,19/Jul/22 04:16,,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"The currently added column, if there is a filter on it, will cause an error in the RowDataToObjectArrayConverter because the number of columns is not correct
We can make BinaryTableStats supports evolution from shorter rowData.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 04:16:49 UTC 2022,,,,,,,,,,"0|z16xeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 04:16;lzljs3620320;master: 4d36d8dcb5d20c9517e3532d8aa42aa828ac4c23;;;",,,,,,,,,,,,,,,,,,,,
Upgrade Spark version of flink-table-store-spark to 3.2.2,FLINK-28578,13471947,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,18/Jul/22 01:44,18/Jul/22 02:25,04/Jun/24 20:42,18/Jul/22 02:25,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"CVE-2022-33891: Apache Spark shell command injection vulnerability via Spark UI. Upgrade to supported Apache Spark maintenance release 3.1.3, 3.2.2 or 3.3.0 or later.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 02:25:59 UTC 2022,,,,,,,,,,"0|z16xbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 01:50;nicholasjiang;[~lzljs3620320], please help to assign this ticket to me.;;;","18/Jul/22 02:25;lzljs3620320;master: 2b8a6aa75b3be48404070088460ba00ae2927835;;;",,,,,,,,,,,,,,,,,,,
1.15.1 web ui console report error about checkpoint size,FLINK-28577,13471908,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,nobleyd,nobleyd,17/Jul/22 12:01,27/Jul/22 11:49,04/Jun/24 20:42,27/Jul/22 11:46,1.15.1,,,,1.15.2,1.16.0,,,Runtime / Web Frontend,,,,,,,0,pull-request-available,,,,,,"1.15.1

1 start-cluster

2 submit job: ./bin/flink run -d ./examples/streaming/TopSpeedWindowing.jar

3 trigger savepoint: ./bin/flink savepoint {{{jobId} ./sp0}}

{{4 open web ui for job and change to checkpoint tab, nothing showed.}}

{{Chrome console log shows some error:}}

{{main.a7e97c2f60a2616e.js:1 ERROR TypeError: Cannot read properties of null (reading 'checkpointed_size')
    at q (253.e9e8f2b56b4981f5.js:1:607974)
    at Sl (main.a7e97c2f60a2616e.js:1:186068)
    at Br (main.a7e97c2f60a2616e.js:1:184696)
    at N8 (main.a7e97c2f60a2616e.js:1:185128)
    at Br (main.a7e97c2f60a2616e.js:1:185153)
    at N8 (main.a7e97c2f60a2616e.js:1:185128)
    at Br (main.a7e97c2f60a2616e.js:1:185153)
    at N8 (main.a7e97c2f60a2616e.js:1:185128)
    at Br (main.a7e97c2f60a2616e.js:1:185153)
    at B8 (main.a7e97c2f60a2616e.js:1:191872)}}

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25557,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 11:46:34 UTC 2022,,,,,,,,,,"0|z16x34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jul/22 03:14;Yu Chen;Well, I think I've found the root cause of this issue: it is mainly introduced by the mistake modification of the flink-runtime-web by [JIRA-25557|https://issues.apache.org/jira/browse/FLINK-25557].

And I can propose a PR to resolve the problem.

cc: [~yunta] ;;;","27/Jul/22 03:28;yunta;[~Yu Chen] Thanks for figuring out this problem, and it seems we cannot have tests to cover the web UI changes:(, and maybe typos could lead such problems.

 

Already assigned this ticket to you, please go ahead.;;;","27/Jul/22 11:46;yunta;merged in master: be4e0fe050b6f71e8522311e598e4d9997134d49

release-1.15: 64038da8b2e974db13052a7b5dbab4cc8fc4c43c;;;",,,,,,,,,,,,,,,,,,
Kafka's two source api performance differences,FLINK-28576,13471894,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,Echo Lee,Echo Lee,17/Jul/22 03:54,18/Jul/22 07:05,04/Jun/24 20:42,18/Jul/22 07:05,,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,,,"I recently found out that the new kafka source api is 10 times more performant than the old one, but don't know what's causing it.

 
{code:java}
// new source api
KafkaSource<String> source = KafkaSource.<String>builder()
                .setBootstrapServers(brokers)
                .setTopics(""benchmark"")
                .setGroupId(""my-group"")
                .setStartingOffsets(OffsetsInitializer.earliest())
                .setBounded(OffsetsInitializer.latest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build(); 

env.fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source"")
        .map(new MapFunction<String, Object>() {
            private int count = 0;
            private long lastTime = System.currentTimeMillis();
                     
                  @Override
            public Object map(String value) throws Exception {
                count++;
                if (count % 100000 == 0) {
                    long currentTime = System.currentTimeMillis();
                    System.out.println(currentTime - lastTime);                                   lastTime = currentTime;              
                      }
                return null;
            }
        });{code}
 

 

 
{code:java}
// old source api
FlinkKafkaConsumer<String> flinkKafkaConsumer = new FlinkKafkaConsumer<String>(""benchmark"",
                new SimpleStringSchema(), properties);
flinkKafkaConsumer.setStartFromEarliest(); 
env.addSource(flinkKafkaConsumer)
        .map(new MapFunction<String, Object>() {
            private int count = 0;            
                  private long lastTime = System.currentTimeMillis();
          
                  @Override
            public Object map(String value) throws Exception {
                count++;
                if (count % 100000 == 0) {
                    long currentTime = System.currentTimeMillis();
                    System.out.println(currentTime - lastTime);                                    lastTime = currentTime;              
                       }
                return null;
            }
        });{code}
 

Two ways to use the same data of the same topic.

Flink version: 1.14.x

Single data size: 1k

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 07:05:04 UTC 2022,,,,,,,,,,"0|z16x00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jul/22 04:00;Echo Lee;CC [~martijnvisser] Can you help explain this difference? thanks.;;;","18/Jul/22 06:26;martijnvisser;[~Echo Lee] Do you have metrics, logs or anything else? I haven't heard of this performance drop by anyone else. ;;;","18/Jul/22 06:35;Echo Lee;[~martijnvisser] Thanks for your reply, I just print the time spent every time I pull 100,000 data, the old api takes an average of 60 seconds, the new one takes only about 4 seconds.

I think my use case is simple, don't know where is the problem.;;;","18/Jul/22 06:55;martijnvisser;Are they two different jobs? Have you migrated from old to new and if so, did you follow the upgrade steps? ;;;","18/Jul/22 07:05;martijnvisser;Let's first continue the discussion in https://apache-flink.slack.com/archives/C03G7LJTS2G/p1658104071159259 (since this is crossposted). Closed the Jira since we don't know yet that this is indeed a bug. ;;;",,,,,,,,,,,,,,,,
support hbase connector 2.2 to write to hbase 2.4.9,FLINK-28575,13471866,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhenw,zhenw,zhenw,16/Jul/22 08:32,17/Aug/22 22:38,04/Jun/24 20:42,,,,,,,,,,Connectors / HBase,,,,,,,0,pull-request-available,stale-assigned,,,,,"hbase does version checks when creating configurations by default

```
  private static void checkDefaultsVersion(Configuration conf) {
    if (conf.getBoolean(""hbase.defaults.for.version.skip"", Boolean.FALSE)) return;
    String defaultsVersion = conf.get(""hbase.defaults.for.version"");
    String thisVersion = VersionInfo.getVersion();
    if (!thisVersion.equals(defaultsVersion)) {
      throw new RuntimeException(
        ""hbase-default.xml file seems to be for an older version of HBase ("" +
        defaultsVersion + ""), this version is "" + thisVersion);
    }
  }
```

so this prevents using connector2.2 to write to like hbase 2.4.9. however this restriction is not necessary. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 17 22:38:06 UTC 2022,,,,,,,,,,"0|z16wts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 06:31;martijnvisser;While it might not be necessary to have such restrictions, I am in favour of them (as I've included in the PR). There are no tests for these type of version supports, they are not or incorrectly documented and can cause unexpected CI results. If we want to have support for a specific version, we should add it. Same as we do with the Hive connector. ;;;","17/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,
Bump the fabric8 kubernetes-client to 6.0.0,FLINK-28574,13471862,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,ConradJam,ConradJam,16/Jul/22 06:19,20/Sep/22 08:50,04/Jun/24 20:42,09/Aug/22 12:47,kubernetes-operator-1.1.0,,,,,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"fabric8 kubernetes-client now is release to [6.0.0|https://github.com/fabric8io/kubernetes-client/releases/tag/v6.0.0] , Later we can upgrade this version and remove the deprecated API usage

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 12:47:57 UTC 2022,,,,,,,,,,"0|z16wsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jul/22 06:24;ConradJam;Hi [~gyfora] , What do you think ？Do you think we should upgrade now or put this work on hold until fabric8 Client has several more versions available from 6.x, such as 6.0.x or 6.1.x;;;","16/Jul/22 07:04;gyfora;Let's try to keep the fabric8 version in sync with the Java Operator SDK instead. There is no reason to upgrade independently.;;;","18/Jul/22 09:17;yunta;I think you could refer to FLINK-28481 first before considering bump fabric8 k8s-client version.;;;","24/Jul/22 07:16;ConradJam;[~gyfora] agree you mind, when Java Operator SDK update we can start this work;;;","09/Aug/22 12:01;nicholasjiang;[~ConradJam], the Java Operator SDK 3.1.1 has been released and bumped the fabric8 kubernetes-client to 5.12.3. I have already bumped the kubernetes-client. This ticket could be closed after above PR is merged.

cc [~gyfora] ;;;","09/Aug/22 12:47;gyfora;merged to main cd45721c53caaa7d16764c6faec68c985f8ebc39;;;",,,,,,,,,,,,,,,
Nested type will lose nullability when converting from TableSchema,FLINK-28573,13471802,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,15/Jul/22 14:59,18/Jul/22 01:52,04/Jun/24 20:42,18/Jul/22 01:52,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"E.g. ArrayDataType, MultisetDataType etc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 18 01:52:34 UTC 2022,,,,,,,,,,"0|z16wfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 01:52;lzljs3620320;master: b1b9827b08d1182b30cdd34464d154560e7e2c62;;;",,,,,,,,,,,,,,,,,,,,
"FlinkSQL executes Table.execute multiple times on the same Table, and changing the Table.execute code position will produce different phenomena",FLINK-28572,13471787,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,muggleC,muggleC,15/Jul/22 13:00,01/Aug/22 13:18,04/Jun/24 20:42,,1.13.6,,,,,,,,Table SQL / API,Table SQL / Planner,,,,,,0,,,,,,,"*The following code prints and inserts fine*

 
{code:java}
public static void main(String[] args) {
Configuration conf = new Configuration();
conf.setInteger(""rest.port"", 2000);
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
env.setParallelism(1);
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
tEnv.executeSql(""create table sensor("" +
"" id string, "" +
"" ts bigint, "" +
"" vc int"" +
"")with("" +
"" 'connector' = 'kafka', "" +
"" 'topic' = 's1', "" +
"" 'properties.bootstrap.servers' = 'hadoop162:9092', "" +
"" 'properties.group.id' = 'atguigu', "" +
"" 'scan.startup.mode' = 'latest-offset', "" +
"" 'format' = 'csv'"" +
"")"");
Table result = tEnv.sqlQuery(""select * from sensor"");
tEnv.executeSql(""create table s_out("" +
"" id string, "" +
"" ts bigint, "" +
"" vc int"" +
"")with("" +
"" 'connector' = 'kafka', "" +
"" 'topic' = 's2', "" +
"" 'properties.bootstrap.servers' = 'hadoop162:9092', "" +
"" 'format' = 'json', "" +
"" 'sink.partitioner' = 'round-robin' "" +
"")"");
result.executeInsert(""s_out"");
result.execute().print();
}
{code}
 

—

*When the code that prints this line is moved up, it can be printed normally, but the insert statement is invalid, as follows*

 
{code:java}
public static void main(String[] args) {
Configuration conf = new Configuration();
conf.setInteger(""rest.port"", 2000);
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
env.setParallelism(1);
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
tEnv.executeSql(""create table sensor("" +
"" id string, "" +
"" ts bigint, "" +
"" vc int"" +
"")with("" +
"" 'connector' = 'kafka', "" +
"" 'topic' = 's1', "" +
"" 'properties.bootstrap.servers' = 'hadoop162:9092', "" +
"" 'properties.group.id' = 'atguigu', "" +
"" 'scan.startup.mode' = 'latest-offset', "" +
"" 'format' = 'csv'"" +
"")"");
Table result = tEnv.sqlQuery(""select * from sensor"");
tEnv.executeSql(""create table s_out("" +
"" id string, "" +
"" ts bigint, "" +
"" vc int"" +
"")with("" +
"" 'connector' = 'kafka', "" +
"" 'topic' = 's2', "" +
"" 'properties.bootstrap.servers' = 'hadoop162:9092', "" +
"" 'format' = 'json', "" +
"" 'sink.partitioner' = 'round-robin' "" +
"")"");
result.execute().print();
result.executeInsert(""s_out"");
}
{code}
 ","flink-table-planner-blink_2.11  

1.13.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 01 13:18:52 UTC 2022,,,,,,,,,,"0|z16wc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Aug/22 13:18;martijnvisser;[~muggleC] Can you verify this with Flink 1.15? ;;;",,,,,,,,,,,,,,,,,,,,
Add AlgoOperator for Chi-squared test,FLINK-28571,13471775,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,taosiyuan163@163.com,taosiyuan163@163.com,taosiyuan163@163.com,15/Jul/22 12:03,10/Oct/22 02:05,04/Jun/24 20:42,04/Aug/22 01:12,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,"Pearson's chi-squared test:https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test

For more information on chi-squared:http://en.wikipedia.org/wiki/Chi-squared_test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 01:12:54 UTC 2022,,,,,,,,,,"0|z16w9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Aug/22 01:12;zhangzp;fixed via df58dbee68131c967ce927825cd710280cfda1ce;;;",,,,,,,,,,,,,,,,,,,,
Introduces a StreamNonDeterministicPlanResolver to validate and try to solve (lookup join only) the non-deterministic updates problem which may cause wrong result or error,FLINK-28570,13471747,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,15/Jul/22 09:46,18/Sep/22 13:19,04/Jun/24 20:42,06/Aug/22 13:02,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18871,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Aug 06 13:02:53 UTC 2022,,,,,,,,,,"0|z16w3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Aug/22 13:02;godfreyhe;Fixed in master:
 fd6c1cc96ce91bdeb0957b5dc26a663d0ba5b1d5: Introduces a StreamNonDeterministicPlanResolver to validate and try to solve (lookup join only) the non-deterministic updates problem which may cause wrong result or error

fa3aa8f3759149af1b3e82a82945fa3eb0d667e0:Optimize upsertKey inference for lookup join and fix ConnectorCatalogTable lost pk on stream source 

a4189170dce89334796170c952b4928740400712:  Extracts common attributes into StreamPhysicalGroupAggregateBase for all sub-classes 

512e4e0f2defa8a1cb2d731462ff65f6fcffaaab:Introduces a base class StreamPhysicalGroupWindowAggregateBase for all related sub-classes 

5daebae30b930db99955cd68b6538e6278df9874: Append upsertMaterialize info by a simpler way of overriding the explainTerms method for StreamPhysicalSink 

bca84305750275f6188539ccbed0796dc2c2d63b: Extends FlinkRelOptUtil to support print upsertKey info
;;;",,,,,,,,,,,,,,,,,,,,
SinkUpsertMaterializer should be aware of the input upsertKey if it is not empty otherwise wrong result maybe produced,FLINK-28569,13471746,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,15/Jul/22 09:45,11/Oct/22 13:17,04/Jun/24 20:42,15/Sep/22 10:50,1.14.5,1.15.2,,,1.16.0,1.17.0,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,"Currently SinkUpsertMaterializer only update row by comparing the complete row in anycase, but this may cause wrong result if input has upsertKey and also non-deterministic column values, see such a case:
{code:java}
@Test
public void testCdcWithNonDeterministicFuncSinkWithDifferentPk() {
tEnv.createTemporaryFunction(
""ndFunc"", new JavaUserDefinedScalarFunctions.NonDeterministicUdf());

String cdcDdl =
""CREATE TABLE users (\n""
+ "" user_id STRING,\n""
+ "" user_name STRING,\n""
+ "" email STRING,\n""
+ "" balance DECIMAL(18,2),\n""
+ "" primary key (user_id) not enforced\n""
+ "") WITH (\n""
+ "" 'connector' = 'values',\n""
+ "" 'changelog-mode' = 'I,UA,UB,D'\n""
+ "")"";

String sinkTableDdl =
""CREATE TABLE sink (\n""
+ "" user_id STRING,\n""
+ "" user_name STRING,\n""
+ "" email STRING,\n""
+ "" balance DECIMAL(18,2),\n""
+ "" PRIMARY KEY(email) NOT ENFORCED\n""
+ "") WITH(\n""
+ "" 'connector' = 'values',\n""
+ "" 'sink-insert-only' = 'false'\n""
+ "")"";
tEnv.executeSql(cdcDdl);
tEnv.executeSql(sinkTableDdl);

util.verifyJsonPlan(
""insert into sink select user_id, ndFunc(user_name), email, balance from users"");
}
{code}

for original cdc source records:
{code}
+I[user1, Tom, tom@gmail.com, 10.02],
-D[user1, Tom, tom@gmail.com, 10.02],
{code}

the above query cannot correctly delete the former insertion row because of the non-deterministic column value 'ndFunc(user_name)'

this canbe solved by letting the SinkUpsertMaterializer be aware of input upsertKey and update by it


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 11 13:16:47 UTC 2022,,,,,,,,,,"0|z16w34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/22 10:50;godfreyhe;Fixed in master: dbcd2d7b86fcb7fa7a26e181f1719ea4c6dad828
in 1.16.0: 390612320fa9be297d9eed1f4b75f8ba2ec83c40;;;","02/Oct/22 19:59;qinjunjerry;Can we backport this fix into 1.14 and 1.15? ;;;","08/Oct/22 12:57;lincoln.86xy;[~godfreyhe] I think it's valueable to port this fix to older versions(those users who encountered such case will get wrong result), this patch does not break the compatibility in some way (jobs can restore from previous state though new behavior changes), what do you think?;;;","11/Oct/22 10:45;Terry1897;+1 to backport this fix into 1.14 and 1.15;;;","11/Oct/22 11:26;Terry1897;But for the query which don't have upsert key and with non-deterministic values, the potential wrong result seems can not be avoid?
Should we disable such plan or give a warning during plan generation?;;;","11/Oct/22 13:16;lincoln.86xy;[~Terry1897] in 1.16, we introduced a [new mechanism|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/determinism/#33-how-to-eliminate-the-impact-of-non-deterministic-update-in-streaming] to do such validation, the case you mentioned which has no upsertKey but with changes will get an error message if set ’table.optimizer.non-deterministic-update.strategy’ to `TRY_RESOLVE`.;;;",,,,,,,,,,,,,,,
Implements a new lookup join operator (sync mode only) with state to eliminate the non determinism,FLINK-28568,13471745,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,15/Jul/22 09:44,25/Apr/24 09:33,04/Jun/24 20:42,09/Aug/22 03:22,,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24412,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 03:22:05 UTC 2022,,,,,,,,,,"0|z16w2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 03:22;godfreyhe;Fixed in master: 3f18cafa0581613ef9900da0478b3501617dc64f;;;",,,,,,,,,,,,,,,,,,,,
Introduce predicate inference from one side of join to the other,FLINK-28567,13471734,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,trushev,trushev,trushev,15/Jul/22 09:09,15/Aug/22 05:00,04/Jun/24 20:42,15/Aug/22 05:00,,,,,,,,,Table SQL / Planner,,,,,,,0,pull-request-available,stale-assigned,,,,,"h2. Context

There is JoinPushTransitivePredicatesRule in Calcite that infers predicates from on a Join and creates Filters if those predicates can be pushed to its inputs.
*Example.* (a0 = b0 AND a0 > 0) => (b0 > 0)

h2. Proposal

Add org.apache.calcite.rel.rules.JoinPushTransitivePredicatesRule to FlinkStreamRuleSets and to FlinkBatchRuleSets

h2. Benefit

Before the changes:

{code}
Flink SQL> explain select * from A join B on a0 = b0 and a0 > 0;

Join(joinType=[InnerJoin], where=[=(a0, b0)], select=[a0, b0], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[hash[a0]])
:  +- Calc(select=[a0], where=[>(a0, 0)])
:     +- TableSourceScan(table=[[default_catalog, default_database, A, filter=[]]], fields=[a0])
+- Exchange(distribution=[hash[b0]])
   +- TableSourceScan(table=[[default_catalog, default_database, B]], fields=[b0])
{code}

After the changes:

{code}
Join(joinType=[InnerJoin], where=[=(a0, b0)], select=[a0, b0], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[hash[a0]])
:  +- Calc(select=[a0], where=[>(a0, 0)])
:     +- TableSourceScan(table=[[default_catalog, default_database, A, filter=[]]], fields=[a0])
+- Exchange(distribution=[hash[b0]])
   +- Calc(select=[b0], where=[>(b0, 0)])
      +- TableSourceScan(table=[[default_catalog, default_database, B, filter=[]]], fields=[b0])
{code}

i.e., b0 > 0 is inferred and pushed down
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 04:31:14 UTC 2022,,,,,,,,,,"0|z16w0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","15/Aug/22 04:31;trushev;It looks like [~godfreyhe] have implemented the same improvement https://issues.apache.org/jira/browse/FLINK-28753;;;",,,,,,,,,,,,,,,,,,,
Adds materialization support to eliminate the non determinism generated by lookup join node,FLINK-28566,13471732,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,,lincoln.86xy,lincoln.86xy,15/Jul/22 08:56,29/Jul/22 13:29,04/Jun/24 20:42,29/Jul/22 06:49,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,"In order to minimize the potential exceptions or data errors when many users use the update stream to lookup join an external 
table (essentially due to the non-deterministic result based on processing-time to lookup external tables). 
When update exists in the input stream and the lookup key does not contain the primary key of the external table,
FLINK automatically adds materialization of the update by default, so that it will only lookup the external table 
when the insert or update_after message arrives, and when the delete or update_before message arrives, it will 
directly querying the latest version of the locally materialized data and sent it to downstream operator.

To do so，we introduce a new option 'table.exec.lookup-join.upsert-materialize' and resue the `UpsertMaterialize`. By default, the materialize operator will be added when an update stream lookup an external table without containing its primary keys(includes no primary key defined). You can also choose no materialization(NONE) or force materialization(FORCE) which will always enable materialization except input is insert only.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 06:49:55 UTC 2022,,,,,,,,,,"0|z16w00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 06:49;lincoln.86xy;the new FLINK-28570 will cover this part, and no need to expose a new  'table.exec.lookup-join.upsert-materialize' option;;;",,,,,,,,,,,,,,,,,,,,
Create NOTICE file for flink-table-store-hive-catalog,FLINK-28565,13471713,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nicholasjiang,lzljs3620320,lzljs3620320,15/Jul/22 07:32,21/Jul/22 03:26,04/Jun/24 20:42,21/Jul/22 03:26,,,,,table-store-0.2.0,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 03:26:28 UTC 2022,,,,,,,,,,"0|z16vw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:26;lzljs3620320;master: d169a125c97180bf1c6f4ac47a4b66aedef67541
release-0.2: c2edf29abd9c8c14d99001b5bb28233f41a8a7e4;;;",,,,,,,,,,,,,,,,,,,,
Update NOTICE/LICENCE files for 1.1.0 release,FLINK-28564,13471710,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,morhidi,,15/Jul/22 07:28,24/Nov/22 01:01,04/Jun/24 20:42,19/Jul/22 06:24,,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-15 07:28:05.0,,,,,,,,,,"0|z16vvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer for VectorSlicer,FLINK-28563,13471706,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hotsuns,hotsuns,hotsuns,15/Jul/22 07:03,29/Jul/22 10:02,04/Jun/24 20:42,29/Jul/22 10:02,,,,,ml-2.2.0,,,,Library / Machine Learning,,,,,,,0,pull-request-available,,,,,,h1. Add Transformer for VectorSlicer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 10:02:20 UTC 2022,,,,,,,,,,"0|z16vug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 10:02;zhangzp;fixed via e5da0da4ae0bf51dc467bc3635b80c9036953315;;;",,,,,,,,,,,,,,,,,,,,
Rocksdb state backend is too slow when using AggregateFunction,FLINK-28562,13471698,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,yue.shang,yue.shang,15/Jul/22 06:20,19/Jul/22 02:08,04/Jun/24 20:42,18/Jul/22 12:40,1.13.2,1.14.3,,,,,,,,,,,,,,0,,,,,,,"Rocksdb state backend is too slow when using AggregateFunction.

just only supports 300 traffic per second use Map<String,Object>.","{code:java}
final ParameterTool params = ParameterTool.fromArgs(args);

// set up the execution environment
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// get input data
DataStream<UserTag> source = env.addSource(new SourceFunction<UserTag>() {
    @Override
    public void run(SourceContext ctx) throws Exception {
        Random rd = new Random();
        while (true){
            UserTag userTag = new UserTag();
            userTag.setUserId(rd.nextLong());
            userTag.setMetricName(UUID.randomUUID().toString());
            userTag.setTagDimension(UUID.randomUUID().toString());
            userTag.setTagValue(rd.nextDouble());
            userTag.setTagTime( new Long(new Date().getTime()).intValue());
            userTag.setMonitorTime(new Long(new Date().getTime()).intValue());
            userTag.setAggregationPeriod(""5s"");
            userTag.setAggregationType(""sum"");
            userTag.setTimePeriod(""hour"");
            userTag.setDataType(""number"");
            userTag.setBaseTime(1657803600);
            userTag.setTopic(UUID.randomUUID().toString());
            for(int i = 0;i<100;i++){
                userTag.setUserTagName(UUID.randomUUID() + ""-""+i);
                ctx.collect(userTag);
            }
            Thread.sleep(1);
        }
    }
    @Override
    public void cancel() {
    }
});

source.windowAll(TumblingProcessingTimeWindows.of(Time.seconds(5)))
        .aggregate(new AggregateFunction<UserTag, Map<String,UserTag>, List<UserTag>>(){
            @Override
            public Map<String, UserTag> createAccumulator() {
                return new HashMap<>();
            }

            @Override
            public Map<String, UserTag> add(UserTag userTag, Map<String, UserTag> stringUserTagMap) {
                stringUserTagMap.put(userTag.getUserTagName(),userTag);
                return stringUserTagMap;
            }

            @Override
            public List<UserTag> getResult(Map<String, UserTag> stringUserTagMap) {
                return new ArrayList<>(stringUserTagMap.values());
            }

            @Override
            public Map<String, UserTag> merge(Map<String, UserTag> acc1, Map<String, UserTag> acc2) {
                acc1.putAll(acc2);
                return acc1;
            }
        })
        .setParallelism(1)
        .name(""NewUserTagAggregation_5s"")
        .print().setParallelism(2);

// execute program
env.execute(); {code}
!image-2022-07-15-14-19-21-200.png!

!image-2022-07-15-14-19-41-678.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/22 06:19;yue.shang;image-2022-07-15-14-19-21-200.png;https://issues.apache.org/jira/secure/attachment/13046787/image-2022-07-15-14-19-21-200.png","15/Jul/22 06:19;yue.shang;image-2022-07-15-14-19-41-678.png;https://issues.apache.org/jira/secure/attachment/13046786/image-2022-07-15-14-19-41-678.png","18/Jul/22 11:56;yue.shang;image-2022-07-18-19-56-45-650.png;https://issues.apache.org/jira/secure/attachment/13046907/image-2022-07-18-19-56-45-650.png","18/Jul/22 11:59;yue.shang;image-2022-07-18-19-59-18-872.png;https://issues.apache.org/jira/secure/attachment/13046908/image-2022-07-18-19-59-18-872.png","18/Jul/22 12:34;yue.shang;image-2022-07-18-20-34-54-407.png;https://issues.apache.org/jira/secure/attachment/13046909/image-2022-07-18-20-34-54-407.png","18/Jul/22 12:36;yue.shang;image-2022-07-18-20-36-23-021.png;https://issues.apache.org/jira/secure/attachment/13046910/image-2022-07-18-20-36-23-021.png",,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 02:08:43 UTC 2022,,,,,,,,,,"0|z16vso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 08:22;martijnvisser;[~yue.shang]  This doesn't seem like a complete bug ticket. There can be multiple reasons why the performance has been subpar in your case. It's better to reach out to the Flink community via the User mailinglist, Stackoverflow or Slack. One thing that is immediately visible from your screenshot is that you're using Kryo serialization which is slower then other serializers. See [https://flink.apache.org/news/2020/04/15/flink-serialization-tuning-vol-1.html|https://flink.apache.org/news/2020/04/15/flink-serialization-tuning-vol-1.html#kryo] for example. ;;;","18/Jul/22 12:36;yue.shang;sorry,i forgot my env opts,

-Dstate.backend.incremental=true
-Drest.flamegraph.enabled=true
-Dstate.backend=rocksdb

 

just  0.numRecordsInPerSecond will be 170/s while use rocksdb,but 0.numRecordsInPerSecond is 70000/s while state.backend change to hashmap.

 

And  i had register kryo type:
{code:java}
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.getConfig().registerKryoType(Map.class);
env.getConfig().registerKryoType(UserTag.class);
env.getConfig().registerKryoType(List.class);{code}
but also not work,it just 300/s.Flame graph  is very terrible.

!image-2022-07-18-20-34-54-407.png!

!image-2022-07-18-20-36-23-021.png!

ALL  application mode jobManager configuration：
$internal.application.main WindowAggTest
$internal.application.program-args --nub;10000
$internal.deployment.config-dir /data/services/-202207062015-master-7cf96f44/bin/bylink/flink/flink-env/flink-1.14.3/conf
$internal.yarn.log-config-file /data/services/-202207062015-master-7cf96f44/bin/bylink/flink/flink-env/flink-1.14.3/conf/log4j.properties
classloader.resolve-order parent-first
execution.checkpointing.externalized-checkpoint-retention RETAIN_ON_CANCELLATION
execution.checkpointing.interval 600s
execution.checkpointing.mode EXACTLY_ONCE
execution.checkpointing.timeout 60s
execution.checkpointing.tolerable-failed-checkpoints 3
execution.target embedded
high-availability.cluster-id application_*******_****
internal.cluster.execution-mode NORMAL
internal.io.tmpdirs.use-local-default true
io.tmp.dirs /data/storage/yarn/local/usercache/*****/appcache/application_*******_****
jobmanager.archive.fs.dir hdfs:///****/flink/completed-jobs/
jobmanager.memory.heap.size 469762048b
jobmanager.memory.jvm-metaspace.size 268435456b
jobmanager.memory.jvm-overhead.max 201326592b
jobmanager.memory.jvm-overhead.min 201326592b
jobmanager.memory.off-heap.size 134217728b
jobmanager.memory.process.size 1024m
jobmanager.rpc.address ****
jobmanager.rpc.port ****
parallelism.default 1
pipeline.classpaths
pipeline.jars file:/********/container_****/WindowsAggTest-1.0-SNAPSHOT.jar
pipeline.name 1757_Admin_aggTest
rest.address **********
rest.flamegraph.enabled true
state.backend rocksdb
state.backend.incremental true
state.checkpoint-storage filesystem
state.checkpoints.dir hdfs:///******/flink/checkpoints/1757
state.checkpoints.num-retained 10
state.savepoints.dir hdfs:///******/flink/checkpoints/1757
taskmanager.memory.process.size 8192m
taskmanager.numberOfTaskSlots 1
web.port 0
web.tmpdir /tmp/flink-web-********8
yarn.application.name 1757_Admin_aggTest
yarn.application.queue ************
yarn.application.type Apache Flink 1.14.3
yarn.flink-dist-jar hdfs:///*******/flink/flink-env/flink-1.14.3/lib/flink-dist_2.12-1.14.3.jar
yarn.provided.lib.dirs hdfs:///*********/flink/flink-env/flink-1.14.3/lib
 ;;;","18/Jul/22 12:40;martijnvisser;[~yue.shang] It still shows that you're serializing with Kryo. Please first reach out to the community for help with this before (re)opening a Jira ticket. This is currently not enough information to conclude that something is wrong with Flink that warrants creation of a ticket. ;;;","19/Jul/22 02:08;yue.shang;ok,

i will reach out to the community for help.

 

thank you.;;;",,,,,,,,,,,,,,,,,
Merge subpartition shuffle data read request for better sequential IO,FLINK-28561,13471694,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kevin.cyj,kevin.cyj,15/Jul/22 06:00,03/Nov/22 07:49,04/Jun/24 20:42,,,,,,,,,,Runtime / Network,,,,,,,0,,,,,,,"Currently, the shuffle data of each subpartition for blocking shuffle is read separately. To achieve better performance and reduce IOPS, we can merge consecutive data requests of the same field together and serves them in one IO request. More specifically,

1) if multiple data requests are reading the same data, for example, reading broadcast data, the reader will read the data only once and send the same piece of data to multiple downstream consumers.

2) if multiple data requests are reading the consecutive data in one file, we will merge those data requests together as one large request and read a larger size of data sequentially which is good for file IO performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-15 06:00:13.0,,,,,,,,,,"0|z16vrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 3.3 profile for SparkSource,FLINK-28560,13471678,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,nicholasjiang,nicholasjiang,15/Jul/22 03:44,15/Jul/22 07:06,04/Jun/24 20:42,15/Jul/22 07:06,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"flink-table-store-spark module support Spark 3.0~3.2 profile, which has published the Spark 3.3.0 version. Spark 3.3 profile can be introduced for SparkSource to follow the release version of Spark 3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 15 07:06:14 UTC 2022,,,,,,,,,,"0|z16vo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 07:05;nicholasjiang;[~lzljs3620320], please help to assign this to me and close this ticket.;;;","15/Jul/22 07:06;lzljs3620320;master: 15ddb22174fb94bd36b176f0eb487f5a1b39d443;;;",,,,,,,,,,,,,,,,,,,
Support DataStream PythonKeyedProcessOperator in Thread Mode,FLINK-28559,13471677,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,15/Jul/22 03:41,29/Jul/22 11:28,04/Jun/24 20:42,29/Jul/22 11:28,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-25724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 11:28:25 UTC 2022,,,,,,,,,,"0|z16vo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 11:28;hxbks2ks;Merged into master via 4cc33f97d1351be4836e973a528042684475a069;;;",,,,,,,,,,,,,,,,,,,,
HistoryServer log retrieval configuration improvement,FLINK-28558,13471674,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,xtsong,xtsong,15/Jul/22 03:20,06/Dec/22 06:13,04/Jun/24 20:42,18/Jul/22 05:12,,,,,1.16.0,,,,Runtime / Configuration,Runtime / REST,,,,,,0,pull-request-available,,,,,,"HistoryServer generates log retrieval urls base on the following configuration:
- historyserver.log.jobmanager.url-pattern
- historyserver.log.taskmanager.url-pattern

The usability can be improved in two ways:
- Explicitly explain in description that only http/https schemas are supported, and add sanity checks for it.
- If the schema is not specified, add ""http://"" by default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 06 06:13:15 UTC 2022,,,,,,,,,,"0|z16vnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 05:12;xtsong;master (1.16): 6eee0c69b6cd28fb086bf44e1d7fdb6f646a467e;;;","17/Nov/22 01:49;leo.zhi;Hi [~xtsong] ,

May I know how to use below two configuations? Is there has some sample I can try?
It would be better to show which log archiving and browsing services I can use.
Many thanks.
 - historyserver.log.jobmanager.url-pattern
 - historyserver.log.taskmanager.url-pattern;;;","17/Nov/22 02:36;xtsong;[~leo.zhi]
Here's the documentation for this feature.
https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/advanced/historyserver/#log-integration

The log archiving and browsing is highly depending on your deployment environment. E.g., Yarn has a log aggregation feature that collects logs store them on HDFS. Flink does not provide such services.;;;","17/Nov/22 03:05;leo.zhi;Hi [~xtsong] ,

Thanks for the quick reply. :)

We use Flink on K8s instead, is there any way we use this feature on K8s?
By the way,the fs what we used is minio :(;;;","17/Nov/22 03:13;xtsong;I'm not familiar whether there's any existing tools that collects log files from a terminated pod. In our production, we use a custom log4j appender to write the logs directly to an external log system. ;;;","17/Nov/22 03:24;leo.zhi;Yes, so close, we use kafka log appender, and also can write log file to minio.

But it's hard to know how use these confirguation between minio and flink historyserver.

Mayby minio is just a file system, not a log system.

Thank for your patience, let me think think think ...;;;","17/Nov/22 06:03;xtsong;This feature helps integrate the log browsing service into the history server, so you can conveniently jump to the corresponding log by clicking on the interested task in history server. This works only if you already have a web service where logs can be browsed. ;;;","06/Dec/22 06:13;yanvcl;How can I use those two configuations to browse the taskmanagers log on YARN environment?;;;",,,,,,,,,,,,,
CheckpointCoordinatorTriggeringTest.discardingTriggeringCheckpointWillExecuteNextCheckpointRequest Process produced no output for 900 seconds,FLINK-28557,13471672,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hxbks2ks,hxbks2ks,15/Jul/22 03:14,15/Jul/22 03:16,04/Jun/24 20:42,15/Jul/22 03:16,1.16.0,,,,,,,,Runtime / Checkpointing,Tests,,,,,,0,test-stability,,,,,,"
{code:java}
.1740649Z Jul 14 04:51:02 ""main"" #1 prio=5 os_prio=0 tid=0x00007f329000b800 nid=0x658 in Object.wait() [0x00007f3299b34000]
2022-07-14T04:51:03.1741235Z Jul 14 04:51:02    java.lang.Thread.State: WAITING (on object monitor)
2022-07-14T04:51:03.1741715Z Jul 14 04:51:02 	at java.lang.Object.wait(Native Method)
2022-07-14T04:51:03.1742197Z Jul 14 04:51:02 	at java.lang.Object.wait(Object.java:502)
2022-07-14T04:51:03.1742867Z Jul 14 04:51:02 	at org.apache.flink.core.testutils.OneShotLatch.await(OneShotLatch.java:61)
2022-07-14T04:51:03.1743707Z Jul 14 04:51:02 	- locked <0x00000000f6778e80> (a java.lang.Object)
2022-07-14T04:51:03.1744626Z Jul 14 04:51:02 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorTriggeringTest.discardingTriggeringCheckpointWillExecuteNextCheckpointRequest(CheckpointCoordinatorTriggeringTest.java:731)
2022-07-14T04:51:03.1745509Z Jul 14 04:51:02 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-14T04:51:03.1746147Z Jul 14 04:51:02 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-14T04:51:03.1746877Z Jul 14 04:51:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-14T04:51:03.1747530Z Jul 14 04:51:02 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-14T04:51:03.1748167Z Jul 14 04:51:02 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-14T04:51:03.1748901Z Jul 14 04:51:02 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-14T04:51:03.1749593Z Jul 14 04:51:02 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-14T04:51:03.1750303Z Jul 14 04:51:02 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-14T04:51:03.1750993Z Jul 14 04:51:02 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-14T04:51:03.1751661Z Jul 14 04:51:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-14T04:51:03.1752326Z Jul 14 04:51:02 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-14T04:51:03.1752968Z Jul 14 04:51:02 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-14T04:51:03.1753635Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-14T04:51:03.1754384Z Jul 14 04:51:02 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-14T04:51:03.1755055Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-14T04:51:03.1755715Z Jul 14 04:51:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-14T04:51:03.1756430Z Jul 14 04:51:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-14T04:51:03.1757080Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-14T04:51:03.1757788Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-14T04:51:03.1758394Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-14T04:51:03.1759016Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-14T04:51:03.1759632Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-14T04:51:03.1760264Z Jul 14 04:51:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-14T04:51:03.1760865Z Jul 14 04:51:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-14T04:51:03.1761460Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-14T04:51:03.1762046Z Jul 14 04:51:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-14T04:51:03.1762624Z Jul 14 04:51:02 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-14T04:51:03.1763187Z Jul 14 04:51:02 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-14T04:51:03.1763891Z Jul 14 04:51:02 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-14T04:51:03.1764674Z Jul 14 04:51:02 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-14T04:51:03.1765391Z Jul 14 04:51:02 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-14T04:51:03.1766264Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-14T04:51:03.1767083Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-14T04:51:03.1767917Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-14T04:51:03.1768722Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator$$Lambda$154/1773638882.accept(Unknown Source)
2022-07-14T04:51:03.1769523Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-14T04:51:03.1770381Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-14T04:51:03.1771153Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-14T04:51:03.1771858Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-14T04:51:03.1772627Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-14T04:51:03.1773538Z Jul 14 04:51:02 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-14T04:51:03.1774431Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-07-14T04:51:03.1775222Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider$$Lambda$30/1188392295.accept(Unknown Source)
2022-07-14T04:51:03.1775879Z Jul 14 04:51:02 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-07-14T04:51:03.1776569Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-07-14T04:51:03.1777399Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-14T04:51:03.1778195Z Jul 14 04:51:02 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-07-14T04:51:03.1778959Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-14T04:51:03.1779771Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-14T04:51:03.1780444Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-14T04:51:03.1781106Z Jul 14 04:51:02 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-14T04:51:03.1781589Z Jul 14 04:51:02 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38187&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-15 03:14:48.0,,,,,,,,,,"0|z16vmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extract header fields of Buffer into a BufferHeader class for blocking shuffle file IO,FLINK-28556,13471665,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,15/Jul/22 02:39,22/Jul/22 00:05,04/Jun/24 20:42,22/Jul/22 00:05,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,This is a small code refactor which also can be reused by following PRs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 00:05:01 UTC 2022,,,,,,,,,,"0|z16vlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 00:05;kevin.cyj;Merged into master via 3a8bf55892f1e46eb912e6fe717ea6a0e426ae69;;;",,,,,,,,,,,,,,,,,,,,
Update history server documentation,FLINK-28555,13471662,13469358,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xtsong,xtsong,xtsong,15/Jul/22 02:21,19/Jul/22 05:19,04/Jun/24 20:42,19/Jul/22 05:19,,,,,1.16.0,,,,Documentation,,,,,,,0,,,,,,,"Update the history server documentation, w.r.t the log-viewing integration and the supported rest apis.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 05:19:44 UTC 2022,,,,,,,,,,"0|z16vko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 05:19;xtsong;master (1.16): 0394a01ebb4f1c6b3c3534acc58dd2467c8196e1;;;",,,,,,,,,,,,,,,,,,,,
Kubernetes-Operator allow readOnlyRootFilesystem via operatorSecurityContext,FLINK-28554,13471621,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,timsn,timsn,timsn,14/Jul/22 16:49,23/Dec/22 18:19,04/Jun/24 20:42,24/Aug/22 07:37,kubernetes-operator-1.0.1,,,,kubernetes-operator-1.2.0,,,,Kubernetes Operator,,,,,,,0,operator,pull-request-available,,,,,"It would be nice if the operator would support using the ""readOnlyRootFilesystem"" setting via the operatorSecurityContext. When using the default operator template the operator won't be able to start when using this setting because the config files mounted in `/opt/flink/conf` are now (of course) also read-only.

It would be nice if the default template would be written in such a way that it allows adding emptyDir volumes to /opt/flink/conf via the values.yaml. Which is not possible right now. Then the config files can remain editable by the operator while keeping the root filesystem read-only.

I have successfully tried that in my branch (see: https://github.com/apache/flink-kubernetes-operator/compare/main...timsn:flink-kubernetes-operator:mount-single-flink-conf-files) which prepares the operator template.

After this small change to the template it is possible add emptyDir volumes for the conf and tmp dirs and in the second step to enable the readOnlyRootFilesystem setting via the values.yaml

values.yaml
{code:java}
[...]

operatorVolumeMounts:
  create: true
  data:
    - name: flink-conf
      mountPath: /opt/flink/conf
      subPath: conf
    - name: flink-tmp
      mountPath: /tmp

operatorVolumes:
  create: true
  data:
    - name: flink-conf
      emptyDir: {}
    - name: flink-tmp
      emptyDir: {}

operatorSecurityContext:
  readOnlyRootFilesystem: true
[...]{code}
I think this could be a viable way to allow this security setting and I could turn this into a pull request if desired. What do you think about it? Or is there even a better way to achive this I didn't think about yet?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 23 18:19:49 UTC 2022,,,,,,,,,,"0|z16vbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 02:07;wangyang0918;If we want to update the operator configuration, could we simply edit the configmap? Why do we need to make the conf directory editable?;;;","27/Jul/22 12:12;timsn;Hi [~wangyang0918], it's not about changing the configuration it's more about making it possible to set
{code:java}
readOnlyRootFilesystem: true{code}
for the operator to comply for security checks in our environment. And without the change to the template (see: 
[https://github.com/apache/flink-kubernetes-operator/compare/main...timsn:flink-kubernetes-operator:mount-single-flink-conf-files]) and in combination with the mentioned emptyDir volumes in the values.yaml (see in the description) it's not possible to run the operator because it simply crashes / refuses to start. You can try that yourself by adding:
{code:java}
operatorSecurityContext:
  readOnlyRootFilesystem: true
{code}
to your values.yaml. The operator won't be able to start.;;;","05/Aug/22 11:26;wangyang0918;[~timsn] Sorry for the late response. I am in favor of supporting the readOnlyRootFilesystem by simply introducing the {{{}subPath{}}}.

Would you like to create a PR for this issue? I am glad to help with reviewing and merging.;;;","05/Aug/22 15:32;timsn;Sounds great, I will look into it as soon as I'm back from vacation.;;;","22/Aug/22 15:21;timsn;[~wangyang0918] I opened up a PR, please take a look: https://github.com/apache/flink-kubernetes-operator/pull/352/;;;","24/Aug/22 07:37;gyfora;merged to main def69e4e11bd95ee6cff813deb962afa70282e11;;;","20/Dec/22 11:40;wangyang0918;After this PR is merged, it seems that we could not update the operator config dynamically via {{{}kubectl edit cm flink-operator-config{}}}.;;;","20/Dec/22 12:40;bgeng777;Want to attach more information:
According to k8s' [doc|https://kubernetes.io/docs/concepts/storage/volumes/#configmap], ""A container using a ConfigMap as a subPath volume mount will not receive ConfigMap updates"". 
It may also be worthwhile to mention that {{dynamic.config.check.interval}} config may not work as expected in some case. For example, we are relying on {{dynamic.config.check.interval}}  to do some default config update by updating {{flink-operator-config}} ConfigMap without restarting the operator Pod. After this jira, the above dynamic update is broken. Also, the {{CONF_OVERRIDE_DIR}} introduced in FLINK-28445 may not work as well.

 ;;;","20/Dec/22 12:56;gyfora;In that case I suggest we roll back this change, and include the rollback-commit in 1.3.1.
Anyways I am planning to prepare a 1.3.1 patch release during the holidays as we have recently identified and fixed some other important issues.;;;","23/Dec/22 18:19;gyfora;Seems like this is already reverted in main and we should add this for 1.3.1 (dc173cbccd5f9a90fe573ce01790684253dcc0a9)

 ;;;",,,,,,,,,,,
The serializer in StateMap has not been updated when metaInfo of StateTable updated,FLINK-28553,13471594,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,masteryhx,masteryhx,masteryhx,14/Jul/22 13:15,31/Oct/22 02:55,04/Jun/24 20:42,11/Oct/22 08:07,1.14.5,1.15.1,,,1.17.0,,,,Runtime / State Backends,,,,,,,0,,,,,,,"When the meta info in StateTable updated, the serializer in StateMap has not been updated.
(See StateTable#setMetaInfo)
The value may be serialized/deserialized/copied incorrectly after triggering state migration of HashMapStateBackend.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26853,,,,,,,,FLINK-23143,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 31 02:55:32 UTC 2022,,,,,,,,,,"0|z16v5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/22 02:55;Yanfei Lei;Thanks [~roman]  for merging in master [8df50536ef913b63620d896423c39cdd01941c55|https://github.com/apache/flink/commit/8df50536ef913b63620d896423c39cdd01941c55]

 ;;;",,,,,,,,,,,,,,,,,,,,
GenerateUtils#generateCompare supports MULTISET and MAP,FLINK-28552,13471566,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nicholasjiang,qingyue,qingyue,14/Jul/22 09:53,15/Nov/22 07:21,04/Jun/24 20:42,15/Nov/22 07:21,table-store-0.2.0,,,,table-store-0.3.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"Currently, changelog mode cannot support map and multiset as the field type.

More specifically,
 * MULTISET is not supported at all, including append-only mode. (
Caused by: java.lang.UnsupportedOperationException: Unsupported type: MULTISET<TINYINT>
at org.apache.flink.table.store.shaded.org.apache.flink.orc.OrcSplitReaderUtil.logicalTypeToOrcType(OrcSplitReaderUtil.java:214)
at org.apache.flink.table.store.shaded.org.apache.flink.orc.OrcSplitReaderUtil.logicalTypeToOrcType(OrcSplitReaderUtil.java:210)
at org.apache.flink.table.store.format.orc.OrcFileFormat.createWriterFactory(OrcFileFormat.java:94)
at org.apache.flink.table.store.file.data.AppendOnlyWriter$RowRollingWriter.lambda$createRollingRowWriter$0(AppendOnlyWriter.java:229)
at org.apache.flink.table.store.file.writer.RollingFileWriter.openCurrentWriter(RollingFileWriter.java:73)
at org.apache.flink.table.store.file.writer.RollingFileWriter.write(RollingFileWriter.java:61)
at org.apache.flink.table.store.file.data.AppendOnlyWriter.write(AppendOnlyWriter.java:108)
at org.apache.flink.table.store.file.data.AppendOnlyWriter.write(AppendOnlyWriter.java:56)
at org.apache.flink.table.store.table.AppendOnlyFileStoreTable$3.writeSinkRecord(AppendOnlyFileStoreTable.java:119)
at org.apache.flink.table.store.table.sink.AbstractTableWrite.write(AbstractTableWrite.java:76)
at org.apache.flink.table.store.connector.sink.StoreWriteOperator.processElement(StoreWriteOperator.java:124)
... 13 more)
 * MAP cannot be pk for key-value mode, and cannot be the fields for value-count mode.

 

Stacktrace

java.lang.UnsupportedOperationException
    at org.apache.flink.table.store.codegen.GenerateUtils$.generateCompare(GenerateUtils.scala:139)
    at org.apache.flink.table.store.codegen.GenerateUtils$.$anonfun$generateRowCompare$1(GenerateUtils.scala:289)
    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:32)
    at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:29)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:194)
    at org.apache.flink.table.store.codegen.GenerateUtils$.generateRowCompare(GenerateUtils.scala:263)
    at org.apache.flink.table.store.codegen.ComparatorCodeGenerator$.gen(ComparatorCodeGenerator.scala:45)
    at org.apache.flink.table.store.codegen.ComparatorCodeGenerator.gen(ComparatorCodeGenerator.scala)
    at org.apache.flink.table.store.codegen.CodeGeneratorImpl.generateRecordComparator(CodeGeneratorImpl.java:53)
    at org.apache.flink.table.store.codegen.CodeGenUtils.generateRecordComparator(CodeGenUtils.java:66)
    at org.apache.flink.table.store.file.utils.KeyComparatorSupplier.<init>(KeyComparatorSupplier.java:40)
    at org.apache.flink.table.store.file.KeyValueFileStore.<init>(KeyValueFileStore.java:59)
    at org.apache.flink.table.store.table.ChangelogValueCountFileStoreTable.<init>(ChangelogValueCountFileStoreTable.java:73)
    at org.apache.flink.table.store.table.FileStoreTableFactory.create(FileStoreTableFactory.java:70)
    at org.apache.flink.table.store.table.FileStoreTableFactory.create(FileStoreTableFactory.java:50)
    at org.apache.flink.table.store.spark.SimpleTableTestHelper.<init>(SimpleTableTestHelper.java:58)
    at org.apache.flink.table.store.spark.SparkReadITCase.startMetastoreAndSpark(SparkReadITCase.java:93)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28704,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 15 07:21:36 UTC 2022,,,,,,,,,,"0|z16uzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jul/22 02:49;complone;I would like to try to tease out the background of this issue;;;","15/Nov/22 07:21;lzljs3620320;master: dbe49f7ab6b936a51c09a066c8212df96e744324;;;",,,,,,,,,,,,,,,,,,,
Store the number of bytes instead of the number of buffers in index entry for sort-shuffle,FLINK-28551,13471563,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,14/Jul/22 09:47,22/Jul/22 05:43,04/Jun/24 20:42,22/Jul/22 05:43,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Currently, in each index entry of sort-shuffle index file, one filed is the number of buffers in the current data region. The problem is that it is hard to know the data boundary before reading the file, to solve the problem, we can store the number of bytes instead of the number of buffers in index entry. Based on this change, we can do some optimization, for example, read larger size of data than a buffer for better sequential IO like what's mentioned in FLINK-28373.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 05:43:30 UTC 2022,,,,,,,,,,"0|z16uyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jul/22 05:43;kevin.cyj;Merged into master via 9b2711ac42dc0facaec4328ec04ef5b5d00752e6;;;",,,,,,,,,,,,,,,,,,,,
Remove the unused field in SortMergeSubpartitionReader,FLINK-28550,13471562,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,14/Jul/22 09:29,21/Jul/22 03:00,04/Jun/24 20:42,21/Jul/22 03:00,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 03:00:03 UTC 2022,,,,,,,,,,"0|z16uyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:00;kevin.cyj;Merged into master via 5923c510f24269a83e9886f96f62571602c448e7;;;",,,,,,,,,,,,,,,,,,,,
Support DataStream PythonProcessOperator in Thread Mode,FLINK-28549,13471557,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,14/Jul/22 09:05,26/Jul/22 02:43,04/Jun/24 20:42,26/Jul/22 02:43,1.16.0,,,,1.16.0,,,,API / Python,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,FLINK-25724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 26 02:43:12 UTC 2022,,,,,,,,,,"0|z16uxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 02:43;hxbks2ks;Merged into master via 7ea31cfcf360bc40b2445c72aae14746577aff9d;;;",,,,,,,,,,,,,,,,,,,,
The commit partition base path is not created when no data is sent which may cause FileNotFoundException,FLINK-28548,13471514,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiangang,Jiangang,Jiangang,14/Jul/22 04:02,16/Aug/22 04:11,04/Jun/24 20:42,16/Aug/22 04:11,1.14.5,1.15.1,1.16.0,,1.16.0,,,,Connectors / FileSystem,,,,,,,0,pull-request-available,,,,,,"The commit partition base path is not created when no data is sent which may cause FileNotFoundException.  The exception is as following:
{code:java}
Caused by: java.io.FileNotFoundException: File /home/ljg/test_sql.db/flink_batch_test/.staging_1657697612169 does not exist.
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:771) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:120) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:828) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:824) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.perflog.FileSystemLinkResolverWithStatistics$1.doCall(FileSystemLinkResolverWithStatistics.java:37) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.perflog.PerfProxy.call(PerfProxy.java:49) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.perflog.FileSystemLinkResolverWithStatistics.resolve(FileSystemLinkResolverWithStatistics.java:39) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:835) ~[hadoop-hdfs-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:238) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:238) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:241) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:376) ~[hadoop-common-2.6.0U203-cdh5.4.4.jar:?]
	at org.apache.flink.hive.shaded.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[flink-sql-connector-hive-2.3.6_2.12-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.table.PartitionTempFileManager.listTaskTemporaryPaths(PartitionTempFileManager.java:87) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.table.FileSystemCommitter.commitPartitions(FileSystemCommitter.java:78) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.connector.file.table.FileSystemOutputFormat.finalizeGlobal(FileSystemOutputFormat.java:89) ~[flink-connector-files-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.jobgraph.InputOutputFormatVertex.finalizeOnMaster(InputOutputFormatVertex.java:153) ~[flink-dist-1.15.0.jar:1.15.0]
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.jobFinished(DefaultExecutionGraph.java:1190) ~[flink-dist-1.15.0.jar:1.15.0]
	... 43 more {code}
We should check whether the base path exists before listStatus for the path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 04:11:31 UTC 2022,,,,,,,,,,"0|z16uns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jul/22 06:34;Jiangang;[~zhangjing2000] [~lzljs3620320]  Could you have a look?  Thanks.;;;","20/Jul/22 06:56;lzljs3620320;Hi [~Jiangang], sorry I dont have time to look to this.
CC [~jark];;;","05/Aug/22 05:14;Jiangang;[~jark] Do you have time to review it? Thanks.;;;","16/Aug/22 04:11;jark;Fixed in master: f2abb51ac91c8b0e9bdd261de791d3aa1ba033dd;;;",,,,,,,,,,,,,,,,,
Add IT cases for SpeculativeScheduler,FLINK-28547,13471507,13450988,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,14/Jul/22 03:18,21/Jul/22 02:59,04/Jun/24 20:42,21/Jul/22 02:59,,,,,1.16.0,,,,Runtime / Coordination,Tests,,,,,,0,pull-request-available,,,,,,Add IT cases for SpeculativeScheduler.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 02:59:48 UTC 2022,,,,,,,,,,"0|z16um8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 08:01;wanglijie;[~zhuzh] please assign the ticket to me, thanks :);;;","21/Jul/22 02:59;zhuzh;Done via fd763672b858e74b24760e5c98ff9af22caa8a14;;;",,,,,,,,,,,,,,,,,,,
Add the release logic for py39 and m1 wheel package,FLINK-28546,13471505,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hxbks2ks,hxbks2ks,hxbks2ks,14/Jul/22 03:09,15/Jul/22 02:52,04/Jun/24 20:42,15/Jul/22 02:52,1.16.0,,,,1.16.0,,,,API / Python,Release System,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 15 02:52:14 UTC 2022,,,,,,,,,,"0|z16uls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jul/22 02:52;hxbks2ks;Merged into master via d1e057a761ab69a5b157ee6034e9f1560ede15c2;;;",,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducerITCase.testRestoreToCheckpointAfterExceedingProducersPool  failed with TimeoutException: Topic flink-kafka-producer-fail-before-notify not present in metadata after 60000 ms,FLINK-28545,13471502,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,hxbks2ks,hxbks2ks,14/Jul/22 02:53,16/Oct/23 12:14,04/Jun/24 20:42,16/Oct/23 12:14,1.15.1,,,,,,,,Connectors / Kafka,,,,,,,0,auto-deprioritized-major,test-stability,,,,,"
{code:java}
2022-07-13T09:49:00.4699245Z Jul 13 09:49:00 [ERROR] org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.testRestoreToCheckpointAfterExceedingProducersPool  Time elapsed: 243.968 s  <<< ERROR!
2022-07-13T09:49:00.4700438Z Jul 13 09:49:00 org.apache.kafka.common.errors.TimeoutException: Topic flink-kafka-producer-fail-before-notify not present in metadata after 60000 ms.
2022-07-13T09:49:00.4702497Z Jul 13 09:49:00 
2022-07-13T09:49:00.8707199Z Jul 13 09:49:00 [INFO] 
2022-07-13T09:49:00.8708010Z Jul 13 09:49:00 [INFO] Results:
2022-07-13T09:49:00.8708577Z Jul 13 09:49:00 [INFO] 
2022-07-13T09:49:00.8709134Z Jul 13 09:49:00 [ERROR] Errors: 
2022-07-13T09:49:00.8711253Z Jul 13 09:49:00 [ERROR]   FlinkKafkaProducerITCase.testRestoreToCheckpointAfterExceedingProducersPool » Timeout
2022-07-13T09:49:00.8712471Z Jul 13 09:49:00 [INFO] 
2022-07-13T09:49:00.8713163Z Jul 13 09:49:00 [ERROR] Tests run: 209, Failures: 0, Errors: 1, Skipped: 1
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38129&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:20 UTC 2023,,,,,,,,,,"0|z16ul4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 02:54;hxbks2ks;cc [~renqs];;;","13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,
Elasticsearch6SinkE2ECase failed with no space left on device,FLINK-28544,13471494,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,14/Jul/22 01:56,20/Jul/22 06:19,04/Jun/24 20:42,20/Jul/22 06:19,1.16.0,,,,,,,,Connectors / ElasticSearch,Tests,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-07-13T02:49:13.5455800Z Jul 13 02:49:13 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.38 s <<< FAILURE! - in org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase
2022-07-13T02:49:13.5465965Z Jul 13 02:49:13 [ERROR] org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase  Time elapsed: 49.38 s  <<< ERROR!
2022-07-13T02:49:13.5466765Z Jul 13 02:49:13 java.lang.RuntimeException: Failed to build JobManager image
2022-07-13T02:49:13.5467621Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:67)
2022-07-13T02:49:13.5468645Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configure(FlinkTestcontainersConfigurator.java:147)
2022-07-13T02:49:13.5469564Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkContainers$Builder.build(FlinkContainers.java:197)
2022-07-13T02:49:13.5470467Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:88)
2022-07-13T02:49:13.5471424Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkContainerTestEnvironment.<init>(FlinkContainerTestEnvironment.java:51)
2022-07-13T02:49:13.5472504Z Jul 13 02:49:13 	at org.apache.flink.streaming.tests.ElasticsearchSinkE2ECaseBase.<init>(ElasticsearchSinkE2ECaseBase.java:58)
2022-07-13T02:49:13.5473388Z Jul 13 02:49:13 	at org.apache.flink.streaming.tests.Elasticsearch6SinkE2ECase.<init>(Elasticsearch6SinkE2ECase.java:36)
2022-07-13T02:49:13.5474161Z Jul 13 02:49:13 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-07-13T02:49:13.5474905Z Jul 13 02:49:13 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-07-13T02:49:13.5475756Z Jul 13 02:49:13 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-07-13T02:49:13.5476734Z Jul 13 02:49:13 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2022-07-13T02:49:13.5477495Z Jul 13 02:49:13 	at org.junit.platform.commons.util.ReflectionUtils.newInstance(ReflectionUtils.java:550)
2022-07-13T02:49:13.5478313Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ConstructorInvocation.proceed(ConstructorInvocation.java:56)
2022-07-13T02:49:13.5479220Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-07-13T02:49:13.5480165Z Jul 13 02:49:13 	at org.junit.jupiter.api.extension.InvocationInterceptor.interceptTestClassConstructor(InvocationInterceptor.java:73)
2022-07-13T02:49:13.5481038Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-07-13T02:49:13.5481944Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-07-13T02:49:13.5482875Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-07-13T02:49:13.5483764Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-07-13T02:49:13.5484642Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-07-13T02:49:13.5486123Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-07-13T02:49:13.5488185Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:77)
2022-07-13T02:49:13.5488883Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestClassConstructor(ClassBasedTestDescriptor.java:355)
2022-07-13T02:49:13.5490237Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateTestClass(ClassBasedTestDescriptor.java:302)
2022-07-13T02:49:13.5491099Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassTestDescriptor.instantiateTestClass(ClassTestDescriptor.java:79)
2022-07-13T02:49:13.5491840Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:280)
2022-07-13T02:49:13.5492618Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:272)
2022-07-13T02:49:13.5493228Z Jul 13 02:49:13 	at java.util.Optional.orElseGet(Optional.java:267)
2022-07-13T02:49:13.5493835Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:271)
2022-07-13T02:49:13.5494551Z Jul 13 02:49:13 	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31)
2022-07-13T02:49:13.5495253Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:197)
2022-07-13T02:49:13.5495940Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5496616Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:196)
2022-07-13T02:49:13.5497286Z Jul 13 02:49:13 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80)
2022-07-13T02:49:13.5497973Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148)
2022-07-13T02:49:13.5498653Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5499323Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-13T02:49:13.5500024Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-13T02:49:13.5500655Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-13T02:49:13.5501345Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5535107Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-13T02:49:13.5535791Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-13T02:49:13.5538031Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-13T02:49:13.5538994Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-07-13T02:49:13.5539797Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-07-13T02:49:13.5540481Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5541154Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-07-13T02:49:13.5541784Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-07-13T02:49:13.5542410Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-07-13T02:49:13.5543090Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-07-13T02:49:13.5543930Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-07-13T02:49:13.5544575Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-07-13T02:49:13.5545351Z Jul 13 02:49:13 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-07-13T02:49:13.5546083Z Jul 13 02:49:13 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-07-13T02:49:13.5546615Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-07-13T02:49:13.5547162Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-07-13T02:49:13.5547713Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-07-13T02:49:13.5548444Z Jul 13 02:49:13 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2022-07-13T02:49:13.5549641Z Jul 13 02:49:13 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
2022-07-13T02:49:13.5550318Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
2022-07-13T02:49:13.5551080Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
2022-07-13T02:49:13.5551656Z Jul 13 02:49:13 	... 57 more
2022-07-13T02:49:13.5552744Z Jul 13 02:49:13 Caused by: java.lang.RuntimeException: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /opt/flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-07-13T02:49:13.5553633Z Jul 13 02:49:13 	at org.rnorth.ducttape.timeouts.Timeouts.callFuture(Timeouts.java:68)
2022-07-13T02:49:13.5554224Z Jul 13 02:49:13 	at org.rnorth.ducttape.timeouts.Timeouts.getWithTimeout(Timeouts.java:43)
2022-07-13T02:49:13.5554761Z Jul 13 02:49:13 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:45)
2022-07-13T02:49:13.5555373Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:252)
2022-07-13T02:49:13.5706429Z Jul 13 02:49:13 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
2022-07-13T02:49:13.5707070Z Jul 13 02:49:13 	... 58 more
2022-07-13T02:49:13.5712449Z Jul 13 02:49:13 Caused by: com.github.dockerjava.api.exception.DockerClientException: Could not build image: ApplyLayer exit status 1 stdout:  stderr: write /opt/flink/opt/flink-s3-fs-presto-1.16-SNAPSHOT.jar: no space left on device
2022-07-13T02:49:13.5765291Z Jul 13 02:49:13 	at com.github.dockerjava.api.command.BuildImageResultCallback.getImageId(BuildImageResultCallback.java:78)
2022-07-13T02:49:13.5766129Z Jul 13 02:49:13 	at com.github.dockerjava.api.command.BuildImageResultCallback.awaitImageId(BuildImageResultCallback.java:50)
2022-07-13T02:49:13.5766802Z Jul 13 02:49:13 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:147)
2022-07-13T02:49:13.5767436Z Jul 13 02:49:13 	at org.testcontainers.images.builder.ImageFromDockerfile.resolve(ImageFromDockerfile.java:40)
2022-07-13T02:49:13.5768029Z Jul 13 02:49:13 	at org.testcontainers.utility.LazyFuture.getResolvedValue(LazyFuture.java:17)
2022-07-13T02:49:13.5768573Z Jul 13 02:49:13 	at org.testcontainers.utility.LazyFuture.get(LazyFuture.java:39)
2022-07-13T02:49:13.5769083Z Jul 13 02:49:13 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-13T02:49:13.5769625Z Jul 13 02:49:13 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-13T02:49:13.5770223Z Jul 13 02:49:13 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-13T02:49:13.5771000Z Jul 13 02:49:13 	at java.lang.Thread.run(Thread.java:750)
2022-07-13T02:49:13.5771336Z Jul 13 02:49:13 
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38110&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 20 06:19:14 UTC 2022,,,,,,,,,,"0|z16ujc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 07:22;martijnvisser;[~hxbks2ks] It looks like the Python Bash e2e tests take up the majority of disk space again:

{code:java}
2022-07-13T02:28:48.9826953Z Jul 13 02:28:48 ##[group]Top 15 biggest directories in terms of used disk space
2022-07-13T02:28:50.8283982Z Jul 13 02:28:50 5778912	.
2022-07-13T02:28:50.8285319Z Jul 13 02:28:50 1615376	./flink-python
2022-07-13T02:28:50.8285977Z Jul 13 02:28:50 1550496	./flink-python/dev
2022-07-13T02:28:50.8286638Z Jul 13 02:28:50 1516288	./flink-end-to-end-tests
2022-07-13T02:28:50.8287306Z Jul 13 02:28:50 1476920	./flink-python/dev/.conda
2022-07-13T02:28:50.8287972Z Jul 13 02:28:50 642240	./flink-python/dev/.conda/pkgs
2022-07-13T02:28:50.8288651Z Jul 13 02:28:50 624860	./flink-dist
2022-07-13T02:28:50.8289280Z Jul 13 02:28:50 624472	./flink-dist/target
2022-07-13T02:28:50.8289931Z Jul 13 02:28:50 565828	./flink-python/dev/.conda/envs
2022-07-13T02:28:50.8290615Z Jul 13 02:28:50 500260	./flink-dist/target/flink-1.16-SNAPSHOT-bin
2022-07-13T02:28:50.8291384Z Jul 13 02:28:50 500256	./flink-dist/target/flink-1.16-SNAPSHOT-bin/flink-1.16-SNAPSHOT
2022-07-13T02:28:50.8292092Z Jul 13 02:28:50 461348	./flink-connectors
2022-07-13T02:28:50.8292552Z Jul 13 02:28:50 373572	./.git
2022-07-13T02:28:50.8292993Z Jul 13 02:28:50 369928	./.git/objects
2022-07-13T02:28:50.8293452Z Jul 13 02:28:50 369920	./.git/objects/pack
{code};;;","18/Jul/22 03:06;hxbks2ks;Thanks [~martijnvisser]. I will take a look.;;;","18/Jul/22 03:12;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38281&view=logs&j=87489130-75dc-54e4-1f45-80c30aa367a3&t=73da6d75-f30d-5d5a-acbe-487a9dcff678;;;","20/Jul/22 06:19;hxbks2ks;I have merged the commit 6b3ac775fe72514192484f7c923da37ebea5f524 in master to clean up the python environment.

I will continue to pay attention to the CI to see whether this problem have been solved
;;;",,,,,,,,,,,,,,,,,
"After compiling Flink, you should provide the artifact file in tar format instead of a folder",FLINK-28543,13471490,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,Yao.Meng,Yao.Meng,14/Jul/22 01:12,14/Jul/22 07:20,04/Jun/24 20:42,14/Jul/22 07:20,1.15.0,1.15.1,,,,,,,Build System,,,,,,,0,flink-dist,,,,,,"After compiling Flink, under the target directory of the flink-dist module, the final binary files are located in the flink-version-bin/flink-version directory, but they are not tar files. Therefore, you always have to execute commands manually to generate tar packages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 07:20:33 UTC 2022,,,,,,,,,,"0|z16uig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 07:20;martijnvisser;As described in the Flink process for creating a release, this is always a manual step https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release;;;",,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] FileSystemBehaviorTestSuite,FLINK-28542,13471477,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,13/Jul/22 22:32,31/Oct/22 11:19,04/Jun/24 20:42,27/Oct/22 13:05,,,,,1.17.0,,,,FileSystems,,,,,,,0,pull-request-available,,,,,,"The FileSystemBehaviorTestSuite in flink-core has an implementation in most modules in flink-filesystems.  All of these implementations (one for each filesystem) should be migrated together.

{color:#000000} {color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29810,FLINK-29811,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 27 13:05:16 UTC 2022,,,,,,,,,,"0|z16ufk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 22:33;rskraba;Can this be assigned to me please?;;;","27/Oct/22 13:05;mapohl;master: 546fcf158f15c74f787d69555e916d79b242a289;;;",,,,,,,,,,,,,,,,,,,
Add OwnerReferences to FlinkDeployment CR in jobmanager Deployment,FLINK-28541,13471445,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,czchen,czchen,13/Jul/22 16:15,14/Jul/22 13:44,04/Jun/24 20:42,14/Jul/22 07:16,,,,,,,,,Kubernetes Operator,,,,,,,0,,,,,,,"`ownerReferences` is used by Argo CD (https://argo-cd.readthedocs.io/en/stable/) to display relation between resources. Since there is no `ownerReferences` in jobmanager Deployment, Argo CD cannot know this Deployment is created by FlinkDeployment CR. Thus Argo CD cannot display full resources managed by FlinkDeployment CR.

Discuss thread in https://apache-flink.slack.com/archives/C03G7LJTS2G/p1657639397473729",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26812,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 13:44:03 UTC 2022,,,,,,,,,,"0|z16u8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 21:52;thw;See https://issues.apache.org/jira/browse/FLINK-26812

 ;;;","14/Jul/22 13:44;czchen;Hi [~thw],

FLINK-26812 is for ownerReferences in native k8s mode. That includes TaskManager pod, JobManager service, Flink config have ownerReferences pointing to JobManage Deployment. This ticket is for ownerFerences in JobManager Deployment pointing to FlinkDeployment CR. They are not the same one.;;;",,,,,,,,,,,,,,,,,,,
Unaligned checkpoint waiting in 'start delay' with AsyncDataStream,FLINK-28540,13471440,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nsharp,nsharp,13/Jul/22 15:48,13/Jul/22 15:48,04/Jun/24 20:42,,1.15.0,,,,,,,,,,,,,,,0,,,,,,,"I am attempting to use unaligned checkpointing with AsyncDataStream, but the checkpoints sit in ""start delay"" until the job finishes.

I have published code that reproduces this to [https://github.com/phxnsharp/AsyncDataStreamCheckpointReproduction]

Reproduction steps:
 * Create a single node Docker swarm.
 * Run the docker-compose.yml file in the repository:
docker stack up -c docker-compose.yml flink
 * Use Flink's web UI to upload the .jar file and run it with default settings.

Expected behavior: Checkpoints happen about once per second since they are unaligned.

Actual behavior: After some number of failed checkpoints (the tasks are not running yet), a single checkpoint sits in ""start delay"" until the job finishes.

 
 * Searching the web seems to indicate the most common issue is asyncInvoke blocking. I added a test in the code to make sure that that is not true.

 * I have tried using rocksdb state backend, which did not help

 * I have tried adding additional TaskWorkers, which did not help

 * I have checked the TaskWorker stats and nothing seems awry. No memory consumption, for example. Nothing obvious in the stack traces

 * If I change the code to be sequential instead of async, checkpoints work fine

 * The log file merely shows the checkpoint being triggered, then it being completed 47 seconds later. No additional information is logged.

Mailing list conversation: [https://lists.apache.org/thread/2y3fb93zfsttq03z11xcnynf10xbpgnn]

Thank you!","Flink 1.15.0 using default options from [https://hub.docker.com/_/flink]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/22 15:48;nsharp;SingleCheckpointLongStartDelay.png;https://issues.apache.org/jira/secure/attachment/13046724/SingleCheckpointLongStartDelay.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,2022-07-13 15:48:53.0,,,,,,,,,,"0|z16u7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable CompactionDynamicLevelBytes in FLASH_SSD_OPTIMIZED,FLINK-28539,13471416,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,usamj,usamj,13/Jul/22 14:03,07/Nov/22 08:50,04/Jun/24 20:42,07/Nov/22 08:49,,,,,,,,,Runtime / State Backends,,,,,,,0,,,,,,,"Investigating the RocksDB predefined options I see that `setLevelCompactionDynamicLevelBytes` is set for SPINNING_DISK options but not FLASH_SSD_OPTIMIZED.

 

From my research it looks like this change would improve the Space Amplification of RocksDB [1] (which can also lead to a trade-off from read/write amplification [2]). It makes sense to me that this feature should be enabled for SSD's as they tend to have less space compared to their HDD counterparts.

There is also an argument to be made to also disable it for SPINNING_DISK options as it could give increased read/write performance [2]

[1] [http://rocksdb.org/blog/2015/07/23/dynamic-level.html]

[2] [https://github.com/EighteenZi/rocksdb_wiki/blob/master/RocksDB-Tuning-Guide.md#amplification-factors]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 07 08:49:24 UTC 2022,,,,,,,,,,"0|z16u20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 03:09;Yanfei Lei;hi [~usamj] , +1 for enabling {{LevelCompactionDynamicLevelBytes}} for FLASH_SSD_OPTIMIZED. And now it's also possible to use {{LevelCompactionDynamicLevelBytes}} by configuring   ""RocksDBConfigurableOptions.USE_DYNAMIC_LEVEL_SIZE: true"" manually.
{quote}There is also an argument to be made to also disable it for SPINNING_DISK options as it could give increased read/write performance [2]
{quote}
As the doc says, ""Spinning disks usually provide much lower random read throughput than flash. If you use level-based compaction, use options.level_compaction_dynamic_level_bytes=true."" I don't see it suggesting disabling {{LevelCompactionDynamicLevelBytes?}}

 ;;;","31/Oct/22 04:25;Yanfei Lei;[~usamj]  Do you want to fix this? (I'd like to open a PR to address this.;;;","07/Nov/22 08:49;Yanfei Lei;Since 1) We can enable CompactionDynamicLevelBytes for SSD by configuring   ""RocksDBConfigurableOptions.USE_DYNAMIC_LEVEL_SIZE: true"" manually.

2)From [rocksdb wiki|https://github.com/facebook/rocksdb/wiki/Tuning-RocksDB-on-Spinning-Disks] , it is recommended to set set level_compaction_dynamic_level_bytes for spinning disk

I am going to close this ticket.;;;",,,,,,,,,,,,,,,,,,
Eagerly emit a watermark the first time we exceed Long.MIN_VALUE,FLINK-28538,13471410,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,13/Jul/22 13:18,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,1.20.0,,,,API / DataStream,,,,,,,0,,,,,,,"The built-in watermark generators are set up to start with a Long.MIN_VALUE watermark, that is updated on each event and periodically emitted later on.

As a result, before the first periodic emission is triggered (shortly after job (re)start) no element is ever considered late.

We should be able to remedy that by emitting a watermark on the first event.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 13 12:40:07 UTC 2023,,,,,,,,,,"0|z16u0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 12:40;Wencong Liu;Hello [~chesnay] , I think the BoundedOutOfOrderWatermarkGenerator will produce the watermark for each record.
{code:java}
public class BoundedOutOfOrderWatermarkGenerator extends WatermarkGenerator {

    private static final long serialVersionUID = 1L;
    private final long delay;
    private final int rowtimeIndex;

    /**
     * @param rowtimeIndex the field index of rowtime attribute, the value of rowtime should never
     *     be null.
     * @param delay The delay by which watermarks are behind the observed timestamp.
     */
    public BoundedOutOfOrderWatermarkGenerator(int rowtimeIndex, long delay) {
        this.delay = delay;
        this.rowtimeIndex = rowtimeIndex;
    }

    @Nullable
    @Override
    public Long currentWatermark(RowData row) {
        return row.getLong(rowtimeIndex) - delay;
    }
} {code};;;",,,,,,,,,,,,,,,,,,,,
OffsetsInitializer.timestamp() fails if no element exceeds timestamps,FLINK-28537,13471386,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,chesnay,chesnay,13/Jul/22 11:02,25/Jul/22 11:29,04/Jun/24 20:42,25/Jul/22 11:29,1.15.0,,,,,,,,Connectors / Kafka,,,,,,,0,,,,,,,"According to a comment in the TimestampOffsetInitializer, if the configured timestamp is not found in Kafka, we just use the latest offset instead.
{code:java}
// First get the current end offsets of the partitions. This is going to be used
// in case we cannot find a suitable offsets based on the timestamp, i.e. the message
// meeting the requirement of the timestamp have not been produced to Kafka yet, in
// this case, we just use the latest offset. {code}
However in practice an exception is thrown in this case.

This is rather unfortunate because it would allow you to easily define a time-based bound for testing.

To reproduce, modify

{\{OffsetsInitializerTest#testTimestampOffsetsInitializer}} to use a significantly larger timestamp (100k+).
{code:java}
java.lang.IllegalArgumentException: Invalid negative offset
        at org.apache.kafka.clients.consumer.OffsetAndTimestamp.<init>(OffsetAndTimestamp.java:36)
        at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.lambda$offsetsForTimes$8(KafkaSourceEnumerator.java:622)
        at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178)
        at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)
        at java.base/java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1746)
        at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
        at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
        at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
        at org.apache.flink.connector.kafka.source.enumerator.KafkaSourceEnumerator$PartitionOffsetsRetrieverImpl.offsetsForTimes(KafkaSourceEnumerator.java:615)
        at org.apache.flink.connector.kafka.source.enumerator.initializer.TimestampOffsetsInitializer.getPartitionOffsets(TimestampOffsetsInitializer.java:57)
        at org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializerTest.testTimestampOffsetsInitializer(OffsetsInitializerTest.java:104)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
        at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
        at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
        at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
        at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
        at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
        at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
        at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548) {code}",,,,,,,,,,,,,,FLINK-28185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 22:06:53 UTC 2022,,,,,,,,,,"0|z16tvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 22:06;mason6345;[~chesnay] I think this is a duplicate of https://issues.apache.org/jira/browse/FLINK-28185;;;",,,,,,,,,,,,,,,,,,,,
Adds an internal postOptimize method for physical dag processing,FLINK-28536,13471367,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lincoln.86xy,lincoln.86xy,13/Jul/22 09:40,14/Jul/22 03:20,04/Jun/24 20:42,14/Jul/22 03:20,,,,,1.16.0,,,,Table SQL / Planner,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 03:20:47 UTC 2022,,,,,,,,,,"0|z16tr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 03:20;godfreyhe;Fixed in master: 2073bd81d2ba5b17b5ab214d69ff022f177fe057;;;",,,,,,,,,,,,,,,,,,,,
Support create namespace/table for SparkCatalog,FLINK-28535,13471365,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,qingyue,qingyue,qingyue,13/Jul/22 09:28,22/Jul/22 04:09,04/Jun/24 20:42,22/Jul/22 04:09,table-store-0.2.0,,,,table-store-0.2.0,table-store-0.3.0,,,Table Store,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 22 04:09:22 UTC 2022,,,,,,,,,,"0|z16tqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 09:34;qingyue;Please assign the ticket to me, thanks:) cc [~lzljs3620320] ;;;","22/Jul/22 04:09;lzljs3620320;master: e57c0bf324876bb67ebfa7abecb56dfc22314056
release-0.2: 24fd3da3b94ba4271dbf9de77103c295ac0414c3;;;",,,,,,,,,,,,,,,,,,,
Spec change event is triggered twice per upgrade,FLINK-28534,13471350,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morhidi,gyfora,gyfora,13/Jul/22 08:11,24/Nov/22 01:02,04/Jun/24 20:42,14/Jul/22 16:13,kubernetes-operator-1.1.0,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,"The event `Detected spec change, starting reconciliation.` is triggered twice during every application/sessionjob upgrade as we first suspend the job then start it in 2 reconcile steps.

We should only trigger this once",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 16:13:22 UTC 2022,,,,,,,,,,"0|z16tnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jul/22 16:13;gyfora;merged to main c011dc2210e90dfbde2e6ed3960f40d94b222c0e;;;",,,,,,,,,,,,,,,,,,,,
SchemaChange supports updateColumnNullability and updateColumnComment,FLINK-28533,13471343,13441352,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,13/Jul/22 07:50,19/Jul/22 03:45,04/Jun/24 20:42,19/Jul/22 03:45,table-store-0.2.0,,,,table-store-0.2.0,,,,Table Store,,,,,,,0,pull-request-available,,,,,,"* Already supported
 ** update / remove table options
 ** add column / change column type
 * More to support
 ** update nullability of column
 ** update column comment",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 19 03:45:07 UTC 2022,,,,,,,,,,"0|z16tls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 07:53;qingyue;[~lzljs3620320] please assign the ticket to me, thanks :);;;","19/Jul/22 03:45;lzljs3620320;master: d9b3eeef306efa10a8c4f1a5d09be11a33849ead;;;",,,,,,,,,,,,,,,,,,,
Support InputFormat as full caching provider in lookup table source,FLINK-28532,13471328,13470246,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,smiralex,renqs,renqs,13/Jul/22 06:22,09/Aug/22 21:13,04/Jun/24 20:42,09/Aug/22 21:13,1.16.0,,,,1.16.0,,,,Table SQL / Runtime,,,,,,,0,pull-request-available,,,,,,Support InputFormat as full caching provider in lookup table source,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 21:13:04 UTC 2022,,,,,,,,,,"0|z16tig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Aug/22 21:13;renqs;Merged to master: 10a6f41fa12284f15af55c359c0c0954800f02de;;;",,,,,,,,,,,,,,,,,,,,
Shutdown cluster after history server archive finished,FLINK-28531,13471316,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,aitozi,aitozi,13/Jul/22 02:59,03/Aug/22 11:48,04/Jun/24 20:42,03/Aug/22 11:48,,,,,,,,,Runtime / Coordination,,,,,,,0,,,,,,,"I met a problem that the job cluster may be shutdown with history server archive file upload not finished.

After some research, It's may be caused by two reason.

First, the {{HistoryServerArchivist#archiveExecutionGraph}} is not wait to complete 
Second, the deregisterApp in the {{KubernetesResourceManagerDriver#deregisterApplication}} will directly remove the deployment. So in the shutdown flow in ClusterEntrypoint, it will first trigger the delete deployment, it will cause the master pod deleted with some operation/future can not finished",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 03 11:47:19 UTC 2022,,,,,,,,,,"0|z16tfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 03:02;aitozi;I propose to fix this in two way:

First, in the Dispatcher, we also add the archive future to the jobTerminationFuture to let it be finished when shutdown.
Second, avoid to delete the master pod in the deregisterApp, and delete the cluster until the ClusterEntrypoint terminationFuture have finished.

;;;","13/Jul/22 03:04;aitozi;cc [~wangyang0918] [~xtsong];;;","13/Jul/22 05:12;xtsong;[~aitozi],
Which version of Flink did you use when encountered this problem? Could it be already fixed by FLINK-24491?;;;","13/Jul/22 06:20;aitozi;[~xtsong] Thanks for your information. It seems a duplicated issue :). I'm using use the flink-1.14 not include this commit now. 
But after looking over the related PR. It seems only solve the first problem I mentioned. I will try to apply this patch and verify whether will this problem gone. I will close this one after verified;;;","13/Jul/22 08:11;xtsong;I think deregister should only happen when job archiving is finished.;;;","03/Aug/22 11:47;aitozi;You are right, the job archiving finished then dispatcher go to shutdown. Closing this ticket now.;;;",,,,,,,,,,,,,,,
Improvement of extraction of conditions that can be pushed into join inputs,FLINK-28530,13471311,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,trushev,trushev,13/Jul/22 02:35,21/Aug/23 22:35,04/Jun/24 20:42,,,,,,,,,,Table SQL / Planner,,,,,,,0,auto-deprioritized-major,pull-request-available,,,,,"Conditions extraction in batch mode was introduced here FLINK-12509 and in stream mode here FLINK-24139
h2. Proposal

This ticket is aimed at replacing current extraction algorithm with new one which covers more complex case with deep nested predicate:
for all n > 0
((((((((a0 and b0) or a1) and b1) or a2) and b2) or a3) ... and bn-1) or an) => (a0 or a1 or ... or an)

*Example.* For n = 3 Flink does not extract (a0 or a1 or a2 or a3):
{code:java}
FlinkSQL> explain select * from A join B on (((((a0=0 and b0=0) or a1=0) and b1=0) or a2=0) and b2=0) or a3=0;

== Optimized Physical Plan ==
Join(joinType=[InnerJoin], where=[OR(AND(OR(AND(OR(AND(=(a0, 0), =(b0, 0)), =(a1, 0)), =(b1, 0)), =(a2, 0)), =(b2, 0)), =(a3, 0))], select=[a0, a1, a2, a3, a4, b0, b1, b2, b3, b4], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[single])
:  +- TableSourceScan(table=[[default_catalog, default_database, A]], fields=[a0, a1, a2, a3, a4])
+- Exchange(distribution=[single])
   +- TableSourceScan(table=[[default_catalog, default_database, B]], fields=[b0, b1, b2, b3, b4])
{code}
while PostgreSQL does:
{code:java}
postgres=# explain select * from A join B on ((((((a0=0 and b0=0) or a1=0) and b1=0) or a2=0) and b2=0) or a3=0);
                                                          QUERY PLAN
------------------------------------------------------------------------------------------------------------------------------
 Nested Loop  (cost=0.00..1805.09 rows=14632 width=40)
   Join Filter: (((((((a.a0 = 0) AND (b.b0 = 0)) OR (a.a1 = 0)) AND (b.b1 = 0)) OR (a.a2 = 0)) AND (b.b2 = 0)) OR (a.a3 = 0))
   ->  Seq Scan on b  (cost=0.00..27.00 rows=1700 width=20)
   ->  Materialize  (cost=0.00..44.17 rows=34 width=20)
         ->  Seq Scan on a  (cost=0.00..44.00 rows=34 width=20)
               Filter: ((a0 = 0) OR (a1 = 0) OR (a2 = 0) OR (a3 = 0))
{code}
h2. Details

Pseudocode of new algorithm:

f – predicate
rel – table
var(rel) – columns
{code:java}
extract(f, rel)
  if f = AND(left, right)
    return AND(extract(left, rel), extract(left, rel))
  if f = OR(left, right)
    return OR(extract(left, rel), extract(left, rel))
  if var(f) subsetOf var(rel)
    return f
  return True

AND(f, True) = AND(True, f) = f
OR(f, True) = OR(True, f) = True
{code}
This algorithm covers deep nested predicates and does not use CNF which increases length of predicate to O(n * e^n) in the worst case.

The same recursive approach is used in PostgreSQL [orclauses.c|https://github.com/postgres/postgres/blob/REL_14_4/src/backend/optimizer/util/orclauses.c#L151-L252] and Apache Spark [predicates.scala|https://github.com/apache/spark/blob/v3.3.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L227-L272]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 21 22:35:21 UTC 2023,,,,,,,,,,"0|z16tew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Major but is unassigned and neither itself nor its Sub-Tasks have been updated for 60 days. I have gone ahead and added a ""stale-major"" to the issue"". If this ticket is a Major, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","21/Aug/23 22:35;flink-jira-bot;This issue was labeled ""stale-major"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Major, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,
ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode failed with CheckpointException: Checkpoint expired before completing,FLINK-28529,13471308,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,hxbks2ks,hxbks2ks,13/Jul/22 02:21,09/Aug/22 21:14,04/Jun/24 20:42,09/Aug/22 21:14,1.16.0,,,,1.16.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-07-12T04:30:49.9912088Z Jul 12 04:30:49 [ERROR] ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode  Time elapsed: 617.048 s  <<< ERROR!
2022-07-12T04:30:49.9913108Z Jul 12 04:30:49 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
2022-07-12T04:30:49.9913880Z Jul 12 04:30:49 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2022-07-12T04:30:49.9914606Z Jul 12 04:30:49 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2022-07-12T04:30:49.9915572Z Jul 12 04:30:49 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode(ChangelogPeriodicMaterializationSwitchStateBackendITCase.java:125)
2022-07-12T04:30:49.9916483Z Jul 12 04:30:49 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:30:49.9917377Z Jul 12 04:30:49 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:30:49.9918121Z Jul 12 04:30:49 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:30:49.9918788Z Jul 12 04:30:49 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:30:49.9919456Z Jul 12 04:30:49 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:30:49.9920193Z Jul 12 04:30:49 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:30:49.9920923Z Jul 12 04:30:49 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:30:49.9921630Z Jul 12 04:30:49 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:30:49.9922326Z Jul 12 04:30:49 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:30:49.9923023Z Jul 12 04:30:49 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:30:49.9923708Z Jul 12 04:30:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-12T04:30:49.9924449Z Jul 12 04:30:49 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-12T04:30:49.9925124Z Jul 12 04:30:49 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-12T04:30:49.9925912Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:30:49.9926742Z Jul 12 04:30:49 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:30:49.9928142Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:30:49.9928715Z Jul 12 04:30:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:30:49.9929311Z Jul 12 04:30:49 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:30:49.9929863Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:30:49.9930376Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:30:49.9930911Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:30:49.9931441Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:30:49.9931975Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:30:49.9932493Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:30:49.9932966Z Jul 12 04:30:49 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-07-12T04:30:49.9933427Z Jul 12 04:30:49 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-07-12T04:30:49.9933911Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:30:49.9934424Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:30:49.9934951Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:30:49.9935471Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:30:49.9936271Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:30:49.9936826Z Jul 12 04:30:49 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-12T04:30:49.9937342Z Jul 12 04:30:49 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-12T04:30:49.9937849Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:30:49.9938359Z Jul 12 04:30:49 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:30:49.9938847Z Jul 12 04:30:49 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:30:49.9939417Z Jul 12 04:30:49 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:30:49.9939950Z Jul 12 04:30:49 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:30:49.9940572Z Jul 12 04:30:49 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:30:49.9941177Z Jul 12 04:30:49 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:30:49.9941827Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:30:49.9942532Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:30:49.9943244Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:30:49.9943976Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:30:49.9944697Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:30:49.9945354Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:30:49.9945960Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:30:49.9946690Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:30:49.9947389Z Jul 12 04:30:49 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:30:49.9948062Z Jul 12 04:30:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:30:49.9948750Z Jul 12 04:30:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:30:49.9949432Z Jul 12 04:30:49 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:30:49.9950079Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:30:49.9950685Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:30:49.9951268Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:30:49.9951835Z Jul 12 04:30:49 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:30:49.9952411Z Jul 12 04:30:49 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
2022-07-12T04:30:49.9953009Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:549)
2022-07-12T04:30:49.9953687Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2089)
2022-07-12T04:30:49.9954419Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2076)
2022-07-12T04:30:49.9955114Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java:97)
2022-07-12T04:30:49.9955812Z Jul 12 04:30:49 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2158)
2022-07-12T04:30:49.9956463Z Jul 12 04:30:49 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2022-07-12T04:30:49.9957003Z Jul 12 04:30:49 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:30:49.9957622Z Jul 12 04:30:49 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2022-07-12T04:30:49.9958393Z Jul 12 04:30:49 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2022-07-12T04:30:49.9959035Z Jul 12 04:30:49 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:30:49.9959627Z Jul 12 04:30:49 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:30:49.9960134Z Jul 12 04:30:49 	at java.lang.Thread.run(Thread.java:748)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 21:14:10 UTC 2022,,,,,,,,,,"0|z16te8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 02:23;hxbks2ks;Hi [~Yanfei Lei], could you help take a look?;;;","13/Jul/22 03:00;Yanfei Lei;Sure, look like it's caused by checkpoint expired, I will check it.;;;","14/Jul/22 02:33;hxbks2ks;Anther failed instance https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38128&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba.
But the stack is different:

{code:java}
2022-07-13T09:36:20.6676645Z Jul 13 09:36:20 [ERROR] ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode  Time elapsed: 2.207 s  <<< ERROR!
2022-07-13T09:36:20.6677831Z Jul 13 09:36:20 java.util.NoSuchElementException: No value present
2022-07-13T09:36:20.6678727Z Jul 13 09:36:20 	at java.util.Optional.get(Optional.java:135)
2022-07-13T09:36:20.6680252Z Jul 13 09:36:20 	at org.apache.flink.test.checkpointing.ChangelogPeriodicMaterializationSwitchStateBackendITCase.testSwitchFromDisablingToEnablingInClaimMode(ChangelogPeriodicMaterializationSwitchStateBackendITCase.java:115)
2022-07-13T09:36:20.6682394Z Jul 13 09:36:20 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-13T09:36:20.6683581Z Jul 13 09:36:20 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-13T09:36:20.6684792Z Jul 13 09:36:20 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-13T09:36:20.6689681Z Jul 13 09:36:20 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-13T09:36:20.6690994Z Jul 13 09:36:20 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-13T09:36:20.6692214Z Jul 13 09:36:20 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-13T09:36:20.6693463Z Jul 13 09:36:20 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-13T09:36:20.6694894Z Jul 13 09:36:20 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-13T09:36:20.6696003Z Jul 13 09:36:20 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-13T09:36:20.6697015Z Jul 13 09:36:20 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-13T09:36:20.6698018Z Jul 13 09:36:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-13T09:36:20.6699071Z Jul 13 09:36:20 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-13T09:36:20.6700062Z Jul 13 09:36:20 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-13T09:36:20.6701007Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-13T09:36:20.6702219Z Jul 13 09:36:20 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-13T09:36:20.6703433Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-13T09:36:20.6704186Z Jul 13 09:36:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-13T09:36:20.6704928Z Jul 13 09:36:20 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-13T09:36:20.6705598Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-13T09:36:20.6706238Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-13T09:36:20.6706879Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-13T09:36:20.6707523Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-13T09:36:20.6708142Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-13T09:36:20.6708756Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-13T09:36:20.6709351Z Jul 13 09:36:20 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-07-13T09:36:20.6726193Z Jul 13 09:36:20 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-07-13T09:36:20.6727363Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-13T09:36:20.6728407Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-13T09:36:20.6729463Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-13T09:36:20.6730706Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-13T09:36:20.6731855Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-13T09:36:20.6733048Z Jul 13 09:36:20 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-07-13T09:36:20.6734076Z Jul 13 09:36:20 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-07-13T09:36:20.6735256Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-13T09:36:20.6736312Z Jul 13 09:36:20 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-13T09:36:20.6737647Z Jul 13 09:36:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-13T09:36:20.6738575Z Jul 13 09:36:20 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-13T09:36:20.6739609Z Jul 13 09:36:20 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-13T09:36:20.6740819Z Jul 13 09:36:20 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-13T09:36:20.6742147Z Jul 13 09:36:20 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-13T09:36:20.6743586Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-13T09:36:20.6745108Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-13T09:36:20.6746788Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-13T09:36:20.6748256Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-13T09:36:20.6749675Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-13T09:36:20.6750944Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-13T09:36:20.6752243Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-13T09:36:20.6753854Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-13T09:36:20.6755257Z Jul 13 09:36:20 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-13T09:36:20.6756612Z Jul 13 09:36:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-13T09:36:20.6758018Z Jul 13 09:36:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-13T09:36:20.6759384Z Jul 13 09:36:20 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-13T09:36:20.6760670Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-13T09:36:20.6761950Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-13T09:36:20.6763255Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-13T09:36:20.6764355Z Jul 13 09:36:20 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-13T09:36:20.6765151Z Jul 13 09:36:20 
{code}
;;;","18/Jul/22 03:00;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38295&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","18/Jul/22 03:01;hxbks2ks;Hi [~Yanfei Lei] Any updates on the progress?;;;","18/Jul/22 03:30;Yanfei Lei;This is related to the instability of triggering checkpoint, I would open a PR after [https://github.com/apache/flink/pull/19864] megered.;;;","19/Jul/22 02:53;hxbks2ks;Hi [~Yanfei Lei], I have merged the PR https://github.com/apache/flink/pull/19864 . You can go on your work.;;;","21/Jul/22 11:49;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38510&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","29/Jul/22 09:40;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38893&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7;;;","29/Jul/22 09:40;hxbks2ks;Hi  [~Yanfei Lei] Any updates on the progress?;;;","01/Aug/22 08:24;Yanfei Lei;Sorry for the late reply, I refactored the test, and the PR is waiting to be reviewed.;;;","01/Aug/22 11:48;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38962&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","04/Aug/22 06:11;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39237&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","09/Aug/22 02:18;hxbks2ks;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=39675&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a;;;","09/Aug/22 21:14;roman;Merged as c5a8b0f3c294b9e13be4aae0f0f77c64d5405418..6b725d13153491e95a6ffdaa673347d86ef2cd54.;;;",,,,,,
Table.getSchema fails on table with watermark,FLINK-28528,13471307,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hxb,xuannan,xuannan,13/Jul/22 02:19,07/Apr/23 05:23,04/Jun/24 20:42,,1.15.1,,,,,,,,API / Python,,,,,,,0,,,,,,,"The bug can be reproduced with the following test. The test can pass if we use the commented way to define the watermark.
{code:python}
    def test_flink_2(self):
        env = StreamExecutionEnvironment.get_execution_environment()
        t_env = StreamTableEnvironment.create(env)
        table = t_env.from_descriptor(
            TableDescriptor.for_connector(""filesystem"")
            .schema(
                Schema.new_builder()
                .column(""name"", DataTypes.STRING())
                .column(""cost"", DataTypes.INT())
                .column(""distance"", DataTypes.INT())
                .column(""time"", DataTypes.TIMESTAMP(3))
                .watermark(""time"", expr.col(""time"") - expr.lit(60).seconds)
                # .watermark(""time"", ""`time` - INTERVAL '60' SECOND"")
                .build()
            )
            .format(""csv"")
            .option(""path"", ""./input.csv"")
            .build()
        )

        print(table.get_schema())
{code}
It causes the following exception
{code:none}
E       pyflink.util.exceptions.TableException: org.apache.flink.table.api.TableException: Expression 'minus(time, 60000)' is not string serializable. Currently, only expressions that originated from a SQL expression have a well-defined string representation.
E       	at org.apache.flink.table.expressions.ResolvedExpression.asSerializableString(ResolvedExpression.java:51)
E       	at org.apache.flink.table.api.TableSchema.lambda$fromResolvedSchema$13(TableSchema.java:455)
E       	at java.util.Collections$SingletonList.forEach(Collections.java:4824)
E       	at org.apache.flink.table.api.TableSchema.fromResolvedSchema(TableSchema.java:451)
E       	at org.apache.flink.table.api.Table.getSchema(Table.java:101)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E       	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E       	at java.lang.reflect.Method.invoke(Method.java:498)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E       	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
E       	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E       	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
E       	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
E       	at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 07 05:23:33 UTC 2023,,,,,,,,,,"0|z16te0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/23 05:16;dianfu;A similar issue reported in slack channel: https://apache-flink.slack.com/archives/C03G7LJTS2G/p1680795285070859;;;","07/Apr/23 05:20;dianfu;The root cause is that currently table.get_schema() only supports expressions defined via SQL string. For example, the following statements should work:
* .watermark(""time"", ""`time` - INTERVAL '60' SECOND"")
* .column_by_expression('TM', 'parse_bq_datetime_udf(window_start)') ;;;","07/Apr/23 05:23;dianfu;The exception happens when converting TableSchema to ResolvedSchema [1]. To resolve this issue, I guess we may need to support ResolvedSchema in PyFlink instead of TableSchema which is already deprecated in Java.

[1] https://github.com/apache/flink/blob/b30b12d97a00ee5a338b2bd4233df1e2b38ce87f/flink-table/flink-table-common/src/main/java/org/apache/flink/table/api/TableSchema.java#L430;;;",,,,,,,,,,,,,,,,,,
Fail to lateral join with UDTF from Table with timstamp column,FLINK-28527,13471305,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xuannan,xuannan,13/Jul/22 02:15,13/Jul/22 02:27,04/Jun/24 20:42,13/Jul/22 02:27,1.15.1,,,,,,,,API / Python,,,,,,,0,,,,,,,"The bug can be reproduced with the following test


{code:python}
    def test_flink(self):
        env = StreamExecutionEnvironment.get_execution_environment()
        t_env = StreamTableEnvironment.create(env)
        table = t_env.from_descriptor(
            TableDescriptor.for_connector(""filesystem"")
            .schema(
                Schema.new_builder()
                .column(""name"", DataTypes.STRING())
                .column(""cost"", DataTypes.INT())
                .column(""distance"", DataTypes.INT())
                .column(""time"", DataTypes.TIMESTAMP(3))
                .watermark(""time"", ""`time` - INTERVAL '60' SECOND"")
                .build()
            )
            .format(""csv"")
            .option(""path"", ""./input.csv"")
            .build()
        )

        @udtf(result_types=DataTypes.INT())
        def table_func(row: Row):
            return row.cost + row.distance

        table = table.join_lateral(table_func.alias(""cost_times_distance""))

        table.execute().print()
{code}

It causes the following exception


{code:none}
E       pyflink.util.exceptions.TableException: org.apache.flink.table.api.TableException: Unsupported Python SqlFunction CAST.
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:146)
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:429)
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:135)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.extractPythonTableFunctionInfo(CommonExecPythonCorrelate.java:133)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.createPythonOneInputTransformation(CommonExecPythonCorrelate.java:106)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.translateToPlanInternal(CommonExecPythonCorrelate.java:95)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
E       	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:136)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
E       	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:79)
E       	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
E       	at scala.collection.Iterator.foreach(Iterator.scala:937)
E       	at scala.collection.Iterator.foreach$(Iterator.scala:937)
E       	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
E       	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
E       	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
E       	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
E       	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
E       	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
E       	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
E       	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:78)
E       	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
E       	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:605)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E       	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E       	at java.lang.reflect.Method.invoke(Method.java:498)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E       	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
E       	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E       	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
E       	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
E       	at java.lang.Thread.run(Thread.java:748)
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-13 02:15:46.0,,,,,,,,,,"0|z16tdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to lateral join with UDTF from Table with timstamp column,FLINK-28526,13471304,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,13/Jul/22 02:15,17/Jan/23 15:06,04/Jun/24 20:42,17/Jan/23 15:06,1.15.1,,,,1.15.4,1.16.1,1.17.0,,API / Python,,,,,,,0,pull-request-available,,,,,,"The bug can be reproduced with the following test


{code:python}
    def test_flink(self):
        env = StreamExecutionEnvironment.get_execution_environment()
        t_env = StreamTableEnvironment.create(env)
        table = t_env.from_descriptor(
            TableDescriptor.for_connector(""filesystem"")
            .schema(
                Schema.new_builder()
                .column(""name"", DataTypes.STRING())
                .column(""cost"", DataTypes.INT())
                .column(""distance"", DataTypes.INT())
                .column(""time"", DataTypes.TIMESTAMP(3))
                .watermark(""time"", ""`time` - INTERVAL '60' SECOND"")
                .build()
            )
            .format(""csv"")
            .option(""path"", ""./input.csv"")
            .build()
        )

        @udtf(result_types=DataTypes.INT())
        def table_func(row: Row):
            return row.cost + row.distance

        table = table.join_lateral(table_func.alias(""cost_times_distance""))

        table.execute().print()
{code}

It causes the following exception


{code:none}
E       pyflink.util.exceptions.TableException: org.apache.flink.table.api.TableException: Unsupported Python SqlFunction CAST.
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:146)
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:429)
E       	at org.apache.flink.table.planner.plan.nodes.exec.utils.CommonPythonUtil.createPythonFunctionInfo(CommonPythonUtil.java:135)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.extractPythonTableFunctionInfo(CommonExecPythonCorrelate.java:133)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.createPythonOneInputTransformation(CommonExecPythonCorrelate.java:106)
E       	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecPythonCorrelate.translateToPlanInternal(CommonExecPythonCorrelate.java:95)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
E       	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:136)
E       	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
E       	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:79)
E       	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
E       	at scala.collection.Iterator.foreach(Iterator.scala:937)
E       	at scala.collection.Iterator.foreach$(Iterator.scala:937)
E       	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
E       	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
E       	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
E       	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
E       	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
E       	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
E       	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
E       	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:78)
E       	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:181)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1656)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:828)
E       	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1317)
E       	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:605)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E       	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E       	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E       	at java.lang.reflect.Method.invoke(Method.java:498)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
E       	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
E       	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
E       	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
E       	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
E       	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
E       	at java.lang.Thread.run(Thread.java:748)
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 17 15:06:00 UTC 2023,,,,,,,,,,"0|z16tdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jul/22 03:46;dianfu;cc [~hxbks2ks] Could you help to take a look at this issue?;;;","17/Jan/23 15:06;dianfu;Fixed in:
- master via d8417565d6fb7f907f54eeabb8a53ebf790ffad8
- release-1.16 via 4150d709906956044928df1c56001eaa0d81188b
- release-1.15 via 4c7001c0dcff400ff2824bf3e75df184c686f2cd;;;",,,,,,,,,,,,,,,,,,,
HiveDialectITCase.testTableWithSubDirsInPartitionDir failed with AssertJMultipleFailuresError,FLINK-28525,13471303,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,luoyuxia,hxbks2ks,hxbks2ks,13/Jul/22 02:14,14/Jul/22 02:51,04/Jun/24 20:42,14/Jul/22 02:16,1.16.0,,,,1.16.0,,,,Connectors / Hive,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-07-12T04:20:27.9597185Z Jul 12 04:20:27 [ERROR] org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir  Time elapsed: 16.31 s  <<< FAILURE!
2022-07-12T04:20:27.9598238Z Jul 12 04:20:27 org.assertj.core.error.AssertJMultipleFailuresError: 
2022-07-12T04:20:27.9598978Z Jul 12 04:20:27 
2022-07-12T04:20:27.9599419Z Jul 12 04:20:27 Multiple Failures (1 failure)
2022-07-12T04:20:27.9600269Z Jul 12 04:20:27 -- failure 1 --
2022-07-12T04:20:27.9601093Z Jul 12 04:20:27 [Any cause contains message 'Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2'] 
2022-07-12T04:20:27.9602679Z Jul 12 04:20:27 Expecting any element of:
2022-07-12T04:20:27.9603145Z Jul 12 04:20:27   [org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9603766Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:20:27.9604403Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9605039Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9605902Z Jul 12 04:20:27 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:20:27.9606408Z Jul 12 04:20:27     java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9606952Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9607777Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9608438Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9609261Z Jul 12 04:20:27 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:20:27.9609899Z Jul 12 04:20:27     java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9610503Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9610998Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9611574Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9612390Z Jul 12 04:20:27 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:20:27.9612953Z Jul 12 04:20:27     java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9613522Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:27.9614119Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:27.9614731Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:27.9615535Z Jul 12 04:20:27 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)]
2022-07-12T04:20:27.9616042Z Jul 12 04:20:27 to satisfy the given assertions requirements but none did:
2022-07-12T04:20:27.9616381Z Jul 12 04:20:27 
2022-07-12T04:20:27.9616795Z Jul 12 04:20:27 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9617407Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:20:27.9618034Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9618650Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9619455Z Jul 12 04:20:27 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:27.9619961Z Jul 12 04:20:27 error: 
2022-07-12T04:20:27.9620268Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:27.9620642Z Jul 12 04:20:27   ""java.io.IOException: Fail to create input splits.""
2022-07-12T04:20:27.9620989Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:27.9621387Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:27.9621777Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:27.9622042Z Jul 12 04:20:27 
2022-07-12T04:20:27.9622345Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:27.9622644Z Jul 12 04:20:27 
2022-07-12T04:20:27.9623052Z Jul 12 04:20:27 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9623647Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:20:27.9624275Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9624882Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9625599Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:20:27.9626485Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:20:27.9627210Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9627963Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9628640Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:20:27.9629328Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9629957Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9630612Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:20:27.9631296Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9631965Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:20:27.9632584Z Jul 12 04:20:27 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:20:27.9633096Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:20:27.9633587Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:20:27.9634077Z Jul 12 04:20:27 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:20:27.9634589Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:20:27.9635099Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:20:27.9635608Z Jul 12 04:20:27 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:20:27.9636121Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:20:27.9636639Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:20:27.9637466Z Jul 12 04:20:27 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:20:27.9638064Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:20:27.9638770Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:20:27.9639397Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:20:27.9640061Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:20:27.9640741Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:20:27.9641395Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:20:27.9642086Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:20:27.9642733Z Jul 12 04:20:27 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:20:27.9643331Z Jul 12 04:20:27 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:20:27.9643915Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:20:27.9644449Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:20:27.9645140Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:20:27.9645726Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:27.9646233Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:27.9646817Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:27.9647352Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:27.9647880Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:27.9648472Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:27.9649053Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:27.9649639Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:27.9650200Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:27.9650750Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:27.9651289Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9651847Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:27.9652401Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:27.9652948Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:27.9653527Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:27.9654071Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:27.9654580Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:27.9655098Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:27.9655614Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:27.9656131Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:27.9656712Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9657210Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:27.9722967Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:27.9723517Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:27.9724476Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:27.9725099Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:27.9725699Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:27.9726331Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:27.9727009Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:27.9727704Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:27.9728566Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:27.9729451Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:27.9730088Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:27.9730690Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:27.9731347Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:27.9732038Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:27.9732705Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:27.9733390Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:27.9734061Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:27.9734703Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:27.9735293Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:27.9735860Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:27.9736414Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:27.9736910Z Jul 12 04:20:27 Caused by: java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9737458Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9738173Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9738855Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9739527Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:20:27.9740200Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:20:27.9740672Z Jul 12 04:20:27 	... 77 more
2022-07-12T04:20:27.9741253Z Jul 12 04:20:27 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9741862Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9742357Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9742928Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9743400Z Jul 12 04:20:27 	... 81 more
2022-07-12T04:20:27.9743836Z Jul 12 04:20:27 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9744404Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:27.9744989Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:27.9745591Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:27.9746145Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:27.9746675Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:27.9747316Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:27.9747803Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:27.9748139Z Jul 12 04:20:27 
2022-07-12T04:20:27.9748386Z Jul 12 04:20:27 
2022-07-12T04:20:27.9748715Z Jul 12 04:20:27 java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9749249Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9749929Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9750603Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9751758Z Jul 12 04:20:27 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:27.9752175Z Jul 12 04:20:27 error: 
2022-07-12T04:20:27.9752495Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:27.9752841Z Jul 12 04:20:27   ""Fail to create input splits.""
2022-07-12T04:20:27.9753166Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:27.9753564Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:27.9753955Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:27.9754216Z Jul 12 04:20:27 
2022-07-12T04:20:27.9754524Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:27.9754828Z Jul 12 04:20:27 
2022-07-12T04:20:27.9755164Z Jul 12 04:20:27 java.io.IOException: Fail to create input splits.
2022-07-12T04:20:27.9755709Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:20:27.9756384Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9757050Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9757908Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:20:27.9758572Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:20:27.9759203Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9759822Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9760640Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:20:27.9761443Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:20:27.9762173Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9762815Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9763491Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:20:27.9764166Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9764806Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9765472Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:20:27.9766148Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9766881Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:20:27.9767512Z Jul 12 04:20:27 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:20:27.9768037Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:20:27.9768523Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:20:27.9769024Z Jul 12 04:20:27 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:20:27.9769542Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:20:27.9770044Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:20:27.9770553Z Jul 12 04:20:27 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:20:27.9771063Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:20:27.9771580Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:20:27.9772093Z Jul 12 04:20:27 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:20:27.9772667Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:20:27.9773281Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:20:27.9773908Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:20:27.9774579Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:20:27.9775254Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:20:27.9775912Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:20:27.9776617Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:20:27.9777275Z Jul 12 04:20:27 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:20:27.9777862Z Jul 12 04:20:27 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:20:27.9778525Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:20:27.9779072Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:20:27.9779706Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:20:27.9780296Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:27.9780807Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:27.9781391Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:27.9781907Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:27.9782524Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:27.9783120Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:27.9783704Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:27.9784279Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:27.9784944Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:27.9785490Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:27.9786026Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9786585Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:27.9787132Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:27.9787690Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:27.9788275Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:27.9788816Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:27.9789316Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:27.9789838Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:27.9790363Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:27.9790877Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:27.9791399Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9791901Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:27.9792377Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:27.9792839Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:27.9793360Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:27.9793956Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:27.9794543Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:27.9795168Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:27.9795836Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:27.9796519Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:27.9797397Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:27.9798108Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:27.9798739Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:27.9799324Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:27.9799964Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:27.9800645Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:27.9801294Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:27.9801978Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:27.9802649Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:27.9803369Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:27.9803953Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:27.9804509Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:27.9805056Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:27.9805680Z Jul 12 04:20:27 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9806293Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9806789Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9807358Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9807830Z Jul 12 04:20:27 	... 81 more
2022-07-12T04:20:27.9808264Z Jul 12 04:20:27 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9808826Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:27.9809412Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:27.9810020Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:27.9810574Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:27.9811101Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:27.9811668Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:27.9812156Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:27.9812487Z Jul 12 04:20:27 
2022-07-12T04:20:27.9812743Z Jul 12 04:20:27 
2022-07-12T04:20:27.9813208Z Jul 12 04:20:27 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9813791Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9814285Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9814846Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9815836Z Jul 12 04:20:27 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:27.9816265Z Jul 12 04:20:27 error: 
2022-07-12T04:20:27.9816580Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:27.9817029Z Jul 12 04:20:27   ""java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:20:27.9817457Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:27.9817840Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:27.9818229Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:27.9818500Z Jul 12 04:20:27 
2022-07-12T04:20:27.9818802Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:27.9819103Z Jul 12 04:20:27 
2022-07-12T04:20:27.9819563Z Jul 12 04:20:27 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9820148Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:20:27.9820648Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:20:27.9821221Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:20:27.9821978Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:20:27.9822648Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:20:27.9823329Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:20:27.9823988Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:20:27.9824614Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:20:27.9825236Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:20:27.9825950Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:20:27.9826762Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:20:27.9827488Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9828118Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9828783Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:20:27.9829448Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9830080Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:20:27.9830742Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:20:27.9831416Z Jul 12 04:20:27 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:20:27.9832074Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:20:27.9832689Z Jul 12 04:20:27 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:20:27.9833281Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:20:27.9833749Z Jul 12 04:20:27 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:20:27.9834242Z Jul 12 04:20:27 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:20:27.9834755Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:20:27.9835265Z Jul 12 04:20:27 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:20:27.9835774Z Jul 12 04:20:27 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:20:27.9836281Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:20:27.9836791Z Jul 12 04:20:27 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:20:27.9837308Z Jul 12 04:20:27 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:20:27.9878683Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:20:27.9879361Z Jul 12 04:20:27 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:20:27.9880007Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:20:27.9880992Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:20:27.9881689Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:20:27.9882343Z Jul 12 04:20:27 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:20:27.9883042Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:20:27.9883708Z Jul 12 04:20:27 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:20:27.9884312Z Jul 12 04:20:27 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:20:27.9884898Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:20:27.9885452Z Jul 12 04:20:27 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:20:27.9886078Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:20:27.9886667Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:27.9887185Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:27.9887783Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:27.9888328Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:27.9888861Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:27.9889461Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:27.9890038Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:27.9890627Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:27.9891205Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:27.9891767Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:27.9892302Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9892954Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:27.9893499Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:27.9894038Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:27.9894619Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:27.9895166Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:27.9895671Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:27.9896190Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:27.9896702Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:27.9897205Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:27.9897720Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:27.9898220Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:27.9898697Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:27.9899162Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:27.9899737Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:27.9900321Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:27.9900916Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:27.9901535Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:27.9902218Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:27.9902915Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:27.9903631Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:27.9904328Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:27.9904956Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:27.9905537Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:27.9906176Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:27.9906872Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:27.9907535Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:27.9908206Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:27.9908884Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:27.9909506Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:27.9934332Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:27.9935034Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:27.9935850Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:27.9936432Z Jul 12 04:20:27 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:27.9937008Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:28.7230243Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:28.7230961Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:28.7231529Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:28.7232062Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:28.7232641Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:28.7233171Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:28.7233510Z Jul 12 04:20:27 
2022-07-12T04:20:28.7233772Z Jul 12 04:20:27 
2022-07-12T04:20:28.7234182Z Jul 12 04:20:27 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:28.7234741Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:28.7235657Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:28.7236274Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:28.7237640Z Jul 12 04:20:27 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:20:28.7238085Z Jul 12 04:20:27 error: 
2022-07-12T04:20:28.7238400Z Jul 12 04:20:27 Expecting throwable message:
2022-07-12T04:20:28.7238827Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:20:28.7239227Z Jul 12 04:20:27 to contain:
2022-07-12T04:20:28.7239617Z Jul 12 04:20:27   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:20:28.7240010Z Jul 12 04:20:27 but did not.
2022-07-12T04:20:28.7240288Z Jul 12 04:20:27 
2022-07-12T04:20:28.7240585Z Jul 12 04:20:27 Throwable that failed the check:
2022-07-12T04:20:28.7240889Z Jul 12 04:20:27 
2022-07-12T04:20:28.7241300Z Jul 12 04:20:27 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:20:28.7241859Z Jul 12 04:20:27 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:20:28.7242447Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:20:28.7243063Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:20:28.7243611Z Jul 12 04:20:27 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:20:28.7244140Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:20:28.7244726Z Jul 12 04:20:27 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:20:28.7245219Z Jul 12 04:20:27 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:20:28.7245692Z Jul 12 04:20:27 at FlinkAssertions.lambda$anyCauseMatches$5(FlinkAssertions.java:106)
2022-07-12T04:20:28.7246209Z Jul 12 04:20:27 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2022-07-12T04:20:28.7246769Z Jul 12 04:20:27 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2022-07-12T04:20:28.7247411Z Jul 12 04:20:27 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2022-07-12T04:20:28.7248230Z Jul 12 04:20:27 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:597)
2022-07-12T04:20:28.7248805Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:20:28.7249320Z Jul 12 04:20:27 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:20:28.7249915Z Jul 12 04:20:27 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:20:28.7250452Z Jul 12 04:20:27 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:20:28.7250974Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:20:28.7251564Z Jul 12 04:20:27 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:20:28.7252146Z Jul 12 04:20:27 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:20:28.7252732Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:20:28.7253299Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:20:28.7253855Z Jul 12 04:20:27 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:20:28.7254446Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:28.7255001Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:20:28.7255548Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:20:28.7256098Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:20:28.7256673Z Jul 12 04:20:27 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:20:28.7257223Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:20:28.7257726Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:20:28.7258233Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:20:28.7258749Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:20:28.7259266Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:20:28.7259785Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:20:28.7260282Z Jul 12 04:20:27 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:20:28.7260767Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:20:28.7261220Z Jul 12 04:20:27 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:20:28.7261749Z Jul 12 04:20:27 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:20:28.7262348Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:20:28.7262934Z Jul 12 04:20:27 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:20:28.7263557Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:20:28.7264243Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:20:28.7264934Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:20:28.7265650Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:20:28.7266415Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:20:28.7267051Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:20:28.7267638Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:20:28.7268289Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:20:28.7268973Z Jul 12 04:20:27 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:20:28.7269643Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:20:28.7270312Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:20:28.7270981Z Jul 12 04:20:27 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:20:28.7271612Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:20:28.7272191Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:20:28.7272809Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:20:28.7273356Z Jul 12 04:20:27 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:20:28.7273758Z Jul 12 04:20:27 
2022-07-12T04:20:30.2231809Z Jul 12 04:20:29 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-07-12T04:20:31.4694903Z Jul 12 04:20:31 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
2022-07-12T04:21:11.2730557Z Jul 12 04:21:11 [WARNING] Tests run: 17, Failures: 0, Errors: 0, Skipped: 3, Time elapsed: 168.97 s - in org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase
2022-07-12T04:21:12.4779976Z Jul 12 04:21:12 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2022-07-12T04:21:14.4325471Z Jul 12 04:21:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase
2022-07-12T04:21:37.8028634Z Jul 12 04:21:37 [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 66.271 s - in org.apache.flink.table.catalog.hive.HiveCatalogITCase
2022-07-12T04:21:44.7479437Z Jul 12 04:21:44 [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.298 s - in org.apache.flink.table.catalog.hive.HiveCatalogUdfITCase
2022-07-12T04:21:45.1103445Z Jul 12 04:21:45 [INFO] 
2022-07-12T04:21:45.1104396Z Jul 12 04:21:45 [INFO] Results:
2022-07-12T04:21:45.1104941Z Jul 12 04:21:45 [INFO] 
2022-07-12T04:21:45.1109327Z Jul 12 04:21:45 [ERROR] Failures: 
2022-07-12T04:21:45.1114586Z Jul 12 04:21:45 [ERROR]   HiveDialectITCase.testTableWithSubDirsInPartitionDir:597 
2022-07-12T04:21:45.1115440Z Jul 12 04:21:45 Multiple Failures (1 failure)
2022-07-12T04:21:45.1116520Z Jul 12 04:21:45 -- failure 1 --
2022-07-12T04:21:45.1117948Z Jul 12 04:21:45 [Any cause contains message 'Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2'] 
2022-07-12T04:21:45.1164459Z Jul 12 04:21:45 Expecting any element of:
2022-07-12T04:21:45.1165322Z Jul 12 04:21:45   [org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1169910Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:21:45.1171238Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1172885Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1174617Z Jul 12 04:21:45 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:21:45.1175568Z Jul 12 04:21:45     java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1176551Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1177691Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1179025Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1180609Z Jul 12 04:21:45 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:21:45.1181829Z Jul 12 04:21:45     java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1183614Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1184567Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1185713Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1187373Z Jul 12 04:21:45 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
2022-07-12T04:21:45.1188746Z Jul 12 04:21:45     java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1189885Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1191052Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1192243Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1193984Z Jul 12 04:21:45 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)]
2022-07-12T04:21:45.1194951Z Jul 12 04:21:45 to satisfy the given assertions requirements but none did:
2022-07-12T04:21:45.1195546Z Jul 12 04:21:45 
2022-07-12T04:21:45.1196295Z Jul 12 04:21:45 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1197109Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:21:45.1206332Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1207287Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1208609Z Jul 12 04:21:45 	...(75 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1209119Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1209475Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1209894Z Jul 12 04:21:45   ""java.io.IOException: Fail to create input splits.""
2022-07-12T04:21:45.1210290Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1210719Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1211164Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1211465Z Jul 12 04:21:45 
2022-07-12T04:21:45.1211803Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1214141Z Jul 12 04:21:45 
2022-07-12T04:21:45.1214598Z Jul 12 04:21:45 org.apache.flink.connectors.hive.FlinkHiveException: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1215300Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:98)
2022-07-12T04:21:45.1216027Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1216931Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1217774Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:21:45.1218721Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:21:45.1220057Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1220779Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1221550Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:21:45.1222353Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1223080Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1223845Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:21:45.1224716Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1225484Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:21:45.1226184Z Jul 12 04:21:45 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:21:45.1226770Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:21:45.1227321Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:21:45.1227885Z Jul 12 04:21:45 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:21:45.1228466Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:21:45.1229041Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:21:45.1229612Z Jul 12 04:21:45 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:21:45.1230191Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:21:45.1230779Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:21:45.1231364Z Jul 12 04:21:45 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:21:45.1232013Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:21:45.1232725Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:21:45.1233441Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:21:45.1234205Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:21:45.1234997Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:21:45.1235754Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:21:45.1236549Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:21:45.1237303Z Jul 12 04:21:45 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:21:45.1238162Z Jul 12 04:21:45 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:21:45.1238831Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:21:45.1239441Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:21:45.1240167Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:21:45.1240837Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:21:45.1241419Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:21:45.1242098Z Jul 12 04:21:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:21:45.1242717Z Jul 12 04:21:45 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:21:45.1243315Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:21:45.1243995Z Jul 12 04:21:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:21:45.1244672Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:21:45.1245398Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:21:45.1246054Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:21:45.1246696Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:21:45.1247309Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1247934Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:21:45.1248573Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:21:45.1249199Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:21:45.1249872Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:21:45.1250496Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:21:45.1251073Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:21:45.1251651Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:21:45.1252239Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:21:45.1252824Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:21:45.1253411Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1253983Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:21:45.1254523Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:21:45.1255038Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:21:45.1255633Z Jul 12 04:21:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:21:45.1256329Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:21:45.1257001Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:21:45.1257716Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:21:45.1258497Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:21:45.1259355Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:21:45.1260182Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:21:45.1261005Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:21:45.1261740Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:21:45.1262408Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:21:45.1263151Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:21:45.1263944Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:21:45.1264708Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:21:45.1265478Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:21:45.1266304Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:21:45.1267029Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:21:45.1267701Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:21:45.1268333Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:21:45.1268960Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:21:45.1269520Z Jul 12 04:21:45 Caused by: java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1270134Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1270913Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1271683Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1272453Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:21:45.1273222Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:21:45.1273756Z Jul 12 04:21:45 	... 77 more
2022-07-12T04:21:45.1274313Z Jul 12 04:21:45 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1274998Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1275556Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1276205Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1276748Z Jul 12 04:21:45 	... 81 more
2022-07-12T04:21:45.1277237Z Jul 12 04:21:45 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1280605Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1281285Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1281989Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1282727Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1283330Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1283990Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1284555Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1284930Z Jul 12 04:21:45 
2022-07-12T04:21:45.1285205Z Jul 12 04:21:45 
2022-07-12T04:21:45.1285569Z Jul 12 04:21:45 java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1286176Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1286954Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1287727Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1288795Z Jul 12 04:21:45 	...(79 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1289279Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1289625Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1290077Z Jul 12 04:21:45   ""Fail to create input splits.""
2022-07-12T04:21:45.1290430Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1297758Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1298208Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1298510Z Jul 12 04:21:45 
2022-07-12T04:21:45.1298845Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1299180Z Jul 12 04:21:45 
2022-07-12T04:21:45.1299551Z Jul 12 04:21:45 java.io.IOException: Fail to create input splits.
2022-07-12T04:21:45.1300164Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:84)
2022-07-12T04:21:45.1300955Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1301855Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1302639Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:21:45.1303495Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:21:45.1304232Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1304952Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1305863Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:21:45.1306812Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:21:45.1307747Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1308485Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1309343Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:21:45.1310135Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1310872Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1311875Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:21:45.1312643Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1313493Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:21:45.1314287Z Jul 12 04:21:45 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:21:45.1314882Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:21:45.1315530Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:21:45.1316104Z Jul 12 04:21:45 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:21:45.1316719Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:21:45.1317298Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:21:45.1318086Z Jul 12 04:21:45 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:21:45.1318676Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:21:45.1319352Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:21:45.1320028Z Jul 12 04:21:45 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:21:45.1320675Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:21:45.1321393Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:21:45.1322201Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:21:45.1322986Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:21:45.1323783Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:21:45.1324543Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:21:45.1326914Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:21:45.1333950Z Jul 12 04:21:45 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:21:45.1334661Z Jul 12 04:21:45 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:21:45.1335343Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:21:45.1335981Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:21:45.1336712Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:21:45.1337873Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:21:45.1338473Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:21:45.1339141Z Jul 12 04:21:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:21:45.1339753Z Jul 12 04:21:45 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:21:45.1340457Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:21:45.1341151Z Jul 12 04:21:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:21:45.1341936Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:21:45.1342615Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:21:45.1343275Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:21:45.1343911Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:21:45.1344655Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1345791Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:21:45.1348562Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:21:45.1349893Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:21:45.1350958Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:21:45.1351785Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:21:45.1352945Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:21:45.1353939Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:21:45.1354928Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:21:45.1355531Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:21:45.1356127Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1356688Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:21:45.1357231Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:21:45.1357933Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:21:45.1358645Z Jul 12 04:21:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:21:45.1359775Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:21:45.1361251Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:21:45.1362006Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:21:45.1363351Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:21:45.1364799Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:21:45.1366301Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:21:45.1367158Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:21:45.1368527Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:21:45.1369798Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:21:45.1370757Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:21:45.1371989Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:21:45.1373338Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:21:45.1374888Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:21:45.1375890Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:21:45.1377089Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:21:45.1378344Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:21:45.1379375Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:21:45.1380320Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:21:45.1381577Z Jul 12 04:21:45 Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1382937Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1383883Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1384769Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1385323Z Jul 12 04:21:45 	... 81 more
2022-07-12T04:21:45.1385899Z Jul 12 04:21:45 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1386561Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1387241Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1387947Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1388578Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1389191Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1389852Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1390975Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1391385Z Jul 12 04:21:45 
2022-07-12T04:21:45.1391680Z Jul 12 04:21:45 
2022-07-12T04:21:45.1392211Z Jul 12 04:21:45 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1395911Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1396951Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1398285Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1400980Z Jul 12 04:21:45 	...(81 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1402240Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1403021Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1404120Z Jul 12 04:21:45   ""java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:21:45.1404837Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1405642Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1410624Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1411207Z Jul 12 04:21:45 
2022-07-12T04:21:45.1411752Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1412578Z Jul 12 04:21:45 
2022-07-12T04:21:45.1413115Z Jul 12 04:21:45 java.util.concurrent.ExecutionException: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1414559Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
2022-07-12T04:21:45.1415593Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
2022-07-12T04:21:45.1416710Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter.getHiveTablePartitionMRSplits(MRSplitsGetter.java:79)
2022-07-12T04:21:45.1417988Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveSourceFileEnumerator.createInputSplits(HiveSourceFileEnumerator.java:69)
2022-07-12T04:21:45.1419359Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.lambda$getDataStream$1(HiveTableSource.java:158)
2022-07-12T04:21:45.1420383Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.logRunningTime(HiveParallelismInference.java:107)
2022-07-12T04:21:45.1421618Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveParallelismInference.infer(HiveParallelismInference.java:95)
2022-07-12T04:21:45.1422780Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource.getDataStream(HiveTableSource.java:153)
2022-07-12T04:21:45.1423885Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveTableSource$1.produceDataStream(HiveTableSource.java:117)
2022-07-12T04:21:45.1425187Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:139)
2022-07-12T04:21:45.1426702Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecTableSourceScan.translateToPlanInternal(BatchExecTableSourceScan.java:63)
2022-07-12T04:21:45.1427775Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1429100Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1430140Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
2022-07-12T04:21:45.1431275Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1432271Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:249)
2022-07-12T04:21:45.1433302Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.batch.BatchExecSink.translateToPlanInternal(BatchExecSink.java:65)
2022-07-12T04:21:45.1434569Z Jul 12 04:21:45 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:148)
2022-07-12T04:21:45.1435867Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.$anonfun$translateToPlan$1(BatchPlanner.scala:88)
2022-07-12T04:21:45.1436847Z Jul 12 04:21:45 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
2022-07-12T04:21:45.1450452Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach(Iterator.scala:937)
2022-07-12T04:21:45.1456329Z Jul 12 04:21:45 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2022-07-12T04:21:45.1457063Z Jul 12 04:21:45 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2022-07-12T04:21:45.1457657Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2022-07-12T04:21:45.1458245Z Jul 12 04:21:45 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2022-07-12T04:21:45.1458840Z Jul 12 04:21:45 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2022-07-12T04:21:45.1459409Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
2022-07-12T04:21:45.1460006Z Jul 12 04:21:45 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
2022-07-12T04:21:45.1460602Z Jul 12 04:21:45 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2022-07-12T04:21:45.1462100Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.BatchPlanner.translateToPlan(BatchPlanner.scala:87)
2022-07-12T04:21:45.1466655Z Jul 12 04:21:45 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2022-07-12T04:21:45.1467395Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1651)
2022-07-12T04:21:45.1468177Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:823)
2022-07-12T04:21:45.1469061Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1312)
2022-07-12T04:21:45.1469831Z Jul 12 04:21:45 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:696)
2022-07-12T04:21:45.1472878Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.lambda$testTableWithSubDirsInPartitionDir$0(HiveDialectITCase.java:595)
2022-07-12T04:21:45.1477862Z Jul 12 04:21:45 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
2022-07-12T04:21:45.1478585Z Jul 12 04:21:45 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
2022-07-12T04:21:45.1479268Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
2022-07-12T04:21:45.1479897Z Jul 12 04:21:45 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
2022-07-12T04:21:45.1480748Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.HiveDialectITCase.testTableWithSubDirsInPartitionDir(HiveDialectITCase.java:592)
2022-07-12T04:21:45.1481429Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T04:21:45.1482018Z Jul 12 04:21:45 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T04:21:45.1482702Z Jul 12 04:21:45 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T04:21:45.1483318Z Jul 12 04:21:45 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T04:21:45.1483924Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T04:21:45.1484607Z Jul 12 04:21:45 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T04:21:45.1485272Z Jul 12 04:21:45 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T04:21:45.1485948Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T04:21:45.1486608Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-07-12T04:21:45.1487262Z Jul 12 04:21:45 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-07-12T04:21:45.1487879Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1488520Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T04:21:45.1489142Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T04:21:45.1489773Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T04:21:45.1490449Z Jul 12 04:21:45 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T04:21:45.1491075Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T04:21:45.1491654Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T04:21:45.1492245Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T04:21:45.1492826Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T04:21:45.1495285Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T04:21:45.1497986Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T04:21:45.1500206Z Jul 12 04:21:45 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T04:21:45.1500764Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T04:21:45.1501293Z Jul 12 04:21:45 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T04:21:45.1508220Z Jul 12 04:21:45 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T04:21:45.1509438Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T04:21:45.1511339Z Jul 12 04:21:45 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T04:21:45.1558574Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T04:21:45.1559403Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T04:21:45.1560236Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T04:21:45.1561086Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T04:21:45.1562106Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T04:21:45.1562844Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T04:21:45.1563528Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T04:21:45.1564281Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T04:21:45.1565084Z Jul 12 04:21:45 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T04:21:45.1565859Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T04:21:45.1566830Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T04:21:45.1567622Z Jul 12 04:21:45 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T04:21:45.1568344Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T04:21:45.1569023Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T04:21:45.1569670Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T04:21:45.1573398Z Jul 12 04:21:45 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-07-12T04:21:45.1574129Z Jul 12 04:21:45 Caused by: java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1574716Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1575306Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1575932Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1576491Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1577041Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1577624Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1578123Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1578660Z Jul 12 04:21:45 
2022-07-12T04:21:45.1578911Z Jul 12 04:21:45 
2022-07-12T04:21:45.1579329Z Jul 12 04:21:45 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1579909Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1580511Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1581135Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1582315Z Jul 12 04:21:45 	...(4 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
2022-07-12T04:21:45.1582895Z Jul 12 04:21:45 error: 
2022-07-12T04:21:45.1583204Z Jul 12 04:21:45 Expecting throwable message:
2022-07-12T04:21:45.1583637Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1""
2022-07-12T04:21:45.1584056Z Jul 12 04:21:45 to contain:
2022-07-12T04:21:45.1584453Z Jul 12 04:21:45   ""Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=2""
2022-07-12T04:21:45.1584858Z Jul 12 04:21:45 but did not.
2022-07-12T04:21:45.1585141Z Jul 12 04:21:45 
2022-07-12T04:21:45.1585434Z Jul 12 04:21:45 Throwable that failed the check:
2022-07-12T04:21:45.1585840Z Jul 12 04:21:45 
2022-07-12T04:21:45.1586246Z Jul 12 04:21:45 java.io.IOException: Not a file: file:/tmp/junit297605250552556431/hive_warehouse/fact_tz/ds=1/hr=1
2022-07-12T04:21:45.1586810Z Jul 12 04:21:45 	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:322)
2022-07-12T04:21:45.1587413Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:134)
2022-07-12T04:21:45.1588028Z Jul 12 04:21:45 	at org.apache.flink.connectors.hive.MRSplitsGetter$MRSplitter.call(MRSplitsGetter.java:96)
2022-07-12T04:21:45.1588573Z Jul 12 04:21:45 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-07-12T04:21:45.1589125Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-07-12T04:21:45.1589710Z Jul 12 04:21:45 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-07-12T04:21:45.1590211Z Jul 12 04:21:45 	at java.lang.Thread.run(Thread.java:748)
2022-07-12T04:21:45.1590705Z Jul 12 04:21:45 at FlinkAssertions.lambda$anyCauseMatches$5(FlinkAssertions.java:106)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 02:16:51 UTC 2022,,,,,,,,,,"0|z16td4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 02:14;hxbks2ks;Hi [~luoyuxia] , could you help take a look?;;;","13/Jul/22 02:36;luoyuxia;Thanks for reporting it. I'll fix it as soon as possible.

The reason is it will read any path first, but the test assumes it'll always read the specific path first, thus cause the unstable test.;;;","13/Jul/22 02:37;hxbks2ks;Thanks for the quick fix.;;;","14/Jul/22 02:16;hxbks2ks;Merged into master via f23ae51fa4f28d7ca4d8c194ac6fc1a05fb5515c;;;",,,,,,,,,,,,,,,,,
"Translate ""User-defined Sources & Sinks""",FLINK-28524,13471302,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,hunterLiu,hunterLiu,13/Jul/22 02:06,13/Jul/22 02:35,04/Jun/24 20:42,13/Jul/22 02:35,,,,,,,,,,,,,,,,0,,,,,,,"The file is docs/content.zh/docs/dev/table/sourcesSinks.md

The link is [用户自定义Sources&Sinks|https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/table/sourcessinks/]",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 02:35:03 UTC 2022,,,,,,,,,,"0|z16tcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 02:10;hunterLiu;I am very interested in this part, please assign it to me;;;","13/Jul/22 02:35;jark;Sorry, there is already an issue for this. FLINK-16105;;;",,,,,,,,,,,,,,,,,,,
Job suspended because ZK session times out,FLINK-28523,13471299,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,hxbks2ks,hxbks2ks,13/Jul/22 02:04,31/Aug/23 10:19,04/Jun/24 20:42,07/Feb/23 10:59,1.16.0,,,,1.15.4,1.16.0,,,Runtime / Checkpointing,,,,,,,0,pull-request-available,test-stability,,,,,"
{code:java}
2022-07-12T02:28:07.2243130Z Jul 12 02:28:07 [ERROR] LocalRecoveryITCase.executeTest  Time elapsed: 29.154 s  <<< FAILURE!
2022-07-12T02:28:07.2243760Z Jul 12 02:28:07 java.lang.AssertionError: Job completed with illegal application status: UNKNOWN.
2022-07-12T02:28:07.2244333Z Jul 12 02:28:07 	at org.junit.Assert.fail(Assert.java:89)
2022-07-12T02:28:07.2245097Z Jul 12 02:28:07 	at org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.testSlidingTimeWindow(EventTimeWindowCheckpointingITCase.java:529)
2022-07-12T02:28:07.2245983Z Jul 12 02:28:07 	at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:84)
2022-07-12T02:28:07.2246802Z Jul 12 02:28:07 	at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:66)
2022-07-12T02:28:07.2247680Z Jul 12 02:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T02:28:07.2248340Z Jul 12 02:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T02:28:07.2249573Z Jul 12 02:28:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T02:28:07.2250788Z Jul 12 02:28:07 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T02:28:07.2251836Z Jul 12 02:28:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T02:28:07.2253000Z Jul 12 02:28:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T02:28:07.2253985Z Jul 12 02:28:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T02:28:07.2254718Z Jul 12 02:28:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T02:28:07.2255385Z Jul 12 02:28:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-12T02:28:07.2256041Z Jul 12 02:28:07 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-12T02:28:07.2256709Z Jul 12 02:28:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-12T02:28:07.2257481Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T02:28:07.2258155Z Jul 12 02:28:07 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T02:28:07.2258837Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T02:28:07.2259514Z Jul 12 02:28:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T02:28:07.2260373Z Jul 12 02:28:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T02:28:07.2261087Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T02:28:07.2261755Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T02:28:07.2262398Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T02:28:07.2263045Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T02:28:07.2263680Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T02:28:07.2264302Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T02:28:07.2265018Z Jul 12 02:28:07 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-07-12T02:28:07.2265581Z Jul 12 02:28:07 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-07-12T02:28:07.2266314Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T02:28:07.2266941Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T02:28:07.2267694Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T02:28:07.2268376Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T02:28:07.2269147Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T02:28:07.2269785Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T02:28:07.2270449Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T02:28:07.2271039Z Jul 12 02:28:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T02:28:07.2271611Z Jul 12 02:28:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T02:28:07.2272316Z Jul 12 02:28:07 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T02:28:07.2273057Z Jul 12 02:28:07 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T02:28:07.2273906Z Jul 12 02:28:07 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T02:28:07.2274912Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T02:28:07.2275865Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T02:28:07.2276771Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T02:28:07.2277820Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T02:28:07.2278699Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T02:28:07.2279479Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T02:28:07.2280208Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T02:28:07.2281076Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T02:28:07.2281914Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T02:28:07.2282738Z Jul 12 02:28:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T02:28:07.2283578Z Jul 12 02:28:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T02:28:07.2284417Z Jul 12 02:28:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T02:28:07.2285196Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T02:28:07.2285914Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T02:28:07.2286603Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T02:28:07.2287377Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26912,FLINK-30914,,,,,,,FLINK-33012,,,,,,FLINK-30507,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 07 10:59:29 UTC 2023,,,,,,,,,,"0|z16tc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jul/22 10:19;chesnay;It looks like ZK is not responsive while syncing data to disk:

Client:
{code:java}
02:28:01,807 [main-SendThread(127.0.0.1:37119)] WARN  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Client session timed out, have not heard from server in 13339ms for sessionid 0x101a9fc62e90000 {code}
Server:

{code}
02:27:48,467 [        SyncThread:0] DEBUG org.apache.zookeeper.server.FinalRequestProcessor            [] - sessionid:0x101a9fc62e90000 type:ping cxid:0xfffffffffffffffe zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a
02:28:01,808 [  NIOWorkerThread-35] WARN  org.apache.zookeeper.server.NIOServerCnxn                    [] - Unable to read additional data from client sessionid 0x101a9fc62e90000, likely client has closed socket
02:28:01,915 [        SyncThread:0] WARN  org.apache.zookeeper.server.persistence.FileTxnLog           [] - fsync-ing the write ahead log in SyncThread:0 took 12932ms which will adversely effect operation latency. File size is 67108880 bytes. See the ZooKeeper troubleshooting guide
{code}

What I don't understand is why the client exists after 13 seconds already; the session-timeout default is 60 seconds (which should be reduced to 20 after negotiation with the ZK server).

{code}
02:27:37,813 [main-SendThread(127.0.0.1:37119)] INFO  org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn [] - Session establishment complete on server localhost/127.0.0.1:37119, sessionid = 0x101a9fc62e90000, negotiated timeout = 20000
{code}

;;;","28/Jul/22 08:43;chesnay;master: e122dec43edac9a03ef86eeacd02ebb5f429eb70;;;","06/Feb/23 15:03;mapohl;Reopening to create a 1.15 backport after FLINK-30914 popped up.;;;","07/Feb/23 10:59;mapohl;1.15: 6e5a7a7d259df2e379457a0b5831229a053b4710;;;",,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-sequence-file,FLINK-28522,13471296,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,12/Jul/22 23:51,04/Nov/22 08:28,04/Jun/24 20:42,04/Nov/22 08:08,,,,,1.17.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 04 08:08:18 UTC 2022,,,,,,,,,,"0|z16tbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 23:51;rskraba;Can this be assigned to me please?;;;","04/Nov/22 08:08;mapohl;master: 276bb778c7ea4555370c6155af5128ea7f7cbe6c;;;",,,,,,,,,,,,,,,,,,,
Hostname verification in ssl context is not working,FLINK-28521,13471220,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,j-d.hatzenbuhler,j-d.hatzenbuhler,j-d.hatzenbuhler,12/Jul/22 13:05,16/Aug/23 22:35,04/Jun/24 20:42,,1.13.6,1.14.5,1.15.1,,,,,,Runtime / Network,,,,,,,0,pull-request-available,security,stale-assigned,,,,"The hostname certificate is not check in ssl context.
Moreover the {{security.ssl.verify-hostname}} is not used anywhere in the code.

The issue come from {{netty4}} where the hostname verification is not enable by default. See [documentation|https://netty.io/4.1/api/io/netty/handler/ssl/SslContext.html#newEngine-io.netty.buffer.ByteBufAllocator-]

h2. How to fix this issue:

In {{org.apache.flink.runtime.io.network.netty.SSLHandlerFactory}}:
* Add a new parameter on instance creation: {{isHostnameVerificationEnabled}}
* This parameter will be set to {{false}} if {{security.ssl.internal.cert.fingerprint}} or {{security.ssl.rest.cert.fingerprint}} is setup
* Add the following code after creating an {{SSLEngine}}: 
{code:java}
SSLEngine sslEngine = sslContext.newEngine(...)
if (isHostnameVerificationEnabled){
    SSLParameters sslParameters = sslEngine.getSSLParameters();
    sslParameters.setEndpointIdentificationAlgorithm(""HTTPS"");
    sslEngine.setSSLParameters(sslParameters);
}
return sslEngine;
{code}

In {{org.apache.flink.runtime.net.SSLUtils}} add new parameter on each {{new SSLHandlerFactory}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Java,,Wed Aug 16 22:35:18 UTC 2023,,,,,,,,,,"0|z16suo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jul/22 14:22;chesnay;Seems intentional that we skip hostname verification, based on this sentence from the SSL docs:

??_Note: Because internal connections are mutually authenticated with shared certificates, Flink can skip hostname verification. This makes container-based setups easier._??;;;","19/Jul/22 15:07;j-d.hatzenbuhler;
It make sense to skip hostname verification when {{security.ssl.internal.cert.fingerprint}} or {{security.ssl.rest.cert.fingerprint}} is setup.
However if those options are not set we need to use {{security.ssl.verify-hostname}} to control the hostname verification.
I changed the proposed solution regarding your remarks.;;;","18/Aug/22 22:38;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;","16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,
RestClient doesn't use SNI TLS extension,FLINK-28520,13471219,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,j-d.hatzenbuhler,j-d.hatzenbuhler,j-d.hatzenbuhler,12/Jul/22 12:36,30/Oct/23 08:26,04/Jun/24 20:42,,1.13.6,1.14.5,1.15.1,,,,,,Runtime / REST,,,,,,,2,pull-request-available,stale-assigned,,,,,"The {{org.apache.flink.runtime.rest.RestClient}} didn't use SNI TLS extension when ssl options are activated.
 
This cause the {{flink cli}} not be able to communicate with {{{}jobmanager{}}}.
h2. How to fix this issue:

Use:
{code:java}
public SslHandler createNettySSLHandler(ByteBufAllocator allocator, String hostname, int port)
{code}
instead of 
{code:java}
public SslHandler createNettySSLHandler(ByteBufAllocator allocator)
{code}

h2. How to reproduce this issue:

Given:
* An existing {{flink}} instance running without ssl options
* An existing certificate for the _hostname_
* An existing load balancer with SNI like {{traefik}} and without a default certificate between the existing {{flink}} instance and the  {{flink cli}}
* A {{flink cli}} configured with the ssl options

When:
* Run the command {{flink list --jobmanager _hostname_:443}}

Then:
* You will get an error {{unrecognized_name}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Java,,Wed Aug 16 22:35:18 UTC 2023,,,,,,,,,,"0|z16sug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Aug/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,,,,,,,,,
Fix the bug that SortMergeResultPartitionReadScheduler may not read data sequentially,FLINK-28519,13471213,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,12/Jul/22 12:15,08/May/23 11:54,04/Jun/24 20:42,29/Jul/22 11:59,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,"Currently, the SortMergeResultPartitionReadScheduler always gets all active subpartition readers and read at most one data region for them. It is common that some subpartitions are requested before others and their region indexes are ahead of others. If all region data of a subpartition can be read in one round, some subpartition readers will always ahead of others which will cause random IO. This patch fixes this case by polling one subpartition reader at a time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32027,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 11:59:18 UTC 2022,,,,,,,,,,"0|z16st4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jul/22 11:59;kevin.cyj;Merged into master via f88489a6af42638679429df3fdb4818c278cacbf;;;",,,,,,,,,,,,,,,,,,,,
Exception: AssertionError: Cannot add expression of different type to set in sub-query with ROW type,FLINK-28518,13471211,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,KoylubaevNT,KoylubaevNT,12/Jul/22 11:54,05/Oct/23 11:35,04/Jun/24 20:42,05/Oct/23 11:35,1.15.1,,,,1.18.0,,,,Table SQL / Planner,,,,,,,0,,,,,,,"All scripts is attached to file: test.sql

Create 2 tables:

 
{code:java}
SET
'sql-client.execution.result-mode' = 'tableau';
SET
'execution.runtime-mode' = 'batch';
SET
'sql-client.execution.mode' = 'streaming';
SET
'parallelism.default' = '8';
SET
'table.dml-sync' = 'true';


CREATE
TEMPORARY TABLE fl (
`id` INT,
`name` STRING) 
WITH (    
'connector' = 'faker',
'number-of-rows' = '10',
'rows-per-second' = '10000',
'fields.id.expression' = '#{number.numberBetween ''0'',''10''}',
    'fields.name.expression' = '#{superhero.name}');

CREATE
TEMPORARY TABLE application (
`id` INT,
`fl_id` INT,
`num` INT,
`db` DOUBLE) 
WITH (
'connector' = 'faker',
'number-of-rows' = '100',
'rows-per-second' = '1000000',
'fields.id.expression' = '#{number.numberBetween ''0'',''1000000''}',
'fields.fl_id.expression' = '#{number.numberBetween ''0'',''10''}',
'fields.num.expression' = '#{number.numberBetween ''-2147483648'',''2147483647''}',
'fields.db.expression' = '#{number.randomDouble ''3'',''-1000'',''1000''}'); {code}
The next SQL throw exception:
{code:java}
select fl.name,
       (select (COLLECT(application.num), COLLECT(application.db))
        from application
        where fl.id = application.fl_id)
from fl;{code}
Error stack trace is (I marked what is different in type: it's just NOT NULL):

 

 
{code:java}
[ERROR] Could not execute SQL statement. Reason:
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" name, RecordType(INTEGER MULTISET EXPR$0, DOUBLE MULTISET EXPR$1) $f0) NOT NULL
expression type is RecordType(INTEGER id, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" name, RecordType(INTEGER MULTISET EXPR$0, DOUBLE MULTISET EXPR$1) NOT NULL $f0) NOT NULL
set is rel#129:LogicalCorrelate.NONE.any.[](left=HepRelVertex#119,right=HepRelVertex#128,correlation=$cor0,joinType=left,requiredColumns={0})
expression is LogicalProject(id=[$0], name=[$1], $f0=[ROW($2, $3)])
  LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0}])
    LogicalTableScan(table=[[default_catalog, default_database, fl]])
    LogicalAggregate(group=[{}], agg#0=[COLLECT($0)], agg#1=[COLLECT($1)])
      LogicalProject(num=[$2], db=[$3])
        LogicalFilter(condition=[=($cor0.id, $1)])
          LogicalTableScan(table=[[default_catalog, default_database, application]])
 {code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27998,,,,,,,,,,,,,,,"13/Jul/22 07:37;KoylubaevNT;SubQueryRowTypeTest.scala;https://issues.apache.org/jira/secure/attachment/13046703/SubQueryRowTypeTest.scala","12/Jul/22 11:50;KoylubaevNT;test.sql;https://issues.apache.org/jira/secure/attachment/13046620/test.sql",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 05 11:34:32 UTC 2023,,,,,,,,,,"0|z16sso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 07:29;martijnvisser;[~jark] [~godfrey] Is this indeed a bug?;;;","13/Jul/22 07:38;KoylubaevNT;I am new in Flink and don't know it very well. 
I think it's a BUG. It's like bug from resolved task: https://issues.apache.org/jira/browse/FLINK-21592, but for other type (ROW).

I wrote a test [^SubQueryRowTypeTest.scala]  which is fail. It's for ""Flink : Table : Planner"" (flink-table-planner_${scala.binary.version});;;","18/Jul/22 14:04;KoylubaevNT;[~martijnvisser] I found that error was in calcite. And they resolved it in 1.30.0 version. 
Is it possible to upgrade version calcite in flink project from 1.26.0 to 1.30.0?;;;","18/Jul/22 14:15;martijnvisser;[~KoylubaevNT] A Calcite upgrade is not an easy task; it's currently tracked under FLINK-28518;;;","25/Aug/23 08:06;337361684@qq.com;Hi, [~martijnvisser]  and [~KoylubaevNT] . This case already fixed as now flink upgrade calcite to 1.30. This issue can be closed.;;;","05/Oct/23 11:34;rmetzger;This is the ticket for the calcite upgrade, which has been closed with 1.18 https://issues.apache.org/jira/browse/FLINK-27998;;;",,,,,,,,,,,,,,,
Bump Flink version to 1.15.1,FLINK-28517,13471209,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,morhidi,morhidi,,12/Jul/22 11:41,24/Nov/22 01:03,04/Jun/24 20:42,13/Jul/22 11:49,kubernetes-operator-1.1.0,,,,kubernetes-operator-1.1.0,,,,Kubernetes Operator,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 11:49:25 UTC 2022,,,,,,,,,,"0|z16ss8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jul/22 11:51;morhidi;started working on this;;;","13/Jul/22 11:49;gyfora;merged to main 29d0187a4a03ab0ce06ef40038002bd62b2ef89d;;;",,,,,,,,,,,,,,,,,,,
UI is misleading when source doesn't emit watermark due to misconfigured idleness,FLINK-28516,13471207,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chesnay,chesnay,12/Jul/22 11:35,11/Mar/24 12:44,04/Jun/24 20:42,,,,,,1.20.0,,,,Connectors / Common,Runtime / Web Frontend,,,,,,0,,,,,,,"When a source is not emitting watermarks because one of the readers is idle, without idleness being set up, then the UI display this in the watermarks tab:
{code:java}
No Watermark (Watermarks are only available if EventTime is used) {code}
My immediate interpretation was that event time was not used by my job. I understand the intention behind the event time remark, but it reads to much as a reason as to why there is no watermark in the current job.

This could be improved by either storing a flag in the job graph for whether event time is properly set up, or enhancing the watermarks tab to also show output watermarks for sources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-07-12 11:35:35.0,,,,,,,,,,"0|z16srs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The files in local recovery directory hasn't be clean up properly after checkpoint abort,FLINK-28515,13471206,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lijinzhong,lijinzhong,lijinzhong,12/Jul/22 11:25,21/Sep/22 06:45,04/Jun/24 20:42,,1.15.1,1.16.0,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,,0,pull-request-available,stale-assigned,,,,,"In my case,  i found that some files in local recovery directory hasn't be clean up properly after checkpoint abort(as shown in the attached picture).

By analyzing flink log, I found that when stateBackend completes the local snapshot but the task has not completed the whole snapshot, 
then checkpoint is aborted (caused by checkpoint timeout or netword-error),  files in the local directory directory may not be cleaned up properly.

I think the reason for local snapshot file residual is:
(1) In the org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable, the comleted localSnapshot info can be registered into org.apache.flink.runtime.state.TaskLocalStateStoreImpl only after task  has completed the whole snapshot. ([AsyncCheckpointRunnable.java#L136|https://github.com/apache/flink/blob/3ec376601f836df6314e771b243ca6f896a7f642/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/AsyncCheckpointRunnable.java#L136]).
(2) If stateBackend completes the local snapshot but the task has not completed the entire snapshot, when checkpoint-aborting is triggered, the TaskLocalStateStore can't clean up the unregistered localSnapshot files. ([TaskLocalStateStoreImpl.java#L301|https://github.com/apache/flink/blob/3ec376601f836df6314e771b243ca6f896a7f642/flink-runtime/src/main/java/org/apache/flink/runtime/state/TaskLocalStateStoreImpl.java#L301])

(3) And when SubtaskCheckpointCoordinatorImpl receive the abort notification, it will cancel all the ongoing stateSnapshot futureTask in 'AsyncCheckpointRunnable.close()'.  For rocksdbKeyedStatebackend, [AsyncSnapshotTask.cancel |https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsyncSnapshotCallable.java#L129]will be invoke during checkpoint abort. After this, the [RocksDBIncrementalSnapshotOperation.get |https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L384]may still run until it completes.
And the localSnapshot files can't be cleaned up in RocksDBIncrementalSnapshotOperation.get(finally) and AsyncSnapshotCallable.call（[finally-cleanup|https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsyncSnapshotCallable.java#L87]).
Then the localSnapshot files  also can't be cleaned up in [AsyncCheckpointRunnable.cleanup|https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/AsyncCheckpointRunnable.java#L391], because [AsyncSnapshotTask.cancel|https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java#L78] return ture.

 

To fix this problem, I think when TaskLocalStateStoreImpl abort Checkpoint, we can try to delete the corresponding localRecovery directory, even if the checkpoint is not unregistered into TaskLocalStateStoreImpl.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/22 11:20;lijinzhong;C7245668-CE31-4F56-B9CB-12E2F1E900C5.png;https://issues.apache.org/jira/secure/attachment/13046619/C7245668-CE31-4F56-B9CB-12E2F1E900C5.png","19/Jul/22 10:28;lijinzhong;image-2022-07-19-18-28-20-239.png;https://issues.apache.org/jira/secure/attachment/13046968/image-2022-07-19-18-28-20-239.png","12/Jul/22 11:21;lijinzhong;image.png;https://issues.apache.org/jira/secure/attachment/13046618/image.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 21 22:37:56 UTC 2022,,,,,,,,,,"0|z16srk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jul/22 03:19;Yanfei Lei;I think this is reasonable. Although TM will delete other checkpoints [when recovering|#L199],  if JM re-allocate the task, the files in local recovery directory of aborted checkpoint would not be clean up properly, it's right to delete early.;;;","13/Jul/22 03:28;lijinzhong;[~roman] [~yunta] Could you please take a look at this ticket?;;;","13/Jul/22 10:04;roman;Thanks for reporting and providing the PR [~lijinzhong].
I think the problem analysis is correct.

Regarding the solution, I have the following concerns:
1. It deletes the folder recursively; however, it doesn't mean the handles are discarded (for the normal case, there is taskStateSnapshot.discardState() call for that). That might potentially leave some state undeleted.
2. We should probably accomodate for the missing abort notification (e.g. job termination)

I think (1) can be fixed easily in the current PR by adding the missing call;
(2) likely requires early state registration - which needs more design and changes. I think it can probably be solved independently if needed.

WDYT?;;;","19/Jul/22 10:30;lijinzhong;[~roman]  I thought about this problem again these days. 

I found the previous analysis for the problem was not comprehensive, the state object of uncompleted checkpoint will be cleaned up in [AsyncCheckpointRunnable.cleanup|#L377];]

But i think there is still a problem that the localSnapshot files can't be cleaned up properly after checkpoint abort.

By reviewing the flink code, i think the reason is:
 # For FutureTask, if it is cancelled, the callable in it may still run until it completes or encounters an exception.
 # When one checkpoint is aborted, its corresponding FutureTask will be cancelled.  
For rocksdbKeyedStatebackend, [AsyncSnapshotTask.cancel |https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsyncSnapshotCallable.java#L129]will be invoke during checkpoint abort. After this, the [RocksDBIncrementalSnapshotOperation.get |https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L384]may still run until it completes.
And the localSnapshot files can't be cleaned up in RocksDBIncrementalSnapshotOperation.get([finally|[https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L447]]) and AsyncSnapshotCallable.call（[finally-cleanup|https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsyncSnapshotCallable.java#L87]).
Then the localSnapshot files  also can't be cleaned up in [AsyncCheckpointRunnable.cleanup|https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/AsyncCheckpointRunnable.java#L391], because [AsyncSnapshotTask.cancel|https://github.com/apache/flink/blob/dcc9cceab962c897c7a5e55deb868eb2d86468ec/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java#L78] return ture.

!image-2022-07-19-18-28-20-239.png|width=812,height=177!

To fix this problem, i think we can 

(1) when TaskLocalStateStoreImpl abort Checkpoint, try to delete the corresponding localRecovery directory, even if the checkpoint is not unregistered into TaskLocalStateStoreImpl.
Or (2) In AsyncSnapshotCallable.cleanup, check whether the snapshot FutureTask is cancelled; If true, force to clean up the localRecovery directory.

[~roman]  WDYT?;;;","21/Jul/22 20:38;roman;I have some concerns regarding the proposed solution: if the AsyncSnapshotCallable is still running at the time of abort notification then folder deletion might fail.

Probably a more robust solution would be to delete any local outdated checkpoints/folders on checkpoint confirmation. So that the deletion will succeed *eventually*. To list all old checkpoint folders we might either list folders directly using FS or change the API (instead of only constructing the path, contruct it, mkdir, and register with TaskLocalStateStore ).
This way missing abort notifications will also be tolerated.

I'm also not sure about the exact reason (and maybe there are multiple). 
Is the issue reproducible? 
If it could it be transformed into a test that could be a good first step IMO.

WDYT?;;;","22/Jul/22 04:48;lijinzhong;Thanks for reply [~roman] 

>> ""if the AsyncSnapshotCallable is still running at the time of abort notification then folder deletion might fail.""

IIUC, when SubtaskCheckpointCoordinatorImpl handle abort notification, it will close all the snapshotIO firstly
 (by AsyncCheckpointRunnable.cleanup -> OperatorSnapshotFutures.discardStateFuture -> AsyncSnapshotTask.cancel -> AsyncSnapshotCallable.cancel).
 so then folder deletion in 'TaskLocalStateStore.abortCheckpoint' should not fail?

I have saw the localSnapshot clean-up problem few times in my production environment.  Although the probability of this problem occurring is relatively small, I think it is reproducible.

Thanks for your advice, roman. I will try to transform it into a test next.;;;","22/Jul/22 09:07;roman;Thanks, that would be great [~lijinzhong]

> IIUC, when SubtaskCheckpointCoordinatorImpl handle abort notification, it will close all the snapshotIO firstly
Right, SubtaskCheckpointCoordinatorImpl first *tries* to close IO and then removes the folder.
But when closing the IO, there is a CAS check in AsyncSnapshotCallable.cancel():
{code:java}
    protected void cancel() {
        closeSnapshotIO();
        if (resourceCleanupOwnershipTaken.compareAndSet(false, true)) {
            cleanup();
        }
    }
{code}
So if it's already running, cleanup() can be skipped by the current thread.
closeSnapshotIO() only closes the registry, and I don't see that the folder is registered with it (Maybe that's a problem?).

Anyways, I think it would be great to have a test to validate the understanding and the fix.;;;","22/Jul/22 10:33;lijinzhong;Thanks for reply [~roman] 

I'd like to explain the reason for this issue in more detail. Please correct me if anything is wrong.

>> So if it's already running, cleanup() can be skipped by the current thread.
closeSnapshotIO() only closes the registry, and I don't see that the folder is registered with it. 

This is right, for this case, closeSnapshotIO() will close all the CheckpointStreams which belong to the folder, but it will not delete the folder.

 

1. But if cleanup() method is skipped here, it will still be invoke in [AsyncSnapshotCallable.call() finally|https://github.com/apache/flink/blob/81379a56495283020a5919e8115936c163b251ba/flink-runtime/src/main/java/org/apache/flink/runtime/state/AsyncSnapshotCallable.java#L87] . 

2. AsyncSnapshotCallable.cleanup() can only delete the ONGOING folder, not COMPLETED.(AsyncSnapshotTask.cleanup() -> IncrementalRocksDBSnapshotResources.release -> SnapshotDirectory.cleanUp)
{code:java}
 public boolean cleanup() throws IOException {
      if (state.compareAndSet(State.ONGOING, State.DELETED)) {
          FileUtils.deleteDirectory(directory.toFile());
      }
      return true;
 } {code}
3. AsyncSnapshotTask.callInternal() will invoke RocksDBIncrementalSnapshotOperation.get(CloseableRegistry snapshotCloseableRegistry), in which the folder status will be [transformed from ONGOING to COMPLETED|https://github.com/apache/flink/blob/81379a56495283020a5919e8115936c163b251ba/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksIncrementalSnapshotStrategy.java#L422]].

If RocksDBIncrementalSnapshotOperation.get(CloseableRegistry) encounters *no* exception after rocksdb AsyncSnapshotTask is cancelled,

(1) the localSnapshot folder can't be cleaned-up by AsyncSnapshotCallable.cleanup() because the folder status is COMPLETED;

(2) the localSnapshot folder can't be cleaned-up by RocksDBIncrementalSnapshotOperation.get(CloseableRegistry)-finally because the completed flag is ture.
{code:java}
@Override
public SnapshotResult<KeyedStateHandle> get(CloseableRegistry snapshotCloseableRegistry)  
 throws Exception {
      try{
          .......
          completed = true;
          return snapshotResult;
       } finally {
           if (!completed) {
              .........
            }
       }
} {code}
 ;;;","21/Aug/22 22:37;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issue is assigned but has not received an update in 30 days, so it has been labeled ""stale-assigned"".
If you are still working on the issue, please remove the label and add a comment updating the community on your progress.  If this issue is waiting on feedback, please consider this a reminder to the committer/reviewer. Flink is a very active project, and so we appreciate your patience.
If you are no longer working on the issue, please unassign yourself so someone else may work on it.
;;;",,,,,,,,,,,,
Remove data flush in SortMergeResultPartition,FLINK-28514,13471201,13469906,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,kevin.cyj,kevin.cyj,12/Jul/22 11:01,21/Jul/22 03:58,04/Jun/24 20:42,21/Jul/22 03:58,,,,,1.16.0,,,,Runtime / Network,,,,,,,0,pull-request-available,,,,,,This patch aims to remove the data flush in SortMergeResultPartition because it is useless.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 21 03:58:45 UTC 2022,,,,,,,,,,"0|z16sqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jul/22 03:58;kevin.cyj;Merged into master via 9a141441cb96837d92c4bd348b4c05617100e14b;;;",,,,,,,,,,,,,,,,,,,,
