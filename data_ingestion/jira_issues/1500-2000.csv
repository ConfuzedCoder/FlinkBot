Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Issue split),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
ProfilingServiceTest.testRollingDeletion is unstable on AZP,FLINK-34013,13563831,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yu Chen,Sergey Nuyanzin,Sergey Nuyanzin,08/Jan/24 01:03,10/Jan/24 02:16,04/Jun/24 20:40,10/Jan/24 02:16,1.19.0,,,,,,,,1.19.0,,,,API / Core,,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56073&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8258 fails as 
{noformat}
Jan 06 02:09:28 org.opentest4j.AssertionFailedError: expected: <2> but was: <3>
Jan 06 02:09:28 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Jan 06 02:09:28 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Jan 06 02:09:28 	at org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197)
Jan 06 02:09:28 	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150)
Jan 06 02:09:28 	at org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145)
Jan 06 02:09:28 	at org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531)
Jan 06 02:09:28 	at org.apache.flink.runtime.util.profiler.ProfilingServiceTest.verifyRollingDeletionWorks(ProfilingServiceTest.java:167)
Jan 06 02:09:28 	at org.apache.flink.runtime.util.profiler.ProfilingServiceTest.testRollingDeletion(ProfilingServiceTest.java:117)
Jan 06 02:09:28 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 06 02:09:28 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Jan 06 02:09:28 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Jan 06 02:09:28 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Jan 06 02:09:28 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Jan 06 02:09:28 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 10 02:16:07 UTC 2024,,,,,,,,,,"0|z1mlps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 01:11;Sergey Nuyanzin;I think it is related to FLINK-33433
[~Yu Chen], [~yunta] could you please have a look?;;;","08/Jan/24 15:20;Yu Chen;Hi [~Sergey Nuyanzin] , really sorry about that. Let me have a look to fix this.;;;","08/Jan/24 17:29;Yu Chen;Hi [~Sergey Nuyanzin], I have reproduced it on my local machine. And it's a bug caused by triggering {*}`{*}stopProfiling{*}`{*} twice for each profiling request.

I'll create a PR to fix this. Thank you for pointing out!;;;","10/Jan/24 02:16;yunta;merged in master: c09f07a406398bc4b2320e9b5ae0a8f5f27a00dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink python fails with  can't read file '/__w/2/s/flink-python/dev/.conda/lib/python3.10/site-packages//google,FLINK-34012,13563830,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hxb,Sergey Nuyanzin,Sergey Nuyanzin,08/Jan/24 01:00,08/Jan/24 07:33,04/Jun/24 20:40,08/Jan/24 07:33,,,,,,,,,1.19.0,,,,API / Python,,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56073&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=20755
{noformat}
Jan 06 03:02:43 Installing collected packages: types-pytz, types-python-dateutil, types-protobuf
Jan 06 03:02:43 Successfully installed types-protobuf-4.24.0.20240106 types-python-dateutil-2.8.19.20240106 types-pytz-2023.3.1.1
Jan 06 03:02:44 mypy: can't read file '/__w/2/s/flink-python/dev/.conda/lib/python3.10/site-packages//google': No such file or directory
Jan 06 03:02:44 Installing missing stub packages:
Jan 06 03:02:44 /__w/2/s/flink-python/dev/.conda/bin/python -m pip install types-protobuf types-python-dateutil types-pytz

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 07:33:08 UTC 2024,,,,,,,,,,"0|z1mlpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 01:04;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=20755;;;","08/Jan/24 01:05;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=20383;;;","08/Jan/24 01:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=20560;;;","08/Jan/24 01:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=20738;;;","08/Jan/24 01:07;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3&l=20819;;;","08/Jan/24 01:07;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=60960eae-6f09-579e-371e-29814bdd1adc&t=7a70c083-6a74-5348-5106-30a76c29d8fa&l=20371;;;","08/Jan/24 01:07;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56084&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=20755;;;","08/Jan/24 07:03;qingyue;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56091&view=logs&j=dd7e7115-b4b1-5414-20ec-97b9411e0cfc&t=c759a57f-2774-59e9-f882-8e4d5d3fbb9f;;;","08/Jan/24 07:33;hxb;Merged into master via 639deeca33757c7380d474d43b8a70bacb84dd20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedThreadDependencyTests.test_add_python_file fails with This function should not be called!,FLINK-34011,13563829,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,08/Jan/24 00:32,12/Jan/24 10:27,04/Jun/24 20:40,12/Jan/24 10:27,1.18.2,,,,,,,,,,,,API / Python,,,,0,test-stability,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55966&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=27052
fails as 
{noformat}
Jan 02 03:27:17 E                   Caused by: pemja.core.PythonException: <class 'Exception'>: This function should not be called!
Jan 02 03:27:17 E                   	at /__w/1/s/flink-python/pyflink/fn_execution/embedded/operation_utils.process_element(operation_utils.py:72)
Jan 02 03:27:17 E                   	at /__w/1/s/flink-python/pyflink/fn_execution/table/operations.process_element(operations.py:102)
Jan 02 03:27:17 E                   	at <string>.<lambda>(<string>:1)
Jan 02 03:27:17 E                   	at /__w/1/s/flink-python/pyflink/table/tests/test_dependency.plus_two(test_dependency.py:52)
Jan 02 03:27:17 E                   	at /tmp/python-dist-98ae39ca-f087-4122-8e67-f358a13f57c7/python-files/blob_p-bedc014754a86159784fd7cebff77ab534802643-43ac971c5ca669babbfe4ee1d5a1ad0e/test_dependency_manage_lib.add_two(test_dependency_manage_lib.py:2)
Jan 02 03:27:17 E                   	at pemja.core.PythonInterpreter.invokeMethodOneArgLong(Native Method)
Jan 02 03:27:17 E                   	at pemja.core.PythonInterpreter.invokeMethodOneArg(PythonInterpreter.java:222)
Jan 02 03:27:17 E                   	at pemja.core.PythonInterpreter.invokeMethod(PythonInterpreter.java:116)
Jan 02 03:27:17 E                   	at org.apache.flink.table.runtime.operators.python.scalar.EmbeddedPythonScalarFunctionOperator.processElement(EmbeddedPythonScalarFunctionOperator.java:170)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:425)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:520)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:110)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:99)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71)
Jan 02 03:27:17 E                   	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338)
{noformat}",,,,,,,,,,,,,,,,,,,,,,FLINK-26974,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 10:27:16 UTC 2024,,,,,,,,,,"0|z1mlpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 10:27;mapohl;Closing this issue in favor of FLINK-26974.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaTableITCase.testPerPartitionWatermarkWithIdleSource is unstable on AZP,FLINK-34010,13563828,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,08/Jan/24 00:24,08/Jan/24 00:24,04/Jun/24 20:40,,1.17.3,,,,,,,,,,,,Connectors / Kafka,,,,0,test-stability,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55965&view=logs&j=9c5a5fe6-2f39-545e-1630-feb3d8d0a1ba&t=99b23320-1d05-5741-d63f-9e78473da39e&l=32434
fails as 
{noformat}
Jan 02 05:24:56 [ERROR] Tests run: 39, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 97.948 s <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase
Jan 02 05:24:56 [ERROR] KafkaTableITCase.testPerPartitionWatermarkWithIdleSource  Time elapsed: 5.494 s  <<< ERROR!
Jan 02 05:24:56 java.util.concurrent.TimeoutException: Can not get the expected result.
Jan 02 05:24:56 	at org.apache.flink.core.testutils.CommonTestUtils.waitUtil(CommonTestUtils.java:214)
Jan 02 05:24:56 	at org.apache.flink.core.testutils.CommonTestUtils.waitUtil(CommonTestUtils.java:230)
Jan 02 05:24:56 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
Jan 02 05:24:56 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableITCase.testPerPartitionWatermarkWithIdleSource(KafkaTableITCase.java:951)
Jan 02 05:24:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 02 05:24:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 02 05:24:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-08 00:24:56.0,,,,,,,,,,"0|z1mlp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache flink: Checkpoint restoration issue on Application Mode of deployment,FLINK-34009,13563805,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vrangana@in.ibm.com,vrangana@in.ibm.com,07/Jan/24 15:52,12/Jan/24 08:39,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,,"Hi Team,

Good Day. Wish you all a happy new year 2024.

We are using Flink (1.18) version on our flink cluster. Job manager has been deployed on ""Application mode"" and HA is disabled (high-availability.type: NONE), under this configuration parameters we are able to start multiple jobs (using env.executeAsync()) of a single application.

Note: We have also setup checkpoint on a s3 instance with RETAIN_ON_CANCELLATION mode (plus other required settings).

Lets say now we start two jobs of the same application (ex: Jobidxxx1, jobidxxx2) and they are currently running on the k8s env. If we have to perform Flink minor upgrade (or) upgrade of our application with minor changes, in that case we will stop the Job Manager and Task Managers instances and perform the necessary up-gradation then when we start both Job Manager and Task Managers instance. On startup we expect the job's to be restored back from the last checkpoint, but the job restoration is not happening on Job manager startup. Please let us know if this is an bug (or) its the general behavior of flink under application mode of deployment.

Additional information: If we enable HA (using Zookeeper) on Application mode, we are able to startup only one job (i.e., per-job behavior). When we perform Flink minor upgrade (or) upgrade of our application with minor changes, the checkpoint restoration is working properly on Job Manager & Task Managers restart process.

It seems checkpoint restoration and HA are inter-related, but why checkpoint restoration doesn't work when HA is disabled.

 

Please let us know if anyone has experienced similar issues or if have any suggestions, it will be highly appreciated. Thanks in advance for your assistance.","Flink version: 1.18

Zookeeper version: 3.7.2

Env: Custom flink docker image (with embedded application class) deployed over kubernetes (v1.26.11).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 08:39:55 UTC 2024,,,,,,,,,,"0|z1mlk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 05:30;vrangana@in.ibm.com;As flink support multi-job execution on Application mode of deployment (with HA being disabled), we need more details of how to enable restoration process via checkpointing (when app / flink is upgraded). Please support us to overcome this issue. Thanks.;;;","12/Jan/24 08:39;masteryhx;[~vrangana@in.ibm.com]

Hi, You could reorganize the info and  ask for help in User mails where more people could help with this.

Let's report in Jira after guaranting it's an issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Taskmanager blocked with deserialization when starting,FLINK-34008,13563767,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,stupid_pig,stupid_pig,06/Jan/24 09:41,06/Jan/24 09:49,04/Jun/24 20:40,,1.15.2,,,,,,,,,,,,Runtime / Task,,,,0,,,,"When starting taskmanager in kubernetes cluster,  some tasks will block with deserialization.

However, th jobmanager will not consider the taskmanager heartbeat timeout, so the entire Job is in a Block state and cannot be checkpointed.

 

Thread dumb(taskmanager):
{code:java}
2024-01-06 11:37:53Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.202-b08 mixed mode):
""flink-akka.actor.default-dispatcher-19"" #339 prio=5 os_prio=0 tid=0x00007f9ca84d6000 nid=0x5c8 waiting on condition [0x00007f9c831fd000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d2d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-akka.actor.default-dispatcher-18"" #338 prio=5 os_prio=0 tid=0x00007f9c8c144000 nid=0x5c7 waiting on condition [0x00007f9c8857d000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d2d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""Attach Listener"" #334 daemon prio=9 os_prio=0 tid=0x00007f9cbae06000 nid=0x59e waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
   Locked ownable synchronizers:	- None
""OkHttp WebSocket xxx"" #323 prio=5 os_prio=0 tid=0x00007f9ca9611800 nid=0x551 waiting on condition [0x00007f9c837fc000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000e7101010> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""flink-taskexecutor-io-thread-3"" #316 daemon prio=5 os_prio=0 tid=0x00007f9c88584800 nid=0x53c waiting on condition [0x00007f9cb0ffc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d8027900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""flink-taskexecutor-io-thread-2"" #312 daemon prio=5 os_prio=0 tid=0x00007f9c9e00a800 nid=0x52d waiting on condition [0x00007f9c9dffc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d8027900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""flink-taskexecutor-io-thread-1"" #311 daemon prio=5 os_prio=0 tid=0x00007f9c8c0bf800 nid=0x52c waiting on condition [0x00007f9ca95fc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d8027900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""prometheus-http-1-5"" #123 prio=5 os_prio=0 tid=0x00007f9c9fa1a000 nid=0x14e waiting on condition [0x00007f9ca377d000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc44f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""prometheus-http-1-4"" #122 prio=5 os_prio=0 tid=0x00007f9c9fa19800 nid=0x14c waiting on condition [0x00007f9c83dfd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc44f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""prometheus-http-1-3"" #115 prio=5 os_prio=0 tid=0x00007f9c9fa18800 nid=0x145 waiting on condition [0x00007f9ca6ffd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc44f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""prometheus-http-1-2"" #114 prio=5 os_prio=0 tid=0x00007f9c9fa18000 nid=0x143 waiting on condition [0x00007f9c9f3fd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc44f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""org.apache.hadoop.hdfs.PeerCache@551ed293"" #102 daemon prio=5 os_prio=0 tid=0x00007f9ca7023800 nid=0x137 sleeping[0x00007f9c867fd000]   java.lang.Thread.State: TIMED_WAITING (sleeping)	at java.lang.Thread.sleep(Native Method)	at org.apache.hadoop.hdfs.PeerCache.run(PeerCache.java:249)	at org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:46)	at org.apache.hadoop.hdfs.PeerCache$1.run(PeerCache.java:124)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""TumblingEventTimeWindows -> Sink: replay-sink-g126 (1/1)#0"" #101 prio=5 os_prio=0 tid=0x00007f9c88581800 nid=0x136 in Object.wait() [0x00007f9c86dfc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Sink: split-sink-g126 (1/1)#0"" #99 prio=5 os_prio=0 tid=0x00007f9c88580000 nid=0x134 in Object.wait() [0x00007f9c879fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Config2LogFunction-olschema -> diffOLSchema -> Sink: other-log-schema-sink-g126 (1/1)#0"" #98 prio=5 os_prio=0 tid=0x00007f9c8c0be000 nid=0x133 in Object.wait() [0x00007f9c87ffc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""other-log-schema (1/1)#0"" #96 prio=5 os_prio=0 tid=0x00007f9c8c0bb800 nid=0x131 in Object.wait() [0x00007f9c88dfc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""discover-schema-g126 -> schema-to-kafka-record-map -> null-filter -> Sink: schema-sink-g126 (1/1)#0"" #95 prio=5 os_prio=0 tid=0x00007f9c8c0bd000 nid=0x130 in Object.wait() [0x00007f9c893fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""aggreagate-window-logschema (1/1)#0"" #94 prio=5 os_prio=0 tid=0x00007f9c8c0bc800 nid=0x12f in Object.wait() [0x00007f9c899fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Config2LogFunction -> filter-g126 -> parser-regex -> preprocess-g126 -> splitter-json -> (null-log-filter -> (Filter, Filter), Sink: error-sink-g126, log2event-g126 -> (Filter -> Sink: event-sink-g126, Sink: data-sample-g126)) (1/1)#0"" #93 prio=5 os_prio=0 tid=0x00007f9c8c0b8000 nid=0x12e in Object.wait() [0x00007f9c89ffc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Source: Custom Source (1/1)#0"" #91 prio=5 os_prio=0 tid=0x00007f9c8c0ba000 nid=0x12b in Object.wait() [0x00007f9c8b1fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Source: Custom Source (1/1)#0"" #90 prio=5 os_prio=0 tid=0x00007f9c8c0b9800 nid=0x12a in Object.wait() [0x00007f9c8b7fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Source: kafka-replay-source (1/1)#0"" #88 prio=5 os_prio=0 tid=0x00007f9c8c0b8800 nid=0x129 in Object.wait() [0x00007f9ca75fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""CloseableReaperThread"" #87 daemon prio=5 os_prio=0 tid=0x00007f9ca7022000 nid=0x128 in Object.wait() [0x00007f9c9ebfd000]   java.lang.Thread.State: WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	- waiting on <0x00000000d6b23f80> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)	- locked <0x00000000d6b23f80> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:215)
   Locked ownable synchronizers:	- None
""Source: kafka-raw-metrics-source -> tuple-process-headers-value -> String2AgentHeader -> Timestamps/Watermarks (1/1)#0"" #86 prio=5 os_prio=0 tid=0x00007f9c8c0b7000 nid=0x127 in Object.wait() [0x00007f9ca13fc000]   java.lang.Thread.State: BLOCKED (on object monitor)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at java.lang.Class.getEnumConstantsShared(Class.java:3320)	at java.lang.Class.enumConstantDirectory(Class.java:3341)	at java.lang.Enum.valueOf(Enum.java:232)	at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2011)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1570)	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287)	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211)	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069)	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573)	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602)	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589)	at org.apache.flink.util.SerializedValue.deserializeValue(SerializedValue.java:67)	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:627)	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""config-map-watch-handler-thread-2"" #84 daemon prio=5 os_prio=0 tid=0x00007f9ca9c4f000 nid=0x125 waiting on condition [0x00007f9cb37d2000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7ce20> (a java.util.concurrent.SynchronousQueue$TransferStack)	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""pool-9-thread-1"" #83 prio=5 os_prio=0 tid=0x00007f9c9e00a000 nid=0x124 waiting on condition [0x00007f9c8bffc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d068> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""flink-akka.actor.default-dispatcher-16"" #82 prio=5 os_prio=0 tid=0x00007f9c9e009000 nid=0x123 waiting on condition [0x00007f9c8c5fd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d2d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-akka.actor.default-dispatcher-15"" #81 prio=5 os_prio=0 tid=0x00007f9ca6479800 nid=0x122 waiting on condition [0x00007f9c8cbfd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d2d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-akka.actor.default-dispatcher-14"" #80 prio=5 os_prio=0 tid=0x00007f9ca6479000 nid=0x121 waiting on condition [0x00007f9ca7bfd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d2d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""Hashed wheel timer #1"" #35 prio=5 os_prio=0 tid=0x00007f9ca4a06000 nid=0x11f sleeping[0x00007f9c9e5fd000]   java.lang.Thread.State: TIMED_WAITING (sleeping)	at java.lang.Thread.sleep(Native Method)	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""prometheus-http-1-1"" #78 prio=5 os_prio=0 tid=0x00007f9c9fa17000 nid=0x11d waiting on condition [0x00007f9c8d3fd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc44f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Flink Netty Server (0) Thread 0"" #73 daemon prio=5 os_prio=0 tid=0x00007f9cac612000 nid=0x11b runnable [0x00007f9c8db6b000]   java.lang.Thread.State: RUNNABLE	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native Method)	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:193)	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.Native.epollWait(Native.java:186)	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.epollWaitNoTimerChange(EpollEventLoop.java:290)	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:347)	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Flink-MetricRegistry-thread-1"" #72 daemon prio=5 os_prio=0 tid=0x00007f9cac686000 nid=0x11a waiting on condition [0x00007f9c8e5fd000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc49e8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""IOManager reader thread #1"" #69 daemon prio=5 os_prio=0 tid=0x00007f9cac619800 nid=0x119 waiting on condition [0x00007f9c99dfc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc4c50> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:372)
   Locked ownable synchronizers:	- None
""IOManager writer thread #1"" #68 daemon prio=5 os_prio=0 tid=0x00007f9cac618800 nid=0x118 waiting on condition [0x00007f9c9a3fc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc4e58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:482)
   Locked ownable synchronizers:	- None
""Timer-1"" #66 daemon prio=5 os_prio=0 tid=0x00007f9cac618000 nid=0x117 in Object.wait() [0x00007f9c9a9fd000]   java.lang.Thread.State: TIMED_WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at java.util.TimerThread.mainLoop(Timer.java:552)	- locked <0x00000000d7dc5060> (a java.util.TaskQueue)	at java.util.TimerThread.run(Timer.java:505)
   Locked ownable synchronizers:	- None
""Timer-0"" #64 daemon prio=5 os_prio=0 tid=0x00007f9cac617000 nid=0x116 in Object.wait() [0x00007f9c9affd000]   java.lang.Thread.State: TIMED_WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at java.util.TimerThread.mainLoop(Timer.java:552)	- locked <0x00000000d7dc5238> (a java.util.TaskQueue)	at java.util.TimerThread.run(Timer.java:505)
   Locked ownable synchronizers:	- None
""New I/O server boss #8"" #60 prio=5 os_prio=0 tid=0x00007f9c9ce34800 nid=0x113 runnable [0x00007f9c9bbfc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc5710> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc5720> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc56c8> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc5818> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""New I/O worker #7"" #59 prio=5 os_prio=0 tid=0x00007f9c9ce33800 nid=0x112 runnable [0x00007f9c9c1fc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc5a60> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc5a50> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc5a08> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc5b60> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""New I/O boss #6"" #58 prio=5 os_prio=0 tid=0x00007f9c9ce33000 nid=0x111 runnable [0x00007f9c9c7fc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc5da8> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc5d98> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc5d50> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc5ea8> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""New I/O worker #5"" #56 prio=5 os_prio=0 tid=0x00007f9c9ce32000 nid=0x110 runnable [0x00007f9c9cdfc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc60f0> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc60e0> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc6098> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc61f0> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""flink-metrics-akka.remote.default-remote-dispatcher-7"" #55 prio=5 os_prio=0 tid=0x00007f9c9e007800 nid=0x10f waiting on condition [0x00007f9c9d3fd000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc63a0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-metrics-akka.remote.default-remote-dispatcher-6"" #54 prio=5 os_prio=0 tid=0x00007f9c9e007000 nid=0x10e waiting on condition [0x00007f9c9d9fd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc63a0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-metrics-scheduler-1"" #49 prio=5 os_prio=0 tid=0x00007f9cac614000 nid=0x109 sleeping[0x00007f9c9f9fd000]   java.lang.Thread.State: TIMED_WAITING (sleeping)	at java.lang.Thread.sleep(Native Method)	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Thread-7"" #48 prio=5 os_prio=0 tid=0x00007f9cac613800 nid=0x108 runnable [0x00007f9c9fffd000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc7300> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc72f0> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc72a8> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:352)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""server-timer"" #47 daemon prio=5 os_prio=0 tid=0x00007f9cac612800 nid=0x107 in Object.wait() [0x00007f9ca05fd000]   java.lang.Thread.State: TIMED_WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at java.util.TimerThread.mainLoop(Timer.java:552)	- locked <0x00000000d7dc74a8> (a java.util.TaskQueue)	at java.util.TimerThread.run(Timer.java:505)
   Locked ownable synchronizers:	- None
""RMI Scheduler(0)"" #44 daemon prio=5 os_prio=0 tid=0x00007f9cac2bb800 nid=0x104 waiting on condition [0x00007f9ca19fc000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc7910> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""GC Daemon"" #42 daemon prio=2 os_prio=0 tid=0x00007f9cac611000 nid=0x102 in Object.wait() [0x00007f9ca25fd000]   java.lang.Thread.State: TIMED_WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at sun.misc.GC$Daemon.run(GC.java:117)	- locked <0x00000000d7dc7d28> (a sun.misc.GC$LatencyLock)
   Locked ownable synchronizers:	- None
""RMI Reaper"" #41 prio=5 os_prio=0 tid=0x00007f9cac2bc800 nid=0x101 in Object.wait() [0x00007f9ca2bfd000]   java.lang.Thread.State: WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	- waiting on <0x00000000d7dc7eb0> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)	- locked <0x00000000d7dc7eb0> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)	at sun.rmi.transport.ObjectTable$Reaper.run(ObjectTable.java:351)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""RMI TCP Accept-8800"" #40 daemon prio=5 os_prio=0 tid=0x00007f9cb7972000 nid=0x100 runnable [0x00007f9ca31fc000]   java.lang.Thread.State: RUNNABLE	at java.net.PlainSocketImpl.socketAccept(Native Method)	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)	at java.net.ServerSocket.implAccept(ServerSocket.java:545)	at java.net.ServerSocket.accept(ServerSocket.java:513)	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:405)	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:377)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""New I/O server boss #4"" #38 prio=5 os_prio=0 tid=0x00007f9ca560e000 nid=0xfe runnable [0x00007f9ca43fc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc85b8> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc85c8> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc8570> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc86c0> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""New I/O worker #3"" #37 prio=5 os_prio=0 tid=0x00007f9ca560d800 nid=0xfd runnable [0x00007f9ca49fc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc8908> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc88f8> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc88b0> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc8a08> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""New I/O boss #2"" #36 prio=5 os_prio=0 tid=0x00007f9ca560c800 nid=0xfc runnable [0x00007f9ca4ffc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc8c50> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc8c40> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc8bf8> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc8d50> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""New I/O worker #1"" #34 prio=5 os_prio=0 tid=0x00007f9ca560c000 nid=0xfb runnable [0x00007f9ca55fc000]   java.lang.Thread.State: RUNNABLE	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)	- locked <0x00000000d7dc8f98> (a sun.nio.ch.Util$3)	- locked <0x00000000d7dc8f88> (a java.util.Collections$UnmodifiableSet)	- locked <0x00000000d7dc8f40> (a sun.nio.ch.EPollSelectorImpl)	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dc9098> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""flink-akka.remote.default-remote-dispatcher-7"" #33 prio=5 os_prio=0 tid=0x00007f9ca76ba000 nid=0xfa waiting on condition [0x00007f9ca5bfd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc9248> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-akka.remote.default-remote-dispatcher-6"" #32 prio=5 os_prio=0 tid=0x00007f9ca76b9800 nid=0xf9 waiting on condition [0x00007f9ca63fd000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dc9248> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-akka.actor.default-dispatcher-5"" #31 prio=5 os_prio=0 tid=0x00007f9ca76b8800 nid=0xf8 waiting on condition [0x00007f9ca69fd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7d2d0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)	at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
   Locked ownable synchronizers:	- None
""flink-scheduler-1"" #27 prio=5 os_prio=0 tid=0x00007f9cac2f6800 nid=0xf4 sleeping[0x00007f9ca89fd000]   java.lang.Thread.State: TIMED_WAITING (sleeping)	at java.lang.Thread.sleep(Native Method)	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)	at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)	at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""config-map-watch-handler-thread-1"" #25 daemon prio=5 os_prio=0 tid=0x00007f9ca9c4e000 nid=0xf2 waiting on condition [0x00007f9ca8ffc000]   java.lang.Thread.State: TIMED_WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7d7ce20> (a java.util.concurrent.SynchronousQueue$TransferStack)	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""OkHttp xxx"" #23 prio=5 os_prio=0 tid=0x00007f9caa215000 nid=0xf0 runnable [0x00007f9ca9bfc000]   java.lang.Thread.State: RUNNABLE	at java.net.SocketInputStream.socketRead0(Native Method)	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)	at java.net.SocketInputStream.read(SocketInputStream.java:171)	at java.net.SocketInputStream.read(SocketInputStream.java:141)	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)	at sun.security.ssl.InputRecord.read(InputRecord.java:503)	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975)	- locked <0x00000000e7105a50> (a java.lang.Object)	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933)	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105)	- locked <0x00000000e7105b48> (a sun.security.ssl.AppInputStream)	at org.apache.flink.kubernetes.shaded.okio.Okio$2.read(Okio.java:140)	at org.apache.flink.kubernetes.shaded.okio.AsyncTimeout$2.read(AsyncTimeout.java:237)	at org.apache.flink.kubernetes.shaded.okio.RealBufferedSource.request(RealBufferedSource.java:72)	at org.apache.flink.kubernetes.shaded.okio.RealBufferedSource.require(RealBufferedSource.java:65)	at org.apache.flink.kubernetes.shaded.okio.RealBufferedSource.readByte(RealBufferedSource.java:78)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.WebSocketReader.readHeader(WebSocketReader.java:117)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.WebSocketReader.processNextFrame(WebSocketReader.java:101)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket.loopReader(RealWebSocket.java:273)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.ws.RealWebSocket$1.onResponse(RealWebSocket.java:209)	at org.apache.flink.kubernetes.shaded.okhttp3.RealCall$AsyncCall.execute(RealCall.java:174)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7dcebc8> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""KubernetesClient-Informer-thread-1"" #22 daemon prio=5 os_prio=0 tid=0x00007f9cb8f18800 nid=0xef waiting on condition [0x00007f9caa1fd000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7dced70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Okio Watchdog"" #21 daemon prio=5 os_prio=0 tid=0x00007f9cbc71f800 nid=0xee in Object.wait() [0x00007f9caeb7c000]   java.lang.Thread.State: TIMED_WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at org.apache.flink.kubernetes.shaded.okio.AsyncTimeout.awaitTimeout(AsyncTimeout.java:348)	at org.apache.flink.kubernetes.shaded.okio.AsyncTimeout$Watchdog.run(AsyncTimeout.java:313)	- locked <0x00000000d7d4d718> (a java.lang.Class for org.apache.flink.kubernetes.shaded.okio.AsyncTimeout)
   Locked ownable synchronizers:	- None
""OkHttp ConnectionPool"" #20 daemon prio=5 os_prio=0 tid=0x00007f9cbc71f000 nid=0xed in Object.wait() [0x00007f9caec7c000]   java.lang.Thread.State: TIMED_WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at java.lang.Object.wait(Object.java:460)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool.lambda$new$0(RealConnectionPool.java:62)	- locked <0x00000000d7409b78> (a org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool)	at org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool$$Lambda$112/269892014.run(Unknown Source)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000000d7e19db0> (a java.util.concurrent.ThreadPoolExecutor$Worker)
""org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner"" #17 daemon prio=5 os_prio=0 tid=0x00007f9cb8f16000 nid=0xea in Object.wait() [0x00007f9cb87c3000]   java.lang.Thread.State: WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	- waiting on <0x00000000d72b06e0> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)	- locked <0x00000000d72b06e0> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:2989)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- None
""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007f9cbc6be000 nid=0xe3 runnable [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
   Locked ownable synchronizers:	- None
""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f9cbc6bd000 nid=0xe2 waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
   Locked ownable synchronizers:	- None
""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f9cd345d800 nid=0xe1 waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
   Locked ownable synchronizers:	- None
""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f9cd345c800 nid=0xe0 runnable [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
   Locked ownable synchronizers:	- None
""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f9cd3447000 nid=0xdf waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
   Locked ownable synchronizers:	- None
""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f9cd345c000 nid=0xde in Object.wait() [0x00007f9cbc1fd000]   java.lang.Thread.State: WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)	- locked <0x00000000d6869340> (a java.lang.ref.ReferenceQueue$Lock)	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
   Locked ownable synchronizers:	- None
""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f9cd345b000 nid=0xdd in Object.wait() [0x00007f9cbcf23000]   java.lang.Thread.State: WAITING (on object monitor)	at java.lang.Object.wait(Native Method)	at java.lang.Object.wait(Object.java:502)	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)	- locked <0x00000000d68694f8> (a java.lang.ref.Reference$Lock)	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
   Locked ownable synchronizers:	- None
""main"" #1 prio=5 os_prio=0 tid=0x00007f9cd3458000 nid=0xd6 waiting on condition [0x00007f9cd5192000]   java.lang.Thread.State: WAITING (parking)	at sun.misc.Unsafe.park(Native Method)	- parking to wait for  <0x00000000d7e1a008> (a java.util.concurrent.CompletableFuture$Signaller)	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManager(TaskManagerRunner.java:487)	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.lambda$runTaskManagerProcessSecurely$5(TaskManagerRunner.java:525)	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner$$Lambda$87/1088104996.call(Unknown Source)	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext$$Lambda$88/421637524.run(Unknown Source)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:525)	at org.apache.flink.runtime.taskexecutor.TaskManagerRunner.runTaskManagerProcessSecurely(TaskManagerRunner.java:505)	at org.apache.flink.kubernetes.taskmanager.KubernetesTaskExecutorRunner.main(KubernetesTaskExecutorRunner.java:39)
   Locked ownable synchronizers:	- None
""VM Thread"" os_prio=0 tid=0x00007f9cd34cf800 nid=0xdc runnable 
""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007f9cd34cd000 nid=0xd7 runnable 
""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007f9cd34ce000 nid=0xda runnable 
""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f9cd34ce800 nid=0xdb runnable 
""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007f9cd3459800 nid=0xd9 runnable 
""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007f9cd3459000 nid=0xd8 runnable 
""VM Periodic Task Thread"" os_prio=0 tid=0x00007f9cd34d0800 nid=0xe4 waiting on condition 
JNI global references: 1928
 {code}
 

Environment:

Flink: 1.15.2

Flink-kubernetes-operator: 1.3.1

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-06 09:41:33.0,,,,,,,,,,"0|z1mlbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Job stuck in suspend state after losing leadership in HA Mode,FLINK-34007,13563759,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,ZhenqiuHuang,ZhenqiuHuang,06/Jan/24 03:35,05/Feb/24 22:10,04/Jun/24 20:40,05/Feb/24 22:10,1.18.1,1.18.2,1.19.0,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"The observation is that Job manager goes to suspend state with a failed container not able to register itself to resource manager after timeout.

JM Log, see attached

 ",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34333,,FLINK-34243,,,,,,,,,,FLINK-34343,"10/Jan/24 20:02;ZhenqiuHuang;Debug.log;https://issues.apache.org/jira/secure/attachment/13065881/Debug.log","16/Jan/24 03:21;ZhenqiuHuang;LeaderElector-Debug.json;https://issues.apache.org/jira/secure/attachment/13066005/LeaderElector-Debug.json","09/Jan/24 20:55;ZhenqiuHuang;job-manager.log;https://issues.apache.org/jira/secure/attachment/13065848/job-manager.log",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 22:10:10 UTC 2024,,,,,,,,,,"0|z1ml9s:",9223372036854775807,Fixes a bug where the leader election wasn't able to pick up leadership again after renewing the lease token caused a leadership loss. This required fabric8io:kubernetes-client to be upgraded from v6.6.2 to v6.9.0.,,,,,,,,,,,,,,,,,,,"09/Jan/24 18:45;ZhenqiuHuang;From initial investigation, the job manager is initially lose the leadership, then goes to SUSPENDED status. Shouldn't the job manager exit directly rather than goes to SUSPENDED status?

 ;;;","10/Jan/24 13:39;mapohl;Thanks for sharing this issue with us, [~ZhenqiuHuang]. In an offline conversation with [~gyfora] it was said that you observed this issue not only with Flink 1.18 but also 1.17 and 1.16. Can you confirm that?;;;","10/Jan/24 13:42;gyfora;Yes we can confirm [~mapohl] ;;;","10/Jan/24 14:12;mapohl;Ok, I went through the log file you shared. AFAIS, suspending the JobManager worked as expected:
* The Job with the ID {{217cee964b2cfdc3115fb74cac0ec550}} was suspended due to the leadership loss for session ID {{9987190b-35f4-4238-b317-057dc3615e4d}}.
* The ResourceManager and the Dispatcher got their leadership revoked as well.
* The ResourceManager is not shut down. 
* The Dispatcher is stopped but the corresponding DispatcherLeaderProcess keeps running. That's the process that should trigger another Dispatcher initialization if it picks up leadership again.

The {{RecipientUnreachableException}} appears because there's no leader being re-elected, I guess. Does this match your findings?

You're not having any other standby JM running in the Flink cluster as far as I understand? We would expect this very same JobManager to pick up leadership again. Do we have some logs from the Kubernetes cluster that we could investigate?;;;","10/Jan/24 15:53;ZhenqiuHuang;I have been investigating the io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector. Looks like there is a bug in the version 5.12.4, when the leader lose leadership, the loop executor will be shutdown. That is why the leader is not acquired again. Thus, bump the kubernetest-client version will probably help.

There is a fix in this PR.
https://github.com/fabric8io/kubernetes-client/commit/042c77b360e77dfaac4ae713518b684dcd0d985b;;;","10/Jan/24 16:30;mapohl;The fix you shared ended up in 6.0.0. That would mean that we shouldn't experience the issue in a Flink 1.18 cluster. The fabric8 client was updated to 6.6.2 in 1.18.0 (FLINK-31997).;;;","10/Jan/24 21:02;ZhenqiuHuang;[~mapohl]
Yes, I mistakenly looked into the flink 1.17 source code. I uploaded another debug log above. The KubernetesLeaderElector check the annotation ""control-plane.alpha.kubernetes.io/leader"" and whether the lockIdentity exists in content. Given this job only has 1 job manager, there should be no other job manager instance try to acquire the lock. The only possibility is that somehow the cluster config map is returned incorrectly.

In this case, even fabric8 LeaderElector will continue to try to acquire leadership (If it can get without exceed deadline), flink will not able to restart services (such RM and dispatcher) as DefaultLeaderRetrievalService is stopped also. To resolve the issue for now, should we focus on gracefully shutdown Job Manager rather than move job to Suspended status?  
;;;","11/Jan/24 07:36;mapohl;I still don't fully understand the error you shared: Shouldn't the KubernetesClientException resolve itself because the logic runs in a loop? Is this stacktrace you shared only a one-time thing or does it reoccur (which would confirm the execution in the loop and indicate that the ConfigMap is in some odd state)? Another thing I'm wondering is why the ConfigMap was concurrently updated (which caused the KubernetesClientException as far as I understand) when there's only one JM running. Are there other processes accessing the ConfigMap?

{quote}
[...] flink will not able to restart services (such RM and dispatcher) as DefaultLeaderRetrievalService is stopped also [...]
{quote}
The DefaultLeaderRetrievalService is not in charge of restarting any services. The LeaderElectionService will trigger the restart of any shut down services (in our case the SessionDispatcherLeaderProcess which would be started by the DefaultDispatcherRunner; the latter one maintains the Dispatcher's leader election) as soon as the JobManager gets the leadership again.;;;","11/Jan/24 10:15;wangyang0918;Do you mean the {{KubernetesLeaderElector}} could not obtain the leadership due to continuous resource conflicts? I am not sure of this because you only share one line DEBUG log.;;;","11/Jan/24 10:38;dmvk;If there would have been a continuous conflict, I'd assume the job would fail sooner.

Can you check whether you're not rate-limited by the k8s apiserver? We've seen similar issues in the past. The ""multicomponent leader election"" was one efforts to lower the pressure there, do you have it enabled?;;;","11/Jan/24 12:02;mapohl;fyi: multi-component leader election is enabled/used by default since 1.16 (FLINK-25806). So, all the deployments [~ZhenqiuHuang] talked about should have been using multi-component leader election.;;;","12/Jan/24 04:02;wangyang0918;[~mapohl] Could you please confirm that whether ""multi-component leader election"" will clean up the leader annotation on the ConfigMap when lost leadership?

It seems that the fabric8 Kubernetes client leader elector will not work properly by {{run()}} more than once if we do not clean up the leader annotation.;;;","12/Jan/24 09:19;mapohl;I couldn't find anything related to ConfigMap cleanup being triggered during leadership loss in the Flink code (Flink's [KubernetesLeaderElector:82|https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L82] sets up the [callback for the leadership loss|https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L124] which is implemented by [KubernetesLeaderElectionDriver#LeaderCallbackHandlerImpl|https://github.com/apache/flink/blob/11259ef52466889157ef473f422ecced72bab169/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L212]). This behavior is on-par with the old (i.e. pre-FLINK-24038) version of the Flink 1.15 codebase (see 1.15 class [KubernetesLeaderElectionDriver:202|https://github.com/apache/flink/blob/bba7c417217be878fffb12efedeac50dec2a7459/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L202]).

That's also not something I would expect. It should be handled by the LeaderElector, instead, because the LeaderElector knows the state of the leader election and can trigger a clean up before the new leader information is written to the ConfigMap entry. Flink shouldn't trigger a clean up because it doesn't know whether a new leader was already elected (in which case cleaning up the ConfigMap entry would result in losing the leadership information of the new leader). And the Flink process wouldn't be able to clean it up, anyway, because the process isn't the leader anymore. Or am I missing something here?

On another note: I came across [this change|https://github.com/apache/flink/pull/22540/files#diff-0e859df42954459619211d2ec60957742b24c9fc6ce55526616fddc540f0f8ffL59-R60] in the FLINK-31997 PR (k8s client update to 6.6.2): We're changing the thread pool size from 1 to 3 essentially allowing the same internal LeaderElector being executed multiple times (because we trigger another {{KubernetesLeaderElector#run}} call when the leadership is revoked). The old version of the code uses a single thread which would mean that the run call would get queued until the previous {{LeaderElector#run}} failed for whatever reason. That change sounds strange but shouldn't be the cause of this Jira issue because the change only went into 1.18 and we're experiencing this also in older versions of Flink.;;;","14/Jan/24 00:23;ZhenqiuHuang;[~mapohl]
Yes, from the observation on the failure case, the ConfigMap was not cleanup when job manager lose the leadership. Even the renewTime field is no longer upgraded by leader elector, it means leader elector already goes out of its run loop. If look into the fabric8 leader elector source code, it looks like only when renew deadline expired, LeaderElector will abort from its run loop. Even through I don't know why  renew deadline expired, enlarge the high-availability.kubernetes.leader-election.renew-deadline value could isolate some transient issues. 

I have started a testing job with debug log of both io.fabric8.kubernetes.client.extended.leaderelection and flink kubernetes leader election modules two days ago. If the job fail, I will post new logs in this thread.

[~wangyang0918]
Would you please elaborate a little bit why ""It seems that the fabric8 Kubernetes client leader elector will not work properly by run() more than once if we do not clean up the leader annotation.""?

;;;","15/Jan/24 02:50;wangyang0918;Maybe I did not make myself clear. I mean the old leader JM should try to remove the annotation of HA ConfigMap {{control-plane.alpha.kubernetes.io/leader}} when lost leadership. From the fabric8 K8s client impl[1], {{isLeader}} callback will be executed only when the holder identity changed.

 

[1]. [https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232];;;","15/Jan/24 08:56;mapohl;Sorry, I misunderstood you initially. If the annotation is the problem, I would assume the issue being somewhere within the k8s client library. The annotation was never touched by the Flink code (there's only one read access in [KubernetesLeaderElector#hasLeadership|https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L115] to verify the leadership). ...even before the changes of FLINK-24038. The lifecycle of the annotation is handled within the fabric8.io client's {{{}LeaderElector{}}}. I'm gonna do some more investigation of that code segment. But having more debug k8s/kubernetes-client logs would help, I guess.;;;","15/Jan/24 09:34;wangyang0918;Maybe this issue is related with fabric8 K8s client behavior change. In v5.12.4[1], the {{onStartLeading}} callback will always be executed when acquired the leadership. But in v6.6.2[2], the {{onStartingLeading}} callback will be executed only when the holder identity changed. In this case, the JobManager does not crash while the holder identity keeps same.

 

[1]. [https://github.com/fabric8io/kubernetes-client/blob/v5.12.4/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L71]

[2]. [https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232]

 ;;;","15/Jan/24 11:21;mapohl;But it should be an issue that is available in different k8s client version (not only 6.6.2):
||Flink||k8s client||Jira issue||
|1.18|6.6.2|FLINK-31997|
|1.17|5.12.4|FLINK-30231|
|1.16|5.12.3|FLINK-28481|
|1.14-1.15|5.5.0|FLINK-22802|

At least based on the reports of this Jira issue, there must have been an incident (which caused the lease to not be renewed) in a k8s cluster that triggered the same failure in multiple Flink clusters (with versions of 1.18, 1.17 and 1.16 at least) that triggered the same issue in all of those deployments. ...if I understand it correctly.

Therefore, the issue should exist in the entire version range [5.12.3, 6.6.2]. But the change you're referring to ([PR #4152|https://github.com/fabric8io/kubernetes-client/pull/4125]) seems to be the only bigger change in {{{}LeaderElector{}}}, indeed.

—

On another note: I remembered that there is a slight difference in the revocation protocol in the [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box] changes:
 * The old implementation (see [1.15 DefaultLeaderElectionService:238|https://github.com/apache/flink/blob/6e1caa390882996bf2d602951b54e4bb2d9c90dc/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L238]) did try to clear the leader information from the ConfigMap.
 * The new implementation (see [1.18+ DefaultLeaderElectionService:484|https://github.com/apache/flink/blob/773feebbb2426ab1a8f7684f59b9a73db8f6a613/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L484]) doesn't clear the component leader information, anymore. Here, the reasoning was that the data wouldn't be able to be updated, anymore, because the leadership is already lost.

But that change still seems to be reasonable based on my findings: In the k8s client 6.6.2 codebase, {{stopLeading}} is either called after noticing the change in the lock identity ([LeaderElector:L238|https://github.com/fabric8io/kubernetes-client/blob/f91e0bd8e364f9a3758af0b90b9c661d0fc0a9eb/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L238]; the lock identity change would prevent the clearing of the data) or when the lease wasn't renewed ([LeaderElector:95|https://github.com/fabric8io/kubernetes-client/blob/f91e0bd8e364f9a3758af0b90b9c661d0fc0a9eb/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L95]) where we would have to assume that other leader information is already written. And this change shouldn't be related to the issues with the lock lifecycle in general because it only affects metadata and not the lock annotation itself, should it? WDYT?;;;","15/Jan/24 12:33;mapohl;{quote}
At least based on the reports of this Jira issue, there must have been an incident (which caused the lease to not be renewed) in a k8s cluster that triggered the same failure in multiple Flink clusters (with versions of 1.18, 1.17 and 1.16 at least) that triggered the same issue in all of those deployments. ...if I understand it correctly.
{quote}
[~ZhenqiuHuang] can you elaborate a bit on the incident itself. ...just to get a bit more context. Did I understand it correctly that there were different Flink versions deployed in a single Kubernetes cluster which run independently and all of them ran into the same issue around the same time (indicating that would have been caused by the same event). Or did the failures in the different Flink clusters happen independently from each other over a longer stretch of time?;;;","16/Jan/24 02:13;wangyang0918;{quote}At least based on the reports of this Jira issue, there must have been an incident (which caused the lease to not be renewed)
{quote}
I am afraid we could not get this conclusion before we have the K8s APIServer audit logs to verify that the lease annotation did not get renewed. Because it could also happen that the lease annotation get renewed normally while the onStartLeading callback is not executed somehow. 

 
{quote}Therefore, the issue should exist in the entire version range [5.12.3, 6.6.2].
{quote}
If this issue only happened in the Flink 1.18, then it should be related with the fabric8 K8s client 6.6.2 behavior change. Otherwise, we still have not find the root cause.

 

You are right. The slight difference in the revocation protocol in the [FLIP-285|https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box] changes about clear the leader information in ConfigMap is not related with this issue.

 

BTW, if we know how to reproduce this issue, it will be easier to find the root cause. Because we might also need the K8s APIServer audit log to do some deep analysis.;;;","16/Jan/24 03:34;ZhenqiuHuang;[~mapohl] [~wangyang0918]
I am intensively testing flink 1.18. Within two days, there are users reported the job manager stuck issue in 1.17 and 1.16. 1.18 and 1.17 job instances are running in the same cluster. 1.16 is in different cluster.

I attached another LeaderElector-Debug.json file that contains debug log of a flink 1.18 app. The issue happened several times:
1. due to the configmap not accessible from api sever then renew timeout exceeded. 
2. a failure on patch on a updated configmap


The interesting part of the behavior of last several days is that job manager was not stuck but exit directly. Then, new job manager pod started correctly that is why new leader is selected in the log above. Hopefully, it is useful for your diagnosis.


[~wangyang0918]
From my initial observation (before creating the jira), the leader annotation update stopped when job manager was stuck. 




;;;","16/Jan/24 13:01;mapohl;The timeout of the renew operation leads to a {{stopLeading}} call in [LeaderElector:96|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L119] which results in LeaderElector being stopped (the {{stopped}} flag is set to true in [LeaderElector:119|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L119] and is never disabled again). Any later call of {{tryAcquireOrRenew}} will not perform any action because of the {{stopped}} flag being true (see [LeaderElector#tryAcquireOrRenew:211|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L211]).

Flink doesn't re-instantiate the {{LeaderElector}} but calls {{LeaderElector#run}} on the same (now stopped) instance in the [KubernetesLeaderElectionDriver#notLeader|https://github.com/apache/flink/blob/11259ef52466889157ef473f422ecced72bab169/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L214] callback.

That explains the behavior in the Flink 1.18 deployments, if I didn't miss anything. It sounds like a bug in the fabric8io k8s client implementation which we should be able to workaround by recreating a new LeaderElector on leadership loss. I replied to [fabric8io:kubernetes-client#5635|https://github.com/fabric8io/kubernetes-client/issues/5635] because the bug report provides the same stacktrace. That also explains why restarting the JobManager fixes the issue. Because that results in a new (not-stopped) LeaderElector being instantiated.

I'm still puzzled about the issues in other Flink versions, though.;;;","17/Jan/24 04:10;wangyang0918;I think you are right that {{stopped}} flag in {{LeaderElector}} never reset after lost leadership. However, I am afraid even though we recreate a new LeaderElector it still not works unless we have a different {{{}HolderIdentity{}}}. From [LeaderElector:232|[https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232]], {{onStartLeading}} only happens when the holder identity changes.;;;","17/Jan/24 07:47;mapohl;good point, a the holder identity would have to be reset as well.

I'm also wondering whether we should enable [isReleaseOnCancel()|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L124]. That config property was never used in the kubernetes-client v5.12.4 codebase and seems to be disabled by default. We're not enabling it in [KubernetesLeaderElector:69|https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L69] on the Flink side. Releasing the lock identify might help in certain situation where we don't want to wait for the lease to timeout. But that's just a minor improvement that I came across while looking through the v6.6.2 code.

[~ZhenqiuHuang] But that finding still doesn't explain that you're seeing the same/similar issues with Flink 1.17- deployments. Because the update of the kubernetes-client to v6.6.2 only happened in 1.18.0.;;;","17/Jan/24 11:35;wangyang0918;I agree with you that we could enable {{{}isReleaseOnCancel{}}}, which will set the holder identity to empty.

 

If this issue also existed in 1.17-, then we might need to get the jstack of JobManager to see where the {{LeaderElector}} stuck.;;;","18/Jan/24 14:28;mapohl;I checked the implementation (since we're getting close to the 1.19 feature freeze). We have the following options:
 # We could downgrade the fabric8io kubernetes client dependency back to v5.12.4 (essentially reverting FLINK-31997).
 # We could fix the issue in the fabric8io kubernetes client and update the dependency as soon as the fix is released. Here, I'm also not that confident that we would be able to bring the fix into Flink before 1.19 should be released. ...because we're relying on the release of another project.
 # Refactor the k8s implementation to allow the restart of the KubernetesLeaderElector within the KubernetesLeaderElectionDriver. That would require updating the lockIdentify as well. The problem is that the lockIdentity is actually not owned by the KubernetesLeaderElector but by the k8s HighAvailabilityServices (because it's also used by the KubernetesStateHandleStore when checking the leadership). Even though moving the lockIdentity into the KubernetesLeaderElector makes sense (the KubernetesStateHandleStore should rely on the LeaderElectionService to detect whether leadership is acquired, instead), it is a larger effort and I am hesitant to work on that one that closely to the 1.19 feature freeze.

I feel like we should apply all the three options in the above order. Option #1 would end up in 1.19.0 and 1.18.3 with option #2 being the follow-up. Option #3 could be considered as a dedicated refactoring effort in 1.20 or later. What's your view on that?

My proposal only covers the issue we identified in k8s client v6.6.2. I ignored the fact that there are issues observed in deployments with Flink 1.17-. Any investigation around Flink 1.17- leadership issues should be moved into a separate Jira issue.;;;","18/Jan/24 14:42;gyfora;The fabric8 community is super response and agile so fixing bugs and getting a release is not an issue. If we feel that there is a bug there we should report it immediately and it may get fixed in a few days.

I personally don't like the idea of reverting to the old fabric8 version. It has a few very serious issues and this can also cause issues for dependent projects like the Kubernetes operator.;;;","18/Jan/24 14:53;mapohl;Fair enough. Or we could come up with a Flink-local implementation of the fabric8io LeaderElector including a possible fix, if a release of the fabric8io kubernetes client takes too long.

I raised the priority for the issue to Blocker for now. I guess we should get this one fixed before the 1.19 release. Additionally, I removed the affected versions 1.17 and 1.16 ([~ZhenqiuHuang] please create another Jira issue if we have more insights into what's causing the leader election issues in the Flink 1.17-);;;","18/Jan/24 18:03;ZhenqiuHuang;[~mapohl] Ack. There are no new observations from last 2 days' testing result. The only thing that probably worth to mention is that when the LeaderElector (3 thread executor) exit from renew deadline out, it is actually one of the thread exit from the loop. From the debug log, I can still observe 2 thread consistently failed to acquire the leadership due to it the stop flag.


For the 1.17, I will create an instance for testing in the same cluster today. Let's see what's the result.;;;","19/Jan/24 03:15;wangyang0918;I also remember that the fabric8 kubernetes-client community has a very good response. If the {{LeaderElector}} is designed for only run once, though I do not think this is the reasonable behavior, then we need to create a new {{LeaderElector}} when lost leadership.

For option #3, it might be unnecessary because {{LeaderElector}} could work as expected when creating a new instance with same lock identity. It is a larger effort to do such refactor without additional benefits.

 

BTW, maybe I miss some background. [~gyfora] Could you please share me why we need to change the thread pool to 3 in {{{}KubernetesLeaderElector{}}}?;;;","19/Jan/24 06:41;gyfora;[~wangyang0918] the tests failed. The executor service (single threaded) previously was only used to execute Flink side logic and now we had to pass it to the LeaderElector itself as well so a single thread kind of deadlocked it somehow.

So I increased to 3 and it made it work. Yesterday I started to think that it may actually be a reason why we see ConfigMap version conficts (and lost leaderships) in the first place.

This is probably unrelated to why it cannot recover the leadership but I am going to try to change back to 1 or use 2 different single threaded executors.;;;","20/Jan/24 06:11;mapohl;Increasing the thread count appeared to be necessary because the old Flink code executed the fabric8io {{LeaderElector#run}} command in [KubernetesLeaderElector#run|https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L103] in the same thread pool that is used in the fabric8io LeaderElector for running the CompetableFutures in the loop:

The {{LeaderElector#run}} call waits for the CompletableFuture returned by {{LeaderElector#acquire}} to complete (see [LeaderElector:70|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L70]). {{LeaderElector#acquire}} will trigger an asynchronous call on the {{executorService}} which wouldn't pick up the task because the single thread is waiting for the acquire call to complete. This deadlock situation is reproduced by Flink's {{{}KubernetesLeaderElectionITCase{}}}. I would imagine that this is the timeout, [~gyfora] was experiencing when upgrading to v6.6.2.

The 3 threads that were introduced by FLINK-31997 shouldn't have caused any issues because the LeaderElector only writes to the ConfigMap in [LeaderElector#tryAcquireOrRenew|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L210] and [LeaderElector#release|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L147]. Both methods are synchronized. Hence, they shouldn't cause a race condition in any way as far as I can see.

I created a [draft PR with a fix|https://github.com/apache/flink/pull/24132] that recreates the fabric8io {{LeaderElector}} instead of reusing it. The fix also covers reverts the thread pool size from 3 to 1 with {{KubernetesLeaderElectionITCase}} passing again. I still have to think of a way to test Flink's KubernetesLeaderElector on the issue that caused the failure of this Jira. The only way I could think of is doing something similar to what's done in the [fabric8io codebase|https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/test/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElectorTest.java#L63] with Mockito. Any other ideas are appreciated. I would like to avoid Mockito.;;;","23/Jan/24 09:55;wangyang0918;It seems that the fabric8 K8s client community leans towards to make the {{LeaderElector}} non-restartable. And using a same lock identity should also work for us when creating a new leader-elector instance. So your PR looks good to me.

 

About the test:

We just need to have a test to guard that a new leader-elector instance should be created instead of reusing the existing one. Right?;;;","23/Jan/24 17:01;mapohl;Just checking whether a new instance of {{LeaderElector}} was created is tricky because {{LeaderElector}} is coupled with the k8s backend. We would either have to mock {{LeaderElector}} or have to work with some backend to work properly within Flink's {{{}KubernetesLeaderElector{}}}. Anyway, I came up with a quite straight-forward ITCase. I added it to the PR which is ready to be reviewed now.;;;","01/Feb/24 09:45;mapohl;* master
** [95417a4857ec87a349c0fa9f4d3951f7d3807844|https://github.com/apache/flink/commit/95417a4857ec87a349c0fa9f4d3951f7d3807844]
** [927972ff4ad6252fd933fcc627c7d95dbbdae431|https://github.com/apache/flink/commit/927972ff4ad6252fd933fcc627c7d95dbbdae431]

* 1.18: will be handled in FLINK-34333;;;","01/Feb/24 09:51;yunta;It seems we have had a long discussion, does this problem also exist in Flink-1.17? [~wangyang0918] [~mapohl];;;","01/Feb/24 10:12;mapohl;Hi [~yunta] we're not aware of any issues related to FLINK-34007 in Flink 1.17. The issue started to appear with the upgrade of the k8s client dependency to v6.6.2 in FLINK-31997 (which ended up in Flink 1.18).

That being said, [~ZhenqiuHuang] reported similar errors in Flink 1.17 and 1.16 deployments as well which we cannot explain. We were not able to investigate the cause due to missing logs. We agreed to cover any other problems in a separate Jira issue if [~ZhenqiuHuang] comes up with new information (see his comment above).;;;","02/Feb/24 07:43;mapohl;I'm reopening the issue because we're seeing test instabilities now:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57203&view=logs&j=64debf87-ecdb-5aef-788d-8720d341b5cb&t=2302fb98-0839-5df2-3354-bbae636f81a7&l=8066]
 * [https://github.com/XComp/flink/actions/runs/7745486791/job/21121947504#step:14:7309] (this one happened on the FLINK-34333 1.18 backport which covers the same change);;;","02/Feb/24 11:33;mapohl;The Azure test failure seems to be unrelated. I created FLINK-34343 to cover this one separately. The GitHub Actions failures is still concerning, though.;;;","05/Feb/24 07:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57270&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&t=a69a379d-ca44-5937-4e62-0ce084a23679&l=7935;;;","05/Feb/24 22:10;mapohl;* master
** [79cccd7103a304bfa07104dcafd1f65a032c88ce|https://github.com/apache/flink/commit/79cccd7103a304bfa07104dcafd1f65a032c88ce]
** [95417a4857ec87a349c0fa9f4d3951f7d3807844|https://github.com/apache/flink/commit/95417a4857ec87a349c0fa9f4d3951f7d3807844]
** [927972ff4ad6252fd933fcc627c7d95dbbdae431|https://github.com/apache/flink/commit/927972ff4ad6252fd933fcc627c7d95dbbdae431]
* 1.18: will be handled in FLINK-34333;;;"
Flink terminates the execution of an application when there is a network problem between TaskManagers,FLINK-34006,13563753,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sophia1997,sophia1997,05/Jan/24 23:29,05/Jan/24 23:29,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,Runtime / Checkpointing,Runtime / Task,,,0,,,,"Flink terminates an application when two TaskManager are disconnected although there are enough resources in the cluster to run the application and we use checkpoint restart.

 

We deploy Flink v(1.17.1) on a cluster of six nodes with Ubuntu 18.04, the cluster consists of a JobManager and five TaskManagers. We use Flink's Standalone resource manager. We set the number of slots per TaskManager to one, and submit a WordCount application with a level of parallelism equal to three. We enable Flink checkpointing and restart failover strategy to attempt a restart in case of failure three times before termination and the time between attempts to 10 seconds.

The application starts running on the first 3 TaskManager.

If the communication is broken between two of the TaskManager that run the application, the job fails, and the JobManager tries to restart the job again. When the job fails the resources on the TaskManager are free. When the JobManager restarts the job, it selects the same three TaskManager it choose in the first attempt, and the job fails again. After three trials, Flink terminates the job with an exception: Connecting to remote task manager has failed.

 

These are the JobManager Configurations:
 * taskmanager.numberOfTaskSlots: 1
 * Enable checkpointing: --checkpointing
 * execution.checkpointing.interval: 3min
 * Enabling restart failover strategy
 * restart-strategy.type: fixed-delay
 * restart-strategy.fixed-delay.attempts: 3
 * restart-strategy.fixed-delay.delay: 10 s

 

command: ./bin/flink run -p 3 examples/streaming/WordCount.jar --checkpointing --input ~/flink/alice.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2024-01-05 23:29:22.0,,,,,,,,,,"0|z1ml8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for MiniBatchAssigner node,FLINK-34005,13563738,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,05/Jan/24 19:46,11/Jan/24 09:54,04/Jun/24 20:40,11/Jan/24 09:54,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 09:54:29 UTC 2024,,,,,,,,,,"0|z1ml54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 09:54;dwysakowicz;Implemented in 881062f352f8bf8c21ab7cbea95e111fd82fdf20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestingCheckpointIDCounter can easily lead to NPEs,FLINK-34004,13563714,,Technical Debt,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,chesnay,chesnay,chesnay,05/Jan/24 15:07,14/Mar/24 07:36,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,,Tests,,,,0,pull-request-available,,,The TestingCheckpointIDCounter builder doesn't define safe defaults for all builder parameters. Using it can easily lead to surprising null pointer exceptions in tests when code is being modified to call more methods.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-05 15:07:45.0,,,,,,,,,,"0|z1mkzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump CI flink version on flink-connector-hbase,FLINK-34003,13563713,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,martijnvisser,martijnvisser,martijnvisser,05/Jan/24 14:39,06/Mar/24 19:09,04/Jun/24 20:40,06/Mar/24 19:09,,,,,,,,,,,,,Connectors / HBase,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33440,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-05 14:39:54.0,,,,,,,,,,"0|z1mkzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump CI flink version on flink-connector-elasticsearch,FLINK-34002,13563712,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,05/Jan/24 14:33,16/Jan/24 09:40,04/Jun/24 20:40,16/Jan/24 09:40,,,,,,,,,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 09:40:25 UTC 2024,,,,,,,,,,"0|z1mkzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 09:40;martijnvisser;Fixed in apache/flink-connector-elasticsearch:main 52f2afdda90d4d8881d64e6d13c8c8f8b94eb922;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ambiguous document description towards the default value for configuring operator-level state TTL,FLINK-34001,13563694,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,luca.yang,luca.yang,05/Jan/24 10:02,08/Jan/24 13:55,04/Jun/24 20:40,08/Jan/24 13:55,1.18.0,,,,,,,,1.19.0,,,,Documentation,Table SQL / API,,,0,pull-request-available,,,"doc:

https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/table/concepts/overview/#idle-state-retention-time

The current TTL value for both left and right side is {{{}""0 ms""{}}}, which means the state retention is not enabled. 

 

but i test find :

The current TTL value for both left and right side is {{{}""0 ms""{}}}, which means the state is permanence keep!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31957,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 13:55:22 UTC 2024,,,,,,,,,,"0|z1mkvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 10:03;luca.yang;{code:java}
//代码占位符
package com.yy.state.OperatorStateTTL

import org.apache.flink.configuration.{Configuration, RestOptions}
import org.apache.flink.runtime.state.filesystem.FsStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.table.api.bridge.scala.{StreamStatementSet, StreamTableEnvironment}

import java.time.ZoneId
import scala.util.Random

/**
 * 看下 EXECUTE PLAN 和 statementset 怎么结合
 * 适配flink web ui 通过打包为jar提交flink任务 报错: Cannot have more than one execute() or executeAsync() call in a single environment
 * idea执行没有问题
 * 参考: https://blog.csdn.net/tianlangstudio/article/details/123086300
 */
object TTLDemoV2 {
  def main(args: Array[String]): Unit = {
    val conf = new Configuration
    conf.setInteger(RestOptions.PORT, 28080)

    // 从指定的checkpoint启动 不配置则无状态启动
    //    conf.setString(""execution.savepoint.path"",""file:///Users/thomas990p/checkpointdir/41f86441111ad4492002188d8d4e1009/chk-136"")

    //  不用local 使用下面的方式 就可以使用多个 execute sql 且都可以生效
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    // 本地任务停止时，保留 checkpoint 数据
    env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)

    env.setParallelism(1)

    env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE)
    env.setStateBackend(new FsStateBackend(""file:///Users/thomas990p/checkpointdir""))
    env.disableOperatorChaining() // 禁用全局任务链

    val tEnv: StreamTableEnvironment = StreamTableEnvironment.create(env)

    // 指定国内时区
    tEnv.getConfig.setLocalTimeZone(ZoneId.of(""Asia/Shanghai""))


    tEnv.executeSql(
      """"""
        |CREATE TABLE s1 (
        |  id String
        |  ,name string
        |) WITH (
        |  'connector' = 'kafka',
        |  'topic' = 's1',
        |  'scan.startup.mode' = 'latest-offset',
        |  'properties.group.id' = 'g1',
        |  'properties.bootstrap.servers' = 'localhost:9092',
        |  'format' = 'csv'
        |)
        |"""""".stripMargin)

    /*
    不允许配置: 'scan.startup.mode' = 'latest-offset' 否则报错:
    Unsupported options found for 'upsert-kafka'.  他必须从earliest消费 无法修改
    */
    tEnv.executeSql(
      """"""
        |CREATE TABLE s2 (
        |  id String
        |  ,age int
        |  ,primary key(id) not enforced
        |) WITH (
        |  'connector' = 'upsert-kafka',
        |  'topic' = 's3',
        |  'properties.bootstrap.servers' = 'localhost:9092',
        |  'key.format' = 'csv'
        |  ,'value.format' = 'csv'
        |)
        |"""""".stripMargin)

    tEnv.executeSql(
      """"""
        |CREATE TABLE s1_sink1 (
        |  id String
        |  ,name string
        |) WITH (
        |  'connector' = 'print'
        |  ,'print-identifier'='s1 sink>>>>'
        |)
        |"""""".stripMargin)
    tEnv.executeSql(
      """"""
        |CREATE TABLE s2_sink1 (
        |  id String
        |  ,age int
        |) WITH (
        |  'connector' = 'print'
        |  ,'print-identifier'='s2 sink>>>>'
        |)
        |"""""".stripMargin)

    tEnv.executeSql(
      """"""
        |CREATE TABLE sink1 (
        |  id String
        |  ,name string
        |  ,age int
        |) WITH (
        |  'connector' = 'print'
        |  ,'print-identifier'='>>>>'
        |)
        |"""""".stripMargin)


    tEnv.executeSql(""insert into s1_sink1 select * from s1"")
    tEnv.executeSql(""insert into s2_sink1 select * from s2"")
    tEnv.executeSql(""EXECUTE PLAN 'file:///Users/thomas990p/flink-plain/plan1.json' "") // two 0ms keep left and right state forever 



  }

}
 {code};;;","05/Jan/24 10:04;luca.yang;{code:java}
//代码占位符
// plan1.json

{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 7,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`s1`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""id"",
              ""dataType"" : ""VARCHAR(2147483647)""
            }, {
              ""name"" : ""name"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""properties.bootstrap.servers"" : ""localhost:9092"",
            ""connector"" : ""kafka"",
            ""format"" : ""csv"",
            ""topic"" : ""s1"",
            ""properties.group.id"" : ""g1"",
            ""scan.startup.mode"" : ""latest-offset""
          }
        }
      }
    },
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647), `name` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, s1]], fields=[id, name])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 8,
    ""type"" : ""stream-exec-exchange_1"",
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""HASH"",
        ""keys"" : [ 0 ]
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647), `name` VARCHAR(2147483647)>"",
    ""description"" : ""Exchange(distribution=[hash[id]])""
  }, {
    ""id"" : 9,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`s2`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""id"",
              ""dataType"" : ""VARCHAR(2147483647) NOT NULL""
            }, {
              ""name"" : ""age"",
              ""dataType"" : ""INT""
            } ],
            ""watermarkSpecs"" : [ ],
            ""primaryKey"" : {
              ""name"" : ""PK_3386"",
              ""type"" : ""PRIMARY_KEY"",
              ""columns"" : [ ""id"" ]
            }
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""properties.bootstrap.servers"" : ""localhost:9092"",
            ""key.format"" : ""csv"",
            ""topic"" : ""s3"",
            ""connector"" : ""upsert-kafka"",
            ""value.format"" : ""csv""
          }
        }
      }
    },
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647) NOT NULL, `age` INT>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, s2]], fields=[id, age])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 10,
    ""type"" : ""stream-exec-exchange_1"",
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""HASH"",
        ""keys"" : [ 0 ]
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647) NOT NULL, `age` INT>"",
    ""description"" : ""Exchange(distribution=[hash[id]])""
  }, {
    ""id"" : 11,
    ""type"" : ""stream-exec-changelog-normalize_1"",
    ""configuration"" : {
      ""table.exec.mini-batch.enabled"" : ""false"",
      ""table.exec.mini-batch.size"" : ""-1""
    },
    ""uniqueKeys"" : [ 0 ],
    ""generateUpdateBefore"" : true,
    ""state"" : [ {
      ""index"" : 0,
      ""ttl"" : ""0 ms"",
      ""name"" : ""changelogNormalizeState""
    } ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647) NOT NULL, `age` INT>"",
    ""description"" : ""ChangelogNormalize(key=[id])""
  }, {
    ""id"" : 12,
    ""type"" : ""stream-exec-exchange_1"",
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""HASH"",
        ""keys"" : [ 0 ]
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647) NOT NULL, `age` INT>"",
    ""description"" : ""Exchange(distribution=[hash[id]])""
  }, {
    ""id"" : 13,
    ""type"" : ""stream-exec-join_1"",
    ""joinSpec"" : {
      ""joinType"" : ""INNER"",
      ""leftKeys"" : [ 0 ],
      ""rightKeys"" : [ 0 ],
      ""filterNulls"" : [ true ],
      ""nonEquiCondition"" : null
    },
    ""rightUpsertKeys"" : [ [ 0 ] ],
    ""state"" : [ {
      ""index"" : 0,
      ""ttl"" : ""0 ms"",
      ""name"" : ""leftState""
    }, {
      ""index"" : 1,
      ""ttl"" : ""0 ms"",
      ""name"" : ""rightState""
    } ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    }, {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647), `name` VARCHAR(2147483647), `id0` VARCHAR(2147483647) NOT NULL, `age` INT>"",
    ""description"" : ""Join(joinType=[InnerJoin], where=[(id = id0)], select=[id, name, id0, age], leftInputSpec=[NoUniqueKey], rightInputSpec=[JoinKeyContainsUniqueKey])""
  }, {
    ""id"" : 14,
    ""type"" : ""stream-exec-calc_1"",
    ""projection"" : [ {
      ""kind"" : ""INPUT_REF"",
      ""inputIndex"" : 0,
      ""type"" : ""VARCHAR(2147483647)""
    }, {
      ""kind"" : ""INPUT_REF"",
      ""inputIndex"" : 1,
      ""type"" : ""VARCHAR(2147483647)""
    }, {
      ""kind"" : ""INPUT_REF"",
      ""inputIndex"" : 3,
      ""type"" : ""INT""
    } ],
    ""condition"" : null,
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647), `name` VARCHAR(2147483647), `age` INT>"",
    ""description"" : ""Calc(select=[id, name, age])""
  }, {
    ""id"" : 15,
    ""type"" : ""stream-exec-sink_1"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.rowtime-inserter"" : ""ENABLED"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`sink1`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""id"",
              ""dataType"" : ""VARCHAR(2147483647)""
            }, {
              ""name"" : ""name"",
              ""dataType"" : ""VARCHAR(2147483647)""
            }, {
              ""name"" : ""age"",
              ""dataType"" : ""INT""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""print"",
            ""print-identifier"" : "">>>>""
          }
        }
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"", ""UPDATE_BEFORE"", ""UPDATE_AFTER"", ""DELETE"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`id` VARCHAR(2147483647), `name` VARCHAR(2147483647), `age` INT>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.sink1], fields=[id, name, age])""
  } ],
  ""edges"" : [ {
    ""source"" : 7,
    ""target"" : 8,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 9,
    ""target"" : 10,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 10,
    ""target"" : 11,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 11,
    ""target"" : 12,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 8,
    ""target"" : 13,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 12,
    ""target"" : 13,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 13,
    ""target"" : 14,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  }, {
    ""source"" : 14,
    ""target"" : 15,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}{code};;;","08/Jan/24 01:55;xuyangzhong;[~luca.yang] Would you like to fix it?

cc [~qingyue] for the double check. ;;;","08/Jan/24 02:02;qingyue;Hi [~luca.yang] and [~xuyangzhong], please correct me if I'm wrong, but since the state never expires is a by-default behavior, so ""state retention is disabled"" is short for ""the TTL for state retention is disabled"".
You can check the explanation of `table.exec.state.ttl` in [config|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/config/]
{quote}Specifies a minimum time interval for how long idle state (i.e. state which was not updated), will be retained. State will never be cleared until it was idle for less than the minimum time, and will be cleared at some time after it was idle. Default is never clean-up the state. NOTE: Cleaning up state requires additional overhead for bookkeeping. Default value is 0, which means that it will never clean up state.
{quote}
Anyway, any ambiguous statement should be corrected. Thanks for the report, I'll fix it.;;;","08/Jan/24 02:10;xuyangzhong;[~qingyue] User may be confused, and will think that ""State retention is disabled"" means that the state is not retained, but in fact it means that the state is permanently retained without expiration. 

BTW, [~luca.yang] I think this pr is an improvement, not a bug, right ? ;;;","08/Jan/24 02:13;luca.yang;Two data streams, how to achieve one stream does not retain the state(???), the other stream permanent state(""ttl"" : ""0 ms"");  thanks;;;","08/Jan/24 02:15;luca.yang;ok.  its not a error. i will close jira.;;;","08/Jan/24 02:18;xuyangzhong;[~luca.yang] Can you give a scenario as an example of why you want one side not to retain state when using a two-stream join(or something else)? ;;;","08/Jan/24 02:20;xuyangzhong;[~luca.yang] not to close it, but change the type to 'improvement';;;","08/Jan/24 02:24;luca.yang;tb1 join tb2(dim table);
The amount of data in tb1 is very large, but only a small part of the data can be matched in tb2;
if use lookup join, You can't avoid caching large amounts of data, nor can you avoid the stress that a large number of queries put on dimension tables;
so  I want to do this using a two-stream join, with the dimension table using the flinkcdc method;;;;","08/Jan/24 02:25;luca.yang;My dimension table has only a few million rows and is suitable for full storage in state;;;","08/Jan/24 02:34;xuyangzhong;[~luca.yang] I guess what you want is that all( or part) data in dimension table can be cached in memory, and then reduce the pressure on frequent queries to the database? You can ref [lookup.cache|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/jdbc/#lookup-cache-1];;;","08/Jan/24 02:43;luca.yang;left table is(tb1) 1000000/s input. and large id  not exists in tb2;

my lookup.cache config is 1000000,but tb2(mysql) ,flink backpressure is often caused by excessive load;

Does lookup.cache have an upper limit? Does a configuration of hundreds of millions cause poor cache performance?

 

 

 ;;;","08/Jan/24 09:37;xuyangzhong;When the data is queried, the cache will record it into the cache in the memory through LRU. Therefore, if the cache miss occurs, it may put greater pressure on the external database. 

An effective method is using cache all, but the community has not yet implemented it. For details, please refer to [here|https://issues.apache.org/jira/browse/FLINK-17178]

If your job is with a CDC data source, I do not recommend that you use a state TTL like 1ms to work around it, because it has no way to handle the correct retraction message(-U, -D). Maybe you can handle it by using UDF or slightly modifying the connector source code.;;;","08/Jan/24 13:55;qingyue;Fixed in master 3171553d407d9560b30464ab4950d0181f08c8e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for IncrementalGroupAggregate node,FLINK-34000,13563654,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,05/Jan/24 04:02,13/Feb/24 11:34,04/Jun/24 20:40,13/Feb/24 11:34,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34038,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 13 11:34:08 UTC 2024,,,,,,,,,,"0|z1mkmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 12:21;dwysakowicz;Implemented in df71d07188e745553b8174297ec7989f05cebf7a..0df5ab5a3318d21e8be3ab9237900664e3741013;;;","09/Jan/24 08:53;dwysakowicz;Reverted in ba49e50e14c6d78c11ee87afbb851da471d3db68..e27a4cbc74beba7dff8a408dcff38d816ff70457 because of FLINK-34038;;;","13/Feb/24 11:34;dwysakowicz;Merged an improved version in 14d5dbc4c53b2e200dc57e3f4c053583f2419b14..5844092408d21023a738077d0922cc75f1e634d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-412: Add the time-consuming span of each stage when starting the Flink job to TraceReporter,FLINK-33999,13563651,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,easonqin,fanrui,fanrui,05/Jan/24 03:07,05/Jan/24 03:22,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Coordination,Runtime / Metrics,,,0,,,,"This is an umbrella Jira of  [FLIP-412| https://cwiki.apache.org/confluence/x/8435E]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-05 03:07:55.0,,,,,,,,,,"0|z1mkls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Job Manager restarted after kube-apiserver connection intermittent,FLINK-33998,13563641,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,xiangyan,xiangyan,05/Jan/24 00:12,17/Jan/24 10:49,04/Jun/24 20:40,17/Jan/24 10:49,1.13.6,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,"We are running Flink on AWS EKS and experienced Job Manager restart issue when EKS control plane scaled up/in.

I can reproduce this issue in my local environment too.

Since I have no control of EKS kube-apiserver, I built a Kubernetes cluster by my own with below setup:
 * Two kube-apiserver, only one is running at a time;
 * Deploy multiple Flink clusters (with Flink Operator 1.4 and Flink 1.13);
 * Enable Flink Job Manager HA;
 * Configure Job Manager leader election timeout;

{code:java}
high-availability.kubernetes.leader-election.lease-duration: ""60s""
high-availability.kubernetes.leader-election.renew-deadline: ""60s""{code}
For testing, I switch the running kube-apiserver from one instance to another each time. When the kube-apiserver is switching, I can see that some Job Managers restart, but some are still running normally.

Here is an example. When kube-apiserver swatched over at 05:{color:#ff0000}{{*53*}}{color}:08, both JM lost connection to kube-apiserver. But there is no more connection error within a few seconds. I guess the connection recovered by retry.

However, one of the JM (the 2nd one in the attached screen shot) reported ""DefaultDispatcherRunner was revoked the leadership"" error after the leader election timeout (at 05:{color:#ff0000}{{*54*}}{color}:08) and then restarted itself. While the other JM was still running normally.

From kube-apiserver audit logs, the normal JM was able to renew leader lease after the interruption. But there is no any lease renew request from the failed JM until it restarted.

 ","Kubernetes 1.24

Flink Operator 1.4

Flink 1.13.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/24 00:11;xiangyan;audit-log-no-restart.txt;https://issues.apache.org/jira/secure/attachment/13065739/audit-log-no-restart.txt","05/Jan/24 00:11;xiangyan;audit-log-restart.txt;https://issues.apache.org/jira/secure/attachment/13065738/audit-log-restart.txt","05/Jan/24 00:09;xiangyan;connection timeout.png;https://issues.apache.org/jira/secure/attachment/13065742/connection+timeout.png","05/Jan/24 00:09;xiangyan;jm-no-restart4.log;https://issues.apache.org/jira/secure/attachment/13065741/jm-no-restart4.log","05/Jan/24 00:10;xiangyan;jm-restart4.log;https://issues.apache.org/jira/secure/attachment/13065740/jm-restart4.log",,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 10:49:07 UTC 2024,,,,,,,,,,"0|z1mkjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 09:26;mapohl;Thanks for reporting the issue, [~xiangyan] . Do you have some capacity to verify this on a newer version of Flink? 1.13.6 is quite out-dated.;;;","12/Jan/24 12:21;xiangyan;Sure, which version do you suggest? [~mapohl] ;;;","12/Jan/24 12:24;mapohl;the community provides bugfixes to the two most-recently released versions. It would be interesting whether  1.18 and/or 1.17 have this issue or whether it was already resolved in the mean time.;;;","15/Jan/24 02:28;xiangyan;I can't reproduce it in 1.14.6. May I know where can I find the change history to confirm the fix? Thanks!;;;","15/Jan/24 07:39;mapohl;I couldn't find anything that sounds related to your issue in the release notes of [Flink 1.14.0|https://nightlies.apache.org/flink/flink-docs-release-1.18/release-notes/flink-1.14/#runtime--coordination]. A more detailed overview of the changes is possible by browsing through all the changes of the [individual 1.14.x releases|https://issues.apache.org/jira/projects/FLINK?selectedItem=com.atlassian.jira.jira-projects-plugin:release-page&status=released&contains=1.14]. But that's quite tedious.;;;","15/Jan/24 13:16;xiangyan;I can narrow it down to between 1.14.4 and 1.14.5. However, I still can't find relevant bug fix. kubernetes.client.version is also the same for 1.14.4 and 1.14.5.;;;","17/Jan/24 10:49;mapohl;Unfortunately, I don't have the capacity to help you with the investigation. I'm gonna close the issue as ""Not a problem"" because it seems to be fixed in later versions. Thanks for sharing it anyway.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in the doc `classloader.parent-first-patterns-additional`,FLINK-33997,13563625,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,rmeneses,morhidi,morhidi,04/Jan/24 19:20,25/Jan/24 10:03,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,,,0,pull-request-available,,,"Typo in the doc:
[https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/ops/debugging/debugging_classloading/#unloading-of-dynamically-loaded-classes-in-user-code]

classloader.parent-first-patterns-additional -> classloader.parent-first-patterns.additional",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 21:41:51 UTC 2024,,,,,,,,,,"0|z1mkg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 21:41;rodmeneses;there's also another typo:
classloader.parent-first-patterns-default
should be:
classloader.parent-first-patterns.default;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support disabling project rewrite when multiple exprs in the project reference the same sub project field.,FLINK-33996,13563619,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hackergin,hackergin,04/Jan/24 18:00,27/Jan/24 21:40,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"When multiple top projects reference the same bottom project, project rewrite rules may result in complex projects being calculated multiple times.

Take the following SQL as an example:
{code:sql}
create table test_source(a varchar) with ('connector'='datagen');

explain plan for select a || 'a' as a, a || 'b' as b FROM (select REGEXP_REPLACE(a, 'aaa', 'bbb') as a FROM test_source);
{code}
The final SQL plan is as follows:
{code:sql}
== Abstract Syntax Tree ==
LogicalProject(a=[||($0, _UTF-16LE'a')], b=[||($0, _UTF-16LE'b')])
+- LogicalProject(a=[REGEXP_REPLACE($0, _UTF-16LE'aaa', _UTF-16LE'bbb')])
   +- LogicalTableScan(table=[[default_catalog, default_database, test_source]])

== Optimized Physical Plan ==
Calc(select=[||(REGEXP_REPLACE(a, _UTF-16LE'aaa', _UTF-16LE'bbb'), _UTF-16LE'a') AS a, ||(REGEXP_REPLACE(a, _UTF-16LE'aaa', _UTF-16LE'bbb'), _UTF-16LE'b') AS b])
+- TableSourceScan(table=[[default_catalog, default_database, test_source]], fields=[a])

== Optimized Execution Plan ==
Calc(select=[||(REGEXP_REPLACE(a, 'aaa', 'bbb'), 'a') AS a, ||(REGEXP_REPLACE(a, 'aaa', 'bbb'), 'b') AS b])
+- TableSourceScan(table=[[default_catalog, default_database, test_source]], fields=[a])
{code}
It can be observed that after project write, regex_place is calculated twice. Generally speaking, regular expression matching is a time-consuming operation and we usually do not want it to be calculated multiple times. Therefore, for this scenario, we can support disabling project rewrite.

After disabling some rules, the final plan we obtained is as follows:
{code:sql}
== Abstract Syntax Tree ==
LogicalProject(a=[||($0, _UTF-16LE'a')], b=[||($0, _UTF-16LE'b')])
+- LogicalProject(a=[REGEXP_REPLACE($0, _UTF-16LE'aaa', _UTF-16LE'bbb')])
   +- LogicalTableScan(table=[[default_catalog, default_database, test_source]])

== Optimized Physical Plan ==
Calc(select=[||(a, _UTF-16LE'a') AS a, ||(a, _UTF-16LE'b') AS b])
+- Calc(select=[REGEXP_REPLACE(a, _UTF-16LE'aaa', _UTF-16LE'bbb') AS a])
   +- TableSourceScan(table=[[default_catalog, default_database, test_source]], fields=[a])

== Optimized Execution Plan ==
Calc(select=[||(a, 'a') AS a, ||(a, 'b') AS b])
+- Calc(select=[REGEXP_REPLACE(a, 'aaa', 'bbb') AS a])
   +- TableSourceScan(table=[[default_catalog, default_database, test_source]], fields=[a])
{code}
After testing, we probably need to modify these few rules:

org.apache.flink.table.planner.plan.rules.logical.FlinkProjectMergeRule

org.apache.flink.table.planner.plan.rules.logical.FlinkCalcMergeRule

org.apache.flink.table.planner.plan.rules.logical.FlinkProjectCalcMergeRule",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 27 21:40:17 UTC 2024,,,,,,,,,,"0|z1mkeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 06:47;xuyangzhong;This is a great idea! When I saw this Jira, another idea popped into my head:

During the plan optimization phase, introduce a dedicated rule to extract reusable expressions within a calc and eventually transform it into two calc nodes. This maybe help us deal with the structure of three-level calcs.;;;","05/Jan/24 12:06;hackergin;[~xuyangzhong]  Introducing a new rule may be a better way, otherwise we would need to modify multiple rules listed above.;;;","05/Jan/24 16:56;jeyhunkarimov;Thanks for reporting the issue [~hackergin] and thanks for your comment [~xuyangzhong]. I mostly agree with [~xuyangzhong]. An alternative (or simpler) solution would be to add an extra check in {{FlinkRelUtil::mergeable}} function, since all projection/calc merging rules use the result of this function before merging.

[~hackergin] IMO adding a new rule would be tricky since

- We will need to ingest the new rule to multiple places in the optimization process (codebase), because 1) project and calc merging rules are used several times and 2) the order of the new optimization rule will matter
- If we add a new rule, and if the two project/calc nodes already merged (by the existing merging rules), it will be non-trivial for our new rule to rollback (seperate the merged project/calc nodes)

What do you guys think? Also, please feel free to check/review the PR.;;;","08/Jan/24 03:02;libenchao;Instead of solving this in optimization phase, I lean to solve it at the codegen phase. Actually the expressions are already reused at the optimization phase,  you can see {{RexProgram}}, however, Flink doesn't utilize that, and will expand all the expressions again before codegen.

There was a issue about the expression reusing in codegen, see FLINK-21573;;;","08/Jan/24 03:54;qingyue;I tend to agree with [~libenchao]. Splitting calc into two layers at the plan level may seem beneficial, but considering that multiple operators will be generated during the runtime phase, it will also incur additional overhead. Therefore, it is uncertain whether there will be actual performance benefits end-to-end.;;;","09/Jan/24 03:14;hackergin;[~libenchao] [~qingyue]  If it can be reused during the codegen phase, then it is indeed a better approach.  

In addition, what I would like to ask is whether the current optimization rule will definitely make the same project rewrite into the same cal node, at least in my actual tests, this is the behavior. otherwise it may not be able to reuse in the codegen phase.;;;","09/Jan/24 05:15;libenchao;bq. In addition, what I would like to ask is whether the current optimization rule will definitely make the same project rewrite into the same cal node, at least in my actual tests, this is the behavior.

[~hackergin] It's decided by the cost-based planner whether to merge it or not, rules are only suggesting alternatives of the final results.;;;","27/Jan/24 21:40;jeyhunkarimov;Hi [~libenchao] Thanks for your comment. I agree that supporting expr reuse in the codegen phase would be a better solution. It requires a bit of time, but I already started working on this (FLINK-21573) and will ping you guys once I am finished. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test in test_file_sink.sh s3 StreamingFileSink for csv ,FLINK-33995,13563617,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,04/Jan/24 17:33,04/Jan/24 17:36,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / FileSystem,,,,0,,,,"test_file_sink.sh s3 StreamingFileSink doesnt have coverage for csv format . 

this task will add new test case to cover when format is `csv`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-04 17:33:53.0,,,,,,,,,,"0|z1mke8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Datadog api key from environment variables if not set in conf,FLINK-33994,13563614,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,skala,skala,04/Jan/24 16:48,04/Jan/24 18:50,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Metrics,,,,0,,,,"Add a way to set Datadog API key from the environment variables. This way during deployment, there is way to set the value from secrets/vault instead of hardcoding key into code.

Someone has created PR :

[https://github.com/apache/flink/pull/19684/files] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 04 18:50:15 UTC 2024,,,,,,,,,,"0|z1mkdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/24 17:13;martijnvisser;That PR isn't in good shape (doesn't have a test and it just changes the behavior from one thing to another, instead of properly configuring it). ;;;","04/Jan/24 18:50;skala;Ok, I just shared the PR that was done by someone in that regard. 

However this is still needed for to store config in environment and not in code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ineffective scaling detection events are misleading,FLINK-33993,13563604,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,04/Jan/24 15:46,11/Jan/24 15:15,04/Jun/24 20:40,05/Jan/24 09:59,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"When the ineffective scaling decision feature is turned off, events are regenerated which look like this:

{noformat}
Skipping further scale up after ineffective previous scale up for 65c763af14a952c064c400d516c25529
{noformat}

This is misleading because no action will be taken. It is fair to inform users about ineffective scale up even when the feature is disabled but a different message should be printed to convey that no action will be taken.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31977,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-04 15:46:36.0,,,,,,,,,,"0|z1mkbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to fetch the jar from private repository in FlinkSessionJob,FLINK-33992,13563595,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,skala,skala,04/Jan/24 14:30,04/Mar/24 19:42,04/Jun/24 20:40,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"FlinkSessionJob spec does not have a capability to download job jar from remote private repository. It can currently only download from public repositories. 

Adding capability to supply credentials  to the *spec.job.jarURI* in FlinkSessionJob, will solve that problem.

If I use initContainer to download the jar in FlinkDeployment and try to access that in FlinkSessionJob, the operator is unable to find the jar in the defined path.
---
apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
  name: job1
spec:
  deploymentName: session-cluster
  job:
    jarURI: file:///opt/flink/job.jar
    parallelism: 4
    upgradeMode: savepoint
(edited)
caused by: java.io.FileNotFoundException: /opt/flink/job.jar (No such file or directory)
at java.base/java.io.FileInputStream.open0(Native Method)
at java.base/java.io.FileInputStream.open(Unknown Source)
at java.base/java.io.FileInputStream.<init>(Unknown Source)
at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
at org.apache.flink.kubernetes.operator.artifact.FileSystemBasedArtifactFetcher.fetch(FileSystemBasedArtifactFetcher.java:44)
at org.apache.flink.kubernetes.operator.artifact.ArtifactManager.fetch(ArtifactManager.java:63)
at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.uploadJar(AbstractFlinkService.java:707)
at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.submitJobToSessionCluster(AbstractFlinkService.java:212)
at org.apache.flink.kubernetes.operator.reconciler.sessionjob.SessionJobReconciler.deploy(SessionJobReconciler.java:73)
at org.apache.flink.kubernetes.operator.reconciler.sessionjob.SessionJobReconciler.deploy(SessionJobReconciler.java:44)
at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:120)
at org.apache.flink.kubernetes.operator.controller.FlinkSessionJobController.reconcile(FlinkSessionJobController.java:109)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 19:42:41 UTC 2024,,,,,,,,,,"0|z1mk9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/24 12:35;asoliman;Hello [~skala] 

I have some thoughts here and please correct me if I am wrong, 

In Kubernetes, an {{[initContainer|https://kubernetes.io/docs/concepts/workloads/pods/init-containers/]}} is a special kind of container that runs before the main container in a Pod and completes its task before the main container starts. This is often used for setup tasks that need to be done before the main container can start. If you're using an {{initContainer}} to download the JAR file, you would need to make sure that the main container can access the downloaded file. This is where Kubernetes [volumes|https://kubernetes.io/docs/concepts/storage/volumes/] come in.

A Kubernetes volume is essentially a directory that is accessible to all containers running in a Pod. Data in a volume is preserved across container restarts, and it can be shared between multiple containers in a Pod.

so that's being said, you might use a volume to share the JAR file between the {{initContainer}} and the main container:
 # Define a volume in your Pod spec. This could be an {{emptyDir}} volume, which is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node.

 # In the {{initContainer}} spec, specify a volume mount that points to the volume you defined. Download the JAR file to a path in this volume.

 # In the main container spec, specify a volume mount that points to the same volume. The main container will now be able to access the JAR file downloaded by the {{{}initContainer{}}}.

This way, the {{initContainer}} can download the JAR file and store it in a location that the main container can access, allowing the main container to use the JAR file when it starts.


cc: [~gyfora] Do you think the explanation makes sense? if yes, if we think of a case where a session cluster will have tens of session jobs, with different job jars (if this is a valid use case). Is it worth implementing a way to download from private repo in the job spec other than using this initContainer way? 

I have some thoughts on how to implement it, if we agree that the feature makes sense. ;;;","04/Mar/24 12:51;gyfora;For session job submissions the jar generally has to be downloaded and submitted from the operator itself. So initContainers are not really applicable here unless you mean download it into the session cluster lib itself which only works in some special cases.;;;","04/Mar/24 12:52;gyfora;It would be great if Flink itself would have a way of downloading jars from a target url for session job submissions instead of having to upload it before from the operator ;;;","04/Mar/24 19:42;asoliman;[~gyfora] 
> For session job submissions the jar generally has to be downloaded and submitted from the operator itself. So initContainers are not really applicable here unless you mean download it into the session cluster lib itself which only works in some special cases.
 
I think yes, this is what they mean by "" download the jar in FlinkDeployment and try to access that in FlinkSessionJob ""
> which only works in some special cases.


Can you elaborate what special cases ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Error Handling for Kinesis Polling Consumer ,FLINK-33991,13563587,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,iemre,iemre,iemre,04/Jan/24 13:36,16/Apr/24 10:33,04/Jun/24 20:40,,,,,,,,,,aws-connector-4.4.0,,,,Connectors / Kinesis,,,,0,,,,"We introduced custom error handling for the Kinesis EFO Consumer as part of https://issues.apache.org/jira/browse/FLINK-33260

PR for the EFO consumer: [https://github.com/apache/flink-connector-aws/pull/110]

 

This ticket is to apply the same logic to the Kinesis Polling Consumer in the same codebase.

Current configuration for the EFO consumer looks like:
{code:java}
flink.shard.consumer.error.recoverable[0].exception=java.net.UnknownHostException
flink.shard.consumer.error.recoverable[1].exception=java.net.SocketTimeoutException {code}
We should re-use the same code for the polling consumer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33260,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-04 13:36:56.0,,,,,,,,,,"0|z1mk7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use default classloader in TaskManager when there are no user jars for job,FLINK-33990,13563577,13417633,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,FrankZou,FrankZou,FrankZou,04/Jan/24 12:15,27/Mar/24 07:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,pull-request-available,,,"TaskManager will create a new class loader for each flink job even when it has no user jars, which may cause metaspace increasing. Flink can use system classloader for the jobs without jars. A similar optimization has been made in JM in FLINK-32265, it make sense to optimize it in TM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-04 12:15:49.0,,,,,,,,,,"0|z1mk5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert Statement With Filter Operation Generates Extra Tombstone using Upsert Kafka Connector,FLINK-33989,13563576,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,flaviu.cicio,flaviu.cicio,04/Jan/24 12:01,10/May/24 07:30,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,Connectors / Kafka,Table SQL / Runtime,,,0,,,,"Given the following Flink SQL tables:
{code:sql}
CREATE TABLE input (
  id STRING NOT NULL, 
  current_value STRING NOT NULL, 
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka', 
  'topic' = 'input', 
  'key.format' = 'raw', 
  'properties.bootstrap.servers' = 'kafka:29092', 
  'properties.group.id' = 'your_group_id', 
  'value.format' = 'json'
);

CREATE TABLE output (
  id STRING NOT NULL, 
  current_value STRING NOT NULL, 
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka', 
  'topic' = 'output', 
  'key.format' = 'raw', 
  'properties.bootstrap.servers' = 'kafka:29092', 
  'properties.group.id' = 'your_group_id', 
  'value.format' = 'json'
); {code}
And, the following entries are present in the input Kafka topic:
{code:json}
[
  {
    ""id"": ""1"",
    ""current_value"": ""abc""
  },
  {
    ""id"": ""1"",
    ""current_value"": ""abcd""
  }
]{code}
If we execute the following statement:
{code:sql}
INSERT INTO output SELECT id, current_value FROM input; {code}
The following entries are published to the output Kafka topic:
{code:json}
[
  {
    ""id"": ""1"",
    ""current_value"": ""abc""
  },
  {
    ""id"": ""1"",
    ""current_value"": ""abcd""
  }
]{code}
But, if we execute the following statement:
{code:sql}
INSERT INTO output SELECT id, current_value FROM input WHERE id IN ('1'); {code}
The following entries are published:
{code:json}
[
  {
    ""id"": ""1"",
    ""current_value"": ""abc""
  },
  null,
  {
    ""id"": ""1"",
    ""current_value"": ""abcd""
  }
]{code}
We would expect the result to be the same for both insert statements.

As we can see, there is an extra tombstone generated as a result of the second statement.

 

Moreover, if we make a select on the input table:
{code:sql}
SELECT * FROM input;
{code}
We will get the following entries:
||op||id||current_value||
|I|1|abc|
|-U|1|abc|
|+U|1|abcd|

We expected to see only the insert and the update_after entries.

The update_before is added at DeduplicateFunctionHelper#122.

This is easily reproducible with this test that we added in the UpsertKafkaTableITCase from flink-connector-kafka:
{code:java}
    @Test
    public void testAggregateFilterOmit() throws Exception {
        String topic = COUNT_FILTER_TOPIC + ""_"" + format;
        createTestTopic(topic, 1, 1);
        env.setParallelism(1);
        // -------------   test   ---------------
        countFilterToUpsertKafkaOmitUpdateBefore(topic);
        // ------------- clean up ---------------
        deleteTestTopic(topic);
    }

    private void countFilterToUpsertKafkaOmitUpdateBefore(String table) throws Exception {
        String bootstraps = getBootstrapServers();
        List<Row> data =
                Arrays.asList(
                        Row.of(1, ""Hi""),
                        Row.of(1, ""Hello""),
                        Row.of(2, ""Hello world""),
                        Row.of(2, ""Hello world, how are you?""),
                        Row.of(2, ""I am fine.""),
                        Row.of(3, ""Luke Skywalker""),
                        Row.of(3, ""Comment#1""),
                        Row.of(3, ""Comment#2""),
                        Row.of(4, ""Comment#3""),
                        Row.of(4, null));

        final String createSource =
                String.format(
                        ""CREATE TABLE aggfilter_%s (""
                                + ""  `id` INT,\n""
                                + ""  `comment` STRING\n""
                                + "") WITH (""
                                + ""  'connector' = 'values',""
                                + ""  'data-id' = '%s'""
                                + "")"",
                        format, TestValuesTableFactory.registerData(data));
        tEnv.executeSql(createSource);

        final String createSinkTable =
                String.format(
                        ""CREATE TABLE %s (\n""
                                + ""  `id` INT,\n""
                                + ""  `comment` STRING,\n""
                                + ""  PRIMARY KEY (`id`) NOT ENFORCED\n""
                                + "") WITH (\n""
                                + ""  'connector' = 'upsert-kafka',\n""
                                + ""  'topic' = '%s',\n""
                                + ""  'properties.bootstrap.servers' = '%s',\n""
                                + ""  'key.format' = '%s',\n""
                                + ""  'value.format' = '%s'""
                                //+ ""  'sink.omit-row-kind' = '-U'""
                                + "")"",
                        table, table, bootstraps, format, format);

        tEnv.executeSql(createSinkTable);

        String initialValues =
                ""INSERT INTO ""
                        + table
                        + "" ""
                        + ""SELECT * ""
                        + ""FROM aggfilter_""
                        + format
                        + "" ""
                        + ""WHERE id > 2"";
        tEnv.executeSql(initialValues).await();

        // ---------- read from the upsert sink -------------------

        final List<Row> result = collectRows(tEnv.sqlQuery(""SELECT * FROM "" + table), 3);
        List<String> actual = TestValuesTableFactory.getResults(String.format(""%s"", table));
        List<Row> expected =
                Arrays.asList(
                        changelogRow(""+I"", 1L, 1L),
                        changelogRow(""+I"", 2L, 1L),
                        changelogRow(""-D"", 2L, 1L),
                        changelogRow(""+I"", 2L, 2L),
                        changelogRow(""+I"", 3L, 1L),
                        changelogRow(""-D"", 3L, 1L),
                        changelogRow(""+I"", 3L, 2L),
                        changelogRow(""-D"", 3L, 2L),
                        changelogRow(""+I"", 3L, 3L),
                        changelogRow(""+I"", 4L, 1L),
                        changelogRow(""-D"", 4L, 1L),
                        changelogRow(""+I"", 4L, 2L),
                        changelogRow(""-D"", 4L, 2L),
                        changelogRow(""+I"", 4L, 3L),
                        changelogRow(""-D"", 4L, 3L));

        assertThat(result).satisfies(matching(deepEqualTo(expected, true)));
    }
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 30 06:30:18 UTC 2024,,,,,,,,,,"0|z1mk54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 02:52;xuyangzhong;Hi, [~flaviu.cicio] . Could you help use `EXPLAIN PLAN_ADVICE` to print the plan for `INSERT INTO output SELECT id, current_value FROM input WHERE id IN ('1');` ? ;;;","30/Jan/24 02:59;xuyangzhong;I speculate that even if the Upsert Kafka sink can accept upsert messages (without update_before), the optimizer will still output update_before to the sink when it determines that omitting update_before would cause correctness issues. Take this SQL as an example; it contains a WHERE condition, which leads the optimizer to still output update_before.

The actual situation can be inferred from the changelog on the plan given by `EXPLAIN PLAN ADVICE`.;;;","03/Feb/24 22:00;jeyhunkarimov;Hi [~flaviu.cicio], with the latest version of {{flink-connector-kafka}} I was not able to reproduce the issue. In both cases, I am getting 


{code:java}
{""id"":1,""comment"":""abc""}
{""id"":1,""comment"":""abcd""}
{code}

as an output. Could you please verify?;;;","20/Feb/24 15:25;flaviu.cicio;Hi, [~xuyangzhong]! 

Here is the output of the {_}EXPLAIN PLAN ADVICE{_}:
{code:java}
| == Abstract Syntax Tree ==
LogicalSink(table=[default_catalog.default_database.output], fields=[id, current_value])
+- LogicalProject(id=[$0], current_value=[$1])
   +- LogicalFilter(condition=[=($0, _UTF-16LE'1')])
      +- LogicalTableScan(table=[[default_catalog, default_database, input]])

== Optimized Physical Plan With Advice ==
Sink(table=[default_catalog.default_database.output], fields=[id, current_value], upsertMaterialize=[true])
+- Calc(select=[_UTF-16LE'1':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" AS id, current_value])
   +- ChangelogNormalize(key=[id])
      +- Exchange(distribution=[hash[id]])
         +- Calc(select=[id, current_value], where=[=(id, _UTF-16LE'1')])
            +- TableSourceScan(table=[[default_catalog, default_database, input]], fields=[id, current_value])

No available advice...

== Optimized Execution Plan ==
Sink(table=[default_catalog.default_database.output], fields=[id, current_value], upsertMaterialize=[true])
+- Calc(select=['1' AS id, current_value])
   +- ChangelogNormalize(key=[id])
      +- Exchange(distribution=[hash[id]])
         +- Calc(select=[id, current_value], where=[(id = '1')])
            +- TableSourceScan(table=[[default_catalog, default_database, input]], fields=[id, current_value])
 
{code}
 

Hi, [~jeyhunkarimov]!

I retested with _flink-connector-kafka_ version *_3.1.0-1.18,_* the issue is still present.;;;","21/Feb/24 02:31;libenchao;I think this is related to FLINK-9528, and it's a by design behavior.;;;","01/Mar/24 02:14;xuyangzhong;Agree with [~libenchao] . This is a behavior by design.;;;","12/Apr/24 21:49;bdbene;Out of curiosity for my own understand, is there an advantage emitting tombstones on the update_before as opposed to doing nothing? I'm looking at a use case where the sources are Debezium format and we're trying to emit well designed events from it. The tombstones are just extra events to filter out and drop unless I'm missing something.

It seems like the [KafkaWriter will do nothing|https://github.com/apache/flink-connector-kafka/blob/369e7be46a70fd50d68746498aed82105741e7d6/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L194] if the serialization step returns null instead of a tombstone. ;;;","30/Apr/24 06:30;flaviu.cicio;Thank you all for your involvement!
 
From my understanding, FLINK-9528 only refers to the internal Flink changelog.
In our case, the issue is when writing to Kafka using the upsert-kafka sink.
As the connector's name suggests, the sink should produce an upsert stream. That is, for each update, there should be an update-after record.
Currently, the sink produces a retraction-like stream. That is, for each update it generates a retraction (tombstone generated from the update-before record) and an update-after.
 
Based on [FLIP-149|https://cwiki.apache.org/confluence/display/FLINK/FLIP-149%3A+Introduce+the+upsert-kafka+Connector] and the [mail thread|https://www.mail-archive.com/dev@flink.apache.org/msg42543.html], which stands behind this change, I didn't find any statement implying this behavior.
The only mention I found was in this GitHub [comment|https://github.com/apache/flink/pull/13850#discussion_r515759845].
 
Thus, I propose a fix that skips the update-before records when writing to Kafka using the kafka-upsert connector.
This can be achieved by rewriting this [code block|https://github.com/apache/flink-connector-kafka/blob/369e7be46a70fd50d68746498aed82105741e7d6/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/DynamicKafkaRecordSerializationSchema.java#L101] to:
 
{code:java}
if (kind == RowKind.DELETE) { 
  valueSerialized = null; 
else if (kind == RowKind.UPDATE_BEFORE) { 
  return null; 
}
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid configuration when using initialized root logger level on yarn application mode,FLINK-33988,13563565,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,04/Jan/24 10:25,17/Jan/24 02:15,04/Jun/24 20:40,17/Jan/24 02:15,1.19.0,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,"relevant https://issues.apache.org/jira/browse/FLINK-33166

When I set env. log. level=DEBUG and start the flink job by yarn application mode, the logs of TM and JM are still INFO.

Preliminary inference is that it is ROOT_ LOG_ LEVEL value transmission link is not complete enough.

So I used the following configuration:

containerized. taskmanager. env. ROOT_ LOG_ LEVEL=DEBUG

containerized. master. env. ROOT_ LOG_ LEVEL=DEBUG

 

When starting the job by yarn application mode, TM and JM can output DEBUG level logs.

 

Repair ideas:

Fill the value of *env. log. level* into the Flink configuration by *containerized. xxx. env. ROOT_ LOG_ LEVEL* before obtaining the environment variable for the container",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33166,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 02:15:11 UTC 2024,,,,,,,,,,"0|z1mk2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 02:37;Zhanghao Chen;[~RocMarshal] Thanks for reporting this. You are right, the impl of FLINK-33166 won't work under YARN Application mode. The impl of FLINK-33166 replies on the preparing the starting context in pure shell code (Standalone & Native K8s follows this track). However, YARN application mode prepares the starting context in Java code (flink-yarn/YarnClusterDescriptor#setupApplicationMasterContainer & flink-yarn/Utils#createTaskExecutorContext, the logging-related config is assembled in YarnLogConfigUtil). It seems thatthe option {{env.log.max}} does not take effect under YARN as well.;;;","17/Jan/24 02:15;fanrui;Merged to master(1.19) via : eb6d6ff56747e3b34e941a4cd301cd851c0b0b25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Table API Support file extention or suffix,FLINK-33987,13563534,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lorge,lorge,04/Jan/24 05:57,04/Jan/24 05:57,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,"Problem:

when sink file by filesystem table API there is no file extension according to format, or no way to costomize file suffix.

https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/table/FileSystemTableSink.java#L238

New feature: give user option to costomize file suffix, or set default file extension by format",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-04 05:57:30.0,,,,,,,,,,"0|z1mjvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend shuffleMaster to support batch snapshot.,FLINK-33986,13563532,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,04/Jan/24 05:51,14/May/24 12:53,04/Jun/24 20:40,14/May/24 12:53,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Extend shuffleMaster to support batch snapshot as follows:
 # Add method supportsBatchSnapshot to identify whether the shuffle master supports taking snapshot in batch scenarios
 # Add method snapshotState and restoreState to snapshot and restore the shuffle master's state.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 12:53:41 UTC 2024,,,,,,,,,,"0|z1mjvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 12:53;zhuzh;65d31e26534836909f6b8139c6bd6cd45b91bba4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support obtain all partitions existing in cluster through ShuffleMaster.,FLINK-33985,13563531,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,04/Jan/24 05:50,02/Apr/24 06:24,04/Jun/24 20:40,02/Apr/24 06:24,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,Support obtain all partitions existing in cluster through ShuffleMaster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 06:24:08 UTC 2024,,,,,,,,,,"0|z1mjv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 06:24;zhuzh;master: a44709662956b306fe686623d00358a6b076f637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce SupportsBatchSnapshot for operator coordinator,FLINK-33984,13563530,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,04/Jan/24 05:44,08/Apr/24 02:26,04/Jun/24 20:40,08/Apr/24 02:26,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 02:26:35 UTC 2024,,,,,,,,,,"0|z1mjuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 02:26;zhuzh;master:
38255652406becbfbcb7cbec557aa5ba9a1ebbb3
558ca75da2fcec875d1e04a8d75a24fd0ad42ccc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce JobEvent and JobEventStore for Batch Job Recovery,FLINK-33983,13563529,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,04/Jan/24 05:43,19/May/24 10:07,04/Jun/24 20:40,19/May/24 10:07,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 19 10:07:09 UTC 2024,,,,,,,,,,"0|z1mjuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/May/24 10:07;zhuzh;master: c7c1d78752836b96591e31422c65b85eca38bd50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce new config options for Batch Job Recovery,FLINK-33982,13563528,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,04/Jan/24 05:41,02/Apr/24 06:20,04/Jun/24 20:40,02/Apr/24 06:20,,,,,,,,,1.20.0,,,,API / Core,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 06:20:56 UTC 2024,,,,,,,,,,"0|z1mjug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Apr/24 06:20;zhuzh;master: ec1311c8eb805f91b3b8d7d7cbe192e8cad05a76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File Descriptor References Not Released After Job Execution in MiniCluster Mode,FLINK-33981,13563527,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengjiajie,fengjiajie,fengjiajie,04/Jan/24 05:22,05/Feb/24 07:11,04/Jun/24 20:40,05/Feb/24 07:10,1.18.0,,,,,,,,1.17.3,1.18.2,1.19.0,,Runtime / State Backends,,,,0,pull-request-available,,,"When using MiniCluster mode, file descriptors like *{{/tmp/minicluster_6e62ab17ab058e0d5df56b97e9ccef71/tm_0/localState}}* are not released after a Job completes. Executing multiple Jobs in the same JVM might result in leftover file descriptors, potentially leading to problems.

After executing the reproducing code provided below (after entering the sleep), running *lsof -p 18162* reveals:
{code:java}
...
java    18162 sa_cluster   30r   DIR              253,1         0    1311962 /tmp/minicluster_79186ba3856bb18fd21c20d9adbaa3da/tm_0/localState (deleted)
java    18162 sa_cluster   31r   DIR              253,1         0    1311962 /tmp/minicluster_79186ba3856bb18fd21c20d9adbaa3da/tm_0/localState (deleted)
java    18162 sa_cluster   32r   DIR              253,1         0    1310787 /tmp/minicluster_f3ac056ffa1c47010698fc02c5e6d4cf/tm_0/localState (deleted)
java    18162 sa_cluster   33r   DIR              253,1         0    1310787 /tmp/minicluster_f3ac056ffa1c47010698fc02c5e6d4cf/tm_0/localState (deleted)
java    18162 sa_cluster   34r   DIR              253,1         0    1311960 /tmp/minicluster_ef6177c97a2c9096758d0a8f132f9f02/tm_0/localState (deleted)
java    18162 sa_cluster   35r   DIR              253,1         0    1311960 /tmp/minicluster_ef6177c97a2c9096758d0a8f132f9f02/tm_0/localState (deleted)
java    18162 sa_cluster   36r   DIR              253,1         0    1311974 /tmp/minicluster_661a2967ea5c377265fa9f41c768db21/tm_0/localState (deleted)
java    18162 sa_cluster   37r   DIR              253,1         0    1311974 /tmp/minicluster_661a2967ea5c377265fa9f41c768db21/tm_0/localState (deleted)
java    18162 sa_cluster   38r   DIR              253,1         0    1311979 /tmp/minicluster_8413f476b77245203ed1ef759eb0d2de/tm_0/localState (deleted)
...
{code}
The code used for reproduction is as follows:
{code:java}
import org.apache.flink.api.common.JobExecutionResult;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.core.execution.JobClient;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.DiscardingSink;
import org.apache.flink.streaming.api.graph.StreamGraph;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

/**
 * javac -cp 'lib/*' TestReleaseFd.java
 * java -Xmx600m -cp '.:lib/*' TestReleaseFd
 */
public class TestReleaseFd {

  public static void main(String[] args) throws Exception {
    for (int i = 0; i < 10; ++i) {
      int round = i;
      Thread thread = new Thread(() -> {
        try {
          Configuration configuration = new Configuration();
          final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(configuration);
          env.setParallelism(1);

          DataStreamSource<Long> longDataStreamSource = env.fromSequence(1, 100000);
          longDataStreamSource.addSink(new DiscardingSink<>());

          StreamGraph streamGraph = env.getStreamGraph();
          streamGraph.setJobName(""test-"" + System.nanoTime());
          JobClient jobClient = env.executeAsync(streamGraph);

          CompletableFuture<JobExecutionResult> jobExecutionResultCompletableFuture = jobClient.getJobExecutionResult();
          JobExecutionResult jobExecutionResult = null;
          while (jobExecutionResult == null) {
            try {
              jobExecutionResult = jobExecutionResultCompletableFuture.get(20, TimeUnit.SECONDS);
            } catch (TimeoutException timeoutException) {
              // ignore
            }
          }
          System.out.println(""finished round: "" + round);
          env.close();
        } catch (Exception e) {
          throw new RuntimeException(e);
        }
      });

      thread.setDaemon(true);
      thread.start();
      thread.join();

      System.out.println(""done ... "" + i);
    }
    
    // ======================= lsof -p 18162
    Thread.sleep(500_000_000);
  }
}
 {code}
The above code can be consistently reproduced in Flink 1.18.0, but there is no issue in Flink 1.14.6.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 06:40:24 UTC 2024,,,,,,,,,,"0|z1mju8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/24 05:25;fengjiajie;The above code can be consistently reproduced in Flink 1.18.0, but there is no issue in Flink 1.14.6.;;;","28/Jan/24 13:49;fengjiajie;I found the issue and will submit a fix later. Please assign the Jira issue to me, thanks.;;;","04/Feb/24 07:12;fanrui;Hi [~fengjiajie] , would you mind backporting it to 1.17 and master branch as well? Thanks a lot.

 

Merged to :

master(1.19) via: 360c1a0831b64bfb79699d81325c8f9783517be1

1.18 via: fc54ffd0c3e77f7fa01ce04c41291bd80900288f

1.17 via: 1419619b07122b5c734105311d8eff939a687596;;;","05/Feb/24 06:40;fengjiajie;Hi [~fanrui] , thanks for the review, I have submitted the changes to 1.17 and master branch, the tests have been passed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganize job configuration,FLINK-33980,13563523,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,04/Jan/24 03:52,16/Jan/24 06:53,04/Jun/24 20:40,16/Jan/24 06:53,,,,,,,,,1.19.0,,,,API / Core,,,,0,pull-request-available,,,"Currently, job configuration in FLINK is spread out across different components, including StreamExecutionEnvironment, CheckpointConfig, and ExecutionConfig. This distribution leads to inconsistencies among the configurations stored within these components. Furthermore, the methods used to configure these components vary; some rely on complex Java objects, while others use ConfigOption, which is a key-value configuration approach. This variation complicates the effective management of job configurations. Additionally, passing complex Java objects (e.g., StateBackend and CheckpointStorage) between the environment, StreamGraph, and JobGraph adds complexity to development.

With the completion of FLIP-381, it is now time to standardize and unify job configuration in FLINK. The goals of this JIRA are as follows:
 # Migrate configuration from non-ConfigOption objects to use ConfigOption.
 # Adopt a single Configuration object to house all configurations.
 # Create complex Java objects, such as RestartBackoffTimeStrategyFactory, CheckpointStorage, and StateBackend, directly from the configuration on the JM side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33581,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 06:53:53 UTC 2024,,,,,,,,,,"0|z1mjtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 06:53;zhuzh;master/release-1.19:
06b37089f0c1cdf70ca21970a40d15c3eaba07ed
290b633c4265540d481ac563454c7f4a3e706b9d
65b8b3baad6a27e6253a93701508ba24dc8fcfe0
d02ef1cebf302c56a0d9d51664d2c7fb6f5be932
eb8af0c589ce46b091f403e848c7dc84b7e3ee8b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for TableSink node,FLINK-33979,13563509,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,04/Jan/24 01:24,12/Jan/24 15:42,04/Jun/24 20:40,12/Jan/24 15:42,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 15:42:37 UTC 2024,,,,,,,,,,"0|z1mjq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 15:42;dwysakowicz;Implemented in 9a9b9ce81ca05398f8891c918c74294402462f5c..413aed084974efe708833b3e1cfeb7a3f5ce544c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-400: AsyncScalarFunction for asynchronous scalar function support,FLINK-33978,13563506,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,asheinberg,asheinberg,asheinberg,04/Jan/24 00:51,09/Jan/24 08:07,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / Planner,Table SQL / Runtime,,,0,pull-request-available,,,https://cwiki.apache.org/confluence/display/FLINK/FLIP-400%3A+AsyncScalarFunction+for+asynchronous+scalar+function+support,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-04 00:51:42.0,,,,,,,,,,"0|z1mjpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive scheduler may not minimize the number of TMs during downscaling,FLINK-33977,13563457,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,03/Jan/24 13:00,04/Jan/24 11:26,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Autoscaler,Runtime / Coordination,,,0,,,,"Adaptive Scheduler uses SlotAssigner to assign free slots to slot sharing groups. Currently, there're two implementations of SlotAssigner available: the 
DefaultSlotAssigner that treats all slots and slot sharing groups equally and the {color:#172b4d}StateLocalitySlotAssigner{color} that assigns slots based on the number of local key groups to utilize local state recovery. The scheduler will use the DefaultSlotAssigner when no key group assignment info is available and use the StateLocalitySlotAssigner otherwise.
 
However, none of the SlotAssigner targets at minimizing the number of TMs, which may produce suboptimal slot assignment under the Application Mode. For example, when a job with 8 slot sharing groups and 2 TMs (each 4 slots) is downscaled through the FLIP-291 API to have 4 slot sharing groups instead, the cluster may still have 2 TMs, one with 1 free slot, and the other with 3 free slots. For end-users, this implies an ineffective downscaling as the total cluster resources are not reduced.
 
We should take minimizing number of TMs into consideration as well. A possible approach is to enhance the {color:#172b4d}StateLocalitySlotAssigner: when the number of free slots exceeds need, sort all the TMs by a score summing from the allocation scores of all slots on it, remove slots from the excessive TMs with the lowest score and proceed the remaining slot assignment.{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-03 13:00:02.0,,,,,,,,,,"0|z1mjeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveScheduler cooldown period is taken from a wrong configuration,FLINK-33976,13563449,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,03/Jan/24 12:16,29/Jan/24 15:22,04/Jun/24 20:40,29/Jan/24 15:22,,,,,,,,,1.19.0,,,,Runtime / Configuration,Runtime / Coordination,,,0,pull-request-available,,,"The new JobManager options introduced in FLINK-21883: `scaling-interval.\{min,max}` of AdaptiveScheduler are resolved from the per-Job configuration instead of JobManager's configuration, which is not correct.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-21883,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 29 15:21:55 UTC 2024,,,,,,,,,,"0|z1mjcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 15:21;dmvk;master:

 

92d6932ac38780e90bf9e0d659f46dd9e9b26f0e

e04a2326126bd7cfbf883baba5313914335e411f

23377711ea4615749adfb9c102e9d38978d6d7b2

5ae48935e665cf2e7dc3b1e65b2968caa40cf348

843fe8fc7e8fabebb8f9cc2daca2cdde77f555cf

e945a36de8c056fa9d12dc3e4c209da87de1391f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests for the new Sink V2 transformations,FLINK-33975,13563448,13563444,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,03/Jan/24 12:13,23/Jan/24 08:22,04/Jun/24 20:40,23/Jan/24 08:22,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,"Create new tests for the SinkV2 api transformations, and migrate some of the tests to use the new API. Some of the old test should be kept using the old API to make sure that the backward compatibility is tested until the deprecation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 08:22:53 UTC 2024,,,,,,,,,,"0|z1mjco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 08:22;gyfora;merged to master 580c4ae0e4e934133e5be415a740d9f7ffbbda2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement the Sink transformation depending on the new SinkV2 interfaces,FLINK-33974,13563447,13563444,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,03/Jan/24 12:12,19/Jan/24 13:05,04/Jun/24 20:40,19/Jan/24 13:05,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,"Implement the changes to the Sink transformation which should depend only on the new API interfaces. The tests should remain the same, to ensure backward compatibility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 13:05:17 UTC 2024,,,,,,,,,,"0|z1mjcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 13:05;gyfora;merged to master fd673a2f46206ff65978f05fcb96b525696aead2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new interfaces for SinkV2 to synchronize the API with the SourceV2 API,FLINK-33973,13563446,13563444,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,pvary,pvary,03/Jan/24 12:08,20/Feb/24 14:52,04/Jun/24 20:40,12/Jan/24 09:16,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,"Create the new interfaces, set inheritance and deprecation to finalize the interface.
After this change the new interafaces will exits, but they will not be functional.

The existing interfaces, and test should be working without issue, to verify that adding the API will be backward compatible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 08:51:28 UTC 2024,,,,,,,,,,"0|z1mjc8:",9223372036854775807,"According to FLIP-372 (https://cwiki.apache.org/confluence/display/FLINK/FLIP-372%3A+Enhance+and+synchronize+Sink+API+to+match+the+Source+API) the SinkV2 API has been changed.

The following interfaces are deprecated: TwoPhaseCommittingSink, StatefulSink, WithPreWriteTopology, WithPreCommitTopology, WithPostCommitTopology.

The following new interfaces has been introduced: CommitterInitContext, CommittingSinkWriter, WriterInitContext, StatefulSinkWriter.

The following interface method's parameter has been changed: Sink.createWriter

The original interfaces will remain available during the 1.19 release line, but they will be removed in consecutive releases. For the changes required when migrating, please consult the Migration Plan detailed in the FLIP",,,,,,,,,,,,,,,,,,,"12/Jan/24 09:16;gyfora;Merged to master 40cbf231e3dc15129b69136941adc15beebe8ef6;;;","30/Jan/24 08:51;martijnvisser;[~pvary] [~gyfora] Can you please add release notes for this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance and synchronize Sink API to match the Source API,FLINK-33972,13563444,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pvary,pvary,03/Jan/24 12:05,06/Feb/24 11:22,04/Jun/24 20:40,,,,,,,,,,,,,,API / DataStream,,,,0,,,,Umbrella jira for the implementation of https://cwiki.apache.org/confluence/display/FLINK/FLIP-372%3A+Enhance+and+synchronize+Sink+API+to+match+the+Source+API,,,,,,,,,,,,,,,,,,,,,FLINK-33328,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-03 12:05:07.0,,,,,,,,,,"0|z1mjbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifies whether to use HBase table that supports dynamic columns.,FLINK-33971,13563418,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,MOBIN,MOBIN,03/Jan/24 07:37,22/Jan/24 02:57,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / HBase,,,,0,pull-request-available,,,"Specifies whether to use HBase table that supports dynamic columns.

Refer to the dynamic.table parameter in this document: [[https://www.alibabacloud.com/help/en/flink/developer-reference/apsaradb-for-hbase-connector#section-ltp-3fy-9qv|http://example.com]]

Sample code for a result table that supports dynamic columns
{code:java}
CREATE TEMPORARY TABLE datagen_source (
  id INT,
  f1hour STRING,
  f1deal BIGINT,
  f2day STRING,
  f2deal BIGINT
) WITH (
  'connector'='datagen'
);
CREATE TEMPORARY TABLE hbase_sink (
  rowkey INT,
  f1 ROW<`hour` STRING, deal BIGINT>,
  f2 ROW<`day` STRING, deal BIGINT>
) WITH (
  'connector'='hbase-2.2',
  'table-name'='<yourTableName>',
  'zookeeper.quorum'='<yourZookeeperQuorum>',
  'dynamic.table'='true'
);
INSERT INTO hbase_sink
SELECT id, ROW(f1hour, f1deal), ROW(f2day, f2deal) FROM datagen_source; {code}

If dynamic.table is set to true, HBase table that supports dynamic columns is used.
Two fields must be declared in the rows that correspond to each column family. The value of the first field indicates the dynamic column, and the value of the second field indicates the value of the dynamic column.

For example, the datagen_source table contains a row of data The row of data indicates that the ID of the commodity is 1, the transaction amount of the commodity between 10:00 and 11:00 is 100, and the transaction amount of the commodity on July 26, 2020 is 10000. In this case, a row whose rowkey is 1 is inserted into the HBase table. f1:10 is 100, and f2:2020-7-26 is 10000.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-03 07:37:20.0,,,,,,,,,,"0|z1mj60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add necessary checks for connector document,FLINK-33970,13563410,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gongzhongqiang,leonard,leonard,03/Jan/24 06:36,27/Mar/24 13:28,04/Jun/24 20:40,03/Jan/24 16:32,,,,,,,,,pulsar-4.1.1,pulsar-4.2.0,,,Documentation,,,,0,pull-request-available,,,"In FLINK-33964, we found the documentation files in independent connector repos lacks basic checks like broken url, this ticket aims to add necessary checks and avoid similar issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33964,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 07 06:37:30 UTC 2024,,,,,,,,,,"0|z1mj48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/24 06:41;gongzhongqiang;[~leonard]  I am willing to take this issue, Please assign to me. Thank you~;;;","03/Jan/24 16:32;tison;master via 5b70da8f88e21057a5c590d139eab558f87e5dca

Thanks a lot [~gongzhongqiang]!;;;","04/Jan/24 12:51;gongzhongqiang;[~tison] I also want to add this check to other connector repo to avoid to build flink doc failed , what do you think about this ?;;;","04/Jan/24 14:17;martijnvisser;This shouldn't be fixed per connector, but once in the shared_utils at https://github.com/apache/flink-connector-shared-utils/tree/ci_utils;;;","04/Jan/24 14:19;martijnvisser;Next to this, I believe this only fixes the problem for 404 links inside the connector docs itself. But that's not where the problem originated, that was in the Hugo integration when the Flink documentation is being built. ;;;","04/Jan/24 14:46;gongzhongqiang;[~martijnvisser] I had tried to use hugo to check document build , but `config.toml` is neccessary for hugo. So I use simple scrips to check dead link.;;;","04/Jan/24 15:02;gongzhongqiang;[~martijnvisser] And I checked my pr cannot solve the original problem , I will try to find the way to both check hugo build and dead link check. Thanks for your kind remind.;;;","05/Jan/24 02:38;gongzhongqiang;[~martijnvisser] [~leonard] Can you give me some advices for this issue?

I have a idea for this issue:

 

*Hugo build check:*

_Because connector docs depends on flink main docs and no dependencies between connector docs , so we can just check flink main docs and single connector docs. Finally,We use hugo build to check docs._
 * -check out flink repo with only `docs` dir cloned ,use *sparse-checkout*-
 * check out flink repo with only docs dir and check other connectors docs exclude current connector docs
 * move connector (which repo related) docs into flink `docs` dir
 * execute hugo build command

 

*Dead links check:*

_Because of hugo build cannot check dead link. It's very useful to add dead link check for this._
 * use npm module *markdown-link-check* to check dead link;;;","05/Jan/24 08:06;gongzhongqiang;I had complete dead link check at [https://github.com/GOODBOY008/flink-connector-shared-utils/tree/ci_utils]

jdbc connector action for test ci : [https://github.com/GOODBOY008/flink-connector-jdbc/actions/runs/7419405021/job/20188882787];;;","07/Jan/24 06:37;gongzhongqiang;Hi [~martijnvisser] [~leonard] 

Here is the pr for complete the docs check for connectors. Here is the [PR|https://github.com/apache/flink-connector-shared-utils/pull/33] in *flink-connector-shared-utils.* 

*Hugo build check:*

_Because connector docs depends on flink main docs and no dependencies between connector docs , so we can just check flink main docs and single connector docs. Finally,We use hugo build to check docs._
 * -check out flink repo with only `docs` dir cloned ,use *sparse-checkout*-
 * check out flink repo with only docs dir and check other connectors docs exclude current connector docs
 * move connector (which repo related) docs into flink `docs` dir
 * execute hugo build command

 

*Dead links check:*

_Because of hugo build cannot check dead link. It's very useful to add dead link check for this._
 * use npm module *markdown-link-check* to check dead link

Currently，dead link check is used only to connector docs, So about enable dead link check in flink main repo, I want to add this check to {color:#FF0000}docs.yml{color:#172b4d} too.{color}{color};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for TableSourceScan node,FLINK-33969,13563405,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,03/Jan/24 03:56,11/Jan/24 09:41,04/Jun/24 20:40,11/Jan/24 09:41,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 09:41:04 UTC 2024,,,,,,,,,,"0|z1mj34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 09:41;dwysakowicz;Implemented in d6209a18bbcfa7d2a4027a55757c0426997651cb..e98ded88876e4b95922123481c38b215ab15b3e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compute the number of subpartitions when initializing executon job vertices,FLINK-33968,13563401,13562406,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,wanglijie,wanglijie,wanglijie,03/Jan/24 02:50,05/Jan/24 02:38,04/Jun/24 20:40,05/Jan/24 02:38,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently, when using dynamic graphs, the subpartition-num of a task is lazily calculated until the task deployment moment, this may lead to some uncertainties in job recovery scenarios:

Before jm crashs, when deploying upstream tasks, the parallelism of downstream vertex may be unknown, so the subpartiton-num will be the max parallelism of downstream job vertex. However, after jm restarts, when deploying upstream tasks, the parallelism of downstream job vertex may be known(has been calculated before jm crashs and been recovered after jm restarts), so the subpartiton-num will be the actual parallelism of downstream job vertex. The difference of calculated subpartition-num will lead to the partitions generated before jm crashs cannot be reused after jm restarts.

We will solve this problem by advancing the calculation of subpartitoin-num to the moment of initializing executon job vertex (in ctor of IntermediateResultPartition)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 05 02:38:50 UTC 2024,,,,,,,,,,"0|z1mj28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 02:38;zhuzh;master/1.19: b25dfaee80727d6662a5fd445fe51cc139a8b9eb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove/Rename log4j2-test.properties in flink-streaming-java's test bundle,FLINK-33967,13563386,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,koalalam,koalalam,02/Jan/24 21:44,02/Jan/24 21:44,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"This file from test classpath is picked automatically by Log4j2. In order to reliably use our own log4j2 test config, we have to specify system property ""log4j2.configurationFile"" which is not ideal as we have to manually set it in IDE config.

https://logging.apache.org/log4j/2.x/manual/configuration.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2024-01-02 21:44:37.0,,,,,,,,,,"0|z1miyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the getNumRecordsInPerSecond Utility Function,FLINK-33966,13563383,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,yang,yang,yang,02/Jan/24 20:43,04/Jan/24 09:40,04/Jun/24 20:40,04/Jan/24 09:40,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,"We have following code in the codebase
{code:java}
        if (isSource && (numRecordsInPerSecond == null || numRecordsInPerSecond.getSum() == 0)) {
            numRecordsInPerSecond =
                    flinkMetrics.get(FlinkMetric.SOURCE_TASK_NUM_RECORDS_IN_PER_SEC);
        }{code}
{code:java}

        if (isSource && (numRecordsInPerSecond == null || numRecordsInPerSecond.getSum() == 0)) {
            numRecordsInPerSecond =
                    flinkMetrics.get(FlinkMetric.SOURCE_TASK_NUM_RECORDS_OUT_PER_SEC);
        }{code}

with two times the same condition check 

 

{*}Definition of done{*}: 

Update getNumRecordsInPerSecond'{{{}s{}}} second {{if}} condition from {{if (isSource && ...)}} to {{{}if (!isSource && ...){}}}. This addresses the redundant check and ensures correct metric fetching for non-source operators.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 04 09:39:55 UTC 2024,,,,,,,,,,"0|z1miy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/24 08:35;yang;Thanks to the clarification from [~fanrui] , now I think it's not a bug. ;;;","04/Jan/24 09:39;fanrui;Merged to main(1.8.0) via 456fb6e9a816d7f4bebdceb5fcac4cd13cace1a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor the configuration for autoscaler standalone,FLINK-33965,13563341,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,02/Jan/24 10:10,03/Jan/24 03:22,04/Jun/24 20:40,03/Jan/24 03:22,,,,,,,,,kubernetes-operator-1.8.0,,,,,,,,0,pull-request-available,,,"Currently, all configurations of autoscaler standalone are maintained in string key.

When autoscaler standalone has a little options, it's easy to maintain. However, I found it's hard to maintain when we add more options.

During I developing the JDBC autoscaler state store and control loop supports multiple thread. It will introduce more options.
h2. Solution:

Introducing the AutoscalerStandaloneOptions to manage all options of autoscaler standalone. And output the doc for it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 03:21:53 UTC 2024,,,,,,,,,,"0|z1miow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/24 03:21;fanrui;Merged to main(1.8.0) via : fa938439ad8a7601462a3be268077aa4078881bc and 3cae5907d3a5c610a1aa8e5d41095ddbcba676ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink documentation can't be build due to error in Pulsar docs,FLINK-33964,13563338,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,leonard,martijnvisser,martijnvisser,02/Jan/24 09:50,03/Jan/24 16:33,04/Jun/24 20:40,03/Jan/24 06:12,pulsar-4.1.0,,,,,,,,pulsar-4.1.1,,,,Connectors / Pulsar,Documentation,,,0,pull-request-available,,,"https://github.com/apache/flink/actions/runs/7380766702/job/20078487743

{code:java}
Start building sites … 
hugo v0.110.0-e32a493b7826d02763c3b79623952e625402b168+extended linux/amd64 BuildDate=2023-01-17T12:16:09Z VendorInfo=gohugoio
Error: Error building site: ""/root/flink/docs/themes/connectors/content.zh/docs/connectors/datastream/pulsar.md:491:1"": failed to extract shortcode: template for shortcode ""generated/pulsar_admin_configuration"" not found
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33970,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 07:02:34 UTC 2024,,,,,,,,,,"0|z1mio8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/24 10:16;leonard;Thanks [~martijnvisser] for reporting this issue, and this may be related to [https://github.com/apache/flink-connector-pulsar/pull/63,]  I'll fix this soon;;;","02/Jan/24 10:18;leonard;Current connector CI did not check the docs build https://github.com/apache/flink-connector-pulsar/actions/runs/6801264179/job/18506266724;;;","03/Jan/24 06:09;leonard;Document action has been fixed https://github.com/apache/flink/actions/runs/7391764752/job/20114595324;;;","03/Jan/24 06:12;leonard;Fixed in flink-connector-pulsar via 

main: 2be651a043b66936ec787033a851ca8c9d7cf95e

v4.1: 05e58b74947dcf46583e7fd1565d459c7b4a44a2;;;","03/Jan/24 07:02;martijnvisser;Thanks [~leonard] [~Tison]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is only one UDF instance after serializing the same task,FLINK-33963,13563320,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lifengchao,lifengchao,02/Jan/24 06:47,04/Feb/24 02:34,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"I define this UDF and expect the following SQL to return 'a', 'b', but it return 'a', 'a'.
{code:java}
public class UdfSerializeFunc extends ScalarFunction {
    static final Logger LOG = LoggerFactory.getLogger(UdfSerializeFunc.class);
    String cache;
    @Override
    public void open(FunctionContext context) throws Exception {
        LOG.warn(""open:{}."", this.hashCode());
    }

    public String eval(String a, String b){
        if(cache == null){
            LOG.warn(""cache_null.cache:{}"", b);
            cache = b;
        }
        return cache;
    }
}
{code}
sql:
{code:sql}
    select
        udf_ser(name, 'a') name1,
        udf_ser(name, 'b') name2
    from heros
{code}
Changing UDF to this will achieve the expected results.
{code:java}
public class UdfSerializeFunc2 extends ScalarFunction {
    static final Logger LOG = LoggerFactory.getLogger(UdfSerializeFunc2.class);
    String cache;
    @Override
    public void open(FunctionContext context) throws Exception {
        LOG.warn(""open:{}."", this.hashCode());
    }

    public String eval(String a, String b){
        if(cache == null){
            LOG.warn(""cache_null.cache:{}"", b);
            cache = b;
        }
        return cache;
    }

    @Override
    public TypeInference getTypeInference(DataTypeFactory typeFactory) {
        return TypeInference.newBuilder()
                .outputTypeStrategy(new TypeStrategy() {
                    @Override
                    public Optional<DataType> inferType(CallContext callContext) {
                        List<DataType> argumentDataTypes = callContext.getArgumentDataTypes();
                        if (argumentDataTypes.size() != 2) {
                            throw callContext.newValidationError(""arg size error"");
                        }
                        if (!callContext.isArgumentLiteral(1) || callContext.isArgumentNull(1)) {
                            throw callContext.newValidationError(""Literal expected for second argument."");
                        }
                        cache = callContext.getArgumentValue(1, String.class).get();
                        return Optional.of(DataTypes.STRING());
                    }
                })
        .build();
    }
}
{code}
 

My complete test code:
{code:java}
public class UdfSerializeFunc extends ScalarFunction {
    static final Logger LOG = LoggerFactory.getLogger(UdfSerializeFunc.class);
    String cache;
    @Override
    public void open(FunctionContext context) throws Exception {
        LOG.warn(""open:{}."", this.hashCode());
    }

    public String eval(String a, String b){
        if(cache == null){
            LOG.warn(""cache_null.cache:{}"", b);
            cache = b;
        }
        return cache;
    }
}

public class UdfSerializeFunc2 extends ScalarFunction {
    static final Logger LOG = LoggerFactory.getLogger(UdfSerializeFunc2.class);
    String cache;
    @Override
    public void open(FunctionContext context) throws Exception {
        LOG.warn(""open:{}."", this.hashCode());
    }

    public String eval(String a, String b){
        if(cache == null){
            LOG.warn(""cache_null.cache:{}"", b);
            cache = b;
        }
        return cache;
    }

    @Override
    public TypeInference getTypeInference(DataTypeFactory typeFactory) {
        return TypeInference.newBuilder()
                .outputTypeStrategy(new TypeStrategy() {
                    @Override
                    public Optional<DataType> inferType(CallContext callContext) {
                        List<DataType> argumentDataTypes = callContext.getArgumentDataTypes();
                        if (argumentDataTypes.size() != 2) {
                            throw callContext.newValidationError(""arg size error"");
                        }
                        if (!callContext.isArgumentLiteral(1) || callContext.isArgumentNull(1)) {
                            throw callContext.newValidationError(""Literal expected for second argument."");
                        }
                        cache = callContext.getArgumentValue(1, String.class).get();
                        return Optional.of(DataTypes.STRING());
                    }
                })
        .build();
    }
}

class UdfSerializeSuite extends AnyFunSuite with BeforeAndAfterAll{
  var env: StreamExecutionEnvironment = _
  var tEnv: StreamTableEnvironment = _

  override protected def beforeAll(): Unit = {
    val conf = new Configuration()
    env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf)
    env.setParallelism(2)
    env.getConfig.enableObjectReuse()

    tEnv = StreamTableEnvironment.create(env)
  }

  /**
   * 2个task，只是每个task有一个udf，udf_ser(name, 'a')和udf_ser(name, 'b')没区分开
   * 它这函数的序列化，单个task的2个udf_ser序列化后还是同一个对象，不是2个
   * getTypeInference中修改udf的属性可以实现2个不同的对象
   */
  test(""UdfSerializeFunc""){
    tEnv.createTemporarySystemFunction(""udf_ser"", classOf[UdfSerializeFunc])

    var sql = """"""
    CREATE TEMPORARY TABLE heros (
      `name` STRING,
      `power` STRING,
      `age` INT
    ) WITH (
      'connector' = 'faker',
      'fields.name.expression' = '#{superhero.name}',
      'fields.power.expression' = '#{superhero.power}',
      'fields.power.null-rate' = '0.05',
      'rows-per-second' = '1',
      'fields.age.expression' = '#{number.numberBetween ''0'',''1000''}'
    )
    """"""
    tEnv.executeSql(sql)

    sql = """"""
    select
        udf_ser(name, 'a') name1,
        udf_ser(name, 'b') name2
    from heros
    """"""
    val rstTable = tEnv.sqlQuery(sql)
    rstTable.printSchema()

    rstTable.execute().print()
  }

  /**
   * 修改ScalarFunction的属性，能使之序列化后是不同的对象
   */
  test(""UdfSerializeFunc2""){
    tEnv.createTemporarySystemFunction(""udf_ser"", classOf[UdfSerializeFunc2])

    var sql = """"""
    CREATE TEMPORARY TABLE heros (
      `name` STRING,
      `power` STRING,
      `age` INT
    ) WITH (
      'connector' = 'faker',
      'fields.name.expression' = '#{superhero.name}',
      'fields.power.expression' = '#{superhero.power}',
      'fields.power.null-rate' = '0.05',
      'rows-per-second' = '1',
      'fields.age.expression' = '#{number.numberBetween ''0'',''1000''}'
    )
    """"""
    tEnv.executeSql(sql)

    sql = """"""
    select
        udf_ser(name, 'a') name1,
        udf_ser(name, 'b') name2
    from heros
    """"""
    val rstTable = tEnv.sqlQuery(sql)
    rstTable.printSchema()

    rstTable.execute().print()
  }

  override protected def afterAll(): Unit = {
    env.execute()
  }

}
{code}
test UdfSerializeFunc log out:
{code:java}
(
  `name1` STRING,
  `name2` STRING
)
14:44:41,183  DEBUG org.apache.flink.table.planner.codegen.OperatorCodeGenerator$ [ScalaTest-run-running-UdfSerializeSuite] [] - Compiling OneInputStreamOperator Code:
StreamExecCalc
14:44:42,819  WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [ScalaTest-run-running-UdfSerializeSuite] [] - No tokens obtained so skipping notifications
14:44:43,092  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [ScalaTest-run-running-UdfSerializeSuite] [] - Starting rest endpoint.
14:44:43,240  WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils [ScalaTest-run-running-UdfSerializeSuite] [] - Log file environment variable 'log.file' is not set.
14:44:43,240  WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils [ScalaTest-run-running-UdfSerializeSuite] [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
14:44:43,711  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [ScalaTest-run-running-UdfSerializeSuite] [] - Rest endpoint listening at localhost:8081
14:44:43,716  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [ScalaTest-run-running-UdfSerializeSuite] [] - Web frontend listening at http://localhost:8081.
14:44:43,717  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [mini-cluster-io-thread-1] [] - http://localhost:8081 was granted leadership with leaderSessionID=18ab2e30-a83a-4ec0-be98-7d49b7628565
14:44:43,789  WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [flink-pekko.actor.default-dispatcher-4] [] - No tokens obtained so skipping notifications
14:44:43,790  WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [flink-pekko.actor.default-dispatcher-4] [] - Tokens update task not started because either no tokens obtained or none of the tokens specified its renewal date
14:44:44,576  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc [Source: Source Generator -> heros[1] -> Calc[2] (1/2)#0] [] - open:969139468.
14:44:44,576  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc [Source: Source Generator -> heros[1] -> Calc[2] (2/2)#0] [] - open:1737783673.
14:44:44,607  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc [Source: Source Generator -> heros[1] -> Calc[2] (1/2)#0] [] - cache_null.cache:a
14:44:44,607  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc [Source: Source Generator -> heros[1] -> Calc[2] (2/2)#0] [] - cache_null.cache:a
+----+--------------------------------+--------------------------------+
| op |                          name1 |                          name2 |
+----+--------------------------------+--------------------------------+
| +I |                              a |                              a |
| +I |                              a |                              a |
| +I |                              a |                              a |
| +I |                              a |                              a |
| +I |                              a |                              a |
| +I |                              a |                              a |
| +I |                              a |                              a |
| +I |                              a |                              a |
{code}
test UdfSerializeFunc2 log out:
{code:java}
(
  `name1` STRING,
  `name2` STRING
)
14:45:18,786  DEBUG org.apache.flink.table.planner.codegen.OperatorCodeGenerator$ [ScalaTest-run-running-UdfSerializeSuite] [] - Compiling OneInputStreamOperator Code:
StreamExecCalc
14:45:20,296  WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [ScalaTest-run-running-UdfSerializeSuite] [] - No tokens obtained so skipping notifications
14:45:20,518  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [ScalaTest-run-running-UdfSerializeSuite] [] - Starting rest endpoint.
14:45:20,635  WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils [ScalaTest-run-running-UdfSerializeSuite] [] - Log file environment variable 'log.file' is not set.
14:45:20,635  WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils [ScalaTest-run-running-UdfSerializeSuite] [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable 'log.file' or configuration key 'web.log.path'.
14:45:21,032  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [ScalaTest-run-running-UdfSerializeSuite] [] - Rest endpoint listening at localhost:8081
14:45:21,034  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [ScalaTest-run-running-UdfSerializeSuite] [] - Web frontend listening at http://localhost:8081.
14:45:21,035  INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [mini-cluster-io-thread-1] [] - http://localhost:8081 was granted leadership with leaderSessionID=2fcfdba0-0e36-4e8b-9f3c-88f2c242458f
14:45:21,089  WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [flink-pekko.actor.default-dispatcher-4] [] - No tokens obtained so skipping notifications
14:45:21,089  WARN  org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [flink-pekko.actor.default-dispatcher-4] [] - Tokens update task not started because either no tokens obtained or none of the tokens specified its renewal date
14:45:21,741  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc2 [Source: Source Generator -> heros[1] -> Calc[2] (1/2)#0] [] - open:1439144392.
14:45:21,741  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc2 [Source: Source Generator -> heros[1] -> Calc[2] (2/2)#0] [] - open:381953409.
14:45:21,742  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc2 [Source: Source Generator -> heros[1] -> Calc[2] (1/2)#0] [] - open:1162638327.
14:45:21,742  WARN  com.java.flink.sql.udf.serialize.UdfSerializeFunc2 [Source: Source Generator -> heros[1] -> Calc[2] (2/2)#0] [] - open:391248806.
+----+--------------------------------+--------------------------------+
| op |                          name1 |                          name2 |
+----+--------------------------------+--------------------------------+
| +I |                              a |                              b |
| +I |                              a |                              b |
| +I |                              a |                              b |
| +I |                              a |                              b |

{code}
 

*This is an issue caused by UDF function serialization.*","local env in idea test.

java 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 04 02:33:38 UTC 2024,,,,,,,,,,"0|z1mik8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 12:00;xuyangzhong;Hi, [~lifengchao] .

This is because in the current implementation, whether two UDX instances are considered being reused is determined by the arrays after serializing the UDX. In `UdfSerializeFunc`, since the cache is always null during the codegen phase, the variables generated after codegen become identical after two POJOs are serialized. In `UdfSerializeFunc2`, since getTypeInference is called before codegen, the different value about field `cache` leads to different serialization results, and after codegen, two different variables are maintained.

To solve this issue, I think it is unreasonable to determine the reuse of UDX POJOs based on whether the serialization is the same or not. Perhaps we could consider determining reuse based on the digest of RexCall. cc [~lsy] ;;;","30/Jan/24 02:29;xuyangzhong;From my personal perspective, the method of adding class member variables into scalar functions is rather uncommon. If the optimizer is instructed to never reuse UDX objects under any circumstances, it will result in additional overhead in terms of memory and the costs for serialization/deserialization.

A short-term feasible approach would be to incorporate similar examples into our documentation to guide advanced UDX developers on how to prevent UDX reuse if they wish to do so (just like in the example, by assigning different values to class member variables by overriding getTypeInference).

In the long term, we could consider this Jira as an improvement item, enabling codegen to prevent UDX reuse based on RexCall or an API approach.

WDYT?;;;","30/Jan/24 02:36;lifengchao;I think the cost of function serialization and its memory usage can be ignored, as serialization only occurs once during the task initialization phase. It seems that UDF in Spark SQL does not have this issue.

我认为函数序列化和其内存占用的开销可以忽略，序列化只会会任务初始化阶段发生一次。spark sql 中udf似乎就没有这个问题。;;;","30/Jan/24 03:08;lifengchao;I think it's quite common to add constant parameters to functions, especially when it comes to performance optimization. 
For example, if I want to achieve udf similar to get_json_object(str, path) in sparl sql, when path is a constant, I only need to parse json path once, instead of parsing json path every times.

我觉得在函数中添加常数参数还是很常见的，特别是在性能优化时.
例如我想要实现类似spark sql中get_json_object(str, path)的udf，当path是常量时，我只需要解析一次json_path,而不是每次重新解析json_path。;;;","04/Feb/24 02:33;xuyangzhong;I agree that we should be more rigorous in our consideration of whether a scalar function should be reused.

However, in the scenario you mentioned, if the path can be extracted once and used permanently, it then implies that the path is a constant across all elements. In such a case, using two scalar functions might not be the best choice. That's why I just say this is not a uncommon scenario.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chaining-agnostic OperatorID generation for improved state compatibility on parallelism change,FLINK-33962,13563314,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,02/Jan/24 05:54,07/Feb/24 12:15,04/Jun/24 20:40,,,,,,,,,,,,,,API / Core,,,,0,,,,"*Background*

Flink restores opeartor state from snapshots based on matching the operatorIDs. Since Flink 1.2, {{StreamGraphHasherV2}} is used for operatorID generation when no user-set uid exists. The generated OperatorID is deterministic with respect to:
 * node-local properties (the traverse ID in the BFS for the stream graph)
 * chained output nodes
 * input nodes hashes

*Problem*

The chaining behavior will affect state compatibility, as the generation of the OperatorID of an Op is dependent on its chained output nodes. For example, a simple source->sink DAG with source and sink chained together is state imcompatible with an otherwise identical DAG with source and sink unchained (either because the parallelisms of the two ops are changed to be unequal or chaining is disabled). This greatly limits the flexibility to perform chain-breaking/joining for performance tuning.

*Proposal*

Introduce {{StreamGraphHasherV3}} that is agnostic to the chaining behavior of operators, which effectively just removes L227-235 of [flink/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphHasherV2.java at master · apache/flink (github.com)|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphHasherV2.java]. 

This will not hurt the deteministicity of the ID generation across job submission as long as the stream graph topology doesn't change, and since new versions of Flink have already adopted pure operator-level state recovery, this will not break state recovery across job submission as long as both submissions use the same hasher.

This will, however, break cross-version state compatibility. So we can introduce a new option to enable using HasherV3 in v1.19 and consider making it the default hasher in v2.0.

Looking forward to suggestions on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 12:15:10 UTC 2024,,,,,,,,,,"0|z1miiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/24 11:27;Zhanghao Chen;Hi [~xtsong], reaching out to you as I think it'll be good to have it in the Flink 2.0 roadmap as well, please instruct me if there's anything else that need to be done to put it in the roadmap. The current chianing-aware mechanism seems to be a tech debt resulted from pre-1.3.x series where Flink does not support operator-level state recovery within an operator-chain. It's no longer an issue after v1.3, but users have been suffering from this since then. It's time to make a change.;;;","03/Jan/24 01:32;xtsong;Thanks for reaching out, [~Zhanghao Chen].

Just some quick responses, I would need to look a bit more into the related components before giving further comments.

Based on your description, in general I think it makes sense to make operator id generation independent from chaining. However, as you have already mentioned, this is a breaking change that may result in state incompatibility. Therefore, I think it deserves a FLIP discussion and an official vote.;;;","03/Jan/24 02:04;Zhanghao Chen;Thanks for the quick response, I'll draft a FLIP and raise discussion first.;;;","07/Feb/24 09:31;Zhanghao Chen;Hi [~srichter], I'm trying to make the hasher upgrade backwards-compatible by reviving the idea of FLINK-5290 authored by you back in Flink 1.2. The details can be found in the linked FLIP doc. Could you kindly help take a review? Thanks a lot in advance.;;;","07/Feb/24 12:15;srichter;Hi [~Zhanghao Chen] ! The proposed change in this Jira makes sense to me, and I think that using the same idea as outlined in the FLIP should also work for this case. From the top of my head I don't see a problem to revive the previous mechanism for compatibility.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle may hang when exclusive buffers per channel is set to 0,FLINK-33961,13563306,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,Jiang Xin,Jiang Xin,02/Jan/24 02:39,02/Jan/24 07:15,04/Jun/24 20:40,02/Jan/24 07:15,,,,,,,,,,,,,Runtime / Network,,,,0,,,,"I found that the Hybrid Shuffle without enabling new mode may hang when exclusive-buffers-per-channel is set to 0. It can be reproduced by adding the following test into `HybridShuffleITCase.java` and running it.
{code:java}
@RepeatedTest(10)
void testHybridFullExchangesWithNonBuffersPerChannel() throws Exception {
    final int numRecordsToSend = 10000;
    Configuration configuration = configureHybridOptions(getConfiguration(), false);
    configuration.set(
            NettyShuffleEnvironmentOptions.NETWORK_HYBRID_SHUFFLE_ENABLE_NEW_MODE, false);
    configuration.set(NETWORK_BUFFERS_PER_CHANNEL, 0);
    JobGraph jobGraph = createJobGraph(numRecordsToSend, false, configuration);
    executeJob(jobGraph, configuration, numRecordsToSend);
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 02 06:57:27 UTC 2024,,,,,,,,,,"0|z1mih4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/24 02:51;Jiang Xin;[~Weijie Guo] Do you have any idea about it?;;;","02/Jan/24 06:38;Weijie Guo;[~Jiang Xin] Thanks for reporting this. Unfortunately, this issue can be considered somewhat by design. In order to avoid additional overhead, we allow the upstream not to calculate the exact backlog, which would result in {{exclusive-buffers-per-channel}} not being set to 0. It can greatly affect performance and may even block the job.

Therefore, we have the following instructions in the documentation: When the legacy Hybrid shuffle mode is used, decreasing the number of exclusive buffers per channel will seriously affect the performance. Therefore, this value should not be set to 0. 

That's one of the reasons we introduced the new hybrid shuffle mode(i.e. TieredStorage Shuffle). If there are no further questions, I will close this issue then.;;;","02/Jan/24 06:57;Jiang Xin;[~Weijie Guo] Thanks for the explanation, I have no more questions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive Scheduler doesn't respect the lowerBound for tasks,FLINK-33960,13563264,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,01/Jan/24 03:12,18/Feb/24 09:38,04/Jun/24 20:40,18/Feb/24 09:38,1.17.2,1.18.1,,,,,,,1.18.2,1.19.0,1.20.0,,Runtime / Coordination,,,,0,pull-request-available,,,"Adaptive Scheduler doesn't respect the lowerBound for tasks when one flink job has more than 1 tasks.

 

When we using the adaptive scheduler and the rescale api, users will set the lowerBound and upperBound for each job vertices. And users expect the parallelism of all vertices between lowerBound and upperBound.

But when one flink job  has more than 1 vertex, and resource isn't enough. Some of lowerBound won't be respect.
h2. How to reproduce this bug:

One job has 2 job vertices, we set the resource requirements are:
 * Vertex1: lowerBound=2, upperBound=2
 * Vertex2: lowerBound=8, upperBound=8

They are same slotSharingGroup, and we only 5 available slots. This job shouldn't run due to the slots cannot meets the resource requiremnt for vertex2.

But the job can runs, and the parallelism of vertex2 is 5.

 
h2. Why does this  bug happen?

Flink calculates the minimumRequiredSlots for each slot sharing group, it should be the {color:#FF0000}max{color} lowerBound for all vertices of current slot sharing group.

But it's using the on the {color:#FF0000}minimum{color} lowerBound.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 08 02:10:56 UTC 2024,,,,,,,,,,"0|z1mi80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/24 02:10;fanrui;Merged to:

master(1.20) via: 777e96f0fbd90e5a45366c0fd54bda85dc813b94 and 71336d6c874cd3e4da3b694e22df132dff51a6a8

1.19 via: 891a9755eba1ba1be7598c35e8b3060dbf28b482 and d039c6f8ace1861517064779999bfa95ac312218

1.18 via: ae8f8b1eb839dff388f3435a3d16e0db33e1e41e and 6cd71506fe474eca3b02fbb064912fba9f242b94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test case ChangelogRecoveryITCase.testMaterialization on 1.17 release branch,FLINK-33959,13563157,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,RocMarshal,RocMarshal,29/Dec/23 11:50,02/Jan/24 10:04,04/Jun/24 20:40,02/Jan/24 09:44,1.17.0,1.17.1,1.17.2,,,,,,,,,,Tests,,,,0,,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55940&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7]

 
Dec 29 07:35:05 [ERROR] ChangelogRecoveryITCase.testMaterialization 
Dec 29 07:35:05 [INFO] Run 1: PASS 
Dec 29 07:35:05 [ERROR] Run 2: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2, backoffTimeMS=0) 
 
Dec 29 07:35:05 [INFO] Run 3: PASS 

 
{code:java}
Dec 29 07:31:37 Caused by: java.io.FileNotFoundException: /tmp/junit666440189967214661/junit7847292894745938127/7c8c2cdf87500be80d28b1001902edcc/dstl/3cc55f1f-bd4f-4bc6-a41d-e8d91b54b4b0 (No such file or directory)
Dec 29 07:31:37 	at java.io.FileInputStream.open0(Native Method)
Dec 29 07:31:37 	at java.io.FileInputStream.open(FileInputStream.java:195)
Dec 29 07:31:37 	at java.io.FileInputStream.<init>(FileInputStream.java:138)
Dec 29 07:31:37 	at org.apache.flink.core.fs.local.LocalDataInputStream.<init>(LocalDataInputStream.java:50)
Dec 29 07:31:37 	at org.apache.flink.core.fs.local.LocalFileSystem.open(LocalFileSystem.java:134)
Dec 29 07:31:37 	at org.apache.flink.core.fs.SafetyNetWrapperFileSystem.open(SafetyNetWrapperFileSystem.java:87)
Dec 29 07:31:37 	at org.apache.flink.runtime.state.filesystem.FileStateHandle.openInputStream(FileStateHandle.java:69)
Dec 29 07:31:37 	at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:89)
Dec 29 07:31:37 	at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
Dec 29 07:31:37 	at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
Dec 29 07:31:37 	... 21 more
 {code}
 ",,,,,,,,,,,,,,,,,,FLINK-28440,,,,,,,,,,,,,,,,,,,,"29/Dec/23 11:51;RocMarshal;image-2023-12-29-19-51-35-996.png;https://issues.apache.org/jira/secure/attachment/13065667/image-2023-12-29-19-51-35-996.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 02 10:04:16 UTC 2024,,,,,,,,,,"0|z1mhk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/23 11:53;RocMarshal;Hi, could someone help to check it ?

Thank you very much :);;;","29/Dec/23 11:54;fanrui;Hi [~masteryhx] [~Feifan Wang] ,  the ChangelogRecoveryITCase.testMaterialization test fails in 1.17 release branch. Would you mind helping follow it in your free time? Thanks a lot!;;;","02/Jan/24 09:45;martijnvisser;Closed as duplicate of FLINK-28440, as explained at https://issues.apache.org/jira/browse/FLINK-30107?focusedCommentId=17672306&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17672306;;;","02/Jan/24 10:04;fanrui;Thanks [~martijnvisser] for the reminder and closing. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for IntervalJoin node,FLINK-33958,13563137,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bvarghese,bvarghese,bvarghese,29/Dec/23 02:56,12/Feb/24 08:28,04/Jun/24 20:40,12/Feb/24 08:28,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 12 08:28:36 UTC 2024,,,,,,,,,,"0|z1mhfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 17:53;dwysakowicz;Implemented in 3688654152282bbd2ae79546b94a3429fef1bd3f..534df6490e0fc179173efabd882ff10a749d508f;;;","18/Jan/24 15:12;Sergey Nuyanzin;Sorry have to reopen since the test introduced within this Jira issue is failing
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56564&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10773
{noformat}
Jan 18 13:57:06 13:57:06.823 [ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.IntervalJoinRestoreTest.testRestore(TableTestProgram, ExecNodeMetadata)[1] -- Time elapsed: 0.883 s <<< FAILURE!
Jan 18 13:57:06 java.lang.AssertionError: 
Jan 18 13:57:06 
Jan 18 13:57:06 Expecting actual:
Jan 18 13:57:06   [""+I[1, 2020-04-15 08:00:01, 2020-04-15 08:00:02]"",
Jan 18 13:57:06     ""+I[2, 2020-04-15 08:00:02, 2020-04-15 08:00:05]"",
Jan 18 13:57:06     ""+I[5, 2020-04-15 08:00:05, 2020-04-15 08:00:06]"",
Jan 18 13:57:06     ""+I[10, 2020-04-15 08:00:11, 2020-04-15 08:00:13]""]
Jan 18 13:57:06 to contain exactly in any order:
Jan 18 13:57:06   [""+I[1, 2020-04-15 08:00:01, 2020-04-15 08:00:02]"",
Jan 18 13:57:06     ""+I[2, 2020-04-15 08:00:02, 2020-04-15 08:00:05]"",
Jan 18 13:57:06     ""+I[5, 2020-04-15 08:00:05, 2020-04-15 08:00:06]"",
Jan 18 13:57:06     ""+I[7, 2020-04-15 08:00:09, 2020-04-15 08:00:11]"",
Jan 18 13:57:06     ""+I[10, 2020-04-15 08:00:11, 2020-04-15 08:00:13]""]
Jan 18 13:57:06 but could not find the following elements:
Jan 18 13:57:06   [""+I[7, 2020-04-15 08:00:09, 2020-04-15 08:00:11]""]
Jan 18 13:57:06 
Jan 18 13:57:06 	at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:299)
Jan 18 13:57:06 	at java.lang.reflect.Method.invoke(Method.java:498)
Jan 18 13:57:06 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
{noformat}
[~bvarghese], [~dwysakowicz] could you please have a look?;;;","22/Jan/24 16:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56564&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10772;;;","23/Jan/24 23:58;bvarghese;Apologize for the delayed response. Im trying to reproduce this locally but Im unable to. I tried running the tests in a loop until failure in IntelliJ to reproduce however that did not work. One option I can think of is adjusting the input data to make the output more predictable. Let me try that. If that doesn't work I can disable that test temporarily or revert the commit.;;;","05/Feb/24 08:22;mapohl;https://github.com/apache/flink/actions/runs/7778642859/job/21208792097#step:10:11231
https://github.com/apache/flink/actions/runs/7778642859/job/21208819419#step:10:11219

Is this actually a production issue or a test issue?

[~lincoln] fyi;;;","05/Feb/24 08:35;mapohl;https://github.com/apache/flink/actions/runs/7778642859/job/21208792097#step:10:11231;;;","05/Feb/24 08:35;mapohl;https://github.com/apache/flink/actions/runs/7778642859/job/21208819419#step:10:11219;;;","05/Feb/24 09:53;dwysakowicz;[~bvarghese] Did you have time to look into the issue?;;;","06/Feb/24 07:39;lincoln.86xy;[~mapohl] After looked at the commit history of sql TimeIntervalJoin operator, the test failure above was most probably a bug(or by-design behavior) in previous version.;;;","12/Feb/24 08:25;mapohl;* https://github.com/apache/flink/actions/runs/7831121355/job/21367168844#step:10:11257;;;","12/Feb/24 08:28;dwysakowicz;We merged a fix for the test:
* master: 1fbf92dfc9ee0e111d6ec740fe87fae27ef87d8b
* 1.19: 04d3b1b1423676dc87c366841b1e521beb9953dc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generic Log-Based Incremental Checkpoint fails when Task Local Recovery is enabled,FLINK-33957,13563134,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,prabhujoseph,prabhujoseph,29/Dec/23 02:22,29/Dec/23 02:25,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,,0,,,,"Generic Log-Based Incremental Checkpoint fails when Task Local Recovery is enabled. The issue happened to one of our users. I am trying to reproduce the issue.

{code}
at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:298) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	... 3 more
Caused by: org.apache.flink.util.SerializedThrowable: java.lang.IllegalStateException: one checkpoint contains at most one materializationID
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	at org.apache.flink.runtime.state.ChangelogTaskLocalStateStore.updateReference(ChangelogTaskLocalStateStore.java:92) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	at org.apache.flink.runtime.state.ChangelogTaskLocalStateStore.storeLocalState(ChangelogTaskLocalStateStore.java:130) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	at org.apache.flink.runtime.state.TaskStateManagerImpl.reportTaskStateSnapshots(TaskStateManagerImpl.java:140) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.reportCompletedSnapshotStates(AsyncCheckpointRunnable.java:237) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:136) ~[flink-dist-1.17.1-amzn-1.jar:1.17.1-amzn-1]
	... 3 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27692,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 29 02:25:41 UTC 2023,,,,,,,,,,"0|z1mhf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/23 02:25;prabhujoseph;[~Yanfei Lei] Could you take a look into this? Any idea when this issue could happen? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump org.apache.zookeeper:zookeeper from 3.7.1 to 3.7.2,FLINK-33956,13563112,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,martijnvisser,martijnvisser,martijnvisser,28/Dec/23 14:19,28/Dec/23 14:19,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"Bumps org.apache.zookeeper:zookeeper from 3.7.1 to 3.7.2.

 Merging this pull request will resolve a critical severity [Dependabot alert|https://github.com/apache/flink/security/dependabot/184] on org.apache.zookeeper:zookeeper.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-28 14:19:40.0,,,,,,,,,,"0|z1mha8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedFileSystemException when trying to save data to Azure's abfss File System,FLINK-33955,13563096,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,alekland,alekland,28/Dec/23 11:44,28/Dec/23 13:33,04/Jun/24 20:40,,1.17.1,1.18.0,,,,,,,,,,,,,,,0,,,,"When using Azure's File System connector for reading and writing files to Azure Data Lake Storage 2 Flink job fails at writing files with given error:

 
{noformat}
Caused by: org.apache.hadoop.fs.UnsupportedFileSystemException: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme ""file""{noformat}
 

Full logs from Job Manager along with stack trace is attached to as [^error.log] file.

The connection itself seems to be good, as the job successfully creates desired structure inside ADLS (and the the `.part` file), but the file itself is empty.

The job is simple, as its only purpose is to save events `a`, `b` and `c` into a file on ADLS. The whole code is presented below:
{code:java}
import org.apache.flink.api.common.serialization.SimpleStringEncoder;
import org.apache.flink.connector.file.sink.FileSink;
import org.apache.flink.core.fs.Path;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
public class DataStreamJob {
  public static void main(String[] args) throws Exception {
    final FileSink<String> sink = FileSink
        .forRowFormat(
            new Path(""abfss://test@stads2dev01.dfs.core.windows.net/output""),
            new SimpleStringEncoder<String>(""UTF-8""))
        .build();
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    env.fromElements(""a"", ""b"", ""c"").sinkTo(sink);
    env.execute(""Test"");
  }
}
{code}
Code is run locally using Flink 1.18.0 (the same behavior was present in version 1.17.1). The only change that was made to `flink-conf.yaml` was to add key for accessing Azure:

 
{code:java}
fs.azure.account.auth.type.stads2dev01.dfs.core.windows.net: SharedKey
fs.azure.account.key.stads2dev01.dfs.core.windows.net: ******{code}
 

The [^pom.xml] file was created by using [Getting Started|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/configuration/overview/#getting-started] documentation - the only thing I added was `flink-azure-fs-hadoop` connector. The whole [^pom.xml] file is attached. The connector JAR was also copied from `opt` directory to `plugins/azure-fs-hadoop` in cluster files according to the documentation.

The interesting fact is that the deprecated method `writeAsText` (instead of FileSink) not only works and creates desired file on ADLS, but *the subsequent jobs that use FileSInk that previously failed now works and creates file successfully* (until cluster's restart). The logs from job with deprecated method are also attached here as [^success.log] file.

I suspect that it is somehow connected to how Azure File System is initialized, where the new FileSink method would create it incorrectly.","Flink 1.17.1 & Flink 1.18.0 with Java 11, ADLS Gen.2 with hierarchical namespace enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/23 11:32;alekland;error.log;https://issues.apache.org/jira/secure/attachment/13065653/error.log","28/Dec/23 11:32;alekland;pom.xml;https://issues.apache.org/jira/secure/attachment/13065652/pom.xml","28/Dec/23 11:32;alekland;success.log;https://issues.apache.org/jira/secure/attachment/13065651/success.log",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 13:27:06 UTC 2023,,,,,,,,,,"0|z1mh6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/23 11:48;martijnvisser;[~alekland] I think this is a duplicate of FLINK-32241, WDYT?;;;","28/Dec/23 13:27;alekland;[~martijnvisser] Thanks for pointing it out. I was testing the settings of the file class and AbstractFileSystem, but I didn't notice that with the array.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large record may cause the hybrid shuffle hang,FLINK-33954,13563074,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Jiang Xin,Jiang Xin,28/Dec/23 07:41,02/Jan/24 06:15,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,"In some cases, the job may hang when there are not enough buffers in the local buffer pool. For instance, the parallelism is 4, so the HashBufferAccumulator is used. The size of the local buffer pool can be 5, and at some point, 3 of all buffers are required by 3 subpartitions and are not finished, so only 2 buffers are left. If a record that is larger than 2 buffers comes, the program would hang at requesting buffers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-28 07:41:22.0,,,,,,,,,,"0|z1mh1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump com.google.guava:guava from 31.1-jre to 33.0.0-jre,FLINK-33953,13563032,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Dec/23 13:55,27/Dec/23 15:26,04/Jun/24 20:40,27/Dec/23 15:26,,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,Resolves two Dependabot reports https://github.com/apache/flink-connector-jdbc/security/dependabot?q=package%3Acom.google.guava%3Aguava+manifest%3Apom.xml+has%3Apatch by upgrading to the now latest available version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 27 15:26:01 UTC 2023,,,,,,,,,,"0|z1mgsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 15:26;martijnvisser;Fixed in apache/flink-connector-jdbc:main 32c1ca4e21eb360fc1c51bcafd21f42ff322c258;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink JDBC connector build fail on JDK 17 ,FLINK-33952,13563030,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,snuyanzin,martijnvisser,martijnvisser,27/Dec/23 13:47,28/Dec/23 08:35,04/Jun/24 20:40,28/Dec/23 08:35,jdbc-3.2.0,,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,test-stability,,"{code:java}
aused by: java.lang.RuntimeException: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @75eeccf5
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:69)
	at org.apache.flink.api.java.typeutils.runtime.kryo.FlinkChillPackageRegistrar.registerSerializers(FlinkChillPackageRegistrar.java:67)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:513)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:522)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:307)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:74)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:425)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:520)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:110)
	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest$TestEntrySource.emit(JdbcExactlyOnceSinkE2eTest.java:222)
	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest$TestEntrySource.emitRange(JdbcExactlyOnceSinkE2eTest.java:207)
	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest$TestEntrySource.run(JdbcExactlyOnceSinkE2eTest.java:189)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338)
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @75eeccf5
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:67)
	... 16 more
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/7311274336/job/19920665057#step:14:533",,,,,,,,,,,,,,,,,,FLINK-33787,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 08:35:18 UTC 2023,,,,,,,,,,"0|z1mgs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 13:50;martijnvisser;[~snuyanzin] Was it intended to test if the v3.1 branch was working for Java 17, or was that only meant for the main branch? ;;;","27/Dec/23 14:19;jiabao.sun;Seems missing f8de82b4c52a688c5bd36c4c4bd3012ff4081eb8 on branch v3.1.
Shall we make a backport or drop jdk 11 and 17 in weekly.yml for v3.1?;;;","27/Dec/23 18:47;Sergey Nuyanzin;oops, seems I forgot to create a backport
since 3.1 works with Flink 1.18 supporting java 17 it would make sense to backport it
thanks, created a backport PR;;;","28/Dec/23 08:35;martijnvisser;Fixed by backporting FLINK-33787 to v3.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"should use aggCallNeedRetractions instead of needRetraction to check retract method, orelse throw not implement retract method",FLINK-33951,13563006,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,27/Dec/23 10:19,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"we check whether exists retract method using needRetraction which is operator level. but not all of the agg should use agg with retract.

so we should check agg when there are is needRetraction 

for example the planner has improvement for max/min function using RelMd Monotonicity. such as there are have two level agg, first agg produce count and second agg max(cnt). we can use MaxAggFunction instead of MaxWithRetractAggFunction because MaxWithRetractAggFunction has low performance.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 09:00:18 UTC 2024,,,,,,,,,,"0|z1mgmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 14:07;martijnvisser;[~jackylau] Can you please add a better description for this ticket? ;;;","02/Jan/24 01:40;xuyangzhong;I'm also a little curious about the background of this jira. BTW, judging from the title, this should be an improvement or tech doubt, instead of a bug, right?;;;","02/Jan/24 02:32;jackylau;[~xuyangzhong] yeap, not a bug.However, this might seem strange to developers. We can transform it into function-level checks.;;;","02/Jan/24 07:57;martijnvisser;[~jackylau] Still, please add a better Jira description. It might be obvious for you, but we need to make sure that its clear for everyone, including in the future when someone would like to say why a certain change was made. ;;;","19/Jan/24 09:00;jackylau;[~martijnvisser] i change the description and ci has passed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update max aggregate functions to new type system,FLINK-33950,13562991,13244062,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,27/Dec/23 08:14,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 09:25:22 UTC 2024,,,,,,,,,,"0|z1mgjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 09:13;jackylau;hi [~twalthr]  do you have time to help review it?;;;","27/Dec/23 14:08;martijnvisser;[~jackylau] Before pinging people, can you first make sure that the CI for your PR has passed? ;;;","29/Dec/23 09:04;jackylau;hi [~martijnvisser] [~twalthr]  the reason of  the ci of my pr why not passed is when we migrate to new stack agg.

the max function accumulate type is RowData. and it will cause the batch plan from hashAgg to SortAgg  because the bellowing code in BatchPhysicalHashAggRule.

and it may cause a decrease in performance. So we need to reach a consensus on whether changes are necessary， could i migrate to

new stach agg now.
 
 
{code:java}
// BatchPhysicalHashAggRule
// HashAgg cannot process aggregate whose agg buffer is not fix length
isAggBufferFixedLength(agg) && {code}
 

 ;;;","02/Jan/24 11:37;martijnvisser;How much performance decrease are we talking about?;;;","06/Jan/24 15:00;jackylau; [~martijnvisser] I haven't specifically tested it, but sort-based approaches do have some performance overhead compared to hash-based. If you think that's acceptable, I'll go ahead and change it to sort-based aggregation to make  the CI passed.;;;","09/Jan/24 04:21;jackylau;hi [~martijnvisser] what is your opinion?;;;","09/Jan/24 08:17;martijnvisser;[~jackylau] We need to migrate them to the new type system anyway. If we don't know the performance implications, we should either consider adding them (Also pinging [~Zakelly] in case we already have a benchmark for this) or run a test on a PoC to make it a factual discussion.;;;","09/Jan/24 11:08;zakelly;[~martijnvisser] IIUC, there is no existing benchmark covering this.;;;","15/Jan/24 19:30;martijnvisser;[~jackylau] Can you create a POC and see what would be the impact of migrating this?
CC [~dwysakowicz] for his thoughts on this;;;","16/Jan/24 02:08;jackylau;hi [~martijnvisser] [~Zakelly] [~dwysakowicz] , i found it will have 69 test failed https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/build/builds/55946/logs/104. and if we change the plan to make the test passed . But some test we should not change, because some unit test is for construct a hash agg to test some other rules, if we change it to sort agg, the test will not take effective.;;;","19/Jan/24 03:12;jackylau;hi [~twalthr]  [~martijnvisser]  [~dwysakowicz] what is your opinion?;;;","19/Jan/24 09:01;twalthr;Hi [~jackylau], sorry for not replying earlier. I get too many pings at the moment. I'm missing some context to this issue. Could you please fill out the description of this ticket? Looking at the current implementation of MaxAggFunction it uses DeclarativeAggregateFunction. So the function is code generated. Thus, DeclarativeAggregateFunction should use the new type system implicitly, no?;;;","19/Jan/24 09:10;twalthr;I read through the comments. Isn't it enough to adjust isAggBufferFixedLength to be more flexible? Even though the accumlator is a row type, the type can still be fixed length if the subtypes are, no?;;;","19/Jan/24 09:14;jackylau;[~twalthr] yeap, but BinaryRowData's doesn't have setRowData method, these code is draw inspiration from Spark, spark also doesn't support. so it may not be easy to do. and there are lots of DeclarativeAggregateFunction we should imigrate. we should fix it , will you have plan to support it?
{code:java}
/**
 * If it is a fixed-length field, we can call this BinaryRowData's setXX method for in-place
 * updates. If it is variable-length field, can't use this method, because the underlying data
 * is stored continuously.
 */
public static boolean isInFixedLengthPart(LogicalType type) { {code};;;","19/Jan/24 09:25;jackylau;[~twalthr]  bellow is spark code
{code:java}
val useHash = Aggregate.supportsHashAggregate(
  aggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))
val forceSortAggregate = forceApplySortAggregate(child.conf)

if (useHash && !forceSortAggregate) {
  HashAggregateExec(..)
}

def isAggregateBufferMutable(schema: StructType): Boolean = {
  schema.forall(f => UnsafeRow.isMutable(f.dataType))
}

def supportsHashAggregate(aggregateBufferAttributes: Seq[Attribute]): Boolean = {
  val aggregationBufferSchema = StructType.fromAttributes(aggregateBufferAttributes)
  isAggregateBufferMutable(aggregationBufferSchema)
}

/**
 * Field types that can be updated in place in UnsafeRows (e.g. we support set() for these types)
 */
public static boolean isMutable(DataType dt) {
  if (dt instanceof UserDefinedType) {
    return isMutable(((UserDefinedType<?>) dt).sqlType());
  }
  PhysicalDataType pdt = PhysicalDataType.apply(dt);
  return pdt instanceof PhysicalPrimitiveType || pdt instanceof PhysicalDecimalType ||
    pdt instanceof PhysicalCalendarIntervalType;
}


and these type do not have RowTypw



 {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
METHOD_ABSTRACT_NOW_DEFAULT should be both source compatible and binary compatible,FLINK-33949,13562980,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,Wencong Liu,Wencong Liu,27/Dec/23 06:08,03/Jan/24 02:50,04/Jun/24 20:40,03/Jan/24 02:50,1.19.0,,,,,,,,1.19.0,,,,Test Infrastructure,,,,0,,,,"Currently  I'm trying to refactor some APIs annotated by @Public in [FLIP-382: Unify the Provision of Diverse Metadata for Context-like APIs - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-382%3A+Unify+the+Provision+of+Diverse+Metadata+for+Context-like+APIs]. When an abstract method is changed into a default method, the japicmp maven plugin names this change METHOD_ABSTRACT_NOW_DEFAULT and considers it as source incompatible and binary incompatible.

The reason maybe that if the abstract method becomes default, the logic in the default method will be ignored by the previous implementations.

I create a test case in which a job is compiled with newly changed default method and submitted to the previous version. There is no exception thrown. Therefore, the METHOD_ABSTRACT_NOW_DEFAULT shouldn't be incompatible both for source and binary. We could add the following settings to override the default values for binary and source compatibility, such as:
{code:java}
<overrideCompatibilityChangeParameters>
    <overrideCompatibilityChangeParameter>
       <compatibilityChange>METHOD_ABSTRACT_NOW_DEFAULT</compatibilityChange>
       <binaryCompatible>true</binaryCompatible>
       <sourceCompatible>true</sourceCompatible>
    </overrideCompatibilityChangeParameter>
</overrideCompatibilityChangeParameters> {code}
By the way, currently the master branch checks both source compatibility and binary compatibility between minor versions. According to Flink's API compatibility constraints, the master branch shouldn't check binary compatibility. There is already jira FLINK-33009 to track it and we should fix it as soon as possible.

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 02:28:14 UTC 2024,,,,,,,,,,"0|z1mggw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 14:15;martijnvisser;[~Wencong Liu] Your change could still be source incompatible, for the situations where someone relied on old behavior of the method, even if there's no error thrown right now for your test setup?;;;","27/Dec/23 14:15;martijnvisser;[~mapohl] Also curious on your thoughts here;;;","27/Dec/23 14:31;Wencong Liu;Thanks [~martijnvisser] for your comments. The implementation classes of the @Public API have already overridden the abstract methods. After an abstract method becomes default, the behaviors of these implementation classes will not change any behaviors themselves. WDYT? 😄;;;","27/Dec/23 14:44;martijnvisser;I was thinking about situations where users have build implementations themselves, not so much Flink internally. ;;;","27/Dec/23 15:02;Wencong Liu;For the users have built implementations themselves, they still don't need any code changes when they upgrade to a new version with abstract->default changes. This change could ensure the source compatibility. [~martijnvisser] ;;;","27/Dec/23 15:47;martijnvisser;[~Wencong Liu] Thanks. For reference, per the JAPICMP project https://github.com/siom79/japicmp/issues/201 which refers to https://docs.oracle.com/javase/specs/jls/se8/html/jls-13.html#jls-13.5.6

{quote}Adding a default method, or changing a method from abstract to default, does not break compatibility with pre-existing binaries, but may cause an IncompatibleClassChangeError if a pre-existing binary attempts to invoke the method.{quote}

;;;","28/Dec/23 03:44;Wencong Liu;Suppose we have two completely independent interfaces, I and J, both declaring a default method M with the same signature. Now, if there is a class T that implements both interfaces I and J but *does not override* the conflicting method M, the compiler would not know which interface's default method implementation to use, as they both have equal priority. If the code containing class T tries to invoke this method at runtime, the JVM would throw an {{IncompatibleClassChangeError}} because it is faced with an impossible decision: it does not know which interface’s default implementation to call.

However, if M is abstract in I or J, the implementation class T *must* provides an explicit implementation of the method. So no matter how interfaces I or J change (as long as the signature of their method  M does not change), it will not affect the behavior of the implementation class T or cause an {{{}IncompatibleClassChangeError{}}}. Class T will continue to use its own method M implementation, disregarding any default implementations from the two interfaces.

 

I have create a test case, where the StreamingRuntimeContext will be added a method return TestObject:
{code:java}
public class TestObject implements TestInterface1, TestInterface2 {
    @Override
    public String getResult() {
        return ""777"";
    }
} 
public interface TestInterface1 {
    String getResult();
}
public interface TestInterface2 {
    default String getResult() {
        return ""666"";
    }
}{code}
The job code is in the follows. The job is compiled with the modifiled StreamingRuntimeContext in Flink.
{code:java}
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();
    DataStreamSource<Integer> source =
            executionEnvironment.fromData(3, 2, 1, 4, 5, 6, 7, 8);
    SingleOutputStreamOperator<String> result = source.map(new RichMapFunction<Integer, String>() {
        @Override
        public String map(Integer integer) {
            StreamingRuntimeContext runtimeContext = (StreamingRuntimeContext)getRuntimeContext();
            return runtimeContext.getTestObject().getResult();
        }
    });
    CloseableIterator<String> jobResult = result.executeAndCollect();
    while (jobResult.hasNext())
        System.out.println(jobResult.next());
} {code}
When I change the abstract method getResult into default in TestInterface1 and recompiled Flink. The job is still able to finish without any code changes and exceptions.

Therefore, I think the METHOD_ABSTRACT_NOW_DEFAULT doesn't break source compatibility. WDYT? [~martijnvisser] 

 ;;;","28/Dec/23 10:30;martijnvisser;[~Wencong Liu] I think this is a Flink specific situation and the JAPICMP plugin follows the specification. There is the option to ignore this error, but unfortunately you can only set those on a global level. I'm not sure that the global situation for Flink should be that METHOD_ABSTRACT_NOW_DEFAULT is considered to be source and binary compatible. ;;;","28/Dec/23 10:31;martijnvisser;Let's wait for the holidays to pass in Europe to pass, so we can ask for input from [~mapohl] and [~chesnay];;;","01/Jan/24 09:44;chesnay;The JLS clearly states that there _may_ be cases where compatibility is broken. Us not coming up with a reproduction doesn't invalidate that.
Since our goal is to provide a _guarantee_ we shouldn't relax the rules imo.;;;","03/Jan/24 02:28;Wencong Liu;Thanks for the explanation from [~chesnay] . Given that all the actively running code might throw related exceptions, it would be unreasonable to directly modify the rules of japicmp. If there's a specific interface that needs to break this rule, we should simply exclude that interface. This ticket can be closed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of org.apache.flink.table.api.dataview.MapView#MapView,FLINK-33948,13562978,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,27/Dec/23 03:52,20/Feb/24 08:44,04/Jun/24 20:40,20/Feb/24 08:44,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 08:44:31 UTC 2024,,,,,,,,,,"0|z1mggg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 08:44;jackylau;merged c24e2c9a8c77406f0f4ca6b5b50e8d2b2ac5d811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bugs about prefix in DelegatingConfiguration ,FLINK-33947,13562977,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,27/Dec/23 03:35,29/Dec/23 15:19,04/Jun/24 20:40,29/Dec/23 15:19,,,,,,,,,1.17.3,1.18.2,1.19.0,,API / Core,,,,0,pull-request-available,,,"It was resulted from [https://github.com/apache/flink/pull/23994#issuecomment-1869905090] 

-  Check and confirm other potential bug points

-  Fix the bugs about prefix key mapping when operating.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 29 07:40:37 UTC 2023,,,,,,,,,,"0|z1mgg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 14:07;RocMarshal;Before officially starting the repair, there are two issues worth discussing:

 
 - First:

Is the *_prefix_* attribute allowed to be {*}_null_{*}?

What are the risks in using *_prefix_* if it is not allowed to be {*}_null_{*}? Or will there be any defects?

 

From the current effect, if the *_prefix_* is {*}_null_{*}, methods such as *_hashCode()_* will result in *_NPE_*

Based on the feedback from the current community regarding the use of this class, there should be no instances of using a *_null_* {_}*prefix*{_}, unless there are no calls to methods that may result in _*NPE*_

 

There are two corresponding alternative solutions here:

1. It is not allowed for _*prefix*_ to be a *_null_* object, so there will be no corresponding method to cause _*NPE*_

2. Alternatively, allow the _*prefix*_ to be a _*null*_ object, but we need to design logic to handle _*null*_ situations for methods that may cause _*NPE*_ to avoid the occurrence of _*NPE*_

 

 
 - Secondly:

Regarding the mapping bug of _*prefix*_ values in various methods,

Only exists in the _*removeKey* *removeConfiguration*_ methods. We just need to fix and add tests;;;","27/Dec/23 14:26;fanrui;I  don't think prefix can be null. If it's null or empty, why don't use Configuration directly?;;;","27/Dec/23 14:44;RocMarshal;hi, [~fanrui] thanks for your attention.

I want to set the prefix attribute as Nonnull (@Nonnull)
There are two benefits to this:

- Ensure that potential _*NPE*_ risks can be addressed

- Maintain minimal changes (in my limited read, since the initial design allowed prefixes to default to empty strings, I tend to maintain this default strategy to ensure some backward compatibility space is left. Of course, if it is necessary not to allow prefixes to be empty strings, I am happy to make corresponding changes)

Please let me know what's your opinion~;;;","27/Dec/23 14:47;fanrui;Mark the prefix as @Nonnull makes sense. And how about allow prefix is empty?;;;","27/Dec/23 15:04;RocMarshal;> And how about allow prefix is empty?

SGTM +1.;;;","29/Dec/23 07:40;fanrui;Merged to

master(1.19) via : 145b99e388550f582c8ee577689f651acf2b6321

1.18 via  : 9bb6fd3228795e45d80e7850823d0e29a0725e11

1.17 via: 78dfe14b5c9ff495fd38beb7c2d7f88b341f8c20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDb sets setAvoidFlushDuringShutdown to true to speed up Task Cancel,FLINK-33946,13562975,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mayuehappy,mayuehappy,mayuehappy,27/Dec/23 03:24,12/Jan/24 06:37,04/Jun/24 20:40,12/Jan/24 06:37,1.19.0,,,,,,,,1.19.0,,,,Runtime / State Backends,,,,0,pull-request-available,,,"When a Job fails, the task needs to be canceled and re-deployed. RocksDBStatebackend will call RocksDB.close when disposing.


{code:java}
if (!shutting_down_.load(std::memory_order_acquire) &&
    has_unpersisted_data_.load(std::memory_order_relaxed) &&
    !mutable_db_options_.avoid_flush_during_shutdown) {
  if (immutable_db_options_.atomic_flush) {
    autovector<ColumnFamilyData*> cfds;
    SelectColumnFamiliesForAtomicFlush(&cfds);
    mutex_.Unlock();
    Status s =
        AtomicFlushMemTables(cfds, FlushOptions(), FlushReason::kShutDown);
    s.PermitUncheckedError();  //**TODO: What to do on error?
    mutex_.Lock();
  } else {
    for (auto cfd : *versions_->GetColumnFamilySet()) {
      if (!cfd->IsDropped() && cfd->initialized() && !cfd->mem()->IsEmpty()) {
        cfd->Ref();
        mutex_.Unlock();
        Status s = FlushMemTable(cfd, FlushOptions(), FlushReason::kShutDown);
        s.PermitUncheckedError();  //**TODO: What to do on error?
        mutex_.Lock();
        cfd->UnrefAndTryDelete();
      }
    }
  } {code}


By default (avoid_flush_during_shutdown=false) RocksDb requires FlushMemtable when Close. When the disk pressure is high or the Memtable is large, this process will be more time-consuming, which will cause the Task to get stuck in the Canceling stage and affect the speed of job Failover.
In fact, it is completely unnecessary to Flush memtable when Flink Task is Close, because the data can be replayed from Checkpoint. So we can set avoid_flush_during_shutdown to true to speed up Task Failover",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 06:37:09 UTC 2024,,,,,,,,,,"0|z1mgfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/24 07:38;mayuehappy;[~masteryhx]   could you please take a look at this ticket ? ;;;","10/Jan/24 13:27;masteryhx;[~mayuehappy] Thanks for reporting this. It makes sense.

Would you like to improve this ?;;;","11/Jan/24 03:07;mayuehappy;[~masteryhx]  thanks , I would like to take this and  I'll draft the pr soon;;;","12/Jan/24 06:37;masteryhx;merged aa0c1b5e into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated org.apache.flink.table.api.dataview.ListView#ListView,FLINK-33945,13562973,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,27/Dec/23 03:21,21/Feb/24 02:52,04/Jun/24 20:40,21/Feb/24 02:52,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 02:52:20 UTC 2024,,,,,,,,,,"0|z1mgfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/24 02:52;jackylau;merged 060dcf8fc65e5fc3ade5b6037f7dfdc7919ba2b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Flink: Process to restore more than one job on job manager startup from the respective savepoints,FLINK-33944,13562970,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vrangana@in.ibm.com,vrangana@in.ibm.com,27/Dec/23 02:36,28/Dec/23 08:37,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,," 
We are using Flink (1.18) version for our Flink cluster. The job manager has been deployed in ""Application mode"" and we are looking for a process to restore multiple jobs (using their respective savepoint directories) when the job manager is started. Currently, we have the option to restore only one job while running ""standalone-job.sh"" using the --fromSavepoint and --allowNonRestoredState. However, we need a way to trigger multiple job executions via Java client (from its respective savepoint location) on Jobmanager startup.

Note: We are not using a Kubernetes native deployment, but we are using k8s standalone mode of deployment.

Additional Query: If there is a process to restore multiple jobs from its respective savepoints on ""Application mode"" of deployment, is the same supported on Session mode of deployment or not?

*Expected process:*
 # Before starting with the Flink/application image upgrade, trigger the savepoints for all the current running jobs.
 # Once the savepoints process completed for all jobs, will trigger the scale down of job manager and task manager instances.
 # Update the image version on the k8s deployment with the update application image.
 # After image version is updated, scale up the job manager and task manager.
 # We need a process to restore the previously running jobs from the savepoint dir and start all the jobs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 08:37:22 UTC 2023,,,,,,,,,,"0|z1mgeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 05:26;vrangana@in.ibm.com;[~wanglijie] Do you have any input on this information request for savepoint restore process for multiple jobs (via Java Client) or Job-manager startup (via standalone-job.sh or jobmanager.sh). ""standalone-job.sh"" supports only one job to be restore from savepoint on Jobmanager startup.;;;","27/Dec/23 14:51;martijnvisser;[~vrangana@in.ibm.com] Given that Application Mode is positioned to have a 1:1 mapping between a Flink Job Manager and a Flink Application, this feels like the wrong way to deploy this? I don't think we should go this route. I think you should consider Session Mode https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#application-mode;;;","27/Dec/23 16:38;vrangana@in.ibm.com;[~martijnvisser] Using ""application mode"" we can run multiple run multiple instance of job executions of a single flink application and ""session mode"" can also configured with the same, also it supports multiple flink application based job executions. We want to use the ""application mode"" to trigger savepoint for each job execution and restore for each job executions back once the flink upgrade / image upgrade. Please confirm if the version existing support this requirement on ""application mode"" or not?;;;","28/Dec/23 07:14;vrangana@in.ibm.com;[~martijnvisser] Can we use Aligned checkpointing instead of Savepoint for restore process when flink is upgraded?;;;","28/Dec/23 08:37;martijnvisser;[~vrangana@in.ibm.com] That's documented at https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/ops/state/checkpoints_vs_savepoints/#capabilities-and-limitations;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache flink: Issues after configuring HA (using zookeeper setting),FLINK-33943,13562966,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,vrangana@in.ibm.com,vrangana@in.ibm.com,27/Dec/23 02:00,27/Dec/23 06:06,04/Jun/24 20:40,27/Dec/23 02:45,1.18.0,,,,,,,,,,,,Build System,,,,0,,,,"Hi Team,

*Note:* Not sure whether I have picked the right component while raising the issue.

Good Day. I am using Flink (1.18) version and zookeeper (3.7.2) for our flink cluster. Job manager has been deployed on ""Application mode"" and when HA is disabled (high-availability.type: NONE) we are able to start multiple jobs (using env.executeAsyn()) for a single application. But when I setup the Zookeeper as the HA type (high-availability.type: zookeeper), we are only seeing only one job is getting executed on the Flink dashboard. Following are the parameters setup for the Zookeeper based HA setup on the flink-conf.yaml. Please let us know if anyone has experienced similar issues and have any suggestions. Thanks in advance for your assistance.

*Note:* We are using a Streaming application and following are the flink-config.yaml configurations.

*Additional query:* Does ""Session mode"" of deployment support HA for multiple execute() executions?
 # high-availability.storageDir: /opt/flink/data
 # high-availability.cluster-id: test
 # high-availability.zookeeper.quorum: localhost:2181
 # high-availability.type: zookeeper
 # high-availability.zookeeper.path.root: /dp/configs/flinkha","Flink version: 1.18

Zookeeper version: 3.7.2

Env: Custom flink docker image (with embedded application class) deployed over kubernetes (v1.26.11).

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 27 06:06:06 UTC 2023,,,,,,,,,,"0|z1mgds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 02:11;wanglijie;Hi [~vrangana@in.ibm.com], currently, HA in Application Mode is only supported for single-execute() applications. You can find more details in [flink docs|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/overview/#application-mode].
;;;","27/Dec/23 02:18;vrangana@in.ibm.com; 
The issue can be reproduced by enabling high-availability.type: zookeeper (with above config's specified on the issue) and in the flink client code try to call env.executeAsync() for multiple instance of job for the same application. Now open Dashboard and check the number of jobs running(same can be tried via REST api call too), then you will find only one job running. When you disable HA (high-availability.type: NONE), then you can see multiple jobs running (same can be seen via REST api call too).

REST api: http://<IP_ADDRESS>:8081/v1/jobs;;;","27/Dec/23 02:19;vrangana@in.ibm.com;[~wanglijie] Is the HA in session mode support execution of multiple execute/executeAsync operations? Sorry I am unable to find any documentation related to HA on session mode and its features / limitations.;;;","27/Dec/23 02:23;wanglijie;[~vrangana@in.ibm.com] I think it should work if you use session mode instead of application mode.;;;","27/Dec/23 02:43;wanglijie;I 'll close this issue because it's not a bug. cc [~vrangana@in.ibm.com] 

 ;;;","27/Dec/23 02:47;vrangana@in.ibm.com;[~wanglijie]  Thanks for the prompt update. Is there a plan to support of HA functionality on application mode (for multiple exections) in near future versions? (or) is there a technical reasoning why its not supported currently?;;;","27/Dec/23 03:02;wanglijie;Maybe you can see the comments in FLINK-19358 and [FLIP-85|https://cwiki.apache.org/confluence/display/FLINK/FLIP-85%3A+Flink+Application+Mode];;;","27/Dec/23 06:06;vrangana@in.ibm.com;Thanks [~wanglijie] for your inputs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelegatingConfiguration misses the perfix for some methods,FLINK-33942,13562912,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,26/Dec/23 08:51,29/Dec/23 02:13,04/Jun/24 20:40,29/Dec/23 02:13,,,,,,,,,1.17.3,1.18.2,1.19.0,,Runtime / Configuration,,,,0,pull-request-available,,,"DelegatingConfiguration misses the perfix for some methods, such as:
 * DelegatingConfiguration#getInteger(org.apache.flink.configuration.ConfigOption<java.lang.Integer>, int)
 * DelegatingConfiguration#getLong(org.apache.flink.configuration.ConfigOption<java.lang.Long>, long)
 * org.apache.flink.configuration.DelegatingConfiguration#getBoolean(org.apache.flink.configuration.ConfigOption<java.lang.Boolean>, boolean)
 * org.apache.flink.configuration.DelegatingConfiguration#getFloat(org.apache.flink.configuration.ConfigOption<java.lang.Float>, float)
 * org.apache.flink.configuration.DelegatingConfiguration#getDouble(org.apache.flink.configuration.ConfigOption<java.lang.Double>, double)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 29 02:13:30 UTC 2023,,,,,,,,,,"0|z1mg1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/23 02:13;fanrui;Merged to:

master(1.19) via: 969a5c62947ee512bc7d5a28bb4e03d15b3e3f2e

1.18 via: 0978d880b17f9039ffb9554df5bb49108a0275e8

1.17 via: ac7c7fa1ab900ed59260bb49750a1bc2b9f1d200;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use field reference index to compute window aggregate time attribute column,FLINK-33941,13562906,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,26/Dec/23 06:40,04/Jan/24 02:49,04/Jun/24 20:40,04/Jan/24 02:49,,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"In some exec nodes like StreamExecGroupWindowAggregate and some rules like BatchPhysicalWindowAggregateRule, planner uses ""AggregateUtil#timeFieldIndex"" to access the actual time field index, instead of using the time field index in LogicalWindow#timeAttribute directly. However, it would be more formal to use the field index instead of the column field.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 04 02:48:59 UTC 2024,,,,,,,,,,"0|z1mg0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/23 07:21;qingyue;Thanks, [~xuyangzhong] reporting this issue. You're right that we should rely on field index rather than field name mapping.;;;","04/Jan/24 02:48;qingyue;Fixed in master: b957480112c00d9d777247fc48b602e9908652a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the auto-derivation rule of max parallelism for enlarged upscaling space,FLINK-33940,13562904,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,26/Dec/23 06:24,03/Jan/24 09:13,04/Jun/24 20:40,,,,,,,,,,,,,,API / Core,,,,0,,,,"*Background*

The choice of the max parallelism of an stateful operator is important as it limits the upper bound of the parallelism of the opeartor while it can also add extra overhead when being set too large. Currently, the max parallelism of an opeartor is either fixed to a value specified by API core / pipeline option or auto-derived with the following rules:

{{min(max(roundUpToPowerOfTwo(operatorParallelism * 1.5), 128), 32767)}}

*Problem*

Recently, the elasticity of Flink jobs is becoming more and more valued by users. The current auto-derived max parallelism was introduced a time time ago and only allows the operator parallelism to be roughly doubled, which is not desired for elasticity. Setting an max parallelism manually may not be desired as well: users may not have the sufficient expertise to select a good max-parallelism value.

*Proposal*

Update the auto-derivation rule of max parallelism to derive larger max parallelism for better elasticity experience out of the box. A candidate is as follows:

{{min(max(roundUpToPowerOfTwo(operatorParallelism * {*}5{*}), {*}1024{*}), 32767)}}

Looking forward to your opinions on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/24 02:52;fanrui;image-2024-01-03-10-52-05-861.png;https://issues.apache.org/jira/secure/attachment/13065702/image-2024-01-03-10-52-05-861.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 09:13:11 UTC 2024,,,,,,,,,,"0|z1mg00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/23 06:29;Zhanghao Chen;cc [~mxm] [~fanrui] Looking forward to the opinions from you, active contributors on autoscaling;;;","26/Dec/23 06:42;gyfora;If we are going to change this, powers of 2 are not really good as they only allow you to double the parallelism. 



it’s better to consider factorial or highly composite numbers ([https://mathworld.wolfram.com/HighlyCompositeNumber.html)] for practical use.;;;","26/Dec/23 06:49;Zhanghao Chen;Here I'm proposing to roundUpToPowerOfTwo(operatorParallelism * {*}5{*}), so we would allow at least a 5-fold increase of parallelism;;;","26/Dec/23 07:30;fanrui;Thanks [~Zhanghao Chen] for driving this improvement and for the ping!:)

1024 as the min value of max parallelism makes sense to me, and our internal flink version also uses 1024 instead of 128. And it's fine for most of jobs.

IIUC, when the parallelism of one job is very small(it's 1 or 2) and the max parallelism is 1024, one subtask will have 1024 keyGroups. From state backend side, too many key groups may effect the performance. (This is my concern to change it by default in Flink Community.)

Note: this performance drop may be insignificant in a real production environment.;;;","26/Dec/23 07:43;gyfora;Consider 840 in instead of 1024, 840 has 32 divisors compared to the 11 divisors of 1024 and it’s a smaller number… it’s a much better max parallelism setting in my opinion 

please see my previous comment regarding highly composite numbers 

 ;;;","26/Dec/23 07:48;fanrui;Thanks [~gyfora] for the reminder! Too many divisors is useful for avoid data skew, 840 makes sense to me. (y);;;","26/Dec/23 08:40;Zhanghao Chen;Thanks [~gyfora] for pointing that. We also observe some data skew introduced by imbalanced key group assignment in our production use, using highly composite numbers should be useful. How about using the following formula: {{{}min(max({*}roundUpToHighlyCompositeNum{*}(operatorParallelism * {*}5{*}), {*}840{*}), {*}45360{*}){}}}, where the highly composite number series is defined in [A002182 - OEIS|https://oeis.org/A002182]?;;;","02/Jan/24 13:48;mxm;[~Zhanghao Chen] Thank you for the proposal. I agree with using highly composite numbers, as this will provide more flexibility to the autoscaler. I'm not sure about the {{operatorParallelism * 5}}. What is the rational for selecting this factor? Why not {{*10}}? ;;;","02/Jan/24 14:59;Zhanghao Chen;[~mxm] There's no concrete rationale for selecting the factor 5, it's just a magic number that most users are satified with in our production env. I'd be good with 10 as well. Do you have any perference on the selection of the factor?;;;","02/Jan/24 16:23;mxm;[~Zhanghao Chen] Even though the factor only affects high parallelism operators > 840, I wonder whether we need to leave more room for scaleup. But I don't have a strong opinion.

{quote}
IIUC, when the parallelism of one job is very small(it's 1 or 2) and the max parallelism is 1024, one subtask will have 1024 keyGroups. From state backend side, too many key groups may effect the performance. (This is my concern to change it by default in Flink Community.)
{quote}

[~fanrui] I think we need to find out how big the performance impact actually is when jumping from 128 to 840 key groups. But 128 may just have been a very conservative number.;;;","03/Jan/24 02:20;Zhanghao Chen;[~mxm] thanks for the quick response, I'll update with some Nextmark benchmark result later on the performance impact of bumping the num of keygroups for an informed decision. As for the factor, 10 sounds good for me, maybe let's also see what [~fanrui] & [~gyfora] think about it.;;;","03/Jan/24 02:52;fanrui;Hi [~Zhanghao Chen] , IIUC flink-benchmarks[1] is checking the performance related to state, and we can see the performance change in this web UI[2].

Such as: the valueGet.HEAP[3][4] means valueState.get for hashmap state backend.

!image-2024-01-03-10-52-05-861.png|width=949,height=359!

 

[1] [https://github.com/apache/flink-benchmarks]


[2][http://flink-speed.xyz|http://flink-speed.xyz/]

[3][https://github.com/apache/flink-benchmarks/blob/master/src/main/java/org/apache/flink/state/benchmark/ValueStateBenchmark.java]

[4]http://flink-speed.xyz/timeline/?ben=valueGet.HEAP&env=3;;;","03/Jan/24 09:13;Zhanghao Chen;Thanks for the input, [~fanrui]. I'll take a look on the benchmark.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make husky in runtime-web no longer affect git global hooks,FLINK-33939,13562902,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,simplejason,simplejason,simplejason,26/Dec/23 06:04,04/Jan/24 03:49,04/Jun/24 20:40,04/Jan/24 03:46,1.9.0,,,,,,,,1.17.3,1.18.2,1.19.0,,Runtime / Web Frontend,,,,1,pull-request-available,,,"Since runtime-web relies on husky to ensure that front-end code changes are detected before `git commit`, husky modifies the global git hooks (core.hooksPath) so that core.hooksPath won't take effect if it's configured globally, I thought it would be a good idea to make the front-end code detection a optional command execution, which ensures that the globally configured hooks are executed correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 04 03:46:29 UTC 2024,,,,,,,,,,"0|z1mfzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/23 06:06;simplejason;Feedback is welcome, I can submit a PR to remove the husky dependency on the front end : );;;","26/Dec/23 06:15;Wencong Liu;Thanks for raising this issue! 😄 I completely agree with your proposal to make front-end code detection an optional command execution in our use of husky with runtime-web. By doing this, we can preserve the functionality of any globally configured git hooks.;;;","26/Dec/23 06:57;xtsong;Sounds good to me. Thanks for reporting and volunteering on this, [~simplejason]. You are assigned. Please go ahead.;;;","04/Jan/24 03:46;xtsong;- master (1.19): 8e8e2d649e98071bc15ee768cf65da3e96f255b4
- release-1.18: 8342ac7f1e9b52438a3f26bf96b2102c7145dcab
- release-1.17: 32af5b368569f6bbd7bff45a7be5ea17d8e59c65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct implicit coercions in relational operators to adopt typescript 5.0,FLINK-33938,13562895,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,laffery,laffery,laffery,26/Dec/23 02:48,09/Jan/24 06:58,04/Jun/24 20:40,09/Jan/24 06:58,1.19.0,,,,,,,,1.17.3,1.18.2,1.19.0,,Runtime / Web Frontend,,,,0,pull-request-available,,,"Since TypeScript 5.0, there is a break change that implicit coercions in relational operators are forbidden [1].

So that the following code in flink-runtime-web/web-dashboard/src/app/components/humanize-date.pipe.ts get error:
{code:java}
public transform(
  value: number | string | Date,
  ...
): string | null | undefined {
  if (value == null || value === '' || value !== value || value < 0) {
    return '-';
  } 
  ...
}{code}
The correctness improvement is availble in here [2][.|https://github.com/microsoft/TypeScript/pull/52048.]

I think we should optimize this type of code for better compatibility.

 

[1] [https://devblogs.microsoft.com/typescript/announcing-typescript-5-0/#forbidden-implicit-coercions-in-relational-operators]

[2] [https://github.com/microsoft/TypeScript/pull/52048|https://github.com/microsoft/TypeScript/pull/52048.]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 09 06:58:58 UTC 2024,,,,,,,,,,"0|z1mfy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/23 08:05;laffery;Would you like to take a look at this ticket? [~simplejason] ;;;","05/Jan/24 06:24;laffery;Hi [~Wencong Liu] , I had created a PR to solve this issue, would you like to take a look?;;;","09/Jan/24 06:58;xtsong;- master (1.19): e07545e458bd22099244a353ac29477ca3a13811
- release-1.18: 12463fbad39edc17af687c1421bba4623f924083
- release-1.17: ce62d45477447537088930ec116f7e18a2743166;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Translate ""Stateful Stream Processing"" page into Chinese",FLINK-33937,13562891,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pingcai678,pingcai678,26/Dec/23 00:54,30/May/24 08:40,04/Jun/24 20:40,,1.8.0,,,,,,,,,,,,chinese-translation,,,,0,translate,,,"The page is located at _""docs/content.zh/docs/concepts/stateful-stream-processing.md""_

_!image-2023-12-26-08-54-14-041.png!_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/23 00:54;pingcai678;image-2023-12-26-08-54-14-041.png;https://issues.apache.org/jira/secure/attachment/13065599/image-2023-12-26-08-54-14-041.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 07:01:00 UTC 2024,,,,,,,,,,"0|z1mfx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 01:35;lxliyou;I am interested in the open-source community, and this is my first time participating in an Apache project. Could you assign this issue to me? Thank you.[~pingcai678] ;;;","30/May/24 07:01;damncake;It is still unresolved. Could you please assign it to me? [~jark] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outputting Identical Results in Mini-Batch Aggregation with Set TTL,FLINK-33936,13562872,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hackergin,hackergin,25/Dec/23 16:54,28/Feb/24 01:56,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"If mini-batch is enabled currently, and if the aggregated result is the same as the previous output, this current aggregation result will not be sent downstream.  This will cause downstream nodes to not receive updated data. If there is a TTL set for states at this time, the TTL of downstream will not be updated either.

The specific logic is as follows.

https://github.com/apache/flink/blob/a18c0cd3f0cdfd7e0acb53283f40cd2033a86472/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGroupAggFunction.java#L224

{code:java}
                    if (!equaliser.equals(prevAggValue, newAggValue)) {
                        // new row is not same with prev row
                        if (generateUpdateBefore) {
                            // prepare UPDATE_BEFORE message for previous row
                            resultRow
                                    .replace(currentKey, prevAggValue)
                                    .setRowKind(RowKind.UPDATE_BEFORE);
                            out.collect(resultRow);
                        }
                        // prepare UPDATE_AFTER message for new row
                        resultRow.replace(currentKey, newAggValue).setRowKind(RowKind.UPDATE_AFTER);
                        out.collect(resultRow);
                    }
                    // new row is same with prev row, no need to output
{code}



When mini-batch is not enabled, even if the aggregation result of this time is the same as last time, new results will still be sent if TTL is set.

https://github.com/apache/flink/blob/e9353319ad625baa5b2c20fa709ab5b23f83c0f4/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupAggFunction.java#L170

{code:java}

                if (stateRetentionTime <= 0 && equaliser.equals(prevAggValue, newAggValue)) {
                    // newRow is the same as before and state cleaning is not enabled.
                    // We do not emit retraction and acc message.
                    // If state cleaning is enabled, we have to emit messages to prevent too early
                    // state eviction of downstream operators.
                    return;
                } else {
                    // retract previous result
                    if (generateUpdateBefore) {
                        // prepare UPDATE_BEFORE message for previous row
                        resultRow
                                .replace(currentKey, prevAggValue)
                                .setRowKind(RowKind.UPDATE_BEFORE);
                        out.collect(resultRow);
                    }
                    // prepare UPDATE_AFTER message for new row
                    resultRow.replace(currentKey, newAggValue).setRowKind(RowKind.UPDATE_AFTER);
                }
{code}


Therefore, based on the consideration of TTL scenarios, I believe that when mini-batch aggregation is enabled, new results should also output when the aggregated result is the same as the previous one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 05 05:21:30 UTC 2024,,,,,,,,,,"0|z1mfsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 03:34;xuyangzhong;Thanks for your report. This bug seems to be that when optimizations about mini-batch agg were first introduced, some behaviors in the group agg function were not aligned. I think we need to fix it.;;;","05/Jan/24 05:21;hackergin;[~xuyangzhong]  Thank you for your reply, I would like to fix this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the default value doc and logic for some state backend and checkpoint related options,FLINK-33935,13562853,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,25/Dec/23 07:20,05/Jan/24 03:27,04/Jun/24 20:40,05/Jan/24 03:27,,,,,,,,,1.19.0,,,,Runtime / Checkpointing,Runtime / State Backends,,,0,pull-request-available,,,"Some state backend and checkpoint related options don't set the default value directly, but but they implement default value based on code. Such as:
 * execution.checkpointing.tolerable-failed-checkpoints
 ** [https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/config/#execution-checkpointing-tolerable-failed-checkpoints]
 * state.backend.type
 ** [https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/config/#state-backend-type]

h2. Option1

execution.checkpointing.tolerable-failed-checkpoints doesn't have default value, but CheckpointConfig#getTolerableCheckpointFailureNumber calls  {color:#9876aa}configuration{color}.getOptional(ExecutionCheckpointingOptions.{color:#9876aa}TOLERABLE_FAILURE_NUMBER{color}).orElse({color:#6897bb}0{color}).

It means the 0 is default value of execution.checkpointing.tolerable-failed-checkpoints.
h2. Option2

state.backend.type does't have default value, but StateBackendLoader#loadFromApplicationOrConfigOrDefaultInternal calls 

loadStateBackendFromConfig(config{color:#cc7832}, {color}classLoader{color:#cc7832}, {color}logger). When the return value is null, Flink will consider the hashmap as the default state backend.

I checked all callers of StateBackendLoader#loadStateBackendFromConfig, if we change the default value of state.backend.type to hashmap. All of them work well.
h2. Why set the default value directly is fine?

From user side, it's clearer.

From flink developers or maintainers side, it's easy to maintain.
h2. Proposed changes:

Adding the default value for them:
 * execution.checkpointing.tolerable-failed-checkpoints: 0
 * state.backend.type: hashmap

Note: this JIAR adds the default value, but the behaviour is absolutely same with old one, so it doesn't introduce any effect for users. (So the FLIP isn't necessary IIUC.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 05 03:27:21 UTC 2024,,,,,,,,,,"0|z1mfoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 03:27;fanrui;Merged to master(1.19) via: 4a852fee28f2d87529dc05f5ba2e79202a0e00b6 and cf0ac00cfe84c34af64308cebfc4a9034b94fddc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Source use raw format maybe lead to data lost,FLINK-33934,13562852,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,catyee,cailiuyang,cailiuyang,25/Dec/23 06:57,08/Apr/24 08:31,04/Jun/24 20:40,,1.12.0,1.13.0,1.14.0,1.15.0,1.16.0,1.17.0,1.18.0,1.19.0,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",Table SQL / Runtime,,,0,,,,"In our product we encounter a case that lead to data lost, the job info: 
   1. using flinkSQL that read data from messageQueue (our internal mq) and write to hive (only select value field, doesn't contain metadata field)
   2. the format of source table is raw format
 
But if we select value field and metadata field at the same time, than the data lost will not appear
 
After we review the code, we found that the reason is the object reuse of Raw-format(see code [RawFormatDeserializationSchema|https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime/src/main/java/org/apache/flink/formats/raw/RawFormatDeserializationSchema.java#L62]), why object reuse will lead to this problem is below (take kafka as example):
    1. RawFormatDeserializationSchema will be used in the Fetcher-Thread of SourceOperator, Fetcher-Thread will read and deserialize data from kafka partition, than put data to ElementQueue (see code [SourceOperator FetcherTask |https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/fetcher/FetchTask.java#L64])
    2. SourceOperator's main thread will pull data from the ElementQueue(which is shared with the FetcherThread) and process it (see code [SourceOperator main thread|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java#L188])
    3. For RawFormatDeserializationSchema, its deserialize function will return the same object([reuse rowData object|https://github.com/apache/flink/blob/master/flink-table/flink-table-runtime/src/main/java/org/apache/flink/formats/raw/RawFormatDeserializationSchema.java#L62])
    4. So, if elementQueue have element that not be consumed, than the fetcherThread can change the filed of the reused rawData that RawFormatDeserializationSchema::deserialize returned, this will lead to data lost;
 
The reason that we select value and metadata field at the same time will not encounter data lost is:
   if we select metadata field there will return a new RowData object see code: [DynamicKafkaDeserializationSchema deserialize with metadata field |https://github.com/apache/flink-connector-kafka/blob/main/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/DynamicKafkaDeserializationSchema.java#L249] and if we only select value filed, it will reuse the RowData object that formatDeserializationSchema returned see code [DynamicKafkaDeserializationSchema deserialize only with value field|https://github.com/apache/flink-connector-kafka/blob/main/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/DynamicKafkaDeserializationSchema.java#L113]
 
To solve this problem, i think we should remove reuse object of RawFormatDeserializationSchema.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14356,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 08:31:24 UTC 2024,,,,,,,,,,"0|z1mfog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/23 08:08;hackergin;[~cailiuyang]  Does this mean that we should turn off Object Reuse for all DeserializeSchema? ;;;","25/Dec/23 09:14;cailiuyang;[~hackergin] Yeah,  turn off Object Reuse for all DeserializeSchema(that will be used by SourceOperator) is the simple way to avoid this data lost problem.

------ update ------

after review kafka source code, kafka will deserialize from ConsumerRecord to real object in SourceOperator MainThread, So it will not encounter this problem, see code: [KafkaRecordEmitter code|https://github.com/apache/flink-connector-kafka/blob/main/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaRecordEmitter.java#L53C43-L53C43] 

 

So, this problem will only appear in which case that DeserializeSchema::deserialize is called in FetcherThread

 ;;;","08/Apr/24 03:35;yunta;I think it's easy to let data lost when using {{raw}} format with another customized connector. Moreover, the object reuse semantic is hidden in the {{raw}} format description. From my point of view, this is a potential bug, cc [~jark].;;;","08/Apr/24 08:31;jark;[~yunta] yes, I think the row format should avoid reusing RowData just like other formats (JSON, AVRO).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SerializedThrowable will be java.lang.StackOverflowError when AsyncLookupFunction throw an exception,FLINK-33933,13562844,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,KarlManong,KarlManong,25/Dec/23 02:42,29/Dec/23 01:06,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"Here is a simple example
{code:java}
// example

public class TableA implements LookupTableSource {


@Nullable
private final LookupCache cache;

public TableA(@Nullable LookupCache cache) {
this.cache = cache;
}

@Override
public LookupRuntimeProvider
getLookupRuntimeProvider(LookupContext context) {

FunctionA lookupFunction = new FunctionA();

if (cache != null) {
return PartialCachingAsyncLookupProvider.of(lookupFunction, cache);
} else {
return AsyncLookupFunctionProvider.of(lookupFunction);
}
}

@Override
public DynamicTableSource copy() {
return new TableA(cache);
}

@Override
public String asSummaryString() {
return ""Async Table"";
}
}

public class LookupFunctionA extends AsyncLookupFunction {


@Override
public CompletableFuture<Collection<RowData>> asyncLookup(RowData keyRow) {
CompletableFuture<Collection<RowData>> future = new
CompletableFuture<>();
future.completeExceptionally(new IOException(""request failed""));
return future;
}

}
{code}
When using TableA, StackOverflowError occurs

 
{code:java}
// code placeholder
java.lang.StackOverflowError
    at java.base/java.lang.Exception.<init>(Exception.java:66)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:66)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
    at org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62)
    at org.apache.flink.util.SerializedThrowable.addAllSuppressed(SerializedThrowable.java:150)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:97)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:93)
    at org.apache.flink.util.SerializedThrowable.<init>(SerializedThrowable.java:62) {code}","tested from 1.16 to 1.18 , the same behavior ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 29 01:00:00 UTC 2023,,,,,,,,,,"0|z1mfmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/23 02:44;KarlManong;`SerializedThrowable` method `addAllSuppressed` may accept alreadySeen ?;;;","27/Dec/23 15:12;martijnvisser;[~KarlManong] I believe that the asyncLookup() method in LookupFunctionA is throwing an exception, specifically an IOException, which is then being wrapped in a SerializedThrowable by Flink's serialization mechanism. However, the SerializedThrowable class is also serializable, so when Flink tries to serialize it, it ends up causing a StackOverflowError due to infinite recursion.

To fix this issue, you should ensure that the asyncLookup() method in LookupFunctionA does not throw an exception. If an exception occurs during the lookup, you should return a CompletableFuture that is completed exceptionally with the exception. ;;;","28/Dec/23 02:08;KarlManong;[~martijnvisser] I do return a CompletableFuture when exception occurs. I borrow the code from [flink-connector-hbase/flink-connector-hbase-2.2/src/main/java/org/apache/flink/connector/hbase2/source/HBaseRowDataAsyncLookupFunction.java at main · apache/flink-connector-hbase (github.com)|https://github.com/apache/flink-connector-hbase/blob/main/flink-connector-hbase-2.2/src/main/java/org/apache/flink/connector/hbase2/source/HBaseRowDataAsyncLookupFunction.java#L136];;;","28/Dec/23 11:38;martijnvisser;How about something like:

{code:java}
@Override
public CompletableFuture<Collection<RowData>> asyncLookup(RowData keyRow) {
    try {
        // Perform the lookup and create a collection of RowData objects
        Collection<RowData> result = new ArrayList<>();
        // TODO: Add your lookup logic here to populate the result collection

        // Return the result as a completed future
        return CompletableFuture.completedFuture(result);
    } catch (Exception e) {
        // If an exception occurs during the lookup, complete the future exceptionally with the exception
        CompletableFuture<Collection<RowData>> future = new CompletableFuture<>();
        future.completeExceptionally(e);
        return future;
    }
}
{code};;;","29/Dec/23 01:00;KarlManong;[~martijnvisser] yes, when exception occurs, We'll got  a completedExceptionally future,
{code:java}
@Override
public CompletableFuture<Collection<RowData>> asyncLookup(RowData keyRow) {
    try {
        // Perform the lookup and create a collection of RowData objects
        Collection<RowData> result = new ArrayList<>();
        // TODO: Add your lookup logic here to populate the result collection

        // Return the result as a completed future
        return CompletableFuture.completedFuture(result);
    } catch (Exception e) {
        // If an exception occurs during the lookup, complete the future exceptionally with the exception

       /*
       * code run to here
       */

        CompletableFuture<Collection<RowData>> future = new CompletableFuture<>();
        future.completeExceptionally(e);
        return future;
    }
} {code}
 

 

 

then Flink fatal error with a StackOverflowError ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Retry Mechanism in RocksDBStateDataTransfer,FLINK-33932,13562843,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiangyu0xf,dianer17,dianer17,25/Dec/23 02:41,21/Feb/24 02:42,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Checkpointing,,,,1,pull-request-available,,,"Currently, there is no retry mechanism for downloading and uploading RocksDB state files. Any jittering of remote filesystem might lead to a checkpoint failure. By supporting retry mechanism in RocksDBStateDataTransfer, we can significantly reduce the failure rate of checkpoint during asynchronous phase.
The exception is as below:
{noformat}
 
2023-12-19 08:46:00,197 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Decline checkpoint 2 by task 5b008c2c048fa8534d648455e69f9497_fbcce2f96483f8f15e87dc6c9afd372f_183_0 of job ffffffffa025f19e0000000000000000 at application-6f1c6e3d-1702480803995-5093022-taskmanager-1-1 @ fdbd:dc61:1a:101:0:0:0:36 (dataPort=38789).
org.apache.flink.util.SerializedThrowable: org.apache.flink.runtime.checkpoint.CheckpointException: Asynchronous task checkpoint failed.
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:320) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:155) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
    at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.flink.util.SerializedThrowable: java.lang.Exception: Could not materialize checkpoint 2 for operator GlobalGroupAggregate[132] -> Calc[133] (184/500)#0.
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.handleExecutionException(AsyncCheckpointRunnable.java:298) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    ... 4 more
Caused by: org.apache.flink.util.SerializedThrowable: java.util.concurrent.ExecutionException: java.io.IOException: Could not flush to file and close the file system output stream to hdfs://xxx/shared/97329cbd-fb90-45ca-92de-edf52d6e6fc0 in order to obtain the stream state handle
    at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:?]
    at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:?]
    at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:544) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:54) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    ... 3 more
Caused by: org.apache.flink.util.SerializedThrowable: java.io.IOException: Could not flush to file and close the file system output stream to hdfs://xxx/shared/97329cbd-fb90-45ca-92de-edf52d6e6fc0 in order to obtain the stream state handle
    at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.closeAndGetHandle(FsCheckpointStreamFactory.java:516) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.contrib.streaming.state.RocksDBStateUploader.uploadLocalFileToCheckpointFs(RocksDBStateUploader.java:157) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.contrib.streaming.state.RocksDBStateUploader.lambda$createUploadFutures$0(RocksDBStateUploader.java:113) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:32) ~[flink-dist-1.17-xxx-SNAPSHOT.jar:1.17-xxx-SNAPSHOT]
    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700) ~[?:?]
    ... 3 more
Caused by: org.apache.flink.util.SerializedThrowable: java.net.ConnectException: Connection timed out
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?]
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777) ~[?:?]
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206) ~[hadoop-common-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:532) ~[hadoop-common-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1835) ~[hadoop-hdfs-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1268) ~[hadoop-hdfs-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1257) ~[hadoop-hdfs-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1414) ~[hadoop-hdfs-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1149) ~[hadoop-hdfs-2.6.0-cdh5.4.4.jar:?]
    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:652) ~[hadoop-hdfs-2.6.0-cdh5.4.4.jar:?]
 {noformat}
We can support retry mechanism for rocksdb uploader to decrease the failure rate of checkpointing in the async phase.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 09:37:29 UTC 2024,,,,,,,,,,"0|z1mfmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/23 07:22;xiangyu0xf;Hi [~dianer17], thanks for creating  this. IMHO, this retry is very useful to improve the success rate of checkpoint. We can introduce a default fixed delay retry mechanism here.

 

I have implemented a poc in this pr: [https://github.com/apache/flink/pull/23986], WDYT?;;;","27/Dec/23 15:06;martijnvisser;[~xiangyu0xf] If I'm understanding you correctly, you don't want to make this configurable but enable this by default, without the option to disable it? ;;;","27/Dec/23 15:07;martijnvisser;I'm no expert on checkpointing, but I'm also wondering if we're safe to just ""retry"" checkpointing. I could imagine that this would have correctness issues. Pinging [~pnowojski] for his thoughts;;;","28/Dec/23 02:41;xiangyu0xf;[~martijnvisser] Yes, like the default `ExponentialBackoffRetryStrategy` defined in `RpcGatewayRetriever` and 
`ExponentialWaitStrategy` defined in `RestClusterClient`.;;;","29/Dec/23 12:45;xiangyu0xf;[~martijnvisser] AFAIK, this retry is not about ""retry"" checkpointing but to ensure that Checkpoint can successfully upload the local file to the remote storage during the asynchronous phase. For files that partially upload to remote but failed eventually, they won‘t be considered as part of this checkpoint because no file path will be returned. This retry mechanism works well in our production for the past two years without any correctness issues.;;;","04/Jan/24 16:23;pnowojski;Yes, I don't think this should cause a correctness issue. However we need this behaviour to be configurable and most likely in the first release it should default to the old behaviour.;;;","07/Jan/24 04:11;xiangyu0xf;[~pnowojski]  Hi Piotr, thx for ur feedback. After a closer look at the code, I think we also need retry mechanism in `RocksDBStateDownloader`. IMHO, we can add default retry configurations in `RocksDBOptions` and implement this in `RocksDBStateDataTransfer`. In this way, both  `RocksDBStateUploader` and `RocksDBStateDownloader` can reuse this retry mechanism. 

 

Can u kindly assign this Jira to me and grant me the permission to change the title and description of this Jira?;;;","07/Jan/24 04:38;xiangyu0xf;[~pnowojski] Also, I will open a flip to discuss add this retry configuration.;;;","09/Jan/24 10:03;xiangyu0xf;[~masteryhx]  Hi Hangxiang, what do u think about this? I'd like to hear more from you.;;;","10/Jan/24 02:38;masteryhx;Thanks for pinging me here.

Retry machanism is common when we want to get or put data by network.

So I think it makes sense for checkpoint failure due to temproal network problem, which may increase a bit overhead for some other reasons.

Since Flink has checkpoint machanism to retry failed checkpoint coarsely, I think it looks good to me if this fine-grained retry could be configurable and don't change current default machanism.;;;","11/Jan/24 06:56;xiangyu0xf;[~masteryhx] Thx for ur reply, I've talked to [~dianer17] to update the description of this issue. Would you kindly assign this issue to me? Also, I would like to hear more from you about this issue in the discussion thread of FLIP-414: [https://lists.apache.org/thread/om4kgd6trx2lctwm6x92q2kdjngxtz9k];;;","11/Jan/24 09:37;masteryhx;Sure, already assigned to you.

Please take a look my extra comments in the thread.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create custom action to enable maven-utils.sh's run_mvn within the workflow,FLINK-33931,13562810,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,23/Dec/23 19:51,30/Jan/24 14:57,04/Jun/24 20:40,30/Jan/24 14:57,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:57:22 UTC 2024,,,,,,,,,,"0|z1mff4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:57;mapohl;Resolved in FLINK-33914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stateless stop job status exception,FLINK-33930,13562787,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hunterLiu,hunterLiu,23/Dec/23 05:50,29/Feb/24 05:34,04/Jun/24 20:40,,,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Run flink session job in k8s operator mode. I want to cancel a job, so setState(suspended) & setUppradeMode(STATELESS) use flink k8s api. But the status of this task will change from completed to canceled.","flink 1.17.1
flink k8s operator 1.6.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,2023-12-23 05:50:23.0,,,,,,,,,,"0|z1mfa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support jdbc read shard Read string fields as shard keys,FLINK-33929,13562693,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sunnny,sunnny,22/Dec/23 08:27,27/Mar/24 09:27,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / JDBC,Table SQL / JDBC,,,0,pull-request-available,,,"Many table structures are very strange table primary keys are generated by the string uuid is exactly very scattered, resulting in the use of flink jdbc shards to read the primary key,I think flink is not only the best streaming computing engine but also has a greater advantage in data transfer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/23 08:32;sunnny;WechatIMG59636.jpg;https://issues.apache.org/jira/secure/attachment/13065549/WechatIMG59636.jpg","28/Dec/23 04:49;sunnny;image-2023-12-28-12-49-45-671.png;https://issues.apache.org/jira/secure/attachment/13065635/image-2023-12-28-12-49-45-671.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 04:47:53 UTC 2023,,,,,,,,,,"0|z1mepc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 15:14;martijnvisser;[~sunnny] Can you please elaborate more on what you're missing by explaining your use case and the systems you want to connect to? The ticket currently isn't really clear for me. ;;;","28/Dec/23 04:47;sunnny;!image-2023-12-28-12-49-45-671.png!

Of course visser, Support jdbc sharding for common part of the database to distribute strings parallel read [~martijnvisser] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should not throw exception while creating view with specify field names even if the query conflicts in field names,FLINK-33928,13562691,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,xuyangzhong,xuyangzhong,22/Dec/23 08:16,19/Jan/24 03:25,04/Jun/24 20:40,19/Jan/24 03:25,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"The following sql should be valid.
{code:java}
create view view1(a, b) as select t1.name, t2.name from t1 join t1 t2 on t1.score = t2.score; {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33490,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 03:24:59 UTC 2024,,,,,,,,,,"0|z1meow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 09:23;337361684@qq.com;Hi, [~xuyangzhong]. I want to take this issue. Thanks.;;;","19/Jan/24 03:24;fsk119;Merged into master: 82fcdfe5634fb82d3ab4a183818d852119dc68a9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cleanup usage of deprecated ExecutionConfigOptions#(TABLE_EXEC_SHUFFLE_MODE, TABLE_EXEC_LEGACY_TRANSFORMATION_UIDS)",FLINK-33927,13562678,13529804,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liyubin117,liyubin117,22/Dec/23 06:36,25/Dec/23 12:13,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"|org.apache.flink.table.api.config.ExecutionConfigOptions#TABLE_EXEC_SHUFFLE_MODE|
|org.apache.flink.table.api.config.ExecutionConfigOptions#TABLE_EXEC_LEGACY_TRANSFORMATION_UIDS|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 25 12:13:34 UTC 2023,,,,,,,,,,"0|z1mem0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/23 12:13;liyubin117;[~qingyue] Looking forward your review :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start a job with a jar in the system classpath in native k8s mode,FLINK-33926,13562639,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,trystan,trystan,21/Dec/23 17:32,21/Dec/23 17:56,04/Jun/24 20:40,,kubernetes-operator-1.6.0,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"It appears that the combination of the running operator-controlled jobs in native k8s + application mode + using a job jar in the classpath is invalid. Avoiding dynamic classloading (as specified in the [docs|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/debugging/debugging_classloading/#avoiding-dynamic-classloading-for-user-code]) is beneficial for some jobs. This affects at least Flink 1.16.1 and Kubernetes Operator 1.6.0.

 

FLINK-29288 seems to have addressed this for standalone mode. If I am misunderstanding how to correctly build jars for this native k8s scenario, apologies for the noise and any pointers would be appreciated!

 

Perhaps related, the [spec documentation|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/custom-resource/reference/#jobspec] declares it optional, but isn't clear about under what conditions that applies.
 * Putting the jar in the system classpath and pointing *jarURI* to that jar leads to linkage errors.
 * Not including *jarURI* leads to NullPointerExceptions in the operator:

{code:java}
{""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""java.lang.NullPointerException"",""stackTrace"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException: java.lang.NullPointerException\n\tat org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:148)\n\tat org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)\n\tat io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:138)\n\tat io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:96)\n\tat org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)\n\tat io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:95)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)\n\tat io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)\n\tat io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:414)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.NullPointerException\n\tat org.apache.flink.kubernetes.utils.KubernetesUtils.checkJarFileForApplicationMode(KubernetesUtils.java:407)\n\tat org.apache.flink.kubernetes.KubernetesClusterDescriptor.deployApplicationCluster(KubernetesClusterDescriptor.java:207)\n\tat org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)\n\tat org.apache.flink.kubernetes.operator.service.NativeFlinkService.deployApplicationCluster(Native"",""additionalMetadata"":{},""throwableList"":[{""type"":""java.lang.NullPointerException"",""additionalMetadata"":{}}]}  {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-21 17:32:05.0,,,,,,,,,,"0|z1medc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extended failure handling for bulk requests (elasticsearch back port),FLINK-33925,13562599,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,schulzp,schulzp,schulzp,21/Dec/23 11:44,15/May/24 06:53,04/Jun/24 20:40,14/Mar/24 03:21,opensearch-1.0.1,,,,,,,,opensearch-1.2.0,opensearch-2.0.0,,,Connectors / Opensearch,,,,0,pull-request-available,,,"This is a back port of the implementation for the elasticsearch connector, see FLINK-32028, to achieve consistent APIs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 03:21:50 UTC 2024,,,,,,,,,,"0|z1me4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 17:00;schulzp;[~martijnvisser], this PR seems stuck. I'm not sure how to interpret the failed CI pipeline. It complains about some ITs violating architecture rules:

{code}
ITCASE tests should use a MiniCluster resource or extension' was violated (3 times):
org.apache.flink.connector.opensearch.sink.OpensearchSinkITCase does not satisfy…
org.apache.flink.connector.opensearch.sink.OpensearchWriterITCase does not satisfy…
org.apache.flink.connector.opensearch.table.OpensearchDynamicSinkITCase does not satisfy…
{code}

Except for {{OpensearchWriterITCase}} my PR did not touch those files. How can we move this forward?;;;","14/Mar/24 03:21;Weijie Guo;main via 9e161cc097b34d3ea0d32787f337e630250547d3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register for INFRA's self-hosted runner trial,FLINK-33924,13562583,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,21/Dec/23 10:30,30/Jan/24 14:56,04/Jun/24 20:40,30/Jan/24 14:56,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-25303,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 21 10:34:30 UTC 2023,,,,,,,,,,"0|z1me0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 10:34;mapohl;I created INFRA-25303 in accordance with [the documentation|https://cwiki.apache.org/confluence/display/INFRA/ASF+Infra+provided+self-hosted+runners] on signing up for the trial;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade upload-artifacts and download-artifacts to version 4,FLINK-33923,13562579,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,21/Dec/23 10:00,30/Jan/24 14:56,04/Jun/24 20:40,30/Jan/24 14:56,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,,,"{{upload-artifacts}} and {{download-artifacts}} can be upgraded to v4. This would bring us support for unique build artifact names (see [https://github.com/actions/toolkit/tree/main/packages/artifact#breaking-changes]) and performance improvements ([https://github.blog/changelog/2023-12-14-github-actions-artifacts-v4-is-now-generally-available/|https://github.blog/changelog/2023-12-14-github-actions-artifacts-v4-is-now-generally-available/)]).

Such a change would require upgrading the node version as stated in FLINK-33277",,,,,,,,,,,,,,,,,,,,,,FLINK-33277,,,,,,,,,,,,,,,,"18/Jan/24 10:35;mapohl;FLINK-33923.log;https://issues.apache.org/jira/secure/attachment/13066125/FLINK-33923.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:56:40 UTC 2024,,,,,,,,,,"0|z1me00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 10:02;mapohl;See further reasoning in FLINK-33277;;;","17/Jan/24 14:17;mapohl;We run into transient issues with the uploads recently ([https://github.com/XComp/flink/actions/runs/7556760147/job/20574504909|https://github.com/XComp/flink/actions/runs/7556760147/job/20574504909)]) that might be resolvable with the upgrade. There's a related issue [https://github.com/actions/upload-artifact/issues/270] ;;;","18/Jan/24 08:53;mapohl;Here are a few test runs with different solutions:
 * Plain upgrade of the {{*-artifact}} actions to v4
 ** [https://github.com/XComp/flink/actions/runs/7556760147]
 ** Result: Failure due to missing clib versions (2.25, 2.27, 2.28) analogous to what was reported in FLINK-33277

{code:java}
##[debug]Running JavaScript Action with default external tool: node20
/__e/node20/bin/node: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.25' not found (required by /__e/node20/bin/node) {code}
 * Tried installing Node 16 through a separate step as the default node version
 ** [https://github.com/XComp/flink/actions/runs/7557113216]
 ** Result: Still error due to missing GLIBC versions for node 20
 * Ubuntu 22.04 base image:
 ** [https://github.com/XComp/flink/actions/runs/7557690888]
 ** There was a problem with the missing caller ID which resulted in each job trying to publish its artifacts under the same name
 ** But the Hadoop 3.1.3 job succeeded and revealed issues with Ubuntu 22.04 (SSL-related tests were failing in {{core}} and there was no {{python}} binary anymore (which made {{PythonEnvUtilsTest}} fail in {{misc}} stage)
 * Ubuntu 20.04 base image:
 ** [https://github.com/XComp/flink/actions/runs/7559103903]
 ** Revealed same issues with SSL-related tests (was cancelled early on)
 * Ubuntu 18.04 as a base image:
 ** [https://github.com/XComp/flink/actions/runs/7559619643]
 ** Compilation failed again but because of missing GLIBC version 2.28
 ** A local run in the Ubuntu 18.04 image revealed that the OpenSSL setup works with netty here
 * Ubuntu 18.04 as a base image with explicit installation of node 16.13.2

{code:java}
# source: https://stackoverflow.com/questions/36399848/install-node-in-dockerfile
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
ARG NODE_VERSION=""16.13.2""
ARG NVM_DIR=/root/.nvm
RUN . ""$NVM_DIR/nvm.sh"" && nvm install ${NODE_VERSION}
RUN . ""$NVM_DIR/nvm.sh"" && nvm use v${NODE_VERSION}
RUN . ""$NVM_DIR/nvm.sh"" && nvm alias default v${NODE_VERSION}
ENV PATH=""/root/.nvm/versions/node/v${NODE_VERSION}/bin/:${PATH}"" {code}

 ** [https://github.com/XComp/flink/actions/runs/7559875126]
 ** error with missing GLIBC version 2.28 observed again

The conclusion is to either switch to a newer Ubuntu version 20.04+, fix the OpenSSL issue and hope that no additional error pop up or try to find a way to install newer GCLIB versions in the current image.;;;","18/Jan/24 10:26;mapohl;The SSL error is probably happening when trying to load the {{tcnative}} library (based on the netty debug logs):
{code:java}
724  [ForkJoinPool-1-worker-1] DEBUG org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl [] - Failed to load netty-tcnative; OpenSslEngine will be unavailable, unless the application has already loaded the symbols by some other means. See https://netty.io/wiki/forked-tomcat
-native.html for more information.
java.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_linux_x86_64, netty_tcnative_linux_x86_64_fedora, netty_tcnative_x86_64, netty_tcnative]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:114) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:725) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:151) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
        at org.apache.flink.runtime.net.SSLUtilsTest.<clinit>(SSLUtilsTest.java:73) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_392]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_392]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_392]
[...] 
        Suppressed: java.lang.UnsatisfiedLinkError: /tmp/liborg_apache_flink_shaded_netty4_netty_tcnative_linux_x86_644214943947193837274.so: libssl.so.1.0.0: cannot open shared object file: No such file or directory
                at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_392]
                at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1934) ~[?:1.8.0_392]
                at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1817) ~[?:1.8.0_392]
                at java.lang.Runtime.load0(Runtime.java:782) ~[?:1.8.0_392]
                at java.lang.System.load(System.java:1100) ~[?:1.8.0_392]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:36) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:396) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:218) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:105) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:725) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:151) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.runtime.net.SSLUtilsTest.<clinit>(SSLUtilsTest.java:73) ~[test-classes/:?]
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_392]
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_392]
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_392]
                at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_392]
[...]
                Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_tcnative_linux_x86_64 in java.library.path
                        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860) ~[?:1.8.0_392]
                        at java.lang.Runtime.loadLibrary0(Runtime.java:843) ~[?:1.8.0_392]
                        at java.lang.System.loadLibrary(System.java:1136) ~[?:1.8.0_392]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:396) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:161) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:105) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:725) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:151) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.runtime.net.SSLUtilsTest.<clinit>(SSLUtilsTest.java:73) ~[test-classes/:?]
                        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_392]
                        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_392]
                        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_392]
                        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_392]
[...]
        Suppressed: java.lang.UnsatisfiedLinkError: /tmp/liborg_apache_flink_shaded_netty4_netty_tcnative_linux_x86_64_fedora282379408997172431.so: libssl.so.10: cannot open shared object file: No such file or directory
                at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_392]
                at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1934) ~[?:1.8.0_392]
                at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1817) ~[?:1.8.0_392]
                at java.lang.Runtime.load0(Runtime.java:782) ~[?:1.8.0_392]
                at java.lang.System.load(System.java:1100) ~[?:1.8.0_392]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:36) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:396) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:218) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:105) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:725) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:151) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                at org.apache.flink.runtime.net.SSLUtilsTest.<clinit>(SSLUtilsTest.java:73) ~[test-classes/:?]
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_392]
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_392]
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_392]
                at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_392]
[...]
          Caused by: java.io.FileNotFoundException: META-INF/native/liborg_apache_flink_shaded_netty4_netty_tcnative_x86_64.so
                at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:186) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                ... 77 more
                Suppressed: java.lang.UnsatisfiedLinkError: no org_apache_flink_shaded_netty4_netty_tcnative_x86_64 in java.library.path
                        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860) ~[?:1.8.0_392]
                        at java.lang.Runtime.loadLibrary0(Runtime.java:843) ~[?:1.8.0_392]
                        at java.lang.System.loadLibrary(System.java:1136) ~[?:1.8.0_392]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:396) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:161) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:105) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:725) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:151) ~[flink-shaded-netty-4.1.100.Final-18.0.jar:?]
                        at org.apache.flink.runtime.net.SSLUtilsTest.<clinit>(SSLUtilsTest.java:73) ~[test-classes/:?]
                        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_392]{code}
 ;;;","30/Jan/24 14:56;mapohl;Resolved in FLINK-33914 with FLINK-34194 for the Docker CI image.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table-planner jar will be removed as it is created at /tmp by default,FLINK-33922,13562572,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dianer17,dianer17,21/Dec/23 09:33,27/Dec/23 15:23,04/Jun/24 20:40,,1.17.0,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"Table-planner jar is created at ""/tmp"" folder, and generally there is a clean mechanism for the ""/tmp"" folder. 

We have a sql-gateway deployed, sometimes when we want to start a sql-client session and connect to the gateway, we'll encounter an exception as below

Caused by: java.util.ServiceConfigurationError: org.apache.flink.table.factories.Factory: Error accessing configuration file
at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1180)
at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.nextProviderClass(ServiceLoader.java:1213) at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1228) at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273) at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309) at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:829)... 68 more
Caused by: java.nio.file.NoSuchFileException:/tmp/flink-table-planner_f342decd-593a-4d9e-890c-d204bbd2dd5f.jar
at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148) at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) at java.base/java.nio.file.Files.readAttributes(Files.java:1851) at java.base/java.util.zip.zipFile$Source.get(zipFile.java:1264)
at java.base/java.util.zip.zipFile$CleanableResource.<init>(ZipFile.java:709) at java.base/java.util.zip.zipFile.<init>(zipFile.java:243) at java.base/java.util.zip.zipFile.<init>(zipFile.java:172) at java.base/java.util.jar.JarFile.<init>(JarFile.java:347)
at java.base/sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:103) at java.base/sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:72) at java.base/sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:168)
at java.base/sun.net.www.protocol.jar.JarFileFactory.getOrCreate(JarFileFactory.java:91) at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:132)
at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream( JarURLConnection.java:175) at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.parse(ServiceLoader.java:1172)... 74 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 27 15:23:56 UTC 2023,,,,,,,,,,"0|z1mdyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 15:23;martijnvisser;[~dianer17] During which process if this JAR generated? I'm wondering if this is Gateway related, not so much Table Planner related;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated IdleStateRetentionTime related method in org.apache.flink.table.api.TableConfig,FLINK-33921,13562569,13529804,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,liyubin117,liyubin117,liyubin117,21/Dec/23 09:28,25/Dec/23 12:09,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"getMinIdleStateRetentionTime()
getMaxIdleStateRetentionTime()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 25 12:09:02 UTC 2023,,,,,,,,,,"0|z1mdxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 09:33;liyubin117;[~qingyue] Could you please assign this to me? thanks!;;;","25/Dec/23 12:09;qingyue;[~liyubin117] Assigned to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandaloneKubernetesTaskManagerFactory doesn't support recent config additions,FLINK-33920,13562553,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gyfora,gyfora,21/Dec/23 08:27,21/Dec/23 08:27,04/Jun/24 20:40,,,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"As reported by a user:

The flink operator always mounts keytab:  
[https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/factory/StandaloneKubernetesTaskManagerFactory.java#L55]

But in core flink there is a config to disable auto mount: 
[https://github.com/apache/flink/blob/cabb28d25c4c58af3ee23fc4a63f9564aefd6146/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/factory/KubernetesTaskManagerFactory.java#L64]

 

The same config should be respected in operator standalone mode as well. We should make sure that other recent config additions are also respected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-21 08:27:03.0,,,,,,,,,,"0|z1mdu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AutoRescalingITCase hangs on AZP,FLINK-33919,13562509,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,Sergey Nuyanzin,Sergey Nuyanzin,20/Dec/23 20:01,22/Jan/24 15:22,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Checkpointing,,,,0,test-stability,,,"This build fails on AZP
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55700&view=logs&j=a657ddbf-d986-5381-9649-342d9c92e7fb&t=dc085d4a-05c8-580e-06ab-21f5624dab16&l=8608
because of waiting
{noformat}
Dec 20 02:07:46 ""main"" #1 [14299] prio=5 os_prio=0 cpu=12675.70ms elapsed=3115.94s tid=0x00007f3f71481600 nid=14299 waiting on condition  [0x00007f3f74913000]
Dec 20 02:07:46    java.lang.Thread.State: TIMED_WAITING (sleeping)
Dec 20 02:07:46 	at java.lang.Thread.sleep0(java.base@21.0.1/Native Method)
Dec 20 02:07:46 	at java.lang.Thread.sleep(java.base@21.0.1/Thread.java:509)
Dec 20 02:07:46 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:152)
Dec 20 02:07:46 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Dec 20 02:07:46 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForOneMoreCheckpoint(CommonTestUtils.java:374)
Dec 20 02:07:46 	at org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingKeyedState(AutoRescalingITCase.java:265)
Dec 20 02:07:46 	at org.apache.flink.test.checkpointing.AutoRescalingITCase.testCheckpointRescalingInKeyedState(AutoRescalingITCase.java:196)
Dec 20 02:07:46 	at java.lang.invoke.LambdaForm$DMH/0x00007f3f0f201400.invokeVirtual(java.base@21.0.1/LambdaForm$DMH)
Dec 20 02:07:46 	at java.lang.invoke.LambdaForm$MH/0x00007f3f0f20c000.invoke(java.base@21.0.1/LambdaForm$MH)
Dec 20 02:07:46 	at java.lang.invoke.Invokers$Holder.invokeExact_MT(java.base@21.0.1/Invokers$Holder)
Dec 20 02:07:46 	at jdk.internal.reflect.DirectMethodHandleAccessor.invokeImpl(java.base@21.0.1/DirectMethodHandleAccessor.java:153)
Dec 20 02:07:46 	at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(java.base@21.0.1/DirectMethodHandleAccessor.java:103)
Dec 20 02:07:46 	at java.lang.reflect.Method.invoke(java.base@21.0.1/Method.java:580)
Dec 20 02:07:46 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Dec 20 02:07:46 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Dec 20 02:07:46 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Dec 20 02:07:46 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Dec 20 02:07:46 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Dec 20 02:07:46 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Dec 20 02:07:46 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Dec 20 02:07:46 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34200,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 20:01:04.0,,,,,,,,,,"0|z1mdkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AsyncSinkWriterThrottlingTest test failure,FLINK-33918,13562504,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,jhughes,jhughes,20/Dec/23 19:03,20/Dec/23 19:31,04/Jun/24 20:40,20/Dec/23 19:31,1.19.0,,,,,,,,,,,,,,,,0,test-stability,,,"From [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55700&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd]
{code:java}
Dec 20 03:09:03 03:09:03.411 [ERROR] org.apache.flink.connector.base.sink.writer.AsyncSinkWriterThrottlingTest.testSinkThroughputShouldThrottleToHalfBatchSize – Time elapsed: 0.879 s <<< ERROR! 
Dec 20 03:09:03 java.lang.IllegalStateException: Illegal thread detected. This method must be called from inside the mailbox thread! 
Dec 20 03:09:03 at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkIsMailboxThread(TaskMailboxImpl.java:262) 
Dec 20 03:09:03 at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:137) 
Dec 20 03:09:03 at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.yield(MailboxExecutorImpl.java:84) 
Dec 20 03:09:03 at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.flush(AsyncSinkWriter.java:367) 
Dec 20 03:09:03 at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.lambda$registerCallback$3(AsyncSinkWriter.java:315) 
Dec 20 03:09:03 at org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService$CallbackTask.onProcessingTime(TestProcessingTimeService.java:199) 
Dec 20 03:09:03 at org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.setCurrentTime(TestProcessingTimeService.java:76) 
Dec 20 03:09:03 at org.apache.flink.connector.base.sink.writer.AsyncSinkWriterThrottlingTest.testSinkThroughputShouldThrottleToHalfBatchSize(AsyncSinkWriterThrottlingTest.java:64) 
Dec 20 03:09:03 at java.lang.reflect.Method.invoke(Method.java:498) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 19:31:36 UTC 2023,,,,,,,,,,"0|z1mdjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 19:06;jhughes;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55701&view=logs&j=1c002d28-a73d-5309-26ee-10036d8476b4&t=d1c117a6-8f13-5466-55f0-d48dbb767fcd]

Same test; slightly different stack trace:
{code:java}
Dec 20 03:28:41 java.lang.IllegalStateException: Illegal thread detected. This method must be called from inside the mailbox thread!
Dec 20 03:28:41 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.checkIsMailboxThread(TaskMailboxImpl.java:262)
Dec 20 03:28:41 	at org.apache.flink.streaming.runtime.tasks.mailbox.TaskMailboxImpl.take(TaskMailboxImpl.java:137)
Dec 20 03:28:41 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl.yield(MailboxExecutorImpl.java:84)
Dec 20 03:28:41 	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.flush(AsyncSinkWriter.java:367)
Dec 20 03:28:41 	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriter.lambda$registerCallback$3(AsyncSinkWriter.java:315)
Dec 20 03:28:41 	at org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService$CallbackTask.onProcessingTime(TestProcessingTimeService.java:199)
Dec 20 03:28:41 	at org.apache.flink.streaming.runtime.tasks.TestProcessingTimeService.setCurrentTime(TestProcessingTimeService.java:76)
Dec 20 03:28:41 	at org.apache.flink.connector.base.sink.writer.AsyncSinkWriterThrottlingTest.testSinkThroughputShouldThrottleToHalfBatchSize(AsyncSinkWriterThrottlingTest.java:64)
Dec 20 03:28:41 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 {code};;;","20/Dec/23 19:27;martijnvisser;[~jhughes] I think you might have to rebase, it appears to have been fixed via FLINK-31472;;;","20/Dec/23 19:31;jhughes;Sorry for the duplicate!;;;","20/Dec/23 19:31;jhughes;Duplicate of https://issues.apache.org/jira/browse/FLINK-31472;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException: hostname can't be null,FLINK-33917,13562501,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ensctom,ensctom,ensctom,20/Dec/23 18:40,02/Jan/24 10:45,04/Jun/24 20:40,02/Jan/24 10:45,,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"In certain scenarios, if the hostname contains certain characters it will throw an exception when it tries to initialize the `InetSocketAddress`

 
{code:java}
java.lang.IllegalArgumentException: hostname can't be null    at java.base/java.net.InetSocketAddress.checkHost(InetSocketAddress.java:149)
    at java.base/java.net.InetSocketAddress.<init>(InetSocketAddress.java:216) {code}
 

[https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L236]

 
{code:java}
       @Override
    public boolean isJobManagerPortReady(Configuration config) {
        final URI uri;
        try (var clusterClient = getClusterClient(config)) {
            uri = URI.create(clusterClient.getWebInterfaceURL());
        } catch (Exception ex) {
            throw new FlinkRuntimeException(ex);
        }
        SocketAddress socketAddress = new InetSocketAddress(uri.getHost(), uri.getPort());
        Socket socket = new Socket();
        try {
            socket.connect(socketAddress, 1000);
            socket.close();
            return true;
        } catch (IOException e) {
            return false;
        }
    }
  {code}
 

Here's a simple test to reproduce

 

URL
{code:java}
@ParameterizedTest
@ValueSource(
        strings = {""http://127.0.0.1:8081"", ""http://123-dev:8081"", ""http://dev-test.abc:8081"", ""http://dev-test.1a:8081"", ""http://dev-test.abc01:8081""})
void testURLAddresses(String inputAddress) {

    assertDoesNotThrow(
            () -> {
                final URL url = new URL(inputAddress);
                new InetSocketAddress(url.getHost(), url.getPort());
            });
} {code}
 

URI

 
{code:java}
@ParameterizedTest
@ValueSource(
        strings = {""http://127.0.0.1:8081"", ""http://123-dev:8081"", ""http://dev-test.abc:8081"", ""http://dev-test.1a:8081"", ""http://dev-test.abc01:8081""})
void testURIAddresses(String inputAddress) {

    assertDoesNotThrow(
            () -> {
                final URI uri = new URI(inputAddress);
                new InetSocketAddress(uri.getHost(), uri.getPort());
            });
}  {code}
 

All test cases past except for  ""http://dev-test.1a:8081"" which is a valid flink host url, but not a valid URI

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 22 08:08:07 UTC 2023,,,,,,,,,,"0|z1mdio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 10:13;mxm;The description doesn’t describe under which circumstances the host name can be parsed as null. One example is {{new URI(""123-test"")}} which will return a null host name because the string is parsed as a URI path. Flink itself returns a stringified URL object. So using URL instead works fine. ;;;","22/Dec/23 06:23;ensctom;Actually uri(""123-test"") does work. The issue here is the  '.' followed by a number

 

uri(""123-test.abc"") works, but uri(""dev-test.1"") does not.

 

Here's a test case you can run 

 
{code:java}
@ParameterizedTest
@ValueSource(
        strings = {""http://127.0.0.1:8081"", ""http://123-dev:8081"", ""http://dev-test.abc:8081"", ""http://dev-test.1a:8081"", ""http://dev-test.abc01:8081""})
void testURIAddresses(String inputAddress) {

    assertDoesNotThrow(
            () -> {
                final URI uri = new URI(inputAddress);
                new InetSocketAddress(uri.getHost(), uri.getPort());
            });
} {code};;;","22/Dec/23 07:14;ensctom;I've also raised this as a JDK bug in the mean time https://bugs.java.com/bugdatabase/view_bug?bug_id=JDK-8322667;;;","22/Dec/23 08:08;mxm;{{new URI(""123-test"").getHost()}} returns null. 

I’m not 100% sure this is a JDK bug. There may be some ambiguity when resolving URIs without all spec parts. But let’s see what upstream says. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adds nightly trigger,FLINK-33916,13562472,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Dec/23 12:55,31/Jan/24 11:57,04/Jun/24 20:40,31/Jan/24 11:57,1.19.0,,,,,,,,1.19.0,,,,Build System / CI,,,,0,github-actions,pull-request-available,,The nightly workflows should be trigger for any supported release branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 11:57:04 UTC 2024,,,,,,,,,,"0|z1mdc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 11:57;mapohl;master: [ff9a3c85f69d9e01c2d5d8648c9da404a41b3048|https://github.com/apache/flink/commit/ff9a3c85f69d9e01c2d5d8648c9da404a41b3048];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Workflow: Add nightly workflow for master and release-1.18,FLINK-33915,13562471,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Dec/23 12:54,31/Jan/24 11:56,04/Jun/24 20:40,31/Jan/24 11:52,1.18.0,1.19.0,,,,,,,1.18.2,1.19.0,,,Build System / CI,,,,0,github-actions,pull-request-available,,"The nightly builds run on master and the two most-recently released versions of Flink as those are the supported versions. This logic is currently captured in [flink-ci/git-repo-sync:sync_repo.sh|https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh#L28].

In [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure] we decided to go ahead and provide nightly builds for {{master}} and {{{}release-1.18{}}}. Keep in mind that 1.18 has no support for JDK 21",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 11:52:30 UTC 2024,,,,,,,,,,"0|z1mdc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 11:52;mapohl;master: [697d328f6bb090db63b80f063d3298446a33a5b9|https://github.com/apache/flink/commit/697d328f6bb090db63b80f063d3298446a33a5b9]
1.18: [a27e03650d9e03c9317e19051f8aa4871f199f4a|https://github.com/apache/flink/commit/a27e03650d9e03c9317e19051f8aa4871f199f4a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Workflow: Add basic CI that will run with the default configuration,FLINK-33914,13562470,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Dec/23 12:50,31/Jan/24 11:46,04/Jun/24 20:40,31/Jan/24 11:46,1.18.0,1.19.0,,,,,,,1.18.2,1.19.0,,,,,,,0,github-actions,pull-request-available,,Runs the Flink CI template with the default configuration (Java 8) and can be enabled in each push to the branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 11:46:31 UTC 2024,,,,,,,,,,"0|z1mdbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/24 11:46;mapohl;master: [85edd784fc72c1784849e2b122cbf3215f89817c|https://github.com/apache/flink/commit/85edd784fc72c1784849e2b122cbf3215f89817c]
1.18: [a64e1c98533649d8567f416fb94432d93ddf1d28|https://github.com/apache/flink/commit/a64e1c98533649d8567f416fb94432d93ddf1d28];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Template: Add CI template for running Flink's test suite,FLINK-33913,13562469,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:49,30/Jan/24 14:54,04/Jun/24 20:40,30/Jan/24 14:54,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,,,We want to have a template that runs the entire Flink test suite.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:54:39 UTC 2024,,,,,,,,,,"0|z1mdbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:54;mapohl;This issue is resolved in FLINK-33914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Template: Add CI template for pre-compile steps,FLINK-33912,13562468,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:46,30/Jan/24 14:54,04/Jun/24 20:40,30/Jan/24 14:54,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,,,We want to have a template that triggers all checks that do not require compilation. Those quick checks (e.g. code format) can run without waiting for the compilation step to succeed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:54:26 UTC 2024,,,,,,,,,,"0|z1mdbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:54;mapohl;This issue is resolved in FLINK-33914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Action: Select workflow configuration,FLINK-33911,13562467,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:45,30/Jan/24 14:54,04/Jun/24 20:40,30/Jan/24 14:54,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,"During experiments, we noticed that the GHA UI isn't capable of utilizing a random count of compositions of workflows. If we get into the 3rd level of composite workflow, the job name will be cut off in the left menu which makes navigating the jobs harder (because you have duplicate of the same job, e.g. Compile, belonging to different job profiles).

As a workaround, we came up with Flink CI workflow profiles to configure the CI template yaml that is used in every job. A profile configuration can be specified through a JSON file that lives in the {{.github/workflow}} folder. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:54:11 UTC 2024,,,,,,,,,,"0|z1mdb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:54;mapohl;This issue was resolved differently in FLINK-33915;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Action: Enable Java version in Flink's CI Docker image,FLINK-33910,13562466,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:40,30/Jan/24 14:53,04/Jun/24 20:40,30/Jan/24 14:53,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,Flink's CI Docker image comes with multiple Java versions which can be enabled through environment variables. We should have a custom action that sets these variables properly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:53:50 UTC 2024,,,,,,,,,,"0|z1mdaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:53;mapohl;This issue is resolved in FLINK-33914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Action: Select the right branch and commit hash,FLINK-33909,13562465,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:38,30/Jan/24 14:53,04/Jun/24 20:40,30/Jan/24 14:53,1.19.0,,,,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,"For nightly builds, we want to select the release branches dynamically (rather than using the automatic selection through GHA schedule). We want to do this dynamically because the GHA feature for branch selection seems to be kind of limited right now, e.g.:
 * Don't run a branch that hasn't have any changes in the past 1 day (or any other time period)
 * Run only the most-recent release branches and ignore older release branches (similar to what we're doing in [flink-ci/git-repo-sync:sync_repo.sh|https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh#L28] right now)

A custom action that selects the branch and commit hash enables us to overwrite this setting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:53:19 UTC 2024,,,,,,,,,,"0|z1mdao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:53;mapohl;This issue was resolved differently in FLINK-33916.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Action: Move files within the Docker image to the root folder to match the user,FLINK-33908,13562464,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:28,30/Jan/24 14:53,04/Jun/24 20:40,30/Jan/24 14:53,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,The way the ci template is setup (right now) is to work in the root user's home folder. For this we're copying the checkout into /root. This copying is done in multiple places which makes it a candidate for a custom action.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 14:53:00 UTC 2024,,,,,,,,,,"0|z1mdag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 14:53;mapohl;This issue is resolved in FLINK-33914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Makes copying test jars being done later,FLINK-33907,13562463,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Dec/23 12:25,19/Jan/24 11:13,04/Jun/24 20:40,19/Jan/24 11:13,1.18.0,1.19.0,,,,,,,1.18.2,1.19.0,,,Build System / CI,,,,0,github-actions,pull-request-available,,"We experienced an issue in GHA which is due to the fact how test resources are pre-computed in GHA:
{code:java}
This fixes the following error when compiling flink-clients:
Error: 2.054 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.2.0:copy-dependencies (copy-dependencies) on project flink-clients: Artifact has not been packaged yet. When used on reactor artifact, copy should be executed after packaging: see MDEP-187. -> [Help 1] {code}
We need to move this goal to a later phase.

The reason why this popped up is (as far as I remember) that we do only do test-compile in GitHub Actions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 11:13:04 UTC 2024,,,,,,,,,,"0|z1mda8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 13:41;mapohl;master: [fdafdaa163ca1d494da2d60733d201d08cb9d7b6|https://github.com/apache/flink/commit/fdafdaa163ca1d494da2d60733d201d08cb9d7b6]
1.18: [ce9f5a2672769e34ffc90b45945a4d538bd8c1fd|https://github.com/apache/flink/commit/ce9f5a2672769e34ffc90b45945a4d538bd8c1fd];;;","12/Jan/24 12:32;gyfora;[~mapohl] this change seems to break the local maven testing. 
`mvn test -Dtest=DefaultPackagedProgramRetrieverTest` fails afterwards with File not found errors;;;","12/Jan/24 12:34;mapohl;Yikes, thanks for letting me know. I will look into it!;;;","12/Jan/24 12:59;mapohl;Hm, it works for me locally:
{code:java}
$ mvn -pl flink-clients test -Dtest=DefaultPackagedProgramRetrieverTest{code}
Your specific Maven command didn't work for me because other modules do not have this test included:
{code:java}
[...]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.2.2:test (default-test) on project flink-parent: No tests matching pattern ""DefaultPackagedProgramRetrieverTest"" were executed! (Set -Dsurefire.failIfNoSpecifiedTests=false to ignore this error.) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException{code}
I feel like I'm missing something from your setup. Maybe you're relying on an old version of {{flink-clients}} and you need to build the module once more to have the test jar properly packaged?;;;","12/Jan/24 13:05;mapohl;I tried the same with a clean Maven repository without encountering any problems. The same is true for the following test run:
{code:java}
mvn test -Dtest=DefaultPackagedProgramRetrieverTest -Dsurefire.failIfNoSpecifiedTests=false -Dfast {code}
I wasn't able to reproduce it.;;;","12/Jan/24 14:51;gyfora;Are you using the maven wrapper? 
./mvnw -pl flink-clients clean test -Dtest=DefaultPackagedProgramRetrieverTest
this fails for me if I clean beforehand;;;","15/Jan/24 12:39;mapohl;I'm re-opening the issue and reverting the current version of this change due to the reason brought up by [~gyfora] in his comments above.;;;","15/Jan/24 14:01;chesnay;Unit tests should not rely on jars having been packaged (because {{test}} runs before {{package}}); change it to an ITCase instead.;;;","16/Jan/24 07:53;mapohl;I had a discussion with [~chesnay] on that issue. His argument from the comment above is reasonable. We would need to improve the Maven setup to make the individual test execution work in this specific case (either through the utilization of the failsafe plugin or by introducing another profile). This is covered by FLINK-34107. There is a (admittedly not so convenient) workaround documented in FLINK-34107. Alternatively, you can run {{mvn -pl flink-client verify}}.

The actual goal of FLINK-33907 is to allow for an improved performance of the build pipeline. That outweighs the need to run two tests individually on the command line in my opinion. [~gyfora] WDYT?;;;","16/Jan/24 08:08;gyfora;[~mapohl] originally I hit this issue in one of our build pipelines that uses 
./mvnw clean deploy

So I wasn't even executing a single test in that case. I think it is very strange if even such a simple command fails, that may be very inconvenient to the users. ;;;","16/Jan/24 09:05;mapohl;Fair point: This is due to the fact that the {{test}} phase is executed before the {{pre-package}} phase even in your {{./mvnw clean deploy}} call ([Maven lifecycle|https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html] for reference). Flink's CI does do an initial build w/t tests. I guess, that's why we didn't run into that issue in our CI. But we shouldn't see this issue anymore with the [new change|https://github.com/apache/flink/pull/24095] because the two tests that are affected are moved into the {{integration-test}} phase with the jar being moved into the right place in {{pre-integration-test}}. The order will be correct again. ;;;","16/Jan/24 09:55;gyfora;sounds good [~mapohl] , thanks for the detailed info !;;;","19/Jan/24 11:13;mapohl;I revisited the issue and merged the additional change. The following includes the initial changes from [my comment above|https://issues.apache.org/jira/browse/FLINK-33907?focusedCommentId=17805589&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17805589]:
* master
** [8dbce40b3052aea39e70f250e101bf7e583bd855|https://github.com/apache/flink/commit/8dbce40b3052aea39e70f250e101bf7e583bd855]
** [fdafdaa163ca1d494da2d60733d201d08cb9d7b6|https://github.com/apache/flink/commit/fdafdaa163ca1d494da2d60733d201d08cb9d7b6]
* 1.18
** [e68180a94281ce977ae468d0c7128b493f50a447|https://github.com/apache/flink/commit/e68180a94281ce977ae468d0c7128b493f50a447]
** [ce9f5a2672769e34ffc90b45945a4d538bd8c1fd|https://github.com/apache/flink/commit/ce9f5a2672769e34ffc90b45945a4d538bd8c1fd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
tools/azure-pipelines/debug_files_utils.sh should support GHA output as well,FLINK-33906,13562460,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Dec/23 12:13,15/Jan/24 13:34,04/Jun/24 20:40,15/Jan/24 13:34,1.18.0,1.19.0,,,,,,,1.18.2,1.19.0,,,Build System / CI,,,,0,github-actions,pull-request-available,,{{tools/azure-pipelines/debug_files_utils.sh}} sets variables to reference the debug output. This is backend-specific and only supports Azure CI right now. We should add support for GHA.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 13:34:37 UTC 2024,,,,,,,,,,"0|z1md9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 13:34;mapohl;master: [a675d0dd63b409be9d0db169d61a837fc2094049|https://github.com/apache/flink/commit/a675d0dd63b409be9d0db169d61a837fc2094049]
1.18: [d40ddc38caca4549f8b67f813b8b7d6fd038a193|https://github.com/apache/flink/commit/d40ddc38caca4549f8b67f813b8b7d6fd038a193];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-382: Unify the Provision of Diverse Metadata for Context-like APIs,FLINK-33905,13562458,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,Wencong Liu,Wencong Liu,20/Dec/23 12:11,11/Jan/24 02:51,04/Jun/24 20:40,11/Jan/24 02:51,1.19.0,,,,,,,,,,,,API / Core,,,,0,pull-request-available,,,"This ticket is proposed for [FLIP-382|https://cwiki.apache.org/confluence/display/FLINK/FLIP-382%3A+Unify+the+Provision+of+Diverse+Metadata+for+Context-like+APIs].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 02:51:59 UTC 2024,,,,,,,,,,"0|z1md94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 02:51;xtsong;master (1.19): 06b46a9cbf0d8fa987bbde570510f75a7558f54d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add zip  as a package to GitHub Actions runners,FLINK-33904,13562457,13562450,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,mapohl,mapohl,mapohl,20/Dec/23 12:09,21/Dec/23 11:13,04/Jun/24 20:40,20/Dec/23 20:30,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,pull-request-available,,"FLINK-33253 shows that {{test_pyflink.sh}} fails in GHA because it doesn't find {{{}zip{}}}. We should add this as a dependency in the e2e test.
{code:java}
/root/flink/flink-end-to-end-tests/test-scripts/test_pyflink.sh: line 107: zip: command not found {code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33253,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 20:30:01 UTC 2023,,,,,,,,,,"0|z1md8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 20:30;mapohl;It will be fixed in the workflow;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reenable tests that edit file permissions,FLINK-33903,13562455,13562450,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,20/Dec/23 12:05,21/Dec/23 11:12,04/Jun/24 20:40,,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,,,"In GitHub Actions the permissions seem to work differently to how it works in Azure CI. In both cases, we run the test suite as root. But in GHA runners, the file permission changes won't have any effects because the test started as a root user have permissions to adapt the files in any case.

This issue is about enabling the tests again by rewriting the tests (ideally, because we shouldn't rely on OS features in the tests). Alternatively, we find a way to make those tests work in GHA as well.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27082,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 12:05:09.0,,,,,,,,,,"0|z1md8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch to OpenSSL legacy algorithms,FLINK-33902,13562452,13562450,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,20/Dec/23 12:00,21/Dec/23 13:56,04/Jun/24 20:40,21/Dec/23 13:56,1.18.0,1.19.0,,,,,,,1.18.1,1.19.0,,,Build System,,,,0,github-actions,pull-request-available,,"In FLINK-33550 we discovered that the GHA runners provided by GitHub have a newer version of OpenSSL installed which caused errors in the SSL tests:
{code:java}
Certificate was added to keystore
Certificate was added to keystore
Certificate reply was installed in keystore
Error outputting keys and certificates
40F767F1D97F0000:error:0308010C:digital envelope routines:inner_evp_generic_fetch:unsupported:../crypto/evp/evp_fetch.c:349:Global default library context, Algorithm (RC2-40-CBC : 0), Properties ()
Nov 14 15:39:21 [FAIL] Test script contains errors. {code}
The workaround is to enable legacy algorithms using the {{-legacy}} parameter in 3.0.0+. We might need to check whether that works for older OpenSSL version (in Azure CI).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 21 13:56:22 UTC 2023,,,,,,,,,,"0|z1md7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 12:04;Sergey Nuyanzin;should there be a follow up task to convert certificates to the ones supporting modern algorithms? ;;;","20/Dec/23 12:59;mapohl;We could do it. But there's no real value in my opinion. We use this in a test case (there's no relevance functional-wise, as far as I understand). Therefore, it's also not really a security issue

But of course, it would be the right thing to do from a theoretical standpoint.;;;","21/Dec/23 13:56;mapohl;master: [87738d3f0e5f6022b6decdf47045503de67d0a72|https://github.com/apache/flink/commit/87738d3f0e5f6022b6decdf47045503de67d0a72]
1.18: [2ee3beeaa2302d96e1c420b03ecc0e1e22812110|https://github.com/apache/flink/commit/2ee3beeaa2302d96e1c420b03ecc0e1e22812110];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trial Period: GitHub Actions,FLINK-33901,13562450,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,20/Dec/23 11:54,02/Apr/24 11:36,04/Jun/24 20:40,,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,0,github-actions,,,"This issue is (in contrast to FLINK-27075 which is used for issues that were collected while preparing [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]) collecting all the subtasks that are necessary to initiate the trial phase for GitHub Actions (as discussed in [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34937,,,,,FLINK-27075,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 11:54:14.0,,,,,,,,,,"0|z1md7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple failures in WindowRankITCase due to NoResourceAvailableException,FLINK-33900,13562448,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,20/Dec/23 11:39,25/Mar/24 16:39,04/Jun/24 20:40,25/Mar/24 16:39,1.18.0,1.19.0,,,,,,,,,,,Table SQL / Planner,,,,0,github-actions,test-stability,,"[https://github.com/XComp/flink/actions/runs/7244405295/job/19733011527#step:12:14989]

There are multiple tests in {{WindowRankITCase}} that fail due to a {{NoResourceAvailableException}} supposedly:
{code:java}
[...]
Error: 09:19:33 09:19:32.966 [ERROR] WindowRankITCase.testTumbleWindowTVFWithOffset  Time elapsed: 300.072 s  <<< FAILURE!
14558Dec 18 09:19:33 org.opentest4j.MultipleFailuresError: 
14559Dec 18 09:19:33 Multiple Failures (2 failures)
14560Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14561Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14562Dec 18 09:19:33 	at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:200)
14563Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:248)
14564Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:214)
14565Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:88)
14566Dec 18 09:19:33 	at org.junit.runner.notification.SynchronizedRunListener.testFinished(SynchronizedRunListener.java:87)
14567Dec 18 09:19:33 	at org.junit.runner.notification.RunNotifier$9.notifyListener(RunNotifier.java:225)
14568Dec 18 09:19:33 	at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)
14569Dec 18 09:19:33 	at org.junit.runner.notification.RunNotifier.fireTestFinished(RunNotifier.java:222)
14570Dec 18 09:19:33 	at org.junit.internal.runners.model.EachTestNotifier.fireTestFinished(EachTestNotifier.java:38)
14571Dec 18 09:19:33 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:372)
14572Dec 18 09:19:33 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
14573Dec 18 09:19:33 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
14574Dec 18 09:19:33 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
14575Dec 18 09:19:33 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
14576Dec 18 09:19:33 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
14577Dec 18 09:19:33 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
14578Dec 18 09:19:33 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
14579Dec 18 09:19:33 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
14580Dec 18 09:19:33 	at org.junit.runners.Suite.runChild(Suite.java:128)
14581Dec 18 09:19:33 	at org.junit.runners.Suite.runChild(Suite.java:27)
14582Dec 18 09:19:33 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
14583Dec 18 09:19:33 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
14584Dec 18 09:19:33 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
14585Dec 18 09:19:33 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
14586Dec 18 09:19:33 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
14587Dec 18 09:19:33 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
14588Dec 18 09:19:33 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
14589Dec 18 09:19:33 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
14590Dec 18 09:19:33 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
14591Dec 18 09:19:33 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
14592Dec 18 09:19:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
14593Dec 18 09:19:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
14594Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
14595Dec 18 09:19:33 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
14596Dec 18 09:19:33 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
14597Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
14598Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
14599Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
14600Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
14601Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
14602Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
14603Dec 18 09:19:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
14604Dec 18 09:19:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
14605Dec 18 09:19:33 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
14606Dec 18 09:19:33 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
14607Dec 18 09:19:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
14608Dec 18 09:19:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
14609Dec 18 09:19:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
14610Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
14611Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
14612Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
14613Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
14614Dec 18 09:19:33 	Suppressed: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14615Dec 18 09:19:33 		at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
14616Dec 18 09:19:33 		at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
14617Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
14618Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
14619Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
14620Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
14621Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
14622Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
14623Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
14624Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
14625Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
14626Dec 18 09:19:33 		at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)
14627Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
14628Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
14629Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
14630Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
14631Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
14632Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
14633Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
14634Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
14635Dec 18 09:19:33 		at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
14636Dec 18 09:19:33 		at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
14637Dec 18 09:19:33 		at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
14638Dec 18 09:19:33 		at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
14639Dec 18 09:19:33 		at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
14640Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
14641Dec 18 09:19:33 		at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
14642Dec 18 09:19:33 		at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
14643Dec 18 09:19:33 		at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
14644Dec 18 09:19:33 		at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
14645Dec 18 09:19:33 		at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
14646Dec 18 09:19:33 		at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
14647Dec 18 09:19:33 		at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
14648Dec 18 09:19:33 		at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
14649Dec 18 09:19:33 		at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
14650Dec 18 09:19:33 		at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
14651Dec 18 09:19:33 		at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
14652Dec 18 09:19:33 		at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
14653Dec 18 09:19:33 		at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
14654Dec 18 09:19:33 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
14655Dec 18 09:19:33 		at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
14656Dec 18 09:19:33 		at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
14657Dec 18 09:19:33 		at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
14658Dec 18 09:19:33 		at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
14659Dec 18 09:19:33 		at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
14660Dec 18 09:19:33 		at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
14661Dec 18 09:19:33 		at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
14662Dec 18 09:19:33 		at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
14663Dec 18 09:19:33 	Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough resources available for scheduling.
14664Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$determineParallelism$22(AdaptiveScheduler.java:880)
14665Dec 18 09:19:33 		at java.util.Optional.orElseThrow(Optional.java:290)
14666Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.determineParallelism(AdaptiveScheduler.java:878)
14667Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphWithAvailableResourcesAsync(AdaptiveScheduler.java:1058)
14668Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCreatingExecutionGraph(AdaptiveScheduler.java:1042)
14669Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.createExecutionGraphWithAvailableResources(WaitingForResources.java:183)
14670Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.resourceTimeout(WaitingForResources.java:179)
14671Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:1246)
14672Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$29(AdaptiveScheduler.java:1261)
14673Dec 18 09:19:33 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
14674Dec 18 09:19:33 		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
14675Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
14676Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
14677Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
14678Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
14679Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
14680Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
14681Dec 18 09:19:33 		at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
14682Dec 18 09:19:33 		at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
14683Dec 18 09:19:33 		at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
14684Dec 18 09:19:33 		at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
14685Dec 18 09:19:33 		at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
14686Dec 18 09:19:33 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
14687Dec 18 09:19:33 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
14688Dec 18 09:19:33 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
14689Dec 18 09:19:33 		at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
14690Dec 18 09:19:33 		at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
14691Dec 18 09:19:33 		at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
14692Dec 18 09:19:33 		at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
14693Dec 18 09:19:33 		at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
14694Dec 18 09:19:33 		at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
14695Dec 18 09:19:33 		at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
14696Dec 18 09:19:33 		at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
14697Dec 18 09:19:33 		... 4 more
14698Dec 18 09:19:33 	Suppressed: java.lang.AssertionError
14699Dec 18 09:19:33 		at org.junit.Assert.fail(Assert.java:87)
14700Dec 18 09:19:33 		at org.junit.Assert.assertTrue(Assert.java:42)
14701Dec 18 09:19:33 		at org.junit.Assert.assertTrue(Assert.java:53)
14702Dec 18 09:19:33 		at org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.after(StreamingWithStateTestBase.scala:83)
14703Dec 18 09:19:33 		at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
14704Dec 18 09:19:33 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
14705Dec 18 09:19:33 		at java.lang.reflect.Method.invoke(Method.java:498)
14706Dec 18 09:19:33 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
14707Dec 18 09:19:33 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
14708Dec 18 09:19:33 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
14709Dec 18 09:19:33 		at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
14710Dec 18 09:19:33 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
14711Dec 18 09:19:33 		at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
14712Dec 18 09:19:33 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
14713Dec 18 09:19:33 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
14714Dec 18 09:19:33 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
14715Dec 18 09:19:33 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
14716Dec 18 09:19:33 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
14717Dec 18 09:19:33 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
14718Dec 18 09:19:33 		... 42 more
14719Dec 18 09:19:33 
14720Error: 09:19:33 09:19:32.971 [ERROR] WindowRankITCase.testTumbleWindowTVFWithCalc  Time elapsed: 300.076 s  <<< FAILURE!
14721Dec 18 09:19:33 org.opentest4j.MultipleFailuresError: 
14722Dec 18 09:19:33 Multiple Failures (2 failures)
14723Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14724Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14725Dec 18 09:19:33 	at org.junit.vintage.engine.execution.TestRun.getStoredResultOrSuccessful(TestRun.java:200)
14726Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunListenerAdapter.fireExecutionFinished(RunListenerAdapter.java:248)
14727Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:214)
14728Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunListenerAdapter.testFinished(RunListenerAdapter.java:88)
14729Dec 18 09:19:33 	at org.junit.runner.notification.SynchronizedRunListener.testFinished(SynchronizedRunListener.java:87)
14730Dec 18 09:19:33 	at org.junit.runner.notification.RunNotifier$9.notifyListener(RunNotifier.java:225)
14731Dec 18 09:19:33 	at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:72)
14732Dec 18 09:19:33 	at org.junit.runner.notification.RunNotifier.fireTestFinished(RunNotifier.java:222)
14733Dec 18 09:19:33 	at org.junit.internal.runners.model.EachTestNotifier.fireTestFinished(EachTestNotifier.java:38)
14734Dec 18 09:19:33 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:372)
14735Dec 18 09:19:33 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
14736Dec 18 09:19:33 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
14737Dec 18 09:19:33 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
14738Dec 18 09:19:33 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
14739Dec 18 09:19:33 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
14740Dec 18 09:19:33 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
14741Dec 18 09:19:33 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
14742Dec 18 09:19:33 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
14743Dec 18 09:19:33 	at org.junit.runners.Suite.runChild(Suite.java:128)
14744Dec 18 09:19:33 	at org.junit.runners.Suite.runChild(Suite.java:27)
14745Dec 18 09:19:33 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
14746Dec 18 09:19:33 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
14747Dec 18 09:19:33 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
14748Dec 18 09:19:33 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
14749Dec 18 09:19:33 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
14750Dec 18 09:19:33 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
14751Dec 18 09:19:33 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
14752Dec 18 09:19:33 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
14753Dec 18 09:19:33 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
14754Dec 18 09:19:33 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
14755Dec 18 09:19:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
14756Dec 18 09:19:33 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
14757Dec 18 09:19:33 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
14758Dec 18 09:19:33 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
14759Dec 18 09:19:33 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
14760Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
14761Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
14762Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
14763Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
14764Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
14765Dec 18 09:19:33 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
14766Dec 18 09:19:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
14767Dec 18 09:19:33 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
14768Dec 18 09:19:33 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
14769Dec 18 09:19:33 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
14770Dec 18 09:19:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
14771Dec 18 09:19:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
14772Dec 18 09:19:33 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
14773Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
14774Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
14775Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
14776Dec 18 09:19:33 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
14777Dec 18 09:19:33 	Suppressed: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14778Dec 18 09:19:33 		at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
14779Dec 18 09:19:33 		at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
14780Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
14781Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
14782Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
14783Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
14784Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
14785Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
14786Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
14787Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
14788Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
14789Dec 18 09:19:33 		at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)
14790Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
14791Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
14792Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
14793Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
14794Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
14795Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
14796Dec 18 09:19:33 		at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
14797Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
14798Dec 18 09:19:33 		at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
14799Dec 18 09:19:33 		at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
14800Dec 18 09:19:33 		at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
14801Dec 18 09:19:33 		at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
14802Dec 18 09:19:33 		at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
14803Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
14804Dec 18 09:19:33 		at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
14805Dec 18 09:19:33 		at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
14806Dec 18 09:19:33 		at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
14807Dec 18 09:19:33 		at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
14808Dec 18 09:19:33 		at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
14809Dec 18 09:19:33 		at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
14810Dec 18 09:19:33 		at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
14811Dec 18 09:19:33 		at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
14812Dec 18 09:19:33 		at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
14813Dec 18 09:19:33 		at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
14814Dec 18 09:19:33 		at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
14815Dec 18 09:19:33 		at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
14816Dec 18 09:19:33 		at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
14817Dec 18 09:19:33 		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
14818Dec 18 09:19:33 		at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
14819Dec 18 09:19:33 		at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
14820Dec 18 09:19:33 		at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
14821Dec 18 09:19:33 		at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
14822Dec 18 09:19:33 		at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
14823Dec 18 09:19:33 		at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
14824Dec 18 09:19:33 		at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
14825Dec 18 09:19:33 		at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
14826Dec 18 09:19:33 	Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Not enough resources available for scheduling.
14827Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$determineParallelism$22(AdaptiveScheduler.java:880)
14828Dec 18 09:19:33 		at java.util.Optional.orElseThrow(Optional.java:290)
14829Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.determineParallelism(AdaptiveScheduler.java:878)
14830Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphWithAvailableResourcesAsync(AdaptiveScheduler.java:1058)
14831Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCreatingExecutionGraph(AdaptiveScheduler.java:1042)
14832Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.createExecutionGraphWithAvailableResources(WaitingForResources.java:183)
14833Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.resourceTimeout(WaitingForResources.java:179)
14834Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:1246)
14835Dec 18 09:19:33 		at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$29(AdaptiveScheduler.java:1261)
14836Dec 18 09:19:33 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
14837Dec 18 09:19:33 		at java.util.concurrent.FutureTask.run(FutureTask.java:266)
14838Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
14839Dec 18 09:19:33 		at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
14840Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
14841Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
14842Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
14843Dec 18 09:19:33 		at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
14844Dec 18 09:19:33 		at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
14845Dec 18 09:19:33 		at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
14846Dec 18 09:19:33 		at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
14847Dec 18 09:19:33 		at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
14848Dec 18 09:19:33 		at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
14849Dec 18 09:19:33 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
14850Dec 18 09:19:33 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
14851Dec 18 09:19:33 		at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
14852Dec 18 09:19:33 		at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
14853Dec 18 09:19:33 		at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
14854Dec 18 09:19:33 		at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
14855Dec 18 09:19:33 		at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
14856Dec 18 09:19:33 		at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
14857Dec 18 09:19:33 		at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
14858Dec 18 09:19:33 		at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
14859Dec 18 09:19:33 		at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
14860Dec 18 09:19:33 		... 4 more
14861Dec 18 09:19:33 	Suppressed: java.lang.AssertionError
14862Dec 18 09:19:33 		at org.junit.Assert.fail(Assert.java:87)
14863Dec 18 09:19:33 		at org.junit.Assert.assertTrue(Assert.java:42)
14864Dec 18 09:19:33 		at org.junit.Assert.assertTrue(Assert.java:53)
14865Dec 18 09:19:33 		at org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.after(StreamingWithStateTestBase.scala:83)
14866Dec 18 09:19:33 		at sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)
14867Dec 18 09:19:33 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
14868Dec 18 09:19:33 		at java.lang.reflect.Method.invoke(Method.java:498)
14869Dec 18 09:19:33 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
14870Dec 18 09:19:33 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
14871Dec 18 09:19:33 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
14872Dec 18 09:19:33 		at org.junit.internal.runners.statements.RunAfters.invokeMethod(RunAfters.java:46)
14873Dec 18 09:19:33 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
14874Dec 18 09:19:33 		at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
14875Dec 18 09:19:33 		at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
14876Dec 18 09:19:33 		at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
14877Dec 18 09:19:33 		at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
14878Dec 18 09:19:33 		at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
14879Dec 18 09:19:33 		at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
14880Dec 18 09:19:33 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
14881Dec 18 09:19:33 		... 42 more
14882Dec 18 09:19:33 
14883Dec 18 09:19:33 09:19:33.353 [INFO] 
14884Dec 18 09:19:33 09:19:33.353 [INFO] Results:
14885Dec 18 09:19:33 09:19:33.353 [INFO] 
14886Error: 09:19:33 09:19:33.353 [ERROR] Failures: 
14887Error: 09:19:33 09:19:33.353 [ERROR] WindowRankITCase.testCumulateWindow
14888Dec 18 09:19:33 09:19:33.353 [INFO]   Run 1: PASS
14889Error: 09:19:33 09:19:33.353 [ERROR]   Run 2: Multiple Failures (2 failures)
14890Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14891Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14892Dec 18 09:19:33 09:19:33.353 [INFO] 
14893Error: 09:19:33 09:19:33.353 [ERROR] WindowRankITCase.testCumulateWindowTVF
14894Dec 18 09:19:33 09:19:33.353 [INFO]   Run 1: PASS
14895Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14896Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14897Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14898Dec 18 09:19:33 09:19:33.354 [INFO] 
14899Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testCumulateWindowTVFWithCalc
14900Dec 18 09:19:33 09:19:33.354 [INFO]   Run 1: PASS
14901Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14902Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14903Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14904Dec 18 09:19:33 09:19:33.354 [INFO] 
14905Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testCumulateWindowWithRankOffset
14906Dec 18 09:19:33 09:19:33.354 [INFO]   Run 1: PASS
14907Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14908Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14909Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14910Dec 18 09:19:33 09:19:33.354 [INFO] 
14911Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testHopWindow
14912Dec 18 09:19:33 09:19:33.354 [INFO]   Run 1: PASS
14913Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14914Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14915Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14916Dec 18 09:19:33 09:19:33.354 [INFO] 
14917Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testHopWindowTVF
14918Error: 09:19:33 09:19:33.354 [ERROR]   Run 1: Multiple Failures (2 failures)
14919Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14920Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14921Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14922Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14923Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14924Dec 18 09:19:33 09:19:33.354 [INFO] 
14925Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testHopWindowTVFWithCalc
14926Dec 18 09:19:33 09:19:33.354 [INFO]   Run 1: PASS
14927Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14928Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14929Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14930Dec 18 09:19:33 09:19:33.354 [INFO] 
14931Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testHopWindowWithRankOffset
14932Dec 18 09:19:33 09:19:33.354 [INFO]   Run 1: PASS
14933Error: 09:19:33 09:19:33.354 [ERROR]   Run 2: Multiple Failures (2 failures)
14934Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14935Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14936Dec 18 09:19:33 09:19:33.354 [INFO] 
14937Error: 09:19:33 09:19:33.354 [ERROR] WindowRankITCase.testHopWindowWithoutRankNumber
14938Dec 18 09:19:33 09:19:33.354 [INFO]   Run 1: PASS
14939Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14940Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14941Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14942Dec 18 09:19:33 09:19:33.355 [INFO] 
14943Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTop1
14944Dec 18 09:19:33 09:19:33.355 [INFO]   Run 1: PASS
14945Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14946Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14947Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14948Dec 18 09:19:33 09:19:33.355 [INFO] 
14949Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTumbleWindow
14950Dec 18 09:19:33 09:19:33.355 [INFO]   Run 1: PASS
14951Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14952Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14953Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14954Dec 18 09:19:33 09:19:33.355 [INFO] 
14955Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTumbleWindowTVF
14956Dec 18 09:19:33 09:19:33.355 [INFO]   Run 1: PASS
14957Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14958Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14959Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14960Dec 18 09:19:33 09:19:33.355 [INFO] 
14961Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTumbleWindowTVFWithCalc
14962Error: 09:19:33 09:19:33.355 [ERROR]   Run 1: Multiple Failures (2 failures)
14963Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14964Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14965Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14966Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14967Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14968Dec 18 09:19:33 09:19:33.355 [INFO] 
14969Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTumbleWindowTVFWithOffset
14970Dec 18 09:19:33 09:19:33.355 [INFO]   Run 1: PASS
14971Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14972Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14973Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14974Dec 18 09:19:33 09:19:33.355 [INFO] 
14975Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTumbleWindowWithRankOffset
14976Dec 18 09:19:33 09:19:33.355 [INFO]   Run 1: PASS
14977Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14978Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14979Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14980Dec 18 09:19:33 09:19:33.355 [INFO] 
14981Error: 09:19:33 09:19:33.355 [ERROR] WindowRankITCase.testTumbleWindowWithoutRankNumber
14982Dec 18 09:19:33 09:19:33.355 [INFO]   Run 1: PASS
14983Error: 09:19:33 09:19:33.355 [ERROR]   Run 2: Multiple Failures (2 failures)
14984Dec 18 09:19:33 	org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
14985Dec 18 09:19:33 	java.lang.AssertionError: <no message>
14986Dec 18 09:19:33 09:19:33.355 [INFO] 
14987Dec 18 09:19:33 09:19:33.355 [INFO] 
14988Error: 09:19:33 09:19:33.355 [ERROR] Tests run: 5638, Failures: 16, Errors: 0, Skipped: 8
14989 {code}
It might be related to FLINK-33641",,,,,,,,,,,,,,,,,,,,,,FLINK-34227,,,,,,,FLINK-33641,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 16:39:37 UTC 2024,,,,,,,,,,"0|z1md6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 16:38;jiabao.sun;I noticed there is information about JUnit4  in the stack trace, but we have already completed the migration to JUnit5. 
Regarding the NoResourceAvailableException issue, should it be outdated as we are using the MiniClusterExtension, which releases resources after each test method?

https://github.com/XComp/flink/actions/runs/7244405295/job/19733011527#step:12:12258
{noformat}
Dec 18 09:19:32 	Suppressed: java.lang.AssertionError
Dec 18 09:19:32 		at org.junit.Assert.fail(Assert.java:87)
Dec 18 09:19:32 		at org.junit.Assert.assertTrue(Assert.java:42)
Dec 18 09:19:32 		at org.junit.Assert.assertTrue(Assert.java:53)
Dec 18 09:19:32 		at org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.after(StreamingWithStateTestBase.scala:83)
{noformat}


;;;","23/Dec/23 08:49;mapohl;The same issue was observed with the {{release-1.18}} branch but with {{WindowAggregateITCase}} as the failing test:
https://github.com/XComp/flink/actions/runs/7303960009/job/19905602108#step:12:12202;;;","05/Feb/24 08:32;mapohl;1.18: https://github.com/apache/flink/actions/runs/7778643029/job/21208794729#step:10:11901;;;","05/Feb/24 08:39;mapohl;1.18: https://github.com/apache/flink/actions/runs/7778643029/job/21208794729#step:10:13862;;;","08/Mar/24 16:26;rskraba;1.18 with WindowAggregateITCase : https://github.com/XComp/flink/actions/runs/7303960009/job/19905602108#step:12:12202;;;","25/Mar/24 14:23;rskraba;1.18 with GroupWindowTableAggregateITCase [https://github.com/apache/flink/actions/runs/8414062328/job/23037443503#step:10:11603];;;","25/Mar/24 16:39;mapohl;I checked for the failures where the logs were not removed yet that it's actually a duplicated of FLINK-34227. Closing this one in favor of FLINK-34227.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java 17 and 21 support for mongodb connector,FLINK-33899,13562437,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,jiabao.sun,jiabao.sun,20/Dec/23 10:51,19/Feb/24 15:58,04/Jun/24 20:40,29/Jan/24 09:20,mongodb-1.0.2,,,,,,,,mongodb-1.0.3,mongodb-1.1.0,,,Connectors / MongoDB,,,,0,pull-request-available,,,"After FLINK-33302 is finished it is now possible to specify jdk version
That allows to add jdk17 and jdk21 support",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 19 15:46:33 UTC 2024,,,,,,,,,,"0|z1md4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/24 09:20;leonard;flink-connector-mongodb(main): ed1ea77d14846a0f751ddb8425fcc7dca949bf55;;;","19/Feb/24 15:46;martijnvisser;Also added to apache/flink-connector-mongodb:

v1.0: aa7a9094e527fb83524fe681fcf089ffa0a3c794;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow triggering unaligned checkpoint via REST api,FLINK-33898,13562430,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,20/Dec/23 10:06,20/Dec/23 14:52,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / REST,,,0,,,,See FLINK-33897. ,,,,,,,,,,,,FLINK-33897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 10:06:47.0,,,,,,,,,,"0|z1md2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow triggering unaligned checkpoint via CLI,FLINK-33897,13562429,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,20/Dec/23 10:05,11/Jan/24 09:42,04/Jun/24 20:40,,,,,,,,,,,,,,Command Line Client,Runtime / Checkpointing,,,0,,,,"After FLINK-6755, user could trigger checkpoint through CLI. However I noticed there would be value supporting trigger it in unaligned way, since the job may encounter a high back-pressure and an aligned checkpoint would fail.

 

I suggest we provide an option '-unaligned' in CLI to support that.

 

Similar option would also be useful for REST api",,,,,,,,,,FLINK-33898,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 09:42:03 UTC 2024,,,,,,,,,,"0|z1md2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 02:54;zakelly;This also require the {{SingleCheckpointBarrierHandler}} changing from aligned to unaligned state when receiving an unaligned barrier. Would like to hear your thoughts [~pnowojski] [~dwysakowicz] ;;;","04/Jan/24 14:27;pnowojski;I have mixed feelings. Shouldn't the solution be to just use/enable unaligned checkpoints? If one sets the alignment timeout to some reasonable value, I don't see a reason for someone to use aligned checkpoints anymore. Maybe instead let's consider deprecating aligned checkpoints without timeout?

Is there some real world motivation behind this feature?

I would be -1 for this feature, if it requires complicating/making changes to the actual barrier handling (apart of replacing {{SingleCheckpointBarrierHandler#aligned}} with {{SingleCheckpointBarrierHandler#alternating}} call). This code is complicated and in the past we had a lot of deadlocks, data corruptions and other critical bugs around those areas, so keeping it as simple as possible and minimising amount of supported features is quite important. ;;;","05/Jan/24 05:00;zakelly;[~pnowojski]  Actually there is real world motivation. When a job encountered high back-pressure and after dozens of minutes of aligned checkpointing without success, the user finds that they need to switch to unaligned cp or enlarge the parallelism. Such change requires a job restart, which puts users in a dilemma because this involves replaying much data and a longer delay. This feature allows users to make an unaligned cp temporarily and restart from it, preventing from the large data replay.

I do agree we could enable timeout for aligned cp by default, which greatly reduce this case. And I also think there would be value giving user a chance to change the configuration and restart the job with less pain when they misconfigured their jobs, by supporting triggering a swift and promising checkpoint or savepoint. As for the complication supporting this feature, IIUC, some changes should apply to the handler states (may introduce a new {{{}BarrierHandlerState{}}}) and less change will make to the {{SingleCheckpointBarrierHandler}} itself. I'm not very familiar with this part so if you think this is a big change, I won't insist on doing it.;;;","05/Jan/24 13:05;pnowojski;By real world motivation, I meant if that really is an issue that someone complained about? If not, and this is just a theoretical possibility that comes from your observation when implementing FLINK-6755 ""it could be implemented, someone might find it useful"", I would put it aside for the time being. Honestly, I doubt many users would use this feature. In most cases just cancelling the job and restarting with new configuration would be faster vs someone first trying to find out in the docs/user mailing list/stack overflow that he can actually trigger unaligned checkpoint from CLI first. This would be only useful to a handful of power users, but those should already know about that it's better to use unaligned checkpoints from the get go.

{quote}
I'm not very familiar with this part so if you think this is a big change, I won't insist on doing it.
{quote}
Adding a new BarrierHandlerState maybe is not a very big change per se, but will visible increase complexity of the code when someone needs to read/understand it.

{quote}
I do agree we could enable timeout for aligned cp by default, which greatly reduce this case
{quote}
Let me start the dev mailing list discussion about that.;;;","11/Jan/24 09:42;zakelly;{quote}By real world motivation, I meant if that really is an issue that someone complained about? 
{quote}
The reason behind this is some of our customers start their job with default configuration and find out a back-pressure and checkpoint failures last for a while. They reached out to me to ask if there is some way that can eliminate the back-pressure without introducing much more delay or pouring much duplicated data into sink (not exactly-once). What they complain is they must suffer first to restart the job.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Correlate node,FLINK-33896,13562424,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bvarghese,jackylau,jackylau,20/Dec/23 09:28,10/Jan/24 11:59,04/Jun/24 20:40,10/Jan/24 11:59,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 10 11:59:16 UTC 2024,,,,,,,,,,"0|z1md1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/24 11:59;dwysakowicz;Implemented in c89933e99d5087f81389560663984012733d3bf8..263f3283724a5081e41f679659fa6a5819350739;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for PythonGroupWindowAggregate node,FLINK-33895,13562423,13556317,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,20/Dec/23 09:26,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 09:26:43.0,,,,,,,,,,"0|z1md1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for PythonGroupAggregate node,FLINK-33894,13562422,13556317,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,20/Dec/23 09:26,11/Mar/24 12:43,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 09:26:02.0,,,,,,,,,,"0|z1md14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for PythonCorrelate node,FLINK-33893,13562421,13556317,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jackylau,jackylau,20/Dec/23 09:25,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 09:25:02.0,,,,,,,,,,"0|z1md0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-383: Support Job Recovery from JobMaster Failures for Batch Jobs,FLINK-33892,13562406,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,wanglijie,wanglijie,20/Dec/23 05:42,30/May/24 06:51,04/Jun/24 20:40,30/May/24 06:51,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"This is the umbrella ticket for [FLIP-383|https://cwiki.apache.org/confluence/display/FLINK/FLIP-383%3A+Support+Job+Recovery+for+Batch+Jobs]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 30 06:44:38 UTC 2024,,,,,,,,,,"0|z1mcxk:",9223372036854775807,"In 1.20, we introduced a batch job recovery mechanism to enable batch jobs to recover as much progress as possible after a JobMaster failover, avoiding the need to rerun tasks that have already been finished.

More information about this feature and how to enable it could be found in: https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/recovery_from_job_master_failure/",,,,,,,,,,,,,,,,,,,"20/May/24 07:14;fanrui;Hi [~JunRuiLi] [~zhuzh] , I saw FLIP-383 is almost done in 1.20, would you mind adding it to the 1.20 release doc? thanks in advance.

 

[1]https://cwiki.apache.org/confluence/display/FLINK/1.20+Release;;;","20/May/24 10:16;JunRuiLi;Thank [~fanrui]  for the reminder. I've updated the release doc to include FLIP-383 for version 1.20.:D;;;","20/May/24 10:19;fanrui;[~JunRuiLi] Thank you very much for the quick update!;;;","30/May/24 05:47;zhuzh;The feature development is done except for some follow-up tasks.
Would you add some release notes to this ticket and close it? [~JunRuiLi];;;","30/May/24 06:37;JunRuiLi;Thanks, [~zhuzh] , for the reminder. It seems that I don't have permission to add a release note. Here is the text for the release note:

In 1.20, we introduced a batch job recovery mechanism to enable batch jobs to recover as much progress as possible after a JobMaster failover, avoiding the need to rerun tasks that have already been finished.

More information about this feature and how to enable it could be found in: https://nightlies.apache.org/flink/flink-docs-master/docs/ops/batch/recovery_from_job_master_failure/

Could you assist me by adding it?;;;","30/May/24 06:44;fanrui;Thanks [~zhuzh] for the reminder, and [~JunRuiLi] for the contribution and release note. I have updated it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the obsolete SingleJobGraphStore,FLINK-33891,13562399,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,20/Dec/23 02:32,25/Dec/23 11:04,04/Jun/24 20:40,25/Dec/23 11:04,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"SingleJobGraphStore was introduced a long time ago in FLIP-6. It is only used in a test case in DefaultDispatcherRunnerITCase#
leaderChange_withBlockingJobManagerTermination_doesNotAffectNewLeader. We can replace it with TestingJobGraphStore there and then safely remove the class. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 25 11:04:29 UTC 2023,,,,,,,,,,"0|z1mcw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 05:06;Zhanghao Chen;[~huweihua] Could you help take a look?;;;","25/Dec/23 11:04;huweihua;resolved in master: c80897642152690ef23978e2090be17e60a58039;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Determine the initial status before receiving the first RecordAttributes ,FLINK-33890,13562395,13553221,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,20/Dec/23 01:38,20/Dec/23 01:38,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Task,,,,0,,,,"Currently, all the operators are initialized with non-backlog mode. Ideally, we should determine the initial status before receiving the first {{RecordAttributes}} so that we don't have to initialize the operator in non-backlog mode and immediately switch to backlog mode before processing any records.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-20 01:38:35.0,,,,,,,,,,"0|z1mcv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vote on the release candidate,FLINK-33889,13562390,13562386,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,19/Dec/23 23:05,14/Jan/24 12:55,04/Jun/24 20:40,14/Jan/24 12:55,1.18.0,,,,,,,,,,,,,,,,0,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-19 23:05:49.0,,,,,,,,,,"0|z1mcu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propose a pull request for website updates,FLINK-33888,13562389,13562386,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,19/Dec/23 23:05,19/Jan/24 16:27,04/Jun/24 20:40,19/Jan/24 16:27,1.18.0,,,,,,,,,,,,,,,,0,pull-request-available,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 * update docs/data/flink.yml

 * 
 ** Add a new major version or update minor version as required
 * update docs/data/release_archive.yml
 * add a blog post announcing the release in {{docs/content/posts}}

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]",,,,,,,,,,,,FLINK-33845,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-19 23:05:49.0,,,,,,,,,,"0|z1mcts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage source and binary releases on dist.apache.org,FLINK-33887,13562388,13562386,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,19/Dec/23 23:05,19/Dec/23 23:07,04/Jun/24 20:40,19/Dec/23 23:07,,,,,,,,,,,,,,,,,0,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-19 23:05:48.0,,,,,,,,,,"0|z1mctk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build and stage Java and Python artifacts,FLINK-33886,13562387,13562386,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,19/Dec/23 23:05,19/Dec/23 23:07,04/Jun/24 20:40,19/Dec/23 23:07,,,,,,,,,,,,,,,,,0,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-19 23:05:48.0,,,,,,,,,,"0|z1mctc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.18.1-rc2,FLINK-33885,13562386,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,19/Dec/23 23:05,19/Jan/24 16:27,04/Jun/24 20:40,19/Jan/24 16:27,1.18.0,,,,,,,,,,,,,,,,0,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.

h4. Prerequisites
Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code}
RC_NUM=""1""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,FLINK-33824,,FLINK-33838,,,,,FLINK-33833,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-19 23:05:48.0,,,,,,,,,,"0|z1mct4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Pulsar dependency to 3.0.2 in Pulsar Connector,FLINK-33884,13562357,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dchristle,dchristle,dchristle,19/Dec/23 17:56,15/Apr/24 05:51,04/Jun/24 20:40,15/Apr/24 05:51,pulsar-4.0.1,,,,,,,,pulsar-4.2.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"The [3.0.2 patch|https://pulsar.apache.org/release-notes/versioned/pulsar-3.0.2/] includes various bug fixes, including a few for the Pulsar client (e.g. [link]([https://github.com/apache/pulsar/pull/21144)). Upgrading the dependency in the connector will pick up these fixes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 05:51:14 UTC 2024,,,,,,,,,,"0|z1mcmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 05:51;tison;Master via https://github.com/apache/flink-connector-pulsar/commit/9f4b902c2a478d0105eec1e32bac3ea40f318d00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump CI flink version on flink-connector-jdbc,FLINK-33883,13562346,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,19/Dec/23 15:31,21/Dec/23 10:12,04/Jun/24 20:40,21/Dec/23 10:12,jdbc-3.2.0,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,0,pull-request-available,,,"As Flink 1.18 released, bump the flink version in jdbc connector .
h4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 21 10:12:09 UTC 2023,,,,,,,,,,"0|z1mck8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 10:12;Sergey Nuyanzin;Merged to main as [e664599508315e8d2e270a82d91ddef3513b6567|https://github.com/apache/flink-connector-jdbc/commit/e664599508315e8d2e270a82d91ddef3513b6567];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UT/IT for checkpointing statistics,FLINK-33882,13562335,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jingge,jingge,19/Dec/23 13:40,02/Feb/24 08:34,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"https://issues.apache.org/jira/browse/FLINK-33588

has been manually tested by [~zhutong66] as follows:
1. I will package the modified code, and the code modification will be done in the jar package of flink-dist-xxx.jar. Replace the jar package with the production Flink client.
2. Submit the Flink SQL task in the production environment to Yarn in application mode and check the Yarn logs
3. Check for any further errors in the Yarn log.
4. On the web interface of Flink web, check if the data displayed on the checkpoint information statistics page is normal.

It would be great to write UT or IT for this change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33588,,,,,,,,,"28/Dec/23 17:32;zhutong66;FLINK-33882.patch;https://issues.apache.org/jira/secure/attachment/13065656/FLINK-33882.patch","20/Dec/23 02:41;zhutong66;image-2023-12-20-10-42-00-237.png;https://issues.apache.org/jira/secure/attachment/13065455/image-2023-12-20-10-42-00-237.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 17:34:10 UTC 2023,,,,,,,,,,"0|z1mchs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 13:42;jingge;[~zhutong66] would you like to work on this task? ;;;","19/Dec/23 14:12;zhutong66;[~jingge] Yes I would like;;;","20/Dec/23 02:42;zhutong66;[~jingge] Are you asking me to write the test section in this test directory of the Flink project? As shown in the screenshot below.

!image-2023-12-20-10-42-00-237.png!;;;","28/Dec/23 17:34;zhutong66;[~jingge] I have completed it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[TtlListState]Avoid copy and update value in TtlListState#getUnexpiredOrNull,FLINK-33881,13562332,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lijinzhong,lijinzhong,lijinzhong,19/Dec/23 13:33,11/Jan/24 06:06,04/Jun/24 20:40,11/Jan/24 06:06,,,,,,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"In some scenarios, 'TtlListState#getUnexpiredOrNull -> elementSerializer.copy(ttlValue)'  consumes a lot of cpu resources.

!image-2023-12-19-21-25-21-446.png|width=529,height=119!

I found that for TtlListState#getUnexpiredOrNull， if none of the elements have expired, it still needs to copy all the elements and update the whole list/map in TtlIncrementalCleanup#runCleanup();

!image-2023-12-19-21-26-43-518.png|width=505,height=266!

I think we could optimize TtlListState#getUnexpiredOrNull by:
1）find the first expired element index in the list;
2）If not found, return to the original list;
3）If found, then constrct the unexpire list (puts the previous elements into the list), and go through the subsequent elements, adding expired elements into the list.
{code:java}
public List<TtlValue<T>> getUnexpiredOrNull(@Nonnull List<TtlValue<T>> ttlValues) {
    //.......
    int firstExpireIndex = -1;
    for (int i = 0; i < ttlValues.size(); i++) {
        if (TtlUtils.expired(ttlValues.get(i), ttl, currentTimestamp)) {
            firstExpireIndex = i;
            break;
        }
    }
    if (firstExpireIndex == -1) {
        return ttlValues;  //return the original ttlValues
    }
    List<TtlValue<T>> unexpired = new ArrayList<>(ttlValues.size());
    for (int i = 0; i < ttlValues.size(); i++) {
        if (i < firstExpireIndex) {
            // unexpired.add(ttlValues.get(i));
            unexpired.add(elementSerializer.copy(ttlValues.get(i)));
        }
        if (i > firstExpireIndex) {
            if (!TtlUtils.expired(ttlValues.get(i), ttl, currentTimestamp)) {
                // unexpired.add(ttlValues.get(i));
                unexpired.add(elementSerializer.copy(ttlValues.get(i)));
            }
        }
    }
    //  .....
} {code}
*In this way, the extra iteration overhead is actually very very small, but the benefit when there are no expired elements is significant.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/23 13:25;lijinzhong;image-2023-12-19-21-25-21-446.png;https://issues.apache.org/jira/secure/attachment/13065435/image-2023-12-19-21-25-21-446.png","19/Dec/23 13:26;lijinzhong;image-2023-12-19-21-26-43-518.png;https://issues.apache.org/jira/secure/attachment/13065434/image-2023-12-19-21-26-43-518.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 06:06:45 UTC 2024,,,,,,,,,,"0|z1mch4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 16:10;zakelly;Thanks [~lijinzhong]  for reporting this! However I think it may be duplicated with the FLINK-30088.;;;","20/Dec/23 02:26;lijinzhong;[~Zakelly]  Thanks for your reply.

I think this ticket is targeting a different optimization point than FLINK-30088.

The way mentioned in ticket could avoid elements copy of the TtlListState if there is no expired data. But FLINK-30088 still need copy list elements, consuming lots of unnecessary cpu.;;;","20/Dec/23 05:56;zakelly;Thanks for clarification! It is definitely useful, but I'm not sure is it safe to do shallow copy instead of deep copy.;;;","20/Dec/23 13:36;lijinzhong;[~Zakelly] Sorry for my typo,  I don't think we should change the deep copy to shallow copy.

I've already revised it： 1） If expired element is not found in the list, return the original list; 2) If found expired element, do the deep copy for unexpired elements.;;;","21/Dec/23 02:28;zakelly;[~lijinzhong] Thus I have no question. And actually I think there may be some value if we could make sure it is safe to do shallow copy.;;;","10/Jan/24 05:34;lijinzhong;[~Zakelly] [~masteryhx] 

I have published a pr, could you help to review it?

 

>>>  ""And actually I think there may be some value if we could make sure it is safe to do shallow copy.""

I think it is not safe to do shallow copy.
The CopyOnWrite mechanism in HeapStateBackend only deep copy StateMapEntry wrapper, not user Object.  It is necessary to deepcopy listState elements to avoid taskThread and snapshotThread access one object concurrently. ;;;","10/Jan/24 12:28;lijinzhong;[~Zakelly]  I have done the performance testing with TtlListStateBenchmark. The results show that TtlListState has  about 27%+ performance improvement with this optimization.

 
|Benchmark|Param: backendType|Param: expiredOption|Param: stateVisibility|Param: updateType| Score
{color:#ff0000}without{color} optimization|Score 
{color:#ff0000}with{color} optimization| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnCreateAndWrite|686.100001|902.399321| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnReadAndWrite|713.428757|906.389711| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|720.125366|902.577086| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|736.226185|881.717185| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|NeverExpired|NeverReturnExpired|OnCreateAndWrite|733.647731|894.295796| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|NeverExpired|NeverReturnExpired|OnReadAndWrite|721.162458|901.177007| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|731.355353|890.06144| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAdd|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|726.943995|909.319816| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnCreateAndWrite|212.313443|261.96109| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnReadAndWrite|211.863397|258.266859| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|215.67765|259.52777| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|208.505043|263.768959| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|NeverExpired|NeverReturnExpired|OnCreateAndWrite|201.646868|257.422463| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|NeverExpired|NeverReturnExpired|OnReadAndWrite|214.599971|267.296024| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|207.070109|263.741483| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAddAll|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|213.35357|265.783929| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnCreateAndWrite|536.94471|688.041948| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnReadAndWrite|538.545608|678.766982| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|530.422217|688.008454| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|540.517729|690.383004| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|NeverExpired|NeverReturnExpired|OnCreateAndWrite|538.089004|695.018344| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|NeverExpired|NeverReturnExpired|OnReadAndWrite|532.924439|690.375419| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|513.289823|715.811655| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listAppend|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|534.967481|724.869126| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnCreateAndWrite|630.801404|822.861466| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnReadAndWrite|495.627707|678.028432| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|645.222027|780.880865| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|513.539414|674.461302| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|NeverExpired|NeverReturnExpired|OnCreateAndWrite|645.480934|763.709594| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|NeverExpired|NeverReturnExpired|OnReadAndWrite|512.678829|673.243958| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|646.656774|822.529003| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGet|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|518.734052|664.093519| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnCreateAndWrite|592.612052|778.660071| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnReadAndWrite|507.275622|655.986257| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|597.026859|814.902181| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|500.038208|695.082031| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|NeverExpired|NeverReturnExpired|OnCreateAndWrite|593.968845|783.100913| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|NeverExpired|NeverReturnExpired|OnReadAndWrite|503.457165|654.044131| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|592.170287|722.38563| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listGetAndIterate|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|498.257928|652.923734| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnCreateAndWrite|557.020907|683.122647| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|Expire3PercentPerIteration|NeverReturnExpired|OnReadAndWrite|556.828811|696.510575| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|554.269049|670.339559| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|Expire3PercentPerIteration|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|547.803797|708.379692| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|NeverExpired|NeverReturnExpired|OnCreateAndWrite|562.205645|700.56704| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|NeverExpired|NeverReturnExpired|OnReadAndWrite|553.76725|683.665343| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnCreateAndWrite|536.061935|679.364527| |
|org.apache.flink.state.benchmark.ttl.TtlListStateBenchmark.listUpdate|HEAP|NeverExpired|ReturnExpiredIfNotCleanedUp|OnReadAndWrite|556.908654|719.853693| |
| Total| | | | |25138.13506|32035.67703| |;;;","10/Jan/24 13:01;zakelly;This is Great! Well done.;;;","11/Jan/24 06:06;masteryhx;Thanks [~lijinzhong] for the great work!

merged 907d0f32 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introducing Retry Mechanism for Listing TaskManager Pods to Prevent API Server Connection Failures,FLINK-33880,13562312,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yu.an,Yu.an,Yu.an,19/Dec/23 10:45,02/Jan/24 03:39,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,Deployment / Kubernetes,,,,0,,,,"When operating in Kubernetes mode, if the JobManager undergoes a restart, it attempts to establish a connection with the API server to retrieve the complete list of TaskManager Pods, facilitating the recovery of previous TaskManagers.

In the context of a large Kubernetes cluster with potentially thousands of concurrently running jobs, a scenario may arise where all JobManagers undergo a restart and subsequently connect to the API server (e.g., during disaster recovery). This influx of requests may overwhelm the API server, reaching its maximum capacity and leading to the refusal of some JobManager requests. Consequently, certain JobManagers may experience failures and initiate reconnection attempts to the API server.

!image-2023-12-21-10-12-37-667.png|width=609,height=305!

!image-2023-12-19-18-44-13-623.png|width=505,height=206!

To enhance this process, we can propose the implementation of a retry mechanism. In the event of a failed connection attempt to the API server, Flink will introduce a waiting period before making subsequent connection attempts, mitigating the risk of overwhelming the server and improving the overall resilience of the system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/23 10:41;Yu.an;image-2023-12-19-18-41-41-308.png;https://issues.apache.org/jira/secure/attachment/13065431/image-2023-12-19-18-41-41-308.png","19/Dec/23 10:44;Yu.an;image-2023-12-19-18-44-13-623.png;https://issues.apache.org/jira/secure/attachment/13065430/image-2023-12-19-18-44-13-623.png","21/Dec/23 02:12;Yu.an;image-2023-12-21-10-12-37-667.png;https://issues.apache.org/jira/secure/attachment/13065489/image-2023-12-21-10-12-37-667.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 02 03:39:12 UTC 2024,,,,,,,,,,"0|z1mcco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/24 03:25;Yu.an;Hi [~guoyangze] , How do you think this issue? Is it necessary to fix this problem on the Flink side? I have prepared the fix in my local branch. If you think this fix is necessary, can you assign this ticket to me and let me push the Pull Request to the Flink repo?;;;","02/Jan/24 03:39;guoyangze;I think it's a valid issue, go ahead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle may stop working for a while during redistribution,FLINK-33879,13562304,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,19/Dec/23 10:07,15/Jan/24 02:18,04/Jun/24 20:40,15/Jan/24 02:18,,,,,,,,,1.19.0,,,,Runtime / Network,,,,0,pull-request-available,,,"Currently, the Hybrid Shuffle can work with the memory tier and disk tier together, however, in the following scenario the result partition would stop working.

Suppose we have a shuffle task with 2 sub-partitions. The LocalBufferPool has 15 buffers, the memory tier can use at most 15-(2*(2+1)+1) = 8 buffers according to `TieredStorageMemoryManagerImpl#getMaxNonReclaimableBuffers`. If the memory tier uses up all 8 buffers and the input channel consumes them very slowly because of problems, e.g. unstable network, the disk tier can still work with 1 reserved buffer. However, if a redistribution happens now and the pool size is decreased to less than 8, then the BufferAccumulator can not request buffers anymore, and thus the result partition stops working until the buffers in the memory tier are recycled.

The purpose is to make the result partition still work with the disk tier and write the shuffle data to disk so that once the input channel is ready, the data on the disk can be consumed immediately.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 02:18:58 UTC 2024,,,,,,,,,,"0|z1mcaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 02:18;Weijie Guo;master(1.19) via 879509d7ca886f8f0ed4dd966e859d3c2a5aa231.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Many Keyed Operators extends `TableStreamOperator` which is marked without key. ,FLINK-33878,13562268,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xuyangzhong,xuyangzhong,19/Dec/23 02:02,19/Dec/23 02:04,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,Many Keyed Operator like `WindowJoinOperator` and `SlicingWindowOperator` extends `TableStreamOperator` which is marked without key. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 02:04:38 UTC 2023,,,,,,,,,,"0|z1mc2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 02:03;xuyangzhong;cc [~qingyue] ;;;","19/Dec/23 02:04;xuyangzhong;Do you think we should fix it? If yes, I want to take this Jira and fix it later. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CollectSinkFunctionTest.testConfiguredPortIsUsed fails due to BindException,FLINK-33877,13562266,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,19/Dec/23 01:12,21/Dec/23 10:02,04/Jun/24 20:40,21/Dec/23 10:02,1.19.0,,,,,,,,1.19.0,,,,API / DataStream,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55646&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9482


{noformat}
Dec 18 17:49:57 17:49:57.241 [ERROR] org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.testConfiguredPortIsUsed -- Time elapsed: 0.021 s <<< ERROR!
Dec 18 17:49:57 java.net.BindException: Address already in use (Bind failed)
Dec 18 17:49:57 	at java.net.PlainSocketImpl.socketBind(Native Method)
Dec 18 17:49:57 	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
Dec 18 17:49:57 	at java.net.ServerSocket.bind(ServerSocket.java:390)
Dec 18 17:49:57 	at java.net.ServerSocket.<init>(ServerSocket.java:252)
Dec 18 17:49:57 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction$ServerThread.<init>(CollectSinkFunction.java:375)
Dec 18 17:49:57 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction$ServerThread.<init>(CollectSinkFunction.java:362)
Dec 18 17:49:57 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunction.open(CollectSinkFunction.java:252)
Dec 18 17:49:57 	at org.apache.flink.streaming.api.operators.collect.utils.CollectSinkFunctionTestWrapper.openFunction(CollectSinkFunctionTestWrapper.java:103)
Dec 18 17:49:57 	at org.apache.flink.streaming.api.operators.collect.CollectSinkFunctionTest.testConfiguredPortIsUsed(CollectSinkFunctionTest.java:138)
Dec 18 17:49:57 	at java.lang.reflect.Method.invoke(Method.java:498)
Dec 18 17:49:57 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Dec 18 17:49:57 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Dec 18 17:49:57 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Dec 18 17:49:57 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Dec 18 17:49:57 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 21 10:02:13 UTC 2023,,,,,,,,,,"0|z1mc2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 03:49;jiabao.sun;Based on the exception stack trace, it is possible that port 50500 is being used by other test cases. 
To make this test more stable, we may need to use a random port.

Hi [~jingge], could you help review it when you have time?
Thanks.;;;","19/Dec/23 15:02;jhughes;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55652&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992;;;","20/Dec/23 02:13;jiabao.sun;Merged into master via 
409f44660eb434a57237af2690c9e4b7f9679442
7cae20f22a24403210ca5d6addf83571fcf46843;;;","20/Dec/23 20:05;Sergey Nuyanzin;Have to reopen since it is reproduced again with same issue
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55700&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=9924

at the same time commit from this PR is already present

UPD:
it is reproduced even more than once
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55700&view=logs&j=d06b80b4-9e88-5d40-12a2-18072cf60528&t=609ecd5a-3f6e-5d0c-2239-2096b155a4d0&l=9951;;;","20/Dec/23 20:13;Sergey Nuyanzin;[~jiabao.sun]could you have a look please?;;;","21/Dec/23 02:19;jiabao.sun;Thanks [~Sergey Nuyanzin] to report this.

I noticed that it will fail only when JDK21, maybe because JDK21 has changed the error message, lacking (Bind failed).

{noformat}
Dec 20 04:05:36 Expecting throwable message:
Dec 20 04:05:36   ""Address already in use""
Dec 20 04:05:36 to contain:
Dec 20 04:05:36   ""Address already in use (Bind failed)""
Dec 20 04:05:36 but did not.
{noformat}
;;;","21/Dec/23 10:02;Sergey Nuyanzin;Merged to master  as [13454fd245bc02d73632da51d90cc33c38aaaa0c|https://github.com/apache/flink/commit/13454fd245bc02d73632da51d90cc33c38aaaa0c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Introduce methodName method in TableTestBase,FLINK-33876,13562234,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,jiabao.sun,jiabao.sun,18/Dec/23 15:50,19/Dec/23 11:17,04/Jun/24 20:40,19/Dec/23 11:17,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,Tests,,,0,pull-request-available,,,"After completing the JUnit5 migration in the table planner, there is an incompatibility issue with JUnit TestName and TestInfo. Therefore, considering introducing the methodName method in TableTestBase. External connectors's TablePlanTest can override this method when performing JUnit 5 migration for TableTestBase to avoid compilation issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 11:17:18 UTC 2023,,,,,,,,,,"0|z1mbvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 11:17;leonard;master (1.19): 945c5062c28c3feee94703e88fe6ef797e3760b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support slots wait mechanism at DeclarativeSlotPoolBridge side for Default Scheduler,FLINK-33875,13562220,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,18/Dec/23 14:36,22/Dec/23 03:17,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Task,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 14:36:53.0,,,,,,,,,,"0|z1mbs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support resource request wait mechanism at DefaultDeclarativeSlotPool side for Default Scheduler,FLINK-33874,13562219,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,18/Dec/23 14:34,22/Dec/23 03:17,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Task,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 14:34:41.0,,,,,,,,,,"0|z1mbs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a Redis HyperLogLog Connector for Flink,FLINK-33873,13562202,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jinsuichen,jinsuichen,18/Dec/23 11:47,18/Dec/23 14:12,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / Redis Streams,,,,0,features,,,"Redis HyperLogLog is a probabilistic data structure used for estimating the cardinality of a dataset, which is the number of unique elements in a set. I think it is possible to create a sink connector for HyperLogLog.

FLINK-15571 is about Redis stream connector.

Since there is no component for the Redis connector as a whole, the issue is created under this component.",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Mon Dec 18 14:12:23 UTC 2023,,,,,,,,,,"0|z1mbo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/23 11:51;jinsuichen;hi, [~MartijnVisser] . This is the issue about Redis HyperLogLog Connector. Would you assign it to me?;;;","18/Dec/23 11:59;martijnvisser;[~jinsuichen] New connectors require a FLIP. Can you please prepare one?;;;","18/Dec/23 13:13;jinsuichen;[~martijnvisser] I'm willing to prepare for it. But I don't have permission to create new FLIP pages. According to the documentation, an email has been sent to the dev mailing list for help.;;;","18/Dec/23 14:12;martijnvisser;[~jinsuichen] I've granted you the necessary permissions;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint history does not display for completed jobs,FLINK-33872,13562200,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hong,hong,hong,18/Dec/23 11:37,19/Dec/23 15:45,04/Jun/24 20:40,19/Dec/23 15:45,1.18.0,,,,,,,,1.18.1,1.19.0,,,Runtime / REST,,,,0,pull-request-available,,,"Prior to https://issues.apache.org/jira/browse/FLINK-32469, we see checkpoint history for completed jobs (CANCELED, FAILED, FINISHED).

After https://issues.apache.org/jira/browse/FLINK-32469, the checkpoint history does not show up for completed jobs. 

*Reproduction steps:*
 # Start a Flink cluster.
 # Submit a job with checkpointing enabled.
 # Wait until at least 1 checkpoint completes.
 # Cancel job.
 # Open the Flink dashboard > Job > Checkpoints > History.

We will see log line in JobManager saying ""FlinkJobNotFoundException: Could not find Flink job ( <job_id> )""

*Snapshot of failure:*

When job is running, we can see checkpoints.

!image-2023-12-18-11-37-11-914.png|width=862,height=295!

When job has been CANCELLED, we no longer see checkpoints data.

!image-2023-12-18-11-37-29-596.png|width=860,height=258!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/23 11:37;hong;image-2023-12-18-11-37-11-914.png;https://issues.apache.org/jira/secure/attachment/13065395/image-2023-12-18-11-37-11-914.png","18/Dec/23 11:37;hong;image-2023-12-18-11-37-29-596.png;https://issues.apache.org/jira/secure/attachment/13065394/image-2023-12-18-11-37-29-596.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 15:45:31 UTC 2023,,,,,,,,,,"0|z1mbns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 15:45;hong;merged commit [{{1e89e2f}}|https://github.com/apache/flink/commit/1e89e2fad33df84e33d6276f8fafd38957cfbd47] into apache:master

 ;;;","19/Dec/23 15:45;hong;merged commit [{{54fd94a}}|https://github.com/apache/flink/commit/54fd94ab750c6cbb4cfa21a911656235c1bb059b] into apache:release-1.18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce getTable call for hive client and optimize graph generation time,FLINK-33871,13562168,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hehuiyuan,hehuiyuan,18/Dec/23 08:31,13/Feb/24 00:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,pull-request-available,,,"HiveCatalog.getHiveTable method wastes a lot of time when generate graph, because the number of calls  is relatively high.


I have an sql task with over 2000 rows,  the HiveCatalog.getHiveTable  method is called 4879 times , but only six hive tables were used. 

![image](https://github.com/apache/flink/assets/18002496/d5f0daf3-f80a-4790-ae21-4e75dff9cfd7)

The client.getTable method costs a lot of time.  

![image](https://github.com/apache/flink/assets/18002496/be0d176f-3915-4b92-a177-f1cfaf6d2927)
There is a statistic that jobmanager interacts with hive when generate graph.

If One call takes approximately 50 milliseconds ,
How much time it spends  : 4879 * 50 =243950ms  = 243.95s  = 4min

We can cache and  client.getTable method  is only  called six times.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 08:31:01.0,,,,,,,,,,"0|z1mbgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split the HighAvailabilityServices into LeaderServices and PersistentServices,FLINK-33870,13562158,13561934,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,guoyangze,guoyangze,guoyangze,18/Dec/23 06:55,18/Dec/23 06:55,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 06:55:23.0,,,,,,,,,,"0|z1mbeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add checkpoint metrics: the latency to close the file,FLINK-33869,13562139,13561967,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hejufang001,hejufang001,18/Dec/23 03:13,18/Dec/23 03:13,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 03:13:58.0,,,,,,,,,,"0|z1mba8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add checkpoint metrics: the latency to write the file,FLINK-33868,13562138,13561967,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hejufang001,hejufang001,18/Dec/23 03:13,18/Dec/23 03:13,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 03:13:18.0,,,,,,,,,,"0|z1mba0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add checkpoint metrics: the rate of file write,FLINK-33867,13562137,13561967,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hejufang001,hejufang001,18/Dec/23 03:12,18/Dec/23 03:12,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-18 03:12:27.0,,,,,,,,,,"0|z1mb9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSinkBuilder in flink-connector-kafka references DeliveryGuarantee in flink-connector-base,FLINK-33866,13562125,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,kurto,kurto,17/Dec/23 21:00,28/Dec/23 11:57,04/Jun/24 20:40,,kafka-3.0.2,,,,,,,,,,,,Connectors / Kafka,,,,0,,,,"I have a Flink project that has code like:

```
KafkaSink.<String>builder().setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
```
 
This worked with flink-connector-kafka 3.0.1 as well as past versions of Flink.
 
This fails to compile with flink-connector-kafka 3.0.2 because that release changed flink-connector-base to a provided dependency so the reference to the DeliveryGuarantee class becomes a compiler error.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 11:57:13 UTC 2023,,,,,,,,,,"0|z1mb74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/23 11:57;martijnvisser;[~tzulitai] WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exponential-delay.attempts-before-reset-backoff doesn't work when it's set in Job Configuration,FLINK-33865,13562111,13560425,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,17/Dec/23 10:08,24/Jan/24 04:54,04/Jun/24 20:40,24/Jan/24 04:54,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"exponential-delay.attempts-before-reset-backoff doesn't work when it's set in Job Configuration.
h2. Reason:

When exponential-delay.attempts-before-reset-backoff is set by job Configuration instead of cluster configuration. ExecutionConfig#configure will call RestartStrategies#parseConfiguration to create the ExponentialDelayRestartStrategyConfiguration. And then RestartBackoffTimeStrategyFactoryLoader#getJobRestartStrategyFactory will create the ExponentialDelayRestartBackoffTimeStrategyFactory by the ExponentialDelayRestartStrategyConfiguration.

Since 1.19, RestartStrategies and RestartStrategyConfiguration are depreated, so ExponentialDelayRestartStrategyConfiguration doesn't support exponential-delay.attempts-before-reset-backoff. So if we set exponential-delay.attempts-before-reset-backoff at job level, it won't be supported.
h2. Solution

If we use the ExponentialDelayRestartStrategyConfiguration to save restartStrategy related options in the ExecutionConfig, all new options are set at job level will be missed. 
So we can use the Configuration to save the restartStrategy options inside of ExecutionConfig.

!image-2023-12-17-17-56-59-138.png|width=1212,height=256!",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32895,,,,,,,,,,,,"17/Dec/23 09:57;fanrui;image-2023-12-17-17-56-59-138.png;https://issues.apache.org/jira/secure/attachment/13065366/image-2023-12-17-17-56-59-138.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 04:54:39 UTC 2024,,,,,,,,,,"0|z1mb40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 10:16;fanrui;FLINK-33980 fixed this bug: attempts doesn't take effect when it's set in job configuration.

 

This Jira only adds an ITCase to test it.;;;","24/Jan/24 04:54;fanrui;Merged to master(1.19.0) via:
* 393ef1cc43942ee4652e20206472b07b7794297b
* 5c895aa670bb51593713d66f7bf380b4e92575d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Benchmark] Get rid of generating key-values in advance to optimize heap usage in state benchmark,FLINK-33864,13562079,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zakelly,zakelly,16/Dec/23 13:55,16/Dec/23 13:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"Currently in state benchmark, the key value is prepared and stored in heap before running, which consumes much heap and affects GC. The point is to limit the key space and iterate them in random but determined order. However, we could develop a deterministic key generating method to achieve this without storing those keys. As a result, we could save the heap and make the benchmark more stable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-16 13:55:37.0,,,,,,,,,,"0|z1maww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compressed Operator state restore failed,FLINK-33863,13562069,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ruibin,ruibin,ruibin,16/Dec/23 06:12,15/Jan/24 18:05,04/Jun/24 20:40,02/Jan/24 10:25,1.18.0,,,,,,,,1.18.2,1.19.0,,,Runtime / State Backends,,,,0,pull-request-available,,,"We encountered an issue when using Flink 1.18.0. Our job enabled Snapshot Compression and used multiple operator states and broadcast states in an operator. When recovering Operator State from a Savepoint, the following error occurred: ""org.xerial.snappy.SnappyFramedInputStream: encountered EOF while reading stream header.""

After researching, I believe the error is due to Flink 1.18.0's support for Snapshot Compression on Operator State (see https://issues.apache.org/jira/browse/FLINK-30113 ). When writing a Savepoint, SnappyFramedInputStream adds a header to the beginning of the data. When recovering Operator State from a Savepoint, SnappyFramedInputStream verifies the header from the beginning of the data.

Currently, when recovering Operator State with Snapshot Compression enabled, the logic is as follows:
For each OperatorStateHandle:
1. Verify if the current Savepoint stream's offset is the Snappy header.
2. Seek to the state's start offset.
3. Read the state's data and finally seek to the state's end offset.
(See: [https://github.com/apache/flink/blob/ef2b626d67147797e992ec3b338bafdb4e5ab1c7/flink-runtime/src/main/java/org/apache/flink/runtime/state/OperatorStateRestoreOperation.java#L172] )

Furthermore, when there are multiple Operator States, they are not sorted according to the Operator State's offset. The broadcast states will always be written to the end of the savepoint. However when reading from savepoint, there are no guarantee that broadcast states will be read at last.

Therefore, if the Operator States are out of order and the final offset is recovered first, the Savepoint stream will be seeked to the end, resulting in an EOF error.

I propose a solution: sort the OperatorStateHandle by offset and then recover the Operator State in order. After testing, this approach resolves the issue.

I will submit a PR. This is my first time contributing code, so any help is really appreciated.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30113,FLINK-34063,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 02 10:25:19 UTC 2024,,,,,,,,,,"0|z1mauo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 02:49;Yanfei Lei;Is it because once the file stream has reached EOF, we can‘t use this stream to build a SnappyFramedInputStream?;;;","20/Dec/23 03:38;ruibin;[~Yanfei Lei] Hi, I will try to illustrate this problem with an example:

 
{code:java}
       | Snappy Header 1 | State 1 | Snappy Header 2 | State 2 | Snappy Header 3 | State 3 | 
       ^                 ^         ^                                             ^         ^
offset a                 b         c                                             d         e    

{code}
This is the layout of a snapshot of compressed operator states.

If we try to restore it in a sequence of State 1, 3, 2 instead of State 1, 2, 3:
 # We will start with offset a.
 # Snappy will verify the header 1 and everything will be ok.
 # We will seek to offset b(from the OperatorStateHandle) and restoring the states until we reach offset C.
 # Now we are restoring State 3,  we will verify the snappy header 2 instead of 3.
 # Then we will seek to offset d and eventually reached offset e.
 # Then we are going to restoring State 2 and when trying to verify the header, an EOF error is thrown.

So there are two problems if we don't sort states by offsets before restoring them:
 # In step 4, we try to restoring State 3, instead the header of State 2 is verified.
 # There is currently no simple way to seek to the correct header position.

 

 

 ;;;","27/Dec/23 07:41;Yanfei Lei;Merged via [d415d93bbf9620ba985136469107edd8c6e31cc6|https://github.com/apache/flink/commit/d415d93bbf9620ba985136469107edd8c6e31cc6] ;;;","28/Dec/23 11:58;martijnvisser;Shouldn't this also be backported to 1.18?;;;","29/Dec/23 02:49;ruibin;[~martijnvisser] I think so. I have created a backport PR for this. https://github.com/apache/flink/pull/24008;;;","02/Jan/24 10:25;martijnvisser;Merged via apache/flink:release-1.18 f9383e6780ae8beb995d9bbd58a8484d19900f55;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Unit Test Failures on 1.18.0,FLINK-33862,13562068,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,prabhujoseph,prabhujoseph,16/Dec/23 05:42,18/Dec/23 15:40,04/Jun/24 20:40,18/Dec/23 12:00,1.18.0,1.19.0,,,,,,,,,,,,,,,0,,,,"Flink Unit Test Failures on 1.18.0. There are 100+ unit test cases failing due to below common issues.

*Issue 1*
{code:java}
./mvnw -DfailIfNoTests=false -Dmaven.test.failure.ignore=true -Dtest=ExecutionPlanAfterExecutionTest test

[INFO] Running org.apache.flink.client.program.ExecutionPlanAfterExecutionTest
org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
	at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
	at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)
	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1645)
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1144)
	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1084)
	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:923)
	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$5(Execution.java:621)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
	... 5 more
Caused by: java.util.concurrent.ExecutionException: Boxed Error
	at scala.concurrent.impl.Promise$.resolver(Promise.scala:87)
	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:79)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
	at org.apache.pekko.actor.ActorRef.tell(ActorRef.scala:141)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:317)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
	... 22 more
Caused by: java.lang.NoClassDefFoundError: io/airlift/compress/Decompressor
	at org.apache.flink.runtime.io.network.buffer.BufferCompressor.<init>(BufferCompressor.java:54)
	at org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.create(ResultPartitionFactory.java:160)
	at org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.create(ResultPartitionFactory.java:137)
	at org.apache.flink.runtime.io.network.NettyShuffleEnvironment.createResultPartitionWriters(NettyShuffleEnvironment.java:229)
	at org.apache.flink.runtime.taskmanager.Task.<init>(Task.java:403)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:793)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
	... 23 more
Caused by: java.lang.ClassNotFoundException: io.airlift.compress.Decompressor
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	... 36 more
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.012 s <<< FAILURE! - in org.apache.flink.client.program.ExecutionPlanAfterExecutionTest
[ERROR] org.apache.flink.client.program.ExecutionPlanAfterExecutionTest.testGetExecutionPlanOfRangePartition  Time elapsed: 3.671 s  <<< FAILURE!
java.lang.AssertionError: Cannot run both #getExecutionPlan and #execute.
	at org.apache.flink.client.program.ExecutionPlanAfterExecutionTest.testGetExecutionPlanOfRangePartition(ExecutionPlanAfterExecutionTest.java:110)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   ExecutionPlanAfterExecutionTest.testGetExecutionPlanOfRangePartition:110 Cannot run both #getExecutionPlan and #execute.

{code}
*Issue 2*
{code:java}
./mvnw -DfailIfNoTests=false -Dmaven.test.failure.ignore=true -Dtest=ConfigOptionsYamlSpecTest test 

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.223 s <<< FAILURE! - in org.apache.flink.docs.configuration.ConfigOptionsYamlSpecTest
[ERROR] org.apache.flink.docs.configuration.ConfigOptionsYamlSpecTest.testNoKeyPrefixOfOtherKey  Time elapsed: 0.197 s  <<< ERROR!
java.lang.NoClassDefFoundError: org/influxdb/InfluxDB$ConsistencyLevel
	at org.apache.flink.metrics.influxdb.InfluxdbReporterOptions.<clinit>(InfluxdbReporterOptions.java:79)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:315)
	at org.apache.flink.docs.util.ConfigurationOptionLocator.discoverOptionsAndApply(ConfigurationOptionLocator.java:164)
	at org.apache.flink.docs.util.ConfigurationOptionLocator.discoverOptionsAndApply(ConfigurationOptionLocator.java:139)
	at org.apache.flink.docs.configuration.ConfigOptionsYamlSpecTest.testNoKeyPrefixOfOtherKey(ConfigOptionsYamlSpecTest.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Caused by: java.lang.ClassNotFoundException: org.influxdb.InfluxDB$ConsistencyLevel
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	... 64 more
{code}
*Issue 3*
{code:java}
./mvnw -DfailIfNoTests=false -Dmaven.test.failure.ignore=true -Dtest=AzureBlobStorageFSFactoryTest test  


Expected: (an instance of java.lang.IllegalArgumentException and exception with message a string containing ""Cannot initialize WASB file system, URI authority not recognized."")
     but: an instance of java.lang.IllegalArgumentException <java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FileSystem> is a java.lang.NoClassDefFoundError
Stacktrace was: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FileSystem
	at org.apache.flink.fs.azurefs.AzureBlobStorageFSFactoryTest.getFactory(AzureBlobStorageFSFactoryTest.java:51)
	at org.apache.flink.fs.azurefs.AzureBlobStorageFSFactoryTest.testCreateFsWithMissingAuthority(AzureBlobStorageFSFactoryTest.java:91)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FileSystem
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	... 57 more

{code}
*Issue 4*
{code:java}
[ERROR] org.apache.flink.table.planner.match.PatternTranslatorTest.testWithinClause  Time elapsed: 5.672 s  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/flink/table/codesplit/JavaCodeSplitter
        at org.apache.flink.table.runtime.generated.GeneratedClass.<init>(GeneratedClass.java:58)
        at org.apache.flink.table.runtime.generated.GeneratedFunction.<init>(GeneratedFunction.java:50)
        at org.apache.flink.table.planner.codegen.MatchCodeGenerator.generateMatchFunction(MatchCodeGenerator.scala:300)
        at org.apache.flink.table.planner.codegen.MatchCodeGenerator.generateIterativeCondition(MatchCodeGenerator.scala:186)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch$PatternVisitor.visitLiteral(CommonExecMatch.java:295)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch$PatternVisitor.visitLiteral(CommonExecMatch.java:252)
        at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1217)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch$PatternVisitor.visitCall(CommonExecMatch.java:306)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch$PatternVisitor.visitCall(CommonExecMatch.java:252)
        at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
        at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecMatch.translatePattern(CommonExecMatch.java:231)
        at org.apache.flink.table.planner.match.PatternTranslatorTestBase.verifyPattern(PatternTranslatorTestBase.scala:114)
        at org.apache.flink.table.planner.match.PatternTranslatorTest.testWithinClause(PatternTranslatorTest.scala:259)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
        at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
        at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
        at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
        at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 15:40:53 UTC 2023,,,,,,,,,,"0|z1maug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/23 10:44;martijnvisser;This isn't a command that you would use to build Flink normally.;;;","18/Dec/23 11:36;prabhujoseph;[FLINK-28203|https://issues.apache.org/jira/browse/FLINK-28203] has marked bundled dependecies as optional. Setting the flag flink.markBundledAsOptional to false has fixed the unit test failures. ;;;","18/Dec/23 12:00;martijnvisser;[~prabhujoseph] Yes, because that's needed for Maven version support of 3.3 and higher. So you're setting a parameter deliberately to a different value, so it's not weird that it's failing, right? :);;;","18/Dec/23 15:40;prabhujoseph;Yes right. Thanks [~martijnvisser].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for WindowRank node,FLINK-33861,13562046,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,15/Dec/23 21:13,20/Dec/23 13:05,04/Jun/24 20:40,20/Dec/23 13:05,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 13:05:55 UTC 2023,,,,,,,,,,"0|z1mapk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 13:05;dwysakowicz;Implemented in aa5766e257a8b40e15d08eafa1e005837694772b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for WindowTableFunction node,FLINK-33860,13562045,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,15/Dec/23 21:12,20/Dec/23 15:46,04/Jun/24 20:40,20/Dec/23 15:46,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 15:46:45 UTC 2023,,,,,,,,,,"0|z1mapc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 15:46;dwysakowicz;Implemented in 1a8b8d512c213ba330f28eca663bc77e2369b61b..cabb28d25c4c58af3ee23fc4a63f9564aefd6146;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support OpenSearch v2,FLINK-33859,13562043,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,15/Dec/23 20:33,23/May/24 13:20,04/Jun/24 20:40,14/May/24 08:11,opensearch-1.1.0,,,,,,,,opensearch-2.0.0,,,,Connectors / Opensearch,,,,0,pull-request-available,,,"The main issue is that in OpenSearch v2 there were several breaking changes like 
[https://github.com/opensearch-project/OpenSearch/pull/9082]
[https://github.com/opensearch-project/OpenSearch/pull/5902]

which made current connector version failing while communicating with v2

 

Also it would make sense to add integration and e2e tests to test against v2",,,,,,,,,,,,,,,,,,,,,,FLINK-30537,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 14 08:11:40 UTC 2024,,,,,,,,,,"0|z1maow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/May/24 08:11;Sergey Nuyanzin;Merged as [22a2934c32898a1c4016d398333f82772522036f|https://github.com/apache/flink-connector-opensearch/commit/22a2934c32898a1c4016d398333f82772522036f];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI fails with No space left on device,FLINK-33858,13562005,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,Sergey Nuyanzin,Sergey Nuyanzin,15/Dec/23 14:27,18/Dec/23 10:29,04/Jun/24 20:40,,,,,,,,,,,,,,Build System / CI,,,,0,,,,"AlibabaCI003-agent01
AlibabaCI003-agent03
AlibabaCI003-agent05
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55558&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=9765]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 10:29:55 UTC 2023,,,,,,,,,,"0|z1magg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 14:30;Sergey Nuyanzin;[~jingge]  could you please have a look since you are one of thos who has enough grants to look at it;;;","15/Dec/23 15:14;jingge;Thanks for the heads-up, checking...

 ;;;","15/Dec/23 16:12;jingge;I don't have the access rights and need to reach out to the right one, WIP...;;;","18/Dec/23 10:16;jingge;Does anyone know if we could just delete /home/agent03/myagent/_work/_temp/debug_files/mvn-1.log? Or we must archive i.e. zip it? Thanks!;;;","18/Dec/23 10:29;Sergey Nuyanzin;zip would be the safest

otherwise if it relates to successful build then it could be removed
if it relates to failure then probably would make sense probably to attach here zip version for all mvn-*, not only mvn-1.log in case the size is suitable ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose BulkProcessor.flush via RequestIndexer to allow custom flush logic,FLINK-33857,13561998,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,schulzp,schulzp,schulzp,15/Dec/23 13:39,29/Dec/23 02:08,04/Jun/24 20:40,29/Dec/23 02:08,elasticsearch-3.0.1,,,,,,,,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,"There's [a bug|https://github.com/elastic/elasticsearch/issues/103406] in elasticsearch that may lead to underestimated bulk request sizes. We are hit by this bug and therefore, I would like to propose a simple improvement to work around it: 

{{ElasticsearchWriter}} uses {{RequestIndexer}} as a facade for ES' {{BulkProcessor}}. As of now, only the {{add(…)}} methods are delegated. Under the hood, the {{BulkProcessor}} takes care of flushing bulk requests to the server when any of the configurable limits is hit (there's another bug: flushing will happen _after_ the limit has exceeded).

(+) *Proposal:* Expose {{BulkProcessor.flush()}} via {{RequestIndexer.flush()}}. This way we can easily implement logic to decide when to flush, inside the emitter.

I created a [pull request|https://github.com/apache/flink-connector-elasticsearch/pull/85]. Feedback is highly welcome. :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 29 02:08:20 UTC 2023,,,,,,,,,,"0|z1maew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/23 02:08;Weijie Guo;main via(b43500510085ff37239efbb107179333df83ae6f);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sub-task spans to TraceReporter for checkpointing,FLINK-33856,13561967,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hejufang001,hejufang001,hejufang001,15/Dec/23 09:34,24/Apr/24 08:09,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"When Flink makes a checkpoint, the interaction performance with the external file system has a great impact on the overall time-consuming. Therefore, it is easy to observe the bottleneck point by adding performance indicators when the task interacts with the external file storage system. These include: the rate of file write , the latency to write the file, the latency to close the file.

In flink side add the above metrics has the following advantages: convenient statistical different task E2E time-consuming; do not need to distinguish the type of external storage system, can be unified in the FsCheckpointStreamFactory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 08:09:39 UTC 2024,,,,,,,,,,"0|z1ma80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 01:41;Jiang Xin;Sounds reasonable. Have you already started working on this?;;;","21/Dec/23 04:07;hejufang001;[~Jiang Xin] Yes, this metrics optimization has been implemented in our company, which is very helpful for locating checkpoint bottlenecks. Now I want to contribute to the community, can you assign this to me?;;;","21/Dec/23 04:49;Jiang Xin;[~hejufang001] I'm sorry, I have no permission to assign but I will keep following up on this issue. [~Weijie Guo] Could you help assign this issue?;;;","21/Dec/23 05:17;Weijie Guo;Yep, it seems feasible. I have assigned this to you. Before you start preparing for PR, I would like to ask [~masteryhx] to check this one.;;;","26/Dec/23 06:25;hejufang001;[~masteryhx] [~Weijie Guo]  I have created a PR for this issue, Could you help take a look?（https://github.com/apache/flink/pull/23989）;;;","27/Dec/23 03:37;masteryhx;[~Weijie Guo] Thanks for pinging me here.

[~hejufang001]

Thanks for the proposal.

I think these metrics sound reasonable.

IIUC, they are checkpoint related task-level metrics.

I think we could use TraceReporter provided by FLINK-33695 but not use current MetricReporter as you could see the reaon mentioned in FLINK-33695

cc [~pnowojski] ;;;","27/Dec/23 15:40;martijnvisser;Just to be thorough, metrics are considered to be public interfaces and new public interfaces should follow the FLIP process, per https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals;;;","03/Jan/24 12:38;hejufang001;[~pnowojski] It seems reasonable to use TraceReporter to report checkpoint related task-level metrics, could this issue be a subtask of FLIP-384?  WDYT;;;","04/Jan/24 14:59;pnowojski;Hi, I second that implementing this as metrics doesn't sound to be right/correct. 

 

[~hejufang001] , I wouldn't make this a subtask of the FLIP-384, but if needed a follow up. There are two things worth notting/discussing:
 * please check the discussion on the dev mailing list in FLIP-384 about the current limitations. Namely we are currently only creating a trace with a single span for the whole checkpoint. Also it's currently very sparsely populated with metrics. There were discussions/plans (CC [~fanrui] if I remember correctly you wanted to follow up on this?) about creating children spans per each subtask/task, to mimic the existing `CheckpointingMetrics` structure. Probably this FLIP requires that change.
 * once we have per subtask spans, or aggregated metrics as in [the recovery spans from FLIP-386|https://cwiki.apache.org/confluence/display/FLINK/FLIP-386%3A+Support+adding+custom+metrics+in+Recovery+Spans] , we might not need some of the metrics, that you are proposing here? For example `writeRate` should be easily computed from the async duration / checkpointed state size?

Anyway, I think FLIP will be required here. ;;;","05/Jan/24 03:12;fanrui;Thanks [~pnowojski] for the ping.:)
{quote} [~fanrui] if I remember correctly you wanted to follow up on this?
{quote}
As I said in the mail list, I propose adding a series of TraceSpan for job start, such as:
 * From JobManager process is started to JobGraph is created
 * From JobGraph is created to JobMaster is created
 * From JobMaster is created to job is running
 * From start request tm from yarn or kubernetes to all tms are ready 
 * etc

And I and [~easonqin]  who my colleague created the FLIP-412: Add the time-consuming span of each stage when starting the Flink job to TraceReporter[2][3] just now to follow up it. IIUC, it's not related to this JIRA, right? They add the Span for different stage.

 

[1][https://lists.apache.org/thread/7lql5f5q1np68fw1wc9trq3d9l2ox8f4]

[2][https://cwiki.apache.org/confluence/x/8435E]

[3]https://issues.apache.org/jira/browse/FLINK-33999;;;","05/Jan/24 08:38;hejufang001;[~pnowojski] Thanks for your advice.

It seems that we need children spans per each subtask/task, so that we can statistics more detailed task-level information and more conveniently to locate the bottleneck of the cp making. such as syncDuration /async duration/ the latency to write file /the latency to close file, of course 'writeRate' is no longer needed.

IMO, I prefer to report metrics separately for different TMs. Because our production environment has a large number of TM and subtasks, if the changelog checkpoint is enabled, the checkpoint may be frequent. I am worried that a large amount of data aggregation to JM may have performance problems.

Maybe a new flip that supports task-level trace reporter can builded ?  I’m willing to participate in the development.;;;","05/Jan/24 09:56;pnowojski;{quote}
Maybe a new flip that supports task-level trace reporter can builded ?  I’m willing to participate in the development.
{quote}
Please again check the FLIP-384 discussions. I was highlighting there a couple of difficulties:
{quote}
However, if we would like to create true distributed traces, with spans
reported from many different
components, potentially both on JM and TM, the problem is a bit deeper. The
issue in that case is how
to actually fill out `parrent_id` and `trace_id`? Passing some context
entity as a java object would be
unfeasible. That would require too many changes in too many places. I think
the only realistic way
to do it, would be to have a deterministic generator of `parten_id` and
`trace_id` values.

For example we could create the parent trace/span of the checkpoint on JM,
and set those ids to
something like: `jobId#attemptId#checkpointId`. Each subtask then could
re-generate those ids
and subtasks' checkpoint span would have an id of
`jobId#attemptId#checkpointId#subTaskId`.
Note that this is just an example, as most likely distributed spans for
checkpointing do not make
sense, as we can generate them much easier on the JM anyway.
{quote}
https://lists.apache.org/thread/7lql5f5q1np68fw1wc9trq3d9l2ox8f4

At the same time:
{quote}
 I am worried that a large amount of data aggregation to JM may have performance problems.
{quote}
I wouldn't worry about that too much. This data is already aggregated on the JM from all of the TMs via {{CheckpointMetricsBuilder}} and {{CheckpointMetrics}}. Besides, it's just a single RPC from subtask -> JM per checkpoint. If that becomes a problem, we would have problems in many different areas as well (for example {{notifyCheckpointCompleted}} is a very similar call but the other direction).

Also AFAIR there are/were different ideas how to solve this potential bottleneck in a more generic way (having multiple job coordinators in the cluster to spread the load).

[~hejufang001] I would suggest that both of yo chat offline about the scope of the changes in [~fanrui]'s FLIP and/or eventual division of work. I'm not sure if [~fanrui] plans to add per task/subtask spans for checkpoints and/or recovery.;;;","05/Jan/24 10:06;fanrui;{quote}I would suggest that both of yo chat offline about the scope of the changes in Rui Fan's FLIP and/or eventual division of work. I'm not sure if Rui Fan plans to add per task/subtask spans for checkpoints and/or recovery.
{quote}
FLIP-412 focuses on adding the time-consuming span of each stage when starting the Flink job. It will be the job level instead subtask or task level according to my idea.;;;","11/Jan/24 17:42;pnowojski;In that case [~hejufang001] it would be great if you started another FLIP for adding per sub-task spans. If you decide to od so, please ping me so on Apache Flink Slack or via a Jira ticket so I don't miss it :);;;","16/Jan/24 03:45;hejufang001;ok, I'll get to work on it. [~pnowojski] ;;;","25/Jan/24 10:03;hejufang001;[~pnowojski] I wrote a draft of a flip, but found I didn't have edit permission for confluence. I emailed the flink community a few days ago to ask for permission, but got no response. Could you please add confluence permission for me?;;;","25/Jan/24 10:08;martijnvisser;[~hejufang001] What's your Confluence ID? I'll add you;;;","25/Jan/24 13:08;hejufang001;[~martijnvisser] I tried to signup for confluence([https://cwiki.apache.org/confluence/signup.action]), but it looks like public signup is not allowed. Can you help me add an account and add edit permission?;;;","02/Feb/24 15:19;martijnvisser;There's been a change at the ASF on who can create Confluence accounts, we're discussing on how to proceed. Hopefully I'll have an answer soon.;;;","26/Feb/24 07:58;pnowojski;Hey [~hejufang001]! The new processes hasn't been yet documented, but it will be basically the same, the only difference being that for initial discussion stage FLIPs should be published not on wiki but via Google Docs (with public read access, disabled comments). Before/after voting a committer will be copy pasting it to wiki. So if you are still willing to work on this, feel free to start the discussion thread using Google Document :);;;","26/Feb/24 08:49;martijnvisser;[~hejufang001] I got pinged by [~pnowojski] that I hadn't updated the process documentation yet, I've just done so at https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals - Let me know if it's clear, or if I need to clarify some parts ;;;","29/Feb/24 12:12;hejufang001;[~martijnvisser] Thank you for your reply. The description in the document is very clear.
[~pnowojski] I created a Google Docs that contains a draft of flip support for subtask spans, maybe you can take a look at it first.（[https://docs.google.com/document/d/1lYsDU5fE-ke4fHXeVYx1YD7vxZ6EtEqvK2xNhXBrwSE]）;;;","24/Apr/24 08:09;pnowojski;Hey [~hejufang001]! Sorry for the usual open source delay :( The FLIP looks good to me. I would only suggest to add a config option to disable/enable those child spans.

It would be also nice to have some aggregated values like {{sum}} or {{max}} in the parent span, in a similar way how recovery spans have those currently, but that might be an independent effort.

I've created a FLIP-448 page for this effort: https://cwiki.apache.org/confluence/display/FLINK/FLIP-448%3A+Add+sub-task+spans+to+TraceReporter+for+checkpointing . Next steps would be to create dev mailing list thread.

nit: I think if you press the button [Publish to web|https://images.template.net/wp-content/uploads/2022/10/How-to-Share-Google-Docs-with-Others-Publish-on-Web.jpg], the wiki widget will be able to generate a preview.;;;",,,,,,,,,,,,,,,,,,
send flag message like checkpoint barrier,FLINK-33855,13561964,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,SpongebobZ,SpongebobZ,15/Dec/23 09:25,28/Dec/23 12:00,04/Jun/24 20:40,28/Dec/23 12:00,,,,,,,,,,,,,Table SQL / API,,,,0,,,,"I use flinksql to consume kafka message. Now I want to send one specific message to that kafka topic, and I hope the sink table that use user defined connector can receive this specific message. maybe it sounds like the barrier to trigger checkpoints. Is there anyway to solve this debt ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 12:00:15 UTC 2023,,,,,,,,,,"0|z1ma7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/23 12:00;martijnvisser;If you have a user question, please reach out to the user mailing list, Slack or Stackoverflow. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots times out,FLINK-33854,13561957,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Dec/23 08:29,28/Feb/24 07:18,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,,Deployment / YARN,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55530&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=30604
{code:java}
Dec 15 02:20:23 ""Listener at 943ab8fca711/37257"" #15 daemon prio=5 os_prio=0 tid=0x00007f12f00ba800 nid=0x183ba in Object.wait() [0x00007f12d35fc000]
Dec 15 02:20:23    java.lang.Thread.State: WAITING (on object monitor)
Dec 15 02:20:23         at java.lang.Object.wait(Native Method)
Dec 15 02:20:23         at java.lang.Thread.join(Thread.java:1252)
Dec 15 02:20:23         - locked <0x00000000a6a85488> (a org.apache.flink.yarn.YarnTestBase$Runner)
Dec 15 02:20:23         at java.lang.Thread.join(Thread.java:1326)
Dec 15 02:20:23         at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.lambda$testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots$3(YARNSessionCapacitySchedulerITCase.java:352)
Dec 15 02:20:23         at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase$$Lambda$841/845893054.run(Unknown Source)
Dec 15 02:20:23         at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
Dec 15 02:20:23         at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.testVCoresAreSetCorrectlyAndJobManagerHostnameAreShownInWebInterfaceAndDynamicPropertiesAndYarnApplicationNameAndTaskManagerSlots(YARNSessionCapacitySchedulerITCase.java:287)
Dec 15 02:20:23         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 07:18:41 UTC 2024,,,,,,,,,,"0|z1ma5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/24 07:18;mapohl;https://github.com/apache/flink/actions/runs/8074215128/job/22059455823#step:10:33778;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Migrate Junit5 for DeclarativeSlotPoolBridge test classes of runtime module,FLINK-33853,13561950,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,RocMarshal,RocMarshal,RocMarshal,15/Dec/23 07:22,15/Dec/23 15:17,04/Jun/24 20:40,15/Dec/23 15:17,,,,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 15:17:29 UTC 2023,,,,,,,,,,"0|z1ma48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 15:17;fanrui;Merged to master 1.19 via 1136ed50311a18c0b5773ae982330cc2936eba3d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-403 High Availability Services for OLAP Scenarios,FLINK-33852,13561934,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,guoyangze,guoyangze,guoyangze,15/Dec/23 05:02,18/Dec/23 06:48,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,,,,,,,,,,,,,,,FLINK-25318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-15 05:02:15.0,,,,,,,,,,"0|z1ma0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No need for patch release - Start End of Life discussion thread for now outdated Flink minor version,FLINK-33851,13561875,13561867,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jingge,jingge,14/Dec/23 15:52,16/Jan/24 22:05,04/Jun/24 20:40,16/Jan/24 22:05,,,,,,,,,,,,,,,,,0,,,,"The idea is to discuss whether we should do a final release for the now not supported minor version in the community. Such a minor release shouldn't be covered by the current minor version release managers. Their only responsibility is to trigger the discussion.

The intention of a final patch release for the now unsupported Flink minor version is to flush out all the fixes that didn't end up in the previous release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:25.0,,,,,,,,,,"0|z1m9nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates the docs stable version,FLINK-33850,13561874,13561867,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:52,19/Jan/24 16:57,04/Jun/24 20:40,19/Jan/24 16:57,,,,,,,,,,,,,,,,,0,pull-request-available,,,"Update docs to ""stable"" in {{docs/config.toml}} in the branch of the _just-released_ version:
 * Change V{{{}ersion{}}} from {{{}x.y.z }}to \{{{}x.y.z{}}}, i.e. {{1.18.0}} to {{1.18.1}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:25.0,,,,,,,,,,"0|z1m9nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No need for patch release - Update reference data for Migration Tests,FLINK-33849,13561873,13561867,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,jingge,jingge,14/Dec/23 15:52,15/Jan/24 09:42,04/Jun/24 20:40,15/Jan/24 09:42,,,,,,,,,,,,,,,,,0,pull-request-available,,,"Update migration tests in master to cover migration from new version. Since 1.18, this step could be done automatically with the following steps. For more information please refer to [this page.|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/README.md]
 # {*}On the published release tag (e.g., release-1.16.0){*}, run 
{panel}
{panel}
|{{$ mvn clean }}{{package}} {{{}-Pgenerate-migration-test-data -Dgenerate.version={}}}{{{}1.16{}}} {{-nsu -Dfast -DskipTests}}|

The version (1.16 in the command above) should be replaced with the target one.

 # Modify the content of the file [apache/flink:flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version|https://github.com/apache/flink/blob/master/flink-test-utils-parent/flink-migration-test-utils/src/main/resources/most_recently_published_version] to the latest version (it would be ""v1_16"" if sticking to the example where 1.16.0 was released). 
 # Commit the modification in step a and b with ""{_}[release] Generate reference data for state migration tests based on release-1.xx.0{_}"" to the corresponding release branch (e.g. {{release-1.16}} in our example), replace ""xx"" with the actual version (in this example ""16""). You should use the Jira issue ID in case of [release]  as the commit message's prefix if you have a dedicated Jira issue for this task.

 # Cherry-pick the commit to the master branch. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:25.0,,,,,,,,,,"0|z1m9n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Other announcements,FLINK-33848,13561872,13561867,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:52,19/Jan/24 16:56,04/Jun/24 20:40,19/Jan/24 16:56,,,,,,,,,,,,,,,,,0,,,,"h3. Recordkeeping

Use [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink] to seed the information about the release into future project reports.

(Note: Only PMC members have access report releases. If you do not have access, ask on the mailing list for assistance.)
h3. Flink blog

Example:

[https://flink.apache.org/2023/11/29/apache-flink-1.17.2-release-announcement/]

 
h3. Social media

Tweet, post on Facebook, LinkedIn, and other platforms. Ask other contributors to do the same.
h3. Flink Release Wiki page

Add a summary of things that went well or that went not so well during the release process. This can include feedback from contributors but also more generic things like the release have taken longer than initially anticipated (and why) to give a bit of context to the release process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:25.0,,,,,,,,,,"0|z1m9mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache mailing lists announcements,FLINK-33847,13561871,13561867,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:52,19/Jan/24 16:30,04/Jun/24 20:40,19/Jan/24 16:30,,,,,,,,,,,,,,,,,0,,,,"Announce on the {{dev@}} mailing list that the release has been finished.

Announce on the release on the {{user@}} mailing list, listing major improvements and contributions.

Announce the release on the [announce@apache.org|mailto:announce@apache.org] mailing list.
{panel}
{panel}
|{{From: Release Manager}}
{{To: dev@flink.apache.org, user@flink.apache.org, user-zh@flink.apache.org, announce@apache.org}}
{{Subject: [ANNOUNCE] Apache Flink 1.2.3 released}}
 
{{The Apache Flink community is very happy to announce the release of Apache Flink 1.2.3, which is the third bugfix release for the Apache Flink 1.2 series.}}
 
{{Apache Flink® is an open-source stream processing framework for distributed, high-performing, always-available, and accurate data streaming applications.}}
 
{{The release is available for download at:}}
{{[https://flink.apache.org/downloads.html]}}
 
{{Please check out the release blog post for an overview of the improvements for this bugfix release:}}
{{<blob post link>}}
 
{{The full release notes are available in Jira:}}
{{<jira release notes link>}}
 
{{We would like to thank all contributors of the Apache Flink community who made this release possible!}}
 
{{Feel free to reach out to the release managers (or respond to this thread) with feedback on the release process. Our goal is to constantly improve the release process. Feedback on what could be improved or things that didn't go so well are appreciated.}}
 
{{Regards,}}
{{Release Manager}}|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 19 16:30:40 UTC 2024,,,,,,,,,,"0|z1m9mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/24 16:30;jingge;https://lists.apache.org/thread/p5f4j11nylvhw15knvlbzjzynh13t65w;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove outdated versions (need PMC role),FLINK-33846,13561870,13561867,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,jingge,jingge,14/Dec/23 15:52,23/Jan/24 01:57,04/Jun/24 20:40,23/Jan/24 01:57,,,,,,,,,1.18.1,,,,,,,,0,,,,"h4. dist.apache.org

For a new bugfix version remove all release files for previous bugfix releases in the same series, e.g., when releasing 1.18.1, remove the 1.18.0 release.
 # If you have not already, check out the Flink section of the {{release}} repository on {{[dist.apache.org|http://dist.apache.org/]}} via Subversion. In a fresh directory:
{code:java}
svn checkout https://dist.apache.org/repos/dist/release/flink --depth=immediates
cd flink
{code}

 # Remove files for outdated releases and commit the changes.
{code:java}
svn remove flink-<version_to_remove>
svn commit
{code}

 # Verify that files  are [removed|https://dist.apache.org/repos/dist/release/flink]
(!) Remember to remove the corresponding download links from the website.

h4. CI

Disable the cron job for the now-unsupported version from (tools/azure-pipelines/[build-apache-repo.yml|https://github.com/apache/flink/blob/master/tools/azure-pipelines/build-apache-repo.yml]) in the respective branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 01:57:19 UTC 2024,,,,,,,,,,"0|z1m9mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 01:57;renqs;Flink 1.18.0 has been removed from SVN.

There's no need to adjust CI for patch version releases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge website pull request,FLINK-33845,13561869,13561867,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:52,19/Jan/24 16:29,04/Jun/24 20:40,19/Jan/24 16:29,,,,,,,,,,,,,,,,,0,pull-request-available,,,"Merge the website pull request to [list the release|http://flink.apache.org/downloads.html].

Make sure to regenerate the website as well, as it isn't build automatically:
 # install Hugo on the local machine
 # build the website
 # create a PR",,,,,,,,,,FLINK-33888,,FLINK-33842,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:24.0,,,,,,,,,,"0|z1m9m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update japicmp configuration,FLINK-33844,13561868,13561867,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:52,19/Jan/24 16:34,04/Jun/24 20:40,19/Jan/24 16:34,,,,,,,,,1.18.2,,,,,,,,0,pull-request-available,,,"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:24.0,,,,,,,,,,"0|z1m9m0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote release 1.18.1,FLINK-33843,13561867,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:52,25/Jan/24 23:08,04/Jun/24 20:40,25/Jan/24 23:08,1.18.0,,,,,,,,,,,,,,,,0,pull-request-available,,,"Once the release has been finalized, the last step of the process is to promote the release within the project and beyond. Please wait for 24h after finalizing the release in accordance with the [ASF release policy|http://www.apache.org/legal/release-policy.html#release-announcements].

*Final checklist to declare this issue resolved:*
 # Website pull request to [list the release|http://flink.apache.org/downloads.html] merged
 # Release announced on the user@ mailing list.
 # Blog post published, if applicable.
 # Release recorded in [reporter.apache.org|https://reporter.apache.org/addrelease.html?flink].
 # Release announced on social media.
 # Completion declared on the dev@ mailing list.
 # Update Homebrew: [https://docs.brew.sh/How-To-Open-a-Homebrew-Pull-Request] (seems to be done automatically - at least for minor releases  for both minor and major releases)
 # Updated the japicmp configuration
 ** corresponding SNAPSHOT branch japicmp reference version set to the just released version, and API compatibiltity checks for {{@PublicEvolving}}  was enabled
 ** (minor version release only) master branch japicmp reference version set to the just released version
 ** (minor version release only) master branch japicmp exclusions have been cleared
 # Update the list of previous version in {{docs/config.toml}} on the master branch.
 # Set {{show_outdated_warning: true}} in {{docs/config.toml}} in the branch of the _now deprecated_ Flink version (i.e. 1.16 if 1.18.0 is released)
 # Update stable and master alias in [https://github.com/apache/flink/blob/master/.github/workflows/docs.yml]
 # Open discussion thread for End of Life for Unsupported version (i.e. 1.16)",,,,,,,,,,FLINK-33838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:52:24.0,,,,,,,,,,"0|z1m9ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish the Dockerfiles for the new release,FLINK-33842,13561862,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:30,25/Jan/24 23:08,04/Jun/24 20:40,25/Jan/24 23:08,,,,,,,,,,,,,,,,,0,pull-request-available,,,"Note: the official Dockerfiles fetch the binary distribution of the target Flink version from an Apache mirror. After publishing the binary release artifacts, mirrors can take some hours to start serving the new artifacts, so you may want to wait to do this step until you are ready to continue with the ""Promote the release"" steps in the follow-up Jira.

Follow the [release instructions in the flink-docker repo|https://github.com/apache/flink-docker#release-workflow] to build the new Dockerfiles and send an updated manifest to Docker Hub so the new images are built and published.

Note: the flink-web PR has to be merged before the pull request to be opened on the [{{official-images}}|https://github.com/docker-library/official-images] repo. 
----
h3. Expectations
 * Dockerfiles in [flink-docker|https://github.com/apache/flink-docker] updated for the new Flink release and pull request opened on the Docker official-images with an updated manifest",,,,,,,,,,FLINK-33845,,,,,,,,,,,,,,,,,,,FLINK-34165,INFRA-25220,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 09:38:05 UTC 2024,,,,,,,,,,"0|z1m9kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 07:58;jingge;dev-1.18: df317cb99bdc28b9f0aafe8616aa075a07499fbf

master: 627987997ca7ec86bcc3d80b26df58aa595b91af;;;","23/Jan/24 09:38;jingge;official-image PR: https://github.com/docker-library/official-images/pull/16114 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Git tag and check no new Jira tickets have 1.18.1 as the Fix version after the last release candidate was tagged,FLINK-33841,13561861,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:30,16/Jan/24 17:43,04/Jun/24 20:40,16/Jan/24 17:43,,,,,,,,,,,,,,,,,0,,,,"Create and push a new Git tag for the released version by copying the tag for the final release candidate, as follows:
{code:java}
$ git tag -s ""release-${RELEASE_VERSION}"" refs/tags/${TAG}^{} -m ""Release Flink ${RELEASE_VERSION}""
$ git push <remote> refs/tags/release-${RELEASE_VERSION}
{code}
 

If PRs have been merged to the release branch after the last release candidate was tagged, make sure that the corresponding Jira tickets have the correct Fix Version set.

 
----
h3. Expectations
 * Release tagged in the source code repository",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:30:57.0,,,,,,,,,,"0|z1m9ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy artifacts to Maven Central Repository (need PMC role),FLINK-33840,13561860,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,renqs,jingge,jingge,14/Dec/23 15:30,16/Jan/24 13:00,04/Jun/24 20:40,16/Jan/24 08:51,,,,,,,,,1.18.1,,,,,,,,0,,,,"Use the [Apache Nexus repository|https://repository.apache.org/] to release the staged binary artifacts to the Maven Central repository. In the Staging Repositories section, find the relevant release candidate orgapacheflink-XXX entry and click Release. Drop all other release candidates that are not being released.
h3. Deploy source and binary releases to dist.apache.org

Copy the source and binary releases from the dev repository to the release repository at [dist.apache.org|http://dist.apache.org/] using Subversion.
{code:java}
$ svn move -m ""Release Flink ${RELEASE_VERSION}"" https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM} https://dist.apache.org/repos/dist/release/flink/flink-${RELEASE_VERSION}
{code}
(Note: Only PMC members have access to the release repository. If you do not have access, ask on the mailing list for assistance.)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 16 13:00:02 UTC 2024,,,,,,,,,,"0|z1m9kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/24 08:50;renqs;Binary artifacts has been released on Maven central: [https://repo1.maven.org/maven2/org/apache/flink/flink-core/1.18.1/]

Sources and binaries has been released: [https://dist.apache.org/repos/dist/release/flink/flink-1.18.1/] ;;;","16/Jan/24 13:00;jingge;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deploy Python artifacts to PyPI,FLINK-33839,13561859,13561858,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:30,15/Jan/24 16:46,04/Jun/24 20:40,15/Jan/24 16:46,,,,,,,,,,,,,,,,,0,,,,"Release manager should create a PyPI account and ask the PMC add this account to pyflink collaborator list with Maintainer role (The PyPI admin account info can be found here. NOTE, only visible to PMC members) to deploy the Python artifacts to PyPI. The artifacts could be uploaded using twine([https://pypi.org/project/twine/]). To install twine, just run:
{code:java}
pip install --upgrade twine==1.12.0
{code}
Please note that Username/Password authentication is no longer supported. Please use API Tokens [https://pypi.org/help/#apitoken]

Download the python artifacts from dist.apache.org and upload it to pypi.org:
{code:java}
svn checkout https://dist.apache.org/repos/dist/dev/flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
cd flink-${RELEASE_VERSION}-rc${RC_NUM}
 
cd python
 
#uploads wheels
for f in *.whl; do twine upload --repository-url https://upload.pypi.org/legacy/ $f $f.asc; done
 
#upload source packages
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-libraries-${RELEASE_VERSION}.tar.gz apache-flink-libraries-${RELEASE_VERSION}.tar.gz.asc
 
twine upload --repository-url https://upload.pypi.org/legacy/ apache-flink-${RELEASE_VERSION}.tar.gz apache-flink-${RELEASE_VERSION}.tar.gz.asc
{code}
If upload failed or incorrect for some reason (e.g. network transmission problem), you need to delete the uploaded release package of the same version (if exists) and rename the artifact to {{{}apache-flink-${RELEASE_VERSION}.post0.tar.gz{}}}, then re-upload.

(!) Note: re-uploading to pypi.org must be avoided as much as possible because it will cause some irreparable problems. If that happens, users cannot install the apache-flink package by explicitly specifying the package version, i.e. the following command ""pip install apache-flink==${RELEASE_VERSION}"" will fail. Instead they have to run ""pip install apache-flink"" or ""pip install apache-flink==${RELEASE_VERSION}.post0"" to install the apache-flink package.

 
----
h3. Expectations
 * Python artifacts released and indexed in the [PyPI|https://pypi.org/project/apache-flink/] Repository https://pypi.org/project/apache-flink/#history",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:30:57.0,,,,,,,,,,"0|z1m9k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Finalize release 1.18.1,FLINK-33838,13561858,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:30,25/Jan/24 23:08,04/Jun/24 20:40,25/Jan/24 23:08,1.18.0,,,,,,,,,,,,,,,,0,,,,"Once the release candidate has been reviewed and approved by the community, the release should be finalized. This involves the final deployment of the release candidate to the release repositories, merging of the website changes, etc.",,,,,,,,,,FLINK-33833,FLINK-33885,FLINK-33843,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:30:57.0,,,,,,,,,,"0|z1m9k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vote on the release candidate,FLINK-33837,13561857,13561853,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,14/Dec/23 15:20,19/Dec/23 23:16,04/Jun/24 20:40,19/Dec/23 23:16,1.18.0,,,,,,,,,,,,,,,,0,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:20:51.0,,,,,,,,,,"0|z1m9js:",9223372036854775807,canceled,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propose a pull request for website updates,FLINK-33836,13561856,13561853,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,14/Dec/23 15:20,19/Dec/23 23:15,04/Jun/24 20:40,19/Dec/23 23:15,1.18.0,,,,,,,,,,,,,,,,0,pull-request-available,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 * update docs/data/flink.yml

 ** Add a new major version or update minor version as required
 * update docs/data/release_archive.yml
 * update version references in quickstarts ({{{}q/{}}} directory) as required (outdated?)
 * add a blog post announcing the release in {{docs/content/posts}}

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:20:51.0,,,,,,,,,,"0|z1m9jk:",9223372036854775807,canceled,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stage source and binary releases on dist.apache.org,FLINK-33835,13561855,13561853,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:20,14/Dec/23 21:55,04/Jun/24 20:40,14/Dec/23 21:55,,,,,,,,,,,,,,,,,0,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:20:51.0,,,,,,,,,,"0|z1m9jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build and stage Java and Python artifacts,FLINK-33834,13561854,13561853,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:20,14/Dec/23 21:55,04/Jun/24 20:40,14/Dec/23 21:55,,,,,,,,,,,,,,,,,0,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:20:51.0,,,,,,,,,,"0|z1m9j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Release Candidate: 1.18.1-rc1,FLINK-33833,13561853,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 15:20,19/Dec/23 23:15,04/Jun/24 20:40,19/Dec/23 23:15,1.18.0,,,,,,,,,,,,,,,,0,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.

h4. Prerequisites
Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code}
RC_NUM=""1""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,FLINK-33824,,FLINK-33838,,,,FLINK-33885,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 15:20:51.0,,,,,,,,,,"0|z1m9iw:",9223372036854775807,canceled,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify that no exclusions were erroneously added to the japicmp plugin,FLINK-33832,13561833,13561825,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,14/Dec/23 11:47,14/Jan/24 13:06,04/Jun/24 20:40,14/Jan/24 13:06,,,,,,,,,,,,,,,,,0,,,,"Verify that no exclusions were erroneously added to the japicmp plugin that break compatibility guarantees. Check the exclusions for the japicmp-maven-plugin in the root pom (see [apache/flink:pom.xml:2175ff|https://github.com/apache/flink/blob/3856c49af77601cf7943a5072d8c932279ce46b4/pom.xml#L2175] for exclusions that:
* For minor releases: break source compatibility for {{@Public}} APIs
* For patch releases: break source/binary compatibility for {{@Public}}/{{@PublicEvolving}}  APIs
Any such exclusion must be properly justified, in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:58.0,,,,,,,,,,"0|z1m9eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkout the release branch,FLINK-33831,13561832,13561825,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 11:47,14/Dec/23 16:18,04/Jun/24 20:40,14/Dec/23 16:18,1.18.0,,,,,,,,,,,,,,,,0,,,,"If you are doing a new minor release, you need to update Flink version in the following repositories and the [AzureCI project configuration|https://dev.azure.com/apache-flink/apache-flink/]:
 * [apache/flink|https://github.com/apache/flink]
 * [apache/flink-docker|https://github.com/apache/flink-docker]
 * [apache/flink-benchmarks|https://github.com/apache/flink-benchmarks]

Patch releases don't require the these repositories to be touched. Simply checkout the already existing branch for that version:
{code:java}
$ git checkout release-$SHORT_RELEASE_VERSION
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:58.0,,,,,,,,,,"0|z1m9e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select executing Release Manager,FLINK-33830,13561831,13561825,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 11:47,14/Dec/23 16:18,04/Jun/24 20:40,14/Dec/23 16:18,1.18.0,,,,,,,,,,,,Release System,,,,0,,,,"h4. GPG Key

You need to have a GPG key to sign the release artifacts. Please be aware of the ASF-wide [release signing guidelines|https://www.apache.org/dev/release-signing.html]. If you don’t have a GPG key associated with your Apache account, please create one according to the guidelines.

Determine your Apache GPG Key and Key ID, as follows:
{code:java}
$ gpg --list-keys
{code}
This will list your GPG keys. One of these should reflect your Apache account, for example:
{code:java}
--------------------------------------------------
pub   2048R/845E6689 2016-02-23
uid                  Nomen Nescio <anonymous@apache.org>
sub   2048R/BA4D50BE 2016-02-23
{code}
In the example above, the key ID is the 8-digit hex string in the {{pub}} line: {{{}845E6689{}}}.

Now, add your Apache GPG key to the Flink’s {{KEYS}} file in the [Apache Flink release KEYS file|https://dist.apache.org/repos/dist/release/flink/KEYS] repository at [dist.apache.org|http://dist.apache.org/]. Follow the instructions listed at the top of these files. (Note: Only PMC members have write access to the release repository. If you end up getting 403 errors ask on the mailing list for assistance.)

Configure {{git}} to use this key when signing code by giving it your key ID, as follows:
{code:java}
$ git config --global user.signingkey 845E6689
{code}
You may drop the {{--global}} option if you’d prefer to use this key for the current repository only.

You may wish to start {{gpg-agent}} to unlock your GPG key only once using your passphrase. Otherwise, you may need to enter this passphrase hundreds of times. The setup for {{gpg-agent}} varies based on operating system, but may be something like this:
{code:bash}
$ eval $(gpg-agent --daemon --no-grab --write-env-file $HOME/.gpg-agent-info)
$ export GPG_TTY=$(tty)
$ export GPG_AGENT_INFO
{code}
h4. Access to Apache Nexus repository

Configure access to the [Apache Nexus repository|https://repository.apache.org/], which enables final deployment of releases to the Maven Central Repository.
 # You log in with your Apache account.
 # Confirm you have appropriate access by finding {{org.apache.flink}} under {{{}Staging Profiles{}}}.
 # Navigate to your {{Profile}} (top right drop-down menu of the page).
 # Choose {{User Token}} from the dropdown, then click {{{}Access User Token{}}}. Copy a snippet of the Maven XML configuration block.
 # Insert this snippet twice into your global Maven {{settings.xml}} file, typically {{{}${HOME}/.m2/settings.xml{}}}. The end result should look like this, where {{TOKEN_NAME}} and {{TOKEN_PASSWORD}} are your secret tokens:
{code:xml}
<settings>
   <servers>
     <server>
       <id>apache.releases.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
     <server>
       <id>apache.snapshots.https</id>
       <username>TOKEN_NAME</username>
       <password>TOKEN_PASSWORD</password>
     </server>
   </servers>
 </settings>
{code}

h4. Website development setup

Get ready for updating the Flink website by following the [website development instructions|https://flink.apache.org/contributing/improve-website.html].
h4. GNU Tar Setup for Mac (Skip this step if you are not using a Mac)

The default tar application on Mac does not support GNU archive format and defaults to Pax. This bloats the archive with unnecessary metadata that can result in additional files when decompressing (see [1.15.2-RC2 vote thread|https://lists.apache.org/thread/mzbgsb7y9vdp9bs00gsgscsjv2ygy58q]). Install gnu-tar and create a symbolic link to use in preference of the default tar program.
{code:bash}
$ brew install gnu-tar
$ ln -s /usr/local/bin/gtar /usr/local/bin/tar
$ which tar
{code}
 
----
h3. Expectations
 * Release Manager’s GPG key is published to [dist.apache.org|http://dist.apache.org/]
 * Release Manager’s GPG key is configured in git configuration
 * Release Manager's GPG key is configured as the default gpg key.
 * Release Manager has {{org.apache.flink}} listed under Staging Profiles in Nexus
 * Release Manager’s Nexus User Token is configured in settings.xml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:57.0,,,,,,,,,,"0|z1m9e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review Release Notes in JIRA,FLINK-33829,13561830,13561825,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,14/Dec/23 11:47,14/Jan/24 12:57,04/Jun/24 20:40,14/Jan/24 12:57,,,,,,,,,,,,,,,,,0,,,,https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&version=12353640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:57.0,,,,,,,,,,"0|z1m9ds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No need for patch release - Cross team testing,FLINK-33828,13561829,13561825,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,14/Dec/23 11:47,14/Dec/23 15:16,04/Jun/24 20:40,14/Dec/23 15:05,,,,,,,,,,,,,,,,,0,,,,"For user facing features that go into the release we'd like to ensure they can actually _be used_ by Flink users. To achieve this the release managers ensure that an issue for cross team testing is created in the Apache Flink Jira. This can and should be picked up by other community members to verify the functionality and usability of the feature.
The issue should contain some entry points which enables other community members to test it. It should not contain documentation on how to use the feature as this should be part of the actual documentation. The cross team tests are performed after the feature freeze. Documentation should be in place before that. Those tests are manual tests, so do not confuse them with automated tests.
To sum that up:
 * User facing features should be tested by other contributors
 * The scope is usability and sanity of the feature
 * The feature needs to be already documented
 * The contributor creates an issue containing some pointers on how to get started (e.g. link to the documentation, suggested targets of verification)
 * Other community members pick those issues up and provide feedback
 * Cross team testing happens right after the feature freeze

 
----
h3. Expectations
 * Jira issues for each expected release task according to the release plan is created and labeled as {{{}release-testing{}}}.
 * All the created release-testing-related Jira issues are resolved and the corresponding blocker issues are fixed.",,,,,,,,,,,,,,,FLINK-30926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:57.0,,,,,,,,,,"0|z1m9dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No need for patch release - Review and update documentation,FLINK-33827,13561828,13561825,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,jingge,jingge,jingge,14/Dec/23 11:47,14/Dec/23 16:17,04/Jun/24 20:40,14/Dec/23 16:17,1.17.0,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:57.0,,,,,,,,,,"0|z1m9dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triage release-blocking issues in JIRA,FLINK-33826,13561827,13561825,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 11:47,14/Jan/24 12:57,04/Jun/24 20:40,14/Jan/24 12:57,,,,,,,,,,,,,,,,,0,,,,"https://issues.apache.org/jira/projects/FLINK/versions/12353640

There could be outstanding release-blocking issues, which should be triaged before proceeding to build a release candidate. We track them by assigning a specific Fix version field even before the issue resolved.

The list of release-blocking issues is available at the version status page. Triage each unresolved issue with one of the following resolutions:
 * If the issue has been resolved and JIRA was not updated, resolve it accordingly.
 * If the issue has not been resolved and it is acceptable to defer this until the next release, update the Fix Version field to the new version you just created. Please consider discussing this with stakeholders and the dev@ mailing list, as appropriate.
 ** When using ""Bulk Change"" functionality of Jira
 *** First, add the newly created version to Fix Version for all unresolved tickets that have old the old version among its Fix Versions.
 *** Afterwards, remove the old version from the Fix Version.
 * If the issue has not been resolved and it is not acceptable to release until it is fixed, the release cannot proceed. Instead, work with the Flink community to resolve the issue.

 
----
h3. Expectations
 * There are no release blocking JIRA issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:57.0,,,,,,,,,,"0|z1m9d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a new version in JIRA (need PMC role),FLINK-33825,13561826,13561825,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,leonard,jingge,jingge,14/Dec/23 11:47,14/Jan/24 13:17,04/Jun/24 20:40,15/Dec/23 10:03,1.18.1,,,,,,,,1.18.1,,,,,,,,0,,,,"When contributors resolve an issue in JIRA, they are tagging it with a release that will contain their changes. With the release currently underway, new issues should be resolved against a subsequent future release. Therefore, you should create a release item for this subsequent release, as follows:
 # In JIRA, navigate to the [Flink > Administration > Versions|https://issues.apache.org/jira/plugins/servlet/project-config/FLINK/versions].
 # Add a new release: choose the next minor version number compared to the one currently underway, select today’s date as the Start Date, and choose Add.
(Note: Only PMC members have access to the project administration. If you do not have access, ask on the mailing list for assistance.)

 
----
h3. Expectations
 * The new version should be listed in the dropdown menu of {{fixVersion}} or {{affectedVersion}} under ""unreleased versions"" when creating a new Jira issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 23:06:23 UTC 2023,,,,,,,,,,"0|z1m9cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 10:03;leonard;1.18.2 version created https://issues.apache.org/jira/projects/FLINK/versions/12354053;;;","15/Dec/23 23:06;jingge;thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepare Flink 1.18.1 Release,FLINK-33824,13561825,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,14/Dec/23 11:47,22/Jan/24 20:06,04/Jun/24 20:40,22/Jan/24 20:06,1.18.0,,,,,,,,,,,,Release System,,,,0,,,,"This umbrella issue is meant as a test balloon for moving the [release documentation|https://cwiki.apache.org/confluence/display/FLINK/Creating+a+Flink+Release] into Jira.
h3. Prerequisites
h4. Environment Variables

Commands in the subtasks might expect some of the following enviroment variables to be set accordingly to the version that is about to be released:
{code:bash}
RELEASE_VERSION=""1.18.1""
SHORT_RELEASE_VERSION=""1.18""
CURRENT_SNAPSHOT_VERSION=""$SHORT_RELEASE_VERSION-SNAPSHOT""
{code}
h4. Build Tools

All of the following steps require to use Maven 3.2.5 and Java 8. Modify your PATH environment variable accordingly if needed.
h4. Flink Source
 * Create a new directory for this release and clone the Flink repository from Github to ensure you have a clean workspace (this step is optional).
 * Run {{mvn -Prelease clean install}} to ensure that the build processes that are specific to that profile are in good shape (this step is optional).

The rest of this instructions assumes that commands are run in the root (or {{./tools}} directory) of a repository on the branch of the release version with the above environment variables set.",,,,,,,,,,,,FLINK-33833,FLINK-33885,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 11:47:56.0,,,,,,,,,,"0|z1m9co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialize PlannerQueryOperation into SQL,FLINK-33823,13561822,13559333,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,14/Dec/23 11:25,20/Dec/23 15:18,04/Jun/24 20:40,20/Dec/23 15:18,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 15:18:40 UTC 2023,,,,,,,,,,"0|z1m9c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Dec/23 15:18;dwysakowicz;Implemented in 5919251d7a94264a6a72c31de0716b3f72d65437;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Slack Invite URL into config.toml,FLINK-33822,13561821,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,14/Dec/23 11:22,14/Dec/23 12:05,04/Jun/24 20:40,14/Dec/23 12:05,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,,,"Instead of 4 locations, we want to update only one location with the invite link. Additionally, we should add documentation on how to update the link.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 12:05:14 UTC 2023,,,,,,,,,,"0|z1m9bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 12:05;mapohl;asf-site: 580291a480fed43d1dc7a0c54dd3a4e4e41a6645;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrowSerializer$finishCurrentBatch consumes too much time,FLINK-33821,13561807,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wenchi,wenchi,14/Dec/23 09:31,14/Dec/23 09:31,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,API / Python,,,,0,,,,"We convert the data into arrow format through flink and send it to doris.
Data convertion likes this: RowData --> arrow --> doris.
But during testing, we found that the `ArrowSerializer` provided by flink-python consumes a lot of time in the `finishCurrentBatch` function.
A total of 1.4G parquet files, the overall conversion time is 70 seconds, but `finishCurrentBatch` takes a total of 40 seconds(especially `writeBatch` cost 39 seconds in `finishCurrentBatch`).

 

So, we compare with spark, data convertion likes this: InternalRow --> arrow --> doris.
Using the same parquet file, the overall conversion time only takes 35 seconds, and `writeBatch` only cost 10 seconds.

 

In spark, we use `org.apache.spark.sql.execution.arrow.ArrowWriter` to convert `InternalRow` into arrowVector, and then serialize arrowVector into binary through `org.apache.arrow.vector.ipc.ArrowStreamWriter$writeBatch`.
Simple code like this:

 
{code:java}
            ArrowWriter arrowWriter = ArrowWriter.create(vectorSchemaRoot);

            // --- phase1: InternalRow to arrowVector
            while (....) {
                arrowWriter.write(iterator.next());
            }
            arrowWriter.finish();

            // --- phase2: arrowVector to binary
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            ArrowStreamWriter writer = new ArrowStreamWriter(vectorSchemaRoot, null, out);
            writer.writeBatch();
            writer.end();

            // --- phase3: get binary
            out.toByteArray(); {code}
 

In flink, we use `org.apache.flink.table.runtime.arrow.serializers.ArrowSerializer`. This class is very useful, not only includes the conversion of RowData to arrowVector, but also the serialization of arrowVector to binary.
Simple code like this:


{code:java}
            arrowSerializer = new ArrowSerializer(rowType, rowType);
            outputStream = new ByteArrayOutputStream();
            arrowSerializer.open(new ByteArrayInputStream(new byte[0]), outputStream);

            // --- phase1: RowData to arrowVector
            while(....) {
                arrowSerializer.write(rowData);
            }

            // --- phase2: arrowVector to binary
            arrowSerializer.finishCurrentBatch();

            // --- phase3: get binary
            outputStream.toByteArray();
            outputStream.reset(); {code}
In phase 1 and phase 3, the time of flink and spark is basically the same. In phase 2, spark's writeBatch function took 10 seconds, but the writeBatch function in flink's finishCurrentBatch took 40 seconds.

Is there any flink related configuration that I am missing? Or, did I use it wrong somewhere in flink?

 

Looking forward to your reply! Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 09:31:59.0,,,,,,,,,,"0|z1m98o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingWithStateTestBase's subclasses might fail during cleanup if sharing the state directory (through a common @TempDir),FLINK-33820,13561803,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,14/Dec/23 08:56,14/Dec/23 08:56,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"FLINK-33641 revealed an issue where StreamingWithStateTestBase implementing test classes failed during cleanup due to some concurrency that's not fully understood, yet (see [FLINK-33641 PR|https://github.com/apache/flink/pull/23914] discussion for more details).

FLINK-33641 provided a workaround for now that creates per-class temporary folders. This Jira is about investigating why this of test failures actually exists.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33641,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-14 08:56:38.0,,,,,,,,,,"0|z1m97s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support setting CompressType in RocksDBStateBackend,FLINK-33819,13561772,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mayuehappy,mayuehappy,mayuehappy,14/Dec/23 03:34,11/Mar/24 12:43,04/Jun/24 20:40,,1.18.0,,,,,,,,1.20.0,,,,Runtime / State Backends,,,,1,pull-request-available,,,"Currently, RocksDBStateBackend does not support setting the compression level, and Snappy is used for compression by default. But we have some scenarios where compression will use a lot of CPU resources. Turning off compression can significantly reduce CPU overhead. So we may need to support a parameter for users to set the CompressType of Rocksdb.

  !image-2023-12-14-11-35-22-306.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20684,FLINK-11313,,,,,,,,"14/Dec/23 03:32;mayuehappy;image-2023-12-14-11-32-32-968.png;https://issues.apache.org/jira/secure/attachment/13065303/image-2023-12-14-11-32-32-968.png","14/Dec/23 03:35;mayuehappy;image-2023-12-14-11-35-22-306.png;https://issues.apache.org/jira/secure/attachment/13065304/image-2023-12-14-11-35-22-306.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 15:52:43 UTC 2024,,,,,,,,,,"0|z1m90w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 02:20;xiangyu0xf;+1 for this. This will save lots of cpu usage for jobs with less state space usage but high state access frequency.;;;","15/Dec/23 03:59;masteryhx;Linked FLINK-20684 as it has been discussed before.

Linked FLINK-11313 which talks about the LZ4 Compression which should be more usable than Snappy.

IMO, it should improve performane if we disbale the compression of L0/L1 in some scenes.

[~mayuehappy] Do you have some test results on it ?

BTW, If we'd like to introduce such a option, it's  better to guarantee the compalibility.;;;","18/Dec/23 09:30;mayuehappy;[~masteryhx] Thanks for replying.
{quote}Linked FLINK-20684 as it has been discussed before.
{quote}
Sorry for missing the previous ticket and creating the duplicated one
{quote}Linked FLINK-11313 which talks about the LZ4 Compression which should be more usable than Snappy.
{quote}
We did not use LZ4 in the production environment, and for small state jobs, we directly turn off the compression. And for job with large states,  we adopted a compression algorithm based on Snappy optimization.
{quote}Do you have some test results on it ?
{quote}
Yes. we did some benchmark test for the *SnappyCompression* and *NoCompression*
And result show that and Read Performance. After turning off Compression, State Benchmark read performance can be improved by *80% to 100%*

We also conducted end-to-end online job testing, and after turning off Compression, {*}the CPU usage of the job decreased by 16%{*}, while the Checkpoint Total Size increased by *4-5 times.*

It is obvious that closing Compression is not only about benefits, but also brings some space amplification. What I want to express is that we may need to provide such a configuration for users to balance how to exchange space for time
 
{quote}BTW, If we'd like to introduce such a option, it's  better to guarantee the compalibility.
{quote}
Sorry, I didn't understand the compatibility issue here. I understand that it is compatible here. After switching the Compression Type, the newly generated file will be compressed using the new Compress Type, and the existing file can still be read and written with old Compress Type. ;;;","10/Jan/24 07:38;mayuehappy;[~masteryhx]  [~pnowojski] [~srichter]  could you please take a look at this ticket ? ;;;","10/Jan/24 13:55;masteryhx;{quote}It is obvious that closing Compression is not only about benefits, but also brings some space amplification. What I want to express is that we may need to provide such a configuration for users to balance how to exchange space for time
{quote}
Yeah, that's also what we found in the production environment which could improve the performance when we use NoCompression for L0 and L1 at the expense of space.

So I'm +1 to introduce such a configuration. Of course, we should remain the default behavious at the first version.
{quote}After switching the Compression Type, the newly generated file will be compressed using the new Compress Type, and the existing file can still be read and written with old Compress Type. 
{quote}
Yes, you're right. I have verified this. RocksDB will handle this situation.

 

[~mayuehappy] Would you like to take this ?

 ;;;","11/Jan/24 03:07;mayuehappy;[~masteryhx]  thanks , I would like to take this and  I'll draft the pr soon;;;","15/Jan/24 13:00;pnowojski;Thanks for drafting the proposal [~mayuehappy]

+1 for making this option configurable. 

It would be great if someone could test out [~masteryhx]'s suggestion. Configuring no compression for L0 and L1 levels, while keeping lower levels compressed:
* By how much the checkpoint  would grow?
* What about performance?

It sounds like this actually could be the default behaviour for us? Would be great to test it out, but not a blocker from my side for implementing this ticket.;;;","22/Jan/24 12:37;mayuehappy;[~pnowojski] [~masteryhx] 

I conducted benchmark tests on Snappy and No_Compression. In the Value State test, I increased the number of setupKeyCounts to 1000 because the default value state test is too small, most of which may be in the memtable. The conclusion is as follows:

Point LookUp operations can improve performance by over *130%+*

Scan type operations can improve performance by *60%*

The performance of write operations has no impact with before

The size of valueState has increased from *78M to 96M* after disabling compression

The size of mapState has increased from *58m to 86m* (Perhaps this data is for reference only, as the actual compression ratio depends on the characteristics of the business data)


|Benchmark|(backendType)|Mode|Score (Snappy)|Score（NoCompression）|Units|performance benfit|
|MapStateBenchmark.mapAdd|ROCKSDB|thrpt|654.362|679.276|ops/ms|{color:#FF0000}3.80737267750878%{color}|
|MapStateBenchmark.mapContains|ROCKSDB|thrpt|104.57|297.213|ops/ms|{color:#FF0000}184.223964808262%{color}|
|MapStateBenchmark.mapEntries|ROCKSDB|thrpt|573.153|933.967|ops/ms|{color:#FF0000}62.9524751680616%{color}|
|MapStateBenchmark.mapGet|ROCKSDB|thrpt|106.288|330.821|ops/ms|{color:#FF0000}211.249623664007%{color}|
|MapStateBenchmark.mapIsEmpty|ROCKSDB|thrpt|88.642|207.76|ops/ms|{color:#FF0000}134.380993208637%{color}|
|MapStateBenchmark.mapIterator|ROCKSDB|thrpt|572.848|912.097|ops/ms|{color:#FF0000}59.2214688713236%{color}|
|MapStateBenchmark.mapKeys|ROCKSDB|thrpt|580.244|949.094|ops/ms|{color:#FF0000}63.568085150385%{color}|
|MapStateBenchmark.mapPutAll|ROCKSDB|thrpt|129.965|130.054|ops/ms|{color:#FF0000}0.0684799753779853%{color}|
|MapStateBenchmark.mapRemove|ROCKSDB|thrpt|723.835|785.637|ops/ms|{color:#FF0000}8.53813369068916%{color}|
|MapStateBenchmark.mapUpdate|ROCKSDB|thrpt|697.409|652.893|ops/ms|{color:#FF0000}-6.38305499355471%{color}|
|MapStateBenchmark.mapValues|ROCKSDB|thrpt|579.399|935.651|ops/ms|{color:#FF0000}61.4864713263226%{color}|
|ValueStateBenchmark.valueAdd|ROCKSDB|thrpt|645.081|636.098|ops/ms|{color:#FF0000}-1.39253830139162%{color}|
|ValueStateBenchmark.valueGet|ROCKSDB|thrpt|103.393|297.646|ops/ms|{color:#FF0000}187.87828963276%{color}|
|ValueStateBenchmark.valueUpdate|ROCKSDB|thrpt|560.153|621.502|ops/ms|{color:#FF0000}10.9521862776777%{color}|
| | | | | | | |
| | | | | | | |
| |DBSize （SnappyCompression）|DBSize （NoCompression）| | | | |
|ValueStateBenchMark|78M|96M| | | | |
|MapStateBenchMark|58M|86M| | | | |;;;","25/Jan/24 06:00;masteryhx;merged 4f7725aa into master which is not blocked by the performance result.

[~mayuehappy] Thanks for the test!

What's the state size and memory configuration in your test ? Could we also test when state is larger than L2/L3 ?

I also think it's worthy discussing and we could find a better default behavious in the next version.;;;","02/Feb/24 15:52;pnowojski;Thanks [~mayuehappy].

{quote}
merged 4f7725aa into master which is not blocked by the performance result.

I also think it's worthy discussing and we could find a better default behavious in the next version.
{quote}

+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for WindowDeduplicate node,FLINK-33818,13561739,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,13/Dec/23 22:49,18/Dec/23 11:45,04/Jun/24 20:40,18/Dec/23 11:45,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 11:45:29 UTC 2023,,,,,,,,,,"0|z1m8tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/23 11:45;dwysakowicz;Implemented in 5799d8d06220608b52f5748549882966fe5b1ae3..011f777036540d0f027b04306714bf9e64003a97;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow ReadDefaultValues = False for non primitive types on Proto3,FLINK-33817,13561735,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dsaisharath,dsaisharath,dsaisharath,13/Dec/23 22:06,26/Feb/24 04:58,04/Jun/24 20:40,26/Feb/24 04:58,1.18.0,,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,1,pull-request-available,,,"*Background*

 

The current Protobuf format [implementation|https://github.com/apache/flink/blob/c3e2d163a637dca5f49522721109161bd7ebb723/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/deserialize/ProtoToRowConverter.java] always sets ReadDefaultValues=False when using Proto3 version. This can cause severe performance degradation for large Protobuf schemas with OneOf fields as the entire generated code needs to be executed during deserialization even when certain fields are not present in the data to be deserialized and all the subsequent nested Fields can be skipped. Proto3 supports hasXXX() methods for checking field presence for non primitive types since Proto version [3.15|https://github.com/protocolbuffers/protobuf/releases/tag/v3.15.0]. In the internal performance benchmarks in our company, we've seen almost 10x difference in performance for one of our real production usecase when allowing to set ReadDefaultValues=False with proto3 version. The exact difference in performance depends on the schema complexity and data payload but we should allow user to set readDefaultValue=False in general.

 

*Solution*

 

Support using ReadDefaultValues=False when using Proto3 version. We need to be careful to check for field presence only on non-primitive types if ReadDefaultValues is false and version used is Proto3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 26 04:58:10 UTC 2024,,,,,,,,,,"0|z1m8so:",9223372036854775807,"Protobuf format now supports set 'protobuf.read-default-values' = 'false' for non-primitive types on pb3. Since the default value for 'protobuf.read-default-values' is 'false', this is a breaking change, if you are using pb3, you can set 'protobuf.read-default-values' = 'true' to keep the original behavior.",,,,,,,,,,,,,,,,,,,"21/Dec/23 21:26;dsaisharath;[~libenchao] , Can you please take a look at this ticket and assign it to me? ;;;","22/Dec/23 09:56;libenchao;[~dsaisharath]  Good catch, I read the release note of ProtoBuf 3.15, it says that oneof fields support ""hasXXX"" methods, does common message fields have ""hasXXX"" too, how do you distinguish them?;;;","22/Dec/23 22:01;dsaisharath;Yes, it is supported for message fields as well (although I'm not exactly sure which version). See this [documentation|https://protobuf.dev/programming-guides/field_presence/#how-to-enable-explicit-presence-in-proto3] for more details. In fact, the hasXXX() method is supported for Primitive types also if defined as optional.

 

We can distinguish the primitive types using {{PbFormatUtils.isSimpleType()}} [method|https://github.com/apache/flink/blob/master/flink-formats/flink-protobuf/src/main/java/org/apache/flink/formats/protobuf/util/PbFormatUtils.java]. Also, We can add user guidance to only enable this setting when using Protobuf version newer than 3.15. The performance difference should also be called out explicitly in the documentation so that the user can make an informed decision. ;;;","25/Dec/23 11:48;libenchao;[~maosuhan] What do you think of this? I have an impression that this is a limitation of proto3, so it's not any more in the later versions of proto3?;;;","05/Jan/24 03:05;dsaisharath;[~maosuhan] Gentle ping on this.;;;","29/Jan/24 20:28;dsaisharath;[~libenchao] [~maosuhan] , Gentle ping on this ticket as there is a very high performance impact here for the Protobuf format.;;;","13/Feb/24 13:52;nathantalewis;I can confirm that this issue affects Flink version 1.17.x as well.;;;","21/Feb/24 05:10;libenchao;According to what [~dsaisharath] provides, I tend to include this improvement to the master, I'll review the PR shortly, hope to get it in in 1.20.0

[~nathantalewis] This is an improvement instead of bugfix, so it will only be merged to the latest development version, and do not apply to bugfix versions such as 1.17.x, 1.18.x, does this sounds good to you?;;;","21/Feb/24 13:23;nathantalewis;[~libenchao], yes that should work. We are currently using a fork with this fix cherry picked in, so we can stay on that until the latest development version goes stable. (y);;;","26/Feb/24 04:58;libenchao;Fixed in master: bf75870c4744c884860ee72bf464a301d18fb477 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceStreamTaskTest.testTriggeringStopWithSavepointWithDrain failed due async checkpoint triggering not being completed ,FLINK-33816,13561652,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,mapohl,mapohl,13/Dec/23 10:25,02/Apr/24 08:03,04/Jun/24 20:40,15/Mar/24 13:33,1.19.0,,,,,,,,1.19.1,1.20.0,,,Runtime / Checkpointing,Runtime / Coordination,,,0,github-actions,pull-request-available,test-stability,"[https://github.com/XComp/flink/actions/runs/7182604625/job/19559947894#step:12:9430]
{code:java}
rror: 14:39:01 14:39:01.930 [ERROR] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.878 s <<< FAILURE! - in org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest
9426Error: 14:39:01 14:39:01.930 [ERROR] org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.testTriggeringStopWithSavepointWithDrain  Time elapsed: 0.034 s  <<< FAILURE!
9427Dec 12 14:39:01 org.opentest4j.AssertionFailedError: 
9428Dec 12 14:39:01 
9429Dec 12 14:39:01 Expecting value to be true but was false
9430Dec 12 14:39:01 	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
9431Dec 12 14:39:01 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
9432Dec 12 14:39:01 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTaskTest.testTriggeringStopWithSavepointWithDrain(SourceStreamTaskTest.java:710)
9433Dec 12 14:39:01 	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
9434Dec 12 14:39:01 	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/24 05:20;jiabao.sun;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13065696/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 08:03:18 UTC 2024,,,,,,,,,,"0|z1m8a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/24 06:12;jiabao.sun; !screenshot-1.png! 

Add thread suspended breakpoints at StreamTask:1177 and StreamTask:1194.
Then resume the first breakpoint, the error will be reproduced.;;;","15/Jan/24 09:10;jiabao.sun;Resuming the first breakpoint will unblock the main thread and trigger a assertion on whether the CompletableFuture is completed. However, at this point, the result has not been returned, so the CompletableFuture seen by the main thread is still in an incomplete state.;;;","15/Mar/24 13:33;pnowojski;Thanks for the fix!

merged commit 5aebb04 into apache:master ;;;","22/Mar/24 13:26;rskraba;(1.19) [https://github.com/apache/flink/actions/runs/8384423618/job/22962033523#step:10:9703]

We might want to backport this to 1.19!  Also: the fix version here might be 1.20.;;;","22/Mar/24 15:50;mapohl;I created the [1.19 backport|https://github.com/apache/flink/pull/24556]. Is this also affecting 1.18? Based on the git history I would assume so.;;;","02/Apr/24 08:03;mapohl;master: [5aebb04b3055fbec6a74eaf4226c4a88d3fd2d6e|https://github.com/apache/flink/commit/5aebb04b3055fbec6a74eaf4226c4a88d3fd2d6e]
1.19: [ece4faee055b3797b39e9c0b55f3e94a3db2f912|https://github.com/apache/flink/commit/ece4faee055b3797b39e9c0b55f3e94a3db2f912];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests against jdk17 for pulsar connector,FLINK-33815,13561649,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,13/Dec/23 10:15,13/Dec/23 12:19,04/Jun/24 20:40,13/Dec/23 12:19,pulsar-4.0.1,,,,,,,,pulsar-4.2.0,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 13 12:19:56 UTC 2023,,,,,,,,,,"0|z1m89k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/23 12:19;leonard;Implemented in main(4.2): 8bfbe33f86a45b4757a9cd1f19f2ede5da771487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler Standalone control loop supports multiple thread,FLINK-33814,13561631,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,13/Dec/23 08:00,04/Jan/24 03:58,04/Jun/24 20:40,04/Jan/24 03:58,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,"When the job list has a lot of jobs, single thread isn't enough.

So Autoscaler Standalone control loop supports multiple thread is very useful for massive production, it's similar to kubernetes.operator.reconcile.parallelism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 04 03:58:33 UTC 2024,,,,,,,,,,"0|z1m85k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/24 03:58;fanrui;Merged main(1.8.0) via : 1da4e77faeb71e4edc91b354555247ab0abffdb7 and 141aac76af0837d4d57b1d896d9e9bcba4593236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve time formatting for easier reading inside the autoscaler,FLINK-33813,13561629,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,13/Dec/23 07:30,14/Dec/23 11:19,04/Jun/24 20:40,14/Dec/23 11:19,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,"The default format of Instant is UTC time, it's better to format it to the corresponding Time Zone.It will be easier reading.

 

!https://f.haiserve.com/download/8f2044829bf190f81cd03ddc5a749dc40b01020000082d9e08215890070201702451618703ac?userid=146850&token=31fa35028392b8b45a2ff21fe8481f1d|width=1511,height=462!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 11:19:12 UTC 2023,,,,,,,,,,"0|z1m854:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 11:19;fanrui;Merged master<1.8> via : 2136c7a3189753c45e167920bd509a687218a57f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscale not working properly for standalone application deployment,FLINK-33812,13561615,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wangsan,wangsan,wangsan,13/Dec/23 03:43,22/Dec/23 09:34,04/Jun/24 20:40,22/Dec/23 09:34,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Currently, when the autoscaler decides to scale up/down a standalone application deployment, taskmanager's replica number will not be updated based on the new override parallelism configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 22 09:34:11 UTC 2023,,,,,,,,,,"0|z1m820:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Dec/23 09:34;mbalassi; [{{f01f8ec}}|https://github.com/apache/flink-kubernetes-operator/commit/f01f8ec0dcde79b21c164143128f0c30ef1aa091] in main.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The CI for frocksdb has broken,FLINK-33811,13561614,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,13/Dec/23 03:39,14/Dec/23 08:33,04/Jun/24 20:40,14/Dec/23 08:33,,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,"!image-2023-12-13-11-37-56-525.png!

It says:

!image-2023-12-13-11-39-11-353.png!

The CI script should be updated",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/23 03:37;zakelly;image-2023-12-13-11-37-56-525.png;https://issues.apache.org/jira/secure/attachment/13065273/image-2023-12-13-11-37-56-525.png","13/Dec/23 03:39;zakelly;image-2023-12-13-11-39-11-353.png;https://issues.apache.org/jira/secure/attachment/13065272/image-2023-12-13-11-39-11-353.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 08:33:06 UTC 2023,,,,,,,,,,"0|z1m81s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 08:33;zakelly;Merged into FRocksDB-6.20.3 via e605685600658f6b146e60c6f0c9543b63f78975...251e7fc45edc1d0f0e2e124232064f67388797c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate RecordAttributes that contains isProcessingBacklog status,FLINK-33810,13561611,13553221,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xuannan,xuannan,xuannan,13/Dec/23 02:53,20/Dec/23 01:39,04/Jun/24 20:40,19/Dec/23 11:04,,,,,,,,,1.19.0,,,,Runtime / Task,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 11:04:00 UTC 2023,,,,,,,,,,"0|z1m814:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 11:04;xtsong;master (1.19): f6bbf1cf364a3b4d04e6b6ddc522bad6431b43c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalog load hivemetastore.xml,FLINK-33809,13561610,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bo Cui,Bo Cui,13/Dec/23 02:40,28/Dec/23 12:19,04/Jun/24 20:40,,1.17.1,1.18.1,1.19.0,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"[https://github.com/apache/flink/blob/3532f59cb9484a67e1b441e2875a26eb3691221f/flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java#L266]

Currently, hivecatalog only load hive-site.xml, we need to merge hivemetastore-site.xml into hive-site.xml, which is inconvenient.

we can enhance hivecatalog, it can load hivemetastore-site.xml and suppors 'properties.*'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-13 02:40:30.0,,,,,,,,,,"0|z1m80w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for WindowJoin node,FLINK-33808,13561600,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,13/Dec/23 01:03,14/Dec/23 09:40,04/Jun/24 20:40,14/Dec/23 09:40,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 09:40:12 UTC 2023,,,,,,,,,,"0|z1m7yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 09:40;dwysakowicz;Implemented in b691a2ee33e8d94b291f6632024bd801a17841a7..01b3db6f8b229ad9683b5c7c2b528f183e25aa3b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update junit to 5.10.1,FLINK-33807,13561594,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,12/Dec/23 23:37,16/Dec/23 09:27,04/Jun/24 20:40,16/Dec/23 09:27,,,,,,,,,1.19.0,,,,Test Infrastructure,,,,0,pull-request-available,,,"There is logging improved for 
>Exceptions thrown for files that cannot be deleted when cleaning up a temporary directory created via {{@TempDir}} now include the root cause.

[https://junit.org/junit5/docs/current/release-notes/index.html#bug-fixes-2]

which could help with debugging of FLINK-33641",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 16 09:27:31 UTC 2023,,,,,,,,,,"0|z1m7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/23 09:27;Sergey Nuyanzin;Merged as [ac88acfbb1b4ebf7336e9a20e0b6d0b0fe32be51|https://github.com/apache/flink/commit/ac88acfbb1b4ebf7336e9a20e0b6d0b0fe32be51];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Async IO Allows Custom Action after Final Retry Failure,FLINK-33806,13561591,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,qqibrow,qqibrow,12/Dec/23 22:38,12/Dec/23 22:38,04/Jun/24 20:40,,,,,,,,,,,,,,API / DataStream,,,,0,,,,"in Async IO Retry Support, if all retries fail, the record is dropped without any further action. However, there are user cases requiring action after the final retry failure occurs, e.g, log out the input or write the input data to an external storage.  To address this, we propose to add a new API in the AsyncRetryStrategy and make changes accordingly. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-12 22:38:45.0,,,,,,,,,,"0|z1m7wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for OverAggregate node,FLINK-33805,13561589,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jhughes,jhughes,jhughes,12/Dec/23 22:16,28/Mar/24 14:07,04/Jun/24 20:40,28/Mar/24 14:07,,,,,,,,,1.20.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 14:07:56 UTC 2024,,,,,,,,,,"0|z1m7w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/24 14:07;dwysakowicz;Implemented in b3334d1527aab6c196752b63c3139ff5529598cc..bf60c8813598d3119375cec057930240642699d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Option to disable showing metrics in JobMananger UI,FLINK-33804,13561586,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qqibrow,qqibrow,12/Dec/23 20:52,18/Dec/23 11:09,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Metrics,,,,0,,,,"Flink allows users to view metric in JobMananger UI. However there are 2 problems we found:
 # The JobManager is required to aggregate metrics from all task managers. When the metric cardinality is quite high, this process can trigger a JobManager Full GC and slow response time.
 # Flink user cases in prod usually have their own dashboard to view metrics. so this feature sometimes is not useful.

In light of this, we propose to add option to disable this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 11:09:48 UTC 2023,,,,,,,,,,"0|z1m7vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 21:22;martijnvisser;I'm not too sure, because it's reads like having finer-grained possibilities to enable/disable certain options in the UI. We don't have that at this moment and starting to add those will mean that we at some point will have a lot of options, who possibly conflict with each other. If Flink users have their own dashboard, why not disable the UI completely?;;;","12/Dec/23 21:28;qqibrow;[~martijnvisser] disabling the metric aggregation to JobMannger is the key.  We have jobs having the job mannager GC issue in production and we have a internal fix. want to contribute back the fix and align with community long term solution here. Can change the title accordingly if that's misleading. ;;;","13/Dec/23 10:45;martijnvisser;[~qqibrow] What is your proposal for the long term solution? Looking at https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/ we don't offer a lot of granularity to disable/enable specific features and I'm not sure that we should.;;;","13/Dec/23 20:36;qqibrow;[~martijnvisser] 
The problem we want to address:

In default setting, the JobManager is required to aggregate metrics from all task managers to power the metrics in UI. When the metric cardinality is quite high, this process can trigger a JobManager Full GC and slow response time. 

There are several options:
Option 1: The issue at hand can be mitigated by setting metrics.fetcher.update-interval=0. However, a problem arises in the JobManager UI where metrics like ""Byte Received"" keep loading indefinitely. This can lead to confusion for users.

Option 2: To address this, we can introduce a whitelist of metrics. Additionally, we can add an option that, when enabled, will only allow the selected metrics to report to the JobManager. This will ensure that the UI, including the overview page and subtask page, continues to function properly.

Option 3: An alternative approach is to follow a similar path as in option 2. However, instead of introducing a new feature flag, we can repurpose the existing metrics.fetcher.update-interval flag. When metrics.fetcher.update-interval is set to 0, the whitelist feature will be automatically activated.
 
 
 ;;;","13/Dec/23 21:53;qqibrow;the whitelist will be a subset of metrics listed here [https://github.com/apache/flink/blob/c3e2d163a637dca5f49522721109161bd7ebb723/flink-runtime/src/main/java/org/apache/flink/runtime/metrics/MetricNames.java#L4] ;;;","18/Dec/23 11:09;martijnvisser;[~qqibrow]  Please open a discussion with a proposal on the Dev ML for this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add observedGeneration to Operator's status spec,FLINK-33803,13561570,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ryanvanhuuksloot,ryanvanhuuksloot,ryanvanhuuksloot,12/Dec/23 17:20,03/Jun/24 16:38,04/Jun/24 20:40,26/Jan/24 10:43,,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Thoughts on adding the observedGeneration status field to the Kubernetes Operator?
 
I saw this issue https://issues.apache.org/jira/browse/FLINK-30858 which is useful but the traditional thing would be to just update the observedGeneration when the reconciliation is complete. There is [tooling]([https://github.com/Shopify/krane/blob/main/README.md#specifying-passfail-conditions]) out there that requires this spec in order to determine if the reconciliation passed for CRDs. Not only that, but it is the expected flow: [https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792]
 
I think we keep the current implementation of the reconciliation spec for now but at the same time, we can update the `status.observedGeneration`?
 
Mailing List Discussion: https://lists.apache.org/thread/1dv33dyqqh18qncot1c6lxgn02do6xnw",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 26 10:43:47 UTC 2024,,,,,,,,,,"0|z1m7s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/24 10:43;gyfora;merged to main 0fa78d350bd4a6cf2ea5e5bca0880092104355d8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ActiveResourceManagerFactoryTest.testGetEffectiveConfigurationForResourceManagerCoarseGrained failed with 239 exit code,FLINK-33802,13561536,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,12/Dec/23 12:05,12/Dec/23 12:26,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Coordination,,,,0,github-actions,test-stability,,"https://github.com/XComp/flink/actions/runs/7172788948/job/19531075115#step:12:8884
{code}
Error: 20:31:32 20:31:32.060 [ERROR] Error occurred in starting fork, check output in log
Error: 20:31:32 20:31:32.060 [ERROR] Process Exit Code: 239
Error: 20:31:32 20:31:32.060 [ERROR] Crashed tests:
Error: 20:31:32 20:31:32.060 [ERROR] org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactoryTest
Error: 20:31:32 20:31:32.061 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
[...]
{code}

{{ActiveResourceManagerFactoryTest.testGetEffectiveConfigurationForResourceManagerCoarseGrained}} was started but never finishes (see {{mvn-1.log}} in failed build artifacts):
{code}
20:26:09,069 [                main] INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactoryTest [] - 
================================================================================
Test testGetEffectiveConfigurationForResourceManagerCoarseGrained(org.apache.flink.runtime.resourcemanager.active.ActiveResourceManagerFactoryTest) is running.
--------------------------------------------------------------------------------
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 12:26:04 UTC 2023,,,,,,,,,,"0|z1m7kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 12:26;mapohl;the test itself shouldn't be the cause for this issue. It only checks some basic configuration filtering.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert to last scaling when job becomes unhealthy after scaling,FLINK-33801,13561518,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,12/Dec/23 09:57,13/Dec/23 08:36,04/Jun/24 20:40,,,,,,,,,,,,,,Autoscaler,,,,0,,,,"Revert to last scaling when job becomes unhealthy after scaling, such as: the memory is under pressure.

In the first version, the unhealthy only for memory is under pressure. We can improve more unhealthy cases in the future.

This Jira is created when review this PR: https://github.com/apache/flink-kubernetes-operator/pull/726#discussion_r1422456112",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 13 08:36:28 UTC 2023,,,,,,,,,,"0|z1m7gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/23 08:36;gyfora;This is basically similar to the rollback feature in the operator but with regards to throughput / processing rate. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow passing parameters to database via jdbc url,FLINK-33800,13561454,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,11/Dec/23 20:35,15/Dec/23 09:15,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,"Currently it does not allow to pass extra properties e.g.
an attempt to connect to 
{{jdbc:postgresql://...?sslmode=require}}
fails with 
{noformat}
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229)
	at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83)
	... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation b70b5cf7-7068-4eb6-83a4-78aed36dbd35.
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)


{noformat}
because of of a logic at {{org.apache.flink.connector.jdbc.catalog.JdbcCatalogUtils#validateJdbcUrl}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-11 20:35:18.0,,,,,,,,,,"0|z1m728:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add e2e's for tls enabled operator,FLINK-33799,13561431,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tagarr,tagarr,11/Dec/23 17:33,14/Mar/24 14:33,04/Jun/24 20:40,,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.9.0,,,,Kubernetes Operator,,,,0,,,,"It would be good to create some E2E tests to ensure a tls enabled flink operator works, so that we don't break anything in the future",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-11 17:33:48.0,,,,,,,,,,"0|z1m6x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatically clean up rocksdb logs when the task failover.,FLINK-33798,13561392,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liming,liming,liming,11/Dec/23 13:52,14/Mar/24 10:13,04/Jun/24 20:40,14/Mar/24 10:13,,,,,,,,,1.17.3,1.18.2,1.19.1,1.20.0,Runtime / State Backends,,,,0,pull-request-available,,,"Since FLINK-24785 relocates rocksdb log, multiple rocksdb logs will be created under the flink log directory, but they are not cleaned up during task failover, resulting in a large number of rocksdb logs under the flink log directory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 13 06:03:31 UTC 2024,,,,,,,,,,"0|z1m6og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 13:58;fanrui;Thanks [~liming] for reporting this bug.

When the log isn't relocated, the log can be cleaned up with rocksdb data together after task is exited. As I understand, after relocation, they should be cleaned as well.

cc [~masteryhx] ;;;","12/Dec/23 03:26;masteryhx;Thanks [~fanrui] pinging me here.

I think you are right. The behavious after relocating is not consistent with before.

We could make it.

Thanks [~liming] for reporting this and we could go ahead.

 ;;;","13/Mar/24 03:57;fanrui;Merged to 
master(1.20) via: 398bb5068703a6461e2e52a90c97078e4263059a
1.17 via: 09e448440b87d5c8add5044a6629026136b05559
1.18 via: 5a1fb807ab8b4bff8801bccb666e9f3d0cb716a0
1.19 via: 943d9a48fdcd13c881e8dc62b3e5df9cc3b64b5e;;;","13/Mar/24 03:58;fanrui;Hi [~liming], IIUC, this JIRA is a file leak related bug instead of improvement, right?

If so, would you mind backing port it to 1.17 , 1.18 and 1.19? thanks a lot;;;","13/Mar/24 06:03;liming;[~fanrui], good idea, I will back port to 1.17 , 1.18 and 1.19.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink application FAILED log in info instead of warn,FLINK-33797,13561385,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,nfraison.datadog,nfraison.datadog,11/Dec/23 13:15,11/Dec/23 13:29,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,pull-request-available,,,When the flink application end in cancelled or failed state the log is reported with INFO log level while it should be warn:  https://github.com/apache/flink/blob/548e4b5188bb3f092206182d779a909756408660/flink-clients/src/main/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrap.java#L174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-11 13:15:29.0,,,,,,,,,,"0|z1m6mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ability to customize java version for python ci in connectors,FLINK-33796,13561371,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,11/Dec/23 11:36,13/Dec/23 00:05,04/Jun/24 20:40,13/Dec/23 00:04,,,,,,,,,connector-parent-1.1.0,,,,Connectors / Parent,,,,0,pull-request-available,,,"In FLINK-33556 there was added ci for python however currently it is sticked to jdk8

 

It would be great to have ability to customize java version",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 13 00:04:46 UTC 2023,,,,,,,,,,"0|z1m6js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/23 00:04;Sergey Nuyanzin;Merged as [969edc5814d021d636b5fed1877f7932214be7ea|https://github.com/apache/flink-connector-shared-utils/commit/969edc5814d021d636b5fed1877f7932214be7ea];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new config to forbid autoscaling in certain periods of a day,FLINK-33795,13561367,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,flashJd,flashJd,flashJd,11/Dec/23 11:16,02/Jan/24 10:46,04/Jun/24 20:40,02/Jan/24 10:46,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,Add new config to forbid autoscaling in certain periods of a day so that we keep flink job unaffected by autoscaling's job restart behavior during this periods,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-11 11:16:03.0,,,,,,,,,,"0|z1m6iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After Flink Avro deserialization fails, subsequent correct data cannot be deserialized correctly",FLINK-33794,13561342,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,weichengji,weichengji,11/Dec/23 09:23,11/Dec/23 09:55,04/Jun/24 20:40,,1.17.0,1.18.0,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"Excuse me, this is my first time submitting Flink Jira.

I found that when using the official AvroDeserializationScheme for avro deserialization, when deserialization fails, the next correct data cannot be deserialized correctly because the previous incorrect data caused the inputStream in the decoder to not be fully read and closed. I think we should modify the deserialize method to the following code block。

The version of Flink I am using is 1.17.1
{code:java}
/** Avro decoder that decodes binary data. */
private transient BinaryDecoder decoder;

@Override
public T deserialize(@Nullable byte[] message) throws IOException {
    if (message == null) {
        return null;
    }
    // read record
    checkAvroInitialized();
    checkAvroDecoder();
    inputStream.setBuffer(message);
    Schema readerSchema = getReaderSchema();
    GenericDatumReader<T> datumReader = getDatumReader();

    datumReader.setSchema(readerSchema);

    return datumReader.read(null, decoder);
}

void checkAvroDecoder() throws IOException {
    if (!decoder.isEnd()) {
        this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    }
} {code}
And I think this class design is unreasonable. The generic type T passed through the forSpecific method should not extend Specifically Record. Users should be asked to pass in a regular Java class and schema for deserialization.

I hope to interpret my doubts.",Flink 1.17.1 & Flink 1.18.0,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,2023-12-11 09:23:49.0,,,,,,,,,,"0|z1m6dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NoSuchMethodError when checkpointing in Google Cloud Storage,FLINK-33793,13561333,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,czchen,czchen,11/Dec/23 08:35,18/Dec/23 13:09,04/Jun/24 20:40,18/Dec/23 13:09,1.18.0,,,,,,,,1.18.1,,,,Runtime / Checkpointing,,,,1,,,,"We have the following exception when checkpointing in Flink 1.18 + Google Cloud Storage. The same code works well in Flink 1.17:

 

{{2023-12-11 07:45:28,861 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'jobmanager-io-thread-5' produced an uncaught exception. Stopping the process...}}
{{java.lang.NoSuchMethodError: 'com.google.common.collect.ImmutableMap com.google.common.collect.ImmutableMap$Builder.buildOrThrow()'}}
{{        at com.google.cloud.storage.UnifiedOpts$Opts.getRpcOptions(UnifiedOpts.java:2096) ~[?:?]}}
{{        at com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:624) ~[?:?]}}
{{        at com.google.cloud.storage.StorageImpl.writer(StorageImpl.java:90) ~[?:?]}}
{{        at org.apache.flink.fs.gs.storage.GSBlobStorageImpl.writeBlob(GSBlobStorageImpl.java:64) ~[?:?]}}
{{        at org.apache.flink.fs.gs.writer.GSRecoverableFsDataOutputStream.createWriteChannel(GSRecoverableFsDataOutputStream.java:229) ~[?:?]}}
{{        at org.apache.flink.fs.gs.writer.GSRecoverableFsDataOutputStream.write(GSRecoverableFsDataOutputStream.java:152) ~[?:?]}}
{{        at org.apache.flink.fs.gs.writer.GSRecoverableFsDataOutputStream.write(GSRecoverableFsDataOutputStream.java:135) ~[?:?]}}
{{        at org.apache.flink.fs.gs.writer.GSRecoverableFsDataOutputStream.write(GSRecoverableFsDataOutputStream.java:128) ~[?:?]}}
{{        at org.apache.flink.runtime.state.filesystem.FsCheckpointMetadataOutputStream.write(FsCheckpointMetadataOutputStream.java:73) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at java.io.DataOutputStream.writeInt(Unknown Source) ~[?:?]}}
{{        at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:98) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:88) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.checkpoint.Checkpoints.storeCheckpointMetadata(Checkpoints.java:83) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.checkpoint.PendingCheckpoint.finalizeCheckpoint(PendingCheckpoint.java:335) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.finalizeCheckpoint(CheckpointCoordinator.java:1404) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.completePendingCheckpoint(CheckpointCoordinator.java:1303) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveAcknowledgeMessage(CheckpointCoordinator.java:1195) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$acknowledgeCheckpoint$1(ExecutionGraphHandler.java:89) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119) ~[flink-dist-1.18.0.jar:1.18.0]}}
{{        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]}}
{{        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]}}
{{        at java.lang.Thread.run(Unknown Source) [?:?]}}

 

The issue has been reported in GitHub [https://github.com/apache/flink/pull/22281#issuecomment-1728553794.] However, it is still not fixed yet in 1.18.0.",Flink 1.18,,,,,,,,,,,,,,,,,FLINK-33603,FLINK-33704,,,,,,,FLINK-31631,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 13:06:29 UTC 2023,,,,,,,,,,"0|z1m6bc:",9223372036854775807,"Flink 1.18.1 contains updated versions of the Google Cloud Storage SDK and Google Cloud Hadoop Connector, which has been completed via FLINK-33704. These updates resolve NoSuchMethod exceptions that occurred in Flink 1.18.0 during checkpointing to Google Cloud Storage.",,,,,,,,,,,,,,,,,,,"11/Dec/23 17:47;cnauroth;I expect this will be fixed by FLINK-33603.

CC: [~jjayadeep];;;","18/Dec/23 10:49;martijnvisser;Fixed via FLINK-33704;;;","18/Dec/23 11:37;qinjunjerry;Could we update the Fix versions field such that everyone knows? ;;;","18/Dec/23 11:59;martijnvisser;[~qinjunjerry] That's done in the actual ticket that fixed the issue, so FLINK-33704;;;","18/Dec/23 12:57;qinjunjerry;Thanks [~martijnvisser] ;;;","18/Dec/23 13:06;martijnvisser;[~qinjunjerry] I've decided to add something to the release notes to make it more explicit;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate the same code for the same logic,FLINK-33792,13561313,13417633,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,FrankZou,FrankZou,FrankZou,11/Dec/23 06:57,15/Jan/24 03:02,04/Jun/24 20:40,15/Jan/24 03:02,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Generate the same code for the same logic, so that we may reuse the generated code between different jobs. This is the precondition for FLINK-28691. The current issue is we use a self-incrementing counter in CodeGenUtils#newName, it means we could not get the same generated class between two queries even when they are exactly the same.",,,,,,,,,,,,FLINK-28691,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 03:02:33 UTC 2024,,,,,,,,,,"0|z1m66w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 07:01;FrankZou;[~libenchao] Hi, I would like to take this ticket, could you please assign it to me?;;;","11/Dec/23 08:24;libenchao;[~FrankZou] Assigned to you.;;;","15/Jan/24 03:02;libenchao;Implemented via d26c1b668b7febc60aab1e4174f568958cd615d3 (1.19.0)

[~zoudan] Thanks for your work! And also thanks [~lsy] for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE when array is null in PostgresArrayConverter in flink-connector-jdbc,FLINK-33791,13561297,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lijingwei.5018,lijingwei.5018,11/Dec/23 03:24,11/Dec/23 08:26,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"{code:java}
// private JdbcDeserializationConverter createPostgresArrayConverter(ArrayType arrayType) {
    // Since PGJDBC 42.2.15 (https://github.com/pgjdbc/pgjdbc/pull/1194) bytea[] is wrapped in
    // primitive byte arrays
    final Class<?> elementClass =
            LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
    final JdbcDeserializationConverter elementConverter =
            createNullableInternalConverter(arrayType.getElementType());
    return val -> {
        @SuppressWarnings(""unchecked"")
        T pgArray = (T) val;
        Object[] in = (Object[]) pgArray.getArray();
        final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
        for (int i = 0; i < in.length; i++) {
            array[i] = elementConverter.deserialize(in[i]);
        }
        return new GenericArrayData(array);
    };
} {code}
When use this method, array is null pgArray.getArray() will throw NPE。",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 08:25:28 UTC 2023,,,,,,,,,,"0|z1m63c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 08:25;Sergey Nuyanzin;[~lijingwei.5018]  can you be more precise about test case?

We have testcase at {{org.apache.flink.connector.jdbc.databases.postgres.catalog.PostgresCatalogITCase#testArrayTypes}} when array is null and it doesn't fail.

Do you have a test case confirming failure you've mentioned, or even better ready to run JUnit test?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upsert statement filter unique key field colume in mysql dielact ,FLINK-33790,13561295,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,lijingwei.5018,lijingwei.5018,11/Dec/23 03:17,15/May/24 03:56,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"example: `col2` and `col4` is unique key in table `my_table`

 
{code:java}
INSERT INTO `my_table`(`col1`, `col2`, `col3`, `col4`, `col5`) 
VALUES (?, ?, ?, ?, ?)
ON DUPLICATE KEY UPDATE 
`col1`=VALUES(`col1`),
`col2`=VALUES(`col2`),
`col3`=VALUES(`col3`),
`col4`=VALUES(`col4`),
`col5`=VALUES(`col5`){code}
result:
{code:java}
INSERT INTO `my_table`(`col1`, `col2`, `col3`, `col4`, `col5`) 
VALUES (?, ?, ?, ?, ?)
ON DUPLICATE KEY UPDATE 
`col1`=VALUES(`col1`),
`col3`=VALUES(`col3`),
`col5`=VALUES(`col5`) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 15 03:56:04 UTC 2024,,,,,,,,,,"0|z1m62w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Feb/24 21:09;jeyhunkarimov;Hi [~lijingwei.5018] thanks for reporting this issue. There is a similar test in 
{code:java}
FieldNamedPreparedStatementImplTest::testUpsertStatement
{code} 
with key fields with field names  = 
{code:java}
{""id"", ""name"", ""email"", ""ts"", ""field1"", ""field_2"", ""__field_3__""}
{code}
 and key fields = 
{code:java}
{""id"", ""__field_3__""}
{code}
The test seems passing. Am I missing something?

;;;","15/May/24 03:56;sudewei.sdw;Hi [~jeyhunkarimov] , i think what [~lijingwei.5018]  tring to say is that the MySqlDialect does not need to update the unique key in the UpsertStatement. So it is an improvement. After adding this feature, the upsert statement would not include the key fields just like the example shows.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose restart time as a metric,FLINK-33789,13561287,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,10/Dec/23 21:48,14/Mar/24 14:33,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,FLINK-30593 added restart time tracking. It would be convenient to also report is as a metric.,,,,,,,,,,,,,,,,,FLINK-30593,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-10 21:48:45.0,,,,,,,,,,"0|z1m614:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table not found for catalogname is capital,FLINK-33788,13561239,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,shaoguang,shaoguang,shaoguang,09/Dec/23 12:16,21/Dec/23 01:29,04/Jun/24 20:40,,1.16.1,,,,,,,,,,,,,,,,0,,,,"{code:java}
        String runMode=args[0];
        String sql = ""select * from student"";
        String sql = ""insert into table student select * from student;"";
        String sql = args[1];
        String catalogName = ""myHive"";
        String defaultDatabase = args[2];

        RuntimeExecutionMode executionMode = ""batch"".equals(runMode) ? RuntimeExecutionMode.BATCH : RuntimeExecutionMode.STREAMING;
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setRuntimeMode(executionMode);

        StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
        


        tenv.getConfig().setSqlDialect(SqlDialect.HIVE);


        Configuration conf = new Configuration();
        conf.addResource(""/ect/hadoop/conf"");
        conf.addResource(""/ect/hive/conf"");
        HiveConf hiveConf = new HiveConf(conf, Configuration.class);
        String hiveVersion = ""3.1.3"";

        HiveCatalog hiveCatalog = new HiveCatalog(catalogName, defaultDatabase, hiveConf, hiveVersion, true);
        tenv.registerCatalog(catalogName, hiveCatalog);

        tenv.useCatalog(catalogName);
        tenv.useDatabase(defaultDatabase);

        tenv.executeSql(sql).print(); {code}
Even if the table exists in hive, the query will still report an error like""Caused by: org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:14 Table not found 'student'""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/23 12:26;shaoguang;image-2023-12-09-20-26-41-150.png;https://issues.apache.org/jira/secure/attachment/13065160/image-2023-12-09-20-26-41-150.png","09/Dec/23 15:13;shaoguang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13065161/screenshot-1.png","17/Dec/23 04:11;shaoguang;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13065364/screenshot-2.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 21 01:29:51 UTC 2023,,,,,,,,,,"0|z1m5qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/23 12:20;shaoguang;Hive will convert catalogname 'myHive' to lowercase 'myhive' when parsing, but the variable catalogs in CatalogManager is the catalogname you defined 'myHive' and the default catalog 'default_catalog', so it will report that the table cannot be found.

 !image-2023-12-09-20-26-41-150.png! 
 ;;;","09/Dec/23 12:41;Sergey Nuyanzin;IIRC by default it is case insensitive if you need case sensitive use quotes like
{code:sql}
""select * from \""myHive\"".study.student;""
{code};;;","09/Dec/23 14:48;shaoguang;源码修改：
org.apache.flink.table.planner.delegation.hive.copy.HiveParserSemanticAnalyzer
修改如下方法中：String qualifiedTableName = HiveParserBaseSemanticAnalyzer.getUnescapedName(tableTree, this.catalogManager.getCurrentCatalog(), this.catalogManager.getCurrentDatabase()).{color:red}toLowerCase(){color}; 去掉toLowerCase()即可保留原来的catalog和表名。
{code:java}
private String processTable(HiveParserQB qb, HiveParserASTNode tabref) throws SemanticException {
        int[] indexes = HiveParserBaseSemanticAnalyzer.findTabRefIdxs(tabref);
        int aliasIndex = indexes[0];
        int propsIndex = indexes[1];
        int tsampleIndex = indexes[2];
        int ssampleIndex = indexes[3];
        HiveParserASTNode tableTree = (HiveParserASTNode)((HiveParserASTNode)tabref.getChild(0));
        String qualifiedTableName = HiveParserBaseSemanticAnalyzer.getUnescapedName(tableTree, this.catalogManager.getCurrentCatalog(), this.catalogManager.getCurrentDatabase()).toLowerCase()
        String originTableName = HiveParserBaseSemanticAnalyzer.getUnescapedOriginTableName(tableTree);
        String alias = HiveParserBaseSemanticAnalyzer.findSimpleTableName(tabref, aliasIndex);
        if (propsIndex >= 0) {
            Tree propsAST = tabref.getChild(propsIndex);
            Map<String, String> props = HiveParserDDLSemanticAnalyzer.getProps((HiveParserASTNode)propsAST.getChild(0));
            if (""TRUE"".equals(props.get(""insideView""))) {
                qb.getAliasInsideView().add(alias.toLowerCase());
            }

            qb.setTabProps(alias, props);
        }

        if (qb.exists(alias)) {
            throw new SemanticException(HiveParserErrorMsg.getMsg(ErrorMsg.AMBIGUOUS_TABLE_ALIAS, tabref.getChild(aliasIndex)));
        } else {
            HiveParserASTNode sampleClause;
            if (tsampleIndex >= 0) {
                sampleClause = (HiveParserASTNode)tabref.getChild(tsampleIndex);
                ArrayList<HiveParserASTNode> sampleCols = new ArrayList();
                if (sampleClause.getChildCount() > 2) {
                    for(int i = 2; i < sampleClause.getChildCount(); ++i) {
                        sampleCols.add((HiveParserASTNode)sampleClause.getChild(i));
                    }
                }

                if (sampleCols.size() > 2) {
                    throw new SemanticException(HiveParserUtils.generateErrorMessage((HiveParserASTNode)tabref.getChild(0), ErrorMsg.SAMPLE_RESTRICTION.getMsg()));
                }

                qb.getParseInfo().setTabSample(alias);
                if (this.unparseTranslator.isEnabled()) {
                    Iterator var25 = sampleCols.iterator();

                    while(var25.hasNext()) {
                        HiveParserASTNode sampleCol = (HiveParserASTNode)var25.next();
                        this.unparseTranslator.addIdentifierTranslation((HiveParserASTNode)sampleCol.getChild(0));
                    }
                }
            } else if (ssampleIndex >= 0) {
                sampleClause = (HiveParserASTNode)tabref.getChild(ssampleIndex);
                Tree type = sampleClause.getChild(0);
                Tree numerator = sampleClause.getChild(1);
                String value = HiveParserBaseSemanticAnalyzer.unescapeIdentifier(numerator.getText());
                SplitSample sample;
                if (type.getType() == 861) {
                    double percent = Double.parseDouble(value);
                    if (percent < 0.0D || percent > 100.0D) {
                        throw new SemanticException(HiveParserUtils.generateErrorMessage((HiveParserASTNode)numerator, ""Sampling percentage should be between 0 and 100""));
                    }

                    int seedNum = this.conf.getIntVar(ConfVars.HIVESAMPLERANDOMNUM);
                    sample = new SplitSample(percent, seedNum);
                } else if (type.getType() == 902) {
                    sample = new SplitSample(Integer.parseInt(value));
                } else {
                    assert type.getType() == 810;

                    long length = (long)Integer.parseInt(value.substring(0, value.length() - 1));
                    char last = value.charAt(value.length() - 1);
                    if (last != 'k' && last != 'K') {
                        if (last != 'm' && last != 'M') {
                            if (last == 'g' || last == 'G') {
                                length <<= 30;
                            }
                        } else {
                            length <<= 20;
                        }
                    } else {
                        length <<= 10;
                    }

                    int seedNum = this.conf.getIntVar(ConfVars.HIVESAMPLERANDOMNUM);
                    sample = new SplitSample(length, seedNum);
                }

                String aliasId = HiveParserBaseSemanticAnalyzer.getAliasId(alias, qb);
                this.nameToSplitSample.put(aliasId, sample);
            }

            qb.setTabAlias(alias, originTableName, qualifiedTableName);
            if (qb.isInsideView()) {
                qb.getAliasInsideView().add(alias.toLowerCase());
            }

            qb.addAlias(alias);
            qb.getParseInfo().setSrcForAlias(alias, tableTree);
            if (!this.aliasToCTEs.containsKey(qualifiedTableName)) {
                this.unparseTranslator.addTableNameTranslation(tableTree, this.catalogManager.getCurrentCatalog(), this.catalogManager.getCurrentDatabase());
                if (aliasIndex != 0) {
                    this.unparseTranslator.addIdentifierTranslation((HiveParserASTNode)tabref.getChild(aliasIndex));
                }
            }

            return alias;
        }
    }
{code}
;;;","09/Dec/23 15:15;shaoguang;[~Sergey Nuyanzin]
Still some wrong like this:
 !screenshot-1.png! 

Do you know why Flink needs to convert many table names, databases, and catalognames to lowercase?;;;","09/Dec/23 15:33;Sergey Nuyanzin;That's interesting

however it seems only hive connector specific right?



[~yuxia] may be you know the reason why it is like that?;;;","11/Dec/23 01:51;luoyuxia;[~shaoguang] Thanks for reporting it. As Hive is case-insentive, when using Hive dialect, we'll always convert table name, database name, catalog name to lowercase. But it seems we shouldn't convert catalog name to lowercase directly, otherwise, it won't find the catalog reigsterd in flink. I think it's a bug that should be fixed. Could you please help fix if you're free? I think 1.17, 1.18, master should be fixed.

But for master, you should submit a pr in a dedicated repo for Hive. https://github.com/apache/flink-connector-hive;;;","11/Dec/23 02:32;shaoguang;[~luoyuxia] I will try to fix it.But I found there are 11 lowercase convert conversions,I should test theme one by one.;;;","11/Dec/23 03:42;luoyuxia;[~shaoguang] Cool! Thanks. One hint is that we may only need to consider the caller of 
{code:java}
String getUnescapedName(
        HiveParserASTNode tableOrColumnNode,
        @Nullable String currentCatalog,
        @Nullable String currentDatabase) {
xx
} {code}
Since it's the only entrypoint to pass catalog. For database name and table name, it's safe and a must to  convert it to lowercase since Hive is case-insentive. But we shouldn't convert catalog name to lowercase.;;;","17/Dec/23 04:19;shaoguang;[~luoyuxia] 
I have fixed and submitted the bug first, but the push may not be successful.Please help to check.
 !screenshot-2.png! ;;;","21/Dec/23 01:29;luoyuxia;[~shaoguang] What's the problem you meet that casue the push fail?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java 17 support for jdbc connector,FLINK-33787,13561206,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Dec/23 22:37,28/Dec/23 08:36,04/Jun/24 20:40,15/Dec/23 22:51,jdbc-3.1.1,,,,,,,,jdbc-3.1.2,jdbc-3.2.0,,,Connectors / JDBC,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,FLINK-33952,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 28 08:36:05 UTC 2023,,,,,,,,,,"0|z1m5j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 22:51;Sergey Nuyanzin;Merged as [f8de82b4c52a688c5bd36c4c4bd3012ff4081eb8|https://github.com/apache/flink-connector-jdbc/commit/f8de82b4c52a688c5bd36c4c4bd3012ff4081eb8];;;","28/Dec/23 08:36;martijnvisser;Backported to apache/flink-connector-jdbc:v3.1 f4dd71fbd0474bb855e05a7b9027c539130fc72b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Hive connector from master branch,FLINK-33786,13561197,13503028,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Dec/23 20:16,09/Dec/23 08:56,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"The connector is going to be externalized at FLINK-30064
Once it is done it would make sense to remove it from master branch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-08 20:16:52.0,,,,,,,,,,"0|z1m5h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableJdbcUpsertOutputFormat could not deal with DELETE record correctly when primary keys were set,FLINK-33785,13561170,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liubodong,liubodong,08/Dec/23 14:43,08/Dec/23 14:49,04/Jun/24 20:40,,jdbc-3.1.1,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"h1. Issue Description

When using jdbc connector to DELETE records in database, I found it CAN NOT delete records correctly.
h1. Reproduction steps

The steps are as follows：
 * Create a table with 5 fields and a pk. DDL in postgres:

 
{code:java}
create table public.fake
(
    id       bigint                 not null default nextval('fake_id_seq'::regclass),
    name     character varying(128) not null,
    age      integer,
    location character varying(256),
    birthday timestamp without time zone     default CURRENT_TIMESTAMP,
    primary key (id, name)
);{code}
!image-2023-12-08-22-24-26-493.png!

 
 * Insert some data into the table:

{code:java}
INSERT INTO public.fake (id, name, age, location, birthday) VALUES (1, 'Jack', 10, null, '2023-12-08 21:35:46.000000');
INSERT INTO public.fake (id, name, age, location, birthday) VALUES (2, 'Jerry', 18, 'Fake Location', '2023-12-08 13:36:17.088295');
INSERT INTO public.fake (id, name, age, location, birthday) VALUES (3, 'John', 20, null, null);
INSERT INTO public.fake (id, name, age, location, birthday) VALUES (4, 'Marry', null, null, '2023-12-08 13:37:09.721785');
{code}
!image-2023-12-08-22-24-58-986.png!
 * Run the flink code:

{code:java}
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    final String[] fieldNames = {""id"", ""name"", ""age"", ""location"", ""birthday""};
    final int[] fieldTypes = {
            Types.BIGINT, Types.VARCHAR, Types.INTEGER, Types.VARCHAR, Types.TIMESTAMP
    };
    final String[] primaryKeys = {""id"", ""name""};
    InternalJdbcConnectionOptions internalJdbcConnectionOptions =
            InternalJdbcConnectionOptions.builder()
                    .setClassLoader(Thread.currentThread().getContextClassLoader())
                    .setDriverName(Driver.class.getName())
                    .setDBUrl(""jdbc:postgresql://localhost:5432/postgres"")
                    .setUsername(""postgres"")
                    .setPassword(""postgres"")
                    .setTableName(""fake"")
                    .setParallelism(1)
                    .setConnectionCheckTimeoutSeconds(10)
                    .setDialect(new PostgresDialect())
                    .build();
    JdbcOutputFormat<Tuple2<Boolean, Row>, Row, JdbcBatchStatementExecutor<Row>> jdbcOutputFormat =
            JdbcOutputFormat.builder()
                    .setFieldNames(fieldNames)
                    .setKeyFields(primaryKeys)
                    .setFieldTypes(fieldTypes)
                    .setOptions(internalJdbcConnectionOptions)
                    .setFlushIntervalMills(1000)
                    .setFlushMaxSize(10)
                    .setMaxRetryTimes(3)
                    .build();

    GenericJdbcSinkFunction<Tuple2<Boolean, Row>> jdbcSinkFunction =
            new GenericJdbcSinkFunction<>(jdbcOutputFormat);

    Timestamp timestamp = Timestamp.valueOf(""2023-12-08 21:35:46.000000"");
    // Row to delete
    Row row = Row.ofKind(RowKind.DELETE, 1L, ""Jack"", 10, null, timestamp);
    Tuple2<Boolean, Row> element = Tuple2.of(false, row);
    env.fromCollection(Collections.singleton(element)).addSink(jdbcSinkFunction);
    env.execute();
} {code}
When the code executed successfully, we can see that the record id=1 and name=Jack was not deleted.
h1. Cause Analysis

In the build method of JdbcOutputFormat.Builder, if 'keyFields' option was set in the JdbcDmlOptions, the method will return a 'org.apache.flink.connector.jdbc.internal.TableJdbcUpsertOutputFormat'.

!image-2023-12-08-22-28-44-948.png!

And in 'org.apache.flink.connector.jdbc.internal.TableJdbcUpsertOutputFormat#createDeleteExecutor', the method get all the fieldNames instead of keyFields to build the delete sql statement. So the detele sql may not execute correctly.

!image-2023-12-08-22-38-08-559.png!
h1. How to fix
 * Use the real keyFields then fallback to fieldNames to build the executor.

!image-2023-12-08-22-42-06-566.png!","Flink: 1.17.1

Jdbc connector: 3.1.1

Postgresql: 16.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/23 14:24;liubodong;image-2023-12-08-22-24-20-295.png;https://issues.apache.org/jira/secure/attachment/13065132/image-2023-12-08-22-24-20-295.png","08/Dec/23 14:24;liubodong;image-2023-12-08-22-24-26-493.png;https://issues.apache.org/jira/secure/attachment/13065131/image-2023-12-08-22-24-26-493.png","08/Dec/23 14:25;liubodong;image-2023-12-08-22-24-58-986.png;https://issues.apache.org/jira/secure/attachment/13065130/image-2023-12-08-22-24-58-986.png","08/Dec/23 14:28;liubodong;image-2023-12-08-22-28-44-948.png;https://issues.apache.org/jira/secure/attachment/13065129/image-2023-12-08-22-28-44-948.png","08/Dec/23 14:38;liubodong;image-2023-12-08-22-38-08-559.png;https://issues.apache.org/jira/secure/attachment/13065128/image-2023-12-08-22-38-08-559.png","08/Dec/23 14:40;liubodong;image-2023-12-08-22-40-35-530.png;https://issues.apache.org/jira/secure/attachment/13065127/image-2023-12-08-22-40-35-530.png","08/Dec/23 14:42;liubodong;image-2023-12-08-22-42-06-566.png;https://issues.apache.org/jira/secure/attachment/13065126/image-2023-12-08-22-42-06-566.png",,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 14:49:30 UTC 2023,,,,,,,,,,"0|z1m5b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 14:49;liubodong;If this class continues to be used in subsequent development, and this report is indeed a BUG, ​​can this issue be assigned to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogStoreFactory can not be configured via StreamExecutionEnvironment,FLINK-33784,13561168,13541314,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hackergin,twalthr,twalthr,08/Dec/23 14:33,13/Mar/24 09:30,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,The logic in TableEnvironment.create() has well-defined ordering which allows to configure most settings via StreamExecutionEnvironment and flink-conf.yaml. The discovery of CatalogStoreFactory should be postponed until the final configuration is merged.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 09:46:58 UTC 2023,,,,,,,,,,"0|z1m5ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 14:40;twalthr;What is required is this:
{code}
        final MutableURLClassLoader userClassLoader =
                FlinkUserCodeClassLoaders.create(
                        new URL[0], settings.getUserClassLoader(), settings.getConfiguration());

        final ExecutorFactory executorFactory =
                FactoryUtil.discoverFactory(
                        userClassLoader, ExecutorFactory.class, ExecutorFactory.DEFAULT_IDENTIFIER);
        final Executor executor = executorFactory.create(settings.getConfiguration());

        // use configuration to init table config
        final TableConfig tableConfig = TableConfig.getDefault();
        tableConfig.setRootConfiguration(executor.getConfiguration());
        tableConfig.addConfiguration(settings.getConfiguration());

        final CatalogStoreFactory catalogStoreFactory =
                TableFactoryUtil.findAndCreateCatalogStoreFactory(
                        tableConfig, userClassLoader);
        final CatalogStoreFactory.Context context =
                TableFactoryUtil.buildCatalogStoreFactoryContext(
                        tableConfig, userClassLoader);
        catalogStoreFactory.open(context);
        final CatalogStore catalogStore =
                settings.getCatalogStore() != null
                        ? settings.getCatalogStore()
                        : catalogStoreFactory.createCatalogStore();
{code}

Unfortunately, this requires some work in downstream code to deal with `ReadableConfig` instead of Configuration. ;;;","11/Dec/23 03:36;hackergin;[~twalthr]   Thank you for pointing out this issue, I would like fix it, please assign this task to me.  

 

Indeed, some modifications need to be made to handle the `ReadableConfig` for the current logic.

 

How about introduce a DelegatingReadableConfig , so we can deal with prefix options.;;;","11/Dec/23 09:39;twalthr;Hi [~hackergin], DelegatingReadableConfig could be one option. Another option could be to simply use a {{ConfigOption.mapType()}} for the CatalogStore? In general, prefix logic in configuration has turned out to be very complex. For the map type we also support the syntax:

{code}
table.catalog-store.{identifier}.{param1}: v1 
table.catalog-store.{identifier}.{param2}: v2
{code}

The only downside of this is that both param1 and param 2 can only be strings.;;;","11/Dec/23 09:46;hackergin;[~twalthr]   Using ConfigOption.mapType seems relatively easy to implement, I will give it a try. Thank you for your suggestion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add options to ignore parsing error in Kafka SQL Connector,FLINK-33783,13561156,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,bartoszdeepbi,bartoszdeepbi,08/Dec/23 14:03,15/Jan/24 19:27,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / Kafka,Table SQL / API,,,1,,,,"h1. Current state

If an unparsable event enters a Flink Kafka Source in SQL then the whole application restarts. For JSON format there is a property that allows to ignore unparsable events. Other formats, like `confluent-avro` etc don't support that.
h1. Desired state

We would like to ignore the parsing exception in Kafka Source in SQL regardless of the format used. Additionally, a new metric should be introduced that returns a count of unparsable events seen so far.

In the future there should be a Dead Letter Queue handling in SQL Sources similar to Kafka Streams: [https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues/].
For now, the universal ignore with metric would be enough.
h1. Implementation

We already have an implementation for this case in the Flink Kafka Connector and we would like to open a pull request for it. However, we created the issue first as per this recommendation [https://flink.apache.org/how-to-contribute/contribute-code/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 19:27:56 UTC 2024,,,,,,,,,,"0|z1m580:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 14:26;martijnvisser;Thanks for opening this. I think this problem should preferably be solved generically for all connectors and formats, not per format and connector. It would be great if a FLIP can be opened for that

Sidenote: I would expect that you should not be able to end up in a situation where you have unparsable events with confluent-avro. Are we talking about a scenario where someone skipped the Schema Registry service and just wrote an arbitrary byte stream (without magic byte) ?;;;","20/Dec/23 09:03;bartoszdeepbi;Hi [~martijnvisser]!

Sorry for the delayed response. I agree that this topic should be a FLIP at some point! Can I create such FLIP now or does it require a bigger discussion first?

I created this issue first as a stopgap solution. I already implemented the feature in a fork, it can be found here: [https://github.com/deep-bi/flink-connector-kafka/releases/tag/deep-v3.0.2]. Maybe opening a pull request would be a great first step for starting a wider discussion about DLQ (Dead Letter Queue) handling in SQL. What do you think?

Regarding your side note, I agree with you. There should be little to no chance of having unparsable events. However, our client use case is a bit different - they want to manage the schemas themselves and disable the auto-register feature (see: https://issues.apache.org/jira/browse/FLINK-33045). In this scenario, they want to control the schema registry outside of the Flink. They also use other things in the pipeline, like Kafka Streams, etc. If some events end up in the topic that are not registered they should be ignored. Again, this is still more of a hypothetical scenario, but it still may happen at some point.;;;","20/Dec/23 09:11;martijnvisser;No worries [~bartoszdeepbi] 
We shouldn't open up a PR yet, it's better to first have a discussion/FLIP on the Dev mailing list. You can include your PoC for the solution in the proposal of course, so that it would be rather easy to open up a PR at a later stage. If you want, I can grant you FLIP permissions to create a page and then you can open the discussion as well. ;;;","12/Jan/24 11:20;bartoszdeepbi;Sure, sounds great [~martijnvisser] !

It would be great if I could create a FLIP, thank you! Any tips on how to write one properly?

How can I open a mailing list discussion, is it from a FLIP page?;;;","15/Jan/24 19:27;martijnvisser;[~bartoszdeepbi] See https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals for all details;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GroupAggregateRestoreTest fails,FLINK-33782,13561119,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bvarghese,mapohl,mapohl,08/Dec/23 09:03,08/Dec/23 13:42,04/Jun/24 20:40,08/Dec/23 13:42,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55321&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=10778]
{code:java}
Dec 07 13:49:11 13:49:11.037 [ERROR] Tests run: 9, Failures: 0, Errors: 8, Skipped: 1, Time elapsed: 4.213 s <<< FAILURE! - in org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateRestoreTest
Dec 07 13:49:11 13:49:11.037 [ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateRestoreTest.testRestore(TableTestProgram, ExecNodeMetadata)[1]  Time elapsed: 0.17 s  <<< ERROR!
Dec 07 13:49:11 org.apache.flink.table.api.TableException: Cannot load Plan from file '/__w/1/s/flink-table/flink-table-planner/src/test/resources/restore-tests/stream-exec-group-aggregate_1/group-aggregate-simple/plan/group-aggregate-simple.json'.
Dec 07 13:49:11 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.loadPlan(TableEnvironmentImpl.java:760)
Dec 07 13:49:11 	at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:279)
Dec 07 13:49:11 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Dec 07 13:49:11 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Dec 07 13:49:11 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Dec 07 13:49:11 	at java.lang.reflect.Method.invoke(Method.java:498)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 13:42:52 UTC 2023,,,,,,,,,,"0|z1m4zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 09:04;mapohl;Investigation efforts from [~jhughes] in [Slack|https://apache-flink.slack.com/archives/C03MR1HQHK2/p1701988881511149?thread_ts=1701964664.357379&cid=C03MR1HQHK2]:
{quote}Relates to https://issues.apache.org/jira/browse/FLINK-33480
Bonnie Varghese put up a fix at using the same ticket: [https://github.com/apache/flink/pull/23893]This change seems to have caused it: [https://github.com/apache/flink/commit/ca1c7ce48127472a7c7965099f8a7227549f09df]
https://issues.apache.org/jira/browse/FLINK-33666
{quote};;;","08/Dec/23 13:42;dwysakowicz;Fixed in ca72f6302bd3d760d2b47cd8b1b8f2e48705117c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated org.apache.flink.table.api.TableConfig#ctor(),FLINK-33781,13561111,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,08/Dec/23 08:13,30/Jan/24 08:46,04/Jun/24 20:40,14/Dec/23 05:49,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 08:46:38 UTC 2024,,,,,,,,,,"0|z1m4y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 01:21;lsy;Merged to master: bb02698dee078446500d94246acf6b162a2b7924;;;","30/Jan/24 08:46;martijnvisser;[~jackylau] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to store default catalog in CatalogStore,FLINK-33780,13561110,13541314,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liyubin117,liyubin117,08/Dec/23 08:09,12/Dec/23 09:16,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Table SQL / API,,,,0,,,,"Flink initially creates a default catalog which is included in the `Map<String, Catalog> catalogs`, but is not stored in the CatalogStore.

After conducting thorough investigation, I've determined that the necessary modification can be made within the `CatalogManager`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/23 05:47;liyubin117;image-2023-12-11-13-47-29-623.png;https://issues.apache.org/jira/secure/attachment/13065183/image-2023-12-11-13-47-29-623.png","11/Dec/23 06:14;liyubin117;image-2023-12-11-14-14-10-002.png;https://issues.apache.org/jira/secure/attachment/13065184/image-2023-12-11-14-14-10-002.png","12/Dec/23 03:09;liyubin117;image-2023-12-12-11-09-53-075.png;https://issues.apache.org/jira/secure/attachment/13065225/image-2023-12-12-11-09-53-075.png","12/Dec/23 05:42;liyubin117;image-2023-12-12-13-42-04-762.png;https://issues.apache.org/jira/secure/attachment/13065231/image-2023-12-12-13-42-04-762.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 09:16:30 UTC 2023,,,,,,,,,,"0|z1m4xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 08:18;liyubin117;[~hackergin] Hi, looking forward to your suggestions :) I have implemented this, and if the feature is neccessary, could you please assign this to me? thanks~;;;","11/Dec/23 03:40;hackergin;Hi, [~liyubin117]  Thanks for pointing this .  could you provide a more detailed description of what problems may arise if we don't save the default catalog ?;;;","11/Dec/23 05:50;liyubin117;[~hackergin] Thanks for your attention:)

In addition to the conceptual necessity (default catalog is also part of catalogs equaly and not differ from others), I also found that the `show create catalog` statement  proposed in https://issues.apache.org/jira/browse/FLINK-24939 would expose the absence of the default catalog in the CatalogStore, I have implemented the statement on our production env using `CatalogStore`.

!image-2023-12-11-14-14-10-002.png|width=821,height=576!;;;","11/Dec/23 14:16;hackergin;[~liyubin117]  `default_catalog` is usually registered by the Catalog instance. I'm wondering what we need to do if we want to save default_catalog to CatalogStore. Especially in SqlGateway, DefaultCatalog is not necessarily GenericInMemory.  I think  these temporary catalogs should not support `show create catalog` ?;;;","12/Dec/23 03:17;liyubin117;[~hackergin] Hi, we could make modifitions in the `CatalogManager` construct function as follows:

!image-2023-12-12-11-09-53-075.png|width=798,height=406!

DefaultCatalog is the current catalog of a session including SqlGateway, `default_catalog` is like `d2` under the context `use catalog d2`, the only difference is that it is created implicitly by Flink, we should treat them equally for semantic consistency, WDYT?

!image-2023-12-12-13-42-04-762.png|width=422,height=184!;;;","12/Dec/23 08:09;hackergin;[~liyubin117]  I agree that saving default_catalog to CatalogStore is a good idea. However, I think that creating an in-memory catalog as the default catalog  may not be the most suitable approach.

I suggest introducing a new method in org.apache.flink.table.catalog.CatalogManager.Builder called defaultCatalog(String defaultCatalogName, CatalogDescriptor catalogDescriptor). This method would allow specifying a CatalogDescriptor for the default catalog, similar to how CatalogManager.createCatalog works.


WDYT ？ ;;;","12/Dec/23 08:14;hackergin;cc [~Leonard];;;","12/Dec/23 09:16;liyubin117;[~hackergin] [~Leonard]  Thanks for your review :D

It maybe more appropriate to initialize the default catalog when calling `Builder.defaultCatalog` rather than when building `CatalogManager`, but introducing a new method in `CatalogManger.Builder` requires changing all related calls.

If we adopt the solution, there is only one minor comment that `CatalogDescriptor` has already included `catalogName`, so it may be more appropriate to use `defaultCatalog(CatalogDescriptor catalogDescriptor)`. WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated BaseExpressions#cast(TypeInformation),FLINK-33779,13561108,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,08/Dec/23 08:02,15/Dec/23 23:00,04/Jun/24 20:40,15/Dec/23 23:00,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 23:00:18 UTC 2023,,,,,,,,,,"0|z1m4xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 23:00;Sergey Nuyanzin;Merged to master as [ef2b626d67147797e992ec3b338bafdb4e5ab1c7|https://github.com/apache/flink/commit/ef2b626d67147797e992ec3b338bafdb4e5ab1c7];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cleanup usage of deprecated TableConfig#setIdleStateRetentionTime(Time, Time)",FLINK-33778,13561105,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,08/Dec/23 07:48,18/Dec/23 11:43,04/Jun/24 20:40,18/Dec/23 11:43,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 11:42:46 UTC 2023,,,,,,,,,,"0|z1m4wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/23 11:42;jackylau;merged master: 3c86dcadf5366fa0026125051d69b0a8913d5e61;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetTimestampITCase>FsStreamingSinkITCaseBase failing in CI,FLINK-33777,13561064,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,jhughes,jhughes,07/Dec/23 22:46,08/Dec/23 13:44,04/Jun/24 20:40,08/Dec/23 13:43,1.19.0,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,test-stability,,"From this CI run: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55334&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906]
{code:java}
Dec 07 19:57:30 19:57:30.026 [ERROR] Errors: 
Dec 07 19:57:30 19:57:30.026 [ERROR] ParquetTimestampITCase>FsStreamingSinkITCaseBase.testNonPart:84->FsStreamingSinkITCaseBase.testPartitionCustomFormatDate:151->FsStreamingSinkITCaseBase.test:186 » Validation 
Dec 07 19:57:30 19:57:30.026 [ERROR] ParquetTimestampITCase>FsStreamingSinkITCaseBase.testPart:89->FsStreamingSinkITCaseBase.testPartitionCustomFormatDate:151->FsStreamingSinkITCaseBase.test:186 » Validation 
Dec 07 19:57:30 19:57:30.026 [ERROR] ParquetTimestampITCase>FsStreamingSinkITCaseBase.testPartitionWithBasicDate:126->FsStreamingSinkITCaseBase.test:186 » Validation  {code}

The errors each appear somewhat similar:
{code:java}
Dec 07 19:54:43 19:54:43.934 [ERROR] org.apache.flink.formats.parquet.ParquetTimestampITCase.testPartitionWithBasicDate Time elapsed: 1.822 s <<< ERROR! 
Dec 07 19:54:43 org.apache.flink.table.api.ValidationException: Unable to find a field named 'f0' in the physical data type derived from the given type information for schema declaration. Make sure that the type information is not a generic raw type. Currently available fields are: [a, b, c, d, e] 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.patchDataTypeFromColumn(SchemaTranslator.java:350) 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.patchDataTypeFromDeclaredSchema(SchemaTranslator.java:337) 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.createConsumingResult(SchemaTranslator.java:235) 
Dec 07 19:54:43 at org.apache.flink.table.catalog.SchemaTranslator.createConsumingResult(SchemaTranslator.java:180) 
Dec 07 19:54:43 at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.fromStreamInternal(AbstractStreamTableEnvironmentImpl.java:141) 
Dec 07 19:54:43 at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.createTemporaryView(StreamTableEnvironmentImpl.scala:121) 
Dec 07 19:54:43 at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.test(FsStreamingSinkITCaseBase.scala:186) 
Dec 07 19:54:43 at org.apache.flink.table.planner.runtime.stream.FsStreamingSinkITCaseBase.testPartitionWithBasicDate(FsStreamingSinkITCaseBase.scala:126)  {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 13:43:23 UTC 2023,,,,,,,,,,"0|z1m4nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 08:10;Sergey Nuyanzin;Thanks for investigating this

the reason is that the PR for FLINK-33718 was based before changes for FLINK-25565 and as a result didn't take them into account...
I should have rebased before merging
Sorry for the inconvenience ;;;","08/Dec/23 13:43;dwysakowicz;Fixed in 0e515dce78832dbbbf5fce9c8cdd113bbb62cdf0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to specify optional profile for connectors,FLINK-33776,13561053,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,07/Dec/23 20:16,17/Jan/24 17:02,04/Jun/24 20:40,08/Dec/23 12:18,,,,,,,,,,,,,Build System / CI,Connectors / Parent,,,0,pull-request-available,,,"The issue is that sometimes the connector should be tested against several versions of sinks/sources

e.g. hive connector should be tested against hive 2 and hive3, opensearch should be tested against 1 and 2
one of the way is using profiles for that",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 16:38:54 UTC 2024,,,,,,,,,,"0|z1m4l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 12:17;Sergey Nuyanzin;Merged as [11be5c693081aba252ab39576ec1c6f7fde5765c|https://github.com/apache/flink-connector-shared-utils/commit/11be5c693081aba252ab39576ec1c6f7fde5765c];;;","17/Jan/24 16:38;echauchot;[~Sergey Nuyanzin] I'm releasing flink-connector-parent and I'm reviewing the release notes. I think this ticket should be classified differently. 
I did:
- type = improvement instead of bug because this ticket is adding a new ability
- remove unrelated link to PR 23910
- this ticket touches only the CI and not the connector-parent pom. as it is related to connectors still I'd put Build/CI + connector/parent as components but remove the connector-parent fix version.

Feel free to change if you disagree.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report JobInitialization traces,FLINK-33775,13561036,13559950,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,07/Dec/23 17:41,18/Jan/24 09:13,04/Jun/24 20:40,05/Jan/24 16:25,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34134,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 05 16:25:31 UTC 2024,,,,,,,,,,"0|z1m4hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 16:25;pnowojski;merged commit 793a66b into apache:master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskSelectiveReadingTest.testReadFinishedInput(StreamTaskSelectiveReadingTest timed out,FLINK-33774,13561012,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,07/Dec/23 15:01,07/Dec/23 15:01,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Task,,,,0,github-actions,test-stability,,"https://github.com/XComp/flink/actions/runs/7095339465/job/19312312402#step:12:10737
{code:java}
Dec 05 04:16:47 ""main"" #1 prio=5 os_prio=0 cpu=1824106.94ms elapsed=1841.32s tid=0x00007fcf28c9c000 nid=0x1bd92 runnable  [0x00007fcf2c709000]
10737Dec 05 04:16:47    java.lang.Thread.State: RUNNABLE
10738Dec 05 04:16:47 	at java.lang.Thread.isAlive(java.base@11.0.19/Native Method)
10739Dec 05 04:16:47 	at org.apache.flink.streaming.runtime.tasks.StreamTaskTestHarness.waitForInputProcessing(StreamTaskTestHarness.java:434)
10740Dec 05 04:16:47 	at org.apache.flink.streaming.runtime.tasks.StreamTaskSelectiveReadingTest.testBase(StreamTaskSelectiveReadingTest.java:165)
10741Dec 05 04:16:47 	at org.apache.flink.streaming.runtime.tasks.StreamTaskSelectiveReadingTest.testReadFinishedInput(StreamTaskSelectiveReadingTest.java:118)
10742Dec 05 04:16:47 	at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(java.base@11.0.19/Native Method)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-07 15:01:17.0,,,,,,,,,,"0|z1m4c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add fairness to scaling decisions,FLINK-33773,13561007,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mxm,mxm,mxm,07/Dec/23 14:50,14/Mar/24 14:33,04/Jun/24 20:40,,,,,,,,,,kubernetes-operator-1.9.0,,,,Autoscaler,Deployment / Kubernetes,,,0,,,,"The current scaling logic is inherently unfair. In a scenario of heavy backlog, whichever pipelines come first, they will end up taking most of the resources. Some kind of fairness should be introduced, for example:

* Cap the max number of resulting pods at a % of the cluster resources
* Allow scale up round-robin across all pipelines",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-07 14:50:11.0,,,,,,,,,,"0|z1m4aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python e2e test fails with exit code 1,FLINK-33772,13561006,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,07/Dec/23 14:47,07/Dec/23 14:47,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,API / Python,Tests,,,0,github-actions,test-stability,,"[https://github.com/XComp/flink/actions/runs/7054951101/job/19204839470]
{code:java}
 org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1
6294	at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:134)
6295	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
6296	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
6297	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
6298	at java.lang.reflect.Method.invoke(Method.java:498)
6299	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
6300	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
6301	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:108)
6302	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
6303	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
6304	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
6305	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
6306	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
6307	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
6308	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
6309Caused by: java.lang.RuntimeException: Python process exits with code: 1
6310	at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:124)
6311	... 14 more{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-07 14:47:00.0,,,,,,,,,,"0|z1m4ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add cluster capacity awareness to Autoscaler,FLINK-33771,13561005,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,07/Dec/23 14:44,07/Feb/24 10:39,04/Jun/24 20:40,18/Jan/24 12:45,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"To avoid starvation of pipelines when the Kubernetes cluster runs out of resources, new scaling attempts should be stopped. 

The Rescaling API will probably prevent most of this cases but we will also have to double-check there. 

For the config-based parallelism overrides, we have pretty good heuristics in the operator to check in Kubernetes for the approximate number of free cluster resources, the max cluster scaleup for the Cluster Autoscaler, and the required scaling costs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 14:54:51 UTC 2023,,,,,,,,,,"0|z1m4ag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 14:54;gyfora;cc [~gsomogyi] this is somewhat related to the resource quota related work you have been doing. The 2 of you should sync;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler logs are full of deprecated key warnings,FLINK-33770,13561002,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,07/Dec/23 14:23,14/Dec/23 13:28,04/Jun/24 20:40,11/Dec/23 09:31,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"We moved all autoscaler configuration from {{kubernetes.operator.job.autoscaler.*}} to {{job.autoscaler.*}}. 

With the latest release, the logs are full with logs like this:

{noformat}
level:  WARN 
logger:  org.apache.flink.configuration.Configuration 
message:  Config uses deprecated configuration key 'kubernetes.operator.job.autoscaler.target.utilization' instead of proper key 'job.autoscaler.target.utilization' 
{noformat}

The reason is that the configuration is loaded for every reconciliation.

This configuration is already widely adopted across hundreds of pipelines. I propose to remove the deprecation from the config keys and make them ""fallback"" keys instead which removes the deprecation warning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 14:54:02 UTC 2023,,,,,,,,,,"0|z1m49s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 14:54;gyfora;Good idea!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ExternalSorter hits ""java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: java.io.EOFException: Can't collect further: memorySource depleted"" when using custom serializer",FLINK-33769,13560969,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vishalp,vishalp,07/Dec/23 10:14,07/Dec/23 10:17,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,Runtime / Task,,,,0,,,,"The [NormalizedKeySorter library|https://github.com/twitter-forks/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/operators/sort/NormalizedKeySorter.java] is used to sort records in-memory. It internally uses a [SimpleCollectingOutputView|https://github.com/twitter-forks/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/io/disk/SimpleCollectingOutputView.java] instantiated using a fixed chunk of managed memory to store the records. When the SimpleCollectingOutputView runs out of memory segments, it [throws an EOFException|https://github.com/twitter-forks/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/io/disk/SimpleCollectingOutputView.java#L76] which [should be caught by the sorter in the write method|https://github.com/twitter-forks/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/operators/sort/NormalizedKeySorter.java#L298]and a {{false}} indicating that the sort buffer was full (javadoc) should returned. The issue here is that the EOFException thrown by the SimpleCollectingOutputView is first caught by the record serializer which offers no guarantee on passing on the exception as it was caught upwards. In the case of Kryo and Thrift, the serializer wraps the caught exception in their own exception classes and throw them upwards which the sorter doesn't catch and the job crashes.
 
Example stacktrace -
{code}
java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: java.io.EOFException: Can't collect further: memorySource depleted
at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:487)
at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:357)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.util.WrappingRuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: java.io.EOFException: Can't collect further: memorySource depleted
at org.apache.flink.runtime.operators.sort.ExternalSorter.getIterator(ExternalSorter.java:262)
at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1222)
at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:105)
at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:479)
... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: java.io.EOFException: Can't collect further: memorySource depleted
at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
at org.apache.flink.runtime.operators.sort.ExternalSorter.getIterator(ExternalSorter.java:259)
... 9 more
Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: java.io.EOFException: Can't collect further: memorySource depleted
at org.apache.flink.runtime.operators.sort.ExternalSorter.lambda$getIterator$1(ExternalSorter.java:256)
at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986)
at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970)
at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
at org.apache.flink.runtime.operators.sort.ExternalSorterBuilder.lambda$doBuild$1(ExternalSorterBuilder.java:397)
at org.apache.flink.runtime.operators.sort.ThreadBase.internalHandleException(ThreadBase.java:121)
at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:75)
Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: java.io.EOFException: Can't collect further: memorySource depleted
at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:80)
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: Can't collect further: memorySource depleted
at com.esotericsoftware.kryo.io.Output.flush(Output.java:165)
at com.esotericsoftware.kryo.io.OutputChunked.flush(OutputChunked.java:45)
at com.esotericsoftware.kryo.io.OutputChunked.endChunks(OutputChunked.java:82)
at com.twitter.beam.coder.scala.ChillCoder.encode(ChillCoder.scala:101)
at com.twitter.eventwrangler.core.attribution.AttributionEventCoder.encode(AttributionEvent.scala:40)
at com.twitter.eventwrangler.core.attribution.AttributionEventCoder.encode(AttributionEvent.scala:22)
at org.apache.beam.sdk.coders.Coder.encode(Coder.java:136)
at org.apache.beam.sdk.transforms.join.UnionCoder.encode(UnionCoder.java:74)
at org.apache.beam.sdk.transforms.join.UnionCoder.encode(UnionCoder.java:32)
at org.apache.beam.sdk.coders.KvCoder.encode(KvCoder.java:73)
at org.apache.beam.sdk.coders.KvCoder.encode(KvCoder.java:37)
at org.apache.beam.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:607)
at org.apache.beam.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:598)
at org.apache.beam.sdk.util.WindowedValue$FullWindowedValueCoder.encode(WindowedValue.java:558)
at org.apache.beam.runners.flink.translation.types.CoderTypeSerializer.serialize(CoderTypeSerializer.java:110)
at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.serialize(TupleSerializer.java:140)
at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.serialize(TupleSerializer.java:37)
at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.write(NormalizedKeySorter.java:297)
at org.apache.flink.runtime.operators.sort.SorterInputGateway.writeRecord(SorterInputGateway.java:77)
at org.apache.flink.runtime.operators.sort.ReadingThread.go(ReadingThread.java:69)
at org.apache.flink.runtime.operators.sort.ThreadBase.run(ThreadBase.java:73)
Caused by: java.io.EOFException: Can't collect further: memorySource depleted
at org.apache.flink.runtime.io.disk.SimpleCollectingOutputView.nextSegment(SimpleCollectingOutputView.java:76)
at org.apache.flink.runtime.memory.AbstractPagedOutputView.advance(AbstractPagedOutputView.java:139)
at org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:205)
at org.apache.beam.runners.flink.translation.wrappers.DataOutputViewWrapper.write(DataOutputViewWrapper.java:44)
at com.esotericsoftware.kryo.io.Output.flush(Output.java:163)
... 20 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-07 10:14:17.0,,,,,,,,,,"0|z1m42g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-379: Support dynamic source parallelism inference for batch jobs,FLINK-33768,13560921,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,xiasun,xiasun,xiasun,07/Dec/23 01:58,07/Feb/24 10:02,04/Jun/24 20:40,01/Feb/24 09:31,1.19.0,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently, for JobVertices without parallelism configured, the AdaptiveBatchScheduler dynamically infers the vertex parallelism based on the volume of input data. Specifically, for Source vertices, it uses the value of `{*}execution.batch.adaptive.auto-parallelism.default-source-parallelism{*}` as the fixed parallelism. If this is not set by the user, the default value of {{1}}  is used as the source parallelism, which is actually a temporary implementation solution.

We aim to support dynamic source parallelism inference for batch jobs. More details see [FLIP-379|https://cwiki.apache.org/confluence/display/FLINK/FLIP-379%3A+Dynamic+source+parallelism+inference+for+batch+jobs].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34356,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 22 11:04:32 UTC 2024,,,,,,,,,,"0|z1m3rs:",9223372036854775807,"In Flink 1.19, we have supported dynamic source parallelism inference for batch jobs, which allows source connectors to dynamically infer the parallelism based on the actual amount of data to consume. This feature is a significant improvement over previous versions, which only assigned a fixed default parallelism to source vertices. 

Source connectors need to implement the inference interface to enable dynamic parallelism inference. Currently, the FileSource connector has already been developed with this functionality in place.

Additionally, the configuration `execution.batch.adaptive.auto-parallelism.default-source-parallelism` will be used as the upper bound of source parallelism inference. And now it will not default to 1. Instead, if it is not set, the upper bound of allowed parallelism set via `execution.batch.adaptive.auto-parallelism.max-parallelism` will be used. If that configuration is also not set, the default parallelism set via `parallelism.default` or StreamExecutionEnvironment#setParallelism() will be used instead.",,,,,,,,,,,,,,,,,,,"22/Jan/24 11:04;zhuzh;master/release-1.19:
c3c836216eaaaf24c1add3b490c8f425fda01d7c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for TemporalJoin node,FLINK-33767,13560894,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,06/Dec/23 22:42,15/Dec/23 10:05,04/Jun/24 20:40,15/Dec/23 10:05,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 10:05:24 UTC 2023,,,,,,,,,,"0|z1m3ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 10:05;dwysakowicz;Implemented in 46d817d8d297b50fe91b5fb9471bda791a6f4319..20a328d80a1dbc50974cf3de9f4b6178246f6dee;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support MockEnvironment in BroadcastOperatorTestHarness,FLINK-33766,13560892,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,koalalam,koalalam,06/Dec/23 22:16,06/Dec/23 22:16,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,Test Infrastructure,,,,0,,,,"Unlike KeyedOneInputStreamOperatorTestHarness, no constructor is available for providing MockEnvironment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-06 22:16:37.0,,,,,,,,,,"0|z1m3lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL to support COLLECT_LIST,FLINK-33765,13560869,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhenzhongxu,zhenzhongxu,06/Dec/23 19:46,07/Dec/23 07:49,04/Jun/24 20:40,07/Dec/23 07:49,,,,,,,,,,,,,API / DataSet,,,,0,,,,"Flink SQL currently supports COLLECT, which returns a multiset, however, given support for casting from multiset to other types (especially array/list) is *very* limited, see [here,|https://github.com/apache/flink/blob/master/docs/content/docs/dev/table/types.md#casting] this is creating lots of headaches for ease of use.

Can we support COLLECT_LIST as a built-in system function? (I believe Spark supports it)",,,,,,,,,,,,,,,,,,,,,,FLINK-21949,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 07:49:16 UTC 2023,,,,,,,,,,"0|z1m3g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 07:49;martijnvisser;[~zhenzhongxu] Please take a look at FLINK-21949 - We should support ARRAY_AGG, not COLLECT_LIST (but the idea is the same);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorporate GC / Heap metrics in autoscaler decisions,FLINK-33764,13560837,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,06/Dec/23 15:07,17/Dec/23 17:53,04/Jun/24 20:40,17/Dec/23 17:53,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"The autoscaler currently doesn't use any GC/HEAP metrics as part of the scaling decisions. 

While the long term goal may be to support vertical scaling (increasing TM sizes) currently this is out of scope for the autoscaler.

However it is very important to detect cases where the throughput of certain vertices or the entire pipeline is critically affected by long GC pauses. In these cases the current autoscaler logic would wrongly assume a low true processing rate and scale the pipeline too high, ramping up costs and causing further issues.

Using the improved GC metrics introduced in https://issues.apache.org/jira/browse/FLINK-33318 we should measure the GC pauses and simply block scaling decisions if the pipeline spends too much time garbage collecting and notify the user about the required action to increase memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 17 17:53:54 UTC 2023,,,,,,,,,,"0|z1m394:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Dec/23 17:53;gyfora;merged to main f6adb400e1c87f06faec948379c264eebba71166;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support manual savepoint redeploy for jobs and deployments,FLINK-33763,13560834,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,06/Dec/23 14:55,11/Dec/23 11:43,04/Jun/24 20:40,11/Dec/23 11:43,,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"A common request is to support a streamlined, user friendly way of redeploying from a target savepoint.

Previously this was only possible by deleting the CR and recreating it with initialSavepointPath. A big downside of this approach is a loss of savepoint/checkpoint history in the status that some platforms may need, resulting in non-cleaned up save points etc.

We suggest to introduce a `savepointRedeployNonce` field in the job spec similar to other action trigger nonces.

If the nonce changes to a new non null value the job will be redeployed from the path specified in the initialSavepointPath (or empty state If the path is empty)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 11:43:10 UTC 2023,,,,,,,,,,"0|z1m38g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 11:43;gyfora;merged to main 9e8dcd97cd07d4b769997ce40e43047301886c41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Versioned release of flink-connector-shared-utils python scripts,FLINK-33762,13560792,13557618,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,pvary,pvary,06/Dec/23 10:52,06/Dec/23 10:52,04/Jun/24 20:40,,,,,,,,,,,,,,API / Python,Connectors / Common,,,0,,,,We need a versioned release of the scripts stored in flink-connector-shared-utils/python directory. This will allow even incompatible changes for these scripts. The connector developers could chose which version of the scripts they depend on.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-06 10:52:41.0,,,,,,,,,,"0|z1m2z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snowflake as JDBC source,FLINK-33761,13560769,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,boris.litvak,boris.litvak,boris.litvak,06/Dec/23 07:34,07/May/24 06:50,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,"Following [https://flink.apache.org/how-to-contribute/contribute-code/,] I would like contribute JDBC source integration with Snowflake.

The implementation adds SnowflakeDialect and the relevant row converter and factory.

Need to:
 * Reach consensus
 * Find a committer willing to review and merge the change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-06 07:34:37.0,,,,,,,,,,"0|z1m2u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Group Window agg has different result when only consuming -D records while using or not using minibatch,FLINK-33760,13560760,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,xuyangzhong,xuyangzhong,06/Dec/23 06:06,25/Jan/24 02:05,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"Add the test in AggregateITCase to re-produce this bug.

 
{code:java}
@Test
def test(): Unit = {
  val upsertSourceCurrencyData = List(
    changelogRow(""-D"", 1.bigDecimal, ""a""),
    changelogRow(""-D"", 1.bigDecimal, ""b""),
    changelogRow(""-D"", 1.bigDecimal, ""b"")
  )

  val upsertSourceDataId = registerData(upsertSourceCurrencyData);
  tEnv.executeSql(s""""""
                     |CREATE TABLE T (
                     | `a` DECIMAL(32, 8),
                     | `d` STRING,
                     | proctime as proctime()
                     |) WITH (
                     | 'connector' = 'values',
                     | 'data-id' = '$upsertSourceDataId',
                     | 'changelog-mode' = 'I,UA,UB,D',
                     | 'failing-source' = 'true'
                     |)
                     |"""""".stripMargin)

  val sql =
    ""SELECT max(a), sum(a), min(a), TUMBLE_START(proctime, INTERVAL '0.005' SECOND), TUMBLE_END(proctime, INTERVAL '0.005' SECOND), d FROM T GROUP BY d, TUMBLE(proctime, INTERVAL '0.005' SECOND)""

  val sink = new TestingRetractSink
  tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink)
  env.execute()

  // Use the result precision/scale calculated for sum and don't override with the one calculated
  // for plus()/minus(), which results in loosing a decimal digit.
  val expected = List(""6.41671935,65947.23071935707000000000,609.02867403703699700000"")
  assertEquals(expected, sink.getRetractResults.sorted)
} {code}
When MiniBatch is ON, the result is `List()`.

 

When MiniBatch is OFF, the result is `List(null,-1.00000000,null,2023-12-06T11:29:21.895,2023-12-06T11:29:21.900,a)`.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 02:05:10 UTC 2024,,,,,,,,,,"0|z1m2s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 07:13;337361684@qq.com;[~xuyangzhong]  It looks like a bug,  I want to take this ticket! ;;;","25/Jan/24 02:05;337361684@qq.com;After offline discussion with [~lincoln.86xy]  and [~xuyangzhong]. The key to this question is whether to retract the data from the window that has already been sent, to ensure final consistency when -D data exists alone int the current window, or we just simply discard the retracted record. There's currently no final solution, a more detailed solution is needed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink parquet writer support write nested array or map type,FLINK-33759,13560757,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,cailiuyang,cailiuyang,06/Dec/23 05:24,27/May/24 22:08,04/Jun/24 20:40,27/May/24 22:08,1.19.0,,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"When we use flink-parquet format wirte Map<String, String>[] type (which will be read by spark job), we encounter an exception:
{code:java}
// code placeholder
Caused by: org.apache.parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
    at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endField(MessageColumnIO.java:329)
    at org.apache.flink.formats.parquet.row.ParquetRowDataWriter$ArrayWriter.writeArrayData(ParquetRowDataWriter.java:438)
    at org.apache.flink.formats.parquet.row.ParquetRowDataWriter$ArrayWriter.write(ParquetRowDataWriter.java:419)
    at org.apache.flink.formats.parquet.row.ParquetRowDataWriter$RowWriter.write(ParquetRowDataWriter.java:471)
    at org.apache.flink.formats.parquet.row.ParquetRowDataWriter.write(ParquetRowDataWriter.java:81)
    at org.apache.flink.formats.parquet.row.ParquetRowDataBuilder$ParquetWriteSupport.write(ParquetRowDataBuilder.java:89){code}
after review the code, we found flink-parquet doesn't support write nested array or map, because [[ArrayWriter|https://github.com/apache/flink/blob/master/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/row/ParquetRowDataWriter.java#L437]|#L437] and [MapWriter|https://github.com/apache/flink/blob/master/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/row/ParquetRowDataWriter.java#L391] doesn't impl `public void write(ArrayData arrayData, int ordinal) {}` function.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-17782,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 27 22:07:13 UTC 2024,,,,,,,,,,"0|z1m2rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/May/24 22:07;jingge;master: https://github.com/apache/flink/commit/57b20051a5aa6426d0a6ded71f5e0d550572428c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for TemporalSort node,FLINK-33758,13560730,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,05/Dec/23 23:04,07/Dec/23 13:00,04/Jun/24 20:40,07/Dec/23 12:52,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 12:52:13 UTC 2023,,,,,,,,,,"0|z1m2lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 12:52;dwysakowicz;Implemented in f751a00fd6f0e70187d2a9ae2ccd6a728d9a2c64;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Rank node,FLINK-33757,13560713,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,05/Dec/23 21:50,12/Dec/23 12:55,04/Jun/24 20:40,12/Dec/23 12:55,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 12:55:05 UTC 2023,,,,,,,,,,"0|z1m2hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 12:55;dwysakowicz;Implemented in 314b418efea8f35d39b05abef5361289b054b6a7..be5cf3c9d679ff141a1041774070c66b46b866a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing record with CUMULATE/HOP windows using an optimization,FLINK-33756,13560699,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jhughes,jhughes,05/Dec/23 19:05,14/Dec/23 23:38,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"I have seen an optimization cause a window fail to emit a record.

With the optimization `TABLE_OPTIMIZER_DISTINCT_AGG_SPLIT_ENABLED` set to true, 
the configuration AggregatePhaseStrategy.TWO_PHASE set, using a HOP or CUMULATE window with an offset, a record can be sent which causes one of the multiple active windows to fail to emit a record.

The linked code (https://github.com/jnh5y/flink/commit/ec90aa501d86f95559f8b22b0610e9fb786f05d4) modifies the `WindowAggregateJsonITCase` to demonstrate the case.  
 
The test `testDistinctSplitDisabled` shows the expected behavior.  The test `testDistinctSplitEnabled` tests the above configurations and shows that one record is missing from the output.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 23:38:43 UTC 2023,,,,,,,,,,"0|z1m2eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/23 16:41;jeyhunkarimov;Hi [~jhughes] I had a chance to look at the issue. I share my findings below.

So, when we enable {{OptimizerConfigOptions.TABLE_OPTIMIZER_DISTINCT_AGG_SPLIT_ENABLED}}, the following optimized execution plan is produced:


{code}
Sink(table=[default_catalog.default_database.MySink], fields=[name, $f1, $f2, window_start, window_end])
+- GlobalWindowAggregate(groupBy=[name], window=[CUMULATE(win_end=[$window_end], max_size=[15 s], step=[5 s], offset=[1 s])], select=[name, MAX(max$0) AS $f1, $SUM0(sum$1) AS $f2, start('w$) AS window_start, end('w$) AS window_end])
   +- Exchange(distribution=[hash[name]])
      +- LocalWindowAggregate(groupBy=[name], window=[CUMULATE(win_start=[window_start], win_end=[window_end], max_size=[15 s], step=[5 s], offset=[1 s])], select=[name, MAX($f5_0) AS max$0, $SUM0($f6_0) AS sum$1, slice_end('w$) AS $window_end])
         +- Calc(select=[name, window_start, window_end, $f5, $f6, $f3 AS $f5_0, $f4 AS $f6_0])
            +- GlobalWindowAggregate(groupBy=[name, $f5, $f6], window=[CUMULATE(slice_end=[$slice_end], max_size=[15 s], step=[5 s], offset=[1 s])], select=[name, $f5, $f6, MAX(max$0) AS $f3, COUNT(distinct$0 count$1) AS $f4, start('w$) AS window_start, end('w$) AS window_end])
               +- Exchange(distribution=[hash[name, $f5, $f6]])
                  +- LocalWindowAggregate(groupBy=[name, $f5, $f6], window=[CUMULATE(time_col=[rowtime], max_size=[15 s], step=[5 s], offset=[1 s])], select=[name, $f5, $f6, MAX(double) FILTER $g_1 AS max$0, COUNT(distinct$0 int) FILTER $g_2 AS count$1, DISTINCT(int) AS distinct$0, slice_end('w$) AS $slice_end])
                     +- Calc(select=[name, double, int, $f5, $f6, ($e = 1) AS $g_1, ($e = 2) AS $g_2, rowtime])
                        +- Expand(projects=[{name, double, int, $f5, null AS $f6, 1 AS $e, rowtime}, {name, double, int, null AS $f5, $f6, 2 AS $e, rowtime}])
                           +- Calc(select=[name, double, int, MOD(HASH_CODE(double), 1024) AS $f5, MOD(HASH_CODE(int), 1024) AS $f6, Reinterpret(TO_TIMESTAMP(ts)) AS rowtime])
                              +- TableSourceScan(table=[[default_catalog, default_database, MyTable, project=[int, double, name, ts], metadata=[], watermark=[-(TO_TIMESTAMP(ts), 1000:INTERVAL SECOND)], watermarkEmitStrategy=[on-periodic]]], fields=[int, double, name, ts])

{code}


As we see, there are two window operators (both with {{Local-Global optimization}} ). (Just to remember that the missing record is  - ""+I[b, 3.0, 1, 2020-10-10T00:00:31, 2020-10-10T00:00:41]"")

As we see from the schema of the second {{LocalWindowAggregate}}, it uses {{window_start}} and {{window_end}} to calculate {{CUMULATE}} windows. At this point (at the second {{LocalWindowAggregate}}), our ""missing"" record becomes like ""+I(b,2020-10-10T00:00:31,2020-10-10T00:00:41,0,null,3.0,0)"". So, at this point, we already lost the original event time of the record. 


As a result, the flaky behaviour happens because of the calling order between {{SlicingWindowOperator::processWatermark}}->{{AbstractWindowAggProcessor::advanceProgress}} and {{SlicingWindowOperator::processElement}}:

- If the {{processWatermark}} is called before the {{processElement}}, then the {{currentProgress}} is updated to {{1602288041000}}. In this case, once the {{processElement}} is called afterwards, it considers the window is already fired and drops the element

- If the {{processElement}} is called before the {{processWatermark}}, then the record processed as expected. 

Is this something expected? WDYT?
;;;","14/Dec/23 04:02;jhughes;Hi [~jeyhunkarimov], nice analysis!  I did see that there were two pairs of Local-Global window aggregates when I very briefly looked initially; I totally agree that has to be part of the issue.  

Out of curiosity, how did you see the value coming out of the various windows?  Was it println debugging or something else?

I like your explanation about the order of `processWatermark` and `processElement`; that explains the apparent flakiness.  

Looks like the different orderings is coming from the exchanging / hashing which is happening between the windows.  Perhaps thinking about how timestamps and the exchange operator will help us sort this out.  (Along with your note that we are ""losing"" the original timestamp in some sense.);;;","14/Dec/23 22:53;jeyhunkarimov;[~jhughes] Yes it is just printing/logging on various places of the codebase.
If you are not working on this issue (or if it is not sth urgent), you can assign it to me, will try to come up with deterministic solution to avoid the flakiness. ;;;","14/Dec/23 23:38;jhughes;Hi [~jeyhunkarimov], I am not actively working on it; I'll assign it to you for now.

I think there are two (likely related) things going on:  First, some watermark is getting miscomputed, and second, the hashing happening in the exchange step is allowing things to happen in either order.

From the initial time that I looked into this, I also ran across `TimeWindow.getWindowStartWithOffset`.  I noticed that this method is being called with an offset of 0L in `TimeWindowUtil.getNextTriggerWatermark`.  I cannot be 100% sure that's the problem, but that's the next place I'd be checking if I were to continue looking!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated StreamExecutionEnvironment#generateSequence,FLINK-33755,13560662,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,05/Dec/23 13:48,05/Dec/23 16:33,04/Jun/24 20:40,05/Dec/23 16:33,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 05 16:33:49 UTC 2023,,,,,,,,,,"0|z1m268:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/23 16:33;Sergey Nuyanzin;Merged to master as [e74592ca92f4eac5bef6e5140ae4e8cc2f0bf1a1|https://github.com/apache/flink/commit/e74592ca92f4eac5bef6e5140ae4e8cc2f0bf1a1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialize QueryOperations into SQL,FLINK-33754,13560649,13559333,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,05/Dec/23 12:00,14/Dec/23 11:35,04/Jun/24 20:40,12/Dec/23 18:05,,,,,,,,,1.19.0,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 11:35:59 UTC 2023,,,,,,,,,,"0|z1m23c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 18:05;dwysakowicz;Implemented in 3532f59cb9484a67e1b441e2875a26eb3691221f;;;","14/Dec/23 02:44;libenchao;[~dwysakowicz] Sorry to comment on a closed ticket. I saw you left out {{PlannerQueryOperation}} in the ticket, is this intentional? Or maybe you planned to add support for {{PlannerQueryOperation}} in another ticket? ;;;","14/Dec/23 11:24;dwysakowicz;[~libenchao] I overlooked this operation. Sorry, I will add support for that operation as well.;;;","14/Dec/23 11:35;libenchao;[~dwysakowicz] Looking forward to it :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ContinuousFileReaderOperator consume records as mini batch,FLINK-33753,13560647,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,prabhujoseph,prabhujoseph,05/Dec/23 11:47,06/Dec/23 15:37,04/Jun/24 20:40,06/Dec/23 15:37,1.16.0,,,,,,,,,,,,,,,,0,,,,"The ContinuousFileReaderOperator reads and collects the records from a split in a loop. If the split size is large, then the loop will take more time, and then the mailbox executor won't have a chance to process the checkpoint barrier. This leads to checkpoint timing out. ContinuousFileReaderOperator could be improved to consume the records in a mini batch, similar to Hudi's StreamReadOperator (https://issues.apache.org/jira/browse/HUDI-2485).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 11:47:35.0,,,,,,,,,,"0|z1m22w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When Duration is greater than or equal to 1 day, the display unit is ms.",FLINK-33752,13560645,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,05/Dec/23 11:46,06/Dec/23 05:31,04/Jun/24 20:40,06/Dec/23 05:31,1.18.0,,,,,,,,1.18.1,1.19.0,,,Runtime / Configuration,,,,0,pull-request-available,,,"When the default value of Duration is 24 hours or 1 day, the display unit is ms. (86400000 ms).

 

For example, the kubernetes operator doc has 3 options, their default value are 86400000 ms.

[https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/]

 

This bug from org.apache.flink.util.TimeUtils#formatWithHighestUnit, and it can be reproduced by TimeUtilsPrettyPrintingTest.

 

!image-2023-12-05-19-58-07-737.png|width=1247,height=957!

 

 

!image-2023-12-05-19-44-17-161.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/23 11:44;fanrui;image-2023-12-05-19-44-17-161.png;https://issues.apache.org/jira/secure/attachment/13064978/image-2023-12-05-19-44-17-161.png","05/Dec/23 11:58;fanrui;image-2023-12-05-19-58-07-737.png;https://issues.apache.org/jira/secure/attachment/13064979/image-2023-12-05-19-58-07-737.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 05:30:56 UTC 2023,,,,,,,,,,"0|z1m22g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 05:30;fanrui;Merged to

master<1.19> via: 9b61b137bdc7eff773847b84e5cde116e6280c1d and 360abe667a6f70782ed69f15e8c644d7962d301f

1.18 via: 6675de8210647038ef8f12dd538a7af189d77f49 and b3e3dce8df65670bf82f2e6cd936593bbccd1924;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use modules correctly when deserializing json plan,FLINK-33751,13560642,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,shuaiqi.guo,shuaiqi.guo,05/Dec/23 11:31,11/Dec/23 10:57,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"when serializing and deserializing SQL job by the following SQL Syntax:
{code:java}
COMPILE PLAN ...;
EXECUTE PLAN ...;{code}
if there are two modules in the environment, some bugs appeard when calling lookupOptionalSqlOperator():
 # if 2 Operators were found, it will return empty;
 # foundOperators is not ordered by modules order.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/23 11:32;shuaiqi.guo;FLINK-33751.patch;https://issues.apache.org/jira/secure/attachment/13064977/FLINK-33751.patch",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 10:57:49 UTC 2023,,,,,,,,,,"0|z1m21s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 03:25;shuaiqi.guo;Hi, [~libenchao] , could you have a review for this pr, thanks!;;;","11/Dec/23 10:57;libenchao;CC [~dwysakowicz] since I saw you have some activities on json plan related issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated config options.,FLINK-33750,13560605,13540672,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,JunRuiLi,JunRuiLi,JunRuiLi,05/Dec/23 07:11,05/Dec/23 07:13,04/Jun/24 20:40,,,,,,,,,,,,,,API / Core,,,,0,2.0-related,,,Remove deprecated config options in FLINK-2.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 07:11:59.0,,,,,,,,,,"0|z1m1tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove deprecated getter method in Configuration.,FLINK-33749,13560604,13540672,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,JunRuiLi,JunRuiLi,JunRuiLi,05/Dec/23 07:10,05/Dec/23 07:12,04/Jun/24 20:40,,,,,,,,,,,,,,API / Core,,,,0,2.0-related,,,"Currently, the Configuration has several getter methods for retrieving config values based on a String key, such as getString(String key, String defaultValue), and all of which have been deprecated.

We should remove these getter methods in FLINK-2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 07:10:14.0,,,,,,,,,,"0|z1m1tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove legacy TableSource/TableSink API in 2.0,FLINK-33748,13560599,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,05/Dec/23 06:29,05/Dec/23 06:33,04/Jun/24 20:40,,,,,,,,,,2.0.0,,,,Table SQL / API,,,,0,2.0-related,,,"{{TableSource}} and {{TableSink}} already marked as deprecated in FLINK-19453, and can be removed in 2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 06:29:57.0,,,,,,,,,,"0|z1m1s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Sink V1 API in 2.0,FLINK-33747,13560597,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Weijie Guo,Weijie Guo,05/Dec/23 06:23,05/Dec/23 06:26,04/Jun/24 20:40,,,,,,,,,,2.0.0,,,,Connectors / Common,,,,0,2.0-related,,,We have already mark sink v1 related API to `@deprecated` in 1.15. They can be removed in Flink 2.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 06:23:31.0,,,,,,,,,,"0|z1m1rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More precise dynamic selection of Hybrid Shuffle or AQE,FLINK-33746,13560589,13560581,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanyuxin,tanyuxin,05/Dec/23 05:24,05/Dec/23 05:24,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Coordination,Runtime / Network,,,0,,,,"We can even adopt more precise and intelligent strategies to select between Hybrid Shuffle and AQE. For instance, the choice could be made based on the edge type between tasks, or we could leverage historical job performance data and other metrics to inform our decision. Such tailored strategies would enable us to utilize each feature where it is most beneficial.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 05:24:43.0,,,,,,,,,,"0|z1m1q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamically choose hybrid shuffle or AQE in a job level,FLINK-33745,13560588,13560581,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanyuxin,tanyuxin,05/Dec/23 05:19,05/Dec/23 05:19,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Network,,,,0,,,,"To enhance the initial integration of Hybrid Shuffle with Adaptive Query Execution (AQE), we could implement a coarse-grained mode selection strategy. For instance, we can opt for either Hybrid Shuffle or AQE at the granularity level of an entire job. This approach would allow us to better align the two features in the early stages of adoption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 05:19:21.0,,,,,,,,,,"0|z1m1ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle avoids restarting the whole job when failover,FLINK-33744,13560587,13560581,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanyuxin,tanyuxin,05/Dec/23 05:16,05/Dec/23 05:16,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Network,,,,0,,,,"If Hybrid shuffle is enabled, the whole job will be restarted when failover. This is a critical issue for large-scale jobs. We should improve the logic and avoid restarting the whole job when failover.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 05:16:20.0,,,,,,,,,,"0|z1m1pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support consuming multiple subpartitions on a single channel,FLINK-33743,13560586,13560581,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,yunfengzhou,tanyuxin,tanyuxin,05/Dec/23 05:12,29/Jan/24 14:39,04/Jun/24 20:40,17/Jan/24 03:35,,,,,,,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"In Flink jobs that use the AdaptiveBatchScheduler and enable adaptive parallelism, a downstream operator might consume multiple subpartitions from an upstream operator. While downstream operators would create an InputChannel for each upstream subpartition in Flink's current implementation, The many InputChannels created in this situation may consume more memory resources than needed, affecting the usability of Hybrid Shuffle and AdaptiveBatchScheduler. In order to solve this problem, we plan to allow one InputChannel to consume multiple subpartitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34225,FLINK-34233,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 03:35:25 UTC 2024,,,,,,,,,,"0|z1m1pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/24 03:35;Weijie Guo;master(1.19) via a886339dbb36b11bc8bb96d35882223583ac3763.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Shuffle should work well with Adaptive Query Execution,FLINK-33742,13560581,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanyuxin,tanyuxin,05/Dec/23 04:12,05/Dec/23 05:13,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Coordination,Runtime / Network,,,0,Umbrella,,,"At present, Hybrid Shuffle and Adaptive Query Execution (AQE), which includes features such as Dynamic Partition Pruning (DPP), Runtime Filter, and Adaptive Batch Scheduler, are not fully compatible. While they can be used concurrently at the same time, the activation of AQE inhibits the key capability of Hybrid Shuffle to perform simultaneous reading and writing. This limitation arises because AQE dictates that downstream tasks may only initiate once upstream tasks have finished, a requirement that is inconsistent with the simultaneous read-write process facilitated by Hybrid Shuffle. In addition, Hybrid Shuffle will restart the whole job when failover, which is also an essential issue for production usage.

To harness the full potential of Hybrid Shuffle and AQE, it is essential to refine their integration. By doing so, we can capitalize on each feature's distinct advantages and enhance overall system performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-05 04:12:29.0,,,,,,,,,,"0|z1m1o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose Rocksdb Histogram statistics in Flink metrics ,FLINK-33741,13560578,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhoujira86,zhoujira86,zhoujira86,05/Dec/23 03:48,14/Jan/24 15:43,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,pull-request-available,,,I'd like to expose ROCKSDB Histogram metrics like db_get db_write to enable trouble shooting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24786,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 02:20:12 UTC 2023,,,,,,,,,,"0|z1m1nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/23 10:59;zhoujira86;And I think we can also parse the multi-line string of the rocksdb statistics. Then we can directly export these rt latency number in metrics.

 ;;;","11/Dec/23 02:20;yunta;[~zhoujira86] I think there exists valuable information in the RocksDB statistics, assigned to you, please go ahead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a document to list the supported sql patterns,FLINK-33740,13560478,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuyangzhong,xuyangzhong,04/Dec/23 13:17,05/Jan/24 03:09,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / API,,,,0,,,,"Introduce a flip with aligning sql usages to list all patterns we support and do not support. 

See more details in https://issues.apache.org/jira/browse/FLINK-33490",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33490,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 02 01:53:17 UTC 2024,,,,,,,,,,"0|z1m11c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/23 12:38;martijnvisser;[~xuyangzhong] Have we now fixed the ticket without a FLIP? If so, that would be opposite of how it should have happened (first get a FLIP approved, them merge the code);;;","02/Jan/24 01:53;xuyangzhong;Hi, [~martijnvisser] . Let's talk in FLINK-33490;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document FLIP-364: Improve the exponential-delay restart-strategy,FLINK-33739,13560429,13560425,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Dec/23 09:07,06/Feb/24 11:37,04/Jun/24 20:40,06/Feb/24 11:37,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 06 11:36:57 UTC 2024,,,,,,,,,,"0|z1m0qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:36;fanrui;Merged to master(1.19) via : 2f040e38eabacb24bba0907590ea4a4c3f91b9ec;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make exponential-delay restart-strategy the default restart strategy,FLINK-33738,13560428,13560425,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Dec/23 09:06,10/Jan/24 02:30,04/Jun/24 20:40,10/Jan/24 02:30,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 10 02:30:06 UTC 2024,,,,,,,,,,"0|z1m0q8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/24 02:30;fanrui;Merged to master(1.19) via: d9fc5ee03afe86e3d4c2fcee50616df2a7c095f2 and cad090aaed770c90facb6edbcce57dd341449a02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge multiple Exceptions into one attempt for exponential-delay restart-strategy,FLINK-33737,13560427,13560425,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Dec/23 09:05,08/Jan/24 02:19,04/Jun/24 20:40,08/Jan/24 02:19,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 02:19:28 UTC 2024,,,,,,,,,,"0|z1m0q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 02:19;fanrui;Merged to master(1.19) via : f9738d63391668396072570454fdc1eb61699098 and ef3cefda35428104af354fc3eb563afee58bf639;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update default value of exponential-delay.max-backoff and exponential-delay.backoff-multiplier,FLINK-33736,13560426,13560425,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Dec/23 09:03,26/Dec/23 02:56,04/Jun/24 20:40,26/Dec/23 02:56,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Update default value of exponential-delay.max-backoff from 5min to 1min.

Update default value of exponential-delay.backoff-multiplier from 2.0 to 1.5.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 26 02:56:19 UTC 2023,,,,,,,,,,"0|z1m0ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Dec/23 02:56;fanrui;Merged to master(1.19) via : d9e8b2a7be9c516bf497d4a68c39ac7f12e3b293;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-364: Improve the exponential-delay restart-strategy,FLINK-33735,13560425,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Dec/23 09:02,06/Feb/24 11:37,04/Jun/24 20:40,06/Feb/24 11:37,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,,,,"This is an umbrella Jira of [FLIP-364: Improve the exponential-delay restart-strategy.|https://cwiki.apache.org/confluence/x/uJqzDw]

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-04 09:02:26.0,,,,,,,,,,"0|z1m0pk:",9223372036854775807,"Flink 1.19 makes a series of improvements to exponential-delay restart-strategy, including: optimizing the default values of related options, support for max attempts, and solving the issue of inaccurate attempts in region failover. After these improvements, Flink 1.19 uses exponential-delay restart-strategy as the default restart-strategy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge unaligned checkpoint state handle,FLINK-33734,13560413,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Feifan Wang,Feifan Wang,Feifan Wang,04/Dec/23 07:40,07/Apr/24 05:45,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"h3. Background

Unaligned checkpoint will write the inflight-data of all InputChannel and ResultSubpartition of the same subtask to the same file during checkpoint. The InputChannelStateHandle and ResultSubpartitionStateHandle organize the metadata of inflight-data at the channel granularity, which causes the file name to be repeated many times. When a job is under backpressure and task parallelism is high, the metadata of unaligned checkpoints will bloat. This will result in:
 # The amount of data reported by taskmanager to jobmanager increases, and jobmanager takes longer to process these RPC requests.
 # The metadata of the entire checkpoint becomes very large, and it takes longer to serialize and write it to dfs.

Both of the above points ultimately lead to longer checkpoint duration.
h3. A Production example

Take our production job with a parallelism of 4800 as an example:
 # When there is no back pressure, checkpoint end-to-end duration is within 7 seconds.
 # When under pressure: checkpoint end-to-end duration often exceeds 1 minute. We found that jobmanager took more than 40 seconds to process rpc requests, and serialized metadata took more than 20 seconds.Some checkpoint statistics:
|metadata file size|950 MB|
|channel state count|12,229,854|
|channel file count|5536|

Of the 950MB in the metadata file, 68% are redundant file paths.

We enabled log-based checkpoint on this job and hoped that the checkpoint could be completed within 30 seconds. This problem made it difficult to achieve this goal.
h3. Propose changes

I suggest introducing MergedInputChannelStateHandle and MergedResultSubpartitionStateHandle to eliminate redundant file paths.

The taskmanager merges all InputChannelStateHandles with the same delegated StreamStateHandle in the same subtask into one MergedInputChannelStateHandle before reporting. When recovering from checkpoint, jobmangager converts MergedInputChannelStateHandle to InputChannelStateHandle collection before assigning state handle, and the rest of the process does not need to be changed. 

Structure of MergedInputChannelStateHandle :

 
{code:java}
{   // MergedInputChannelStateHandle
    ""delegate"": {
        ""filePath"": ""viewfs://hadoop-meituan/flink-yg15/checkpoints/retained/1234567/ab8d0c2f02a47586490b15e7a2c30555/chk-31/ffe54c0a-9b6e-4724-aae7-61b96bf8b1cf"",
        ""stateSize"": 123456
    },
    ""size"": 2000,
    ""subtaskIndex"":0,
    ""channels"": [ // One InputChannel per element
        {
            ""info"": {
                ""gateIdx"": 0,
                ""inputChannelIdx"": 0
            },
            ""offsets"": [
                100,200,300,400
            ],
            ""size"": 1400
        },
        {
            ""info"": {
                ""gateIdx"": 0,
                ""inputChannelIdx"": 1
            },
            ""offsets"": [
                500,600
            ],
            ""size"": 600
        }
    ]
}
 {code}
MergedResultSubpartitionStateHandle is similar.

 

 

WDYT [~roman] , [~pnowojski] , [~fanrui] ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/24 11:34;Feifan Wang;flamegraph.control-group.html;https://issues.apache.org/jira/secure/attachment/13066865/flamegraph.control-group.html","18/Feb/24 11:34;Feifan Wang;flamegraph.merge-handle-and-serialize-on-tm.html;https://issues.apache.org/jira/secure/attachment/13066866/flamegraph.merge-handle-and-serialize-on-tm.html","18/Feb/24 11:34;Feifan Wang;flamegraph.only-merge-handle.html;https://issues.apache.org/jira/secure/attachment/13066867/flamegraph.only-merge-handle.html","18/Feb/24 07:12;Feifan Wang;image-2024-02-18-15-12-20-665.png;https://issues.apache.org/jira/secure/attachment/13066861/image-2024-02-18-15-12-20-665.png",,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 07 05:45:12 UTC 2024,,,,,,,,,,"0|z1m0mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 04:48;zakelly;hi [~Feifan Wang] ,

 

I'm wondering if this proposal aims to reduce the meta size only and whether [FLIP-306|https://cwiki.apache.org/confluence/display/FLINK/FLIP-306%3A+Unified+File+Merging+Mechanism+for+Checkpoints] could alleviate this problem? And can this proposal work together with the [FLIP-306|https://cwiki.apache.org/confluence/display/FLINK/FLIP-306%3A+Unified+File+Merging+Mechanism+for+Checkpoints]?;;;","06/Dec/23 13:15;pnowojski;That's a good question [~Zakelly]. IIUC FLIP-306 focuses on merging files, but unless it also wants to change the structure of the state handles, it would suffer with the same problem as described here. But please correct me if I'm wrong.

Apart of that:
{quote}jobmangager converts MergedInputChannelStateHandle to InputChannelStateHandle collection before assigning state handle, and the rest of the process does not need to be changed. 
{quote}
Doesn't this mean we will have a very similar problem during recovery? That sending out the RPCs during recovery will take a long time? Wouldn't it be better to keep the state handles merged during recovery until they reach their destined subtasks on TMs?;;;","06/Dec/23 15:09;fanrui;Thanks [~Feifan Wang]  creating this  Jira, and thanks  [~pnowojski]  for the feedback!
{quote}Doesn't this mean we will have a very similar problem during recovery?
{quote}
Yes, you are right.

I have a offline discuss with [~Feifan Wang] in advance. If jobmangager converts MergedInputChannelStateHandle to InputChannelStateHandle collection before assigning state handle, the whole recovery logic won't be change, so this Jira is easy to implement.

Why [~Feifan Wang] and I think the checkpoint duration deserve more attention than recovery duration? Because the checkpoint is very frequent than recovery.

Of course, if we want to improve the recovery logic, it's fine for me. As I understand, the MergedInputChannelStateHandle can be used directly when the parallelism isn't changed. When the rescale happen, channel info should be stored with filePath together during recovery.
{quote} jobmangager converts MergedInputChannelStateHandle to InputChannelStateHandle collection before assigning state handle
{quote}
Or we can consider it as the stage one? And improve the recovery duration as the stage two? Look forward to more feedback from you, thanks~

----------------------------------------------------------------------------------------------------------------------------------

Also,  I have some questions about this proposal:
{quote}Of the 950MB in the metadata file, 68% are redundant file paths.
{quote}
Can we think the _metadata file size will be reduced 68% after this proposal?
{quote}Structure of MergedInputChannelStateHandle :
{quote}
How does flink serialize the MergedInputChannelStateHandle? Does it store the field name? If we change the inputChannelIdx to idx, can it reduce the file size? IIUC, the idx is very frequent in the metadata.

I'm thinking could we make other optimizations to make MergedInputChannelStateHandle simpler while ensuring that InputChannelStateHandle can be restored?;;;","07/Dec/23 03:48;Feifan Wang;Thanks to [~Zakelly] , [~pnowojski] and [~fanrui]  for participating in the discussion. 

 

to [~Zakelly] :

Yes, this proposal only aims to reduce the meta size of unaligned checkpoint. I also think that FLIP-306 does not solve the above problems. At the same time, I think my above proposal can work with FLIP-306.

 


to [~pnowojski] : 
{quote}That sending out the RPCs during recovery will take a long time? 
{quote}
Yes，in theory sending these rpc during recovery also takes a long time, but we have not paid attention to it before. First, because our job can accept a recovery time of several minutes from a business perspective. The second is that this kind of checkpoint only occurs during backpressure, and we have not tried to use this kind of checkpoint to restore the job.
{quote}Wouldn't it be better to keep the state handles merged during recovery until they reach their destined subtasks on TMs?
{quote}
I hold the same view with [~fanrui] on this issue. It is acceptable to me to solve the problems during checkpoint creation and recovery in two steps.

 

 

to [~fanrui] :
{quote}Can we think the _metadata file size will be reduced 68% after this proposal?
{quote}
Yes, but only for checkpoints where unaligned checkpoint handles account for the vast majority as mentioned above.
{quote}How does flink serialize the MergedInputChannelStateHandle? Does it store the field name? 
{quote}
The current serialization method of metadata objects is compact, and field names are not saved in the file. The serialization of each handle is hardcoded.

 

 ;;;","07/Dec/23 08:56;pnowojski;Recovery times are very often important. But yes, obviously checkpointing time is more crucial :) If you would be able to improve both that would be great! Of course this could be done in two steps. ;;;","07/Dec/23 11:57;Feifan Wang;We do also want to improve the speed of recovery, but currently checkpointing time is indeed the most important, so I completely agree with completing it in two steps.

 

When checking the source code, I found that the types of inputChannelState and resultSubpartitionState in OperatorSubtaskState are concrete classes instead of interfaces. And the serialization of these two handles does not use the handle type flag like other handles. This means that in order to introduce MergedInputChannelStateHandle , we may need to introduce MetadataV5Serializer . What do you think? Or do you have any other suggestions? [~pnowojski] ;;;","08/Dec/23 12:46;roman;Thanks for the proposal!

You're right [~Feifan Wang], we do need to add a new Metadata Serializer version. 

Actually, I think it should be possible to solve the original problem (big _metadata file) by only changing how these state handles are (de)serialized.
I.e. merge them in JM  in MetadataSerializer. That would be less efficient of course (as it wouldn't reduce RPC message size), but the code might be simpler and the change easier to implement. To be clear, I personally don't have any preference.

As for recovery, it's not only about performance, but also RPC message size: with checkpointing improved (but not recovery), RPC message size might be significantly larger on recovery than on checkpoint. We should probably document this (and add a feature toggle), until recovery is also optimized.

I have a couple of more questions:
1. Does it make sense to also merge state from multiple subtasks (as implemented in [FLINK-26803|https://issues.apache.org/jira/browse/FLINK-26803])?
2. What happens when the delegate is in-memory state handle (`ByteStreamStateHandle`)?;;;","11/Dec/23 07:54;Feifan Wang;Thanks [~roman] , merge handle in JM in MetadataSerializer can only reduce the size of metadata file, and dfs capacity tends not to be an issue. In the Production example I mentioned above, the main problem is that checkpointing takes too long. More specifically, we observed that it took more than 40 seconds for the JM to process the checkpoint ack rpc and more than 20 seconds to serialize the metadata object. So I still think that handles should be merged in TM.
{quote}1. Does it make sense to also merge state from multiple subtasks (as implemented in FLINK-26803)?
{quote}
Yes, since multiple subtasks will reuse unaligned checkpoint files after the ISSUE is completed, merging handles between multiple subtasks can further reduce redundant data. But this may require changing the way the checkpoint metadata objects are organized. And this optimization is constant level, but merging handles within subtask can reduce the number of file paths from n^2 to n. So I'm not sure if merging handles between subtasks is worth it at this stage.
{quote}2. What happens when the delegate is in-memory state handle (`ByteStreamStateHandle`)?
{quote}
IIUC, ByteStreamStateHandle in each InputChannelStateHandle/ResultSubpartitionStateHandle is exclusive and uses a random UUID as the handle name. I just looked at this code and saw that FLINK-17972 was created while [~roman]  were writing this code. I think the MergedInputChannelStateHandle mentioned above is an implementation of FLINK-17972. As for ByteStreamStateHandle, we may need to change channel granular splitting to subtask granular splitting. WDYT [~roman] ?;;;","13/Dec/23 17:53;pnowojski;{quote}

Yes, since multiple subtasks will reuse unaligned checkpoint files after the ISSUE is completed, merging handles between multiple subtasks can further reduce redundant data. But this may require changing the way the checkpoint metadata objects are organized. And this optimization is constant level, but merging handles within subtask can reduce the number of file paths from n^2 to n. So I'm not sure if merging handles between subtasks is worth it at this stage.

{quote}

+1 to merging within subtask first. I doubt that we will need to merge state handles across subtasks, as there is already tons of communication between JM and each subtask, but we can evaluate it later if it proves to be still an issue.;;;","02/Feb/24 11:57;Feifan Wang;I found that only merging handles was not enough. Although the metadata was obviously smaller, it still took a long time for jobmanager to process rpc and serialize metadata. I speculate the reason is that although the merge handle avoids duplicate file paths, but the number of objects in the metadata is not significantly reduced (mainly channel infos). So I tried serializing the channel infos directly on the taskmanager side, and the test results showed that this worked well. Below are the results of my test:
{code:java}
# control group :
checkpoint time         (    s ) --- avg: 82.73                max: 155.88               min: 49.90
store metadata time     (    s ) --- avg: 23.45                max: 47.60                min: 9.61
metadata size           (   MB ) --- avg: 696.55               max: 954.98               min: 461.35
store metadata speed    ( MB/s ) --- avg: 32.41                max: 61.82                min: 18.29

# only merge handle :
checkpoint time         (    s ) --- avg: 66.22                max: 123.12               min: 38.68
store metadata time     (    s ) --- avg: 12.76                max: 26.18                min: 4.02
metadata size           (   MB ) --- avg: 269.14               max: 394.26               min: 159.86
store metadata speed    ( MB/s ) --- avg: 23.93                max: 46.55                min: 11.33

# not only merge handles, but also serialize channel infos on TaskMangager :
checkpoint time         (    s ) --- avg: 30.63                max: 74.27                min: 5.16
store metadata time     (    s ) --- avg: 0.87                 max: 11.23                min: 0.12
metadata size           (   MB ) --- avg: 232.22               max: 392.86               min: 45.34
store metadata speed    ( MB/s ) --- avg: 291.00               max: 386.80               min: 23.18{code}
Based on the results of the above test, I think serializing channel infos on the taskmanger side should be done together. I submitted a PR to implement this solution, please have a look [~pnowojski] ,[~fanrui] ,[~roman] , [~Zakelly] .;;;","03/Feb/24 06:06;fanrui;Thanks [~Feifan Wang] for the effort. IIUC, the checkpoint avg time is reduced from 82 s to 30s, right? IMHO, I think the checkpoint time is still too long. Do you know why it still needs 30s? What's the bottleneck after your PR? Do we have any improvements for it?;;;","18/Feb/24 02:57;Feifan Wang;Sorry to keep you waiting [~fanrui] . Checkpoint production consists of multiple stages, this PR only solves the problem of metadata expansion. There are other stages that may take too long, such as uploading files to HDFS.  As for the bottleneck after this PR, I didn't take the time to investigate and if necessary, I would take a look.;;;","18/Feb/24 07:14;Feifan Wang;Hi [~fanrui] , I rerun the test job , and find some long ""Async Duration"". The checkpoint storage is HDFS, I think the bottleneck now should be long-tail latency.

!image-2024-02-18-15-12-20-665.png|width=860,height=200!;;;","18/Feb/24 07:48;roman;Thanks for the PR [~Feifan Wang] 

Do I understand correctly, that the improvement now comes mostly from moving `offsets` serialization from JM to TM?

I'm wondering whether it would also make sense to move storing the handle from JM to TM and only return a pointer to it. Do you have the numbers for RPC times?;;;","18/Feb/24 08:08;roman;As for
{quote}IIUC, ByteStreamStateHandle in each InputChannelStateHandle/ResultSubpartitionStateHandle is exclusive and uses a random UUID as the handle name. I just looked at this code and saw that FLINK-17972 was created while [~roman]  were writing this code. I think the MergedInputChannelStateHandle mentioned above is an implementation of FLINK-17972. 
{quote}
:D Yes, that's my understanding as well.

{quote}As for ByteStreamStateHandle, we may need to change channel granular splitting to subtask granular splitting. WDYT [~roman] ?{quote}
I'm not sure I understand. ByteStreamStateHandle is already created for subtask, and the bytes are extracted from it for every channel.;;;","18/Feb/24 08:29;zakelly;[~Feifan Wang] Sorry for the late reply as I was on vacation for a few weeks.

Overall it makes sense to offload file creation from JM to TM, especially in conjunction with the file merging mechanism in TM side introduced by FLIP-306. I thought it is better to show some detailed analysis about the performance enhancement and some explanation about the implementation as well as the test setup. I'm curious about why the checkpoint time reduced by 36s (from 66s to 30s) when serializing in TM side while the metadata time is only 12s. And how the long-tail of async duration happened, is it due to massive file creation requests and hotspot issue in NN node of HDFS?;;;","18/Feb/24 12:11;Feifan Wang;[~roman] :
{quote}Do I understand correctly, that the improvement now comes mostly from moving `offsets` serialization from JM to TM?
{quote}
Yes, 68% of the time reduction comes from offloading offset serialization to TM.
{quote}I'm wondering whether it would also make sense to move storing the handle from JM to TM and only return a pointer to it.
{quote}
I think it also make sense, but this method will make rescale more difficult.
{quote}Do you have the numbers for RPC times?
{quote}
I haven't tracked the processing time of rpc, so I don't have specific figures. The previous judgment that RPC took a long time came from the flame graph.

[^flamegraph.control-group.html]

[^flamegraph.only-merge-handle.html]

[^flamegraph.merge-handle-and-serialize-on-tm.html]
{quote}As for ByteStreamStateHandle, we may need to change channel granular splitting to subtask granular splitting.
{quote}
What I originally wanted to say here is that with the merged channel state handle, we don't need to extract bytes for each channel. This may not be critical at this point, so we can leave it alone.

 

[~Zakelly] :
{quote}Overall it makes sense to offload file creation from JM to TM
{quote}
In fact, this PR does not offload the file creation (metadata file) to TM, but only reorganizes the handle and offloads the serialization of the offset in the handle to TM.

The test that produced the above results was conducted based on FLINK-1.16.1. I constructed a job with 2000 parallelism and actively limited the throughput of the sink operator to simulate back pressure. The following figure is the topology diagram of the test job.

!https://km.sankuai.com/api/file/cdn/2059636095/80577277714?contentType=1&isNewContent=false|width=670,height=104!
{quote}I'm curious about why the checkpoint time reduced by 36s (from 66s to 30s) when serializing in TM side while the metadata time is only 12s.
{quote}
Time saved of metadata serialization is only 12s, but it can save more time of the RPC processing.
{quote}And how the long-tail of async duration happened, is it due to massive file creation requests and hotspot issue in NN node of HDFS?
{quote}
The test job running on a public HDFS, many other jobs running on it, there are many uncontrollable factors, and I did not investigate them carefully.;;;","25/Mar/24 13:00;Feifan Wang;Kindly ping [~Zakelly] , [~roman] .;;;","07/Apr/24 05:45;zakelly;I'd +1 for this optimization.;;;",,,,,,,,,,,,,,,,,,,,,,
[FLIP-321] Update the stability docs to add the migration periods.,FLINK-33733,13560402,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,becket_qin,becket_qin,becket_qin,04/Dec/23 06:07,15/Dec/23 08:17,04/Jun/24 20:40,15/Dec/23 08:17,1.19.0,,,,,,,,1.19.0,,,,Documentation,,,,0,,,,"This ticket updates the docs to add the migration period for deprecated APIs, which are specified in [FLIP-321.|https://cwiki.apache.org/confluence/display/FLINK/FLIP-321%3A+Introduce+an+API+deprecation+process]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 08:16:39 UTC 2023,,,,,,,,,,"0|z1m0kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 08:16;becket_qin;Just realized that I forgot to make the PR title start with FLINK-33733, causing the PR link missing from the Jira ticket...

The PR link and git commit hash are following:
PR link: [https://github.com/apache/flink/pull/23865]
Patch merged to master: d4a3687aacdea61920098dd7814776655fde19db;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Lookup Join ProjectPushDown will encouter ArrayIndexOutOfBoundsException,FLINK-33732,13560390,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lklhdu,lklhdu,04/Dec/23 02:54,04/Dec/23 06:00,04/Jun/24 20:40,04/Dec/23 06:00,,,,,,,,,,,,,,,,,0,,,,"Here is my sql:
```

SET 'execution.runtime-mode' = 'streaming';
SET 'table.dynamic-table-options.enabled' = 'true';
SET 'table.exec.source.cdc-events-duplicate' = 'false';
SET 'pipeline.operator-chaining' = 'false';

CREATE CATALOG catalog_hive WITH (
'type' = 'hive',

...
);

 

create table kafka_source(
 item1 STRING,
item2 INT,
item3 string,
PRIMARY KEY (item1, item2) NOT ENFORCED,
process_time as proctime()
– WATERMARK FOR `ts` AS ts - INTERVAL '10' SECOND
) WITH (
'connector' = 'kafka',
'format' = 'json',

...
);

 

CREATE TABLE blackhole_sink (
comp STRING,
order STRING,
order_line INT,
order_sequence INT,
material_code STRING,
warehouse_code STRING,
quantity DOUBLE
)WITH (
'connector' = 'blackhole'
);

 

insert into
blackhole_sink

select item1,comp from kafka_source a left join 

catalog_hive.db.hive_lookup_tb 

/*+ OPTIONS('streaming-source.partition.include'='latest',
'streaming-source.monitor-interval'='60 min','streaming-source.enable'='true') */
FOR SYSTEM_TIME AS OF kafka_source.process_time as b on a.item1 = b.comp;

```
The error stack is as follows：
java.io.IOException: java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: 10
    at com.netease.sloth.flink.connector.filesystem.table.meta.TableMetaStore.lambda$runSecured$1(TableMetaStore.java:700) ~[classes/:?]
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_332]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_332]
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754) ~[hadoop-common-2.7.5.jar:?]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 04 05:59:42 UTC 2023,,,,,,,,,,"0|z1m0hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/23 05:59;lklhdu;Duplicates the issue [FLINK-30679] Can not load the data of hive dim table when project-push-down is introduced - ASF JIRA (apache.org), so closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
failover.flip1 package can be rename to failover,FLINK-33731,13560387,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,04/Dec/23 02:42,04/Dec/23 08:33,04/Jun/24 20:40,04/Dec/23 08:33,1.17.2,1.18.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently, there is a org.apache.flink.runtime.executiongraph.failover.flip1 package.

I propose rename the failover.flip1 to failover, in the other word: removing the flip1. I have 2 reasons:
 * The naming of the package should be based on business semantics, not FLIP number, and the code under the failover.flip1 package has also many changes after FLIP-1.
 * All code under the failover.flip1 package are Internal code instead @Public code, so they can be renamed directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 04 08:33:17 UTC 2023,,,,,,,,,,"0|z1m0h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/23 02:43;fanrui;Hi [~zhuzh] , what do you think?;;;","04/Dec/23 07:45;zhuzh;LGTM. It is a legacy issue. When the new failover logics were introduced, there were legacy classes directly in package {{org.apache.flink.runtime.executiongraph.failover}}, so the sub-package was introduced to avoid mixing them up. 
Now that the legacy failover classes are removed already and it's time to do the renaming.;;;","04/Dec/23 08:33;fanrui;Merged master<1.19> via: 53367b0890000c954db4a60b00f515d6c3f7d432;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the compatibility table to only include last three released versions,FLINK-33730,13560361,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jingge,jingge,jingge,03/Dec/23 17:35,13/Dec/23 09:23,04/Jun/24 20:40,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,"Update the compatibility table ([apache-flink:./docs/content/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content/docs/ops/upgrading.md#compatibility-table] and [apache-flink:./docs/content.zh/docs/ops/upgrading.md|https://github.com/apache/flink/blob/master/docs/content.zh/docs/ops/upgrading.md#compatibility-table]) according to the discussion[1].

 

[1] https://lists.apache.org/thread/7yx396x5lmtws0s4t0sf9f2psgny11d6

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-03 17:35:16.0,,,,,,,,,,"0|z1m0bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Events are getting lost when an exception occurs within a processing function,FLINK-33729,13560355,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rtrojczak,rtrojczak,03/Dec/23 14:53,15/Apr/24 10:03,04/Jun/24 20:40,,1.15.3,,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,"We have a Flink job using a Pulsar source that reads from an input topic, and a Pulsar sink that is writing to an output topic.  Both Flink and Pulsar connector are of version 1.15.3. The Pulsar version that I use is 2.10.3.

Here is a simple project that is intended to reproduce this problem: [https://github.com/trojczak/flink-pulsar-connector-problem/]

All of my tests were done on my local Kubernetes cluster using the Flink Kubernetes Operator and Pulsar is running on  my local Docker. But the same problem occurred on a ""normal"" cluster.

Expected behavior: When an exception is thrown within the code (or a TaskManager pod is restarted for any other reason, e.g. OOM exception), the processing should be picked up from the last event sent to the output topic.

Actual behavior: The events before the failure are sent correctly to the output topic, next some of the events from the input topic are missing, then from some point the events are being processed normally until the next exception is thrown, and so on. Finally, from 100 events that should be sent from the input topic to the output topic, only 40 are sent.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 10:03:21 UTC 2024,,,,,,,,,,"0|z1m0a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/23 09:58;Weijie Guo;Thanks for reporting this! [~Tison] Could you help confirm this is a valid ticket?;;;","15/Apr/24 10:03;syhily;[~Weijie Guo] Yep, this is a valid ticket. But it's not from the Flink side. We need to figure out why the transaction didn't works on Pulsar. So I think this ticket should be submitted to Pulsar community.

[~rtrojczak] I can't find any checkpoint configuration from your sample code. I can only see two line of codes that enable checkpoint. I think this is the main reason that your code fails as expect. Flink checkpoint needs a lot of configuration to use. Such as the storage. You can check the link below to get your application proper configured.

https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/#enabling-and-configuring-checkpointing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not rewatch when KubernetesResourceManagerDriver watch fail,FLINK-33728,13560354,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,zhoujira86,zhoujira86,zhoujira86,03/Dec/23 14:38,22/Feb/24 01:21,04/Jun/24 20:40,22/Feb/24 01:20,,,,,,,,,1.20.0,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,"I met massive production problem when kubernetes ETCD slow responding happen. After Kube recoverd after 1 hour, Thousands of Flink jobs using kubernetesResourceManagerDriver rewatched when recieving ResourceVersionTooOld,  which caused great pressure on API Server and made API server failed again... 

 

I am not sure is it necessary to

getResourceEventHandler().onError(throwable)

in  PodCallbackHandlerImpl# handleError method?

 

We can just neglect the disconnection of watching process. and try to rewatch once new requestResource called. And we can leverage on the akka heartbeat timeout to discover the TM failure, just like YARN mode do.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-20417,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 01:20:42 UTC 2024,,,,,,,,,,"0|z1m09s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/23 03:18;zhoujira86;[~wangyang0918] [~mapohl] [~gyfora] Would you please let me know you thinking?;;;","08/Dec/23 14:08;gyfora;So you mean whenever the watch disconnects we completely abandon that pod?;;;","09/Dec/23 02:14;zhoujira86;[~gyfora] my proposal is keep the jobmanager running after watch fail, and do not rewatch before next request resource called. 

A healthy watch listener can get notification from kubernetes of two kind:

add pod and delete pod.

1. add pod is necessary when request resource, when we are not requesting resource, this notification is allowed to be lost.

2. delete pod can allow us detect pod failure more quickly, but we can also discover it by detecting the lost of akka heartbeat timeout.

 

according to the statement above, we can tolerate the lost of watch connection when we are not requesting resource;;;","11/Dec/23 17:20;mapohl;Thanks for creating this Jira issue, [~zhoujira86]. AFAIU, you're proposing the lazy initialization of the watcher after an connection error occurred that left the resourceVersion in an out-dated state (i.e. the resourceVersion which is used by the k8s client doesn't match any pod in the k8s cluster). Re-initialization of the watcher wouldn't happen when the error is detected but when Flink realizes that the TM is gone and initiates a new TM pod.

Correct me if I'm wrong here but isn't the watcher watching multiple pods (all TM pods belonging to the Flink cluster) and the {{KubernetesTooOldResourceVersionException}} can be triggered by an error coming from a single pod? If that's the case, not re-initializing the watcher right away would leave us hanging for other pods' lifecycle events wouldn't it? We would lose the ability to detect the deletion of other pods. But I guess that's what you mean in your comment above with ""delete pod can allow us detect pod failure more quickly, but we can also discover it by detecting the lost of akka heartbeat timeout.""?!;;;","12/Dec/23 02:38;zhoujira86;Hi [~mapohl] , thanks for the comment above. sorry for my poor writing english :P, but I think your re-clarification  is exactly what I am proposing. I'd like to introduce a lazy re-initialization of watch mechanism which will tolerate a disconnection of the watch until a new POD is requested.

And I think your concern is how we detect a TM loss without a active watcher.  I have test my change in a real K8S environment. With a disconnected watcher, I killed a TM pod. after no more than 50s, the task restarted with a exception
{code:java}
// code placeholder
 java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id flink-6168d34cf9d3a5d31ad8bb02bce6a370-taskmanager-1-8 timed out.
	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1306)
	at org.apache.flink.runtime.heartbeat.HeartbeatMonitorImpl.run(HeartbeatMonitorImpl.java:111)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitC {code}
moreover, I think YARN also do not have a watcher mechanism, so FLINK scheduled in yarn also relays on a heartbeat timeout mechanism? 

 

And an active rewatching strategy can really cause great pressure on API server, especially in the early versions without the resource version zero set in the watch-list request.;;;","20/Dec/23 02:17;zhoujira86;[~mapohl] Hi Matthias , would you please let me know what additional test is needed to prove my proposal can move forward.;;;","22/Dec/23 20:26;mapohl;Sorry for not being responsive on that one. I was on sick leave the past two weeks. I hope to get back on that issue after the upcoming holiday week.;;;","08/Jan/24 09:46;zhoujira86;Hi Matthias, wish you had recovered and enjoyed a wonderful Holiday :).  Can we have a discussion on my proposal [~mapohl] ;;;","08/Jan/24 16:22;mapohl;Hi [~zhoujira86], thanks for your patience. I managed to look into the issue once more. I'm not sure whether your proposal would work. As mentioned already in previous comments, we would lose the pods that are terminated. This might be handled by the TaskManager's heartbeats in some way. But I am concerned about the termination logic of a pod: The termination event for pods calls [onPodTerminated|https://github.com/apache/flink/blob/b865151d23ef92879941a63f40c9fac7c6b9b98c/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManagerDriver.java#L416] which deals with cleaning up the corresponding resource request (see [KubernetesResourceManagerDriver:424ff|https://github.com/apache/flink/blob/b865151d23ef92879941a63f40c9fac7c6b9b98c/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/KubernetesResourceManagerDriver.java#L424]. To me it looks like there could be a scenario where we do not complete open requests properly.

That seems to be an edge case because we need to lose the watcher after the request was initiated but before the request is fulfilled, if I understand the code correctly. But anyway, that sounds like a possible memory issue due to missing cleanup. [~xtsong] can you help on this issue?;;;","09/Jan/24 02:43;zhoujira86;[~mapohl] I think your concern is really very important. I think my statement is not good enough. After your reminder, I'd like to change it to ：

 

We can just neglect the disconnection of watching process {color:#FF0000}if there is no pending request{color}. and try to rewatch once new requestResource called.

 

And we can choose to fail all CompletableFuture And the [requestWorkerIfRequired|https://github.com/apache/flink/blob/2b9b9859253698c3c90ca420f10975e27e6c52d4/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/active/ActiveResourceManager.java#L332] will request the resource again, this will trigger the rewatch.

 

WDYT [~mapohl] [~xtsong] ;;;","09/Jan/24 04:09;xtsong;Thanks for pulling me in.

I'm also concerned about solely relying on heartbeat timeout to detecting pod failure. In addition the cleaning-up issue, it can also delay the detection of pod failure in many cases.

IIUC, the problem we are trying to solve here is to avoid massive Flink jobs trying to re-creating watches at the same time. That doesn't necessarily result in the proposed solution.
1. I think this is not a problem of individual Flink jobs, but a problem of the K8s cluster that runs massive Flink workloads. Ideally, such problems, i.e. how to better deal with the massive workloads, should be solved on the K8s cluster side. However, I don't have the expertise to come up with a cluster-side solution.
2. If 1) is not feasible, I think we can introduce a random backoff. User may configure a max backoff time (default 0), and Flink randomly pick a time that is no greater than the max to re-create the watch. Ideally, that would spread the pressure on API server over a longer and configurable period.

WDYT?;;;","09/Jan/24 08:51;zhoujira86;[~xtsong] In a default FLINK setting, when the KubenetesClient  disconnects from KUBE API server, it will try to reconnect for infinitely times. As kubernetes.watch.reconnectLimit is -1. But KubenetesClient treat ResourceVersionTooOld as a special exception, as it will escape from the normal reconnects. And then it will cause FLINK FlinkKubeClient to retry connect for kubernetes.transactional-operation.max-retries times, and these retries have not interval between them. If the watcher does not recover, the JM will kill it self.

 

So I think the problem we are trying to solve is not only to avoid massive Flink jobs trying to re-creating watches at the same time.  But also how to allow FLINK to continue running even when the KUBE API SERVER is in a disorder situation. As for most of the times, FLINK TMs have no dependency on API SERVER.

 

If you think it is not acceptable to recover the watcher only requesting resource, I think another possible way is , we can retry to rewatch pods periodically.

 

WDYT? :) ;;;","15/Jan/24 01:59;xtsong;Sorry for the late reply, I was distracted by some other works last week.

I think you are right about that JM will kill itself if the re-watch does not succeed. I think it is expected in most cases that the client try re-watch immediately after seeing a ResourceVersionTooOld exception. However, if the first attempt to re-watch fail, JM should not kill itself immediately, but may retry with some backoff interval.

cc [~wangyang0918];;;","15/Jan/24 02:21;wangyang0918;Not only the {{KubernetesResourceManagerDriver}} will create a new watch when received the {{{}TooOldResourceVersion{}}}, but also the fabric8 K8s client has the similar logic in {{{}Reflector.java{}}}[1], which we are using for the Flink Kubernetes HA implementation.

 

In my opinion, the K8s APIServer should have the ability to protect itself by using the flow control[2]. Then it will reject some requests if it could not process too many requests. Flink will then retry to create a new watch when the previous one failed. What Flink could do more is using a {{ExponentialBackoffDelayRetryStrategy}} to replace current continuous retry strategy.

 

[1]. [https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/informers/impl/cache/Reflector.java#L288]

[2]. [https://kubernetes.io/docs/concepts/cluster-administration/flow-control/]

 ;;;","15/Jan/24 06:08;zhoujira86;[~xtsong] [~wangyang0918] Ok, glad to hear that. Would you please help assign the ticket to me?;;;","15/Jan/24 06:17;xtsong;Sure, thanks for volunteering working on this. I've assigned you to the ticket. Please go ahead.;;;","22/Feb/24 01:20;xtsong;master (1.20): e7e31a99d6f93d4dadda21fbd1ebee079fe2418e;;;",,,,,,,,,,,,,,,,,,,,,,,,
JoinRestoreTest is failing on AZP,FLINK-33727,13560314,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,02/Dec/23 14:15,07/Dec/23 14:32,04/Jun/24 20:40,04/Dec/23 06:16,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,test-stability,,"Since {{JoinRestoreTest}} was introduced in FLINK-33470 it seems to be a reason

{noformat}
Dec 02 04:42:26 04:42:26.408 [ERROR] Failures: 
Dec 02 04:42:26 04:42:26.408 [ERROR]   JoinRestoreTest>RestoreTestBase.testRestore:283 
Dec 02 04:42:26 Expecting actual:
Dec 02 04:42:26   [""+I[9, carol, apple, 9000]"",
Dec 02 04:42:26     ""+I[8, bill, banana, 8000]"",
Dec 02 04:42:26     ""+I[6, jerry, pen, 6000]""]
Dec 02 04:42:26 to contain exactly in any order:
Dec 02 04:42:26   [""+I[Adam, null]"",
Dec 02 04:42:26     ""+I[Baker, Research]"",
Dec 02 04:42:26     ""+I[Charlie, Human Resources]"",
Dec 02 04:42:26     ""+I[Charlie, HR]"",
Dec 02 04:42:26     ""+I[Don, Sales]"",
Dec 02 04:42:26     ""+I[Victor, null]"",
Dec 02 04:42:26     ""+I[Helena, Engineering]"",
Dec 02 04:42:26     ""+I[Juliet, Engineering]"",
Dec 02 04:42:26     ""+I[Ivana, Research]"",
Dec 02 04:42:26     ""+I[Charlie, People Operations]""]
{noformat}
examples of failures

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55120&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12099
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55129&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11786
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55136&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12099
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55137&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11779",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33470,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 14:32:04 UTC 2023,,,,,,,,,,"0|z1m00w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/23 14:16;Sergey Nuyanzin;[~dwysakowicz], [~jhughes] could you please have a look here please?;;;","02/Dec/23 17:58;jhughes;From a quick look, the data is coming from `DeduplicationTestPrograms.java`.  

I believe that this shows that the various `RestoreTest`s are being executed concurrently and are interfering with each other.  

Two obvious ideas would be:
1. Have each RestoreTest use differently named sinks/sources.  (Right now, the DeduplicationTestPrograms and JoinTestPrograms both use sinks called ""MySink"".)
2. Do something at the JUnit level so that implementations of RestoreTestBase do not run concurrently.

Thoughts?;;;","02/Dec/23 22:01;Sergey Nuyanzin;I don't think it is related to concurrent execution.

I was able to find a way to reproduce it locally with 100%.
Just open IntellijIDEA and run all tests for {{RestoreTestBase}}

Even  more, I started commenting tests and realised if there at least one test e.g. {{ExpandRestoreTest}} before {{JoinRestoreTest}} then {{JoinRestoreTest}} fails with 100% at least for my env. If I comment out also {{ExpandRestoreTest}} then it starts passing.
It seems it relies on some internal state...
;;;","02/Dec/23 23:23;Sergey Nuyanzin;Based on local tests seems these {{MySink}} is kind of ""state holder"" for the tests...

Renaming helps
{quote}
  (Right now, the DeduplicationTestPrograms and JoinTestPrograms both use sinks called ""MySink"".)
{quote}
is there any reason to have same name?

Also I guess it is worth noting: there is a number of other tests with same potential issue e.g. with sink name ""sink_t"", probably something else;;;","03/Dec/23 09:12;Sergey Nuyanzin;[~jhughes] there is also a question about {{DeduplicationRestoreTest}}
what is the reason to have {{runSql}} this in {{DeduplicationTestPrograms}} 
e.g. for {{org.apache.flink.table.planner.plan.nodes.exec.stream.DeduplicationTestPrograms#DEDUPLICATE}}
{code:java}
.runSql(
    ""insert into deduplicate_sink ""
    + ""select order_id, user, product, order_time \n""
    + ""FROM (""
    + ""  SELECT *,""
    + ""    ROW_NUMBER() OVER (PARTITION BY product ORDER BY event_time ASC) AS row_num\n""
    + ""  FROM MyTable)""
    + ""WHERE row_num = 1"")
{code}

I'm asking since I tried to remove it just to see what happens and tests just continue passing..
So from tests point of view it seems there is no difference whether we have this or not for {{DeduplicationTestPrograms}} 
Or did I miss anything?;;;","03/Dec/23 14:19;jhughes;> is there any reason to have same name?

Yes and no.  The SQL text will need to reference the input and output tables.  In other restore tests, it makes sense to have functions which generate sources / sinks, so being able to reuse a table name is nice.

> what is the reason to have {{runSql}} this in {{DeduplicationTestPrograms?}}
`runSql` adds the steps which will actually execute something to the TestProgram.  

I'm pretty sure that if you removed the run SQL, that'd be like removing the section in a JUnit test function which does anything and then asserts that it works.  (That'd explain why the tests pass without it.)

In some sense, I see the TestProgram and RestoreTestBase as setting up a Builder/DSL for JUnit tests that a) test things about CompiledPlans and b) make sure that a streaming can be restored sensibly.  ;;;","03/Dec/23 14:26;jhughes;{quote}...
It seems it relies on some internal state...
{quote}
The tests have internal state.  Your testing shows that it not being reset between test classes!

Thanks for digging into that; that will help us identify what we need to sort out with the TestRestoreBase.  

If you are looking to sort things immediately, I'd suggest adding `Disabled` to `testRestore` here: [https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/testutils/RestoreTestBase.java#L229]

That'd turn off all of these tests until [~twalthr] [~dwysakowicz] [~bvarghese] and I have a solution.[https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dwysakowicz];;;","03/Dec/23 14:30;Sergey Nuyanzin;{quote}
I'm pretty sure that if you removed the run SQL, that'd be like removing the section in a JUnit test function which does anything and then asserts that it works.  (That'd explain why the tests pass without it.)
{quote}
I don't understand it

as another experiment I just changed SQL to the wrong SQL with syntax error.
e.g. just add a couple of commas in wrong places
{code:java}
.runSql(
    ""insert, into, deduplicate_sink ""
    + ""select order_id, user, product, order_time \n""
    + ""FROM (""
    + ""  SELECT *,""
    + ""    ROW_NUMBER() OVER (PARTITION BY product ORDER BY event_time ASC) AS row_num\n""
    + ""  FROM MyTable)""
    + ""WHERE row_num = 1"")
{code}
, however tests are continuing passing. Is it expected?
Then it's is not clear what is the reason to have it and how to check that it tests what it is expected to test?

{quote}
If you are looking to sort things immediately, I'd suggest adding `Disabled` to `testRestore` here: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/testutils/RestoreTestBase.java#L229
{quote}
what is wrong with current PR for this JIRA?;;;","03/Dec/23 14:47;jhughes;{quote}..., however tests are continuing passing. Is it expected?
{quote}
""Yes"", but the reason is a little confusing.

The RestoreTest framework has two methods:  `generateTestSetupFiles` and `testRestore`.  

Presently, the method `generateTestSetupFiles` is disabled and only run by test developers before a PR is submitted.  This method takes the SQL, gets and saves a compiled plan, and runs through the beforeRestore data making some comparisons, and finally stopping the job and taking a savepoint.  

The second method uses the compiled plan and the savepoint.

Since you are only running the second method, changing the SQL is irrelevant and not tested (unless you manually run `generateTestSetupFiles`).
{quote}what is wrong with current PR for this JIRA?
{quote}
CI failing is showing that the RestoreTestBase has some limitations/assumptions around state which we need to address.  The current PR fixes CI, but does not address those, rather it works around them.  I'd prefer that we fix the limitations rather than work around them.  That's why I'm suggesting to disable the RestoreTests as a whole until Monday when Dawid and Timo can weigh in.

 ;;;","03/Dec/23 14:55;jhughes;As an alternative, to the existing PR and disabling the RestoreTests, I'm totally fine with you reverting the commits from my PR:
[https://github.com/apache/flink/pull/23680] 
[https://github.com/apache/flink/commit/e886dfdda6cd927548c8af0a88e78171e7ba34a8]
[https://github.com/apache/flink/commit/5edc7d7b18e88cc86e84d197202d8cbb40621864] ;;;","03/Dec/23 14:56;Sergey Nuyanzin;{quote}
Presently, the method `generateTestSetupFiles` is disabled and only run by test developers before a PR is submitted.  This method takes the SQL, gets and saves a compiled plan, and runs through the beforeRestore data making some comparisons, and finally stopping the job and taking a savepoint.  
{quote}
if so I'm curious whether it would be more helpful for others to have at least a comment about that in sources?
IMHO it could help others who is editing the code around however not aware of it and since currently there is no any such info in code and the code is not a part of ""turned on"" tests, then it could be simply be broken and neither contributor nor reviewer could notice that ;;;","03/Dec/23 15:12;jhughes;{quote}if so I'm curious whether it would be more helpful for others to have at least a comment about that in sources
{quote}
Absolutely!  The RestoreTest framework is new, and this discussion shows that there are a number of non-obvious assumptions.  [~bvarghese] and I are new to Flink, and as we've worked with it, we have extended it to have the features necessary to test various capabilities.

I apologize for taking out CI!  The order merges resulted in PRs not testing all of the programs together in one branch until they were merged.  (Which is why I'm willing to suggest disabling all RestoreTests temporarily or reverting the commits which caused the issue.)

As an additional improvement to the RestoreTestBase, we could save the SQL text in a file and fail if it is changed.  Of course, then folks could still update the test files which ought to be ""immutable"" in some sense.;;;","03/Dec/23 23:08;Sergey Nuyanzin;{quote}
I apologize for taking out CI!  The order merges resulted in PRs not testing all of the programs together in one branch until they were merged.  (Which is why I'm willing to suggest disabling all RestoreTests temporarily or reverting the commits which caused the issue.)
{quote}
I noticed that within FLINK-33470 there was a deletion of {{JoinJsonPlanTest}} and {{JoinJsonPlanITCase}} and i would guess the idea was to replace it with new tests... In this case just disabling of new tests is not an option since in that case we are in a situation when old tests are removed and new are still not ready. 
If revert is more preferable option for you then current PR, then ok I created a revert PR https://github.com/apache/flink/pull/23861 and if it is ok we can close the PR for this issue and commit revert  PR and reopen FLINK-33470 where further work could be continued;;;","04/Dec/23 00:03;jhughes;That works for me; I approved that PR.;;;","04/Dec/23 06:16;Sergey Nuyanzin;reverted at  18b67b104e025b142a8321e5163edf7fbd439580 and 026bd4be9bafce86ced42d2a07e8b8820f7e6d9d;;;","04/Dec/23 10:06;dwysakowicz;Thanks for taking care of it [~Sergey Nuyanzin] and sorry for the problems. ;;;","07/Dec/23 14:32;mapohl;The following build failed before the revert was pushed: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55160&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;",,,,,,,,,,,,,,,,,,,,,,,,
Print cost time for stream queries in SQL Client,FLINK-33726,13560306,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,02/Dec/23 10:08,08/Dec/23 16:58,04/Jun/24 20:40,08/Dec/23 16:58,,,,,,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,"The time cost information is expected when executing stream queries in SQL CLI. 

For example:
{code:java}
Flink SQL> select * from (values ('abc', 123));
+--------+--------+
| EXPR$0 | EXPR$1 |
+--------+--------+
|    abc |    123 |
+--------+--------+ 
Received a total of 1 rows  (0.22 seconds){code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 16:57:41 UTC 2023,,,,,,,,,,"0|z1lzz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 16:57;jingge;master: 4eb5b588e4d1230c1b038d632b7ed2a3676db32c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathUtils.isPowerOf2  does not cover the case of value=0,FLINK-33725,13560207,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,callthingsoff,callthingsoff,01/Dec/23 11:19,01/Dec/23 11:19,04/Jun/24 20:40,,,,,,,,,,,,,,API / Core,,,,0,,,,"org.apache.flink.util.MathUtils.isPowerOf2, 
 
This static method does not cover the case of value=0.
Should the document explain that value cannot be =0?
Or could it be re implemented as the following code?
 
public static boolean isPowerOf2(long value) {
  return value > 0 && (value & (value - 1)) == 0;
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-01 11:19:49.0,,,,,,,,,,"0|z1lzd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application mode doesn't support multiple lines argument,FLINK-33724,13560205,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,chenjx,chenjx,01/Dec/23 10:51,06/Dec/23 01:59,04/Jun/24 20:40,06/Dec/23 01:59,1.18.0,,,,,,,,,,,,Deployment / YARN,,,,0,,,,"When running a job in application mode, flink will save program arguments in flink-conf.yaml. These arguments will be loaded when deploying an application on YARN. However multiple lines argument cannot be loaded properly.

In my case, I submit a job with sql which is a multiple lines argument:

 
{code:java}
/bin/flink run-application
-t yarn-application
...
myjob.jar
""INSERT INTO tableA
SELECT
a,
b,
c 
FROM tableB;""{code}
In flink-conf.yaml，it saved as:
{code:java}
$internal.application.program-args： ""INSERT INTO tableA 
SELECT 
a,
b,
c 
FROM tableB;""{code}
And produced some warning logs when loaded from flink-conf.yaml :
{code:java}
INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.application.program-args, ""INSERT INTO tableA SELECT a,
WARN  org.apache.flink.configuration.GlobalConfiguration           [] - Error while trying to split key and value in configuration file /{working dir}/flink-conf.yaml:{line num} ""SELECT ""
...{code}
I dig into the source code and find out that the reader and writer methods of YAML file cannot properly parse YAML format. Maybe it would be better to use some third-party tools, such as snakeyaml, to do this job.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 01:58:32 UTC 2023,,,,,,,,,,"0|z1lzco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 01:58;chenjx;Noticed that this issue has been mentioned in FLIP-366，and a decision has been made to reject using a standard YAML parser to parse ""flink-conf.yaml"".
Therefore, closing this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow triggering incremental checkpoint explicitly from REST API,FLINK-33723,13560204,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,01/Dec/23 10:20,14/Dec/23 01:53,04/Jun/24 20:40,14/Dec/23 01:53,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,"Currently, when a job is configured to run with incremental checkpoint disabled, user manually triggers an incremental checkpoint actually triggering a full checkpoint. That is because the files from full checkpoint cannot be shared with an incremental checkpoint. So it is better to remove the ""INCREMENTAL"" option in triggering checkpoint from REST API to avoid misunderstanding.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33498,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 01:53:55 UTC 2023,,,,,,,,,,"0|z1lzcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 01:53;masteryhx;merged 07d159bf into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MATCH_RECOGNIZE in batch mode ignores events order,FLINK-33722,13560198,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,grzegorz.kolakowski,grzegorz.kolakowski,01/Dec/23 09:57,22/Apr/24 14:52,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,Library / CEP,,,,0,pull-request-available,,,"MATCH_RECOGNIZE in batch mode seems to ignore ORDER BY clause. Let's consider the following example:
{code:sql}
    FROM events
        MATCH_RECOGNIZE (
            PARTITION BY user_id
            ORDER BY ts ASC
            MEASURES
                FIRST(A.ts) as _start,
                LAST(A.ts) as _middle,
                LAST(B.ts) as _finish
            ONE ROW PER MATCH
            AFTER MATCH SKIP PAST LAST ROW
            PATTERN (A{2} B) WITHIN INTERVAL '2' HOURS
            DEFINE
                A AS active is false,
                B AS active is true
        ) AS T {code}
where _events_ is a Postgresql table containing ~10000 records.
{code:java}
CREATE TABLE events (
  id INT,
  user_id INT,
  ts TIMESTAMP(3),
  active BOOLEAN,
  WATERMARK FOR ts AS ts - INTERVAL '5' SECOND,
  PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://postgres:5432/test',
    'username' = 'test',
    'password' = 'test',
    'table-name' = 'events'
); {code}
It can happen that _finish is smaller than _start or _middle, which is wrong.
{noformat}
   user_id                  _start                 _middle                 _finish
         1 2023-11-23 14:34:42.346 2023-11-23 14:34:48.370 2023-11-23 14:34:44.264{noformat}
 

Repository where I reproduced the problem: [https://github.com/grzegorz8/flink-match-recognize-in-batch-debugging]
----
 

According to [~dwysakowicz]:  In BATCH the CepOperator is always created to process records in processing time:
[https://github.com/apache/flink/blob/7f7bee70e3ac0d9fb27d7e09b41d6396b748dada/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/batch/BatchExecMatch.java#L54]
A comparator is passed along to the operator covering the sorting on ts field: [https://github.com/apache/flink/blob/fea9ffedecf81a97de5c31519ade3bab8228e743/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/common/CommonExecMatch.java#L173] but this is only secondary sorting. It is applied only within records of the same timestamp.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24865,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-01 09:57:09.0,,,,,,,,,,"0|z1lzb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend BashJavaUtils to Support Reading and Writing Standard YAML Files,FLINK-33721,13560178,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,01/Dec/23 08:32,25/Jan/24 10:34,04/Jun/24 20:40,23/Jan/24 07:11,,,,,,,,,1.19.0,,,,API / Core,,,,0,pull-request-available,,,"Currently, Flink's shell scripts, such as those used for end-to-end (e2e) testing and Docker image building, require the ability to read from and modify Flink's configuration files. With the introduction of standard YAML files as the configuration format, the existing shell scripts are not equipped to correctly handle read and modify operations on these files.

To accommodate this requirement and enhance our script capabilities, we propose an extension to the BashJavaUtils functionality. This extension will enable BashJavaUtils to support the reading and modifying of standard YAML files, ensuring that our shell scripts can seamlessly interact with the new configuration format.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34232,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 07:11:40 UTC 2024,,,,,,,,,,"0|z1lz6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 07:11;zhuzh;master/release-1.19:
1f7622d4d23bfcb76f466469ec36585054864f04
c148b62166d8fec9e7e525a836de890c2f12973b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building Flink took suspiciously long in e2e1 stage with Hadoop 3.1.3 enabled,FLINK-33720,13560167,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,01/Dec/23 07:39,01/Dec/23 07:39,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,Build System / CI,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55085&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&t=ba24ad14-6ea3-5ee3-c4ec-9e7cd2c9e754]

the stage for building Flink hit the time limit of 6 hours and was cancelled. There are no logs provided. But I suspect it to be an infrastructure problem (and, therefore, not falling into the class of issues we can resolve). I created this issue for documentation purposes, anyway.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-01 07:39:15.0,,,,,,,,,,"0|z1lz48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup the usage of deprecated StreamTableEnvironment#toRetractStream,FLINK-33719,13560161,13529804,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,qingyue,qingyue,01/Dec/23 07:00,19/Jan/24 06:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 02:00:38 UTC 2023,,,,,,,,,,"0|z1lz2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 06:17;jackylau;hi [~qingyue] could i take it?;;;","18/Dec/23 02:00;qingyue;[~jackylau] assigned to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup the usage of deprecated StreamTableEnvironment#toAppendStream,FLINK-33718,13560160,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,qingyue,qingyue,01/Dec/23 06:59,07/Dec/23 18:57,04/Jun/24 20:40,07/Dec/23 18:57,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 18:57:05 UTC 2023,,,,,,,,,,"0|z1lz2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 18:57;Sergey Nuyanzin;Merged as [a3ee78767e228898a5d5133a48aacadfe0a3d671|https://github.com/apache/flink/commit/a3ee78767e228898a5d5133a48aacadfe0a3d671];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup the usage of deprecated StreamTableEnvironment#fromDataStream,FLINK-33717,13560159,13529804,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,qingyue,qingyue,01/Dec/23 06:56,18/Jan/24 07:27,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"{code:java}
PythonScalarFunctionOperatorTestBase
AvroTypesITCase {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 07:27:37 UTC 2024,,,,,,,,,,"0|z1lz2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 06:17;jackylau;hi [~qingyue] could i take it?;;;","12/Dec/23 03:14;qingyue;Hi, [~jackylau]. Thanks for the interest, assigned to you.;;;","18/Jan/24 07:27;jiabao.sun;Hi [~jackylau], are you still working on this?
Could I help finished this task?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup the usage of deprecated StreamTableEnvironment#createTemporaryView,FLINK-33716,13560158,13529804,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jackylau,qingyue,qingyue,01/Dec/23 06:50,12/Dec/23 03:15,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"{code:java}
ExpressionTestBase
HiveTableSinkITCase
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 03:15:46 UTC 2023,,,,,,,,,,"0|z1lz28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 06:16;jackylau;hi [~qingyue] can i take it?;;;","12/Dec/23 03:15;qingyue;[~jackylau] assigned to you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance history server to archive multiple histories per jobid,FLINK-33715,13560153,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dongwoo.kim,dongwoo.kim,01/Dec/23 06:04,01/Dec/23 06:05,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"Hello Flink team,

I'd like to propose an improvement to how the job manager archives job histories and how flink history server fetches the history. 

*Currently, only one job history per jobid is available to be archived and fectched.*  
When a flink job tries to archive the job's history more than once, usually 'FileAlreadyExistsException' error happens.
This makes sense in most cases, since a job typically gets a new ID when it gets restarted from latest checkpoint/savepoint.

 

*_However, there's a specific situation where this behavior can be problematic:_*

*_1) When we upgrade a job using the savepoint mode, the job's first history gets successfully archived._*
*_2) If the same job later fails due to an error, its history isn't archived again because there's already a record with the same job ID._*

This can be an issue because the most valuable information – why the job failed – gets lost.

 

To simply solve this, I suggest to include currentTimeMillis to the history filename along with jobid. ( \{jobid}-\{currentTimeMillis} )
And also in the history fetching side parse jobid before the *""-""* delimiter and fetch all the histories for that jobid.
For UI we can keep current display or maybe enhance with adding extra hierarchy for each jobid since each jobid can now have multiple histories.

 

If we could reach an agreement I'll be glad to take on the implementation.
Thanks in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-01 06:04:06.0,,,,,,,,,,"0|z1lz14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation about the usage of RuntimeContext#getExecutionConfig,FLINK-33714,13560148,13560145,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,01/Dec/23 05:34,12/Dec/23 07:29,04/Jun/24 20:40,12/Dec/23 07:29,1.19.0,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 07:29:43 UTC 2023,,,,,,,,,,"0|z1lz00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 07:29;Weijie Guo;master(1.19) via 3531998adad20f3904d1421a35a69fef44b2b69f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate RuntimeContext#getExecutionConfig,FLINK-33713,13560147,13560145,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,01/Dec/23 05:33,11/Dec/23 08:13,04/Jun/24 20:40,11/Dec/23 08:13,,,,,,,,,1.19.0,,,,API / Core,,,,0,pull-request-available,,,Deprecate RuntimeContext#getExecutionConfig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 08:13:12 UTC 2023,,,,,,,,,,"0|z1lyzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 08:13;zhuzh;Done via 3aa70df4e7da93ed32c26cfabdaeb606233419b1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-391: Deprecate RuntimeContext#getExecutionConfig,FLINK-33712,13560145,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,01/Dec/23 04:36,16/Jan/24 06:49,04/Jun/24 20:40,16/Jan/24 06:49,,,,,,,,,1.19.0,,,,API / Core,,,,0,,,,"Deprecate the RuntimeContext#getExecutionConfig and  introduce alternative getter methods that allow users to access specific information without exposing unnecessary runtime details. More details see: [FLIP-391|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=278465937]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-01 04:36:23.0,,,,,,,,,,"0|z1lyzc:",9223372036854775807,RuntimeContext#getExecutionConfig is now be deprecated in FLINK-1.19. And this method is planned to be removed in Flink-2.0. More details can be found at https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=278465937.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix numbers of field  of the taxi event / Fix broken link,FLINK-33711,13560140,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yoda-mon,yoda-mon,01/Dec/23 02:00,02/Dec/23 02:55,04/Jun/24 20:40,,,,,,,,,,,,,,Documentation / Training / Exercises,,,,0,pull-request-available,,,"* Fix number of fields of the taxi event from 11 to 10.
 ** According to https://issues.apache.org/jira/browse/FLINK-23926, the number of fields decreased from 11 to 10.
 * Fix broken link
 ** Fix ""use taxi data streams"" link. On Chinese document, it seems to be added additional ancher for this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-12-01 02:00:12.0,,,,,,,,,,"0|z1lyy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler redeploys pipeline for a NOOP parallelism change,FLINK-33710,13560114,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,30/Nov/23 18:36,06/Dec/23 15:54,04/Jun/24 20:40,01/Dec/23 10:57,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"The operator supports two modes to apply autoscaler changes:

# Use the internal Flink config {{pipeline.jobvertex-parallelism-overrides}} 
# Make use of Flink's Rescale API 

For (1), a string has to be generated for the Flink config with the actual overrides. This string has to be deterministic for a given map. But it is not.

Consider the following observed log:

{noformat}
  >>> Event  | Info    | SPECCHANGED     | SCALE change(s) detected (Diff: FlinkDeploymentSpec[flinkConfiguration.pipeline.jobvertex-parallelism-overrides : 92542d1280187bd464274368a5f86977:3,9f979ed859083299d29f281832cb5be0:1,84881d7bda0dc3d44026e37403420039:1,1652184ffd0522859c7840a24936847c:1 -> 9f979ed859083299d29f281832cb5be0:1,84881d7bda0dc3d44026e37403420039:1,92542d1280187bd464274368a5f86977:3,1652184ffd0522859c7840a24936847c:1]), starting reconciliation. 
{noformat}

The overrides are identical but the order is different which triggers a redeploy. This does not seem to happen often but some deterministic string generation (e.g. sorting by key) is required to prevent any NOOP updates.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 15:54:35 UTC 2023,,,,,,,,,,"0|z1lysg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/23 10:57;fanrui;Merged to main<1.8.0> via : 158cbe29169cbfb7fa7ad676fb0273fd7ef6d25e;;;","06/Dec/23 15:54;mxm;Additional fix via ca1d8472d1a1e817268950dae079592581fa5b8f to prevent any existing deployments to get affected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report CheckpointStats as Spans,FLINK-33709,13560085,13559950,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,30/Nov/23 14:39,11/Dec/23 13:22,04/Jun/24 20:40,11/Dec/23 13:22,,,,,,,,,1.19.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 13:22:16 UTC 2023,,,,,,,,,,"0|z1lym0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 13:22;pnowojski;Merged f5039a8824e to master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Span and TraceReporter concepts,FLINK-33708,13560084,13559950,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,30/Nov/23 14:38,11/Dec/23 13:21,04/Jun/24 20:40,11/Dec/23 13:21,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 13:21:29 UTC 2023,,,,,,,,,,"0|z1lyls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 13:21;pnowojski;Merged 066f34fde30, f9fa5498514, ddfb5a3a029 and 589ea2e7524 to master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify the snapshot migration on Java17,FLINK-33707,13560069,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunta,yunta,30/Nov/23 12:05,27/Mar/24 02:46,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,,"This task is like FLINK-33699, I think we could introduce a StatefulJobSnapshotMigrationITCase-like test to restore snapshots containing scala code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15736,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 02:46:16 UTC 2024,,,,,,,,,,"0|z1lyig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Mar/24 13:53;m.orazow;Hello,

Would this affect the Flink 1.18 version?;;;","27/Mar/24 02:46;masteryhx;Hi, Scala API has been marked as deprecated.

Do we still need this ?


https://issues.apache.org/jira/browse/FLINK-32560;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build_wheels_on_macos fails on AZP,FLINK-33706,13560065,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,30/Nov/23 11:45,01/Mar/24 10:12,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,API / Python,Test Infrastructure,,,0,test-stability,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55044&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=427
fails on AZP as 
{noformat}
       note: This error originates from a subprocess, and is likely not a problem with pip.
        ERROR: Failed cleaning build dir for crcmod
        Building wheel for dill (setup.py): started
        Building wheel for dill (setup.py): finished with status 'done'
        Created wheel for dill: filename=dill-0.0.0-py3-none-any.whl size=899 sha256=39d0b4b66ce11f42313482f4ad825029e861fd6dab87a743a95d75a44a1fedd6
        Stored in directory: /Users/runner/Library/Caches/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d
        Building wheel for hdfs (setup.py): started
        Building wheel for hdfs (setup.py): finished with status 'done'
        Created wheel for hdfs: filename=UNKNOWN-0.0.0-py3-none-any.whl size=928 sha256=cb3fd7d8c71b52bbc27cfb75842f9d4d9c6f3b847f3f4fe50323c945a0e38ccc
        Stored in directory: /Users/runner/Library/Caches/pip/wheels/68/dd/29/c1a590238f9ebbe4f7ee9b3583f5185d0b9577e23f05c990eb
        WARNING: Built wheel for hdfs is invalid: Wheel has unexpected file name: expected 'hdfs', got 'UNKNOWN'
        Building wheel for pymongo (pyproject.toml): started
        Building wheel for pymongo (pyproject.toml): finished with status 'done'
        Created wheel for pymongo: filename=pymongo-4.6.1-cp38-cp38-macosx_10_9_x86_64.whl size=478012 sha256=5dfc6fdb6a8a399f8f9da44e28bae19be244b15c8000cd3b2d7d6ff513cc6277
        Stored in directory: /Users/runner/Library/Caches/pip/wheels/54/d8/0e/2a61e90bb3872d903b15eb3c94cb70f438fb8792a28fee7bb1
        Building wheel for docopt (setup.py): started
        Building wheel for docopt (setup.py): finished with status 'done'
        Created wheel for docopt: filename=UNKNOWN-0.0.0-py3-none-any.whl size=920 sha256=612c56cd1a6344b8def6c4d3c3c1c8bb10e1f2b0d978fee0fc8b9281026e8288
        Stored in directory: /Users/runner/Library/Caches/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c
        WARNING: Built wheel for docopt is invalid: Wheel has unexpected file name: expected 'docopt', got 'UNKNOWN'
      Successfully built dill pymongo
      Failed to build fastavro crcmod hdfs docopt
      ERROR: Could not build wheels for fastavro, which is required to install pyproject.toml-based projects
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 01 10:12:04 UTC 2024,,,,,,,,,,"0|z1lyhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 14:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55086&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb;;;","08/Dec/23 14:01;jhughes;Another instance here: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55339&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5;;;","18/Dec/23 09:51;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55593&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=815;;;","20/Feb/24 14:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57649&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=694;;;","01/Mar/24 10:12;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57994&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=350;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade flink-shaded to 18.0,FLINK-33705,13560045,,Technical Debt,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,30/Nov/23 09:58,11/Mar/24 13:10,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,"Currently flink-shaded is in a process of releasing

Once it is released it would make sense to upgrade the dependency in Flink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 15:38:38 UTC 2024,,,,,,,,,,"0|z1lyd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/24 15:50;Sergey Nuyanzin;Merged to master as [5d9d8748b64ff1a75964a5cd2857ab5061312b51|https://github.com/apache/flink/commit/5d9d8748b64ff1a75964a5cd2857ab5061312b51];;;","01/Feb/24 15:38;Sergey Nuyanzin;As it was decided in release meeting

reverted as [d6c7eee8243b4fe3e593698f250643534dc79cb5|https://github.com/flink-ci/flink-mirror/commit/d6c7eee8243b4fe3e593698f250643534dc79cb5]

first perf regression should be fixed on flink-shaded level
for more details have a look at FLINK-34148;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update GCS filesystems to latest available versions,FLINK-33704,13560044,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,30/Nov/23 09:54,18/Dec/23 10:49,04/Jun/24 20:40,18/Dec/23 10:48,,,,,,,,,1.18.1,1.19.0,,,Connectors / FileSystem,FileSystems,,,0,pull-request-available,,,Update GS SDK from 2.15.0 to 2.29.1 and GS Hadoop Connector from 2.2.15 to 2.2.18,,,,,,,,,,,,,,,,,,,,FLINK-33793,,,,,,,,,,,,,,FLINK-33603,,,,"18/Dec/23 10:47;martijnvisser;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13065392/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 10:49:40 UTC 2023,,,,,,,,,,"0|z1lycw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/23 11:12;martijnvisser;Fixed in apache/flink:master 492a886248208904276fcd2bda138a79c86bc71c;;;","18/Dec/23 08:06;martijnvisser;Fixed in apache/flink:release-1.18 8951f907b4633d6826852d62920ef638905c7c78

This is an alternative resolution to FLINK-33603 which also helps solving FLINK-33793;;;","18/Dec/23 10:30;jingge;Thanks [~martijnvisser] for fixing it! Can we resolve the ticket now?;;;","18/Dec/23 10:45;martijnvisser;[~jingge] I'm trying to confirm if this issue occurs in 1.17 or not. ;;;","18/Dec/23 10:47;martijnvisser;[~jingge] Closing this issue, it doesn't appear in 1.17.2

!screenshot-1.png|width=623,height=294!;;;","18/Dec/23 10:49;jingge;[~martijnvisser] thanks for letting us know.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use maven 3.8.6 for releasing of flink-shaded,FLINK-33703,13560042,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,30/Nov/23 09:51,07/Dec/23 12:48,04/Jun/24 20:40,,,,,,,,,,,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,"Currently there is maven-enforcer-plugin configuration (for release only)
{noformat}
<requireMavenVersion>
                                            <!-- maven version must be lower than 3.3. See FLINK-3158 -->
                                            <version>(,3.3)</version>
                                        </requireMavenVersion>
{noformat}
which seems to be outdated and for ci 3.8.6 is used

We should keep them in sync and use 3.8.6 for both",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-30 09:51:55.0,,,,,,,,,,"0|z1lycg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add IncrementalDelayRetryStrategy implementation of RetryStrategy,FLINK-33702,13560027,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangyu0xf,xiangyu0xf,xiangyu0xf,30/Nov/23 08:52,05/Dec/23 05:36,04/Jun/24 20:40,05/Dec/23 05:36,,,,,,,,,1.19.0,,,,API / DataStream,,,,0,pull-request-available,,,"RetryStrategy now supports FixedRetryStrategy and ExponentialBackoffRetryStrategy. 
In certain scenarios, we also need IncrementalDelayRetryStrategy to reduce the retry count and perform the action more timely. 

IncrementalDelayRetryStrategy will increase the retry delay at a fixed rate for each attempt.",,,,,,,,,,,,FLINK-33684,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 05 05:36:11 UTC 2023,,,,,,,,,,"0|z1ly94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 08:56;xiangyu0xf;Hi [~lincoln.86xy], I'll also work on this if it is ok from your side.;;;","01/Dec/23 03:47;xiangyu0xf;Hi [~huwh] , would u kindly assign this to me and help review this pr?;;;","01/Dec/23 13:50;xiangyu0xf;`AsyncRetryStrategy` is designed for AsyncWaitOperator to use. It may not suitable for `CollectResultFetcher` to use.;;;","01/Dec/23 15:26;xiangyu0xf;`RetryStrategy` in org.apache.flink.util.concurrent can be used in `CollectResultFetcher`.;;;","05/Dec/23 05:36;huweihua;resolved in master: 0acfc1a51e8802ebd0eb1ab5c00bbfab5032ebdf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
restart.time-tracking doc is wrong,FLINK-33701,13560024,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,30/Nov/23 08:27,30/Nov/23 10:44,04/Jun/24 20:40,30/Nov/23 10:20,kubernetes-operator-1.8.0,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,"The {color:#6a8759}restart.time-tracking.limit {color}as the upper bound instead of {color:#6a8759}restart.time.{color}

 

!image-2023-11-30-16-27-06-149.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/23 08:27;fanrui;image-2023-11-30-16-27-06-149.png;https://issues.apache.org/jira/secure/attachment/13064835/image-2023-11-30-16-27-06-149.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 30 10:20:13 UTC 2023,,,,,,,,,,"0|z1ly8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 10:20;fanrui;Merged master<1.8.0> via 34b12399467d26b23f761c3db998286728f2ac01;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CustomSSLEngineProvider supports SHA-256,FLINK-33700,13559997,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Bo Cui,Bo Cui,30/Nov/23 02:45,30/Nov/23 04:35,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / RPC,,,,0,pull-request-available,,,"The algorithm of CustomSSLEngineProvider supports only SHA1.
https://github.com/apache/flink/blob/72654384686d127172b48b0071ea7656b16e9134/flink-rpc/flink-rpc-akka/src/main/java/org/apache/flink/runtime/rpc/pekko/CustomSSLEngineProvider.java#L58C1-L59C1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-30 02:45:52.0,,,,,,,,,,"0|z1ly2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify the snapshot migration on Java21,FLINK-33699,13559996,13552124,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunta,yunta,30/Nov/23 02:41,07/May/24 16:21,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,,"In Java 21 builds, Scala is being bumped to 2.12.18, which causes incompatibilities within Flink.

This could affect loading savepoints from a Java 8/11/17 build. We already have tests extending {{SnapshotMigrationTestBase}} to verify the logic of migrating snapshots generated by the older Flink version. I think we can also introduce similar tests to verify the logic across different Java versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 07 16:21:36 UTC 2024,,,,,,,,,,"0|z1ly28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 07:18;Sergey Nuyanzin;I wonder whether it should be a release testing related task?;;;","30/Nov/23 11:37;yunta;I think it's better to introduce new tests to cover this problem. I checked the [CI|https://dev.azure.com/snuyanzin/flink/_build/results?buildId=2620&view=logs&j=0a15d512-44ac-5ba5-97ab-13a5d066c22c&t=9a028d19-6c4b-5a4e-d378-03fca149d0b1] you triggered before and noticed that the {{StatefulJobSnapshotMigrationITCase}} related tests have passed, which proves what I guessed before, most checkpoints/savepoints should be restored successfully.

;;;","07/May/24 16:21;afilipchik;hey folks, is anyone actively working on this one? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the backoff time calculation in ExponentialBackoffDelayRetryStrategy,FLINK-33698,13559968,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangyu0xf,xiangyu0xf,xiangyu0xf,29/Nov/23 18:36,02/Dec/23 13:26,04/Jun/24 20:40,02/Dec/23 01:01,,,,,,,,,1.19.0,,,,API / DataStream,,,,0,pull-request-available,,,"The backoff time calculation in `ExponentialBackoffDelayRetryStrategy` is problematic due to the lack of a reset when reusing the object.

 

Current Version:
{code:java}
@Override
public long getBackoffTimeMillis(int currentAttempts) {
    if (currentAttempts <= 1) {
        // equivalent to initial delay
        return lastRetryDelay;
    }
    long backoff = Math.min((long) (lastRetryDelay * multiplier), maxRetryDelay);
    this.lastRetryDelay = backoff;
    return backoff;
} {code}
Fixed Version:
{code:java}
@Override
public long getBackoffTimeMillis(int currentAttempts) {
    if (currentAttempts <= 1) {
        // reset to initialDelay
        this.lastRetryDelay = initialDelay;
        return lastRetryDelay;
    }

    long backoff = Math.min((long) (lastRetryDelay * multiplier), maxRetryDelay);
    this.lastRetryDelay = backoff;
    return backoff;
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33684,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 02 01:01:25 UTC 2023,,,,,,,,,,"0|z1lxw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 18:44;xiangyu0xf;Hi [~lincoln.86xy] , what do you think about this?;;;","30/Nov/23 01:55;lincoln.86xy;[~xiangyu0xf] thanks for reporting this issue! Are you interested to submit a pr?;;;","30/Nov/23 01:57;xiangyu0xf;[~lincoln.86xy] Sure, would you kindly assign this to me?;;;","30/Nov/23 05:31;lincoln.86xy;[~xiangyu0xf] assigned to you, I can help review when the pr is ready :);;;","02/Dec/23 01:01;lincoln.86xy;fixed in master: 5da214c963c219c8b3da727ffde5d6995b3770b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-386: Support adding custom metrics in Recovery Spans,FLINK-33697,13559952,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,29/Nov/23 17:02,12/Jan/24 08:40,04/Jun/24 20:40,12/Jan/24 08:16,,,,,,,,,1.19.0,,,,Runtime / Metrics,Runtime / State Backends,,,0,pull-request-available,,,"h1. Motivation

[FLIP-386|https://cwiki.apache.org/confluence/x/VAuZE] is building on top of [FLIP-384|https://cwiki.apache.org/confluence/display/FLINK/FLIP-384%3A+Introduce+TraceReporter+and+use+it+to+create+checkpointing+and+recovery+traces]. The intention here is to add a capability for state backends to attach custom attributes during recovery to recovery spans. For example RocksDBIncrementalRestoreOperation could report both remote download time and time to actually clip/ingest the RocksDB instances after rescaling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33696,FLINK-33695,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 08:16:46 UTC 2024,,,,,,,,,,"0|z1lxsg:",9223372036854775807,"A braking change has been introduced to the `StateBackend` interface. This is relevant only to users that are implementing their own custom state backends.

Newly added methods `org.apache.flink.runtime.state.StateBackend#createKeyedStateBackend(KeyedStateBackendParameters<K> parameters)` and `org.apache.flink.runtime.state.StateBackend#createOperatorStateBackend(OperatorStateBackendParameters parameters)` have replaced previous versions of the `createKeyedStateBackend` and `createOperatorStateBackend` methods. The new `parameters` POJO classes contain as fields all of the arguments that were passed directly to those methods.  ",,,,,,,,,,,,,,,,,,,"12/Jan/24 08:16;pnowojski;Merged to master as: b660b06cb70^..e6556fa898d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-385: Add OpenTelemetryTraceReporter and OpenTelemetryMetricReporter,FLINK-33696,13559951,,New Feature,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pnowojski,pnowojski,pnowojski,29/Nov/23 17:01,11/Mar/24 13:10,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,,Runtime / Metrics,,,,0,,,,"h1. Motivation

[FLIP-384|https://cwiki.apache.org/confluence/display/FLINK/FLIP-384%3A+Introduce+TraceReporter+and+use+it+to+create+checkpointing+and+recovery+traces] is adding TraceReporter interface. However with [FLIP-384|https://cwiki.apache.org/confluence/display/FLINK/FLIP-384%3A+Introduce+TraceReporter+and+use+it+to+create+checkpointing+and+recovery+traces] alone, Log4jTraceReporter would be the only available implementation of TraceReporter interface, which is not very helpful.

In this FLIP I’m proposing to contribute both MetricExporter and TraceReporter implementation using OpenTelemetry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33697,FLINK-33695,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 11 13:09:12 UTC 2024,,,,,,,,,,"0|z1lxs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 13:20;pnowojski;merged 7db2ecad165 to master;;;","22/Jan/24 09:26;yunta;[~pnowojski], is this ticket done for FLIP-385?;;;","24/Jan/24 14:06;srichter;merged in 7db2ecad;;;","02/Feb/24 15:48;pnowojski;No it's not yet done. I still have to open a PR adding OpenTelemetry reporters.

7db2ecad was adding only slf4j reporter.;;;","11/Mar/24 13:09;lincoln.86xy;[~pnowojski] Since there's still some work to be done, I'm going to change this issue to version 1.20. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-384: Introduce TraceReporter and use it to create checkpointing and recovery traces,FLINK-33695,13559950,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,29/Nov/23 16:54,05/Jan/24 16:25,04/Jun/24 20:40,05/Jan/24 16:25,,,,,,,,,1.19.0,,,,Runtime / Checkpointing,Runtime / Metrics,,,0,,,,"https://cwiki.apache.org/confluence/x/TguZE

*Motivation*
Currently Flink has a limited observability of checkpoint and recovery processes.

For checkpointing Flink has a very detailed overview in the Flink WebUI, which works great in many use cases, however it’s problematic if one is operating multiple Flink clusters, or if cluster/JM dies. Additionally there are a couple of metrics (like lastCheckpointDuration or lastCheckpointSize), however those metrics have a couple of issues:
* They are reported and refreshed periodically, depending on the MetricReporter settings, which doesn’t take into account checkpointing frequency.
** If checkpointing interval > metric reporting interval, we would be reporting the same values multiple times.
** If checkpointing interval < metric reporting interval, we would be randomly dropping metrics for some of the checkpoints.

For recovery we are missing even the most basic of the metrics and Flink WebUI support. Also given the fact that recovery is even less frequent compared to checkpoints, adding recovery metrics would have even bigger problems with unnecessary reporting the same values.

In this FLIP I’m proposing to add support for reporting traces/spans (example: Traces) and use this mechanism to report checkpointing and recovery traces. I hope in the future traces will also prove useful in other areas of Flink like job submission, job state changes, ... . Moreover as the API to report traces will be added to the MetricGroup , users will be also able to access this API. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33696,,FLINK-33697,,,FLINK-23411,FLINK-7894,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 16:54:03.0,,,,,,,,,,"0|z1lxs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCS filesystem does not respect gs.storage.root.url config option,FLINK-33694,13559938,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,plucas,plucas,plucas,29/Nov/23 16:07,18/Jan/24 19:31,04/Jun/24 20:40,18/Jan/24 17:23,1.17.2,1.18.0,,,,,,,1.17.3,1.18.2,1.19.0,,FileSystems,,,,0,gcs,pull-request-available,,"The GCS FileSystem's RecoverableWriter implementation uses the GCS SDK directly rather than going through Hadoop. While support has been added to configure credentials correctly based on the standard Hadoop implementation configuration, no other options are passed through to the underlying client.

Because this only affects the RecoverableWriter-related codepaths, it can result in very surprising differing behavior whether the FileSystem is being used as a source or a sink—while a {{{}gs://{}}}-URI FileSource may work fine, a {{{}gs://{}}}-URI FileSink may not work at all.

We use [fake-gcs-server|https://github.com/fsouza/fake-gcs-server] in testing, and so we override the Hadoop GCS FileSystem config option {{{}gs.storage.root.url{}}}. However, because this option is not considered when creating the GCS client for the RecoverableWriter codepath, in a FileSink the GCS FileSystem attempts to write to the real GCS service rather than fake-gcs-server. At the same time, a FileSource works as expected, reading from fake-gcs-server.

The fix should be fairly straightforward, reading the {{gs.storage.root.url}} config option from the Hadoop FileSystem config in [{{GSFileSystemOptions}}|https://github.com/apache/flink/blob/release-1.18.0/flink-filesystems/flink-gs-fs-hadoop/src/main/java/org/apache/flink/fs/gs/GSFileSystemOptions.java#L30] and, if set, passing it to {{storageOptionsBuilder}} in [{{GSFileSystemFactory}}|https://github.com/apache/flink/blob/release-1.18.0/flink-filesystems/flink-gs-fs-hadoop/src/main/java/org/apache/flink/fs/gs/GSFileSystemFactory.java].

The only workaround for this is to build a custom flink-gs-fs-hadoop JAR with a patch and use it as a plugin.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 17:23:06 UTC 2024,,,,,,,,,,"0|z1lxpc:",9223372036854775807,"This fix resolves the issue where the `gs.storage.root.url` setting in the Hadoop configuration was not being acknowledged by the Sink. Warning: If you have been using this property to configure the GCS Source, please ensure that your tests or pipelines are not adversely affected by the GCS Sink now also correctly adhering to this configuration.",,,,,,,,,,,,,,,,,,,"29/Nov/23 16:25;martijnvisser;[~plucas] Do you want to take a stab at fixing it?;;;","30/Nov/23 15:04;plucas;[~martijnvisser] aye, have a PR open but Azure needs a re-run because I missed a license header. I'm not sure if flinkbot listens to me for commands.;;;","30/Nov/23 15:55;martijnvisser;[~plucas] Just a question, you state that ""The GCS FileSystem's RecoverableWriter implementation uses the GCS SDK directly rather than going through Hadoop. While support has been added to configure credentials correctly based on the standard Hadoop implementation configuration, no other options are passed through to the underlying client.""

But is that actually true, given the statement ""However, if you use any of those other options, the provided credentials will not be used by the google-cloud-storage library, which provides RecoverableWriter support, so Flink recoverable-write operations would be expected to fail."" that's at the bottom of https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/gcs/#authentication-to-access-gcs ? ;;;","01/Dec/23 13:13;plucas;[~martijnvisser] my comment is perhaps not precise, in that ""support has been added to configure credentials correctly"" should be qualified with the comment from the docs about which credentials mechanisms are supported. But it is still true that other potentially-interesting options are not proxied through, such as setting the root URL.

This change as written doesn't affect any credentials handling, only adding support for this one additional option. However, I could see an argument for implementing the behavior in {{org.apache.flink.fs.gs.utils.ConfigUtils}} as the credentials behavior is rather than in {{GSFileSystemOptions}} as I did to start with.;;;","04/Dec/23 15:07;plucas;I've updated the PR to more explicitly look for the specific Hadoop connector config for this option instead of defining it as a Flink {{{}ConfigOption{}}}.;;;","18/Jan/24 17:23;mapohl;master: [a41229b24d82e8c561350c42d8a98dfb865c3f69|https://github.com/apache/flink/commit/a41229b24d82e8c561350c42d8a98dfb865c3f69]
1.18: [846ab49afd20ecf49fe76e18dd3e9b41143bf207|https://github.com/apache/flink/commit/846ab49afd20ecf49fe76e18dd3e9b41143bf207]
1.17: [257c526d6ae404f4598aeb2b9efa85674df2e6cd|https://github.com/apache/flink/commit/257c526d6ae404f4598aeb2b9efa85674df2e6cd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Force aligned barrier logic doesn't work when the aligned checkpoint timeout is enabled ,FLINK-33693,13559895,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fanrui,fanrui,fanrui,29/Nov/23 12:45,07/Dec/23 14:59,04/Jun/24 20:40,07/Dec/23 14:59,1.17.2,1.18.0,,,,,,,1.17.3,1.18.1,1.19.0,,Runtime / Checkpointing,,,,0,pull-request-available,,,"FLINK-21945 will convert the unaligned checkpoint barrier and timeoutable aligned checkpoint barrier to force aligned barrier. However, Force aligned barrier logic doesn't work for  timeoutable aligned checkpoint barrier.

 

Bug at org.apache.flink.streaming.runtime.io.RecordWriterOutput#broadcastEvent, we will call withUnalignedUnsupported to transmit the unaligned checkpoint barrier to {color:#9876aa}FORCED_ALIGNED {color}barrier when the shuffle type cannot use unaligned checkpoint.

However, the if has one condition: isPriorityEvent. When  aligned checkpoint timeout is enabled, flink will emit one timeoutable barrier. It isn't PriorityEvent.

 

But timeoutable barrier should call withUnalignedUnsupported here.

!image-2023-11-29-20-41-17-038.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/23 12:41;fanrui;image-2023-11-29-20-41-17-038.png;https://issues.apache.org/jira/secure/attachment/13064812/image-2023-11-29-20-41-17-038.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 14:59:00 UTC 2023,,,,,,,,,,"0|z1lxfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 14:59;fanrui;Merged to

master<1.19> via : a4d14ec224bbaf498ca2e912ff51539872a34878

1.18 via : 3884f65a3d049acd5f092d5b48a1d02ff24b77e6

1.17 via : 2dd1076dbbf0f47e4b5d15e67e0265679397f93e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkKafkaProducer could miss super.close,FLINK-33692,13559866,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,zhumingustc,zhumingustc,29/Nov/23 09:39,04/Apr/24 12:06,04/Jun/24 20:40,04/Apr/24 12:05,1.13.6,1.17.1,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,," 

When flink job restarted 、canceled or failed. It will execute close method. But in following FlinkKafkaProducer source code. If flush or close throw InterruptedExecption. super.close method must be missed.
{code:java}
@Override
public void close() throws FlinkKafkaException {
    // First close the producer for current transaction.
    try {
        final KafkaTransactionState currentTransaction = currentTransaction();
        if (currentTransaction != null) {
            // to avoid exceptions on aborting transactions with some pending records
            flush(currentTransaction);

            // normal abort for AT_LEAST_ONCE and NONE do not clean up resources because of
            // producer reusing, thus
            // we need to close it manually
            switch (semantic) {
                case EXACTLY_ONCE:
                    break;
                case AT_LEAST_ONCE:
                case NONE:
                    currentTransaction.producer.flush();
                    currentTransaction.producer.close(Duration.ofSeconds(0));
                    break;
            }
        }
        // If flush() or close() wasinterrupted, super.close might be missed.
        super.close();
    } catch (Exception e) {
        asyncException = ExceptionUtils.firstOrSuppressed(e, asyncException);
    } finally {
        // We may have to close producer of the current transaction in case some exception was
        // thrown before
        // the normal close routine finishes.
        if (currentTransaction() != null) {
            try {
                currentTransaction().producer.close(Duration.ofSeconds(0));
            } catch (Throwable t) {
                LOG.warn(""Error closing producer."", t);
            }
        }
        // Make sure all the producers for pending transactions are closed.
        pendingTransactions()
                .forEach(
                        transaction -> {
                            try {
                                transaction.getValue().producer.close(Duration.ofSeconds(0));
                            } catch (Throwable t) {
                                LOG.warn(""Error closing producer."", t);
                            }
                        });
        // make sure we propagate pending errors
        checkErroneous();
    }
} {code}
super.close() method is to execute following code.  It ensures  '{*}KafkaTransactionState{*}'  released correctlly. 
{code:java}
@Override
public void close() throws Exception {
    super.close();

    if (currentTransactionHolder != null) {
        abort(currentTransactionHolder.handle);
        currentTransactionHolder = null;
    }
} {code}
 ",,345600,345600,,0%,345600,345600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,Thu Apr 04 12:05:52 UTC 2024,,,,,,,,,,"0|z1lx9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Dec/23 12:29;martijnvisser;[~tzulitai] Thoughts on this?;;;","04/Apr/24 12:05;martijnvisser;See PR; we believe the current code is fine, and we haven't had a response from the original reporter;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support agg push down for 'count(*)/count(1)/count(column not null)',FLINK-33691,13559865,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,337361684@qq.com,337361684@qq.com,337361684@qq.com,29/Nov/23 09:03,06/Dec/23 09:37,04/Jun/24 20:40,06/Dec/23 09:37,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Now,  PushLocalAggIntoScanRule cannot push down 'count( * )/count(1)/count(column not null)', but it can push down count(column nullable). The reason is that count( * ) and count( 1 ) will be optimized to a scan with calc as '0 AS $f0' to reduce read cost, which will not match the push down rule",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 09:37:16 UTC 2023,,,,,,,,,,"0|z1lx94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 09:37;fsk119;Merged into master: 3060ccd49cc8d19634b431dbf0f09ac875d0d422;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
build_wheels_on_macos sometimes fails with read timed out on AZP,FLINK-33690,13559859,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,29/Nov/23 07:28,29/Nov/23 07:28,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,API / Python,,,,0,test-stability,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55013&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=217
failed as
{noformat}
...
        File ""/private/var/folders/3s/vfzpb5r51gs6y328rmlgzm7c0000gn/T/pip-standalone-pip-3jt0u_hl/__env_pip__.zip/pip/_vendor/urllib3/response.py"", line 573, in stream
          data = self.read(amt=amt, decode_content=decode_content)
        File ""/private/var/folders/3s/vfzpb5r51gs6y328rmlgzm7c0000gn/T/pip-standalone-pip-3jt0u_hl/__env_pip__.zip/pip/_vendor/urllib3/response.py"", line 538, in read
          raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
        File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/contextlib.py"", line 131, in __exit__
          self.gen.throw(type, value, traceback)
        File ""/private/var/folders/3s/vfzpb5r51gs6y328rmlgzm7c0000gn/T/pip-standalone-pip-3jt0u_hl/__env_pip__.zip/pip/_vendor/urllib3/response.py"", line 440, in _error_catcher
          raise ReadTimeoutError(self._pool, None, ""Read timed out."")
      pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 07:28:11.0,,,,,,,,,,"0|z1lx7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonObjectAggFunction can't retract previous data which is invalid when enabling local global agg,FLINK-33689,13559855,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xu_shuai_,xu_shuai_,29/Nov/23 07:16,16/Jan/24 02:26,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"Run the test as following and enable LocalGlobal in sql/AggregateITCase . 
{code:java}
def testGroupJsonObjectAggWithRetract(): Unit = {
  val data = new mutable.MutableList[(Long, String, Long)]
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  data.+=((2L, ""Hallo"", 2L))
  val sql =
    s""""""
       |select
       |   JSON_OBJECTAGG(key k value v)
       |from (select
       |             cast(SUM(a) as string) as k,t as v
       |       from
       |             Table6
       |       group by t)
       |"""""".stripMargin
  val t = failingDataSource(data).toTable(tEnv, 'a, 'c, 't)
  tEnv.createTemporaryView(""Table6"", t)
  val sink = new TestingRetractSink
  tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink).setParallelism(1)
  env.execute()
  val expected =
    List(
      ""{\""30\"":2}""
    )
  assertThat(sink.getRetractResults).isEqualTo(expected)
} {code}
The result is as following.
{code:java}
List({""14"":2,""30"":2}) {code}
However,  \{""14"":2}  should be retracted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 07:16:37.0,,,,,,,,,,"0|z1lx6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse Channels in RestClient to save connection establish time,FLINK-33688,13559854,13559834,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,29/Nov/23 07:01,15/Dec/23 14:11,04/Jun/24 20:40,,,,,,,,,,,,,,Client / Job Submission,,,,0,pull-request-available,,,RestClient can reuse the connections to Dispatcher when submitting http requests to a long running Flink cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 07:01:49.0,,,,,,,,,,"0|z1lx6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse RestClusterClient in StandaloneClusterDescriptor to avoid frequent thread create/destroy,FLINK-33687,13559852,13559834,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,29/Nov/23 06:58,13/Dec/23 11:50,04/Jun/24 20:40,,,,,,,,,,,,,,Client / Job Submission,,,,0,pull-request-available,,,`RestClusterClient` can also be reused when submitting programs to a long-running Flink Cluster,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 06:58:03.0,,,,,,,,,,"0|z1lx68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse ClusterDescriptor in AbstractSessionClusterExecutor when executing jobs on the same cluster,FLINK-33686,13559848,13559834,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,29/Nov/23 06:51,14/Dec/23 02:44,04/Jun/24 20:40,,,,,,,,,,,,,,Client / Job Submission,,,,0,pull-request-available,,,Multiple `RemoteExecutor` instances can reuse the same `StandaloneClusterDescriptor` when executing jobs to a same running cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 06:51:45.0,,,,,,,,,,"0|z1lx5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandaloneClusterId need to distinguish different remote clusters,FLINK-33685,13559844,13559834,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,29/Nov/23 06:49,12/Dec/23 15:43,04/Jun/24 20:40,,,,,,,,,,,,,,Client / Job Submission,,,,0,pull-request-available,,,"`StandaloneClusterId` is a singleton, which means `StandaloneClusterDescriptor` cannot distinguish different remote running clusters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 06:49:20.0,,,,,,,,,,"0|z1lx4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the retry strategy in CollectResultFetcher,FLINK-33684,13559840,13559834,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiangyu0xf,xiangyu0xf,29/Nov/23 06:46,02/Dec/23 13:24,04/Jun/24 20:40,,,,,,,,,,,,,,API / DataStream,,,,0,pull-request-available,,,"Currently  CollectResultFetcher use a fixed retry interval.
{code:java}
private void sleepBeforeRetry() {
    if (retryMillis <= 0) {
        return;
    }

    try {
        // TODO a more proper retry strategy?
        Thread.sleep(retryMillis);
    } catch (InterruptedException e) {
        LOG.warn(""Interrupted when sleeping before a retry"", e);
    }
} {code}
This can be improved with a RetryStrategy.",,,,,,,,,,FLINK-33702,,,,,,,,,,,,,,,,,,,FLINK-33698,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 06:46:32.0,,,,,,,,,,"0|z1lx3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-407 Improve Flink Client performance in interactive scenarios,FLINK-33683,13559834,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xiangyu0xf,xiangyu0xf,xiangyu0xf,29/Nov/23 06:39,24/Jan/24 09:06,04/Jun/24 20:40,,,,,,,,,,,,,,Client / Job Submission,Table SQL / Client,,,0,,,,"Now there are lots of unnecessary overhead involved in submitting jobs and fetching results to a long-running flink cluster. This works well for streaming and batch job, because in these scenarios users will not frequently submit jobs and fetch result to a running cluster.

 

But in OLAP scenario, users will continuously submit lots of short-lived jobs to the running cluster. In this situation, these overhead will have a huge impact on the E2E performance.  Here are some examples of unnecessary overhead:
 * Each `RemoteExecutor` will create a new `StandaloneClusterDescriptor` when executing a job on the same remote cluster
 * `StandaloneClusterDescriptor` will always create a new `RestClusterClient` when retrieving an existing Flink Cluster
 * Each `RestClusterClient` will create a new `ClientHighAvailabilityServices` which might contains a resource-consuming ha client(ZKClient or KubeClient) and a time-consuming leader retrieval operation
 * `RestClient` will create a new connection for every request which costs extra connection establishment time

 

Therefore, I suggest creating this ticket and following subtasks to improve this performance. This ticket is also relates to  FLINK-25318.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25318,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 09:03:29 UTC 2024,,,,,,,,,,"0|z1lx28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Jan/24 09:03;xiangyu0xf;[~guoyangze]  Hi, this Flip(https://cwiki.apache.org/confluence/display/FLINK/FLIP-407%3A+Improve+Flink+Client+performance+in+interactive+scenarios) has been approved, would you kindly assign this Jira to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reuse source operator input records/bytes metrics for SourceOperatorStreamTask,FLINK-33682,13559828,13559826,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Abandoned,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,29/Nov/23 05:32,26/Jan/24 03:54,04/Jun/24 20:40,27/Dec/23 06:33,,,,,,,,,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"For SourceOperatorStreamTask, source opeartor is the head operator that takes input. We can directly reuse source operator input records/bytes metrics for it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 27 06:32:36 UTC 2023,,,,,,,,,,"0|z1lx0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Dec/23 06:32;Zhanghao Chen;Closing this ticket as we found a better unified implementation for it. See [FLINK-33681|https://issues.apache.org/jira/browse/FLINK-33681];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Display source/sink numRecordsIn/Out & numBytesIn/Out on UI,FLINK-33681,13559826,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Zhanghao Chen,Zhanghao Chen,29/Nov/23 05:30,29/May/24 01:12,04/Jun/24 20:40,,1.17.2,1.18.0,,,,,,,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"Currently, the numRecordsIn & numBytesIn metrics for sources and the numRecordsOut & numBytesOut metrics for sinks are always 0 on the Flink web dashboard.

FLINK-11576 brings us these metrics on the opeartor level, but it does not integrate them on the task level. On the other hand, the summay metrics on the job overview page is based on the task level I/O metrics. As a result, even though new connectors supporting FLIP-33 metrics will report operator-level I/O metrics, we still cannot see the metrics on dashboard.

This ticket serves as an umbrella issue to integrate standard source/sink I/O metrics with the corresponding task I/O metrics. 

!screenshot-20231224-195605-1.png|width=608,height=333!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-7286,FLINK-11576,,,,,,,,"25/Dec/23 03:17;Zhanghao Chen;screenshot-20231224-195605-1.png;https://issues.apache.org/jira/secure/attachment/13065586/screenshot-20231224-195605-1.png","25/Dec/23 04:04;Zhanghao Chen;screenshot-20231225-120421.png;https://issues.apache.org/jira/secure/attachment/13065589/screenshot-20231225-120421.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 29 00:42:02 UTC 2024,,,,,,,,,,"0|z1lx0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Dec/23 03:40;Zhanghao Chen;[~renqs] Hi, I'm reaching out to you to have some inputs. From the comments of FLINK-11576 & [FLIP-33|https://cwiki.apache.org/confluence/display/FLINK/FLIP-33%3A+Standardize+Connector+Metrics], it seems that at least in some previous versions of Flink, the I/O metrics of the source/sink can be seen from the web dashboard. However, I verified that this is not the case in Flink 1.17 & 1.18 (see the screenshot attached). As I do not have a working environment of ealier versions of Flink, not sure if this is a bug fix or just an improvement.

After investigation, the current version of Flink records task I/O metrics as follows:
 # numBytesIn: collected in InputGateWithMetrics, which only accounts for Flink internal traffic. Before FLINK-29567, the operator-level numBytesIn & numBytesOut counter reuses the task-level I/O counters, so Flink is able to collect source input bytes metrics as well.
 # numBytesOut: collected in ResultPartition, which only accounts for Flink internal traffic as well.Before FLINK-29567, the operator-level numBytesIn & numBytesOut counter reuses the task-level I/O counters, so Flink is able to collect sink output bytes metrics as well.
 # numRecordsIn: reuse the numRecordsIn metric of the head operators SingleInput MultitpleInput. I didn't find evidence on how Flink was able to collect source numberRecordsIn metrics in previous versions.
 # numRecordsOut:  reuse the numRecordsOut metric of the tail operator in the opeartor chain. Specifically, the reuse will skip if the opeartor is a SinkWriterOperator, which means sink output metrics won't be exposed to the UI as well.

We can see that the I/O metrics of the source/sink are not integrated into the task-level I/O metrics now, and hence cannot be seen on the UI.

Fix proposal: just reuse opeartor I/O records/bytes metrics for task I/O metrics for SourceOperator & SinkWriterOperator.

Looking forward to your suggestions on it.;;;","25/Dec/23 04:05;Zhanghao Chen;Attached is a working demo:

!screenshot-20231225-120421.png|width=669,height=374!;;;","09/Jan/24 06:57;Zhanghao Chen;[~renqs] Kindly remind~ Would you mind take a look at the issue? I think the feature is useful for debugging, and would like to hear from someone who is a master in this field. Many thanks in advance;;;","28/May/24 21:04;becket_qin;[~Zhanghao Chen] Thanks for firing the ticket. The solution sounds good to me. Are you interested in authoring the patch? I am happy to review.

BTW, technically speaking this is a user sensible behavior change. So a FLIP would be required. It is probably going to be a quick FLIP, though.;;;","29/May/24 00:42;Zhanghao Chen;[~becket_qin] Thanks for the attention! I've already drafted a [patch|https://github.com/apache/flink/pull/23998] for review. Agree that we'll need a FLIP, I'll draft one for discussion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to build document with docker,FLINK-33680,13559825,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tomato,tomato,29/Nov/23 05:29,30/Nov/23 06:56,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Documentation,,,,0,docement,doc-site,pull-request-available,"Follow the documentation, the documentation comes from [https://github.com/apache/flink/blob/master/docs/README.md]

 

The implementation results are as follows:
{code:java}
[root@bigdatadev workpace]# git clone https://github.com/apache/flink.git
...

[root@bigdatadev workpace]# cd flink/docs/
[root@bigdatadev docs]# ./setup_docs.sh


[root@bigdatadev docs]# docker pull jakejarvis/hugo-extended:latest
latest: Pulling from jakejarvis/hugo-extended
Digest: sha256:f659daa3b52693d8f6fc380e4fc5d0d3faf5b9c25ef260244ff67625c59c45a7
Status: Image is up to date for jakejarvis/hugo-extended:latest
docker.io/jakejarvis/hugo-extended:latest


[root@bigdatadev docs]#  docker run -v $(pwd):/src -p 1313:1313 jakejarvis/hugo-extended:latest server --buildDrafts --buildFuture --bind 0.0.0.0
Start building sites … 
hugo v0.113.0-085c1b3d614e23d218ebf9daad909deaa2390c9a+extended linux/amd64 BuildDate=2023-06-05T15:04:51Z VendorInfo=docker
Built in 515 ms
Error: error building site: assemble: ""/src/content/_index.md:36:1"": failed to extract shortcode: template for shortcode ""columns"" not found
 {code}
 

[root@bigdatadev docs]# vim content/_index.md
{panel}
 30 # Apache Flink Documentation
 31 
 32 {{< center >}}
 33 *{*}Apache Flink{*}* is a framework and distributed processing engine for stateful computations over *unbounded* and *bounded* data streams. Flink has been designed to run in {*}all common cluster en    vironments{*}, perform computations at *in-memory* speed and at {*}any scale{*}.
 34 {{< /center >}}
 35 
{color:#de350b} 36 \{{< columns >}}{color}
 37 
 38 ### Try Flink
 39 
 40 If you’re interested in playing around with Flink, try one of our tutorials:
 41 
 42 * [Fraud Detection with the DataStream API]({{{}< ref ""docs/try-flink/datastream"" >{}}})
{panel}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 30 06:56:11 UTC 2023,,,,,,,,,,"0|z1lx08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 08:57;mapohl;The [docs build in CI|https://github.com/apache/flink/actions] doesn't have this issue. I tried to reproduce it locally. Looks like you missed running the submodule update that's documented in [docs/README.md:20|https://github.com/apache/flink/blob/5c1e9f3b1449cb77276d578b344d9a69c7cf9a3c/docs/README.md#L20].

I'm gonna close this issue. Feel free to reopen it if you think that the cause is a different one.;;;","30/Nov/23 02:42;tomato;Thank you for your reply. I checked my execution record and found that I had executed the `git submodule update --init --recursive` command, but I ignored the alarm and executed it successfully in the root directory.
 
I plan to optimize the documentation description to make it more detailed, so as to avoid the same problem in the future.
{panel}
[root@bigdatadev docs]#  git clone [https://github.com/apache/flink.git|https://github.com/section9-lab/flink.git]
[root@bigdatadev docs]#  cd flink/docs/
[root@bigdatadev docs]# git submodule update --init --recursive
You need to run this command from the toplevel of the working tree.
[root@bigdatadev docs]#  ./setup_docs.sh
...
{panel}
 ;;;","30/Nov/23 06:56;mapohl;What git version are you using? I tried to run the command from within the {{./docs}} subfolder and didn't have any issues.
{code:java}
$ git --version
git version 2.34.1 {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestoreMode uses NO_CLAIM as default instead of LEGACY,FLINK-33679,13559816,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zakelly,easonqin,easonqin,29/Nov/23 03:47,17/Jan/24 02:17,04/Jun/24 20:40,17/Jan/24 02:17,,,,,,,,,1.19.0,,,,Documentation,Runtime / State Backends,,,0,pull-request-available,,,"RestoreMode uses NO_CLAIM as default instead of LEGACY.
{code:java}
public enum RestoreMode implements DescribedEnum {
    CLAIM(
            ""Flink will take ownership of the given snapshot. It will clean the""
                    + "" snapshot once it is subsumed by newer ones.""),
    NO_CLAIM(
            ""Flink will not claim ownership of the snapshot files. However it will make sure it""
                    + "" does not depend on any artefacts from the restored snapshot. In order to do that,""
                    + "" Flink will take the first checkpoint as a full one, which means it might""
                    + "" reupload/duplicate files that are part of the restored checkpoint.""),
    LEGACY(
            ""This is the mode in which Flink worked so far. It will not claim ownership of the""
                    + "" snapshot and will not delete the files. However, it can directly depend on""
                    + "" the existence of the files of the restored checkpoint. It might not be safe""
                    + "" to delete checkpoints that were restored in legacy mode "");

    private final String description;

    RestoreMode(String description) {
        this.description = description;
    }

    @Override
    @Internal
    public InlineElement getDescription() {
        return text(description);
    }

    public static final RestoreMode DEFAULT = NO_CLAIM;
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 17 02:17:07 UTC 2024,,,,,,,,,,"0|z1lwy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 01:42;masteryhx;Hi, NO_CLAIM should be more production friendly than Legacy mode, so it's the better default option.

Do you have any other concerns about this ?;;;","30/Nov/23 05:03;easonqin;Hi [~masteryhx] , I'm confused about the term 'LEGACY' which is commented as 'This is the mode in which Flink worked so far.' However, the default restore mode is 'NO_CLAIM'. Perhaps the issue title is misleading.;;;","15/Jan/24 10:29;zakelly;Ah... I think the doc is misleading by the word 'so far', actually it is replaced by the NO_CLAIM since 1.15.
I'd like to fix the doc everywhere.;;;","17/Jan/24 02:17;masteryhx;merged fb7324b0 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove configuration getters/setters that return/set complex Java objects,FLINK-33678,13559812,13540672,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,JunRuiLi,JunRuiLi,29/Nov/23 02:38,05/Dec/23 06:37,04/Jun/24 20:40,,,,,,,,,,,,,,API / Core,,,,0,2.0-related,,,"FLINK-33581/FLIP-381 Deprecate configuration getters/setters that return/set complex Java objects. 
In Flink 2.0 we should remove these deprecated method and fields. This change will prevent users from configuring their jobs by passing complex Java objects, encouraging them to use ConfigOption instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 02:38:09.0,,,,,,,,,,"0|z1lwxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flink-conf.yaml from flink dist,FLINK-33677,13559810,13540672,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhuzh,zhuzh,29/Nov/23 02:08,05/Dec/23 06:37,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Configuration,,,,0,2.0-related,,,"FLINK-33297/FLIP-366 supports parsing standard YAML files for Flink configuration. A new configuration file config.yaml, which should be a standard YAML file, is introduced.
To ensure compatibility, in Flink 1.x, the old configuration parser will still be used if the old configuration file flink-conf.yaml exists. Only if it does not exist, the new configuration file will be used.
In Flink 2.0, we should remove the old configuration file from flink dist, as well as the old configuration parser.
",,,,,,,,,,FLINK-33297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-29 02:08:39.0,,,,,,,,,,"0|z1lwww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for WindowAggregate node,FLINK-33676,13559778,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,28/Nov/23 21:39,08/Apr/24 11:58,04/Jun/24 20:40,08/Apr/24 11:58,,,,,,,,,1.20.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 08 11:58:31 UTC 2024,,,,,,,,,,"0|z1lwps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/24 11:58;dwysakowicz;Implemented in 0dc2a0f74cf8d8ac10a54fa6c4f0c1f32dc2e273..68cc61a86187021c61e7f51ccff8c5912125d013;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialize ValueLiteralExpressions into SQL,FLINK-33675,13559728,13559333,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,28/Nov/23 15:25,30/Nov/23 13:03,04/Jun/24 20:40,30/Nov/23 13:03,,,,,,,,,1.19.0,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 30 13:03:25 UTC 2023,,,,,,,,,,"0|z1lweo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 13:03;dwysakowicz;Implemented in 88509e20df2edd80db9a893da21a9b3cb2777bf9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the wmassigners module, words are misspelled",FLINK-33674,13559689,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tomato,tomato,28/Nov/23 11:09,11/Mar/24 12:43,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Runtime,,,,0,easyfix,pull-request-available,,"RowTimeMiniBatch{color:#FF0000}Assginer{color}Operator changed to RowTimeMiniBatch{color:#ff8b00}Assigner{color}Operator

RowTimeMiniBatch{color:#FF0000}Assginer{color}OperatorTest changed to RowTimeMiniBatch{color:#ff8b00}Assigner{color}OperatorTest",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Nov 29 04:16:08 UTC 2023,,,,,,,,,,"0|z1lw60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 12:09;martijnvisser;[~tomato] Please see the last item on agreement/consensus in https://flink.apache.org/how-to-contribute/contribute-code/#consensus and the overall code contribution guide;;;","29/Nov/23 04:16;tomato;Thank you very much, the documentation on the contributor guide has taught me a lot. Just started trying to contribute to the flink community, what do I need to do about this PR?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SizeLimits not being set on emptyDir,FLINK-33673,13559677,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,tagarr,tagarr,28/Nov/23 09:32,03/Jun/24 11:58,04/Jun/24 20:40,,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.9.0,,,,Kubernetes Operator,,,,0,,,,"The operator should set a sizeLimit on any emptyDir's it creates. See [https://main.kyverno.io/policies/other/a/add-emptydir-sizelimit/add-emptydir-sizelimit/. |https://main.kyverno.io/policies/other/a/add-emptydir-sizelimit/add-emptydir-sizelimit/]

This issue is to set a sizeLimit. The default one in question is for artifacts. My initial guess at a setting would be around 512Mb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 03 11:58:40 UTC 2024,,,,,,,,,,"0|z1lw3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Mar/24 09:29;asoliman;[~tagarr] Can you provide an example where the emptyDir is being created? Do you mean creating for the artifacts? ;;;","03/Jun/24 11:55;tagarr;It is defined in both the operator and the deployment examples e.g. [https://github.com/apache/flink-kubernetes-operator/blob/454ac6e71a4ed590e2e9e414e723c8d5c635faf4/helm/flink-kubernetes-operator/templates/flink-operator.yaml#L236 |https://github.com/apache/flink-kubernetes-operator/blob/454ac6e71a4ed590e2e9e414e723c8d5c635faf4/helm/flink-kubernetes-operator/templates/flink-operator.yaml#L236]and [https://github.com/apache/flink-kubernetes-operator/blob/454ac6e71a4ed590e2e9e414e723c8d5c635faf4/examples/pod-template.yaml#L47] amongst other places (docs etc);;;","03/Jun/24 11:58;tagarr;Though CR's we could document that users should specify a size limit, the main issue is in the operator itself and the 
flink-artifacts-volume volume mount. Though I'm not sure what the best setting should be and was hoping for feedback from some of the maintainers;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use MapState.entries() instead of keys() and get() in over window,FLINK-33672,13559667,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,28/Nov/23 08:34,12/Dec/23 03:30,04/Jun/24 20:40,12/Dec/23 02:15,,,,,,,,,1.19.0,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"In code logic related with over windows, such as org.apache.flink.table.runtime.operators.over.ProcTimeRangeBoundedPrecedingFunction
{code:java}
private transient MapState<Long, List<RowData>> inputState;

public void onTimer(
        long timestamp,
        KeyedProcessFunction<K, RowData, RowData>.OnTimerContext ctx,
        Collector<RowData> out)
        throws Exception {
    //...
    Iterator<Long> iter = inputState.keys().iterator();
    //...
    while (iter.hasNext()) {
        Long elementKey = iter.next();
        if (elementKey < limit) {
            // element key outside of window. Retract values
            List<RowData> elementsRemove = inputState.get(elementKey);
            // ...
        }
    }
    //...
} {code}
As we can see, there is a combination of key iteration and get the value for iterated key from inputState. However for RocksDB, the key iteration calls entry iteration, which means actually we could replace it by entry iteration without introducing any extra overhead. And as a result, we could save a function call of get() by using getValue() of iterated entry at very low cost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 02:15:40 UTC 2023,,,,,,,,,,"0|z1lw14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 09:25;mayuehappy;Thanks to [~Zakelly]  for discovering this optimization point. I think this is effective and can reduce the time consumption of rocksdb.get();;;","01/Dec/23 09:51;zakelly;This is a trivial work and I would like to take this.;;;","12/Dec/23 02:15;fsk119;Merged into master: 080119cca53d9890257982b6a74a7d6f913253c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SurefireBooterForkException caused test failure,FLINK-33671,13559661,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,28/Nov/23 08:04,28/Nov/23 08:07,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Build System / CI,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54975&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11785

{code}
Nov 28 05:56:36 05:56:36.545 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (integration-tests) on project flink-table-planner_2.12: There are test failures.
Nov 28 05:56:36 05:56:36.545 [ERROR] 
Nov 28 05:56:36 05:56:36.545 [ERROR] Please refer to /__w/1/s/flink-table/flink-table-planner/target/surefire-reports for the individual test results.
Nov 28 05:56:36 05:56:36.545 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
Nov 28 05:56:36 05:56:36.545 [ERROR] ExecutionException Error occurred in starting fork, check output in log
Nov 28 05:56:36 05:56:36.545 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException Error occurred in starting fork, check output in log
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2(MojoExecutor.java:370)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute(MojoExecutor.java:351)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:215)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:171)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:163)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:294)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
Nov 28 05:56:36 05:56:36.546 [ERROR] 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:960)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:293)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:196)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:225)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:406)
Nov 28 05:56:36 05:56:36.547 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:347)
Nov 28 05:56:36 05:56:36.547 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:662)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:465)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:442)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Nov 28 05:56:36 05:56:36.548 [ERROR] 	at java.lang.Thread.run(Thread.java:748)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32722,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 08:07:40 UTC 2023,,,,,,,,,,"0|z1lvzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 08:07;mapohl;A similar error (unrelated to Flink tests) was documented in FLINK-32722.;;;","28/Nov/23 08:07;mapohl;All tests finished according to the watchdog logs:
{code}
$ grep "" \[[A-Z]*\] "" watchdog | grep ""[Rr]un"" | grep -o ""[^ ]*$"" | sort | uniq -c | sort -n | grep ""org.apache.flink"" | grep -v ""      2""
      1 org.apache.flink:flink-runtime:tests:?:jar
      1 /__w/1/.m2/repository/org/apache/flink/flink-runtime/1.19-SNAPSHOT/flink-runtime-1.19-SNAPSHOT.jar
      1 /__w/1/.m2/repository/org/apache/flink/flink-runtime/1.19-SNAPSHOT/flink-runtime-1.19-SNAPSHOT.pom
      1 /__w/1/.m2/repository/org/apache/flink/flink-runtime/1.19-SNAPSHOT/flink-runtime-1.19-SNAPSHOT-tests.jar
      1 /__w/1/.m2/repository/org/apache/flink/flink-table-runtime/1.19-SNAPSHOT/flink-table-runtime-1.19-SNAPSHOT.jar
      1 /__w/1/.m2/repository/org/apache/flink/flink-table-runtime/1.19-SNAPSHOT/flink-table-runtime-1.19-SNAPSHOT.pom
      1 /__w/1/.m2/repository/org/apache/flink/flink-table-runtime/1.19-SNAPSHOT/flink-table-runtime-1.19-SNAPSHOT-tests.jar
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Public operators cannot be reused in multi sinks,FLINK-33670,13559649,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zicat,zicat,28/Nov/23 06:41,05/Dec/23 11:04,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"Dear all:

   I find that some public operators cannot be reused when submit a job with multi sinks. I have an example as follows:
{code:java}
CREATE TABLE source (
    id              STRING,
    ts              TIMESTAMP(3),
    v              BIGINT,
    WATERMARK FOR ts AS ts - INTERVAL '3' SECOND
) WITH (...);
CREATE VIEW source_distinct AS
SELECT * FROM (
    SELECT *, ROW_NUMBER() OVER w AS row_nu
    FROM source
    WINDOW w AS (PARTITION BY id ORDER BY proctime() ASC)
) WHERE row_nu = 1;
CREATE TABLE print1 (
     id             STRING,
     ts             TIMESTAMP(3)
) WITH('connector' = 'blackhole');
INSERT INTO print1 SELECT id, ts FROM source_distinct;
CREATE TABLE print2 (
     id              STRING,
     ts              TIMESTAMP(3),
     v               BIGINT
) WITH('connector' = 'blackhole');
INSERT INTO print2
SELECT id, TUMBLE_START(ts, INTERVAL '20' SECOND), SUM(v)
FROM source_distinct
GROUP BY TUMBLE(ts, INTERVAL '20' SECOND), id; {code}
!image-2023-11-28-14-31-30-153.png|width=384,height=145!

I try to check the code, Flink add the rule of CoreRules.PROJECT_MERGE by default, This will create different digests of  the deduplicate operator and finally fail to match same sub plan.

In real production environment, Reuse same sub plan like deduplicate is more worthy than project merge. A good solution is to interrupt the project merge crossing shuffle operators in multi sinks cases. 

How did you consider it? Looking forward to your reply.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/23 06:31;zicat;image-2023-11-28-14-31-30-153.png;https://issues.apache.org/jira/secure/attachment/13064733/image-2023-11-28-14-31-30-153.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 05 11:04:00 UTC 2023,,,,,,,,,,"0|z1lvx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 05:04;libenchao;I would take this as a CTE(common table expression) optimization. Currently, there is a {{SubplanReuser}} to reuse RelNode as much as possible based on digest after optimization. It's limited since it's done after optimization, as you can see in this Jira's description.

One way to improve that is make ""temporary view"" as a real ""common table expression"" in the whole optimization process.;;;","30/Nov/23 03:29;zicat;Making ""temporary view"" as a real ""common table expression"" in the whole optimization process is a perfect solution to resolve this issue. It's can be controlled by sql developers.

I would like to know which stage the CTE feature is currently in.;;;","05/Dec/23 11:04;jeyhunkarimov;Hi [~zicat] I tried with Flink master branch, below is the plan I get for your case. Looking at the Optimized execution plan, it seems the deduplicate part (reuse_id=1) is reused between the two sinks. Do you confirm?

 

== Abstract Syntax Tree ==
LogicalSink(table=[default_catalog.default_database.print1], fields=[id, ts])
+- LogicalProject(id=[$0], ts=[$1])
   +- LogicalProject(id=[$0], ts=[$1], v=[$2], row_nu=[$3])
      +- LogicalFilter(condition=[=($3, 1)])
         +- LogicalProject(id=[$0], ts=[$1], v=[$2], row_nu=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY PROCTIME() NULLS FIRST)])
            +- LogicalTableScan(table=[[default_catalog, default_database, source]])

LogicalSink(table=[default_catalog.default_database.print2], fields=[id, EXPR$1, EXPR$2])
+- LogicalProject(id=[$1], EXPR$1=[TUMBLE_START($0)], EXPR$2=[$2])
   +- LogicalAggregate(group=[\{0, 1}], EXPR$2=[SUM($2)])
      +- LogicalProject($f0=[$TUMBLE($1, 20000:INTERVAL SECOND)], id=[$0], v=[$2])
         +- LogicalProject(id=[$0], ts=[$1], v=[$2], row_nu=[$3])
            +- LogicalFilter(condition=[=($3, 1)])
               +- LogicalProject(id=[$0], ts=[$1], v=[$2], row_nu=[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY PROCTIME() NULLS FIRST)])
                  +- LogicalTableScan(table=[[default_catalog, default_database, source]])

== Optimized Physical Plan ==
Sink(table=[default_catalog.default_database.print1], fields=[id, ts])
+- Calc(select=[id, ts], where=[=(w0$o0, 1)])
   +- OverAggregate(partitionBy=[id], orderBy=[$3 ASC], window#0=[ROW_NUMBER(*) AS w0$o0 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[id, ts, v, $3, w0$o0])
      +- Sort(orderBy=[id ASC, $3 ASC])
         +- Calc(select=[id, ts, v, PROCTIME() AS $3])
            +- Exchange(distribution=[hash[id]])
               +- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, ts, v])

Sink(table=[default_catalog.default_database.print2], fields=[id, EXPR$1, EXPR$2])
+- Calc(select=[id, w$start AS EXPR$1, EXPR$2])
   +- HashWindowAggregate(groupBy=[id], window=[TumblingGroupWindow('w$, ts, 20000)], properties=[w$start, w$end, w$rowtime], select=[id, SUM(v) AS EXPR$2])
      +- Calc(select=[ts, id, v], where=[=(w0$o0, 1)])
         +- OverAggregate(partitionBy=[id], orderBy=[$3 ASC], window#0=[ROW_NUMBER(*) AS w0$o0 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[id, ts, v, $3, w0$o0])
            +- Sort(orderBy=[id ASC, $3 ASC])
               +- Calc(select=[id, ts, v, PROCTIME() AS $3])
                  +- Exchange(distribution=[hash[id]])
                     +- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, ts, v])

== Optimized Execution Plan ==
OverAggregate(partitionBy=[id], orderBy=[$3 ASC], window#0=[ROW_NUMBER(*) AS w0$o0 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[id, ts, v, $3, w0$o0])(reuse_id=[1])
+- Exchange(distribution=[forward])
   +- Sort(orderBy=[id ASC, $3 ASC])
      +- Exchange(distribution=[keep_input_as_is[hash[id]]])
         +- Calc(select=[id, ts, v, PROCTIME() AS $3])
            +- Exchange(distribution=[hash[id]])
               +- TableSourceScan(table=[[default_catalog, default_database, source]], fields=[id, ts, v])

Sink(table=[default_catalog.default_database.print1], fields=[id, ts])
+- Calc(select=[id, ts], where=[(w0$o0 = 1)])
   +- Reused(reference_id=[1])

Sink(table=[default_catalog.default_database.print2], fields=[id, EXPR$1, EXPR$2])
+- Calc(select=[id, w$start AS EXPR$1, EXPR$2])
   +- HashWindowAggregate(groupBy=[id], window=[TumblingGroupWindow('w$, ts, 20000)], properties=[w$start, w$end, w$rowtime], select=[id, SUM(v) AS EXPR$2])
      +- Exchange(distribution=[keep_input_as_is[hash[id]]])
         +- Calc(select=[ts, id, v], where=[(w0$o0 = 1)])
            +- Exchange(distribution=[keep_input_as_is[hash[ts]]])
               +- Reused(reference_id=[1])

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update the documentation for RestartStrategy, Checkpoint Storage, and State Backend.",FLINK-33669,13559648,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,28/Nov/23 06:40,05/Dec/23 12:23,04/Jun/24 20:40,05/Dec/23 12:23,1.19.0,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,"After the deprecation of complex Java object getter and setter methods in FLIP-381, Flink now recommends the use of ConfigOptions for the configuration of RestartStrategy, Checkpoint Storage, and State Backend. It is necessary that we update FLINK documentation to clearly instruct users on this new recommended approach.",,,,,,,,,,,,,,,,,,,,,,,FLINK-33581,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 05 12:23:47 UTC 2023,,,,,,,,,,"0|z1lvww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/23 12:23;zhuzh;master:
e27f8a3a0783d551457a2f424b01267bd1c8c2c2
d9bcb3b40ed5cefadbbaf391dacaa0ecd7fc8243
52d8d3583e5c989da84126a8805ab335408c46c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decoupling Shuffle network memory and job topology,FLINK-33668,13559639,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Jiang Xin,Jiang Xin,Jiang Xin,28/Nov/23 03:08,08/Mar/24 09:47,04/Jun/24 20:40,08/Mar/24 09:46,,,,,,,,,1.20.0,,,,Runtime / Network,,,,0,pull-request-available,,,"With FLINK-30469  and FLINK-31643, we have decoupled the shuffle network memory and the parallelism of tasks by limiting the number of buffers for each InputGate and ResultPartition in Hybrid Shuffle. However, when a task contains multiple ResultPartitions or many shuffle tasks are running simultaneously on the same TaskManager, ""Insufficient number of network buffers"" errors would still occur. This usually happens when Slot Sharing Group is enabled or a TaskManager contains multiple slots.

We want to make sure that the TaskManager does not encounter ""Insufficient number of network buffers"" even if there are dozens of InputGates and ResultPartitions running on the same TaskManager simultaneously. I have given this some thought, and here is my rough proposal.

1. InputGate or ResultPartition only apply for buffers from LocalBufferPool, which means that InputGate will no longer ask for exclusive buffers from NetworkBufferPool directly.
2. When creating a LocalBufferPool, we need to specify the maximum, minimum, and expected number of buffers. Whenever a new LBP is created or destroyed, a redistribution will occur, to adjust the buffer count of all LocalBufferPools using the expected value as a weight and between the minimum and maximum values. According to the test, the minimum value can be set to 4 to make the Flink Job work despite the possibility of lower performance. With this minimum value, a task with 20 shuffle edges needs only 5MB of memory to avoid ""insufficient network buffer"" error.
3. During runtime, InputGate and ResultPartition both calculate the number of buffers used by their internal data structures based on the pool size of their corresponding LocalBufferPool, such as the exclusive buffer queue of InputGate and BufferAccumulator of ResultPartition.",,,,,,,,,,,,,,,,,,,,,,FLINK-33004,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 09:46:12 UTC 2024,,,,,,,,,,"0|z1lvuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 01:49;lsy;Big +1, there also has  a depulicated issue:  https://issues.apache.org/jira/browse/FLINK-33004;;;","08/Mar/24 09:46;Weijie Guo;master(1.20) via 4171b980717e5dc313d781210a5bbfd35449ee50.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for MatchRecognize node,FLINK-33667,13559615,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,27/Nov/23 21:26,07/Dec/23 13:00,04/Jun/24 20:40,06/Dec/23 08:49,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 08:49:55 UTC 2023,,,,,,,,,,"0|z1lvpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 08:49;dwysakowicz;Implemented in 60cc00e5e6abc0b7309a48a37e171dae9fa98183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MergeTableLikeUtil uses different constraint name than Schema,FLINK-33666,13559577,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jeyhunkarimov,twalthr,twalthr,27/Nov/23 15:20,14/Dec/23 19:14,04/Jun/24 20:40,07/Dec/23 08:45,1.18.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"{{MergeTableLikeUtil}} uses a different algorithm to name constraints than {{Schema}}. 

{{Schema}} includes the column names.
{{MergeTableLikeUtil}} uses a hashCode which means it might depend on JVM specifics.

For consistency we should use the same algorithm. I propose to use {{Schema}}'s logic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 14 19:14:26 UTC 2023,,,,,,,,,,"0|z1lvh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 09:26;jeyhunkarimov;Hi [~twalthr] can you please assign this task to me or give me access to self-assign tasks?;;;","07/Dec/23 08:41;twalthr;Thanks [~jeyhunkarimov] for working on this.;;;","07/Dec/23 08:45;twalthr;Fixed in master: ca1c7ce48127472a7c7965099f8a7227549f09df;;;","07/Dec/23 22:43;jhughes;This change seems to prevent `org.apache.flink.table.api.internal.TableEnvironmentImpl.loadPlan` from working on Compiled Plans which used the previous method.  

The PR changes restore test plans which should never be updated.  

I was able to see this since [~bvarghese]'s separate PR was merged to master today and it failed CI here:


https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55321&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","14/Dec/23 18:09;twalthr;[~jhughes] was this issue resolved? Was it just a problem of the test base? What was the complete exception stack trace?;;;","14/Dec/23 19:14;jhughes;[~twalthr] Yes, the issue has been resolved.

The issue was a change in expectations around PK.  From offline conversations, you assured me that the change is ok and will not impact folks using compiled plans.  I think we are good.

An example stack trace is:

```
Dec 07 13:49:11 13:49:11.037 [ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.GroupAggregateRestoreTest.testRestore(TableTestProgram, ExecNodeMetadata)[1] Time elapsed: 0.17 s <<< ERROR! 
Dec 07 13:49:11 org.apache.flink.table.api.TableException: Cannot load Plan from file '/__w/1/s/flink-table/flink-table-planner/src/test/resources/restore-tests/stream-exec-group-aggregate_1/group-aggregate-simple/plan/group-aggregate-simple.json'. 
Dec 07 13:49:11 at org.apache.flink.table.api.internal.TableEnvironmentImpl.loadPlan(TableEnvironmentImpl.java:760) 
Dec 07 13:49:11 at org.apache.flink.table.planner.plan.nodes.exec.testutils.RestoreTestBase.testRestore(RestoreTestBase.java:279)
....

Dec 07 13:49:11 Caused by: org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: The schema of table 'default_catalog.default_database.sink_t' from the persisted plan does not match the schema loaded from the catalog: '( 
Dec 07 13:49:11 `b` BIGINT NOT NULL, 
Dec 07 13:49:11 `cnt` BIGINT, 
Dec 07 13:49:11 `avg_a` DOUBLE, 
Dec 07 13:49:11 `min_c` STRING, 
Dec 07 13:49:11 CONSTRAINT `PK_129` PRIMARY KEY (`b`) NOT ENFORCED 
Dec 07 13:49:11 )' != '( 
Dec 07 13:49:11 `b` BIGINT NOT NULL, 
Dec 07 13:49:11 `cnt` BIGINT, 
Dec 07 13:49:11 `avg_a` DOUBLE, 
Dec 07 13:49:11 `min_c` STRING, 
Dec 07 13:49:11 CONSTRAINT `PK_b` PRIMARY KEY (`b`) NOT ENFORCED 
Dec 07 13:49:11 )'. Make sure the table schema in the catalog is still identical. (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[5]->org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink[""dynamicTableSink""]->org.apache.flink.table.planner.plan.nodes.exec.spec.DynamicTableSinkSpec[""table""]) 
...
```;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid code execution,FLINK-33665,13559552,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,Peihui,Peihui,27/Nov/23 12:05,28/Nov/23 09:08,04/Jun/24 20:40,,,,,,,,,,,,,,Library / CEP,,,,0,,,,"!image-2023-11-27-20-00-28-715.png!

!image-2023-11-27-20-00-59-544.png!

As shown in the above pictures, 
{code:java}
// code placeholder
nfaState.getPartialMatches()
                .removeIf(pm -> pm.getStartEventID() != null && !partialMatches.contains(pm)); {code}
This code is unnecessary. After method processMatchesAccordingToSkipStrategy,  
{code:java}
// code placeholder
nfaState.setNewPartialMatches(newPartialMatches) {code}
It is very time-consuming when there are many partial matches.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/23 12:00;Peihui;image-2023-11-27-20-00-28-715.png;https://issues.apache.org/jira/secure/attachment/13064708/image-2023-11-27-20-00-28-715.png","27/Nov/23 12:01;Peihui;image-2023-11-27-20-00-59-544.png;https://issues.apache.org/jira/secure/attachment/13064707/image-2023-11-27-20-00-59-544.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 09:08:24 UTC 2023,,,,,,,,,,"0|z1lvbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 12:09;Peihui;If the problem persists, please assign it to me.:);;;","28/Nov/23 09:08;mapohl;Thanks [~Peihui] for raising this issue.

[~dwysakowicz] could you give your thoughts on that one?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setup cron build for java 21,FLINK-33664,13559551,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,27/Nov/23 11:54,29/Nov/23 07:16,04/Jun/24 20:40,29/Nov/23 07:16,,,,,,,,,,,,,Build System / CI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 07:16:33 UTC 2023,,,,,,,,,,"0|z1lvbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 07:16;Sergey Nuyanzin;Merged to master as [76f754833298beb3e7589d521e75325adf283bf3|https://github.com/apache/flink/commit/76f754833298beb3e7589d521e75325adf283bf3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialize CallExpressions into SQL,FLINK-33663,13559539,13559333,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,27/Nov/23 10:40,05/Dec/23 08:47,04/Jun/24 20:40,05/Dec/23 08:47,,,,,,,,,1.19.0,,,,Table SQL / API,,,,0,pull-request-available,,,The task is about introducing {{CallSyntax}} and implementing versions for non-standard SQL functions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 05 08:47:26 UTC 2023,,,,,,,,,,"0|z1lv8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/23 08:47;dwysakowicz;Implemented in bc6c2cec37c45f021ae22a2a7b5ab9537b8506cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump com.h2database:h2,FLINK-33662,13559533,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Nov/23 09:54,27/Nov/23 10:43,04/Jun/24 20:40,27/Nov/23 10:43,,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 10:43:24 UTC 2023,,,,,,,,,,"0|z1lv7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 10:43;martijnvisser;Fixed in apache/flink-connector-jdbc:main b477d452ba3aac38d53d1f5d4c4820bdad3ad9cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'RocksDB Memory Management end-to-end test' failed due to unexpected error in logs,FLINK-33661,13559527,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,27/Nov/23 09:22,27/Nov/23 09:23,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / State Backends,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54942&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=5132

This seems to be the same issue as FLINK-30785.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30785,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-27 09:22:56.0,,,,,,,,,,"0|z1lv68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregateITCase.testConstantGroupKeyWithUpsertSink due to unexpected behavior,FLINK-33660,13559522,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,27/Nov/23 08:58,27/Nov/23 08:58,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Coordination,Table SQL / Runtime,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54895&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11063

{code}

Nov 25 05:02:47 05:02:47.850 [ERROR] org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testConstantGroupKeyWithUpsertSink  Time elapsed: 107.302 s  <<< ERROR!
Nov 25 05:02:47 java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Nov 25 05:02:47 	at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
Nov 25 05:02:47 	at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
Nov 25 05:02:47 	at org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testConstantGroupKeyWithUpsertSink(AggregateITCase.scala:1620)
Nov 25 05:02:47 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 25 05:02:47 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 25 05:02:47 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 25 05:02:47 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 25 05:02:47 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Nov 25 05:02:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Nov 25 05:02:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Nov 25 05:02:47 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:94)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
Nov 25 05:02:47 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
Nov 25 05:02:47 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Nov 25 05:02:47 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
Nov 25 05:02:47 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Nov 25 05:02:47 	at scala.collection.convert.Wrappers$IteratorWrapper.forEachRemaining(Wrappers.scala:26)
Nov 25 05:02:47 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Nov 25 05:02:47 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Nov 25 05:02:47 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Nov 25 05:02:47 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Nov 25 05:02:47 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Nov 25 05:02:47 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)
Nov 25 05:02:47 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
Nov 25 05:02:47 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Nov 25 05:02:47 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Nov 25 05:02:47 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
Nov 25 05:02:47 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
Nov 25 05:02:47 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Nov 25 05:02:47 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
Nov 25 05:02:47 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Nov 25 05:02:47 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Nov 25 05:02:47 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Nov 25 05:02:47 Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
Nov 25 05:02:47 	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
Nov 25 05:02:47 	at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
Nov 25 05:02:47 	at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1640)
Nov 25 05:02:47 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Nov 25 05:02:47 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Nov 25 05:02:47 	at java.lang.Thread.run(Thread.java:748)
Nov 25 05:02:47 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Nov 25 05:02:47 	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
Nov 25 05:02:47 	... 6 more
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Nov 25 05:02:47 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
Nov 25 05:02:47 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Nov 25 05:02:47 	at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Nov 25 05:02:47 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1287)
Nov 25 05:02:47 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
Nov 25 05:02:47 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Nov 25 05:02:47 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Nov 25 05:02:47 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
Nov 25 05:02:47 	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
Nov 25 05:02:47 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
Nov 25 05:02:47 	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
Nov 25 05:02:47 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
Nov 25 05:02:47 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
Nov 25 05:02:47 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
Nov 25 05:02:47 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
Nov 25 05:02:47 	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
Nov 25 05:02:47 	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
Nov 25 05:02:47 	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
Nov 25 05:02:47 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
Nov 25 05:02:47 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
Nov 25 05:02:47 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
Nov 25 05:02:47 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
Nov 25 05:02:47 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
Nov 25 05:02:47 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Nov 25 05:02:47 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Nov 25 05:02:47 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=1, backoffTimeMS=0)
Nov 25 05:02:47 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
Nov 25 05:02:47 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
Nov 25 05:02:47 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
Nov 25 05:02:47 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
Nov 25 05:02:47 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
Nov 25 05:02:47 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:765)
Nov 25 05:02:47 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:742)
Nov 25 05:02:47 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)
Nov 25 05:02:47 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)
Nov 25 05:02:47 	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
Nov 25 05:02:47 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 25 05:02:47 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 25 05:02:47 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)
Nov 25 05:02:47 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
Nov 25 05:02:47 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)
Nov 25 05:02:47 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)
Nov 25 05:02:47 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Nov 25 05:02:47 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Nov 25 05:02:47 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Nov 25 05:02:47 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Nov 25 05:02:47 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Nov 25 05:02:47 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Nov 25 05:02:47 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Nov 25 05:02:47 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Nov 25 05:02:47 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Nov 25 05:02:47 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Nov 25 05:02:47 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Nov 25 05:02:47 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Nov 25 05:02:47 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Nov 25 05:02:47 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Nov 25 05:02:47 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Nov 25 05:02:47 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Nov 25 05:02:47 	... 4 more
Nov 25 05:02:47 Caused by: java.lang.Exception: Artificial Failure
Nov 25 05:02:47 	at org.apache.flink.table.planner.runtime.utils.FailingCollectionSource.run(FailingCollectionSource.java:173)
Nov 25 05:02:47 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114)
Nov 25 05:02:47 	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71)
Nov 25 05:02:47 	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-27 08:58:18.0,,,,,,,,,,"0|z1lv54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid unnecessary retries when restore from savepoint failed.,FLINK-33659,13559520,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,Leo Zhou,Leo Zhou,27/Nov/23 08:44,27/Nov/23 10:25,04/Jun/24 20:40,27/Nov/23 10:25,,,,,,,,,,,,,Runtime / Checkpointing,,,,0,,,,"when restore a job from savepoint failed, if restart strategy is enabled, flink will try to restart the job, and the restore would fail again.

We may wrap the exception as [SuppressRestartsException|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/execution/SuppressRestartsException.java] to avoid unnecessary retries.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-27 08:44:58.0,,,,,,,,,,"0|z1lv4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hosted runner lost communication,FLINK-33658,13559516,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,27/Nov/23 08:40,27/Nov/23 08:47,04/Jun/24 20:40,27/Nov/23 08:47,,,,,,,,,,,,,Test Infrastructure,,,,0,github-actions,test-stability,,"Some jobs failed due to lost communication: https://github.com/XComp/flink/actions/runs/6997726518

{quote}
The hosted runner: GitHub Actions 15 lost communication with the server. Anything in your workflow that terminates the runner process, starves it for CPU/Memory, or blocks its network access can cause this error.
{quote}

This is not really something we can fix. The issue is created for documentation purposes.",,,,,,,,,,,,,,,,,,,,,,FLINK-33552,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-27 08:40:04.0,,,,,,,,,,"0|z1lv3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert message in top n without row number didn't consider it's number and may incorrect,FLINK-33657,13559511,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zlzhang0122,zlzhang0122,27/Nov/23 08:18,27/Nov/23 08:20,04/Jun/24 20:40,,1.16.2,1.17.1,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"The new insert message in top n without row number didn't consider it's order and just 
collectInsert() to the next operator, this may incorrect when the next operator collect all the top n records and aggregate it.
 
For example：
create table user_info(

user_id int,

item_id int,
app string,

dt timestamp
) whith(
'connector'='kafka',

...
);
create table redis_sink (
redis_key string,
hash_key string,
hash_value string
)
with (
'connector' = 'redis',
'command' = 'hmset'
'nodes' = 'xxx',
'additional-ttl' = 'xx'
);
create view base_lastn

as select * from(
select user_id,item_id,app,dt,row_number() over(partition by item_id, app order by dt desc) as rn from user_action
)t where rn<=5;

insert into redis_sink

select
concat(""id"", item_id) as redis_key, app as hash_key,user_id as hash_value from base_lastn where rn=1;
insert into redis_sink

select
concat(""list_"", item_id) as redis_key, app as hash_key,listagg(user_id,"","") as hash_value from base_lastn where group by item_id, app;
 

There will be a scene that the value in the top 1 will not appear in the first or last value of the top 5.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-27 08:18:54.0,,,,,,,,,,"0|z1lv2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If json.ignose-parse-errors =true is configured and Array parsing errors occur, other columns will be empty",FLINK-33656,13559481,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,duke13,duke13,27/Nov/23 03:50,30/Nov/23 07:13,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,,,,"If json.ignore-parse-errors is set to true and Array parsing errors occur, the fields following array are resolved as empty in the complete json message

Create Table DDL

create table default_catalog.default_database.test (
id                   STRING
,sheetNo              STRING
,attentionList        ARRAY<STRING>
)
WITH ('connector' = 'kafka',
'topic' = 'test',
'properties.bootstrap.servers' = 'xxx',
'scan.startup.mode' = 'latest-offset',
'format' = 'json',
'json.ignore-parse-errors' = 'true',
'json.map-null-key.mode' = 'LITERAL'
);

1.、 !image-2023-11-27-13-59-42-066.png|width=501,height=128!

!image-2023-11-27-14-00-04-672.png|width=631,height=85!

2.

!image-2023-11-27-14-00-41-176.png|width=506,height=67!

!image-2023-11-27-14-01-12-187.png|width=493,height=118!

3.

!image-2023-11-27-14-02-52-065.png|width=493,height=181!

!image-2023-11-27-14-03-10-885.png|width=546,height=144!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/23 05:58;duke13;image-2023-11-27-13-58-22-513.png;https://issues.apache.org/jira/secure/attachment/13064691/image-2023-11-27-13-58-22-513.png","27/Nov/23 05:59;duke13;image-2023-11-27-13-59-42-066.png;https://issues.apache.org/jira/secure/attachment/13064692/image-2023-11-27-13-59-42-066.png","27/Nov/23 06:00;duke13;image-2023-11-27-14-00-04-672.png;https://issues.apache.org/jira/secure/attachment/13064693/image-2023-11-27-14-00-04-672.png","27/Nov/23 06:00;duke13;image-2023-11-27-14-00-41-176.png;https://issues.apache.org/jira/secure/attachment/13064694/image-2023-11-27-14-00-41-176.png","27/Nov/23 06:01;duke13;image-2023-11-27-14-01-12-187.png;https://issues.apache.org/jira/secure/attachment/13064695/image-2023-11-27-14-01-12-187.png","27/Nov/23 06:02;duke13;image-2023-11-27-14-02-01-252.png;https://issues.apache.org/jira/secure/attachment/13064696/image-2023-11-27-14-02-01-252.png","27/Nov/23 06:02;duke13;image-2023-11-27-14-02-30-666.png;https://issues.apache.org/jira/secure/attachment/13064697/image-2023-11-27-14-02-30-666.png","27/Nov/23 06:02;duke13;image-2023-11-27-14-02-52-065.png;https://issues.apache.org/jira/secure/attachment/13064698/image-2023-11-27-14-02-52-065.png","27/Nov/23 06:03;duke13;image-2023-11-27-14-03-10-885.png;https://issues.apache.org/jira/secure/attachment/13064699/image-2023-11-27-14-03-10-885.png","30/Nov/23 06:47;duke13;image-2023-11-30-14-47-02-300.png;https://issues.apache.org/jira/secure/attachment/13064833/image-2023-11-30-14-47-02-300.png",,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 30 06:51:22 UTC 2023,,,,,,,,,,"0|z1luw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 08:05;martijnvisser;[~duke13] Can you please provide a reproducer? I don't see how we could reproduce this based off the images;;;","27/Nov/23 08:46;duke13;If the type of the attentionList field is defined as Array, but the value does not conform to the array parsing structure, and the attentionList field is at the head or not at the end of the json message, all subsequent fields of attentionList are returned as null.

For example: \{""attentionList"":{},""id"":""1234"",""sheetNo"":""1700803757032""} In the json string, attentionList is located at the head of the entire json message, which is read in the flink sql json format. The source table structure is as follows:
create table default_catalog.default_database.test (
id                   STRING
,sheetNo              STRING
,attentionList        ARRAY<STRING>
)
WITH ('connector' = 'kafka',
'topic' = 'test',
'properties.bootstrap.servers' = 'xxx',
'scan.startup.mode' = 'latest-offset',
'format' = 'json',
'json.ignore-parse-errors' = 'true',
'json.map-null-key.mode' = 'LITERAL'
);
Obviously the attentionList field is not parsed correctly, which will result in the id and sheetNo fields being empty;
Suppose you modify the json message :\{""id"":""1234"",""attentionList"":{},""sheetNo"":""1700803757032""}, depending on the location of attentionList, sheetNo will be empty.;;;","30/Nov/23 06:51;duke13;Hi,[~martijnvisser]

I found that in version 1.18 json parsing uses a new class class, and the method is controlled by the decode.json-parser.enabled parameter, but this parameter does not take effect according to the value configured in the sql. And array resolution through JsonParserRowDataDeserializationSchema class should be using the reconstruction after the class, resulting in abnormal array resolution after eventually lead to the problem.

!image-2023-11-30-14-47-02-300.png|width=353,height=321!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Archunit to 1.1.0+,FLINK-33655,13559469,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,26/Nov/23 22:29,27/Nov/23 08:47,04/Jun/24 20:40,27/Nov/23 08:47,1.19.0,,,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,,,"ASM 9.5 (which has support for java 21[1]) came only with ArchUnit 1.1.0 [2]

With current ArchUnit(1.0.0) in case of jdk21 it fails

{noformat}
mvn clean install -DskipTests -Dfast -Pjava21-target
mvn verify -pl flink-architecture-tests/flink-architecture-tests-production/ -Darchunit.freeze.store.default.allowStoreUpdate=false
{noformat}
like
{noformat}
Nov 26 16:07:42 16:07:42.024 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.742 s <<< FAILURE! - in org.apache.flink.architecture.rules.ITCaseRules
Nov 26 16:07:42 16:07:42.025 [ERROR] ITCaseRules.ITCASE_USE_MINICLUSTER  Time elapsed: 0.005 s  <<< ERROR!
Nov 26 16:07:42 com.tngtech.archunit.library.freeze.StoreUpdateFailedException: Updating frozen violations is disabled (enable by configuration freeze.store.default.allowStoreUpdate=true)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.ViolationStoreFactory$TextFileBasedViolationStore.save(ViolationStoreFactory.java:125)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule$ViolationStoreLineBreakAdapter.save(FreezingArchRule.java:277)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.removeObsoleteViolationsFromStore(FreezingArchRule.java:154)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.removeObsoleteViolationsFromStoreAndReturnNewViolations(FreezingArchRule.java:146)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.evaluate(FreezingArchRule.java:127)
Nov 26 16:07:42 	at com.tngtech.archunit.lang.ArchRule$Assertions.check(ArchRule.java:84)
Nov 26 16:07:42 	at com.tngtech.archunit.library.freeze.FreezingArchRule.check(FreezingArchRule.java:97)

{noformat}

[1] https://asm.ow2.io/versions.html#9.5
[2] https://github.com/TNG/ArchUnit/pull/1098",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 08:47:47 UTC 2023,,,,,,,,,,"0|z1lutk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 08:47;Sergey Nuyanzin;Merged to master as [83a6d1d57d7d544243a924a70267437e5e0eb906|https://github.com/apache/flink/commit/83a6d1d57d7d544243a924a70267437e5e0eb906];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deprecate DummyStreamExecutionEnvironment,FLINK-33654,13559468,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,jingge,jingge,jingge,26/Nov/23 20:31,28/Nov/23 13:37,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,Deprecate DummyStreamExecutionEnvironment first since it will take time to remove it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 13:16:23 UTC 2023,,,,,,,,,,"0|z1lutc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 13:16;jingge;Need to do more research before deprecate it or not. [~martijnvisser] It would be great to hear your thoughts. Do you have any concern about it? Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a benchmark for balanced tasks scheduling,FLINK-33653,13559442,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,26/Nov/23 10:19,22/Dec/23 03:17,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Task,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-26 10:19:59.0,,,,,,,,,,"0|z1lunk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First Steps documentation is having empty page link,FLINK-33652,13559441,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,pranav.sharma,pranav.sharma,26/Nov/23 09:58,01/Dec/23 05:07,04/Jun/24 20:40,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,," 

Under this page URL [link|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/try-flink/local_installation/], under ""Summary"" heading, the ""concepts"" link is pointing to an empty page [link_on_concepts|https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/concepts/]. Upon visiting, the tab heading contains HTML as well. (Attached screenshots)

It may be pointed to concepts/overview instead.",Web,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/23 09:53;pranav.sharma;image-2023-11-26-15-23-02-007.png;https://issues.apache.org/jira/secure/attachment/13064688/image-2023-11-26-15-23-02-007.png","26/Nov/23 09:55;pranav.sharma;image-2023-11-26-15-25-04-708.png;https://issues.apache.org/jira/secure/attachment/13064687/image-2023-11-26-15-25-04-708.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 07:03:38 UTC 2023,,,,,,,,,,"0|z1lunc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 13:01;Wencong Liu;Hello [~pranav.sharma], thanks for the careful investigation. Feel free to open a pull request!;;;","28/Nov/23 07:03;pranav.sharma;Hi [~Wencong Liu] , I have raised a PR [GitHub Pull Request #23815|https://github.com/apache/flink/pull/23815].

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the Chinese version checkpointing doc in fault tolerance,FLINK-33651,13559429,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,26/Nov/23 02:59,26/Nov/23 20:21,04/Jun/24 20:40,26/Nov/23 20:21,,,,,,,,,,,,,Documentation,Runtime / Checkpointing,,,0,pull-request-available,,,"the Chinese doc missed some items compared to the English doc[1], would you mind adding them together? Such as:
 * checkpoint storage
 * unaligned checkpoints:
 * checkpoints with finished tasks:

[1] [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/#enabling-and-configuring-checkpointing]

 

Reference: https://github.com/apache/flink/pull/23795#discussion_r1404855432",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 26 20:21:00 UTC 2023,,,,,,,,,,"0|z1luko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/23 20:21;jingge;master: db0403c3f39ccd60f0c0a59f12caa63c521e1d17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Checkpoint Storage doc is missed,FLINK-33650,13559397,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,25/Nov/23 12:04,26/Nov/23 02:45,04/Jun/24 20:40,26/Nov/23 02:45,,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,"Checkpoint Storage and available-checkpoint-storage-options part is missed in the checkpoints page

[https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/checkpoints/#checkpoint-storage]

https://nightlies.apache.org/flink/flink-docs-master/zh/docs/ops/state/checkpoints/

 

!image-2023-11-25-20-04-34-670.png|width=365,height=352!

!image-2023-11-25-20-04-50-937.png|width=379,height=265!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/23 12:03;fanrui;image-2023-11-25-20-03-22-169.png;https://issues.apache.org/jira/secure/attachment/13064673/image-2023-11-25-20-03-22-169.png","25/Nov/23 12:04;fanrui;image-2023-11-25-20-04-34-670.png;https://issues.apache.org/jira/secure/attachment/13064672/image-2023-11-25-20-04-34-670.png","25/Nov/23 12:04;fanrui;image-2023-11-25-20-04-50-937.png;https://issues.apache.org/jira/secure/attachment/13064671/image-2023-11-25-20-04-50-937.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 26 02:44:58 UTC 2023,,,,,,,,,,"0|z1ludk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/23 02:44;fanrui;Merged master<1.19> via 09702a22e43fddfa1ac78e2b8db92c2d1f5395a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exponential Delay doc is missed,FLINK-33649,13559376,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,25/Nov/23 06:30,25/Nov/23 11:51,04/Jun/24 20:40,25/Nov/23 11:51,,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,"Exponential Delay doc is missed in 2 pages:
 * Configuration page missed Exponential Delay related options
 ** [https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#fault-tolerance]
 ** EN and zh doc
 * task-failure-recovery missed Exponential Delay
 ** [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#task-failure-recovery]
 ** Only zh version miss

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 25 11:51:33 UTC 2023,,,,,,,,,,"0|z1lu8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/23 11:51;fanrui;Merged master<1.19> via bd9f312fe3518695afb0be86d0348a86daff0220;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table usfs doc in Chinese is out-of-date,FLINK-33648,13559373,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,jingge,jingge,25/Nov/23 04:09,25/Nov/23 04:09,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Documentation,Table SQL / API,,,0,,,,"The English version [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/udfs/] is very different to the Chinese version [https://nightlies.apache.org/flink/flink-docs-master/zh/docs/dev/table/functions/udfs/]

 

Not only the content but also the code examples.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-25 04:09:10.0,,,,,,,,,,"0|z1lu88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for LookupJoin node,FLINK-33647,13559371,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,25/Nov/23 03:24,12/Dec/23 13:42,04/Jun/24 20:40,12/Dec/23 13:42,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 13:42:42 UTC 2023,,,,,,,,,,"0|z1lu7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 13:42;dwysakowicz;Implemented in 7e4a2f3fe6c558a08ec68dcb8e21ba43e85f3cf1..c49ab9ae42940c58579326683735ba79512f604e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup the rest usage of deprecated TableEnvironment#registerFunction and in docs,FLINK-33646,13559367,13529804,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,24/Nov/23 22:26,25/Nov/23 23:57,04/Jun/24 20:40,25/Nov/23 23:56,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"It seems there are still few places where it is not cleaned up , also docs
e.g. 
{noformat}
docs/content.zh/docs/dev/table/functions/udfs.md
docs/content.zh/docs/dev/table/tableApi.md
docs/content/docs/dev/table/tableApi.md
flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/AggregatedTable.java
flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/FlatAggregateTable.java
flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/GroupedTable.java
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/harness/TableAggregateHarnessTest.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/AggregateITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/GroupWindowITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/MatchRecognizeITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/OverAggregateITCase.scala
flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/planner/runtime/utils/UserDefinedFunctionTestUtils.scala

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 25 23:57:16 UTC 2023,,,,,,,,,,"0|z1lu6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/23 23:57;Sergey Nuyanzin;Merged to master as [d12d49ed22691ff2d4a2b619c95b6c382ba2e9ee|https://github.com/apache/flink/commit/d12d49ed22691ff2d4a2b619c95b6c382ba2e9ee];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Env vars in config not added to Taskmanagers in Standalone mode,FLINK-33645,13559343,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tagarr,tagarr,tagarr,24/Nov/23 14:59,07/Dec/23 12:20,04/Jun/24 20:40,07/Dec/23 12:20,kubernetes-operator-1.6.1,kubernetes-operator-1.7.0,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"When a flink deployment provides env var config to the taskmanager e.g.
containerized.taskmanager.env.MY_ENV_VAR: MY_DATA
 
The operator is not setting the env vars on the Taskmanager pods when running in Standalone mode. This is working in Native mode",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 12:20:02 UTC 2023,,,,,,,,,,"0|z1lu1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 12:20;gyfora;merged to main e4528260be78dabeb6552afda10e78802021984d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-393: Make QueryOperations SQL serializable,FLINK-33644,13559333,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,24/Nov/23 13:33,07/Feb/24 10:06,04/Jun/24 20:40,20/Dec/23 15:18,,,,,,,,,1.19.0,,,,Table SQL / API,,,,0,,,,https://cwiki.apache.org/confluence/display/FLINK/FLIP-393%3A+Make+QueryOperations+SQL+serializable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34399,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 10:27:40 UTC 2023,,,,,,,,,,"0|z1ltzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 02:13;fsk119;Hi [~dwysakowicz]. I am confused that how can we get the SQL from the ModifyOperation? Why this FLIP only involves QueryOperation?;;;","15/Dec/23 10:27;twalthr;[~fsk119] QueryOperations are intended to be put into a catalog as a view. This was also prepared with `QueryOperation.asSerializableString()`. `ModifyOperation` was never considered for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow StreamExecutionEnvironment's executeAsync API to use default JobName,FLINK-33643,13559328,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,wangm92,wangm92,wangm92,24/Nov/23 12:09,28/Nov/23 03:08,04/Jun/24 20:40,28/Nov/23 03:08,1.19.0,,,,,,,,1.19.0,,,,Client / Job Submission,,,,0,pull-request-available,,,"On the `execute` API of StreamExecutionEnvironment, jobName is allowed to be Null. In this case, the default jobName in StreamGraphGenerator (`DEFAULT_STREAMING_JOB_NAME` or `DEFAULT_BATCH_JOB_NAME`) will be used, but the logic of `executeAsync` does not allow jobName to be Null. I think the processing logic should be unified here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 03:08:31 UTC 2023,,,,,,,,,,"0|z1lty8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 03:08;huweihua;resolved in master: 220287539854c8f32f9d85541026f2b018bd5ef3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use docker image with jdk21 in CI,FLINK-33642,13559326,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,24/Nov/23 12:06,25/Nov/23 23:58,04/Jun/24 20:40,25/Nov/23 23:57,,,,,,,,,1.19.0,,,,Build System / CI,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 25 23:58:03 UTC 2023,,,,,,,,,,"0|z1ltxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/23 23:58;Sergey Nuyanzin;Merged to master as [2613428314ab3f51c6019776106b4a731dc03579|https://github.com/apache/flink/commit/2613428314ab3f51c6019776106b4a731dc03579];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JUnit5 fails to delete a directory on AZP for various table-planner tests,FLINK-33641,13559324,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabao.sun,Sergey Nuyanzin,Sergey Nuyanzin,24/Nov/23 11:20,20/Dec/23 17:42,04/Jun/24 20:40,20/Dec/23 17:42,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,test-stability,,"this build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54856&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11289
fails with 
{noformat}
Nov 24 02:21:53 	Suppressed: java.nio.file.DirectoryNotEmptyException: /tmp/junit1727687356898183357/junit4798298549994985259/1ac07a5866d81240870d5a2982531508
Nov 24 02:21:53 		at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
Nov 24 02:21:53 		at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
Nov 24 02:21:53 		at java.nio.file.Files.delete(Files.java:1126)
Nov 24 02:21:53 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.deleteAndContinue(TempDirectory.java:293)
Nov 24 02:21:53 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.postVisitDirectory(TempDirectory.java:288)
Nov 24 02:21:53 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.postVisitDirectory(TempDirectory.java:264)
Nov 24 02:21:53 		at java.nio.file.Files.walkFileTree(Files.java:2688)
Nov 24 02:21:53 		at java.nio.file.Files.walkFileTree(Files.java:2742)
Nov 24 02:21:53 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:264)
Nov 24 02:21:53 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:249)
Nov 24 02:21:53 		... 92 more

{noformat}


not sure however this might be related to recent JUnit4 => JUnit5 upgrade",,,,,,,,,,,,,,,,,,,,,FLINK-30132,,,,,FLINK-33637,,,,,FLINK-33900,,,,,,FLINK-33820,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 17:40:53 UTC 2023,,,,,,,,,,"0|z1ltxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/23 09:37;jiabao.sun;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54898&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","25/Nov/23 09:40;jiabao.sun;Seems TempDirectory#deleteAllFilesAndDirectories does not support recursive deletion.
We can change the StreamingWithStateTestBase#before method to prevent creating subdirectories.;;;","25/Nov/23 09:53;Sergey Nuyanzin;couldn't it be the case that tempdir is shared between tests and one test is writing somewhere in that dir while another one trying do perform delete operation?

I tend to think that it is not  a problem with recursive removal otherwise in case it is a problem with recursive removal it could be reproduced always and not once per 5-10 ci runs;;;","25/Nov/23 10:25;jiabao.sun;Thanks [~Sergey Nuyanzin] for your reminder, this problem may need to continue to investigate.;;;","29/Nov/23 07:20;Sergey Nuyanzin;same case with another test 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55012&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94;;;","29/Nov/23 07:59;mapohl;FLINK-30132 seems to cover the same topic. I closed FLINK-30132 in favor of FLINK-33641 because here's more discussion going on already. The test failures reported in FLINK-30132 are:
 * LocalRecoveryITCase#testRecoverLocallyFromProcessCrashWithWorkingDirectory ([already-wiped-out build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43366&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7])
 * JoinITCase.testLeftOuterJoin ([related 1.19 build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54925&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11391]);;;","30/Nov/23 11:36;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55042&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11999;;;","30/Nov/23 11:37;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55042&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=11778;;;","30/Nov/23 16:55;jiabao.sun;I found a similar issue in the JUnit community.

https://github.com/junit-team/junit5/issues/2811#issuecomment-1703476137

{noformat}
If a file is still open during TempDirectory.close(), deleteAndContinue() will succeed during the walk, but the held file will remain in the directory, merely marked for deletion. Then the subsequent delete() of the dir will fail.

So when TempDirectory hits this problem, it's likely that something involved in the test is still holding a handle to some file in the dir.
{noformat}
;;;","07/Dec/23 14:25;mapohl;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55146&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=11857]
{code:java}
 Dec 03 01:19:24 01:19:24.311 [ERROR] Tests run: 337, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 201.007 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase
Dec 03 01:19:24 01:19:24.311 [ERROR] org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase.testPojoField  Time elapsed: 0.37 s  <<< ERROR!
Dec 03 01:19:24 java.io.IOException: Failed to delete temp directory /tmp/junit5892046034617387805. The following paths could not be deleted (see suppressed exceptions for details): , 78d09796046c6a48aad962a2efde7d8b
Dec 03 01:19:24 	at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.createIOExceptionWithAttachedFailures(TempDirectory.java:350)
Dec 03 01:19:24 	at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:251)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.jupiter.engine.execution.ExtensionValuesStore.lambda$closeAllStoredCloseableValues$3(ExtensionValuesStore.java:68)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Dec 03 01:19:24 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)
Dec 03 01:19:24 	at java.base/java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
Dec 03 01:19:24 	at java.base/java.util.stream.Sink$ChainedReference.end(Sink.java:261)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Dec 03 01:19:24 	at org.junit.jupiter.engine.execution.ExtensionValuesStore.closeAllStoredCloseableValues(ExtensionValuesStore.java:68)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.AbstractExtensionContext.close(AbstractExtensionContext.java:80)
Dec 03 01:19:24 	at org.junit.jupiter.engine.execution.JupiterEngineExecutionContext.close(JupiterEngineExecutionContext.java:53)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.JupiterTestDescriptor.cleanUp(JupiterTestDescriptor.java:222)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$cleanUp$1(TestMethodTestDescriptor.java:155)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.cleanUp(TestMethodTestDescriptor.java:155)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.cleanUp(TestMethodTestDescriptor.java:68)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$cleanUp$10(NodeTestTask.java:167)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.cleanUp(NodeTestTask.java:167)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:98)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
Dec 03 01:19:24 	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
Dec 03 01:19:24 	at scala.collection.convert.Wrappers$IteratorWrapper.forEachRemaining(Wrappers.scala:31)
Dec 03 01:19:24 	at java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1939)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276)
Dec 03 01:19:24 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1708)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
Dec 03 01:19:24 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
Dec 03 01:19:24 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Dec 03 01:19:24 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
Dec 03 01:19:24 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Dec 03 01:19:24 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Dec 03 01:19:24 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Dec 03 01:19:24 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:387)
Dec 03 01:19:24 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1312)
Dec 03 01:19:24 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1843)
Dec 03 01:19:24 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1808)
Dec 03 01:19:24 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:188)
Dec 03 01:19:24 	Suppressed: java.nio.file.DirectoryNotEmptyException: /tmp/junit5892046034617387805
Dec 03 01:19:24 		at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:289)
Dec 03 01:19:24 		at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:104)
Dec 03 01:19:24 		at java.base/java.nio.file.Files.delete(Files.java:1152)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.deleteAndContinue(TempDirectory.java:293)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.postVisitDirectory(TempDirectory.java:288)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.postVisitDirectory(TempDirectory.java:264)
Dec 03 01:19:24 		at java.base/java.nio.file.Files.walkFileTree(Files.java:2803)
Dec 03 01:19:24 		at java.base/java.nio.file.Files.walkFileTree(Files.java:2857)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:264)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:249)
Dec 03 01:19:24 		... 93 more
Dec 03 01:19:24 	Suppressed: java.nio.file.DirectoryNotEmptyException: /tmp/junit5892046034617387805/78d09796046c6a48aad962a2efde7d8b
Dec 03 01:19:24 		at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:289)
Dec 03 01:19:24 		at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:104)
Dec 03 01:19:24 		at java.base/java.nio.file.Files.delete(Files.java:1152)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.deleteAndContinue(TempDirectory.java:293)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.postVisitDirectory(TempDirectory.java:288)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath$1.postVisitDirectory(TempDirectory.java:264)
Dec 03 01:19:24 		at java.base/java.nio.file.Files.walkFileTree(Files.java:2803)
Dec 03 01:19:24 		at java.base/java.nio.file.Files.walkFileTree(Files.java:2857)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.deleteAllFilesAndDirectories(TempDirectory.java:264)
Dec 03 01:19:24 		at org.junit.jupiter.engine.extension.TempDirectory$CloseablePath.close(TempDirectory.java:249)
Dec 03 01:19:24 		... 93 more{code}
The same workflow run but separate stage had the same issue:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55146&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9;;;","07/Dec/23 14:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55160&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=11361;;;","07/Dec/23 14:33;mapohl;JoinITCase.testInnerJoinWithDuplicateKey & WindowAggregateITCase:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55189&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4];;;","07/Dec/23 14:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55253&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11732;;;","11/Dec/23 14:51;mapohl;{{{}WindowDistinctAggregateITCase{}}}: [https://github.com/XComp/flink/actions/runs/7167579892/job/19514311693#step:12:11835] (GHA workflow for [FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink's+current+Azure+CI+infrastructure]/FLINK-27075 work);;;","12/Dec/23 11:55;mapohl;LimitITCase: https://github.com/XComp/flink/actions/runs/7172788948/job/19531081853#step:12:11583 (GHA workflow for FLIP-396/FLINK-27075 work);;;","12/Dec/23 12:00;mapohl;* {{AggregateITCase}}: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55420&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11399
;;;","12/Dec/23 12:03;mapohl;[~lincoln] (because you were assigned in [today's 1.19 release call|https://cwiki.apache.org/confluence/display/FLINK/1.19+Release#id-1.19Release-12/12/2023] to look into existing blocker issues): this is popping up in numerous places. Do we have someone who can look into it? 

[~jiabao.sun] do you have capacity to look into it further?;;;","12/Dec/23 12:08;mapohl;RankITCase.testTopNWithoutRowNumber: https://github.com/XComp/flink/actions/runs/7176084685/job/19540635669#step:12:11545 (FLINK-27075 GHA workflow);;;","12/Dec/23 12:48;lincoln.86xy;[~mapohl] Of course, I'll track this issue. [~jiabao.sun] Any updates from your side？;;;","12/Dec/23 13:34;jiabao.sun;Hi [~mapohl], sorry for the late reply.
This problem is difficult to reproduce locally. 
I guess it is RocksDBBackend still writing data asynchronous when cleaning up the temporary directory.
Can we use Junit5's TestExcutionExceptionHandler to ignore this IOException until we find the root cause?;;;","12/Dec/23 14:27;jiabao.sun;Sorry, TestExcutionExceptionHandler cannot capture the Exception thrown by CloseableResource.
I will continue to try. [~lincoln.86xy], please help assist in the investigation.
Thanks.;;;","12/Dec/23 15:11;lincoln.86xy;[~jiabao.sun] Has this problem ever been reproduced by you in your local environment(and what's your local os, macos/linux/windows)?
I see the possible related issue which you mentioned above (https://github.com/junit-team/junit5/issues/2811#issuecomment-1703476137) pointed to windows os only, if this is the problem, I think we first need a windows environment to try to reproduce it and find out exactly what triggers it, then we can look for a solution to it, WDYT?;;;","12/Dec/23 15:21;jiabao.sun;Thanks [~lincoln.86xy].
This problem has never been reproduced locally(macos).
It was surprising that the mentioned failure occurred in the CI environment running on Linux.

I tried to set a breakpoint at TempDirectory:293:fileOperations.delete(path), and encountered this error when wrote some new files before deleting the folder. Therefore, I suspect that there might be another thread writing to the TempDirectory while it is being cleared.

;;;","12/Dec/23 16:01;mapohl;It looks like it's more frequently appearing in the GitHub Actions infrastructure (FLINK-27075; e.g. the [most-recent run|https://github.com/XComp/flink/actions/runs/7178593131/job/19547405107]). Maybe, that helps to investigate it? ...by using the GitHub Actions workflow to debug the issue. ...just as another idea.;;;","12/Dec/23 16:03;jiabao.sun;I submitted a PR that manually creates a temporary folder and manually deletes it after each test to suppress the DirectoryNotEmptyException.
This is a temporary solution that we can avoid test failures until we find the root cause.

[~mapohl] [~lincoln.86xy], WDYT?;;;","12/Dec/23 16:57;mapohl;Thanks for the proposal. I looked into it a bit more and responded in the PR (y);;;","14/Dec/23 08:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55496&view=logs&j=26b84117-e436-5720-913e-3e280ce55cae&t=77cc7e77-39a0-5007-6d65-4137ac13a471&l=11707;;;","14/Dec/23 09:31;mapohl;master: [b2b8323ccd931be85dbaa6542552c26e6a29a105|https://github.com/apache/flink/commit/b2b8323ccd931be85dbaa6542552c26e6a29a105];;;","20/Dec/23 11:33;mapohl;We experience the issue still. The follow GitHub Actions workflows included the fix above:

[https://github.com/XComp/flink/actions/runs/7267597869/job/19802079134#step:12:11646]

[https://github.com/XComp/flink/actions/runs/7246692424/job/19739487910#step:12:11634]

 

The workflow is based on 1cf27ff26332e5b3bf2fba4f749dac1805af72de which has b2b8323ccd931be85dbaa6542552c26e6a29a105 included:
{code:java}
$ git log 1cf27ff26332e5b3bf2fba4f749dac1805af72de | grep b2b8323ccd931be85dbaa6542552c26e6a29a105
commit b2b8323ccd931be85dbaa6542552c26e6a29a105{code};;;","20/Dec/23 13:31;jiabao.sun;Hi [~mapohl], please take a look at this PR [https://github.com/apache/flink/pull/23917]

In the previous fix, I think we shouldn't reuse the tempFolder to create a new temporary folder.
Although we clean up the newly created directory during the ""after"" method, it is still possible for the checkpoint thread to continue writing, resulting in the failure of JUnit5 TemporaryFolder cleanup.
 
https://github.com/apache/flink/pull/23917#discussion_r1428188735

I think this problem was fixed by 23917, so could you help include the commit ac88acfbb1b4ebf7336e9a20e0b6d0b0fe32be51 and try it again?;;;","20/Dec/23 17:40;mapohl;Thanks for the point, [~jiabao.sun] . I will update my GHA test branch to include ac88acfbb1b4ebf7336e9a20e0b6d0b0fe32be51. (y)

I'm gonna close the issue again in the meantime.;;;",,,,,,,,,,
Support configuring FLINK_CONF_FILE through environment variables,FLINK-33640,13559318,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,wangm92,wangm92,24/Nov/23 10:17,24/Nov/23 10:17,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Client / Job Submission,,,,0,,,,"In the current client submission logic, _FLINK_CONF_DIR_ can be configured through environment variables, but _FLINK_CONF_FILE_ is hard-coded in {_}{color:#c1c7d0}flink-conf.yaml{color}{_}. 
However, there are some scenarios, such as: different job types (streaming or batch) have different configuration files. It is expected to support setting _FLINK_CONF_FILE_ to select the corresponding config file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-24 10:17:36.0,,,,,,,,,,"0|z1ltw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support zookeeper 3.9.0+,FLINK-33639,13559304,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,24/Nov/23 10:05,24/Nov/23 12:36,04/Jun/24 20:40,,,,,,,,,,,,,,BuildSystem / Shaded,Runtime / Coordination,,,0,,,,This is a follow up task for Flink once Curator's CURATOR-687 task to support ZooKeeper 3.9.0+ is done. The corresponding module needs to be added to \{{flink-shaded}} along the updated curator version.,,,,,,,,,,CURATOR-687,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-24 10:05:48.0,,,,,,,,,,"0|z1ltsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support variable-length data generation for variable-length data types,FLINK-33638,13559285,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,24/Nov/23 09:21,02/Dec/23 00:47,04/Jun/24 20:40,02/Dec/23 00:47,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Ecosystem,,,,0,pull-request-available,,,"Currently, for variable-length data types (varchar, varbinary, string, bytes), datagen connector always generates max-length data, we can extending datagen to generate variable length values(using a new option to enable it, e.g.,'fields.f0.var-len'='true').

the topic has been discussed in the mail thread [https://lists.apache.org/thread/kp6popo4cnhl6vx31sdn6mlscpzj9tgc]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 02 00:41:16 UTC 2023,,,,,,,,,,"0|z1ltoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 10:24;liyubin117;[~lincoln] Hi, could you please assign this to me? thanks!;;;","24/Nov/23 13:35;lincoln.86xy;[~liyubin117] assigned to you, thank you for contributing this!;;;","27/Nov/23 02:50;liyubin117;[~lincoln] thanks very much!;;;","29/Nov/23 01:58;liyubin117;[~lincoln] I have finished the feature according to https://issues.apache.org/jira/browse/FLINK-25284

now CI has passed, PTAL, thanks :D ;;;","02/Dec/23 00:41;lincoln.86xy;fixed in master: ac707cfeb24585ec4d68ee3dee22b64e42c454b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Introduce ArchTest rules to ban Junit 4 for table-planner,FLINK-33637,13559281,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,jiabao.sun,jiabao.sun,24/Nov/23 08:55,12/Dec/23 16:33,04/Jun/24 20:40,29/Nov/23 06:12,1.18.0,,,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,,,"Currently, table-planner have already completed the migration to JUnit 5, and we need to prevent the introduction of JUnit 4 tests in new PRs.
",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33641,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 06:12:34 UTC 2023,,,,,,,,,,"0|z1ltns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 06:12;leonard;Implemented in master: b9f3b957cc136a65c55b0b93d5e0bab2b412f8f3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement JdbcAutoScalerEventHandler,FLINK-33636,13559261,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,24/Nov/23 02:58,20/Feb/24 16:01,04/Jun/24 20:40,20/Feb/24 16:01,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,0,pull-request-available,,,Design doc: https://docs.google.com/document/d/1wXs6lLvX5jw6KNsz1pP9rdNLGoeInWkd5lbZo-rQp4s/edit?usp=sharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 16:01:01 UTC 2024,,,,,,,,,,"0|z1ltjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/24 16:01;fanrui;Merged to main(1.8.0) via : 7cad846d8704a469229728b494f79d4e1d5c4513 to 37ca517f33c65c0a60eb54b90db85408124b45d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some connectors can not compile in 1.19-SNAPSHOT,FLINK-33635,13559260,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,24/Nov/23 02:44,25/Dec/23 02:07,04/Jun/24 20:40,25/Dec/23 02:07,1.19.0,,,,,,,,1.19.0,,,,Connectors / Common,,,,0,,,,"The sink API compatibility was broken in FLINK-25857. 

org.apache.flink.api.connector.sink2.Sink#createWriter(InitContext) was changed to org.apache.flink.api.connector.sink2.Sink#createWriter(WriterInitContext).

All external connectors sink can not compile as this change.

For example:

es: https://github.com/apache/flink-connector-elasticsearch/actions/runs/6976181890/job/18984287421

aws: https://github.com/apache/flink-connector-aws/actions/runs/6975253086/job/18982104160",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25857,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 25 02:07:35 UTC 2023,,,,,,,,,,"0|z1ltj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 02:51;jiabao.sun;It seems caused by the refactor of Sink.InitContext. 
https://github.com/apache/flink/pull/23555

I meet the same problem in mongodb connector. 

;;;","24/Nov/23 02:55;tanyuxin;The failed reason is that the APIs are changed by [FLINK-25857|https://issues.apache.org/jira/browse/FLINK-25857]. This change may affect all connector's implementation. I think we should fix the issue with high priority.;;;","24/Nov/23 02:57;jiabao.sun;mongodb: https://github.com/apache/flink-connector-mongodb/actions/runs/6917152935/job/18818013536;;;","27/Nov/23 09:47;leonard;A blocker must be fixed in 1.19, my attitude here: https://issues.apache.org/jira/browse/FLINK-25857?focusedCommentId=17789995&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17789995;;;","27/Nov/23 15:31;martijnvisser;One note from my end: I think it's not unreasonable that the {{main}} branch of an externalized connector could be failing because something has changed on the Flink side. We will have exactly the same situation if something is changed as part of Flink 2.0, and it will require work for a connector maintainer at that point. That being said, that's not the case for this specific situation (since the discussion should have been brought back first to the Dev mailing list)

In a normal situation, this wouldn't have been a bug, but just a Jira ticket to make Connector X compatible with Flink 1.x.y;;;","12/Dec/23 13:44;mbalassi;I just merged [https://github.com/apache/flink/pull/23876,] that should resolve this issue once the new snapshot is published. [~Weijie Guo] please confirm.;;;","25/Dec/23 01:39;lincoln.86xy;[~Weijie Guo] Could you have a check on this？As Marton mentioned above, this compilation failures should be fixed by #23876，and I've checked recent nightly builds of some connectors(e.g., es, aws, mongodb), no failures now.;;;","25/Dec/23 02:07;Weijie Guo;Yep, I have checked the nightly CI of ElasticSearch connector, It's working now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Conditions to Flink CRD's Status field,FLINK-33634,13559222,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tagarr,tagarr,23/Nov/23 14:51,21/Feb/24 04:46,04/Jun/24 20:40,,kubernetes-operator-1.7.0,,,,,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"From [https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties] it is considered best practice to provide Conditions in the Status of CRD's. Some tooling even expects there to be a Conditions field in the status of a CR. This issue to to propose adding a Conditions field to the CR status

e.g.
status:
    conditions:
     - lastTransitionTime: '2023-11-23T12:38:51Z'
       status: 'True'
       type: Ready",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 14:14:24 UTC 2024,,,,,,,,,,"0|z1ltao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 15:33;gyfora;Could you please elaborate a bit on what exactly do you suggest we put in the conditions field? ;;;","24/Nov/23 14:34;tagarr;So we would use the io.fabric8.kubernetes.api.model.Condition class. I think the first condition type to implement would be _type: Ready_ which we could use when FlinkApplication/FlinkSessionJob is fully running and FlinkSessionCluster is ready to accept jobs. Later we could add additional types e.g. _type: Warning_ to inform the user that for example their FlinkDeployment is using emphermal storage. Interesting article about conditions here https://maelvls.dev/kubernetes-conditions/;;;","08/Dec/23 14:12;gyfora;sounds good :) ;;;","10/Dec/23 22:47;ryanvanhuuksloot;Is the idea of this issue to migrate the current custom `Status` objects to a more uniform and standardized spec? It would lead to a duplication of logic where there is a Condition with `type:Ready` and having `JobStatus` in the `.status.jobStatus`.

*Note: I think `type: Ready` is too vague for Flink.*

I'm not an operator expert but is it common to duplicate with objects and Conditions in status? 
If yes, (y) you can ignore the rest of the post.
If no, any smart idea how to make it less annoying to consistently search through all of the Conditions in the operator code? (There is a lot of usages in the operator of `.getStatus().getJobStatus().getState()`)

Maybe it is easiest to start with something like `jobManagerDeploymentStatus`. It seems pretty innocuous and we could duplicate the implementation in the short term.

Potentially moving towards a spec for FlinkDeployment like:

```

status:
conditions:
 - type: JobManagerReady
status: ""True""
lastTransitionTime: '2023-11-23T12:38:51Z'
lastHeartbeatTime: '2023-11-23T12:39:51Z'
observedGeneration: 1
reason: Ready
message: ""JobManager is running and ready to receive REST API calls.""
 - type: ReconciliationSucceeded
status: ""True""
lastTransitionTime: '2023-11-23T12:38:51Z'
reason: ReconciliationSucceeded
message: ""Reconciliation succeeded.""
 - type: JobRunning
status: ""True""
lastTransitionTime: '2023-11-23T12:38:51Z'
lastHeartbeatTime: '2023-11-23T12:40:51Z'
observedGeneration: 12
reason: Running
message: ""Job is running.""
jobId: 0b0f0c0a-0b0f-0c0a-0b0f-0c0a0b0f0c0a
jobName: flink-kubernetes-operator-test

```

Could add information about the Savepoint/Checkpoint as well.

 

Apologies if you got spam emailed. It kept submitting when trying to do same paragraph new lines.;;;","11/Dec/23 05:50;gyfora;I don't think the idea was to migrate from status to condition. I think the status is completely fine as it is currently (of course there are always some improvements to be made later for new versions). Most of the status is not userfacing anyways.

I think we can just take some part of it as you mentions like the job/jobmanager state and expose that.

Things like save points / reconciliations succeeded I definitely would not add.;;;","15/Jan/24 14:14;gyfora;Based on the in initial PR I suggest that we should create a FLIP with the design of the conditions for the Flink CRs.

[~ryanvanhuuksloot] has some good points, I also added some comments to the PR. Once we have a FLIP with the design we can vote on it then continue to implement it in the PR instead of arbitrarily putting something out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic creation of RBAC for instances of Flink Deployments,FLINK-33633,13559218,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Won't Do,,tagarr,tagarr,23/Nov/23 14:38,11/Dec/23 17:28,04/Jun/24 20:40,24/Nov/23 09:26,kubernetes-operator-1.7.0,,,,,,,,,,,,Kubernetes Operator,,,,0,,,,"Currently users have to manually create RBAC e.g. the flink service account. When operator is watching all namespaces; creation of a FlinkDeployment in a specific namespace may fail if the kube admin has failed to create the required RBAC. To improve usability the operator could be coded to automatically create these rbac resources in the instance namespace if not present",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 17:28:45 UTC 2023,,,,,,,,,,"0|z1lt9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 15:31;gyfora;I am not completely sure about this feature. It's generally an anti pattern for operators to do such actions. Operators should run with minimal permissions and in most prod envs admins do not want the Flink operator to have access to creating services accounts, roles and role bindings;;;","24/Nov/23 08:48;tagarr;Yes I understand about the need to restrict the operators api access. What if this was a helm install option that is off by default. The added operator code would only be enabled with an env var configured during the helm install ?;;;","24/Nov/23 08:54;gyfora;The problem is that this adds extra code logic to both the operator and the helm chart. And it brings basically 0 production value and encourages a privilege creep that poses a potential security risk.

When admins add a new namespace to the watched namespaces they need to add the add the necessary rbac. If they miss it, the operator will send error logs but won't fail so they can correct it.;;;","24/Nov/23 09:11;tagarr;Completely understand. Thanks for your review;;;","24/Nov/23 09:14;tagarr;What's the process of closing issues. I can put in a resolution of Feedback Received or Won't Do. Do I also need to specify a version ?;;;","24/Nov/23 09:16;gyfora;You can simply close it with won't do, no need to do anything else :) ;;;","11/Dec/23 17:21;ryanvanhuuksloot;Would there be any appetite to have the operator create just a ClusterRole for Flink?

We can then leave the RoleBinding to the specific deployment to not scope creep. It at least removes one extra resource per deployment. I think this would be safe.;;;","11/Dec/23 17:28;gyfora;In my personal opinion, creating the role without the binding doesn't really simplify anything it may just complicate the process because you need to know what role the operator is going to create exactly. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add custom mutator plugin,FLINK-33632,13559217,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,,tagarr,tagarr,23/Nov/23 14:31,15/Jan/24 11:05,04/Jun/24 20:40,15/Jan/24 11:05,kubernetes-operator-1.7.0,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,Currently users have the ability to provide custom validators to the operator. It would be great if we followed the same pattern to provide custom mutators,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 11:05:03 UTC 2024,,,,,,,,,,"0|z1lt9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 11:05;gyfora;merged to main 70b2d9ef812fc951ce6f24eddb21e5798d62a3c9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove HA ConfigMap label setup,FLINK-33631,13559206,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,23/Nov/23 12:45,23/Nov/23 12:45,04/Jun/24 20:40,,,,,,,,,,2.0.0,,,,Deployment / Kubernetes,,,,0,,,,"FLINK-33598 removed the ConfigMap labels for identifying HA ConfigMaps. Instead, the single ConfigMap is addressed by its name to reduce k8s API requests. The labeling of ConfigMaps was kept for the sake of backwards-compatibility. It can be removed as part of the 2.0 efforts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-23 12:45:30.0,,,,,,,,,,"0|z1lt74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoordinationResponse should be wrapped by SerializedValue in TaskOperatorEventGateway and JobMasterOperatorEventGateway,FLINK-33630,13559201,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,renqs,renqs,23/Nov/23 12:28,23/Nov/23 14:24,04/Jun/24 20:40,,1.17.1,1.18.0,,,,,,,,,,,Runtime / Coordination,,,,0,,,,"FLINK-26077 introduced a two-way RPC between operator and coordinator, but {{CoordinationResponse}} is not wrapped by {{{}SerializedValue{}}}:

 
[https://github.com/apache/flink/blob/c61c09e464073fae430cab2dd56bd608f9d275fd/flink-rpc/flink-rpc-akka/src/main/java/org/apache/flink/runtime/rpc/pekko/a.java#L254-L255|https://github.com/apache/flink/blob/34620fc5c5698d00e64c6b15f8ce84f807a2e0d7/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMasterOperatorEventGateway.java#L54]
 
This might be a problem if the implementation of {{CoordinationResponse}} is provided in user code and loaded by user code classloader, because Pekko RPC handler always uses app classloader for serializing and deserializing RPC parameters and return values, which will lead to {{ClassNotFoundException}} in this case. Similar to what we do for the request, we need to wrap a {{SerializesValue}} around the response to make sure RPC calls won't cause {{{}ClassNotFoundException{}}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 23 14:24:47 UTC 2023,,,,,,,,,,"0|z1lt60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 14:24;ruanhang1993;Hi, all. 

I would like to help to resolve this problem. Please assign this issue to me. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce ArchTest rules to ban Junit 4 for modules already completed the migration,FLINK-33629,13559193,13417682,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,23/Nov/23 11:22,24/Nov/23 15:37,04/Jun/24 20:40,24/Nov/23 15:37,1.18.0,,,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,,,"Currently, many modules have already completed the migration to JUnit 5, and we need to prevent the introduction of JUnit 4 tests in new PRs. 

However, completely removing the dependency on JUnit 4 is challenging. It requires defining JUnit 4 as optional in the flink-parent module and allowing sub-modules to selectively include it. However, this approach still cannot solve the issue of transitive dependencies. 

Therefore, considering introducing new rules in ArchTest to prohibit the usage of JUnit 4 in modules that have already completed the migration.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 15:37:07 UTC 2023,,,,,,,,,,"0|z1lt48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 15:37;leonard;master(1.19): 34436e70d58f61f2746e707647ad0135788e2b95;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading Flink's default ZooKeeper version to 3.8.3,FLINK-33628,13559188,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mapohl,mapohl,mapohl,23/Nov/23 10:30,27/Nov/23 09:47,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Flink currently supports ZooKeeper 3.7.1 as its default version which was added in Flink 1.17 (FLINK-29420). The ZooKeeper project released 3.8 and 3.9 in the meantime. 3.7 will reach its EOL with a new minor release. ZooKeeper 3.8 is already provided by \{{flink-shaded}} and, therefore, allows most likely an easy switch.

This issue is about chaning to ZooKeeper 3.8.3. A final discussion should be initiated in the mailing list before merging the change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-23 10:30:58.0,,,,,,,,,,"0|z1lt34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump snappy-java to 1.1.10.5 in flink-statefun,FLINK-33627,13559172,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rskraba,rskraba,23/Nov/23 09:02,23/Nov/23 09:29,04/Jun/24 20:40,,statefun-3.3.0,,,,,,,,,,,,Stateful Functions,,,,0,pull-request-available,,,"Xerial published a security alert for a Denial of Service attack that [exists on 1.1.10.1|https://github.com/xerial/snappy-java/security/advisories/GHSA-55g7-9cwv-5qfv].

See FLINK-33149 for flink core and connectors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-23 09:02:00.0,,,,,,,,,,"0|z1lszk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong style in flink ui,FLINK-33626,13559166,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Sergey Nuyanzin,zjureel,zjureel,23/Nov/23 08:06,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,,1.20.0,,,,Documentation,Project Website,,,0,pull-request-available,,,"https://nightlies.apache.org/flink/flink-docs-master/

 !image-2023-11-23-16-06-44-000.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/23 08:06;zjureel;image-2023-11-23-16-06-44-000.png;https://issues.apache.org/jira/secure/attachment/13064636/image-2023-11-23-16-06-44-000.png","23/Nov/23 08:23;Wencong Liu;image-2023-11-23-16-23-57-678.png;https://issues.apache.org/jira/secure/attachment/13064640/image-2023-11-23-16-23-57-678.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 22:50:38 UTC 2023,,,,,,,,,,"0|z1lsy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 08:26;Wencong Liu;This is a similar issue with FLINK-33356 The navigation bar on Flink’s official website is messed up. - ASF JIRA (apache.org)

[~Sergey Nuyanzin]  could you revert the modification to the file {*}book{*}?
!image-2023-11-23-16-23-57-678.png!;;;","23/Nov/23 10:00;Sergey Nuyanzin;oops, sorry, for that
yep, sure, going to do that;;;","23/Nov/23 13:34;Sergey Nuyanzin;Merged to master as [414a8ebefcf871521d4be4134bae3ed4709aba28|https://github.com/apache/flink/commit/414a8ebefcf871521d4be4134bae3ed4709aba28];;;","24/Nov/23 09:14;guoyangze;Could we find a way to guard it?;;;","24/Nov/23 22:50;Sergey Nuyanzin;That's a good question
i played a bit with it to try to understand how to reproduce it
The only way I was able to find is switching to the branch based on commit from previous issue with same problem (FLINK-33356) and apply {{git submodule update --init --recursive}}.
I guess I made it once by mistake...

How to avoid this in future... besides careful looking into file changeset from contributor's and reviewers sides currently don't know...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-390: Support System out and err to be redirected to LOG or discarded,FLINK-33625,13559165,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Nov/23 08:06,06/Feb/24 07:18,04/Jun/24 20:40,02/Dec/23 10:51,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,Get more from https://cwiki.apache.org/confluence/x/4guZE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 02 10:51:39 UTC 2023,,,,,,,,,,"0|z1lsy0:",9223372036854775807,"System.out and System.err output the content to the taskmanager.out and taskmanager.err files. In a production environment, if flink users use them to print a lot of content, the limits of yarn or kubernetes may be exceeded, eventually causing the TaskManager to be killed. Flink supports redirect the System.out and System.err to the log file, and the log file can be rolled to avoid unlimited disk usage.",,,,,,,,,,,,,,,,,,,"02/Dec/23 10:51;fanrui;Merged master<1.19.0> via: 186ed0eb0449a7bf3c216067798dc47a0c5b36a6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Guava to 32.1.3-jre in flink-table,FLINK-33624,13559132,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,jingge,jingge,jingge,23/Nov/23 02:28,24/Nov/23 05:13,04/Jun/24 20:40,24/Nov/23 05:13,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 05:12:48 UTC 2023,,,,,,,,,,"0|z1lsqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 05:12;jingge;master: 34620fc5c5698d00e64c6b15f8ce84f807a2e0d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metaspce leak caused by Flink Netty Client thread,FLINK-33623,13559129,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,gabry.wu,gabry.wu,23/Nov/23 01:48,22/Dec/23 20:11,04/Jun/24 20:40,,1.13.5,1.18.0,,,,,,,,,,,API / Core,,,,0,metaspace-leak,,,"Hi, folks, 

We found that there is a Flink Netty Client thread with contextClassLoader `ChildFirstClassLoader`, and it causes a metaspace leak.

TIPs

 classloader.check-leaked-classloader = false

 

!image-2023-11-23-09-47-50-536.png|width=1175,height=651!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/23 01:47;gabry.wu;image-2023-11-23-09-47-50-536.png;https://issues.apache.org/jira/secure/attachment/13064629/image-2023-11-23-09-47-50-536.png","30/Nov/23 02:30;gabry.wu;image.png;https://issues.apache.org/jira/secure/attachment/13064828/image.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 22 20:11:03 UTC 2023,,,,,,,,,,"0|z1lsq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 03:16;gabry.wu;I think Flink Netty Client and Fink Netty Server have nothing to do with user code, so it's better its thread factory uses app class loader.;;;","23/Nov/23 04:42;gabry.wu;Flink Netty Client and Fink Netty Server uses different classloader, Netty Server uses sun.misc.Launcher$AppClassLoader, however Flink Netty Client use org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders

 

When job stopped, Flink Netty Client thread doesn't stop, so there is always a classloader leak;;;","23/Nov/23 08:35;martijnvisser;[~gabry.wu] Please validate with the latest version of Flink, since 1.13 isn't supported in the community anymore;;;","23/Nov/23 13:41;gabry.wu;[~martijnvisser] 1.18.0 has the same `issue`;;;","27/Nov/23 12:24;gabry.wu;I change value of Flink Netty Client to null, everything goes well. [~martijnvisser] ;;;","27/Nov/23 12:44;martijnvisser;[~mapohl] WDYT?;;;","28/Nov/23 14:05;mapohl;Thanks for raising this issue, [~gabry.wu]. Can you provide a minimal Flink example that makes this reproducible? It's hard to identify where the classloader is coming from (I couldn't find any code segment where the classloading is explicitly set). I'm wondering whether it's related to FLINK-25023. In FLINK-25023 we experience a classloader leak which is caused by the fact that certain user code might trigger the thread creation (in the case of FLINK-33623: the thread that's used by the netty client) where the thread derives the classloader from the user code (see the [FLINK-25023 PR comment|https://github.com/apache/flink/pull/17916#discussion_r761288714] from [~dmvk] for a more detailed description of what's going on) 

What do you mean by the following text snippet in the issue description:
{quote}TIPs

classloader.check-leaked-classloader = false
{quote};;;","30/Nov/23 02:31;gabry.wu;[~mapohl] it's not related to FLINK-25023. 

you can start a minimal flink cluster on your desktop, and submit an example `$FLINK_HOME/examples/batch/WordCount.jar`, and this leak will be there.

Flink Netty Client is using `org.apache.flink.runtime.io.network.netty.NettyServer.THREAD_FACTORY_BUILDER` to create a thread when flink job starts, this factory will create a thread with parent context class loader as its `contextClassLoader`

Here is a clue.

!image.png!;;;","30/Nov/23 02:32;gabry.wu;` classloader.check-leaked-classloader = false` causes ChildFirstClassLoader leaks. and if it's true, SafetyNetWrapperClassLoader leaks;;;","30/Nov/23 06:48;mapohl;Thanks for the pointers. I'm gonna have a look.;;;","22/Dec/23 08:04;gabry.wu;[~mapohl] any updates?;;;","22/Dec/23 20:11;mapohl;Unfortunately, I haven't had the time, yet, to look into it. But it's on my list of items to check after the holiday week.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table.optimizer.reuse-sub-plan-enabled doesn't work when a Table is converted to a DataStream ,FLINK-33622,13559128,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sap1ens,sap1ens,23/Nov/23 01:42,08/Dec/23 18:43,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / Planner,,,,0,,,,"I have a source (a DataStream converted to a Table), a SQL transformation (really anything, could be a join or a simple ""SELECT * FROM""), and *two* Table API sinks (added via a 
StatementSet).
Here's the execution plan for this case:
{code:java}
Calc(select=[id, address, amount])(reuse_id=[1])
+- DropUpdateBefore
   +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[id, event_signature, address, amount, contract_address])Sink(table=[default_catalog.default_database.clickhouse_sink_for_factory_with_topologyId_clcy8vodu000108l6002fcsdm_], fields=[id, address, amount])
+- Reused(reference_id=[1])Sink(table=[default_catalog.default_database.blackhole_sink_for_factory_with_topologyId_clcy8vodu000108l6002fcsdm_], fields=[id, address, amount])
+- Reused(reference_id=[1]) {code}
As you can see, a transformation is reused by both sinks. 

In another case, before writing a transformation to one of the sinks, I convert the Table to a DataStream and then back to a Table (I actually apply some filtering on the DataStream, but the problem persists even after removing it, so it's irrelevant).

In this case, sinks don't reuse the results of the transformation; here's an execution plan:
{code:java}
Sink(table=[default_catalog.default_database.clickhouse_sink_for_factory_with_topologyId_clcy8vodu000108l6002fcsdm_], fields=[id, address, amount])
+- TableSourceScan(table=[[*anonymous_datastream_source$3*]], fields=[id, address, amount])Sink(table=[default_catalog.default_database.blackhole_sink_for_factory_with_topologyId_clcy8vodu000108l6002fcsdm_], fields=[id, address, amount])
+- Calc(select=[id, address, amount])
   +- DropUpdateBefore
      +- TableSourceScan(table=[[*anonymous_datastream_source$1*]], fields=[id, event_signature, address, amount, contract_address]) {code}
So, the data is processed twice. It could be a big problem for a heavy stateful operation. 

This feels like a bug in the optimizer. The same situation can be achieved by turning off *table.optimizer.reuse-sub-plan-enabled* option.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/23 01:41;sap1ens;Screenshot 2023-11-22 at 11.09.46 AM.png;https://issues.apache.org/jira/secure/attachment/13064628/Screenshot+2023-11-22+at+11.09.46%E2%80%AFAM.png","23/Nov/23 01:41;sap1ens;Screenshot 2023-11-22 at 11.10.29 AM.png;https://issues.apache.org/jira/secure/attachment/13064627/Screenshot+2023-11-22+at+11.10.29%E2%80%AFAM.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 18:43:01 UTC 2023,,,,,,,,,,"0|z1lsps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 08:48;jeyhunkarimov;Hi [~sap1ens] thanks for reporting the issue. 

First, indeed conversion of {{Table}} to {{DataStream}} and back to {{Table}} leads to some medadata loss. As a result, the optimizer cannot reuse the common plan(s).
Second, why don't you apply the filtering (that you did with {{DataStream}}) with the {{Table}} API?
;;;","08/Dec/23 18:43;sap1ens;> why don't you apply the filtering (that you did with {{{}DataStream{}}}) with the {{Table}} API?

I need to apply some transformations and filtering based on RowKind. E.g. handle DELETEs a certain way, drop UPDATEs, etc. AFAIK, Table API doesn't expose RowKind information about the current Row. 

But, in general, I may need to resort to lower-level API for state and timers.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store with PyFlink,FLINK-33621,13559117,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mfatihaktas,mfatihaktas,22/Nov/23 23:32,27/Nov/23 15:53,04/Jun/24 20:40,,,,,,,,,,,,,,Documentation,Table Store,,,0,documentation,,,"We are working on a project that requires setting up [Table Store|https://nightlies.apache.org/flink/flink-table-store-docs-stable/] with PyFlink. However, we could not find any documentation on (1) if this is possible, (2) if possible, how to set it up. We could find only the links through [https://nightlies.apache.org/flink/flink-table-store-docs-stable/,] which does not seem to address PyFlink.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 15:53:01 UTC 2023,,,,,,,,,,"0|z1lsnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 02:17;lzljs3620320;Hi [~mfatihaktas]

First, I think you can take a look to Paimon ([https://paimon.apache.org/),] Table Store has became Paimon, a separate project now. 

And you can use Table API in Python, and convert Table to DataStream, similar Java code is in: https://paimon.apache.org/docs/master/api/flink-api/;;;","27/Nov/23 15:53;mfatihaktas;Hi [~lzljs3620320],

Thank you for your comment. The pointers you shared are helpful.

I realized that I should re-phrase my question as follows: Is there a way to set up `Table Store/Paimon` with the `MiniCluster` managed by PyFlink? We need this to run Flink jobs locally with Paimon.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Java 21 to docker image,FLINK-33620,13559099,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Nov/23 19:59,24/Nov/23 12:05,04/Jun/24 20:40,24/Nov/23 12:05,,,,,,,,,,,,,,,,,0,,,,"currently there is only jdk 8, 11, 17 at https://github.com/zentol/flink-ci-docker",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 12:05:22 UTC 2023,,,,,,,,,,"0|z1lsjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 12:05;Sergey Nuyanzin;Closing since the PR is merged and new image with jdk21 is available at 
https://hub.docker.com/layers/chesnay/flink-ci/java_8_11_17_21_maven_386/images/sha256-1cf3950db45dda05cb504a31ca3d38f2cc6c9ed344cde67310e8c2566e758541?context=explore;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (Remaining Tests),FLINK-33619,13559076,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,jiabao.sun,jiabao.sun,22/Nov/23 17:13,24/Nov/23 08:42,04/Jun/24 20:40,24/Nov/23 08:42,1.18.0,,,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,,,JUnit5 Migration Module: flink-table-planner (Remaining Tests),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 08:42:18 UTC 2023,,,,,,,,,,"0|z1lse8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 08:42;leonard;Implemented by master(1.19): e7ce0a2969633168b9395c683921aa49362ad7a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade swagger for flink-shaded to 2.2.19,FLINK-33618,13559040,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Nov/23 13:38,27/Nov/23 14:35,04/Jun/24 20:40,27/Nov/23 14:34,,,,,,,,,shaded-18.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 14:34:30 UTC 2023,,,,,,,,,,"0|z1ls68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 14:34;Sergey Nuyanzin;Merged to master [e2dbeaf7673f7337e24f6283c6c247e952f89d84|https://github.com/apache/flink-shaded/commit/e2dbeaf7673f7337e24f6283c6c247e952f89d84];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade tcnative for flink-shaded,FLINK-33617,13559038,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,22/Nov/23 13:33,27/Nov/23 14:37,04/Jun/24 20:40,27/Nov/23 14:37,,,,,,,,,shaded-18.0,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 14:37:50 UTC 2023,,,,,,,,,,"0|z1ls5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Nov/23 14:37;Sergey Nuyanzin;Merged to master as [a192a0d1b4fa295c4786d1959c1bdb54796c4a41|https://github.com/apache/flink-shaded/commit/a192a0d1b4fa295c4786d1959c1bdb54796c4a41];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multi lookup join error,FLINK-33616,13559002,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,luca.yang,luca.yang,22/Nov/23 10:17,24/Nov/23 01:36,04/Jun/24 20:40,24/Nov/23 01:36,1.17.1,,,,,,,,,,,,Table SQL / API,,,,0,,,,"stream1 lookup join jdbc1 on ... lookup join jdbc2 on jdbc1.intfield1 = cast(jdbc2.stringfield2 as int)

show error: Temporal table join requires an equality condition on fields of table [default_catalog.default_database.t22].

 

test code:

 
{code:java}
//代码占位符
package com.yy.flinkSqlJoin

import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment
import org.apache.flink.table.api.Expressions.row
import org.apache.flink.table.api.{DataTypes, Table}
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment
import org.apache.flink.types.Row

import java.time.ZoneId;

/**
+I  插入
-U  更新前
+U  更新后
-D  撤回消息  会往kafka发一条null 对应mysql删除一条消息.
 * https://www.yuque.com/u430335/qea2i2/kw4qqu
 * 因为inner/left join不会发出回撤流 都是append 所以sink只需要支持append语义即可.
 * 要求事实表维度表关联键key1 必须在维度表的DDL中指定为主键 primary key (key1)
 * 测试使用:
 *    kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic user_order
 *    kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic user_payment
 *    kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic out
 * kafka数据:
 *    订单:
 *      {""order_id"":100,""ts"":1665367200000}  -- step2
 *      {""order_id"":101,""ts"":1665367200000}  -- step6
 *    支付(mysql):
 *      use db_yy;
        create table user_pay (
                        order_id bigint
                      ,paymoney bigint
                      ,primary key (order_id)
                      )ENGINE=InnoDB DEFAULT CHARSET=utf8;

        insert into user_pay values(100,111); -- step1
        update user_pay set paymoney=222 where order_id=100; -- step3
        insert into user_pay values(101,33);  -- step4
        update user_pay set paymoney=44 where order_id=101; -- step5

 *    代码回撤流输出(只有insert):
 *        8> (true,+I[100, 2022-10-10T02:00:00Z, 111]) -- step2 之后.  注意: lookup join是事实表为准,匹配维度表最新的数据. 没有也输出,维度表如果更新了,不会发回撤流更新结果
 *        (true,+I[101, 2022-10-10T02:00:00Z, 44]) -- step6 之后.
 *    kafka topic输出:
 *        {""order_id"":100,""d_timestamp"":""2022-10-10 02:00:00Z"",""paymoney"":111}
 *        {""order_id"":101,""d_timestamp"":""2022-10-10 02:00:00Z"",""paymoney"":44}
 *
 *    逻辑:
 *        lookup join 也分为 inner join; left join; full join.
 *        lookup join是取事实表匹配维度表时的最新的数据. 要求维度表的join字段是外部connector的主键(kafka不行).
 *
 */
object LookUpJoinJDBCDemo {


  def main(args: Array[String]): Unit = {
//    Class.forName(""com.mysql.cj.jdbc.Driver"")
    // flink1.13 流处理环境初始化
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    val tEnv = StreamTableEnvironment.create(env)

    // 指定国内时区
    tEnv.getConfig.setLocalTimeZone(ZoneId.of(""Asia/Shanghai""))

    // 订单表
    /*
    kafka参数:
     d_timestamp 从kafka元数据或者原始数据中获取
        d_timestamp TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'
     参数:json.fail-on-missing-field  https://nightlies.apache.org/flink/flink-docs-master/zh/docs/connectors/table/formats/json/#%e5%a6%82%e4%bd%95%e5%88%9b%e5%bb%ba%e4%b8%80%e5%bc%a0%e5%9f%ba%e4%ba%8e-json-format-%e7%9a%84%e8%a1%a8
        当解析字段缺失时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）
     参数: json.ignore-parse-errors
        当解析异常时，是跳过当前字段或行，还是抛出错误失败（默认为 false，即抛出错误失败）。如果忽略字段的解析异常，则会将该字段值设置为null
     注意: 下面 with中的配置是kafka输入表的配置
     */
//    val UserOrderTableSql =
//      """"""
//        |create table user_order (
//        | order_id bigint,
//        | ts bigint,
//        | d_timestamp as TO_TIMESTAMP_LTZ(ts,3),
//        | proctime AS PROCTIME() -- 事实表需要处理时间,维度表不需要
//        |)WITH(
//        |    'connector' = 'kafka',
//        |      'topic' = 'user_order',
//        |      'properties.bootstrap.servers' = 'localhost:9092',
//        |      'properties.group.id' = 'g1',
//        |      'scan.startup.mode' = 'latest-offset',
//        |      'format' = 'json',
//        |      'json.fail-on-missing-field' = 'false', -- 解析字段缺失 是跳过还是报错.
//        |      'json.ignore-parse-errors' = 'true' -- 跳过解析异常的数据
//        |)
//        |"""""".stripMargin
//    tEnv.executeSql(UserOrderTableSql)

    // scala int 到 java Integer的隐式转换

    /*
     case class C1(age:Int,name:String,time:Long)
     flink stream 事件时间
     */


    val table = tEnv.fromValues(
      DataTypes.ROW(
        DataTypes.FIELD(""order_id"", DataTypes.STRING())
        , DataTypes.FIELD(""ts"", DataTypes.INT())
        , DataTypes.FIELD(""d_timestamp"", DataTypes.TIMESTAMP_LTZ(3))
      ),
      row(""100"", Integer.valueOf(1), java.lang.Long.valueOf(1691722303347L))
      , row(""100"", Integer.valueOf(2), java.lang.Long.valueOf(1691732303347L))
      , row(""100"", Integer.valueOf(3), java.lang.Long.valueOf(1691742303347L))
      , row(""100"", Integer.valueOf(3), java.lang.Long.valueOf(1691752303347L))
    )
    tEnv.createTemporaryView(""user_order_pre1"", table)
    tEnv.executeSql(
      """"""
        |create view user_order as select *,proctime() as proctime from user_order_pre1
        |"""""".stripMargin)

    tEnv.from(""user_order"").execute().print()



    // 支付表 时态表 维度表 必须有主键定义. kafka connector不支持主键. 维度表是有界表. join取最新版本. 所以这里两种kafka connector都有问题. 这里用mysql测试.
    val paymentFlow =
      """"""
        |create table user_pay (
        | order_id string,
        | paymoney bigint,
        | PRIMARY KEY(order_id) NOT ENFORCED
        |)WITH(
        |    'connector' = 'jdbc',
        |    'url' = 'jdbc:mysql://localhost:3306/db_yy?useSSL=false&useUnicode=true&characterEncoding=UTF-8&serverTimezone=Asia/Shanghai',
        |    'table-name' = 'user_pay',
        |    'username' = 'root',
        |    'password' = '123123123'
        |)
        |"""""".stripMargin
    tEnv.executeSql(paymentFlow)

    tEnv.executeSql(
      """"""
        |create table t22
        |(
        |    id  string,
        |    age int,
        |    bi bigint
        |)with(
        |   'connector' = 'jdbc',
        |    'url' = 'jdbc:mysql://localhost:3306/db_yy?useSSL=false&useUnicode=true&characterEncoding=UTF-8&serverTimezone=Asia/Shanghai',
        |    'table-name' = 't',
        |    'username' = 'root',
        |    'password' = '123123123'
        |)
        |"""""".stripMargin)

    tEnv.executeSql(
      """"""
        |create view t33 as select *,cast(age as string) as age1 from t22
        |"""""".stripMargin)

    // 结果表
    /*
    注意: 下面 with中的配置是kafka输出表的配置
     */
    val resTableSQL =
      """"""
        |create table user_res (
        | order_id bigint,
        | d_timestamp TIMESTAMP_LTZ(3),
        | paymoney bigint
        |)WITH(
        |    'connector' = 'kafka',
        |      'topic' = 'out',
        |      'properties.bootstrap.servers' = 'localhost:9092',
        |      'format' = 'json',
        |      'sink.partitioner' = 'default' -- 默认分区器
        |)
        |"""""".stripMargin
    tEnv.executeSql(resTableSQL)

    // 关联表并输出 注意: r是维度表关联处理时间后的表别名 inner join 事实表流来了去维度表匹配,匹配到才发往下游,匹配不到则丢掉
    val tb1: Table = tEnv.sqlQuery(
      """"""
        |select
        |   l.order_id,
        |   l.d_timestamp,
        |   r.paymoney,
        |   r2.age
        |from user_order as l
        |join
        |   user_pay FOR SYSTEM_TIME AS OF l.proctime AS r
        |on l.order_id = r.order_id
        |left join
        |   t22 FOR SYSTEM_TIME AS OF l.proctime AS r2
        |-- on r.order_id = r2.id
        |on r.order_id = cast(r2.age as string) -- error: Temporal table join requires an equality condition on fields of table [default_catalog.default_database.t22].
        |"""""".stripMargin)
    // 特别注意: 这里维表join on的条件(r1 r2)不能有cast来类型转换 不能on的两侧类型不同,否则报错: Temporal table join requires an equality condition on fields of table
    // 特别注意: 这里维表join on的条件(l r2)不能有cast来类型转换 不能on的两侧类型不同,否则报错: implicit type conversion between VARCHAR(2147483647) and INTEGER is not supported on join's condition now

    tEnv.toDataStream(tb1).print()

    // lookup join 之 left join;  事实表流 来数据去外部维度表匹配 无论是否匹配到 都会发往下游.
//    val tb1: Table = tEnv.sqlQuery(
//      """"""
//        |select
//        |   l.order_id,
//        |   l.d_timestamp,
//        |   r.paymoney
//        |from user_order as l left join user_pay FOR SYSTEM_TIME AS OF l.proctime AS r
//        |on l.order_id = r.order_id
//        |"""""".stripMargin)


    /*
    报错: Unknown join type LEFT
    lookup join 不支持right join. 因为事实表是驱动表,和right join的逻辑不符合.
     */
//    val tb1: Table = tEnv.sqlQuery(
//      """"""
//        |select
//        |   l.order_id,
//        |   l.d_timestamp,
//        |   r.paymoney
//        |from user_order as l right join user_pay FOR SYSTEM_TIME AS OF l.proctime AS r
//        |on l.order_id = r.order_id
//        |"""""".stripMargin)

        /*
         报错: Unknown join type FULL
        look up join 不支持 full outer join. 因为左侧是驱动表 所以只支持 inner join(flatmap + _.filter.collect) 和 left join(flatmap + collect)
         */
//        val tb1: Table = tEnv.sqlQuery(
//          """"""
//            |select
//            |   l.order_id,
//            |   l.d_timestamp,
//            |   r.paymoney
//            |from user_order as l full join user_pay FOR SYSTEM_TIME AS OF l.proctime AS r
//            |on l.order_id = r.order_id
//            |"""""".stripMargin)

//    tEnv.toRetractStream(tb1, classOf[Row]).print()

    // 注意 这里传入Table类型的变量名即可
//    tEnv.executeSql(""insert into user_res select * from "" + tb1)


    env.execute(""job1"")
  }

}
 {code}
 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 22 13:29:09 UTC 2023,,,,,,,,,,"0|z1lrxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/23 10:31;luca.yang;lookup join ,dim table field cannot use cast function after ""on"".;;;","22/Nov/23 13:29;xuyangzhong;Hi, [~luca.yang] . In the lookup join, if you have only single equi-condition, the column in lookup join cannot be computed, also cannot be CAST here. You can try this test to reproduce this requirement.
{code:java}
tEnv.executeSql(s""""""
                   | CREATE TEMPORARY TABLE t1 (
                   |   a STRING,
                   |   b BIGINT,
                   |   c BIGINT,
                   |   proctime as PROCTIME()
                   | ) with (
                   |   'connector' = 'xxx'
                   | )
                   |"""""".stripMargin)

tEnv.executeSql(s""""""
                   | CREATE TEMPORARY TABLE t2 (
                   |   a STRING,
                   |   b BIGINT,
                   |   c BIGINT
                   | ) with (
                   |   'connector' = 'xxx'
                   | )
                   |"""""".stripMargin)

tEnv.executeSql(
  ""select t1.* from t1 join t2 for system_time as of t1.proctime on t1.a = cast(t2.b as string)"") {code}
Take the test above as an example. In lookup table ""t2"", the column ""b"" need to be treated as a known constant (in a row) for querying the lookup table. If the ""b"" is wrapped by a udx or cast like ""sourceT.a = f(lookupT.b)"", the b cannot be inferred by a, because we cannot infer the inverse function ""lookupT.b = f-1(sourceT.a)"".

In the test, the following SQL can work fine.
{code:java}
select t1.* from t1 join t2 for system_time as of t1.proctime on cast(t1.a as bigint) = t2.b {code}
Can you try to rewrite your SQL like this one?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"build_wheels_on_macos failed with ""Multiple top-level packages discovered in a flat-layout""",FLINK-33615,13558994,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,22/Nov/23 08:55,29/Nov/23 08:33,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Build System,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54783&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb&l=169]
{code:java}
       × Getting requirements to build wheel did not run successfully.
        │ exit code: 1
        ╰─> [14 lines of output]
            error: Multiple top-level packages discovered in a flat-layout: ['bson', 'gridfs', 'pymongo'].
      
            To avoid accidental inclusion of unwanted files or directories,
            setuptools will not proceed with this build.
      
            If you are trying to create a single distribution with multiple packages
            on purpose, you should not rely on automatic discovery.
            Instead, consider the following options:
      
            1. set up custom discovery (`find` directive with `include` or `exclude`)
            2. use a `src-layout`
            3. explicitly set `py_modules` or `packages` with a list of names
      
            To find more information, look for ""package discovery"" on setuptools docs.
            [end of output]
      
        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error
      
      × Getting requirements to build wheel did not run successfully.
      │ exit code: 1
      ╰─> See above for output.
      
      note: This error originates from a subprocess, and is likely not a problem with pip. {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-22 08:55:47.0,,,,,,,,,,"0|z1lrw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker pull failed,FLINK-33614,13558982,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,22/Nov/23 07:54,22/Nov/23 07:54,04/Jun/24 20:40,,,,,,,,,,,,,,Test Infrastructure,,,,0,github-actions,test-stability,,"https://github.com/XComp/flink/actions/runs/6949002998/job/18906544962#step:2:26
{code:java}
  Error response from daemon: Head ""https://registry-1.docker.io/v2/chesnay/flink-ci/manifests/java_8_11_17_maven_386_v2"": Get ""https://auth.docker.io/token?account=githubactions&scope=repository%3Achesnay%2Fflink-ci%3Apull&service=registry.docker.io"": EOF
24  Warning: Docker pull failed with exit code 1, back off 6.074 seconds before retry.
25  /usr/bin/docker pull chesnay/flink-ci:java_8_11_17_maven_386_v2
26  Error response from daemon: Get ""https://registry-1.docker.io/v2/chesnay/flink-ci/manifests/sha256:566fc00ee070850d47dc082e50bcdfbc7a07cea5b7bd3bc08c3943eba1cc9e4b"": EOF
27  Warning: Docker pull failed with exit code 1, back off 7.016 seconds before retry.
28  /usr/bin/docker pull chesnay/flink-ci:java_8_11_17_maven_386_v2
29  Error response from daemon: Get ""https://registry-1.docker.io/v2/chesnay/flink-ci/manifests/sha256:566fc00ee070850d47dc082e50bcdfbc7a07cea5b7bd3bc08c3943eba1cc9e4b"": EOF
30  Error: Docker pull failed with exit code 1 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-22 07:54:54.0,,,,,,,,,,"0|z1lrtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDF Runner process leak in Process Mode,FLINK-33613,13558978,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,Yu Chen,Yu Chen,22/Nov/23 07:33,24/Nov/23 10:23,04/Jun/24 20:40,24/Nov/23 10:23,1.17.0,,,,,,,,1.17.3,1.18.1,1.19.0,,API / Python,,,,0,pull-request-available,,,"While working with PyFlink, we found that in Process Mode, the Python UDF process may leak after a failover of the job. It leads to a rising number of processes with their threads in the host machine, which eventually results in failure to create new threads.

 

You can try to reproduce it with the attached test task `streamin_word_count.py`.

(Note that the job will continue failover, and you can watch the process leaks by `ps -ef` on Taskmanager.

 

Our test environment:
 * K8S Application Mode
 * 4 Taskmanagers with 12 slots/TM
 * Job's parallelism was set to 48 

The udf process `pyflink.fn_execution.beam.beam_boot` should be consistence with slots of TM (12), but we found that there are 180 processes on one Taskmanager after several failovers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/23 07:24;Yu Chen;ps-ef.txt;https://issues.apache.org/jira/secure/attachment/13064606/ps-ef.txt","22/Nov/23 07:24;Yu Chen;streaming_word_count-1.py;https://issues.apache.org/jira/secure/attachment/13064605/streaming_word_count-1.py",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 10:23:52 UTC 2023,,,,,,,,,,"0|z1lrsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 10:23;dianfu;Fixed in:
- master via 977463cce3ea0f88e2f184c30720bf4e8e97fd4a
- release-1.18 via f6d005681e3f5f83ab1074660c4d9878dabb9176
- release-1.17 via c1818f530617af8996f4a74bb064e203186fd98e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The table plan of hybrid shuffle may introduce additional blocking edges occasionally,FLINK-33612,13558956,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,22/Nov/23 03:14,17/Dec/23 06:34,04/Jun/24 20:40,17/Dec/23 06:34,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"To enhance the performance of hybrid shuffle, it is imperative to address the inconsistency between hybrid shuffle mode and blocking shuffle mode in certain query plans of TPC-DS (such as q88.sql, q14a.sql, q14b.sql, etc). 
In hybrid shuffle mode, these plans introduce additional blocking shuffle edges and result in increased shuffle times, potentially impacting overall efficiency. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 01:55:14 UTC 2023,,,,,,,,,,"0|z1lrnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 01:55;lsy;merged via master branch: 9cac80be18c6aff0cebdfe706327c1693822e884;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Large Protobuf Schemas,FLINK-33611,13558907,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dsaisharath,dsaisharath,dsaisharath,21/Nov/23 17:26,07/Feb/24 12:43,04/Jun/24 20:40,07/Feb/24 05:05,1.18.0,,,,,,,,1.20.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"h3. Background

Flink serializes and deserializes protobuf format data by calling the decode or encode method in GeneratedProtoToRow_XXX.java generated by codegen to parse byte[] data into Protobuf Java objects. FLINK-32650 has introduced the ability to split the generated code to improve the performance for large Protobuf schemas. However, this is still not sufficient to support some larger protobuf schemas as the generated code exceeds the java constant pool size [limit|https://en.wikipedia.org/wiki/Java_class_file#The_constant_pool] and we can see errors like ""Too many constants"" when trying to compile the generated code. 

*Solution*

Since we already have the split code functionality already introduced, the main proposal here is to now reuse the variable names across different split method scopes. This will greatly reduce the constant pool size. One more optimization is to only split the last code segment also only when the size exceeds split threshold limit. Currently, the last segment of the generated code is always being split which can lead to too many split methods and thus exceed the constant pool size limit",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34408,,FLINK-34403,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 07 05:05:58 UTC 2024,,,,,,,,,,"0|z1lrcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Dec/23 12:13;libenchao;I'm wondering how large will it be that can reach this limit?;;;","21/Dec/23 20:17;dsaisharath;[~libenchao] , The constant pool size [limit|https://docs.oracle.com/javase/specs/jvms/se12/html/jvms-4.html#jvms-4.4] is 65536 entries in Java. The constant pool size includes a lot of things but if we count only the identifier names and assume there are 2 identifiers(one for protobuf and one for rowdata) the generated code used for each field in the schema for rough estimation. There cannot be more than 65536/2 = 32768 fields in the Protobuf schema. Of course, the actual number is lower than that because we did not include split method names, class names etc.. ;;;","21/Dec/23 21:21;dsaisharath;Fwiw, I've run into this issue for one of our real production use case schema at my company which has about 44,368 fields in the Protobuf schema;;;","22/Dec/23 09:51;libenchao;44,368 is sure a quite large one, I haven't met such a case before. I agree we can improve it.;;;","22/Dec/23 22:13;dsaisharath;Yes, it is quite large indeed but it's one of the most important use cases in my company that we need to support with Flink. Please help us review the solution/PR and lmk if there is any concerns. Thanks!;;;","25/Dec/23 11:40;libenchao;[~dsaisharath] Sure, I'll review the PR. I may respond slow since I'm participating in the community in my spare time.;;;","02/Jan/24 13:17;libenchao;{quote}the main proposal here is to now reuse the variable names across different split method scopes. This will greatly reduce the constant pool size.
{quote}
[~dsaisharath] I'm wondering where the ""constants"" come from, IIUC, local variables in method should not affect class's constant pool?;;;","03/Jan/24 22:14;dsaisharath;[~libenchao] All identifier names in the code are part of the constant pool including local variable names. You can use the javap tool on a simple class file to examine the constant pool contents - [ref|[https://blogs.oracle.com/javamagazine/post/java-class-file-constant-pool].]

 

Here's an example class and it's constant pool content obtained with javap - 

 

 
{code:java}
public class Hello {

  public void sayHello1() {
    Integer a1 = 1;
    int b = 2;
    String c = ""hi"";
  }

  public void sayHello2() {
    Integer a2 = 3;
    int b = 2;
    String c = ""hello"";
  }
}{code}
{code:java}
Constant pool:
   #1 = Methodref          #6.#25         // java/lang/Object.""<init>"":()V
   #2 = Methodref          #26.#27        // java/lang/Integer.valueOf:(I)Ljava/lang/Integer;
   #3 = String             #28            // hi
   #4 = String             #29            // hello
   #5 = Class              #30            // com/uber/athena/athenax/connector/kafka/formats/protobuf/deserialize/Hello
   #6 = Class              #31            // java/lang/Object
   #7 = Utf8               <init>
   #8 = Utf8               ()V
   #9 = Utf8               Code
  #10 = Utf8               LineNumberTable
  #11 = Utf8               LocalVariableTable
  #12 = Utf8               this
  #13 = Utf8               Lcom/uber/athena/athenax/connector/kafka/formats/protobuf/deserialize/Hello;
  #14 = Utf8               sayHello1
  #15 = Utf8               a1
  #16 = Utf8               Ljava/lang/Integer;
  #17 = Utf8               b
  #18 = Utf8               I
  #19 = Utf8               c
  #20 = Utf8               Ljava/lang/String;
  #21 = Utf8               sayHello2
  #22 = Utf8               a2
  #23 = Utf8               SourceFile
  #24 = Utf8               Hello.java
  #25 = NameAndType        #7:#8          // ""<init>"":()V
  #26 = Class              #32            // java/lang/Integer
  #27 = NameAndType        #33:#34        // valueOf:(I)Ljava/lang/Integer;
  #28 = Utf8               hi
  #29 = Utf8               hello
  #30 = Utf8               com/uber/athena/athenax/connector/kafka/formats/protobuf/deserialize/Hello
  #31 = Utf8               java/lang/Object
  #32 = Utf8               java/lang/Integer
  #33 = Utf8               valueOf
  #34 = Utf8               (I)Ljava/lang/Integer; {code}
 

 

As we can see from the above example, local variable names are part of the constant pool

 ;;;","04/Jan/24 12:54;libenchao;[~dsaisharath] Thanks for the explanation, it helps a lot.;;;","05/Jan/24 03:03;dsaisharath;[~libenchao] , Thanks for the suggestion to add the test case to the pull request. While working on making an appropriate test case, I discovered something interesting. The local variable names are somehow not part of the Java constant pool for large schemas. I believe Java is triggering some optimizations internally and storing the variable names elsewhere when the code size becomes too large, therefore I'm not sure if reusing variable names has any impact on supporting large schemas. Perhaps, it can reduce the work needed for the Java compiler to rewrite variable names and result in faster compile times but I haven't conducted any experiment on that aspect. Apart from that, making the code change to reduce too many split methods has the most impact in supporting large schemas as I found that method names are always included in the constant pool even when the code size is too large from my experiment. In fact, this is the main reason which causes compilation errors with ""too many constants error""

With that being said, I would still prefer to keep the changes to reuse variable names since the change itself is non-intrusive, harmless, and can only improve the performance for compilation. Please let me know your thoughts;;;","08/Jan/24 02:08;libenchao;[~dsaisharath] Thanks for the analysis and the effort trying to improve Flink Protobuf Format.

bq. Apart from that, making the code change to reduce too many split methods has the most impact in supporting large schemas as I found that method names are always included in the constant pool even when the code size is too large from my experiment. In fact, this is the main reason which causes compilation errors with ""too many constants error""

I'm wondering if there is a real case that will run into this, if yes, I think it's still worth to improve it if there is a way.

bq. With that being said, I would still prefer to keep the changes to reuse variable names since the change itself is non-intrusive, harmless, and can only improve the performance for compilation. Please let me know your thoughts

I'm inclined to not include it for now, since there code does not solve a real problem yet, and might add a small burden to the maintenance since other contributors need to understand the code and it's intention. 

 If there is a significant improvement in the compilation, let's reconsider this, what do you think?
;;;","08/Jan/24 19:09;dsaisharath;[~libenchao] Thanks for the feedback, I agree with you on the points mentioned. Let me update the pull request as discussed above;;;","07/Feb/24 05:05;libenchao;Fixed via df03ada10e226053780cb2e5e9742add4536289c (master)

[~dsaisharath] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoGroupTaskTest.testCancelCoGroupTaskWhileCoGrouping failed with an AssertionError,FLINK-33610,13558887,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,mapohl,mapohl,21/Nov/23 15:02,23/Nov/23 13:06,04/Jun/24 20:40,23/Nov/23 13:06,1.19.0,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54741&view=logs&j=675bf62c-8558-587e-2555-dcad13acefb5&t=5878eed3-cc1e-5b12-1ed0-9e7139ce0992&l=7690]
{code:java}
 Nov 21 01:07:49 01:07:49.733 [ERROR] Tests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.204 s <<< FAILURE! - in org.apache.flink.runtime.operators.CoGroupTaskTest
Nov 21 01:07:49 01:07:49.733 [ERROR] org.apache.flink.runtime.operators.CoGroupTaskTest.testCancelCoGroupTaskWhileCoGrouping  Time elapsed: 1.01 s  <<< FAILURE!
Nov 21 01:07:49 java.lang.AssertionError: Test threw an exception even though it was properly canceled.
Nov 21 01:07:49 	at org.apache.flink.runtime.operators.CoGroupTaskTest.testCancelCoGroupTaskWhileCoGrouping(CoGroupTaskTest.java:448)
Nov 21 01:07:49 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 21 01:07:49 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
Nov 21 01:07:49 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 21 01:07:49 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
[...]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 23 13:06:08 UTC 2023,,,,,,,,,,"0|z1lr8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 15:32;mapohl;It's hard to grasp what the actual error is. The tests in this class should be improved: The exception that's caught should be stored in an {{AtomicReference}} instead of only storing the fact that an exception was caught. The {{printStacktrace}} is not provided anywhere in the logs.;;;","22/Nov/23 03:45;jiabao.sun;{code}
java.lang.NullPointerException: Cannot invoke ""org.apache.flink.runtime.operators.util.CoGroupTaskIterator.close()"" because ""this.coGroupIterator"" is null
at org.apache.flink.runtime.operators.CoGroupDriver.cleanup(CoGroupDriver.java:185)
at org.apache.flink.runtime.operators.testutils.DriverTestBase.testDriverInternal(DriverTestBase.java:270)
at org.apache.flink.runtime.operators.testutils.DriverTestBase.testDriver(DriverTestBase.java:195)
at org.apache.flink.runtime.operators.CoGroupTaskTest.access$200(CoGroupTaskTest.java:45)
at org.apache.flink.runtime.operators.CoGroupTaskTest$3.run(CoGroupTaskTest.java:427)
{code}

CoGroupDriver.cleanup() methods will run out of NPE in the case of concurrent calling:

* TaskCancelThread -> cleanup()
* taskRunner -> testDriver() -> testDriverInternal() -> finally cleanup(); 

{code:java}
@Override
public void cleanup() throws Exception {
    if (this.coGroupIterator != null) {
        this.coGroupIterator.close();
        this.coGroupIterator = null;
    }
} {code}

I'm willing to fix this problem.;;;","22/Nov/23 09:32;mapohl;{quote}
The printStacktrace is not provided anywhere in the logs.
{quote}
Ah, thanks for having another look. Indeed, the {{printStacktrace}} isn't swallowed.;;;","23/Nov/23 13:06;mapohl;master: [fa93d4fac9fd53f5fdf5a54fdfe7dfa7832f6262|https://github.com/apache/flink/commit/fa93d4fac9fd53f5fdf5a54fdfe7dfa7832f6262];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Take into account the resource limit specified in the pod template.,FLINK-33609,13558879,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,surendralilhore,surendralilhore,21/Nov/23 13:58,26/Nov/23 17:35,04/Jun/24 20:40,26/Nov/23 17:31,1.16.0,,,,,,,,,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,Flink is currently not considering the pod template resource limits and is only utilizing the limit obtained from the configured or default limit factor. Flink should consider both the value obtained from the limit factor and the pod template resource limits. It should take the maximum value of the pod template resource limits and the value obtained from the limit factor calculation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15656,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 26 17:31:40 UTC 2023,,,,,,,,,,"0|z1lr6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/23 17:31;surendralilhore;There is one more Jira to fix this issue in better way : FLINK-33548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpsertTestDynamicTableSink's keySerializationSchama should extract and serialize the primary key fields from RowData,FLINK-33608,13558858,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,21/Nov/23 11:59,25/Nov/23 14:25,04/Jun/24 20:40,25/Nov/23 14:25,1.18.0,,,,,,,,1.19.0,,,,Connectors / Common,,,,0,pull-request-available,,,"UpsertTestDynamicTableSink's keySerializationSchama should extract and serialize the primary key fields from RowData.

{code:sql}
CREATE TABLE UpsertFileSinkTable (
  user_id INT,
  user_name STRING,
  user_count BIGINT,
  PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
  'connector' = 'upsert-files',
  'key.format' = 'json',
  'value.format' = 'json',
  'output-filepath' = '..'
);


INSERT INTO UpsertFileSinkTable
SELECT user_id, user_name, COUNT(*) AS user_count
FROM (VALUES (1, 'Bob'), (22, 'Tom'), (42, 'Kim'), (42, 'Kim'), (42, 'Kim'), (1, 'Bob'))
  AS UserCountTable(user_id, user_name)
GROUP BY user_id, user_name;
{code}

Results:
|| Key || Value ||
| {""user_id"":1,""user_name"":""Bob"",""user_count"":2} | {""user_id"":1,""user_name"":""Bob"",""user_count"":2} |
| {""user_id"":22,""user_name"":""Tom"",""user_count"":1} | {""user_id"":22,""user_name"":""Tom"",""user_count"":1} |
| {""user_id"":42,""user_name"":""Kim"",""user_count"":3} | {""user_id"":42,""user_name"":""Kim"",""user_count"":3} |

Expected:
| {""user_id"":1} | {""user_id"":1,""user_name"":""Bob"",""user_count"":2} |
| {""user_id"":22} | {""user_id"":22,""user_name"":""Tom"",""user_count"":1} |
| {""user_id"":42} | {""user_id"":42,""user_name"":""Kim"",""user_count"":3} |
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27527,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 25 14:25:55 UTC 2023,,,,,,,,,,"0|z1lr20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/23 14:25;leonard;Fixed in master(1.19): e932b7bbfdc5cbb0f65579f4112c18c98b9efad4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add checksum verification for Maven wrapper as well,FLINK-33607,13558850,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,21/Nov/23 11:06,04/Jun/24 03:27,04/Jun/24 20:40,,1.18.0,1.19.0,,,,,,,,,,,Build System,,,,0,pull-request-available,,,"FLINK-33503 enabled us to add checksum checks for the Maven wrapper binaries along the update from 3.1.0 to 3.2.0.

But there seems to be an issue with verifying the wrapper's checksum under windows (see [related PR discussion in Guava|https://github.com/google/guava/pull/6807/files]).

This issue covers the fix as soon as MVRAPPER-103 is resolved. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33503,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 28 04:21:27 UTC 2024,,,,,,,,,,"0|z1lr08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/May/24 04:13;showuon;PR: [https://github.com/apache/flink/pull/24852]

After MVRAPPER-103 is resolved in 3.3.0, we should upgrade Maven wrapper to 3.3.0 or later to enable the feature to verify the wrapper's checksum.

This PR did:
 # Upgrade Maven wrapper to the latest 3.3.2 version to include more bug fixes.
 # Update the {{wrapperSha256Sum}} for {{maven-wrapper-3.3.2.jar}};;;","28/May/24 04:21;showuon;[~mapohl] , I've opened a PR to improve it. Please help assign the ticket to me. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support streaming mode in Flink JDBC driver,FLINK-33606,13558849,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,sangram,sangram,21/Nov/23 11:03,23/Nov/23 04:40,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Table SQL / JDBC,,,,0,,,,"With Flink's 1.18 release we now have JDBC driver for Flink. This is a very useful addition as there are plenty of tools which can now connect to Flink and run SQL.

However, one notable difference with other JDBC drivers vs. Flink is the streaming/dynamic tables where the data is continously updated. For example, we can run an aggregate query with GROUP BY and the groups are always being updated as the fresh data arrives.

This is extremely useful for real-time monitoring applications. However, the current JDBC driver implementation only supports batch mode.

This is Jira is to add support for streaming mode for Flink JDBC driver.

 

Implementation details:

FlinkResultSet doesn't have any additional methods to expose RowKind or the unique ID for that row. However this information is already available in FlinkResultSet class, it just needs to be exposed via methods.

 

Wiki page with relevant details:

https://cwiki.apache.org/confluence/display/FLINK/FLIP-293%3A+Introduce+Flink+Jdbc+Driver+For+Sql+Gateway

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 23 04:40:29 UTC 2023,,,,,,,,,,"0|z1lr00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/23 02:45;libenchao;Leaving out streaming/changelog out of Flink JDBC Driver was intentional, since AFAIK there is no such protocols in JDBC yet. 

As you said, ""This is a very useful addition as there are plenty of tools which can now connect to Flink and run SQL."", this all uses the standard JDBC protocol. Are u proposing to add a specific method to {{FlinkResultSet}} to get the {{RowKind}}? I'm wondering how will you gonna to use that method, especially for existing tools that constructed against standard JDBC protocol?

 ;;;","22/Nov/23 11:24;sangram;Hi,

So the tools will evolve and adapt. We are building a UI based on Flink SQL, and right now we can not do streaming for aggregated queries.

Moreover, we are not breaking any JDBC contract with changes, we are going to provide some additional features on top of plain JDBC. This is true for many of the databases where there's some special feature.

With FlinkResultSet/FlinkResultSetMetaData, we can optionally provide metadata for RowKind and Unique ID for the row (hash value of the object). These are already available in the implementation but not exposed.

 ;;;","22/Nov/23 12:30;martijnvisser;But without a standard, that would mean that Flink would have to implement specific implementations for each different system. I don't think that's an approach we want to take;;;","22/Nov/23 17:06;sangram;Hi

We will not be breaking any standard and Flink doesn't have to do any specific implementations for different systems.

Ask here is to expose Flink specific functionality along with JDBC common spec. This is not uncommon for other databases.

For example, see the postgres specific implementation which is not part of the JDBC spec.

Postgres specific metadata interface: 

[https://github.com/pgjdbc/pgjdbc/blob/ab1f2914a21edf9f8b7bfb35ad0d08789723b19f/pgjdbc/src/main/java/org/postgresql/PGResultSetMetaData.java]

 

And the implementation along with JDBC spec:

[https://github.com/pgjdbc/pgjdbc/blob/ab1f2914a21edf9f8b7bfb35ad0d08789723b19f/pgjdbc/src/main/java/org/postgresql/jdbc/PgResultSetMetaData.java|https://github.com/pgjdbc/pgjdbc/blob/ab1f2914a21edf9f8b7bfb35ad0d08789723b19f/pgjdbc/src/main/java/org/postgresql/jdbc/PgResultSetMetaData.java#L29]

 

For example, getBaseColumnName() is not part of JDBC spec ([https://docs.oracle.com/javase/8/docs/api/java/sql/ResultSetMetaData.html] ), but it still exposed as it is postgres specific feature.

 

We will expose Flink specific metadata, whether to use it or not is upto the clients.;;;","22/Nov/23 17:09;martijnvisser;I think it would be good to have a FLIP and a discussion on the Dev mailing list;;;","23/Nov/23 04:40;libenchao;+1 to have a FLIP since this would be in public api realm. I think this would be a good extension to the Flink JDBC Driver.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink1.17 fd leak,FLINK-33605,13558833,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,oxwangfeng521,oxwangfeng521,21/Nov/23 09:31,21/Nov/23 12:43,04/Jun/24 20:40,21/Nov/23 11:20,1.15.0,1.17.1,,,,,,,,,,,,,,,0,,,,"Environmental information:

Flink 1.17.1;

 

executed the command in the Flink background, lsof - n | grep deleted, and found that a historical process held a handle to a temporary data file. The historical data file did not released successfully, resulting in disk space not being released；

!https://dts-szv.clouddragon.huawei.com/v1/nfs/downLoadFile?filePath=20231110P0J0003UW30_image-CORS-1699580879639.png!

 

Flink SQL model：

error msg：Could not upload job files，As follows：

!https://dts-szv.clouddragon.huawei.com/v1/nfs/downLoadFile?filePath=20231110P0J000414VM_image-1699581171355.jpeg!

 

when disk space is full;， The operation is as follows

cd /proc/33425/fd/
ls -l |grep blobStorage |grep deleted | awk '\{print $9}' |grep xargs truncate -s 0

33425 is taskmanager pid；

and disk space is release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/23 11:02;oxwangfeng521;image-2023-11-21-19-02-32-929.png;https://issues.apache.org/jira/secure/attachment/13064585/image-2023-11-21-19-02-32-929.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 12:43:28 UTC 2023,,,,,,,,,,"0|z1lqwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 09:37;leonard;duplicated with https://issues.apache.org/jira/browse/FLINK-33604;;;","21/Nov/23 09:39;oxwangfeng521;the other is closeed;;;","21/Nov/23 10:17;martijnvisser;There isn't enough information in this ticket. Please improve the description and the title, else it will have to be closed as invalid;;;","21/Nov/23 11:21;martijnvisser;I've closed the ticket as Invalid, it can be re-opened when there's clear information that there is a bug in Flink. I would recommend to first reach out to the User mailing list or Slack to see if this is a problem with the configuration on your end first.;;;","21/Nov/23 11:52;oxwangfeng521;All information has been completed;;;","21/Nov/23 12:43;martijnvisser;[~oxwangfeng521] I'm sorry but there's only one screenshot which has a partial error in there ""Could not upload job files"". There's no reproducer for the behavior. The description doesn't provide me with the necessary information as well ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink1.17 fd leak,FLINK-33604,13558832,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,oxwangfeng521,oxwangfeng521,21/Nov/23 09:31,21/Nov/23 09:40,04/Jun/24 20:40,21/Nov/23 09:40,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-21 09:31:27.0,,,,,,,,,,"0|z1lqw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix guava shading for GCS connector,FLINK-33603,13558788,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,jjayadeep,jjayadeep,21/Nov/23 03:39,18/Dec/23 10:53,04/Jun/24 20:40,18/Dec/23 10:48,1.18.0,,,,,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,,,"GCS connector has guava shading issue, This change introduced dependency on guava version({{{}31.1-jre{}}}) required by {{{}google-cloud-storage{}}}. Upgrade of {{google-cloud-storage}} lead to runtime failure because of new functionalities added in {{{}31.1-jre{}}}.

This change pins guava version to the one required by storage client specifically in {{{}flink-gs-fs-hadoop{}}}, leaving all other filesystem implementation untouched.",,,,,,,,,,,,,,,,,,,,FLINK-33793,,,,,,,,,,,,,FLINK-33704,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 18 10:48:56 UTC 2023,,,,,,,,,,"0|z1lqmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/23 10:48;martijnvisser;FLINK-33704;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector should be compatible with Flink 1.18,FLINK-33602,13558780,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,tison,tison,tison,21/Nov/23 01:14,21/Nov/23 01:14,04/Jun/24 20:40,,1.18.0,pulsar-4.1.0,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,"Currently, the build and test job always fails - https://github.com/apache/flink-connector-pulsar/actions/runs/6937440214",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-21 01:14:12.0,,,,,,,,,,"0|z1lqkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Expand node,FLINK-33601,13558765,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,20/Nov/23 22:58,21/Nov/23 17:28,04/Jun/24 20:40,21/Nov/23 17:28,,,,,,,,,1.19.0,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 17:28:47 UTC 2023,,,,,,,,,,"0|z1lqhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 17:28;dwysakowicz;Implemented in 01864277016544641be81663d6274dd726f25843..e0d87b3a9a587815f1c986a3de5bc60842dc617e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Print cost time for batch queries in SQL Client,FLINK-33600,13558723,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jark,jark,20/Nov/23 15:23,02/Dec/23 10:10,04/Jun/24 20:40,02/Dec/23 10:07,,,,,,,,,1.19.0,,,,Table SQL / Client,,,,1,pull-request-available,,,"Currently, there is no cost time information when executing batch queries in SQL CLI. But this is very helpful in OLAP/ad-hoc scenarios. 

For example: 
{code}
Flink SQL> select * from (values ('abc', 123));
+--------+--------+
| EXPR$0 | EXPR$1 |
+--------+--------+
|    abc |    123 |
+--------+--------+
1 row in set  (0.22 seconds)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33726,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 02 10:07:36 UTC 2023,,,,,,,,,,"0|z1lq88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 01:52;lsy;+1, it seems very userful.;;;","21/Nov/23 05:03;libenchao;+1;;;","02/Dec/23 10:07;jingge;master: c20c13fb5cb78eff2cbd08ea48f1cb7cf9a1981c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run restore tests with RocksDB state backend,FLINK-33599,13558719,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,20/Nov/23 15:00,06/Dec/23 15:39,04/Jun/24 20:40,06/Dec/23 15:39,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 15:39:41 UTC 2023,,,,,,,,,,"0|z1lq7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Dec/23 15:39;dwysakowicz;Implemented in 43fec308b3298ed2aad639b94140c9a2173c10cd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Watch HA configmap via name instead of lables to reduce pressure on APIserver ,FLINK-33598,13558699,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,yunta,yunta,yunta,20/Nov/23 13:05,24/Nov/23 06:54,04/Jun/24 20:40,24/Nov/23 06:54,1.17.1,1.18.0,,,,,,,1.17.3,1.18.1,1.19.0,,Deployment / Kubernetes,,,,0,pull-request-available,,,"As FLINK-24819 described, the k8s API server would receive more pressure when HA is enabled, due to the configmap watching being achieved via filter with labels instead of just querying the configmap name. This could be done after FLINK-24038, which reduced the number of configmaps to only one as {{<clusterId>-cluster-config-map}}.

This ticket would not touch {{<clusterId>-<jobId>-config-map}}, which stores the checkpoint information, as that configmap is directly accessed by JM and not watched by taskmanagers.",,,,,,,,,,,,,,,,,,,,FLINK-24819,,,,,,,,,,,FLINK-24038,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 06:54:15 UTC 2023,,,,,,,,,,"0|z1lq2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 06:54;yunta;Merged
master: 608546e090f5d41c6a8b9af2c264467279181027 ... b7e8b792c086c3c445ee8429fbcfe035097a878c

release-1.18: 6f30c6e427251dd4b2e4ad03f89bed06a519b05f
release-1.17: 18d5a4696eccac3b5e7fe1d579547feef4537c08;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not use a nested column for a join condition,FLINK-33597,13558678,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,dwysakowicz,dwysakowicz,dwysakowicz,20/Nov/23 10:43,21/May/24 12:40,04/Jun/24 20:40,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,,,,"Query:
{code}
SELECT A.after.CUSTOMER_ID FROM `CUSTOMERS` A INNER JOIN `PRODUCTS` B ON A.after.CUSTOMER_ID = B.after.PURCHASER;
{code}

fails with:

{code}
java.lang.RuntimeException: Error while applying rule FlinkProjectWatermarkAssignerTransposeRule, args [rel#411017:LogicalProject.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#411016,exprs=[$2, $2.CUSTOMER_ID]), rel#411015:LogicalWatermarkAssigner.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#411014,rowtime=$rowtime,watermark=SOURCE_WATERMARK())]
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:250)
...
Caused by: java.lang.IllegalArgumentException: Type mismatch:
rel rowtype: RecordType(RecordType:peek_no_expand(INTEGER NOT NULL CUSTOMER_ID, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" CUSTOMER_NAME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" TITLE, INTEGER DOB) after, INTEGER $f8) NOT NULL
equiv rowtype: RecordType(RecordType:peek_no_expand(INTEGER NOT NULL CUSTOMER_ID, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" CUSTOMER_NAME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" TITLE, INTEGER DOB) after, INTEGER NOT NULL $f8) NOT NULL
Difference:
$f8: INTEGER -> INTEGER NOT NULL

	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:592)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:613)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:144)
	... 50 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31830,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 29 15:53:50 UTC 2023,,,,,,,,,,"0|z1lpy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/23 10:44;dwysakowicz;I figured the problem is with nullability of a field of a nested row. When accessing a NOT NULL field of a NULLABLE ROW we add an additional CAST: e.g. CAST($0.customer_id AS INT) This behaviour is Flink specific (https://github.com/apache/flink/blob/0b4e32e86342a7b10d612157cefe1dc114801aa1/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/calcite/FlinkRexBuilder.java#L56)

However the CAST can be stripped by the RexSimplify#simplifyCast. This is done in case of a JOIN when pushing down the join condition. Unfortunately the result is when e.g. reordering rels, e.g pushing projection through watermark assigner, the types mismatch, because we’re recreating the nested field access with the correct non-simplified type.

My suggestion would be to disable simplifying CAST for nested fields accesses https://github.com/apache/calcite/blob/7049f9a33501c199fe04b589dd067adcae9f1ee7/core/src/main/java/org/apache/calcite/rex/RexSimplify.java#L2185

{code}
 private RexNode simplifyCast(RexCall e) {
        RexNode operand = e.getOperands().get(0);
        operand = simplify(operand, UNKNOWN);
        if (operand.isA(SqlKind.FIELD_ACCESS) && ((RexFieldAccess) operand)
                .getReferenceExpr()
                .getType()
                .isStruct()) {
            return rexBuilder.makeCast(e.getType(), operand, true);
        }
        if (sameTypeOrNarrowsNullability(e.getType(), operand.getType())) {
            return operand;
        }
        ....
{code};;;","20/Nov/23 11:06;twalthr;There was some recent discussion about nested fields and nullability. Maybe this is fixed in current master already? How would you disable the simplifying? Would we need a custom RexSimplify class?;;;","20/Nov/23 14:57;dwysakowicz;Yes, we would need a custom RexSimplify. Until recently we had that though.

> There was some recent discussion about nested fields and nullability. Maybe this is fixed in current master already?

Do you have any pointers to the discussions? I can't see any changes that would help us. I am aware of https://issues.apache.org/jira/browse/FLINK-31349 but this talks about ROW constructor and as far as I can tell does not touch the {{GET}} behaviour.;;;","28/Nov/23 04:27;lincoln.86xy;[~twalthr] IIUC, [FLINK-31830|https://issues.apache.org/jira/browse/FLINK-31830] might be what you're talking about.  [~qingyue] had a detailed investigation on this issue and is preparing a FLIP recently. ;;;","29/Nov/23 15:53;twalthr;Thanks for linking the issue [~lincoln.86xy]. Yes, this was the issue I was talking about. Seems we are trying to solve the same issue. I remember this was an issue multiple times in the past. If we can solve it with a minimal fix that would definitely be preferred over a full blown FLIP but I will read the linked issue in more detail and join the discussion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support fold expression before transfer to RexNode,FLINK-33596,13558673,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,20/Nov/23 09:59,16/Jan/24 13:47,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"Hive will fold expression in optimization stage. But flink-hive-parser use flink optimization.

And flink can't know some hive function can be constant value.

It can be reproduce by follow sql
{code:java}
select distinct(dep), UNIX_TIMESTAMP()  from employee {code}
Some hive code reference:

https://github.com/apache/hive/blob/rel/release-2.3.9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRexExecutorImpl.java#L62

https://github.com/apache/hive/blob/rel/release-2.3.9/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L1776

https://github.com/apache/hive/blob/rel/release-2.3.9/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L1069",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-20 09:59:44.0,,,,,,,,,,"0|z1lpx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamDependencyTests.test_set_requirements_with_cached_directory failed with ModuleNotFoundError,FLINK-33595,13558651,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,20/Nov/23 07:30,25/Mar/24 14:24,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,API / Python,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54682&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3&l=24663]
{code:java}
[...]
Nov 18 02:53:13 E                   ModuleNotFoundError: No module named 'python_package1'
Nov 18 02:53:13 E                   
Nov 18 02:53:13 E                   	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
Nov 18 02:53:13 E                   	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
Nov 18 02:53:13 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:61)
Nov 18 02:53:13 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
Nov 18 02:53:13 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
Nov 18 02:53:13 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:421)
Nov 18 02:53:13 E                   	... 7 more
Nov 18 02:53:13 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 295, in _execute
Nov 18 02:53:13 E                       response = task()
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 370, in <lambda>
Nov 18 02:53:13 E                       lambda: self.create_worker().do_instruction(request), request)
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 630, in do_instruction
Nov 18 02:53:13 E                       getattr(request, request_type), request.instruction_id)
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 667, in process_bundle
Nov 18 02:53:13 E                       bundle_processor.process_bundle(instruction_id))
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1062, in process_bundle
Nov 18 02:53:13 E                       element.data)
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 231, in process_encoded
Nov 18 02:53:13 E                       self.output(decoded_value)
Nov 18 02:53:13 E                     File ""apache_beam/runners/worker/operations.py"", line 526, in apache_beam.runners.worker.operations.Operation.output
Nov 18 02:53:13 E                     File ""apache_beam/runners/worker/operations.py"", line 528, in apache_beam.runners.worker.operations.Operation.output
Nov 18 02:53:13 E                     File ""apache_beam/runners/worker/operations.py"", line 237, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Nov 18 02:53:13 E                     File ""apache_beam/runners/worker/operations.py"", line 240, in apache_beam.runners.worker.operations.SingletonElementConsumerSet.receive
Nov 18 02:53:13 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 169, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Nov 18 02:53:13 E                       with self.scoped_process_state:
Nov 18 02:53:13 E                     File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 196, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
Nov 18 02:53:13 E                       self.process_element(input_processor.next())
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/pyflink/fn_execution/table/operations.py"", line 102, in process_element
Nov 18 02:53:13 E                       return self.func(value)
Nov 18 02:53:13 E                     File ""<string>"", line 1, in <lambda>
Nov 18 02:53:13 E                     File ""/__w/2/s/flink-python/pyflink/table/tests/test_dependency.py"", line 184, in add_one
Nov 18 02:53:13 E                       from python_package1 import plus
Nov 18 02:53:13 E                   ModuleNotFoundError: No module named 'python_package1'
Nov 18 02:53:13 E                   
Nov 18 02:53:13 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
Nov 18 02:53:13 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:332)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:315)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:834)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
Nov 18 02:53:13 E                   	at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
Nov 18 02:53:13 E                   	... 3 more {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26644,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 14:24:43 UTC 2024,,,,,,,,,,"0|z1lps8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/23 07:31;mapohl;I'm linking FLINK-26644 because it's the same test failure. The logs are different, though. I assume that both issue have different causes because of this.;;;","20/Nov/23 08:31;mapohl;Popped up in the GHA work (FLINK-27075) as well: [https://github.com/XComp/flink/actions/runs/6908469019/job/18798148972#step:12:24193]

FYI: So far, I only saw it on Java 17 builds;;;","25/Jan/24 10:57;mapohl;https://github.com/XComp/flink/actions/runs/7646851520/job/20836886000#step:10:24486;;;","25/Mar/24 14:24;rskraba;There might be a bit more happening here in the stack trace: [https://github.com/apache/flink/actions/runs/8414062328/job/23037457625#step:10:24129] , but it also end up with a ModuleNotFoundError in the middle of it:
{code:java}
Mar 25 02:39:42                 if answer[1] == REFERENCE_TYPE:
Mar 25 02:39:42                     raise Py4JJavaError(
Mar 25 02:39:42                         ""An error occurred while calling {0}{1}{2}.\n"".
Mar 25 02:39:42 >                       format(target_id, ""."", name), value)
Mar 25 02:39:42 E                   py4j.protocol.Py4JJavaError: An error occurred while calling o3678.await.
Mar 25 02:39:42 E                   : java.util.concurrent.ExecutionException: org.apache.flink.table.api.TableException: Failed to wait job finish
Mar 25 02:39:42 E                       at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
Mar 25 02:39:42 E                       at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
Mar 25 02:39:42 E                       at org.apache.flink.table.api.internal.TableResultImpl.awaitInternal(TableResultImpl.java:118)
Mar 25 02:39:42 E                       at org.apache.flink.table.api.internal.TableResultImpl.await(TableResultImpl.java:81)
Mar 25 02:39:42 E                       at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 25 02:39:42 E                       at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
Mar 25 02:39:42 E                       at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 25 02:39:42 E                       at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Mar 25 02:39:42 E                       at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
Mar 25 02:39:42 E                       at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
Mar 25 02:39:42 E                       at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
Mar 25 02:39:42 E                       at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
Mar 25 02:39:42 E                       at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
Mar 25 02:39:42 E                       at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
Mar 25 02:39:42 E                       at java.base/java.lang.Thread.run(Thread.java:833)
Mar 25 02:39:42 E                   Caused by: org.apache.flink.table.api.TableException: Failed to wait job finish
Mar 25 02:39:42 E                       at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:85)
Mar 25 02:39:42 E                       at org.apache.flink.table.api.internal.InsertResultProvider.isFirstRowReady(InsertResultProvider.java:71)
Mar 25 02:39:42 E                       at org.apache.flink.table.api.internal.TableResultImpl.lambda$awaitInternal$1(TableResultImpl.java:105)
 {code}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support BuiltInMethod TO_TIMESTAMP with timezone options,FLINK-33594,13558646,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xuzifu,xuzifu,20/Nov/23 06:26,20/Nov/23 12:20,04/Jun/24 20:40,20/Nov/23 12:19,1.8.4,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"Support BuiltInMethod TO_TIMESTAMP with timezone options，TO_TIMESTAMPS now only use utc timezone，but many scenarios we need timzone to choose，so need a pr to support it as TO_TIMESTAMP('2023-08-10', 'yyyy-MM-dd', 'Asia/Shanghai')

this scenario in presto，starrocks，trino：

as presto,trino,starrocks:
SELECT timestamp '2012-10-31 01:00 UTC' AT TIME ZONE 'America/Los_Angeles';
2012-10-30 18:00:00.000 America/Los_Angeles

so we maybe need this function in to_timestamps",,,,,,,,,,,,,,,,,,,,,FLINK-32871,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 12:20:32 UTC 2023,,,,,,,,,,"0|z1lpr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/23 08:30;martijnvisser;I'm inclined to close this ticket as invalid because

TO_TIMESTAMP is meant to convert char to a timestamp. If you want to convert to a TIMESTAMP_LTZ, you should just use CONVERT_TZ for that. 

> as presto,trino,starrocks:
SELECT timestamp '2012-10-31 01:00 UTC' AT TIME ZONE 'America/Los_Angeles';

This isn't comparable, since this is not using the function;;;","20/Nov/23 09:49;xuzifu;if user want to use many timezone in one sql with TO_TIMESTAMP，they can not choose timezone for it. In a global view，they would use udf for it. this is not friendly to platform developers. Add the timezone arguments can aviod it and not bring any risk. [@MartijnVisser|https://github.com/MartijnVisser] as I know many users occur the problem which confused them for a long time;;;","20/Nov/23 11:51;libenchao;This is duplicated with FLINK-32871, exactly same reporter/summary/description.;;;","20/Nov/23 12:20;martijnvisser;[~libenchao] Thanks, I've closed the ticket/PR and redirected back to FLINK-32871. [~xuzifu] we should first have consensus before PRs are opened;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Left title bar of the official documentation for the master branch is misaligned,FLINK-33593,13558634,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,lsy,lsy,20/Nov/23 03:34,21/Nov/23 01:56,04/Jun/24 20:40,21/Nov/23 01:56,1.19.0,,,,,,,,,,,,Documentation,,,,0,,,,"The left title bar of the official documentation for the Master branch is misaligned, but the 1.18 branch is normal, so I'm guessing there's something wrong with this and we should fix it.

!image-2023-11-20-11-34-07-084.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/23 03:34;lsy;image-2023-11-20-11-34-07-084.png;https://issues.apache.org/jira/secure/attachment/13064560/image-2023-11-20-11-34-07-084.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 10:31:10 UTC 2023,,,,,,,,,,"0|z1lpog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/23 10:31;tanyuxin;[~lsy] Hi, Dalong, the issue has been resolved at https://issues.apache.org/jira/browse/FLINK-33356. I have personally checked the web page on my local machine and everything appears to be functioning normally. Have you attempted to refresh your web browser's cache?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The return type of the function is void,not convenient to use",FLINK-33592,13558494,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,,zhangtao9876,zhangtao9876,18/Nov/23 06:44,22/Nov/23 01:16,04/Jun/24 20:40,22/Nov/23 01:15,,,,,,,,,,,,,API / DataStream,,,,0,pull-request-available,,,"{code:java}
@PublicEvolving
public void setRestartStrategy(
        RestartStrategies.RestartStrategyConfiguration restartStrategyConfiguration) {
    config.setRestartStrategy(restartStrategyConfiguration);
} {code}
StreamExecutionEnvironment usually has many parameters that need to be set.The return type is void, making it inconvenient to use.

Others set methods 'return this; ' , only this method has a void return type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Nov 22 01:14:08 UTC 2023,,,,,,,,,,"0|z1lp48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Nov/23 16:29;JunRuiLi;Hi [~zhangtao9876] ,

The community has already accepted FLIP-381: Deprecate configuration getters/setters that return/set complex Java objects ([here|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=278464992]). Therefore, it is not recommended to modify this method, and it is also recommended to use ConfigOption instead of the setter API. Additionally, since the setRestartStrategy method is annotated by @PublicEnvolving, any modifications to it would require community discussion.;;;","22/Nov/23 01:14;zhangtao9876;I see.

Thank you very much for your reply;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup usage of deprecated TableTestBase#addFunction,FLINK-33591,13558489,13529804,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Nov/23 02:10,22/Nov/23 14:53,04/Jun/24 20:40,22/Nov/23 14:24,,,,,,,,,1.19.0,,,,Table SQL / API,,,,0,pull-request-available,,,I would say it is a subtask of cleaning up for {{TableEnvironment#registerFunction FLINK-31597}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 22 14:24:27 UTC 2023,,,,,,,,,,"0|z1lp34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/23 14:24;Sergey Nuyanzin;Merged to master [d92423069b0538f58fd6c72fae0952b873c3b352|https://github.com/apache/flink/commit/d92423069b0538f58fd6c72fae0952b873c3b352];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointStatsTracker.totalNumberOfSubTasks not updated,FLINK-33590,13558465,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,roman,roman,17/Nov/23 19:16,17/Nov/23 21:46,04/Jun/24 20:40,17/Nov/23 21:46,,,,,,,,,,,,,,,,,0,,,,"On rescaling, the DoP is obtained from the JobGraph. 
However, JobGraph vertices are not updated once created. This results in missing traces on rescaling (isComplete returns false).

Instead, it should be obtained from DoP store.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-17 19:16:51.0,,,,,,,,,,"0|z1loxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken documentation page for Kinesis connector,FLINK-33589,13558458,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,17/Nov/23 18:19,20/Nov/23 09:56,04/Jun/24 20:40,20/Nov/23 09:56,1.16.2,1.17.1,1.18.0,aws-connector-4.1.0,,,,,1.19.0,,,,Connectors / AWS,Documentation,,,0,pull-request-available,,,"Recent change in connector version declaration in docs (FLINK-33567) resulted in broken documentation page for Kinesis connector for Flink 1.16, 1.17, and 1.18

 

https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/kinesis/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 09:56:02 UTC 2023,,,,,,,,,,"0|z1low8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 18:53;a.pilipenko;cc [~martijnvisser];;;","17/Nov/23 22:01;dannycranmer;Merged commit [{{536e583}}|https://github.com/apache/flink/commit/536e58354c8b5862d1703918a3dfd11e5928e063] into apache:master 
Merged commit [{{077346f}}|https://github.com/apache/flink/commit/077346f4197b94727453f38d206ae49d8aa7afc3] into apache:release-1.18
Merged commit [{{9052407}}|https://github.com/apache/flink/commit/9052407919c1af3a72383325010f5f9380d793b2] into apache:release-1.17
Merged commit [{{36bab98}}|https://github.com/apache/flink/commit/36bab982dfd0bba59266921924e28c5d9a3e0063] into apache:release-1.16;;;","17/Nov/23 22:01;dannycranmer;Triggered docs build;;;","20/Nov/23 08:33;martijnvisser;This change has now broken the docs for the Cassandra connector https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/cassandra/ and https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/cassandra/ - Those versions don't exist;;;","20/Nov/23 09:03;a.pilipenko;[~martijnvisser]  Cassandra connector does not use connector artifact shortcode at all, I don't think this is related to this change

https://github.com/apache/flink-connector-cassandra/blob/main/docs/content/docs/connectors/datastream/cassandra.md?plain=1#L38;;;","20/Nov/23 09:09;martijnvisser;Deleted my comment; made a wrong assumption. Something is still broken after this PR but will investigate what's the issue;;;","20/Nov/23 09:13;a.pilipenko;[~martijnvisser] it's the same with your link as well
{code:java}
{{< artifact flink-connector-cassandra withScalaVersion >}} {code}
I see that correct shortcode is used in [v3.1|https://github.com/apache/flink-connector-cassandra/blob/v3.1/docs/content.zh/docs/connectors/datastream/cassandra.md], however docs point to 3.0: [https://github.com/apache/flink/blob/release-1.18/docs/setup_docs.sh#L48C1-L48C40]

 ;;;","20/Nov/23 09:42;martijnvisser;The Flink docs pointed to the wrong Cassandra branch; fixed that, so that issue should be resolved (and was indeed caused by something else). Apologies [~a.pilipenko] :);;;","20/Nov/23 09:56;dannycranmer;Looks like this is now resolved. Thanks [~martijnvisser]  for reporting and [~a.pilipenko] for fixing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Flink Checkpointing Statistics Bug,FLINK-33588,13558450,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhutong66,zhutong66,zhutong66,17/Nov/23 17:39,25/Jan/24 23:03,04/Jun/24 20:40,19/Dec/23 13:42,1.14.5,1.14.6,1.15.2,1.16.0,1.17.0,1.17.1,1.18.0,,1.18.1,1.19.0,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"When the Flink task is first started, the checkpoint data is null due to the lack of data, and Percentile throws a null pointer exception when calculating the percentage. After multiple tests, I found that it is necessary to set an initial value for the statistical data value of the checkpoint when the checkpoint data is null (i.e. at the beginning of the task) to solve this problem.

The following is an abnormal description of the bug：

2023-09-13 15:02:54,608 ERROR org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler [] - Unhandled exception.
org.apache.commons.math3.exception.NullArgumentException: input array
    at org.apache.commons.math3.util.MathArrays.verifyValues(MathArrays.java:1650) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.commons.math3.stat.descriptive.AbstractUnivariateStatistic.test(AbstractUnivariateStatistic.java:158) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:272) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.commons.math3.stat.descriptive.rank.Percentile.evaluate(Percentile.java:241) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics$CommonMetricsSnapshot.getPercentile(DescriptiveStatisticsHistogramStatistics.java:159) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.metrics.DescriptiveStatisticsHistogramStatistics.getQuantile(DescriptiveStatisticsHistogramStatistics.java:53) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.checkpoint.StatsSummarySnapshot.getQuantile(StatsSummarySnapshot.java:108) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.rest.messages.checkpoints.StatsSummaryDto.valueOf(StatsSummaryDto.java:81) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.createCheckpointingStatistics(CheckpointingStatisticsHandler.java:129) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:84) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.rest.handler.job.checkpoints.CheckpointingStatisticsHandler.handleRequest(CheckpointingStatisticsHandler.java:58) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.rest.handler.job.AbstractAccessExecutionGraphHandler.handleRequest(AbstractAccessExecutionGraphHandler.java:68) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$0(AbstractExecutionGraphHandler.java:87) ~[flink-dist_2.12-1.14.5.jar:1.14.5]
    at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602) [?:1.8.0_151]
    at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) [?:1.8.0_151]
    at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) [?:1.8.0_151]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_151]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_151]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_151]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_151]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_151]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_151]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33882,,,,,,,"12/Dec/23 18:24;zhutong66;FLINK-33588.patch;https://issues.apache.org/jira/secure/attachment/13065263/FLINK-33588.patch","11/Dec/23 09:35;zhutong66;image-2023-12-11-17-35-23-391.png;https://issues.apache.org/jira/secure/attachment/13065203/image-2023-12-11-17-35-23-391.png","13/Dec/23 03:35;zhutong66;image-2023-12-13-11-35-43-780.png;https://issues.apache.org/jira/secure/attachment/13065271/image-2023-12-13-11-35-43-780.png","15/Dec/23 05:59;zhutong66;image-2023-12-15-13-59-28-201.png;https://issues.apache.org/jira/secure/attachment/13065340/image-2023-12-15-13-59-28-201.png","19/Dec/23 06:02;zhutong66;image-2023-12-19-14-02-49-083.png;https://issues.apache.org/jira/secure/attachment/13065423/image-2023-12-19-14-02-49-083.png","19/Dec/23 06:03;zhutong66;image-2023-12-19-14-03-27-062.png;https://issues.apache.org/jira/secure/attachment/13065424/image-2023-12-19-14-03-27-062.png","12/Dec/23 18:24;zhutong66;newCommit-FLINK-33688.patch;https://issues.apache.org/jira/secure/attachment/13065264/newCommit-FLINK-33688.patch",,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,java,Tue Dec 19 13:41:51 UTC 2023,,,,,,,,,,"0|z1loug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Nov/23 04:32;zhutong66;This submission greatly contributes to the stability of Flink task operation. This change has been put into production in the company and the effect is obvious.;;;","11/Dec/23 08:57;jingge;[~zhutong66] do you want to fix the checkstyle issue in the PR and cp to 1.18? This fix could be released in 1.18.1 if it could be done at a fast pace. WDYT?;;;","11/Dec/23 09:25;zhutong66;[~jingge] I want to fix the checkstyle issue in the PR and cp to 1.18.1，The PR I have submitted so far is shown in the following figure，Do you need me to do anything now?

!image-2023-12-11-17-35-23-391.png|width=1336,height=237!;;;","12/Dec/23 08:11;jingge;The CI has checkstyle failure;;;","12/Dec/23 09:40;zhutong66;[~jingge] I will handle it immediately;;;","12/Dec/23 18:16;zhutong66;[~jingge] New PR is here #23915 https://github.com/apache/flink/pull/23915;;;","13/Dec/23 03:35;zhutong66;[~jingge] The New PR #23915 [https://github.com/apache/flink/pull/23915]

and CI report is Succes! 

!image-2023-12-13-11-35-43-780.png!;;;","14/Dec/23 17:25;zhutong66;[~jingge] The New PR #23931

[https://github.com/apache/flink/pull/23931 |https://github.com/apache/flink/pull/23931]

and CI report is no problem 

and I have already compressed the submission;;;","15/Dec/23 05:59;zhutong66;[~jingge] CI report is no problem is Success

!image-2023-12-15-13-59-28-201.png!;;;","16/Dec/23 16:34;zhutong66;[~jingge] The comments above may have connection issues and always redirect to old PRs. I have corrected them。

The New PR #23931 

[https://github.com/apache/flink/pull/23931 |https://github.com/apache/flink/pull/23931]

and CI report is no problem 

and I have already compressed the submission;;;","18/Dec/23 06:51;zhutong66;[~jingge] I have made the necessary modifications as per your request. Can this PR be released in version 1.18.1?

https://github.com/apache/flink/pull/23931;;;","18/Dec/23 10:32;jingge;master: 5e1deeea19c2e73f3bc6f1f9881a778d13abf6a4;;;","18/Dec/23 10:33;jingge;[~zhutong66] bp to 1.18: https://github.com/apache/flink/pull/23947;;;","18/Dec/23 15:15;jingge;[~zhutong66] could you please write a test for your change?;;;","19/Dec/23 06:04;zhutong66;[~jingge] I have tested this change on the local cluster using the latest Flink code and there are no issues. The screenshot of the test indicates the test results on （[https://github.com/apache/flink/pull/23931]）# 23931 (Verifying this change), which completely fixes this bug.Also, I have tested many test cases before.

!image-2023-12-19-14-03-27-062.png|width=934,height=655!;;;","19/Dec/23 07:48;jingge;[~zhutong66] Thanks for the feedback. Could you please add UT or IT?;;;","19/Dec/23 08:11;zhutong66;[~jingge] Yes I can use a Flink Job to test the result images as UT and IT;;;","19/Dec/23 11:19;jingge;[~zhutong66] Thanks for the feedback. Could you please elaborate on how you will create the test? The test should be part of the CI and used as regression tests.;;;","19/Dec/23 11:39;zhutong66;[~jingge]

My testing steps are as follows:
1. I will package the modified code, and the code modification will be done in the jar package of flink-dist-xxx.jar. Replace the jar package with the production Flink client.
2. Submit the Flink SQL task in the production environment to Yarn in application mode and check the Yarn logs
3. Check for any further errors in the Yarn log.
4. On the web interface of Flink web, check if the data displayed on the checkpoint information statistics page is normal.;;;","19/Dec/23 11:52;jingge;[~zhutong66] It is great that you tested the change manually like you just described. However, as a software, it would be better to write regression test for it to make sure it works with anyone upcoming changes. WDYT?;;;","19/Dec/23 13:37;jingge;1.18: c53493ecc45a02b3db595bdd28ac145482703322;;;","19/Dec/23 13:41;jingge;created a follow-up ticket for test: https://issues.apache.org/jira/browse/FLINK-33882;;;",,,,,,,,,,,,,,,,,,,
Tidy up docs around JDBC,FLINK-33587,13558419,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,rmoff,rmoff,rmoff,17/Nov/23 14:20,27/Nov/23 14:47,04/Jun/24 20:40,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,"The documentation around using JDBC with Flink is, IMHO, a bit disjointed and could be easier to follow. Specifically on
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/jdbcdriver/]
 ** Add note to this page regarding use of this vs Hive JDBC option through HiveServer2 endpoint
 * [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/hive-compatibility/hiveserver2/] should simply redirect to [https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql-gateway/hiveserver2/] since this is a feature of the SQL Gateway. Having two pages of the same title is confusing.

There are also various nits in the above pages, such as missing capitalisation, links to github repos that would be useful to add, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-17 14:20:19.0,,,,,,,,,,"0|z1lonk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add GHA support in release-1.17 branch,FLINK-33586,13558411,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,17/Nov/23 13:38,08/Dec/23 15:50,04/Jun/24 20:40,08/Dec/23 15:50,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 15:50:59 UTC 2023,,,,,,,,,,"0|z1lols:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 15:50;mapohl;In FLIP-396 we decided (vote hasn't been done, yet) to not roll out GHA for release-1.17 but wait for 1.19 to be released. This avoid to do backports for 1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Zookeeper to 3.7.1,FLINK-33585,13558379,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,martijnvisser,martijnvisser,martijnvisser,17/Nov/23 09:46,17/Nov/23 09:46,04/Jun/24 20:40,,,,,,,,,,,,,,Runtime / Configuration,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-17 09:46:23.0,,,,,,,,,,"0|z1lof4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Hadoop Filesystems to 3.3.6,FLINK-33584,13558377,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,snuyanzin,martijnvisser,martijnvisser,17/Nov/23 09:28,22/Apr/24 07:22,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / FileSystem,FileSystems,,,0,pull-request-available,,,"Update the Hadoop filesystems to 3.3.6. Some of the key changes:

{code:java}
* A big update of dependencies to try and keep those reports of transitive CVEs under control -both genuine and false positives.
* Critical fix to ABFS input stream prefetching for correct reading.
* Vectored IO API for all FSDataInputStream implementations, with high-performance versions for file:// and s3a:// filesystems. file:// through java native IO s3a:// parallel GET requests.
* Arm64 binaries. Note, because the arm64 release was on a different platform, the jar files may not match those of the x86 release -and therefore the maven artifacts.
* Security fixes in Hadoop’s own code.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-17 09:28:36.0,,,,,,,,,,"0|z1loeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support state TTL hint for regular join,FLINK-33583,13558372,13556057,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,17/Nov/23 09:07,11/Jan/24 02:08,04/Jun/24 20:40,11/Jan/24 02:08,1.19.0,,,,,,,,1.19.0,,,,Table SQL / API,Table SQL / Planner,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 02:08:12 UTC 2024,,,,,,,,,,"0|z1lodk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 02:08;qingyue;Fixed in master 21403e31f4761bdddf5e4e802e0e5eb9b4533202;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update flink-shaded version,FLINK-33582,13558365,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,17/Nov/23 08:05,21/Nov/23 02:13,04/Jun/24 20:40,21/Nov/23 02:12,1.17.3,,,,,,,,1.17.3,,,,BuildSystem / Shaded,,,,0,pull-request-available,,,"This is a follow-up task for https://issues.apache.org/jira/browse/FLINK-33417. 

After flink-shaded 16.2 is released, we should update the flink-shaded version for Flink 1.17 to resolve the issue thoroughly.

Note the update is only for 1.17.x, because 1.18.x and 1.19.x have been updated and the issue does not exist.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 02:13:18 UTC 2023,,,,,,,,,,"0|z1loc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 02:13;Weijie Guo;release-1.16 via cef22667ae65fb6fda78f22a05a8ca91540af687.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-381: Deprecate configuration getters/setters that return/set complex Java objects,FLINK-33581,13558359,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,17/Nov/23 07:40,21/Feb/24 06:21,04/Jun/24 20:40,28/Nov/23 06:18,,,,,,,,,1.19.0,,,,API / DataStream,,,,0,pull-request-available,,,"Deprecate the non-ConfigOption objects in the StreamExecutionEnvironment, CheckpointConfig, and ExecutionConfig, and ultimately removing them in FLINK 2.0",,,,,,,,,,,,,,,,,,,,,,,,FLINK-33669,,,,,FLINK-33980,,,,,,,,,"30/Nov/23 09:59;fanrui;image-2023-11-30-17-59-42-650.png;https://issues.apache.org/jira/secure/attachment/13064856/image-2023-11-30-17-59-42-650.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 02:27:46 UTC 2024,,,,,,,,,,"0|z1loao:",9223372036854775807,"The non-ConfigOption objects in the StreamExecutionEnvironment, CheckpointConfig, and ExecutionConfig and their corresponding getter/setter interfaces are now be deprecated in FLINK-1.19. And these objects and methods are planned to be removed in Flink-2.0. The deprecated interfaces include the getter and setter methods of RestartStrategy, CheckpointStorage, and StateBackend.

More details can be found at https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=278464992.",,,,,,,,,,,,,,,,,,,"28/Nov/23 06:18;zhuzh;Done via
0e0099b4eb1285929fec02326f661cba899eedcd
139db3f4bc7faed4478393a91a063ad54d15a523
dae2eb5b61f71b9453a73e4f0b3c69fd28f54ebf;;;","30/Nov/23 10:02;fanrui;Hey [~JunRuiLi] , after these getters/setters are removed, should we remove related doc as well? Such as restart-strategy page[1].

If yes, would you mind removing them? Or if you agree, I'll be happy to remove it.

 

!image-2023-11-30-17-59-42-650.png!

 

[1] [https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/task_failure_recovery/#fixed-delay-restart-strategy]

 ;;;","30/Nov/23 12:12;JunRuiLi;Hey [~fanrui] , thank you for the reminder. Indeed, I have already created FLINK-33669 to track the necessary updates to the documentation. I haven't submitted the corresponding PR yet, but it won't be long before I do.:D;;;","01/Dec/23 02:41;fanrui;Hey [~JunRuiLi] , thank you for the feedback! Great work :);;;","04/Dec/23 13:00;zhuzh;for pyflink:
17b82a7662c76dc9f41125782ba5232bb1a7eea4
4a7def9fe00387df66af8daf23603a6ea0848e03
17b82a7662c76dc9f41125782ba5232bb1a7eea4;;;","30/Jan/24 08:45;martijnvisser;[~JunRuiLi] Can you please include in the release notes information on what's deprecated, and what users should be using?;;;","31/Jan/24 02:27;JunRuiLi;Thanks [~martijnvisser] for the kind reminder. I've updated the release notes ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
composite actions do not work in release branchs where GHA work wasn't backported,FLINK-33580,13558354,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,17/Nov/23 07:17,17/Nov/23 07:17,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-17 07:17:50.0,,,,,,,,,,"0|z1lo9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Join sql error,FLINK-33579,13558345,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,17/Nov/23 05:38,10/Mar/24 10:32,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,,0,,,," 
{code:java}
set pipeline.operator-chaining=true;
 set execution.runtime-mode=BATCH;
  set table.exec.disabled-operators = NestedLoopJoin;
explain plan for
select
*
from
orders,
supplier,
customer
where
c_custkey = o_custkey and
c_nationkey = s_nationkey {code}
 

 

 

error:
{code:java}
org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 
 
FlinkLogicalJoin(condition=[AND(=($21, $2), =($24, $15))], joinType=[inner])
:- FlinkLogicalJoin(condition=[true], joinType=[inner])
:  :- FlinkLogicalTableSourceScan(table=[[paimon, tpch100g_paimon, orders]], fields=[uuid, o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, ts])
:  +- FlinkLogicalTableSourceScan(table=[[paimon, tpch100g_paimon, supplier]], fields=[uuid, s_suppkey, s_name, s_address, s_nationkey, s_phone, s_acctbal, s_comment, ts])
+- FlinkLogicalTableSourceScan(table=[[paimon, tpch100g_paimon, customer]], fields=[uuid, c_custkey, c_name, c_address, c_nationkey, c_phone, c_acctbal, c_mktsegment, c_comment, ts])
 
This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.
 
at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:70)
at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
at scala.collection.Iterator.foreach(Iterator.scala:937)
at scala.collection.Iterator.foreach$(Iterator.scala:937)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
at scala.collection.IterableLike.foreach(IterableLike.scala:70)
at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:93)
at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:58)
at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1(BatchCommonSubGraphBasedOptimizer.scala:45)
at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.$anonfun$doOptimize$1$adapted(BatchCommonSubGraphBasedOptimizer.scala:45)
at scala.collection.immutable.List.foreach(List.scala:388)
at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:329)
at org.apache.flink.table.planner.delegation.PlannerBase.getExplainGraphs(PlannerBase.scala:541)
at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:115)
at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:47)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainInternal(TableEnvironmentImpl.java:620)
at org.apache.flink.table.api.internal.TableEnvironmentInternal.explainInternal(TableEnvironmentInternal.java:96)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1296)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:658)
at org.grg_banking.flink.sqlexecute.FlinkUtils.exeucteSqlFile2(FlinkUtils.java:262)
at org.apache.flink.catalog.test.TestCatalog.testBatchDev(TestCatalog.java:136)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=BATCH_PHYSICAL, FlinkRelDistributionTraitDef=any, sort=[].
Missing conversion is FlinkLogicalJoin[convention: LOGICAL -> BATCH_PHYSICAL, FlinkRelDistributionTraitDef: any -> hash[2, 15]true]
There is 1 empty subset: rel#313:RelSubset#7.BATCH_PHYSICAL.hash[2, 15]true.[], the relevant part of the original plan is as follows
299:FlinkLogicalJoin(condition=[true], joinType=[inner])
  272:FlinkLogicalTableSourceScan(subset=[rel#297:RelSubset#5.LOGICAL.any.[]|#5.LOGICAL.any.[]], table=[[paimon, tpch100g_paimon, orders]], fields=[uuid, o_orderkey, o_custkey, o_orderstatus, o_totalprice, o_orderdate, o_orderpriority, o_clerk, o_shippriority, o_comment, ts])
  274:FlinkLogicalTableSourceScan(subset=[rel#298:RelSubset#6.LOGICAL.any.[]|#6.LOGICAL.any.[]], table=[[paimon, tpch100g_paimon, supplier]], fields=[uuid, s_suppkey, s_name, s_address, s_nationkey, s_phone, s_acctbal, s_comment, ts]) {code}
 

 

When I change to
{code:java}
select
  *
  from
  orders,
  customer,
  supplier
      
  where
c_custkey = o_custkey and
   c_nationkey = s_nationkey {code}
 will not report an error",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 10 10:32:18 UTC 2024,,,,,,,,,,"0|z1lo7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 06:49;lincoln.86xy;[~waywtdcc] can you have a try to enable the join reordering(which is disabled by default)?
set 'table.optimizer.join-reorder-enabled' = 'true';;;;","17/Nov/23 06:56;waywtdcc;After I turned on 'table.optimizer.join-reorder-enabled' = 'true';, the execution time was longer than select
   *
   from
   orders,
   customer,
   supplier
      
   where
c_custkey = o_custkey and
    c_nationkey = s_nationkey is much more efficient in execution. This is the statement of tpch.;;;","17/Nov/23 06:57;waywtdcc;But I hope to join in the order of join filter: (orders join customer) join supplier, because this has the highest performance. But after I set 'table.optimizer.join-reorder-enabled' = 'true'; the execution plan became (orders join (customer join supplier)),
Because the data volume of customer and supplier is smaller than that of orders, the performance is relatively low.;;;","17/Nov/23 07:01;waywtdcc;I think this situation should be automatically detected without opening reorder, and the join should be executed in the order of join condition.;;;","10/Mar/24 10:32;jeyhunkarimov;Hi [~waywtdcc] when I tried to reproduce locally as of the current master (d6a4eb966fbc47277e07b79e7c64939a62eb1d54), the above-mentioned exception seems to be gone. 

Could you please verify?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove scala-2.12 system variable,FLINK-33578,13558344,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,17/Nov/23 05:31,17/Nov/23 12:11,04/Jun/24 20:40,17/Nov/23 12:09,1.18.0,1.19.0,,,,,,,1.19.0,,,,Build System,Build System / CI,,,0,pull-request-available,,,We're only relying on Scala 2.12 right now. Now need to have the scala version specified. This makes the build system scripts easier.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27075,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 12:11:23 UTC 2023,,,,,,,,,,"0|z1lo7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 12:09;mapohl;master: [d1f124300c9851810409027c6715acef811e2320|https://github.com/apache/flink/commit/d1f124300c9851810409027c6715acef811e2320];;;","17/Nov/23 12:11;mapohl;No backport is necessary because omitting the system variable doesn't harm the CI run. Therefore, GHA in the release branches doesn't rely on it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make ""config.yaml"" as the default Flink configuration file",FLINK-33577,13558338,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,17/Nov/23 04:15,29/Jan/24 06:19,04/Jun/24 20:40,25/Jan/24 15:41,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,"This update ensures that the flink-dist package in FLINK will include the new configuration file ""config.yaml"" by default.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34257,,,,FLINK-34087,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 15:41:27 UTC 2024,,,,,,,,,,"0|z1lo60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/24 15:41;zhuzh;master/release-1.19:
9721ce835f5a7f28f2ad187346e009633307097b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Introduce new Flink conf file ""config.yaml"" supporting standard YAML syntax",FLINK-33576,13558336,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,17/Nov/23 04:07,15/Jan/24 08:13,04/Jun/24 20:40,15/Jan/24 08:13,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,0,pull-request-available,,,"Introduce new Flink conf file config.yaml, and this file will be parsed by standard YAML syntax.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 08:13:59 UTC 2024,,,,,,,,,,"0|z1lo5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 08:13;zhuzh;master/release-1.19: 
ddc7171dd83ade50e6b551d3bc2d31acb5ff6451
b6b2a0f7abebc92f849bfa3ef6f1a71f25b9672d
0153b903d4df2f49c95ee2c53737aa5e492439be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-394: Add Metrics for Connector Agnostic Autoscaling,FLINK-33575,13558307,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,mason6345,mason6345,mason6345,16/Nov/23 22:38,01/Feb/24 05:12,04/Jun/24 20:40,,,,,,,,,,,,,,Connectors / Common,Kubernetes Operator,Runtime / Metrics,,0,,,,https://cwiki.apache.org/confluence/display/FLINK/FLIP-394%3A+Add+Metrics+for+Connector+Agnostic+Autoscaling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 01 05:12:34 UTC 2024,,,,,,,,,,"0|z1lnz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 05:12;Zhanghao Chen;[~mason6345] Hi, we also need this feature, what are the plan for next steps? I'd be willing to help if needed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testRecoverAfterMultiplePersistsStateWithMultiPart andtestRecoverAfterMultiplePersistsStateWithMultiPart run into timeouts,FLINK-33574,13558229,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,16/Nov/23 13:21,08/Jan/24 11:38,04/Jun/24 20:40,08/Jan/24 11:38,1.19.0,,,,,,,,,,,,Connectors / FileSystem,,,,0,test-stability,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54446&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9]

Multiple connect_1 stages fail due to a timeout:
{code:java}
 Nov 09 02:09:33 ""main"" #1 prio=5 os_prio=0 tid=0x00007efd5400b800 nid=0x7c0e waiting on condition [0x00007efd5ccd8000]
Nov 09 02:09:33    java.lang.Thread.State: WAITING (parking)
Nov 09 02:09:33 	at sun.misc.Unsafe.park(Native Method)
Nov 09 02:09:33 	- parking to wait for  <0x00000000b762d130> (a java.util.concurrent.CompletableFuture$Signaller)
Nov 09 02:09:33 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Nov 09 02:09:33 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Nov 09 02:09:33 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
Nov 09 02:09:33 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Nov 09 02:09:33 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Nov 09 02:09:33 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartUploadToComplete(RecoverableMultiPartUploadImpl.java:233)
Nov 09 02:09:33 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartsUpload(RecoverableMultiPartUploadImpl.java:223)
Nov 09 02:09:33 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetRecoverable(RecoverableMultiPartUploadImpl.java:152)
Nov 09 02:09:33 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetCommitter(RecoverableMultiPartUploadImpl.java:122)
Nov 09 02:09:33 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetCommitter(RecoverableMultiPartUploadImpl.java:56)
Nov 09 02:09:33 	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeForCommit(S3RecoverableFsDataOutputStream.java:178)
Nov 09 02:09:33 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testResumeAfterMultiplePersist(AbstractHadoopRecoverableWriterITCase.java:375)
Nov 09 02:09:33 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(AbstractHadoopRecoverableWriterITCase.java:330)
Nov 09 02:09:33 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testRecoverAfterMultiplePersistsStateWithMultiPart(AbstractHadoopRecoverableWriterITCase.java:318)

[...]{code}
And
{code:java}
 Nov 09 01:53:59 ""main"" #1 prio=5 os_prio=0 cpu=3732.81ms elapsed=1707.61s tid=0x00007f7bec028000 nid=0x3e5 waiting on condition  [0x00007f7bf2c80000]
Nov 09 01:53:59    java.lang.Thread.State: WAITING (parking)
Nov 09 01:53:59 	at jdk.internal.misc.Unsafe.park(java.base@11.0.19/Native Method)
Nov 09 01:53:59 	- parking to wait for  <0x00000000aff7e730> (a java.util.concurrent.CompletableFuture$Signaller)
Nov 09 01:53:59 	at java.util.concurrent.locks.LockSupport.park(java.base@11.0.19/LockSupport.java:194)
Nov 09 01:53:59 	at java.util.concurrent.CompletableFuture$Signaller.block(java.base@11.0.19/CompletableFuture.java:1796)
Nov 09 01:53:59 	at java.util.concurrent.ForkJoinPool.managedBlock(java.base@11.0.19/ForkJoinPool.java:3128)
Nov 09 01:53:59 	at java.util.concurrent.CompletableFuture.waitingGet(java.base@11.0.19/CompletableFuture.java:1823)
Nov 09 01:53:59 	at java.util.concurrent.CompletableFuture.get(java.base@11.0.19/CompletableFuture.java:1998)
Nov 09 01:53:59 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartUploadToComplete(RecoverableMultiPartUploadImpl.java:233)
Nov 09 01:53:59 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartsUpload(RecoverableMultiPartUploadImpl.java:223)
Nov 09 01:53:59 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetRecoverable(RecoverableMultiPartUploadImpl.java:152)
Nov 09 01:53:59 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetRecoverable(RecoverableMultiPartUploadImpl.java:56)
Nov 09 01:53:59 	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.persist(S3RecoverableFsDataOutputStream.java:167)
Nov 09 01:53:59 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testResumeAfterMultiplePersist(AbstractHadoopRecoverableWriterITCase.java:351)
Nov 09 01:53:59 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(AbstractHadoopRecoverableWriterITCase.java:330)
Nov 09 01:53:59 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testRecoverFromIntermWithoutAdditionalStateWithMultiPart(AbstractHadoopRecoverableWriterITCase.java:312)

[...]{code}",,,,,,,,,,,,,,,,,,,,,,FLINK-33115,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 11:38:47 UTC 2024,,,,,,,,,,"0|z1lnhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Jan/24 23:31;Sergey Nuyanzin;[~mapohl] it seems it is a duplicate of FLINK-33115
please correct me if I'm wrong;;;","08/Jan/24 11:38;mapohl;Ok, good point. Let's collect it under FLINK-33115.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink GCP Connector implement retries to handle 429's and 503's,FLINK-33573,13558211,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jjayadeep,jjayadeep,16/Nov/23 10:34,16/Nov/23 10:34,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,"GCS connector have predefined quotas and limits and when these quotas are breached GCS returns a status code of 429's. GCS recommends handling these scenarios with retries as mentioned here [https://cloud.google.com/storage/docs/retry-strategy.]

 

Current, Flink gcs connector has not implemented the retries strategy and this change is to implement it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-16 10:34:25.0,,,,,,,,,,"0|z1lnds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minimize ConfigMap API operations for autoscaler state,FLINK-33572,13558210,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,16/Nov/23 10:19,24/Nov/23 20:19,04/Jun/24 20:40,16/Nov/23 10:22,kubernetes-operator-1.6.0,,,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"The newly introduced flush operation after the refactoring the autoscaler interfaces, optimizes the number of write operations to the underlying state store. A couple of further optimizations:

1. Any writes should be deferred until flush is called.
2. The flush routine should detect whether a write is needed and writing if there are no changes
3. Clearing state should only require one write operation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 10:22:16 UTC 2023,,,,,,,,,,"0|z1lndk:",9223372036854775807,Minimize ConfigMap operations for autoscaler state,,,,,,,,,,,,,,,,,,,"16/Nov/23 10:22;mxm;Implemented via 02840b96ef3116ea95a440af4f945398900d89df and ab0ec081eac86619a22632616fa4c01c074ecd33.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump json-path from 2.7.0 to 2.8.0,FLINK-33571,13558204,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liyubin117,liyubin117,16/Nov/23 09:25,16/Nov/23 09:35,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,,,0,,,,"json-path has critical bugs in 2.7.0 used in flink project, see [https://github.com/json-path/JsonPath/issues/906]

cve: [https://www.cve.org/CVERecord?id=CVE-2023-1370]

the current version is vulnerable to Denial of Service (DoS) due to a StackOverflowError when parsing a deeply nested JSON array or object, and the issue has been fixed in 2.8.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-16 09:25:31.0,,,,,,,,,,"0|z1lnc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRecoverySwitchStateBackendITCase failed fataly with exit code 127,FLINK-33570,13558195,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,16/Nov/23 08:49,16/Nov/23 12:23,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Runtime / State Backends,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54569&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8699

{code}
Nov 15 01:39:29 01:39:29.859 [ERROR] Error occurred in starting fork, check output in log
Nov 15 01:39:29 01:39:29.859 [ERROR] Process Exit Code: 127
Nov 15 01:39:29 01:39:29.859 [ERROR] Crashed tests:
Nov 15 01:39:29 01:39:29.859 [ERROR] ChangelogRecoverySwitchStateBackendITCase
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1314)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1159)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:932)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2(MojoExecutor.java:370)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute(MojoExecutor.java:351)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:215)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:171)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:163)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:294)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:960)
Nov 15 01:39:29 01:39:29.859 [ERROR] 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:293)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:196)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at java.lang.reflect.Method.invoke(Method.java:498)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:225)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:406)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:347)
Nov 15 01:39:29 01:39:29.860 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Nov 15 01:39:29 01:39:29.860 [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-tests && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /__w/2/s/flink-tests/target/surefire/surefirebooter1608310773658633260.jar /__w/2/s/flink-tests/target/surefire 2023-11-15T00-57-33_024-jvmRun2 surefire3628754069868698207tmp surefire_1872103361156590312007tmp
Nov 15 01:39:29 01:39:29.860 [ERROR] Error occurred in starting fork, check output in log
Nov 15 01:39:29 01:39:29.860 [ERROR] Process Exit Code: 127
Nov 15 01:39:29 01:39:29.860 [ERROR] Crashed tests:
Nov 15 01:39:29 01:39:29.860 [ERROR] ChangelogRecoverySwitchStateBackendITCase
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:465)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:442)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Nov 15 01:39:29 01:39:29.860 [ERROR] 	at java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/23 12:22;mapohl;FLINK-33570.test-failure.log.gz;https://issues.apache.org/jira/secure/attachment/13064458/FLINK-33570.test-failure.log.gz",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 12:23:05 UTC 2023,,,,,,,,,,"0|z1lna8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 12:21;mapohl;We had 127 exit code reported in FLINK-25621 as well. Let's see whether the issue reappears. It's hard to get anything out of the logs from what I'm seeing.;;;","16/Nov/23 12:23;mapohl;I attached the Maven logs for the run (starting from the test run that didn't finish) for reproducibility purposes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Could not deploy yarn-application when using yarn over s3a filesystem.,FLINK-33569,13558194,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,liubodong,liubodong,16/Nov/23 08:49,20/Nov/23 14:49,04/Jun/24 20:40,20/Nov/23 14:47,1.17.1,1.18.0,,,,,,,,,,,Deployment / YARN,,,,0,,,," 

I now use the `yarn-application` mode to deploy Flink. I found that when I set Hadoop's storage to the s3a file system, Flink could not submit tasks to Yarn.
The error is reported as follows:
{code:java}
------------------------------------------------------------

 The program finished with the following exception:



org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster

        at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:481)

        at org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer.run(ApplicationClusterDeployer.java:67)

        at org.apache.flink.client.cli.CliFrontend.runApplication(CliFrontend.java:212)

        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1098)

        at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)

        at java.security.AccessController.doPrivileged(Native Method)

        at javax.security.auth.Subject.doAs(Subject.java:422)

        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)

        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)

        at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)

        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)

Caused by: org.apache.hadoop.fs.PathIOException: `Cannot get relative path for URI:file:///tmp/application_1700122774429_0001-flink-conf.yaml5526160496134930395.tmp': Input/output error

        at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.getFinalPath(CopyFromLocalOperation.java:360)

        at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.uploadSourceFromFS(CopyFromLocalOperation.java:222)

        at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:169)

        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$26(S3AFileSystem.java:3854)

        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)

        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)

        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)

        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2480)

        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2499)

        at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:3847)

        at org.apache.flink.yarn.YarnApplicationFileUploader.copyToRemoteApplicationDir(YarnApplicationFileUploader.java:397)

        at org.apache.flink.yarn.YarnApplicationFileUploader.uploadLocalFileToRemote(YarnApplicationFileUploader.java:202)

        at org.apache.flink.yarn.YarnApplicationFileUploader.registerSingleLocalResource(YarnApplicationFileUploader.java:181)

        at org.apache.flink.yarn.YarnClusterDescriptor.startAppMaster(YarnClusterDescriptor.java:1050)

        at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:626)

        at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:474)

        ... 10 more


 {code}
I found by looking through the source code and debugging that when Hadoop uses the s3a file system, uploading and downloading files must use URIs with `scheme` to build path parameters.

In the `org.apache.flink.yarn.YarnClusterDescriptor` class, when uploading a temporarily generated `yaml` configuration file, the absolute path of the file is used instead of the URI as the path construction parameter, but other file upload and download behaviors They all use URI as the path parameter.

This is the reason for the error reported above.","h1. *Env:*
 * OS: ArchLinux kernel:{color:#000000}6.6.1 AMD64{color}
 * Flink: 1.17.1
 * Hadoop: 3.3.6
 * Minio: 2023-11-15

h1. Settings
h2. hadoop core-site.xml:

 
{code:java}
  <property>
    <name>fs.defaultFS</name>    
    <value>s3a://hadoop</value>
  </property>
  <property>
    <name>fs.s3a.path.style.access</name>
    <value>true</value>
  </property>
  <!-- minio username -->
  <property>
    <name>fs.s3a.access.key</name>
    <value>admin</value>
  </property>
  <!-- minio password -->
  <property>
    <name>fs.s3a.secret.key</name>
    <value>password</value>
  </property>
  <!-- minio endpoint -->
  <property>
    <name>fs.s3a.endpoint</name>
    <value>http://localhost:9000</value>
  </property>
  <property>
    <name>fs.s3a.connection.establish.timeout</name>
    <value>5000</value>
  </property>
  <property>
    <name>fs.s3a.multipart.size</name>
    <value>512M</value>
  </property>
  <property>
    <name>fs.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
  </property>
  <property>
    <name>fs.AbstractFileSystem.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3A</value>
  </property>
  <!-- S3 end -->{code}
h1. Flink run command:

./bin/flink run-application -t yarn-application ./examples/streaming/TopSpeedWindowing.jar

 

 ",,,,,,,,,,,,,,,,,,,,,FLINK-33472,,,,,,,,,,,,,,,,"16/Nov/23 08:47;liubodong;2023-11-16_16-47.png;https://issues.apache.org/jira/secure/attachment/13064451/2023-11-16_16-47.png","16/Nov/23 08:46;liubodong;image-2023-11-16-16-46-21-684.png;https://issues.apache.org/jira/secure/attachment/13064452/image-2023-11-16-16-46-21-684.png","16/Nov/23 08:48;liubodong;image-2023-11-16-16-48-40-223.png;https://issues.apache.org/jira/secure/attachment/13064450/image-2023-11-16-16-48-40-223.png",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 14:47:44 UTC 2023,,,,,,,,,,"0|z1lna0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 08:52;liubodong;I want to fix this issue by using 
{code:java}
new Path(tmpConfigurationFile.toURI()){code}
instead of
{code:java}
new Path(tmpConfigurationFile.getAbsolutePath()){code};;;","20/Nov/23 14:47;mapohl;I'm closing this one as a duplicate of FLINK-33472. [~liubodong] thanks for offering your help. There is already a contributor who offered to fix this issue. Feel free to collaborate with [~zhengzhili] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinkV2MetricsITCase.testCommitterMetrics fails with NullPointerException,FLINK-33568,13558190,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,pvary,mapohl,mapohl,16/Nov/23 08:36,21/Nov/23 08:34,04/Jun/24 20:40,20/Nov/23 18:19,1.19.0,,,,,,,,1.19.0,,,,Runtime / Metrics,,,,0,pull-request-available,,,"{code}
Nov 16 01:48:57 01:48:57.537 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.023 s <<< FAILURE! - in org.apache.flink.test.streaming.runtime.SinkV2MetricsITCase
Nov 16 01:48:57 01:48:57.538 [ERROR] org.apache.flink.test.streaming.runtime.SinkV2MetricsITCase.testCommitterMetrics  Time elapsed: 0.745 s  <<< ERROR!
Nov 16 01:48:57 java.lang.NullPointerException
Nov 16 01:48:57 	at org.apache.flink.test.streaming.runtime.SinkV2MetricsITCase.assertSinkCommitterMetrics(SinkV2MetricsITCase.java:254)
Nov 16 01:48:57 	at org.apache.flink.test.streaming.runtime.SinkV2MetricsITCase.testCommitterMetrics(SinkV2MetricsITCase.java:153)
Nov 16 01:48:57 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54602&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8546
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54602&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8605",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25857,,,,,,,,,,,,"16/Nov/23 21:31;pvary;Screenshot 2023-11-16 at 22.30.51.png;https://issues.apache.org/jira/secure/attachment/13064468/Screenshot+2023-11-16+at+22.30.51.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 20 18:19:55 UTC 2023,,,,,,,,,,"0|z1ln94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 12:27;mapohl;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54566&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=7610]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54566&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8933]
 * https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54566&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8747;;;","16/Nov/23 12:51;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54534&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8932;;;","16/Nov/23 12:57;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9037;;;","16/Nov/23 13:41;mapohl;This issue starts appearing in [20231113.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=results] (29e49482). The previous build [20231112.1|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54512&view=results] is based on 3dd98430. {{git log}} lists only three changes. One of those is [c3a07f98e5d1d7624adc967932f57d31355d9ddd|https://github.com/apache/flink/commit/c3a07f98e5d1d7624adc967932f57d31355d9ddd] which touches sink metric code:
{code:java}
commit c3a07f98e5d1d7624adc967932f57d31355d9ddd
Author: pvary <peter.vary.apache@gmail.com>
Date:   Sun Nov 12 17:31:34 2023 +0100    [FLINK-25857] Add committer metrics to track the status of committables
    
    Temporarily disables failing python connector tests which should be moved to external repositories.
    
    ---------
    
    Co-authored-by: Peter Vary <peter_vary4@apple.com> {code}
[~pvary]  can you have a look at this?;;;","16/Nov/23 15:47;pvary;Checking;;;","16/Nov/23 21:31;pvary;With the patch it looks like this:

!Screenshot 2023-11-16 at 22.30.51.png|width=612,height=128!;;;","17/Nov/23 12:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54646&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8548;;;","20/Nov/23 06:35;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","20/Nov/23 07:39;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b;;;","20/Nov/23 07:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca;;;","20/Nov/23 12:23;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54709&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8698;;;","20/Nov/23 18:19;mapohl;master: [7092bb4cc2afaa92c02743f46f23d2606c0e3c4e|https://github.com/apache/flink/commit/7092bb4cc2afaa92c02743f46f23d2606c0e3c4e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink documentation should only display connector downloads links when a connector is available,FLINK-33567,13558186,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,16/Nov/23 08:27,23/Nov/23 08:37,04/Jun/24 20:40,16/Nov/23 13:21,1.17.0,1.18.0,,,,,,,1.16.3,1.17.3,1.18.1,1.19.0,Documentation,,,,0,pull-request-available,,,"We currently have the situation that:

1. When visiting the master documentation, a message is correctly displayed that there are only connectors available for stable (= released) versions of Flink
2. When visiting the docs for release-1.18 or release-1.17, sometimes download links to non-existing links are displayed, because there's no compatible version of the connector (yet) available. 

In order to solve this, we should:
1. Add a Flink Compatibility collection to the connector repo doc
2. Use the compatibility collection in the documentation to display the correct links if a connector version for that Flink version is available, and else display the message that there's no connector release available for that Flink version",,,,,,,,,,,,,,,,,,,,FLINK-33492,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 19:37:46 UTC 2023,,,,,,,,,,"0|z1ln88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 08:27;martijnvisser;I have a local version working, I'll open a PR shortly;;;","16/Nov/23 12:49;martijnvisser;Fixed in:

apache/flink master 39eddc9a7295718e5171d922fb9888efa543c0e9
apache/flink release-1.18 c1987063b87d97cd4d2fff86cfffe1189b728be7
apache/flink release-1.17 98115db2a57ca0532a82b433fd2a5d8b59c462ad 
apache/flink release-1.16 3650fbdc02c514a4823788f2f24b7068015f4803 

apache/flink-connector-elasticsearch v3.0 d096e92ad168cd3bc0e7d0e405a2d0411cbb7c3a
apache/flink-connector-aws v4.1 456ec1e122acfdd0de10262be75b4b145f8e2342
apache/flink-connector-pulsar v4.0 772561c0b29dd5a7af6a5320983411c0385eee76
apache/flink-connector-jdbc v3.1 54a1bf8c545452733fe4c6d3ae3a729cc8154caa
apache/flink-connector-rabbitmq v3.0 844b8b24df8e4e951e24263ec712209602992c29
apache/flink-connector-gcp-pubsub v3.0 5d93ee8d6888d196da8badbc03fce1f925526c2a
apache/flink-connector-mongodb v1.0 78fe3ed7cd72de67f1a1ef02b8bcd2387d3efeb8
apache/flink-connector-opensearch v1.0 12bd07274dc6a607cfb1ae24e3b1c8cef444e8bf
apache/flink-connector-kafka v3.0 cd7cf119f03b22f88c61cb7e019c739f513473a6
apache/flink-connector-hbase v3.0 93bfdac6a707b97f5a34f09b687a82e70c87e5ff;;;","16/Nov/23 13:20;martijnvisser;apache/flink-connector-cassandra v3.1 bcf04ebc6dd218ce339a6c4b1c80193f8296a99f;;;","17/Nov/23 19:37;mason6345;[~martijnvisser] thanks for fixing this!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase sql-connector needs overwrite the rowKey,FLINK-33566,13558160,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jankowilliam,jankowilliam,16/Nov/23 03:17,16/Nov/23 09:07,04/Jun/24 20:40,,1.18.0,,,,,,,,hbase-3.0.0,,,,Connectors / HBase,,,,0,flink-connector-hbase,HBase-2.0,pull-request-available,"When I want to write label values of 50+to a rowkey column family in HBase (all values are label values of 0/1), for example:
{""id"":""1111"",""q1"":""0"",""q2"":""1"",""q3"":""0"",""q4"":""1"",...,""q49"":""0"",""q50"":""1""}
Here are four label values for example:
{""id"":""1111"",""q1"":""0"",""q2"":""1"",""q3"":""0"",""q4"":""1""}
{code:java}
--source：
CREATE TABLE kafka_table_(
 `id` STRING,
 `q1` STRING,
 `q2` STRING,
 `q3` STRING,
 `q4` STRING
)  WITH (
...
   'connector'='kafka',
   'format'='json',
...
);
--sink:
CREATE TABLE hbase_table_ (
 rowkey  STRING,
 cf ROW<q1 INT,q2 INT,q3 INT,q4 INT>,
 PRIMARY KEY (rowkey ) NOT ENFORCED
) WITH (
  'connector' = 'my-hbase-2.2',
  'table-name' = 'test_table',
  'zookeeper.quorum' = '127.0.0.1'
);
--insert:
insert into hbase_table_ 
select 
 id AS rowkey ,
 ROW( cast(q1 as INT),cast(q2 as INT),cast(q3 as INT),cast(q4 as INT)) as cf
 from kafka_table_  ;{code}
hbase:
hbase(main):016:0> scan 'test_table'
ROW                                                        COLUMN+CELL                                                                                                                                                                
 1111                                                      column=cf:q1, timestamp=0000000000001, value=\x00\x00\x00\x00                                                                                                              
 1111                                                      column=cf:q2, timestamp=0000000000001, value=\x00\x00\x00\x01                                                                                                              
 1111                                                      column=cf:q3, timestamp=0000000000001, value=\x00\x00\x00\x00                                                                                                              
 1111                                                      column=cf:q4, timestamp=0000000000001, value=\x00\x00\x00\x01 
 
Upstream data has a fixed value of 50+k-v data, among which very few value values are 1 (the default label value is 0). For example, only 1 or 2 values are 1: q2=1, q4=1, so I want HBase to store the following values:
hbase(main):016:0> scan 'test_table'
ROW                                                        COLUMN+CELL                                                                                                                                                                
 1111                                                      column=cf:q2, timestamp=0000000000001, value=\x00\x00\x00\x01                                                                                                             
 1111                                                      column=cf:q4, timestamp=0000000000001, value=\x00\x00\x00\x01 
 
When I use the ""sink. ignore null value"" keyword here, It just don't update the null value, and downstream third parties will still read all the values (such as 50+), but there are only 2 values that are truly 1:
{code:java}
--sink:
CREATE TABLE hbase_table_ (
 rowkey  STRING,
 cf ROW<q1 INT,q2 INT,q3 INT,q4 INT>,
 PRIMARY KEY (rowkey ) NOT ENFORCED
) WITH (
  'connector' = 'my-hbase-2.2',
  'table-name' = 'test_table',
  'sink.ignore-null-value' = 'true',
  'zookeeper.quorum' = '127.0.0.1'
); 
--insert:
insert into hbase_table_ 
select 
 id AS rowkey ,
 ROW(
 case when q1 <> '0' then cast(q1 as INT) else null end,
 case when q2 <> '0' then cast(q2 as INT) else null end,
 case when q3 <> '0' then cast(q3 as INT) else null end,
 case when q4 <> '0' then cast(q4 as INT) else null end 
) as cf
 from kafka_table_ ; {code}
hbase(main):016:0> scan 'test_table'
ROW                                                        COLUMN+CELL                                                                                                                                                                
 1111                                                      column=cf:q1, timestamp=0000000000001, value=\x00\x00\x00\x00                                                                                                              
 1111                                                      column=cf:q2, timestamp=0000000000002, value=\x00\x00\x00\x01                                                                                                              
 1111                                                      column=cf:q3, timestamp=0000000000001, value=\x00\x00\x00\x00                                                                                                              
 1111                                                      column=cf:q4, timestamp=0000000000002, value=\x00\x00\x00\x01 
 
There are no other configurations available, so I hope to have the function of overwriting and writing rowKey, that is, deleting the rowkey before adding new data.","flink: 1.18.0

hbase: 2.2.3

flink-connector-hbase-2.2:3.0.0-1.18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-16 03:17:35.0,,,,,,,,,,"0|z1ln2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The concurrentExceptions doesn't work,FLINK-33565,13558151,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,16/Nov/23 02:23,23/Jan/24 10:10,04/Jun/24 20:40,23/Jan/24 10:10,1.17.1,1.18.0,,,,,,,1.19.0,,,,Runtime / Coordination,,,,0,pull-request-available,,,"First of all, thanks to [~mapohl] for helping double-check in advance that this was indeed a bug .

Displaying exception history in WebUI is supported in FLINK-6042.
h1. What's the concurrentExceptions?

When an execution fails due to an exception, other executions in the same region will also restart, and the first Exception is rootException. If other restarted executions also report Exception at this time, we hope to collect these exceptions and Displayed to the user as concurrentExceptions.
h2. What's this bug?

The concurrentExceptions is always empty in production, even if other executions report exception at very close times.
h1. Why doesn't it work?

If one job has all-to-all shuffle, this job only has one region, and this region has a lot of executions. If one execution throw exception:
 * JobMaster will mark the state as FAILED for this execution.
 * The rest of executions of this region will be marked to CANCELING.
 ** This call stack can be found at FLIP-364 [part-4.2.3|https://cwiki.apache.org/confluence/display/FLINK/FLIP-364%3A+Improve+the+restart-strategy#FLIP364:Improvetherestartstrategy-4.2.3Detailedcodeforfull-failover] 

When these executions throw exception as well, it JobMaster will mark the state from CANCELING to CANCELED instead of FAILED.

The CANCELED execution won't call FAILED logic, so their exceptions are ignored.

Note: all reports are executed inside of JobMaster RPC thread, it's single thread. So these reports are executed serially. So only one execution is marked to FAILED, and the rest of executions will be marked to CANCELED later.
h1. How to fix it?

Offline discuss with [~mapohl] , we need to discuss with community should we keep the concurrentExceptions first.
 * If no, we can remove related logic directly
 * If yew, we discuss how to fix it later.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33121,,,,,,,"18/Dec/23 04:17;fanrui;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13065379/screenshot-1.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 10:10:26 UTC 2024,,,,,,,,,,"0|z1ln0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/23 14:16;mapohl;Thanks for raising the issue, [~fanrui] . I had a brief discussion with [~chesnay] on that topic:

There's a difference between the {{Default-}} and the {{{}AdaptiveScheduler{}}}. The latter one doesn't support pipelined regions. The {{DefaultScheduler}} does support them. Therefore, concurrent exceptions should NOT happen when using the {{{}AdaptiveScheduler{}}}. But there was an issue in the past that cannot be explained till now where concurrent exceptions caused an issue in a run that had the {{AdaptiveScheduler}} enabled (see FLINK-33121). So far, they looked into it but struggled to find the cause for this.

On the other hand, {{DefaultScheduler}} comes with pipelined region support. The scenario that they have considered when thinking about concurrent exceptions was that you can have two pipelined regions being executed concurrently. They are both failing independently with one of the two errors becoming the root cause for the job's failure. The {{PipelinedRegionSchedulingStrategy}} is in charge of scheduling vertex restarts. Apparently, it would be possible to put the vertices of two different pipelines together to reduce the number of restarts.

I looked into the code of \{{PipelinedRegionSchedulingStrategy#restartTasks}}. I struggled to find the merge behavior, though. Based on my finding, the \{{PipelinedRegionSchedulingStrategy}} does indeed merge pipelined regions together. But only based on the vertices that are already selected for a restart. Because of this, I'm not sure whether the conclusion Chesnay and I came up with in the first place is correct. I wanted to share it, anyway. I'm wondering whether you find a mistake in our reasoning.;;;","21/Nov/23 05:22;fanrui;Hi [~mapohl] , thanks for providing these background.:)
{quote}There's a difference between the {{Default-}} and the {{{}AdaptiveScheduler{}}}. The latter one doesn't support pipelined regions. The {{DefaultScheduler}} does support them. Therefore, concurrent exceptions should happen when using the {{{}AdaptiveScheduler{}}}.
{quote}
I don't understand why concurrent exceptions should happen when using the {{{}AdaptiveScheduler{}}}. When one job only has all-to-all shuffle, AdaptiveScheduler and DefaultScheduler should have similar exception-related logic, right?

 
{quote}But there was an issue in the past that cannot be explained till now where concurrent exceptions caused an issue in a run that had the {{AdaptiveScheduler}} enabled (see FLINK-33121). So far, they looked into it but struggled to find the cause for this.
{quote}
I will take a look FLINK-33121 as well.

 
{quote}On the other hand, {{DefaultScheduler}} comes with pipelined region support. The scenario that they have considered when thinking about concurrent exceptions was that you can have two pipelined regions being executed concurrently. They are both failing independently with one of the two errors becoming the root cause for the job's failure. The {{PipelinedRegionSchedulingStrategy}} is in charge of scheduling vertex restarts. Apparently, it would be possible to put the vertices of two different pipelines together to reduce the number of restarts.

I looked into the code of {{{}PipelinedRegionSchedulingStrategy#restartTasks{}}}. I struggled to find the merge behavior, though. Based on my finding, the {{PipelinedRegionSchedulingStrategy}} does indeed merge pipelined regions together. But only based on the vertices that are already selected for a restart. 
{quote}
Yeah, I agree with you. When multiple regions fail at the same time:
 * Action1: flink should restart them together 
 * Action2: and pick one as the root cause, the rest of exceptions as the concurrent exceptions 

IIUC, this action1 was proposed by [FLIP-364|https://cwiki.apache.org/confluence/x/uJqzDw] , the part 1.2 is related to this. FLIP-364 proposes merging multiple Exceptions into one restart.

And action2 was mentioned by [~zhuzh]  in the [mail list|https://lists.apache.org/thread/l7wyc7pndpsvh2h7hj3fw2td9yphrlox] of FLIP-364.

 

Please correct me if I misunderstood anything, and looking forward to your feedback, thanks~;;;","22/Nov/23 09:57;mapohl;{quote}
I don't understand why concurrent exceptions should happen when using the AdaptiveScheduler. When one job only has all-to-all shuffle, AdaptiveScheduler and DefaultScheduler should have similar exception-related logic, right?
{quote}
That's correct. It was a typo in my comment. I meant that ""concurrent exceptions should NOT happen when using the AdaptiveScheduler"". I corrected it in my comment above.;;;","23/Nov/23 10:47;fanrui;Thanks for the clarification!

So you mean that when a job has multiple regions and using Default Scheduler and region failover,  the concurrent exceptions can happen. However, we cannot find any merge logic in {{{}PipelinedRegionSchedulingStrategy#restartTasks{}}}, it's unexpected. right?

If yes, we can discuss how to merge them.;;;","23/Nov/23 13:25;mapohl;Correct, I struggled to find the merge logic in {{{}PipelinedRegionSchedulingStrategy{}}}. I was wondering whether I'm missing something or whether you agree with my finding.;;;","23/Nov/23 13:57;fanrui;Before creating this JIRA, I found that the concurrentExceptions doesn't work either multi-region or single region. So I totally agree with you.

 ;;;","24/Nov/23 05:29;fanrui;Give some background here: [FLIP-364|https://cwiki.apache.org/confluence/x/uJqzDw] is improving the restart-strategy. Currently, one pain point of restart-strategy is the restart-attempt is increased too fast (Read more details from FLIP-364 1.2 part).

The core solution is similar and related to concurrentExceptions: it's that we merge some exceptions into one restart-attempt.

[~zhuzh]  and me would the restart-attempt is matched with the concurrentExceptions, when we merge some exceptions into one restart-attempt: we pick the first exception as the root cause, the rest of exceptions as the concurrent exceptions.

[~mapohl] WDYT?;;;","01/Dec/23 08:21;mapohl;Sorry for the delay in replying. I have a few other things going on right now that keep me busy. :-/ I think the overall idea makes sense. I still want to do some investigation into whether we're missing something here with the pipelined regions. But that shouldn't be a blocker for what you're planning. I think what you laid out in FLIP-364 sounds reasonable. (y);;;","01/Dec/23 08:59;fanrui;Thanks [~mapohl] for your feedback.

I'm trying to fix this bug after FLIP-364. During the development of FLIP-364, I will also read the relevant code to see how to merge Exception. Try to ensure that FLIP-364 can work well with merge Exception.

 ;;;","18/Dec/23 04:19;fanrui;Hi [~mapohl], I have finished the POC, and wanna to check whether my solution is fine for you.

POC can be found in this [PR|https://github.com/apache/flink/pull/23867], this PR will be updated in the future, and I have backuped a permanent branch here: [https://github.com/1996fanrui/flink/commits/33565/permanent-backup1/]

Core idea is: For each failure, RestartStrategy will return whether current failure is a new attempt(Based on FLIP-364).
 * If it's a new attempt, it's root exception, and it will be the latest root exception.
 * If it's not a new attempt, it will be a concurrentException and it will be added to the latest RootException.

Core changes:

My poc branch has 3 commits related to this JIRA:
 # FLINK-33565[Exception] Archive exceptions into the exception history immediately when they occur, instead of archiving them when restarting
 # FLINK-33565[Exception] Restart strategy checks whether current failure is a new attempt
 # FLINK-33565[Scheduler] ConcurrentExceptions works with exception merging

The first commit is refactoring, actually, I don't know why archiving exception when restarting task instead of immediately. It means, when one task failure, we can see the exception history after flink restart this task. So the first commit is only a refactoring. It archives exceptions into the exception history immediately when they occur, instead of archiving them when restarting.

The second commit is related to restart strategy, adding a return value indicates whether current failure is a new attempt.

The third commit is core solution of this JIRA:
 * If it's a new attempt, it's root exception. and it will be the latest root exception.
 * If it's not a new attempt, it will be a concurrentException and it will be added to the latest RootException.

The last commit, I added a job demo with 6 regions, all tasks will fail when processing the first record, this demo job can be run directly. Here is the result, we can see all failed tasks in the WebUI.

If you agree with my solution, I can go ahead. If not, we can discuss first. Looking forward to your opinions, thanks~

!screenshot-1.png|width=923,height=468!;;;","22/Dec/23 20:31;mapohl;Hi [~fanrui], thanks for letting me know and sorry for being less responsive. I was on sick leave the past two weeks. I might be able to have another look at it after the upcoming holiday week.;;;","23/Dec/23 05:59;fanrui;Thank you, and please take good care of yourself! Merry Chrismas.:);;;","08/Jan/24 02:37;fanrui;Hey [~mapohl] , I have summited the PR[1] for this JIRA. I didn't finish the detailed test due to I wanna check with you whether the solution is fine.

Would you mind helping take a look this PR in your free time? It's better to finish it in 1.19, thanks~ :)

 

[1]https://github.com/apache/flink/pull/24003;;;","23/Jan/24 10:10;fanrui;Merged to master(1.19.0) via:
* ad3f13f1d5b1604fda76292944233be78a87b456
* d031f6008d7cc2cb7928adbf84ff89e7bccf9dde
* 9f20bc41db85b0928cd98c172a85cb43076e0ec0
* bb437bd07f583ffa4c86bc525659abe477d6d051
* 8a7e54993980d84886c5c24d2fe9d2f7f42b9f74;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for GroupWindowAggregate node,FLINK-33564,13558143,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,bvarghese,bvarghese,bvarghese,16/Nov/23 00:28,24/Nov/23 15:58,04/Jun/24 20:40,24/Nov/23 15:58,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 15:58:08 UTC 2023,,,,,,,,,,"0|z1lmyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 15:58;dwysakowicz;Implemented in 34ec781ac7547d376d09854983d169a3aca5130f..92bc1dc81bd52af195a751984fa344f11dda9fdc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement type inference for Agg functions,FLINK-33563,13558078,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,15/Nov/23 13:58,16/Nov/23 12:49,04/Jun/24 20:40,16/Nov/23 12:49,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,"* COLLECT
* DISTINCT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 12:49:00 UTC 2023,,,,,,,,,,"0|z1lmk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 12:49;dwysakowicz;Implemented in 2e5facef83e08ec1e77ddabae0213b7e924e392a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize RabbitMQ Python connector code,FLINK-33562,13558059,13557618,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mbalassi,mbalassi,15/Nov/23 10:41,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,,,,1.20.0,,,,API / Python,Connectors/ RabbitMQ,,,0,,,,See description of parent ticket for context.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-15 10:41:06.0,,,,,,,,,,"0|z1lmg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Pulsar Python connector code,FLINK-33561,13558054,13557618,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mbalassi,mbalassi,15/Nov/23 10:34,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,,,,1.20.0,,,,API / Python,Connectors / Pulsar,,,0,,,,See description of parent ticket for context.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-15 10:34:38.0,,,,,,,,,,"0|z1lmew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Kinesis Python connector code,FLINK-33560,13558053,13557618,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,mbalassi,mbalassi,15/Nov/23 10:29,04/Jan/24 13:37,04/Jun/24 20:40,04/Jan/24 13:37,1.18.0,aws-connector-4.2.0,,,,,,,1.19.0,aws-connector-4.3.0,,,API / Python,Connectors / Kinesis,,,0,pull-request-available,,,See description of parent ticket for context.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 04 13:36:44 UTC 2024,,,,,,,,,,"0|z1lmeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/24 13:36;dannycranmer;Merged commit [{{b7f2e26}}|https://github.com/apache/flink-connector-aws/commit/b7f2e268e377ee18e9133ef1367bdecf04d62d71] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Kafka Python connector code,FLINK-33559,13558051,13557618,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,mbalassi,mbalassi,15/Nov/23 10:28,11/Dec/23 08:41,04/Jun/24 20:40,11/Dec/23 08:41,1.18.0,,,,,,,,1.19.0,,,,API / Python,Connectors / Kafka,,,0,pull-request-available,,,See description of parent ticket for context.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 08:41:13 UTC 2023,,,,,,,,,,"0|z1lme8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 08:41;gaborgsomogyi;c38a040 on main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Elasticsearch Python connector code,FLINK-33558,13558048,13557618,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,mbalassi,mbalassi,15/Nov/23 10:25,11/Mar/24 12:43,04/Jun/24 20:40,,1.18.0,,,,,,,,1.20.0,,,,API / Python,Connectors / ElasticSearch,,,0,,,,See description of parent ticket for context.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-15 10:25:30.0,,,,,,,,,,"0|z1lmdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Cassandra Python connector code,FLINK-33557,13558046,13557618,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ferenc-csaky,mbalassi,mbalassi,15/Nov/23 10:24,14/Mar/24 07:36,04/Jun/24 20:40,,1.18.0,,,,,,,,1.20.0,,,,API / Python,Connectors / Cassandra,,,0,pull-request-available,,,See description of parent ticket for context.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-15 10:24:51.0,,,,,,,,,,"0|z1lmd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test infrastructure for externalized python code,FLINK-33556,13558037,13557618,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pvary,mbalassi,mbalassi,15/Nov/23 10:20,07/Dec/23 09:36,04/Jun/24 20:40,07/Dec/23 09:34,1.18.0,,,,,,,,1.19.0,,,,API / Python,Connectors / Common,,,0,pull-request-available,,,We need to establish the reusable parts of the python infrastructure as part of the shared connector utils such that it can be easily reused. Ideally we would create a github workflow similar to https://github.com/apache/flink-connector-shared-utils/blob/ci_utils/.github/workflows/ci.yml.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 07 09:34:21 UTC 2023,,,,,,,,,,"0|z1lmb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 08:44;gaborgsomogyi;e4f3898 on master;;;","07/Dec/23 09:34;gaborgsomogyi;7691962 on ci_utils;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory:,FLINK-33555,13558033,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,mapohl,mapohl,15/Nov/23 09:42,16/Feb/24 10:54,04/Jun/24 20:40,,1.19.0,1.20.0,,,,,,,,,,,Tests,,,,0,test-stability,,,"https://github.com/XComp/flink/actions/runs/6868936761/job/18680977238#step:12:13492

{code}
Error: 21:44:15 21:44:15.144 [ERROR]   LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory:119 [The task was deployed to AllocationID(fcf411eadbae8beed895a78ea1653046) but it should have been deployed to AllocationID(dec337d82b9d960004ffd73be8a2c5d5) for local recovery., The task was deployed to AllocationID(a61fd8a6bc5ef9d467f32f918bdfb385) but it should have been deployed to AllocationID(fcf411eadbae8beed895a78ea1653046) for local recovery., The task was deployed to AllocationID(dec337d82b9d960004ffd73be8a2c5d5) but it should have been deployed to AllocationID(a61fd8a6bc5ef9d467f32f918bdfb385) for local recovery.] ==> expected: <true> but was: <false>
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/24 09:27;mapohl;FLINK-33555.log;https://issues.apache.org/jira/secure/attachment/13066764/FLINK-33555.log",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 16 10:54:45 UTC 2024,,,,,,,,,,"0|z1lma8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/24 07:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57550&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=15485
{code}
Feb 16 01:14:56 01:14:56.299 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 39.33 s <<< FAILURE! -- in org.apache.flink.test.recovery.LocalRecoveryITCase
Feb 16 01:14:56 01:14:56.299 [ERROR] org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory -- Time elapsed: 39.27 s <<< FAILURE!
Feb 16 01:14:56 org.opentest4j.AssertionFailedError: [The task was deployed to AllocationID(34c031bb72931f33a70b6a55fe30501c) but it should have been deployed to AllocationID(ee1115e87892e59107adfa6b7bfbfd13) for local recovery., The task was deployed to AllocationID(ee1115e87892e59107adfa6b7bfbfd13) but it should have been deployed to AllocationID(34c031bb72931f33a70b6a55fe30501c) for local recovery.] ==> expected: <true> but was: <false>
Feb 16 01:14:56 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Feb 16 01:14:56 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Feb 16 01:14:56 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Feb 16 01:14:56 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Feb 16 01:14:56 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:214)
Feb 16 01:14:56 	at org.apache.flink.test.recovery.LocalRecoveryITCase.testRecoverLocallyFromProcessCrashWithWorkingDirectory(LocalRecoveryITCase.java:119)
Feb 16 01:14:56 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 16 01:14:56 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Feb 16 01:14:56 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Feb 16 01:14:56 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Feb 16 01:14:56 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Feb 16 01:14:56 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
{code}

I'm moving this one out of FLINK-27075 as it appeared in Azure Pipelines as well.;;;","16/Feb/24 09:20;mapohl;[~srichter] Did any of you work on local recovery topics other than adding tests for 1.19? It doesn't look like it based on the git history. I'm just trying to determine whether that's a new issue that we might need to resolve before releasing 1.19.;;;","16/Feb/24 10:54;srichter;[~mapohl] I did some work related to local recovery (essentially using available local state in rescaling), but that doesn't affect allocation or non-rescaling scenarios.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SemiJoinITCase.testRewriteScalarQueryWithCorrelation3 failed due to heartbeat timeout,FLINK-33554,13558032,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Nov/23 09:35,15/Nov/23 09:35,04/Jun/24 20:40,,,,,,,,,,,,,,Tests,,,,0,github-actions,test-stability,,"https://github.com/XComp/flink/actions/runs/6868936761/job/18680976575#step:12:11771

{code}
[...]
Nov 14 22:16:06 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Nov 14 22:16:06 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
Nov 14 22:16:06 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:765)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:742)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
Nov 14 22:16:06 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1645)
Nov 14 22:16:06 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1144)
Nov 14 22:16:06 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1084)
Nov 14 22:16:06 	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:785)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182)
Nov 14 22:16:06 	at org.apache.flink.runtime.scheduler.SimpleExecutionSlotAllocator$LogicalSlotHolder.release(SimpleExecutionSlotAllocator.java:203)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:482)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeAndReleaseSlots(DefaultDeclarativeSlotPool.java:474)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:445)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.internalReleaseTaskManager(DeclarativeSlotPoolService.java:275)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.releaseTaskManager(DeclarativeSlotPoolService.java:231)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:549)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.handleTaskManagerConnectionLoss(JobMaster.java:1469)
Nov 14 22:16:06 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1464)
Nov 14 22:16:06 	at org.apache.flink.runtime.heartbeat.DefaultHeartbeatMonitor.run(DefaultHeartbeatMonitor.java:158)
Nov 14 22:16:06 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Nov 14 22:16:06 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Nov 14 22:16:06 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
Nov 14 22:16:06 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Nov 14 22:16:06 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
Nov 14 22:16:06 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
Nov 14 22:16:06 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Nov 14 22:16:06 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Nov 14 22:16:06 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Nov 14 22:16:06 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Nov 14 22:16:06 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Nov 14 22:16:06 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Nov 14 22:16:06 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Nov 14 22:16:06 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Nov 14 22:16:06 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Nov 14 22:16:06 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Nov 14 22:16:06 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Nov 14 22:16:06 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Nov 14 22:16:06 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Nov 14 22:16:06 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Nov 14 22:16:06 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Nov 14 22:16:06 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Nov 14 22:16:06 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Nov 14 22:16:06 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Nov 14 22:16:06 	... 4 more
Nov 14 22:16:06 Caused by: java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id ced94aab-3f37-4d9c-b8b7-86af1e5df83a timed out.
Nov 14 22:16:06 	... 30 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-15 09:35:38.0,,,,,,,,,,"0|z1lma0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileUtilsTest.testDeleteDirectoryConcurrently failed (no progress/output),FLINK-33553,13558031,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Nov/23 09:31,15/Nov/23 09:31,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,"https://github.com/XComp/flink/actions/runs/6866957256/job/18674730884#step:12:6367

{code}
[...]
Nov 14 17:15:36 17:15:36.889 [INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.093 s - in org.apache.flink.core.io.LocatableSplitAssignerTest
Nov 14 17:45:50 ==============================================================================
Nov 14 17:45:50 Process produced no output for 1800 seconds.
Nov 14 17:45:50 ==============================================================================
Nov 14 17:45:50 ==============================================================================
Nov 14 17:45:50 The following Java processes are running (JPS)
Nov 14 17:45:50 ==============================================================================
[...]
Nov 14 17:45:51 ""ForkJoinPool-151-worker-1"" #630 daemon prio=5 os_prio=0 tid=0x00007fb26cdf3000 nid=0x87a in Object.wait() [0x00007fb1dece9000]
Nov 14 17:45:51    java.lang.Thread.State: WAITING (on object monitor)
Nov 14 17:45:51 	at java.lang.Object.wait(Native Method)
Nov 14 17:45:51 	at java.lang.Thread.join(Thread.java:1252)
Nov 14 17:45:51 	- locked <0x00000000e8443420> (a org.apache.flink.util.FileUtilsTest$Deleter)
Nov 14 17:45:51 	at org.apache.flink.core.testutils.CheckedThread.trySync(CheckedThread.java:104)
Nov 14 17:45:51 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:92)
Nov 14 17:45:51 	at org.apache.flink.core.testutils.CheckedThread.sync(CheckedThread.java:81)
Nov 14 17:45:51 	at org.apache.flink.util.FileUtilsTest.testDeleteDirectoryConcurrently(FileUtilsTest.java:201)
Nov 14 17:45:51 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-15 09:31:03.0,,,,,,,,,,"0|z1lm9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job (and its data) is lost,FLINK-33552,13558030,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,15/Nov/23 09:27,27/Nov/23 08:47,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,"No investigation possible.

https://github.com/XComp/flink/actions/runs/6866313050

{code}
Flink CI (Java 8/Scala 2.12/Hadoop 2.10.2) / Test (module: misc)
The hosted runner: GitHub Actions 18 lost communication with the server. Anything in your workflow that terminates the runner process, starves it for CPU/Memory, or blocks its network access can cause this error.
{code}

This is kept for documentation purposes for now.",,,,,,,,,,,,,,,,,,,,,FLINK-33658,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 27 08:47:25 UTC 2023,,,,,,,,,,"0|z1lm9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 09:28;mapohl;Same happened with this job: https://github.com/XComp/flink/actions/runs/6866957256/job/18674765328
Affected stages:
* connect_2
* tests
* e2e 1
* e2e 2;;;","27/Nov/23 08:47;mapohl;https://github.com/XComp/flink/actions/runs/6997726518;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveTemporalJoinITCase.testProcTimeTemporalJoinHiveTable failed due to unsupported feature,FLINK-33551,13558029,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,mapohl,mapohl,15/Nov/23 09:24,16/Nov/23 13:12,04/Jun/24 20:40,16/Nov/23 12:00,1.19.0,,,,,,,,1.19.0,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,"https://github.com/XComp/flink/actions/runs/6865564561/job/18670787612#step:12:23311
{code}
Error: 16:08:30 16:08:30.479 [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.706 s <<< FAILURE! - in org.apache.flink.connectors.hive.HiveTemporalJoinITCase
Error: 16:08:30 16:08:30.480 [ERROR] org.apache.flink.connectors.hive.HiveTemporalJoinITCase.testProcTimeTemporalJoinHiveTable  Time elapsed: 2.618 s  <<< ERROR!
Nov 14 16:08:30 org.apache.flink.table.api.TableException: Processing-time temporal join is not supported yet.
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.createJoinOperator(StreamExecTemporalJoin.java:293)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.getJoinOperator(StreamExecTemporalJoin.java:255)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecTemporalJoin.translateToPlanInternal(StreamExecTemporalJoin.java:179)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:167)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:258)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecCalc.translateToPlanInternal(CommonExecCalc.java:94)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:167)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:258)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:177)
Nov 14 16:08:30 	at org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:167)
Nov 14 16:08:30 	at org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85)
Nov 14 16:08:30 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
Nov 14 16:08:30 	at scala.collection.Iterator.foreach(Iterator.scala:937)
Nov 14 16:08:30 	at scala.collection.Iterator.foreach$(Iterator.scala:937)
Nov 14 16:08:30 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
Nov 14 16:08:30 	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
Nov 14 16:08:30 	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
Nov 14 16:08:30 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
Nov 14 16:08:30 	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
Nov 14 16:08:30 	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
Nov 14 16:08:30 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
Nov 14 16:08:30 	at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84)
Nov 14 16:08:30 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:184)
Nov 14 16:08:30 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277)
Nov 14 16:08:30 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:1055)
Nov 14 16:08:30 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1120)
Nov 14 16:08:30 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:735)
Nov 14 16:08:30 	at org.apache.flink.connectors.hive.HiveTemporalJoinITCase.testProcTimeTemporalJoinHiveTable(HiveTemporalJoinITCase.java:109)
Nov 14 16:08:30 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33023,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 12:28:04 UTC 2023,,,,,,,,,,"0|z1lm9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 09:32;mapohl;https://github.com/XComp/flink/actions/runs/6868936761/job/18681141648#step:12:23310

It happens with Hadoop 3.2.3;;;","15/Nov/23 16:58;mapohl;The issue is not related to the GHA work (FLINK-27075). It does also appear in Azure CI:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54566&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23798
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54534&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23437
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23797
* ...;;;","15/Nov/23 17:05;mapohl;The issue started appearing in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54508&view=results (3dd98430). The previous nightly (7542b56f) didn't have this error.

{code}
$ git log 7542b56f..3dd98430 --oneline --name-only
[...]
4cd8285274a [FLINK-33023][table-planner][JUnit5 Migration] Module: flink-connectors/flink-connector-hive (TableTestBase)
flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTemporalJoinITCase.java
[...]
{code}

The test was touched in FLINK-33023. I'm marking this as the cause for now. [~jiabao.sun] can you have a look into it?;;;","16/Nov/23 02:27;jiabao.sun;Sure;;;","16/Nov/23 03:18;jiabao.sun;Some of the remaining codes were not deleted, resulting in the problem.
https://github.com/apache/flink/pull/23349/files#diff-1e88862e59ad54f8c596e5e56f126fec6a09768bee986c9547d14d2add1e048bR109-R112

The fix is ready, [~mapohl] please take a look when you have time.
Sorry my negligence.;;;","16/Nov/23 08:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54602&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23798;;;","16/Nov/23 12:00;mapohl;master: [b2b31522c8854de0a80d112b0959a7d12bfe298b|https://github.com/apache/flink/commit/b2b31522c8854de0a80d112b0959a7d12bfe298b];;;","16/Nov/23 12:28;mapohl;Build doesn't contain the fix, yet (links are added for documentation purposes):
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54566&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=24159
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54534&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23438
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23798
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54512&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23817
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54508&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=23838;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""DataSet allround end-to-end test"" failed due to certificate error",FLINK-33550,13558025,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Later,,mapohl,mapohl,15/Nov/23 09:20,20/Dec/23 12:01,04/Jun/24 20:40,20/Dec/23 12:01,,,,,,,,,,,,,Tests,,,,0,github-actions,test-stability,,"https://github.com/XComp/flink/actions/runs/6865535375/job/18670359423

{code}
Certificate was added to keystore
Certificate was added to keystore
Certificate reply was installed in keystore
Error outputting keys and certificates
40F767F1D97F0000:error:0308010C:digital envelope routines:inner_evp_generic_fetch:unsupported:../crypto/evp/evp_fetch.c:349:Global default library context, Algorithm (RC2-40-CBC : 0), Properties ()
Nov 14 15:39:21 [FAIL] Test script contains errors.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 12:01:19 UTC 2023,,,,,,,,,,"0|z1lm8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 09:21;mapohl;* https://github.com/XComp/flink/actions/runs/6865564561/job/18670825765#step:14:1286
* https://github.com/XComp/flink/actions/runs/6865564561/job/18670414709#step:14:1288;;;","15/Nov/23 09:25;mapohl;https://github.com/XComp/flink/actions/runs/6866166097/job/18672591431#step:14:1287;;;","15/Nov/23 09:26;mapohl;https://github.com/XComp/flink/actions/runs/6866313050/job/18672918064#step:14:1350;;;","15/Nov/23 09:32;mapohl;https://github.com/XComp/flink/actions/runs/6868936761/job/18681163974#step:14:1287;;;","15/Nov/23 09:42;mapohl;https://github.com/XComp/flink/actions/runs/6868936761/job/18681008614#step:14:1290;;;","15/Nov/23 09:43;mapohl;https://github.com/XComp/flink/actions/runs/6871897090/job/18689723401#step:14:1287;;;","15/Nov/23 09:43;mapohl;https://github.com/XComp/flink/actions/runs/6873649211/job/18694242611#step:14:1286;;;","15/Nov/23 14:32;mapohl;This issue is caused by the switch to Java8:
* [Java 8 run|https://github.com/XComp/flink/actions/runs/6877650528] failed
* [Java 11 run|https://github.com/XComp/flink/actions/runs/6878191818] succeeded;;;","15/Nov/23 16:40;mapohl;The GHA workflow uses {{ubuntu-latest}}, i.e. 22.04, which uses OpenSSL 3.0.0. OpenSSL 3.0.0 seems to have some backwards incompatibilities with Java 8.;;;","16/Nov/23 12:03;mapohl;Downgrading to 20.04 doesn't seem to be that straight-forward, either, because with 20.04 we start running into docker issues (see [related build|https://github.com/XComp/flink/actions/runs/6888060140]) :(;;;","17/Nov/23 14:43;mapohl;https://www.misterpki.com/java-keystore-pkcs12-legacy/;;;","20/Dec/23 12:01;mapohl;This issue is superceded by FLINK-33902.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception ""Factory does not implement interface YieldingOperatorFactory"" thrown in batch mode ",FLINK-33549,13558023,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xu_shuai_,luoyuxia,luoyuxia,15/Nov/23 09:01,07/Feb/24 02:32,04/Jun/24 20:40,,,,,,,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"When run a job in batch, it throws the following exception
{code:java}
java.lang.NullPointerException: Factory does not implement interface org.apache.flink.streaming.api.operators.YieldingOperatorFactory
    at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:104)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory.getMailboxExecutor(AbstractStreamOperatorFactory.java:67)
    at org.apache.flink.table.runtime.operators.AsyncWaitOperatorFactory.createStreamOperator(AsyncWaitOperatorFactory.java:79)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.createOperator(TableOperatorWrapper.java:115)
    at org.apache.flink.table.runtime.operators.multipleinput.MultipleInputStreamOperatorBase.createAllOperators(MultipleInputStreamOperatorBase.java:259)
    at org.apache.flink.table.runtime.operators.multipleinput.MultipleInputStreamOperatorBase.<init>(MultipleInputStreamOperatorBase.java:88)
    at org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperator.<init>(BatchMultipleInputStreamOperator.java:48)
    at org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperatorFactory.createStreamOperator(BatchMultipleInputStreamOperatorFactory.java:51)
    at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:81)
    at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:212)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.<init>(RegularOperatorChain.java:60)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:756)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:743)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:959)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:928)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:751)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:567)
    at java.lang.Thread.run(Thread.java:834) {code}
 

When I disable multiple-input by setting table.optimizer.multiple-input-enabled = false, it works then. 

Should be introduced by FLINK-23621.

[In here|https://github.com/apache/flink/blob/a1aed4f877099328d4833f8a2781d2edbaaddc70/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamOperatorFactoryUtil.java#L60], when the operator factory is instanceof YieldingOperatorFactory, it will set mailbox executor. But when it's 
BatchMultipleInputStreamOperatorFactory, althogh it'll still set mailbox executor but it won't set the mailbox executor. for the operators wrapped by the BatchMultipleInputStreamOperator. Then the exception is thrown.
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 23 03:39:10 UTC 2023,,,,,,,,,,"0|z1lm80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 03:39;xu_shuai_;Hi [~luoyuxia] , I'd like to fix this. Could you assign it to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace taskManager/jobmanager.resource with resources conforming to ResourceRequirements,FLINK-33548,13557997,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gyfora,gyfora,15/Nov/23 04:05,08/Dec/23 14:06,04/Jun/24 20:40,,,,,,,,,,,,,,Kubernetes Operator,,,,1,,,,"Resource requests/limits are often a question for the Kubernetes operator as it doesn't follow the standard ResourceRequirements schema used in container-like resources.

We should leave the current TaskManagerSpec/JobManagerSpec.resource field as is, deprecate it and introduce a new resources field with the proper schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 14:06:21 UTC 2023,,,,,,,,,,"0|z1lm28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/23 06:05;richard.su;[~gyfora] Hi Gyula, may I ask if there has been any progress on this issue? I had seen the discussion of 
 # FLINK-33609
 # FLINK-24150;;;","08/Dec/23 14:06;gyfora;I don't think that anyone has started working on this yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL primitive array type after upgrading to Flink 1.18.0,FLINK-33547,13557993,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xccui,xccui,15/Nov/23 02:35,01/Feb/24 09:41,04/Jun/24 20:40,24/Nov/23 04:27,1.18.0,,,,,,,,,,,,Table SQL / Runtime,,,,0,,,,"We have some Flink SQL UDFs that use object array (Object[]) arguments and take boxed arrays (e.g., Float[]) as parameters. After upgrading to Flink 1.18.0, the data created by ARRAY[] SQL function became primitive arrays (e.g., float[]) and it caused argument mismatch issues. I'm not sure if it's expected.",,,,,,,,,,,,,,,,,,,,,,FLINK-33523,,,,,,,,,FLINK-31835,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 24 04:27:37 UTC 2023,,,,,,,,,,"0|z1lm1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 04:14;xuyangzhong;Maybe caused by FLINK-31835. ;;;","23/Nov/23 12:34;jeyhunkarimov;Hi [~xccui], the mismatch occurs depending on the input type. 
The data created by ARRAY[] SQL function becomes primitive arrays only when the ARRAY elements are NOT NULL.
This makes sense since primitives (e.g., float, int) cannot be null. 

That is why you get the described exception.;;;","24/Nov/23 03:30;xccui;Hi [~jeyhun] , thanks for your attention!

What you explained makes sense. However, sometimes it's tricky to deal with primitive array parameters. They make it harder for users to write generic UDFs, e.g., one that takes an arbitrary array and returns the first 3 elements. Also, this is a breaking change in 1.18.0. All the old UDFs using Object[] as arguments before can't directly work for primitive arrays generated from ARRAY functions now.

Type inference and dealing with null values are challenging in Flink SQL. Users won't understand why a UDF accepting an ARRAY<INT> argument can't work for an ARRAY<INT NOT NULL> parameter. It's also impossible for UDF developers to code a bunch of functions taking different primitive arrays. We need some changes here.;;;","24/Nov/23 04:11;prabhujoseph;[~xccui]  Thanks for the details. This issue seems to be a duplicate of [FLINK-33523|https://issues.apache.org/jira/browse/FLINK-33523]. Shall we track this issue in FLINK-33523? Thanks.;;;","24/Nov/23 04:26;xccui;Sure. I'll close this.;;;","24/Nov/23 04:27;xccui;Duplicated with https://issues.apache.org/jira/browse/FLINK-33523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement type inference for order(asc/desc) function,FLINK-33546,13557990,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,jackylau,jackylau,15/Nov/23 01:53,17/Nov/23 12:43,04/Jun/24 20:40,16/Nov/23 16:00,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 12:43:12 UTC 2023,,,,,,,,,,"0|z1lm0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 16:00;dwysakowicz;Implemented in 54906feae0bb2978bbd58bf91c6d458d44262437;;;","17/Nov/23 11:10;jackylau;hi [~dwysakowicz] 

the input any may not cover this case if the input is literal, it should throw exception

that is why have
if (!child.isInstanceOf[NamedExpression]) {
      ValidationFailure(s""Sort should only based on field reference"");;;","17/Nov/23 12:43;dwysakowicz;[~jackylau]

We can not cover it 100% in the API. In the type inference we check types not expression kinds. We could check for literals, but we could not check e.g. for function calls. We could add a weaker restriction and prohibit literals, but from the logical point of view, I don't think there is anything that would make us do that. I would not introduce that limitation at that level.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink implementation can cause dataloss during broker issue when not using EXACTLY_ONCE if there's any batching,FLINK-33545,13557968,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,aeolus811tw,aeolus811tw,aeolus811tw,14/Nov/23 21:20,08/Apr/24 18:42,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"In the current implementation of KafkaSource and KafkaSink there are some assumption that were made:
 # KafkaSource completely relies on Checkpoint to manage and track its offset in *KafkaSourceReader<T>* class
 # KafkaSink in *KafkaWriter<IN>* class only performs catch-flush when *DeliveryGuarantee.EXACTLY_ONCE* is specified.

KafkaSource is assuming that checkpoint should be properly fenced and everything it had read up-til checkpoint being initiated will be processed or recorded by operators downstream, including the TwoPhaseCommiter such as *KafkaSink*

*KafkaSink* goes by the model of:

 
{code:java}
flush -> prepareCommit -> commit{code}
 

In a scenario that:
 * KafkaSource ingested records #1 to #100
 * KafkaSink only had chance to send records #1 to #96
 * with a batching interval of 5ms

when checkpoint has been initiated, flush will only confirm the sending of record #1 to #96.

This allows checkpoint to proceed as there's no error, and record #97 to 100 will be batched after first flush.

Now, if broker goes down / has issue that caused the internal KafkaProducer to not be able to send out the record after a batch, and is on a constant retry-cycle (default value of KafkaProducer retries is Integer.MAX_VALUE), *WriterCallback* error handling will never be triggered until the next checkpoint flush.

This can be tested by creating a faulty Kafka cluster and run the following code:
{code:java}
Properties props = new Properties(); 
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER);
props.put(ProducerConfig.CLIENT_ID_CONFIG, ""example-producer"");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); 
props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE); 
props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE); 
props.put(ProducerConfig.ACKS_CONFIG, ""all""); 
final KafkaProducer<String, String> producer = new KafkaProducer<>(props);
try {
    for (int i = 0; i < 10; i++) {
        System.out.printf(""sending record #%d\n"", i);
        String data = UUID.randomUUID().toString();
        final ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC, Integer.toString(i), data);
        producer.send(record, new CB(Integer.toString(i), data));
        Thread.sleep(10000); //sleep for 10 seconds
    }
} catch (Exception e) {
    e.printStackTrace();
} finally {
    System.out.println(""flushing"");
    producer.flush();
    System.out.println(""closing"");
    producer.close();
}{code}
Once callback returns due to network timeout, it will cause Flink to restart from previously saved checkpoint (which recorded reading up to record #100), but KafkaWriter never sent record #97 to #100.

This will result in dataloss of record #97 to #100

Because KafkaWriter only catches error *after* callback, if callback is never invoked (due to broker issue) right after the first flush has taken place, those records are effectively gone unless someone decided to go back and look for it.

This behavior should be ok if user has set {*}DeliveryGuarantee.NONE{*}, but is not expected for {*}DeliveryGuarantee.AT_LEAST_ONCE{*}.

There is a divergence of the process in the event of {*}EXACTLY_ONCE{*}.

prepareCommit will produce a list of KafkaCommittable that corresponds to Transactional KafkaProducer to be committed. And a catch up flush will take place during *commit* step. Whether this was intentional or not, due to the fact that flush is a blocking call, the second flush for EXACTLY_ONCE at the end of EXACTLY_ONCE actually ensured everything fenced in the current checkpoint will be sent to Kafka, or fail the checkpoint if not successful.

 

Due the above finding, I'm recommending one of the following fixes:
 # need to perform second flush for AT_LEAST_ONCE
 # or move flush to the end of the KafkaSink process.

I'm leaning towards 2nd option as it does not make sense to flush then do checkpoint, it should be right before checkpoint completes then we flush, given that's what commit is meant to do.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 21 05:26:50 UTC 2024,,,,,,,,,,"0|z1llvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 21:20;aeolus811tw;we have a fix to address this issue, can someone assign this bug to me, thanks!;;;","16/Nov/23 16:56;tzulitai;hey [~aeolus811tw], thanks for the detailed writeup.

I'm still trying to understand the need for the second flush, but wanted to give a quick comment on:

> prepareCommit will produce a list of KafkaCommittable that corresponds to Transactional KafkaProducer to be committed. And a catch up flush will take place during *commit* step. Whether this was intentional or not, due to the fact that flush is a blocking call, the second flush for EXACTLY_ONCE at the end of EXACTLY_ONCE actually ensured everything fenced in the current checkpoint will be sent to Kafka, or fail the checkpoint if not successful.

the {{producer.commitTransaction()}} call does a flush first before sending out the request to commit the Kafka transaction. The assumption in the {{KafkaWriter}} code has been that by the time {{commitTransaction()}} is called, all pending records are already flushed, so the internal flush done at commit time should have been a no-op.

If I am understanding you correctly, as it sounds like in this ticket, that second flush actually is not a no-op?;;;","16/Nov/23 17:09;tzulitai;{code:java}
In a scenario that:
 * KafkaSource ingested records #1 to #100
 * KafkaSink only had chance to send records #1 to #96
 * with a batching interval of 5ms

when checkpoint has been initiated, flush will only confirm the sending of record #1 to #96.{code}
this is where I'm a bit lost. A flush() call on a producer is blocking until all records are acked + after the blocking flush returns, we check if any of the records that were flushed resulted in an error in which case we fail the job and not complete the checkpoint. See that code here:

[https://github.com/apache/flink-connector-kafka/blob/main/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaWriter.java#L201-L208]

So, how could the KafkaWriter only flush records #1 to #96 and already proceed to process the checkpoint barrier? This can only ever happen with unaligned checkpointing, and even in that case, records #97 to #100 will be part of the channel state that will be replayed upon recovery.;;;","16/Nov/23 17:39;tzulitai;https://issues.apache.org/jira/browse/FLINK-33293 related issue;;;","16/Nov/23 19:06;aeolus811tw;Hi Tzu-Li,

Thanks for the response.

Below is my analysis of the issue, and please correct me if i am wrong on any part

 
{code:java}
this is where I'm a bit lost. A flush() call on a producer is blocking until all records are acked + after the blocking flush returns, we check if any of the records that were flushed resulted in an error in which case we fail the job and not complete the checkpoint.{code}
 

The problem with the first flush is that it happened before actual snapshot is tracked, inside `SinkWriteOperator.prepareSnapshotPreBarrier`

If broker is alive and well, the first flush will have no issue, which is what we are seeing.

AT_LEAST_ONCE and NONE are currently behaving the same in this regards, since they don't generate commitable after this.

from this point til the actual snapshot being triggered in `KafkaSource.snapshotState` (record #1 ~ #100) there seem to be a race condition where some record may have been ingested, but wasn't part of the first flush (record #1 ~ #96) and have now made it to the producer (#97 ~ #100).

since the whole flink process is concurrent it only allows the internal KafkaProducer to actually commit its current buffer, not necessary all records that are still flowing through. And the way flink is utilizing KafkaProducer asynchronously, it can't catch error until KafkaProducer actually attempted to commit (flush).

There is only a short window of time between the first flush and the snapshotState of the KafkaSource (approx 15ms at most for situations that i have tested)

 
{code:java}
If I am understanding you correctly, as it sounds like in this ticket, that second flush actually is not a no-op?{code}
 

You are correct that the 2nd flush is a no-op as commitTransaction will flush before committing; making it actually doing 3 flushes in total when EXACTLY_ONCE is set. But there's still a flush that ensure all records have been committed before completing the checkpoint.

the problem is there's no 2nd flush for AT_LEAST_ONCE when checkpoint is finalizing to ensure there is no data left in the buffer / still being sent by the internal KafkaProducer.

This resulted in the current triggered checkpoint to be successful, as it only required first flush to be successful.

I supposed the original design decided that any read status of the non-flushed records can go into next checkpoint, but if broker is having issue at this point then the next checkpoint will not be successful, causing Flink restart from previous successful checkpoint handling to kick in.

 
{code:java}
So, how could the KafkaWriter only flush records #1 to #96 and already proceed to process the checkpoint barrier? This can only ever happen with unaligned checkpointing, and even in that case, records #97 to #100 will be part of the channel state that will be replayed upon recovery.{code}
I'm assuming this is referring to channel state of KafkaSink / functional operator precede it?

doesn't this require KafkaSource to keep track of these uncommitted records? or operators before KafkaSink and after KafkaSource to keep these records in states? because once it reaches KafkaSink, it doesn't keep track of anything beyond first flush.

 

I made following modification to test this scenario:
 # added flag to track pending records in FlinkKafkaInternalProducer between send & flush methods
 # changed prepareCommit to also check for this flag, and only return empty commitable by default if DeliveryGuarantee.NONE is set
 # in KafkaCommitter i use the flag producer.isInTransaction to determine whether it should be a commitTransaction call / flush call (as with the change #1 & #2 AT_LEAST_ONCE will also reach this segment)

in my testing the
{code:java}
LOG.debug(""Committing {} committables."", committables); {code}
debug line that's supposed to be triggered only if there's record to be flushed/committed do get triggered from time to time when there's broker stability issue, confirming my suspicion.

 ;;;","16/Nov/23 20:07;aeolus811tw;I'm also aware of the issue FLINK-31305

i don't believe that change really make any difference here.

it changed from throwing FlinkRuntimeException within WriterCallback to storing it in asyncProducerException

I would say original way to handle the exception was fine, as flush is a blocking call that will always trigger WriterCallback, storing then check v.s throwing right away doesn't really make any difference;;;","01/Dec/23 01:22;mason6345;Hi all, I just saw this ticket. I've read through the thread here and I'm still confused how this can happen.
{quote}The problem with the first flush is that it happened before actual snapshot is tracked, inside `SinkWriteOperator.prepareSnapshotPreBarrier`

If broker is alive and well, the first flush will have no issue, which is what we are seeing.

AT_LEAST_ONCE and NONE are currently behaving the same in this regards, since they don't generate commitable after this.
{quote}
Are you saying that an exception is not thrown by the KafkaWriter#flush method due to the Kafka producer timeout configuration?
{quote}from this point til the actual snapshot being triggered in `KafkaSource.snapshotState` (record #1 ~ #100) there seem to be a race condition where some record may have been ingested, but wasn't part of the first flush (record #1 ~ #96) and have now made it to the producer (#97 ~ #100).
{quote}
The checkpoint barrier would have been between record 96 and 97. So the checkpoint afterwards would covers the at-least-once guarantee of 97-100. If you are saying that the checkpoint barrier is not respected, then that would be a bug in Flink. By any chance, does your KafkaSink serialization schema produce multiple records from one input record?

The original description describes a scenario when the flush succeeds after a broker becomes healthy again. If the flush request is within checkpoint timeout, then this is fine right? If not, I would expect Flink to fail the checkpoint as it as exceeded the checkpoint timeout.

It seems that https://issues.apache.org/jira/browse/FLINK-33293 and this one suggests that Flink runtime does not throw exception on checkpoint timeout when Kafka producer timeout is very close.;;;","01/Dec/23 05:16;aeolus811tw;Hi Mason,

thank you for the comment.

In the issue reported i made couple assumptions:
 # Flink Checkpoint does complete without any hindrance, causing future job failure not to recover from proper offset, leading to data loss
 # Broker has successfully acked record during first flush (with ack=-1 or ack=all) as by design, making it impossible for data loss to be originated from broker issue

{code:java}
Are you saying that an exception is not thrown by the KafkaWriter#flush method due to the Kafka producer timeout configuration?{code}
observation showed that first flush does complete successfully, which allowed checkpoint to proceed, job failure caused by Broker outage is observed and Flink will attempt to restore from the created checkpoint (the one that was successfully created)
{code:java}
The checkpoint barrier would have been between record 96 and 97. So the checkpoint afterwards would covers the at-least-once guarantee of 97-100. If you are saying that the checkpoint barrier is not respected, then that would be a bug in Flink.{code}
my understanding is based on the documentation

[https://nightlies.apache.org/flink/flink-docs-master/docs/learn-flink/fault_tolerance/]

[https://flink.apache.org/2018/02/28/an-overview-of-end-to-end-exactly-once-processing-in-apache-flink-with-apache-kafka-too/]

checkpoint barrier produced by the initial operator of a task manager (KafkaSource in this case) and flow along the graph within the same task manager, not every operator will generate a barrier.

In this scenario KafkaSource will generate a barrier at record 100 instead of between 96 and 97, which it will successfully complete as there's no issue committing read offset back to Broker at this point.

this is based on the precondition of everything before the barrier generated will be processed or tracked in the state, if anything breaks this precondition, there will be data loss.

Due to this, i believe the issue lies within the implementation of KafkaSink that did not fully honor the contract of checkpoint.
{code:java}
By any chance, does your KafkaSink serialization schema produce multiple records from one input record? {code}
data loss has been observed in 1-to-1 record streaming as well as 1-to-many scenarios.
{code:java}
The original description describes a scenario when the flush succeeds after a broker becomes healthy again. If the flush request is within checkpoint timeout, then this is fine right? If not, I would expect Flink to fail the checkpoint as it as exceeded the checkpoint timeout.{code}
As per observation, if broker became healthy within the producer timeout / checkpoint timeout period, there's no data loss observed.

If broker incurred network / unexpected failure for a prolong period of time, data loss will be observed from time to time.

the requirement for data loss to appear to be Flink failure induced restart.

 

My hypothesis after process of elimination, albeit with some uncertainty is that:

Based on the information illustrated in the implementation of prebarrier: https://issues.apache.org/jira/browse/FLINK-9428

Pre-Barrier will be invoked before barrier is emitted, but there's no mentioning of Pre-Barrier blocking other records from being processed simultaneously and I was not able to find any synchronize / blocking.

If the above is true and Flink failed right after checkpoint succeed, there will be data loss as KafkaSink does not do anything after Pre-Barrier phase except when being set as EXACTLY_ONCE

However, given that i'm not able to confirm some of the suspicion i have, I figured the safest way to perform flush is within the commit phase of Two Phase Commit, instead of pre-commit / prepareCommit phase, hence the adding flush to KafkaCommitter for AT_LEAST_ONCE.

 

Let me know if I have any misunderstand of how checkpointing, barrier handling, and 2PC works, or if there's any mistake in my assumption.

Thanks

 

 ;;;","05/Dec/23 23:35;mason6345;The reason why you don't find any synchronization is because the task is single threaded. Even if the issue you describe exists, it would need to be fixed in the Flink runtime and not the Kafka connector, as such a bug would affect all sink connectors.

Assuming the Flink runtime is correct, the only way for data loss to occur if there is a case when the Kafka Producer API doesn't throw an exception when the broker has not ack'ed the record.

Otherwise, to check if Flink runtime is correct, we would need to analyze how the mailexecutor behaves.;;;","06/Dec/23 18:00;aeolus811tw;Hi Mason,

Kafka Producer only throw exception in following situation:
 # broker connection failure (either by broker or intermediary means such as firewall timeout)
 # timeout took place (delivery or request timeout)
 # record issue that violates producer setting (request size limit etc)

anything other than these producer will keep retrying (as default of retry is Integer.MAX_VALUE) and will not throw error

however a flush should block until any of the above 3 condition took place.

this can be verified using the snippet i included above in the description of the issue.

that being said, due to the nature of KafkaProducer and stability of it, I had to assume it worked as intended.

if a task is truly single thread, including committing / barrier handling, there shouldn't be any data loss. But that's not what was observed.

the records that were lost generally aren't large in volume, only resembles one or two producer thread failure. And in the case of AT_LEAST_ONCE this shouldn't happen.

Given that, I think it might be safer to put commit / flush to the end of the commit cycle, instead of before actual checkpoint is taking place.;;;","16/Jan/24 18:37;yang;Hello [~aeolus811tw] [~mason6345] [~tzulitai] , Just find this ticket, do you think if there will be a fix about this?
I'll need to activate kafkaProducer property ""batch.size"" , ""linger.ms"" and ""compression.type"" for batching and compression and we use at_least_once semantic and kafka ack ""1"".  I imagine I am in the batching scenario described by you;;;","25/Jan/24 22:19;martijnvisser;[~mason6345] WDYT about [~aeolus811tw] his last comments? ;;;","26/Jan/24 07:59;mason6345;I do think this issue here is not with the Flink runtime but it could be possible that the Kafka client flush doesn't flush all records (I haven't personally encountered this yet). As explained, if even there was a bug with the Flink runtime, the proper fix would need to be in Flink, rather than this connector.

Your proposal is to pretty much explicitly retry the flush, but there's no telling if that 2nd flush won't exhibit the same behavior as the first. As such, the best thing that the connector can do here is to 
 # maintain a counter of records to be flushed.
 # on completion of flush during checkpoint phase, verify that there are no exceptions AND no pending records.
 # if there was lingering records, fail immediately–causing job restart and subsequently checkpoint to fail.

The best thing we can do here is to fail and replay from last checkpoint, to maintain at least/exactly once semantics. This is also inline with how FlinkKafkaProducer has implemented this. My best guess is there is some race condition during broker network issues. I think we can implement this first and make carefully logging when this condition occurs.

WDYT of my proposal? [~aeolus811tw] [~yang] [~tzulitai] [~martijnvisser] ;;;","28/Jan/24 09:50;aeolus811tw;Hi [~mason6345] 

I'm not sure that's possible without going deeper into KafkaProducer code.

KafkaProducer is only a facade to the actual buffer that holds and sends record via RecordAccumulator class, it then performs a blocking wait on the iterative request result of which doesn't really give us any insight into how many records have been synced successfully. To achieve what is described on tracking exact pending record will require us to go deeper into the Kafka internal classes, which might make this more prone to issues

as per my PR that i have done the following:
 # use variable to track whether there's pending record
 # if there is and Guarantee is AT_LEAST_ONCE, we perform second commit / follow EXACTLY_ONCE path to trigger the commit on producer
 # if there's any failure, it should throw error immediately as per EXACTLY_ONCE route

and since step 3 is the final step before flink checkpoint is committed, if this happens the retry logic should take place as you described

Do let me know if this implementation satisfies the proposal: [~yang] [~tzulitai] [~martijnvisser] 

Thanks

 ;;;","28/Jan/24 10:12;yang;Hello,

If I understand correctly, [~mason6345] 's proposal doesn't necessarily imply that we need to track each exact pending record. Instead, we can continue utilizing [~aeolus811tw] 's variable to monitor pending records. If this variable remains true after the first flush, rather than attempting a second flush, we should consider failing the checkpoint. This could compel Flink to replay from the last checkpoint.

Thanks;;;","29/Jan/24 08:23;martijnvisser;Since the reported exception is related to the Kafka Client, the first thing I actually want to see is that this gets reproduced with the latest version of the Flink Kafka connector. The currently latest available Flink Kafka connector (v3.0.2) uses Kafka Client 3.2.3, while the newer Flink Kafka Connector v3.1 that's currently undergoing a vote uses Kafka Client v3.4.0. It can very well be that we encountered a bug in the used Kafka Client, that has been fixed since. 

Even better, we actually should upgrade the used Kafka Client to the latest version (v3.6.0). Potentially we should even wait until the 2PC changes are coming in. ;;;","30/Jan/24 02:23;mason6345;[~yang] yup that's what I'm proposing. We could track an AtomicInteger and decrement it in the writer callback. Then, we need to assert that the counter is 0 (this is what we assume to be the contract of the KafkaProducer#flush API). ;;;","13/Feb/24 20:15;mason6345;Hi [~aeolus811tw], have you had time to review the followup feedback? As mentioned before there's no guarantee that a second commit would succeed and it would possibly need multiple. The best way to do this is by throwing an exception and allowing Flink to restart and try again ;;;","15/Feb/24 18:47;aeolus811tw;Hi [~mason6345] , [~yang] 

I have simplified the approach to implement the suggestion.

Now the change is:
 # changed hasRecordsInBuffer flag to flushGated
 # only set this to true at
 ## the beginning of flush, and unset at the end of flush within FlinkKafkaInternalProducer
 ## the beginning of commitTransaction, and unset at the end of commitTransaction within FlinkKafkaInternalProducer
 # added check for this flag in FlinkKafkaInternalProducer::send and throw KafkaException when detected to be true (flush gating is taking place)
 # added FlinkKafkaInternalProducer::setFlushGate(boolean closed) as public for unit testing purpose

this limited the modification to the code at a minimal level to only FlinkKafkaInternalProducer and KafkaWriterITCase classes, and should not have any impact on any other scenario given that commitTransaction & flush should be an atomic operation in KafkaProducer

Are you guys ok with the above approach?;;;","21/Feb/24 05:26;aeolus811tw;Hi [~mason6345] 

I have replicated the behavior you referred to in
{code:java}
https://github.com/apache/flink-connector-kafka/blob/15f2662eccf461d9d539ed87a78c9851cd17fa43/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer.java#L1107
{code}
based on the logic displayed, it seemed *FlinkKafkaProducer* (the now deprecated producer) also considered the possibility of race condition,

as *pendingRecords* is only increased in *FlinkKafkaProducer::invoke* akin to *KafkaWriter::write* / *FlinkKafkaInternalProducer::send*

and decreased only in *FlinkKafkaProducer::acknowledgeMessage* that is only invoked within *Callback::onCompletion* akin to *KafkaWriter$WriterCallback::onCompletion*

the updated implementation are:
 # add pendingRecords typed AtomicLong (same as FlinkKafkaProducer)
 # increase the variable in FlinkKafkaInternalProducer::send
 # create an intermediate callback class TrackingCallback that decorates callback parameter of FlinkKafkaInternalProducer::send
 # decrease pendingRecords in the TrackingCallback decorated class
 # check the pendingRecords variable after flush, within FlinkKafkaInternalProducer::flush to ensure nothing else was sent while flushing has taken place, and throw IllegalStateException (same exception as FlinkKafkaProducer)

 

 ;;;",,,,,,,,,,,,,,,,,,,,,
Flink documentation fails to build,FLINK-33544,13557923,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,chesnay,martijnvisser,martijnvisser,14/Nov/23 14:49,28/Nov/23 08:31,04/Jun/24 20:40,28/Nov/23 08:31,,,,,,,,,,,,,Build System,Documentation,,,0,,,,"{code:java}
hugo
README.md
LICENSE
fatal: detected dubious ownership in repository at '/root/flink'
To add an exception for this directory, call:

	git config --global --add safe.directory /root/flink
{code}

https://github.com/apache/flink/actions/runs/6861406527/job/18657037038#step:5:153",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 28 08:31:51 UTC 2023,,,,,,,,,,"0|z1llls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 08:31;martijnvisser;Fixed with a newer Docker image;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Java 17-specific code into its own module to please Intellij,FLINK-33543,13557890,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,14/Nov/23 12:16,15/Nov/23 08:35,04/Jun/24 20:40,15/Nov/23 08:35,1.19.0,,,,,,,,1.19.0,,,,API / Type Serialization System,Build System,,,0,pull-request-available,,,"FLINK-32380 introduced support for Java records (which are introduced in Java 14). Test classes were added and included via Maven profiles. This works on the command line. Intellij runs into issues though when build {{flink-core}}.

[~snuyanzin] filed [IDEA-338060|https://youtrack.jetbrains.com/issue/IDEA-338060/How-to-make-IntellijIdea-aware-of-maven-compiler-plugin-testExcludes] to cover this issue.

A workaround on the Flink side is to have a dedicated module for Java 17 code (more specifically the test classes which use Java record). This would make {{flink-core}} compilable again. The newly introduced module can be enabled for the {{java17}} profile.",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32380,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 15 08:35:15 UTC 2023,,,,,,,,,,"0|z1lleg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 08:35;mapohl;master: [a1aed4f877099328d4833f8a2781d2edbaaddc70|https://github.com/apache/flink/commit/a1aed4f877099328d4833f8a2781d2edbaaddc70];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update HBase connector tests to JUnit5,FLINK-33542,13557882,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,14/Nov/23 12:01,17/Nov/23 10:11,04/Jun/24 20:40,17/Nov/23 10:11,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 10:11:51 UTC 2023,,,,,,,,,,"0|z1llco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 10:11;martijnvisser;Fixed in apache/flink-connector-hbase:

main: a36b137d0474312b893934dbdf36afde8bb2819f
v3.0: c0216b75242b97f03e895b9127d0f17174ca2ea5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RAND_INTEGER  can't be existed in a IF statement,FLINK-33541,13557861,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,dianer17,dianer17,14/Nov/23 10:45,15/Dec/23 15:24,04/Jun/24 20:40,15/Dec/23 15:24,1.17.0,1.18.0,,,,,,,1.17.3,1.18.1,1.19.0,,Table SQL / API,,,,0,pull-request-available,,,"The minimum produce steps:

Flink SQL> select if(1=1, rand_integer(100), 0);
[ERROR] Could not execute SQL statement. Reason:
java.lang.Exception: Unsupported operand types: IF(boolean, INT, INT NOT NULL)

 

But we do not see the exception reported in 1.14, not sure which version this bug was introduced.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/23 05:31;xuyangzhong;image-2023-11-24-13-31-21-209.png;https://issues.apache.org/jira/secure/attachment/13064656/image-2023-11-24-13-31-21-209.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 15 15:24:43 UTC 2023,,,,,,,,,,"0|z1ll80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 12:08;martijnvisser;Isn't this by design, per the 1.15 release notes? https://nightlies.apache.org/flink/flink-docs-master/release-notes/flink-1.15/#table-api--sql;;;","14/Nov/23 15:43;libenchao;[~martijnvisser] Could you point out the ticket number you are referring to? I didn't find one in the 1.15 release note.;;;","21/Nov/23 06:03;xuyangzhong;The root cause is that in 1.14, the cast rule allow to cast from 'int' to 'int nullable'. However in 1.15, the new built-in cast rule has more strict restrictions on this situation. The difference about code can be found as follows.

1.14: PlannerTypeUtils

 
{code:java}
/**
 * Can the two types operate with each other. Such as: 1.CodeGen: equal, cast, assignment.
 * 2.Join keys.
 */
public static boolean isInteroperable(LogicalType t1, LogicalType t2) {
    // ......

    if (t1.getTypeRoot() != t2.getTypeRoot()) {
        return false;
    }

    switch (t1.getTypeRoot()) {
        case ARRAY:
        case MAP:
        case MULTISET:
        case ROW:
            List<LogicalType> children1 = t1.getChildren();
            List<LogicalType> children2 = t2.getChildren();
            if (children1.size() != children2.size()) {
                return false;
            }
            for (int i = 0; i < children1.size(); i++) {
                if (!isInteroperable(children1.get(i), children2.get(i))) {
                    return false;
                }
            }
            return true;
        default:
            return t1.copy(true).equals(t2.copy(true)); // difference
    }
} {code}
 


1.15: NumericPrimitiveCastRule

 
{code:java}
private static boolean matches(LogicalType input, LogicalType target) {
    // Exclude identity casting
    if (input.is(target.getTypeRoot())) { // difference
        return false;
    }

    // Conversions between primitive numerics
    if ((input.is(INTEGER_NUMERIC) || input.is(APPROXIMATE_NUMERIC))
            && (target.is(INTEGER_NUMERIC) || target.is(APPROXIMATE_NUMERIC))) {
        return true;
    }

    // ......    
} {code}
 

 

 ;;;","21/Nov/23 06:11;xuyangzhong;Maybe we should add the logic back in IfCallGen#generate. If we agree on this, l‘ll try to fix this bug. cc [~libenchao] [~martijnvisser] ;;;","21/Nov/23 09:37;libenchao;I haven't looked into the details yet. One general thought is, {{IF}}'s operand type checker should allow operands which can derive a common type, {{INT}} and {{INT NOT NULL}} are definitely have a common type {{INT}}, so we should allow it. No matter how we transformed {{IF}} into other forms, it should not break this assumption.

Feel free to summit a PR to fix it [~xuyangzhong]. ;;;","23/Nov/23 06:22;xuyangzhong;Hi, [~libenchao]  when I try to fix it and add tests, I notice currently the CAST rule is correct. It forbids casting from not null to nullable.  The root cause is not IF but is RAND_INTEGER. The details are added in the pr. Just copy it here.

Currently RAND and RAND_INTEGER are always return type ""NOT NULL"" as built in functions in {{FlinkSqlOperatorTable}} during validation. But when codegen, the result type is always nullable in {{{}RandCallGen{}}}.

Actually, the result type of RAND and RAND_INTEGER should depend on the types of arguments. For example, If the argument is nullable, the return type should be nullable.

This bug causes the error with pattern ""IF(1 = 1, RAND(0), 0)"", because the following logic:
 # the result type of RAND is always ""not null"" when validation in {{{}FlinkSqlOperatorTable{}}}, and the third param is 0 that is always ""not null"", so the return type of operator IF is always ""not null"".
 # when codegen operator RAND, the return type of RAND(0) is always ""nullable"" in `{{{}RandCallGen{}}}`.
 # when codegen operator IF, the CAST rule cannot cast from ""nullable"" (the second arg RAND) to ""not null""(the result type), so the exception throws.

The pr is available, I'm grateful if you can help review it.;;;","24/Nov/23 05:14;libenchao;{quote}I notice currently the CAST rule is correct. It forbids casting from not null to nullable. The root cause is not IF but is RAND_INTEGER
{quote}
I don't agree on this, casting not null to nullable should be valid. The opposite should be invalid.
{quote}Actually, the result type of RAND and RAND_INTEGER should depend on the types of arguments.
{quote}
In this case, the argument is literal which is not null, so the result type is not null is ok. And I don't see many usages to pass in a nullable field as argument.

{quote}when codegen operator RAND, the return type of RAND(0) is always ""nullable"" in `RandCallGen`.
{quote}
Isn't this is the root cause?;;;","24/Nov/23 05:44;xuyangzhong;Hi, [~libenchao] , I checked the code again and sorry for this mistake. It forbids CAST from nullable to not null in `LogicalTypeCasts#defaultMethod` and I think this is reasonable.

!image-2023-11-24-13-31-21-209.png|width=489,height=325!
{quote}In this case, the argument is literal which is not null, so the result type is not null is ok. And I don't see many usages to pass in a nullable field as argument.
{quote}
For example, we have a table with column ""a int not null"" and ""b int"". I think the result type with RAND(a, 10) and RAND(b, 10) is different. The former is int not null and the latter is int. This difference will and should also affect the result type about its output. 
{quote} Isn't this is the root cause?
{quote}
The inconsistence about the behavior for result type during validation and code generation in RAND and RAND_INTEGER causes this bug.

 

Look for your thoughts : );;;","24/Nov/23 06:05;libenchao;There is no such usage: ""RAND(a, 10) and RAND(b, 10) "".

The usages for RAND is ""RAND() and RAND(seed)"" ;;;","24/Nov/23 08:58;xuyangzhong;You're right, sorry about typo for it. What I want to express is that a column can be used as seed and bound in RAND_INTEGER or seed in RAND, and it may be nullable or not null.;;;","24/Nov/23 09:37;libenchao;It's clear now~;;;","15/Dec/23 15:24;libenchao;Fixed via:
master(1.19.0): 45f966e8c3c5e903b3843391874f7d2478122d8c
1.18.1: d60818150005661006a71e4155fc605d7543362b
1.17.3: 58f8162613d9f615e60fb0c9e23692d25469d6f0

Thanks [~xuyangzhong] for the fix and thanks [~dianer17] for reporting the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test CI on arm,FLINK-33540,13557843,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,14/Nov/23 08:34,14/Nov/23 08:34,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-14 08:34:56.0,,,,,,,,,,"0|z1ll40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential regression (Nov. 7th): listAddAll on RocksDB with Java8,FLINK-33539,13557817,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,14/Nov/23 03:27,14/Nov/23 03:47,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,Runtime / State Backends,,,,0,,,,"!image-2023-11-14-10-56-36-189.png|width=981,height=228!

There is a drop from commit fccf26bb449 to fa8ea3f9a44 .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/23 02:56;zakelly;image-2023-11-14-10-56-36-189.png;https://issues.apache.org/jira/secure/attachment/13064377/image-2023-11-14-10-56-36-189.png","14/Nov/23 03:47;zakelly;image-2023-11-14-11-47-52-499.png;https://issues.apache.org/jira/secure/attachment/13064378/image-2023-11-14-11-47-52-499.png",,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 14 03:47:59 UTC 2023,,,,,,,,,,"0|z1lky8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 03:45;zakelly;Seems introduced by FLINK-5865 (cee2611e). Manually triggered benchmark:

Before this commit:
{code:java}
11:11:13  Benchmark                          (backendType)   Mode  Cnt    Score   Error   Units
11:11:13  ListStateBenchmark.listAddAll               HEAP  thrpt   30  820.639 ± 6.355  ops/ms
11:11:13  ListStateBenchmark.listAddAll            ROCKSDB  thrpt   30  290.898 ± 4.836  ops/ms
11:11:13  ListStateBenchmark.listAddAll  ROCKSDB_CHANGELOG  thrpt   30  170.711 ± 9.054  ops/ms {code}
 After this commit:
{code:java}
11:31:29  Benchmark                          (backendType)   Mode  Cnt    Score    Error   Units
11:31:29  ListStateBenchmark.listAddAll               HEAP  thrpt   30  817.607 ±  6.156  ops/ms
11:31:29  ListStateBenchmark.listAddAll            ROCKSDB  thrpt   30  276.837 ±  3.548  ops/ms
11:31:29  ListStateBenchmark.listAddAll  ROCKSDB_CHANGELOG  thrpt   30  170.888 ± 10.792  ops/ms {code}
 

What's strange is, there's no regression on other state backends, as well as with higher java version. Only RocksDB with Java8 is affected.;;;","14/Nov/23 03:47;zakelly;!image-2023-11-14-11-47-52-499.png|width=1383,height=304!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Create a ""best match"" option for CEP when using the optional operator in the Pattern API ",FLINK-33538,13557765,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wblehma,wblehma,13/Nov/23 18:58,13/Nov/23 18:58,04/Jun/24 20:40,,,,,,,,,,,,,,Library / CEP,,,,0,,,,"I’ll try to detail my issue and the initial experimentation I’ve done in the CEP source code to show the potential of making this nearly as fast as having no optional events in the pattern.

 

Using Flink CEP I have implemented a pattern that detects a sequence of events. Each event has internal attributes that are checked and inter-event time deltas are also checked as part of the pattern.

 

When I require all events in my sequence (say 10 elements long) Flink CEP works well and is super fast at detecting the “perfect” sequences. Should one or more of the events not get recorded, I would still like to detect the partial sequence. Since I don’t know which events might be missing I must make all of the events optional. While this worked, it was significantly slower, to the point that it is an unworkable solution.

 

CEP is set to alert as fast as possible – so when everything is optional, once it passes a single event the path to final in ComputationState is immediately found and the potentialMatch is emitted. If a perfect match is coming, I will not find the pattern unless the skip strategy is set to noSkip(). Because all are optional, in the instance of a perfect match, CEP wants to emit all 2^10 possible matches when noSkip() is used. This is what causes the extreme performance drop off.

 

I would like to add an option to Flink CEP that allows the “best match” when the optional() attribute is used in the Pattern API. The default can still be “fastest match” which would operate as it does today. In the “best match” scenario, the potentialMatches would be held back (not emitted) until the full pattern time has passed.

 

I have made some modifications to the source code (primarily org.apache.flink.cep.nfa.NFA) that show this can work and be nearly as fast as the perfect match option. My basic strategy is the following:
 * Once a longer match is found, discard all of the shorter length sub-matches from partialMatches and potentialMatches.
 * Eg –  [E1, $endState$]
 * [E1, E2, $endState$] - discard the [E1, $endState$] and [E2, $endState$] and those partialMatch paths as well
 * [E1, E2, E3, $endState$] – discard the [E1, E2, $endState$], etc.


 * DeweyNumber allows a way to quickly assess match lengths, by comparing DeweyNumber.lengths.
 * DeweyNumber provides a way to assess matches, with its versioning – matches tend to use a version of zero (eg. 1.0.0.0.0.0 is a perfect match to the first 6 events of the pattern)
 * Delay emitting the “best match” until the options are exhausted, eliminating the shorter matches as quickly as possible.

 

I’m at a place in my modifications where it would be very beneficial to work with an expert that would understand how best to accomplish this without impacting current functionality/performance. I’m happy to collaborate and share my work if that helps. My current modifications are a combination of mods and logs to allow me to see what is going on internally in the CEP processing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-13 18:58:05.0,,,,,,,,,,"0|z1lkmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs version nav is missing 1.18 as an option,FLINK-33537,13557758,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,rmoff,rmoff,13/Nov/23 17:52,17/Nov/23 01:57,04/Jun/24 20:40,16/Nov/23 16:16,1.19.0,,,,,,,,1.19.0,,,,Documentation,,,,0,pull-request-available,,,The docs site is missing 1.18 from the version navigation in the bottom-left  !image-2023-11-13-17-51-58-830.png! ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/23 17:51;rmoff;image-2023-11-13-17-51-58-830.png;https://issues.apache.org/jira/secure/attachment/13064375/image-2023-11-13-17-51-58-830.png",,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 16:15:54 UTC 2023,,,,,,,,,,"0|z1lkl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 06:30;tanyuxin;[~rmoff] Thanks for reporting this issue. I will take a look.;;;","16/Nov/23 16:15;Sergey Nuyanzin;Merged as [09c13449b67fe30a658f56b7731c70dea6b94848|https://github.com/apache/flink/commit/09c13449b67fe30a658f56b7731c70dea6b94848];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink Table API CSV streaming sink fails with ""IOException: Stream closed""",FLINK-33536,13557740,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,prabhujoseph,prabhujoseph,13/Nov/23 15:42,04/Jan/24 17:34,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,FileSystems,Table SQL / API,,,0,pull-request-available,,,"Flink Table API CSV streaming sink fails with ""IOException: Stream closed"". Prior to Flink 1.18, CSV streaming sink used to fail with ""S3RecoverableFsDataOutputStream cannot sync state to S3"" which is fixed by [FLINK-28513|https://issues.apache.org/jira/browse/FLINK-28513]. The fix seems not complete, it fails with this issue now.

*Repro*

{code}
SET 'execution.runtime-mode' = 'streaming';

create table dummy_table (
  id int,
  data string
) with (
  'connector' = 'filesystem',
  'path' = 's3://prabhuflinks3/dummy_table/',
  'format' = 'csv'
);

INSERT INTO dummy_table SELECT * FROM (VALUES (1, 'Hello World'), (2, 'Hi'), (2, 'Hi'), (3, 'Hello'), (3, 'World'), (4, 'ADD'), (5, 'LINE'));

{code}


*Error*

{code}
Caused by: java.io.IOException: Stream closed.
	at org.apache.flink.core.fs.RefCountedFileWithStream.requireOpened(RefCountedFileWithStream.java:76)
	at org.apache.flink.core.fs.RefCountedFileWithStream.write(RefCountedFileWithStream.java:52)
	at org.apache.flink.core.fs.RefCountedBufferingFileStream.flush(RefCountedBufferingFileStream.java:104)
	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeAndUploadPart(S3RecoverableFsDataOutputStream.java:209)
	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeForCommit(S3RecoverableFsDataOutputStream.java:177)
	at org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileWriter.closeForCommit(OutputStreamBasedPartFileWriter.java:75)
	at org.apache.flink.streaming.api.functions.sink.filesystem.BulkPartWriter.closeForCommit(BulkPartWriter.java:65)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.closePartFile(Bucket.java:263)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.onProcessingTime(Bucket.java:379)
	at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.onProcessingTime(Buckets.java:338)
	at org.apache.flink.connector.file.table.stream.AbstractStreamingWriter.endInput(AbstractStreamingWriter.java:155)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:96)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.lambda$finish$0(StreamOperatorWrapper.java:149)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.finish(StreamOperatorWrapper.java:149)
	at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.finish(StreamOperatorWrapper.java:156)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.finishOperators(RegularOperatorChain.java:115)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.endData(StreamTask.java:619)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.lambda$completeProcessing$0(SourceStreamTask.java:367)
	at org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:126)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.lang.Thread.run(Thread.java:750)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28513,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 15 11:16:30 UTC 2023,,,,,,,,,,"0|z1lkh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 16:01;martijnvisser;Which S3 plugin was used here?;;;","13/Nov/23 16:45;prabhujoseph;We are using flink-s3-fs-hadoop-1.18.0.jar.;;;","13/Nov/23 19:31;samrat007;Hello [~prabhujoseph]  and [~martijnvisser] ,

I was able to reproduced the issue and have been investigating FLINK-28513. I'm currently uncertain about what might have been overlooked during the initial work. I'll proceed with debugging to identify the root cause and will share the findings along with any necessary fixes.

 ;;;","14/Nov/23 00:28;prabhujoseph;Thanks [~samrat007].;;;","15/Nov/23 11:16;samrat007;[~prabhujoseph] , [~martijnvisser]  
I have tested the changes and raised a PR. Please review the changes whenever time ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support autoscaler for session jobs,FLINK-33535,13557696,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,13/Nov/23 10:30,08/Jan/24 06:18,04/Jun/24 20:40,08/Jan/24 06:18,,,,,,,,,,,,,Autoscaler,Kubernetes Operator,,,1,pull-request-available,,,"Currently the operator autoscaler is not enabled for session jobs. There are some issues around the actual scaling of the jobs, such as: 

https://issues.apache.org/jira/browse/FLINK-33534

Furthermore there is a bug which in any case prevents the submission of these jobs due to how the jobid is generated, causing collisions if we only change the parallelism override without the metadata generation.

We could consider still enabling the autoscaler for a limited 1.18 rescale api support",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 08 06:18:45 UTC 2024,,,,,,,,,,"0|z1lk7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jan/24 06:18;gyfora;merged to main 66e143362174491cef4b1d251b3fa21058fcf1c1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PipelineOptions.PARALLELISM_OVERRIDES is not picked up from jar submission request,FLINK-33534,13557657,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,gyfora,gyfora,13/Nov/23 05:38,20/Dec/23 03:55,04/Jun/24 20:40,19/Dec/23 01:24,1.17.1,1.18.0,,,,,,,1.19.0,,,,Runtime / Configuration,Runtime / REST,,,0,pull-request-available,,,"PARALLELISM_OVERRIDES are currently only applied when they are part of the JobManager / Cluster configuration.

When this config is provided as part of the JarRunRequestBody it is completely ignored and does not take effect. 

The main reason is that the dispatcher reads this value from it's own configuration object and does not include the extra configs passed through the rest request.

This is a blocker for supporting the autoscaler properly for FlinkSessionJobs in the autoscaler",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 19 01:24:36 UTC 2023,,,,,,,,,,"0|z1ljyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 05:39;gyfora;cc [~mxm] [~fanrui] ;;;","28/Nov/23 08:20;yunfengzhou;Hi [~gyfora] [~mxm] [~fanrui] , I'm interested in this ticket and am willing to fix this problem. Could you please assign it to me?

I would also like to learn if there are existing programs that could be used to reproduce this problem, which could help me know your concerns and needs in detail.

Besides, the Flink CLI's document claims that we can specify configurations via `-D` arguments([link|https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/cli/#submitting-a-job]). For example,

 
{code:java}
$ ./bin/flink run \
      --detached \
      --Dpipeline.max-parallelism=120 \
       ./examples/streaming/StateMachineExample.jar{code}
Could this method be a workaround to this problem?

 ;;;","02/Dec/23 09:23;gyfora;I assigned it to you! I haven’t tried to repro this outside the operator ;;;","19/Dec/23 01:24;Weijie Guo;master(1.19) via 4cc24c1dd17b0abe3c4372652c7ab88fedc7e478.
release-1.18 via 6aa64ebb88045abef8b900b1ff3e15b171da5709.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation about performance for huge ProtoBuf definations,FLINK-33533,13557653,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,libenchao,libenchao,13/Nov/23 03:59,13/Nov/23 05:15,04/Jun/24 20:40,,,,,,,,,,,,,,Documentation,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,0,,,,"This is brought up in the discussion of FLINK-32650. For huge ProtoBuf definations, the generated java code would hive JIT C2 optimization threshold. To mitigate this problem temporarily, users can add {{-XX:-DontCompileHugeMethods}} to JVM args manually.

See related issues in protobuf-java project[1][2]

[1] https://github.com/protocolbuffers/protobuf/pull/10367
[2] https://github.com/protocolbuffers/protobuf/issues/10247",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32650,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-13 03:59:49.0,,,,,,,,,,"0|z1ljxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move the serialization of ShuffleDescriptorGroup out of the RPC main thread,FLINK-33532,13557650,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsdy,guoyangze,guoyangze,13/Nov/23 02:26,06/Mar/24 13:34,04/Jun/24 20:40,16/Nov/23 07:54,,,,,,,,,1.19.0,,,,Runtime / Network,,,,0,pull-request-available,,,"Currently, the serilization of ShuffleDescriptorGroup would happen in the RPC main thread even for light jobs. That would harm the QPS of OLAP session, e.g. the deserialization will take 3ms at TM side for each task of a 128*128 wordcount job. We propose to move it out of the rpc main thread. For heavy jobs, it still worth to cache the serialized value for memory efficiency. At the same time we need to make the serialization thread pool configurable.",,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34105,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 07:54:48 UTC 2023,,,,,,,,,,"0|z1ljx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Nov/23 07:54;guoyangze;master: 8ef71ba1f190a5cf6093adb6c40c6a6d04944d20

d18a4bfe596fc580f8280750fa3bfa22007671d9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nightly Python fails with NPE at metadataHandlerProvider on AZP (StreamDependencyTests.test_add_python_archive),FLINK-33531,13557647,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hxb,Sergey Nuyanzin,Sergey Nuyanzin,13/Nov/23 00:54,20/Dec/23 17:54,04/Jun/24 20:40,20/Nov/23 11:23,1.18.0,1.19.0,,,,,,,1.18.1,1.19.0,,,API / Python,,,,0,pull-request-available,test-stability,,"It seems starting 02.11.2023 every master nightly fails with this (that's why it is a blocker)

for instance
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54512&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901]
{noformat}
2023-11-12T02:10:24.5082784Z Nov 12 02:10:24         if is_error(answer)[0]:
2023-11-12T02:10:24.5083620Z Nov 12 02:10:24             if len(answer) > 1:
2023-11-12T02:10:24.5084326Z Nov 12 02:10:24                 type = answer[1]
2023-11-12T02:10:24.5085164Z Nov 12 02:10:24                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2023-11-12T02:10:24.5086061Z Nov 12 02:10:24                 if answer[1] == REFERENCE_TYPE:
2023-11-12T02:10:24.5086850Z Nov 12 02:10:24 >                   raise Py4JJavaError(
2023-11-12T02:10:24.5087677Z Nov 12 02:10:24                         ""An error occurred while calling {0}{1}{2}.\n"".
2023-11-12T02:10:24.5088538Z Nov 12 02:10:24                         format(target_id, ""."", name), value)
2023-11-12T02:10:24.5089551Z Nov 12 02:10:24 E                   py4j.protocol.Py4JJavaError: An error occurred while calling o3371.executeInsert.
2023-11-12T02:10:24.5090832Z Nov 12 02:10:24 E                   : java.lang.NullPointerException: metadataHandlerProvider
2023-11-12T02:10:24.5091832Z Nov 12 02:10:24 E                   	at java.util.Objects.requireNonNull(Objects.java:228)
2023-11-12T02:10:24.5093399Z Nov 12 02:10:24 E                   	at org.apache.calcite.rel.metadata.RelMetadataQueryBase.getMetadataHandlerProvider(RelMetadataQueryBase.java:122)
2023-11-12T02:10:24.5094480Z Nov 12 02:10:24 E                   	at org.apache.calcite.rel.metadata.RelMetadataQueryBase.revise(RelMetadataQueryBase.java:118)
2023-11-12T02:10:24.5095365Z Nov 12 02:10:24 E                   	at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:844)
2023-11-12T02:10:24.5096306Z Nov 12 02:10:24 E                   	at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:307)
2023-11-12T02:10:24.5097238Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:337)
2023-11-12T02:10:24.5098014Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:556)
2023-11-12T02:10:24.5098753Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:420)
2023-11-12T02:10:24.5099517Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.executeRuleInstance(HepPlanner.java:243)
2023-11-12T02:10:24.5100373Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance$State.execute(HepInstruction.java:178)
2023-11-12T02:10:24.5101313Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.lambda$executeProgram$0(HepPlanner.java:211)
2023-11-12T02:10:24.5102410Z Nov 12 02:10:24 E                   	at org.apache.flink.calcite.shaded.com.google.common.collect.ImmutableList.forEach(ImmutableList.java:422)
2023-11-12T02:10:24.5103343Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:210)
2023-11-12T02:10:24.5104105Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepProgram$State.execute(HepProgram.java:118)
2023-11-12T02:10:24.5104868Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:205)
2023-11-12T02:10:24.5105616Z Nov 12 02:10:24 E                   	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:191)
2023-11-12T02:10:24.5106421Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:64)
2023-11-12T02:10:24.5107359Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:78)
2023-11-12T02:10:24.5108346Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
2023-11-12T02:10:24.5109407Z Nov 12 02:10:24 E                   	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156)
2023-11-12T02:10:24.5110241Z Nov 12 02:10:24 E                   	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156)
2023-11-12T02:10:24.5111078Z Nov 12 02:10:24 E                   	at scala.collection.Iterator.foreach(Iterator.scala:937)
2023-11-12T02:10:24.5111734Z Nov 12 02:10:24 E                   	at scala.collection.Iterator.foreach$(Iterator.scala:937)
2023-11-12T02:10:24.5112410Z Nov 12 02:10:24 E                   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
2023-11-12T02:10:24.5113145Z Nov 12 02:10:24 E                   	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
2023-11-12T02:10:24.5113840Z Nov 12 02:10:24 E                   	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
2023-11-12T02:10:24.5114528Z Nov 12 02:10:24 E                   	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2023-11-12T02:10:24.5115232Z Nov 12 02:10:24 E                   	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156)
2023-11-12T02:10:24.5116054Z Nov 12 02:10:24 E                   	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154)
2023-11-12T02:10:24.5116775Z Nov 12 02:10:24 E                   	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
2023-11-12T02:10:24.5117583Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
2023-11-12T02:10:24.5118581Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
2023-11-12T02:10:24.5119603Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
2023-11-12T02:10:24.5120689Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
2023-11-12T02:10:24.5121602Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:324)
2023-11-12T02:10:24.5122403Z Nov 12 02:10:24 E                   	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
2023-11-12T02:10:24.5123301Z Nov 12 02:10:24 E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277)
2023-11-12T02:10:24.5124195Z Nov 12 02:10:24 E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862)
2023-11-12T02:10:24.5125097Z Nov 12 02:10:24 E                   	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1097)
2023-11-12T02:10:24.5125945Z Nov 12 02:10:24 E                   	at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:59)
2023-11-12T02:10:24.5126720Z Nov 12 02:10:24 E                   	at org.apache.flink.table.api.Table.executeInsert(Table.java:1074){noformat}
UPD: 
It looks like it is a very similar to what has already been reported at FLINK-32424 and FLINK-24241
However something made it happening every nightly",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 17:54:35 UTC 2023,,,,,,,,,,"0|z1ljwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 14:57;gaborgsomogyi;Since I've added python 3.11 lately I've double checked my part.
This has been added on 15th of Oct:
{code:java}
commit 2da9a9639216b8c48850ee714065f090a80dcd65
Author: Gabor Somogyi <gabor_somogyi2@apple.com>
Date:   Sun Oct 15 09:31:08 2023 +0200

    [FLINK-33030][python] Add python 3.11 support
    
    Also bump grpcio-tools version
...
{code}
Seems like the latest green nightly happened 30th of Oct so that's not the cause:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54166&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602]

I've gone back time starting from the mentioned 30th of Oct, double checked the master nightlies and seems like it was stable. No idea what happened but after that it became unstable.;;;","14/Nov/23 09:16;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54289&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=25494
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54289&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=25333
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54289&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=25657
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54289&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=25941
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54289&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3&l=25559;;;","14/Nov/23 09:56;lincoln.86xy;[~dianfu] it would be appreciated if you have time to help take a look at this, it blocks the daily build.;;;","16/Nov/23 08:34;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54602&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602;;;","16/Nov/23 08:53;hxb;I will take a look. This case only fails on Python 3.9, which is very strange, because the stack does not seem to have much to do with this case or the Python environment. And other branches are still normal.  I will try to confirm which commit caused it firstly.;;;","16/Nov/23 12:25;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54534&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54566&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54512&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54486&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54446&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54422&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54348&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54339&view=results;;;","16/Nov/23 15:24;lincoln.86xy;[~hxb] thank you for looking into this!;;;","17/Nov/23 12:04;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54646&view=results;;;","20/Nov/23 06:24;hxb;After doing some experiments, I came to the following conclusions:
1. In Python 3.9 + Cython 0.29.36 environment, the `test_denpendency.py` test will fail stably in my private Azure pipeline. Although I don't think Python and Cython versions have anything to do with this test failure.
2. Change the Python or Cython version of this test and the failure case will no longer appear.
3. This problem cannot be reproduced locally using the same versions of all packages such as Python and Cython.
4. After reverting the commit that may cause the problem, this case will still fail in Azure.（I didn't revert all the commits because I don't think these are the root causes.）

My preferred solution right now is to upgrade Cython to address testing issues caused by the Azure environment.;;;","20/Nov/23 06:38;Sergey Nuyanzin;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a];;;","20/Nov/23 07:38;Sergey Nuyanzin;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54697&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a];;;","20/Nov/23 07:42;Sergey Nuyanzin;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3]
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a];;;","20/Nov/23 11:23;hxb;Merged into master via 942d234444636b13df7a118274d307943f5524cd;;;","16/Dec/23 13:50;Sergey Nuyanzin;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55583&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=27725]

reproduced for 1.18

probably need to have a backport;;;","18/Dec/23 16:28;jingge;let's add it into 1.18.1 release;;;","18/Dec/23 22:15;Sergey Nuyanzin;Merged to 1.18.1 as
[0d6ab1db6c04cd88f646d545075bea539bac9fcf|https://github.com/apache/flink/commit/0d6ab1db6c04cd88f646d545075bea539bac9fcf];;;","20/Dec/23 17:54;mapohl;It looks like this fix is not good enough? The following [1.18 nightly run|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55653&view=logs&j=b53e1644-5cb4-5a3b-5d48-f523f39bcf06&t=b68c9f5c-04c9-5c75-3862-a3a27aabbce3&l=25428] included the fixed [0d6ab1db|https://github.com/apache/flink/commit/0d6ab1db6c04cd88f646d545075bea539bac9fcf] mentioned above but failed with the same error if I'm not mistaken?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add entropy to Google Cloud Storage path for better scalability,FLINK-33530,13557641,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,talat,talat,12/Nov/23 22:55,12/Nov/23 22:55,04/Jun/24 20:40,,1.15.4,1.17.1,1.19.0,,,,,,,,,,,,,,0,,,,We are using GCS. To better scalability we need entropy support like as S3 FS Plugin.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-12 22:55:46.0,,,,,,,,,,"0|z1ljv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PyFlink fails with ""No module named 'cloudpickle""",FLINK-33529,13557619,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,prabhujoseph,prabhujoseph,prabhujoseph,12/Nov/23 17:29,14/Nov/23 07:50,04/Jun/24 20:40,14/Nov/23 02:54,1.17.2,1.18.0,,,,,,,1.17.3,1.18.1,1.19.0,,API / Python,,,,0,pull-request-available,,,"PyFlink fails with ""No module named 'cloudpickle"" on Flink 1.18. The same program works fine on Flink 1.17. This is after the change (https://issues.apache.org/jira/browse/FLINK-32034).

*Repro:*

{code}
[hadoop@ip-1-2-3-4 ~]$ python --version
Python 3.7.16

[hadoop@ip-1-2-3-4 ~]$ rpm -qa | grep flink
flink-1.18.0-1.amzn2.x86_64

[hadoop@ip-1-2-3-4 ~]$ flink-yarn-session -d

[hadoop@ip-1-2-3-4 ~]$ flink run -py /tmp/batch_wc.py --output s3://prabhuflinks3/OUT2/
{code}


*Error*

{code}
ModuleNotFoundError: No module named 'cloudpickle'

	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.createStageBundleFactory(BeamPythonFunctionRunner.java:656)
	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:281)
	at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.open(AbstractExternalPythonFunctionOperator.java:57)
	at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:92)
	at org.apache.flink.table.runtime.operators.python.table.PythonTableFunctionOperator.open(PythonTableFunctionOperator.java:114)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:753)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:728)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:693)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:922)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
{code}

*Analysis*

1. On Flink 1.17 and Python-3.7.16, PythonEnvironmentManagerUtils#getSitePackagesPath used to return following two paths

{code}
[root@ip-172-31-45-97 tmp]# python flink1.17-get_site_packages.py /tmp
/tmp/lib/python3.7/site-packages
/tmp/lib64/python3.7/site-packages
{code}

whereas Flink 1.18 (FLINK-32034) has changed the PythonEnvironmentManagerUtils#getSitePackagesPath and only one path is returned

{code}
[root@ip-172-31-45-97 tmp]# python flink1.18-get_site_packages.py /tmp
/tmp/lib64/python3.7/site-packages
[root@ip-172-31-45-97 tmp]#
{code}

The pyflink dependencies are installed in ""/tmp/lib/python3.7/site-packages"" which is not returned by the getSitePackagesPath in Flink1.18 causing the pyflink job failure.

*Attached batch_wc.py, flink1.17-get_site_packages.py and flink1.18-get_site_packages.py.*
","Python 3.7.16 or Python 3.9
YARN",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/23 17:31;prabhujoseph;batch_wc.py;https://issues.apache.org/jira/secure/attachment/13064352/batch_wc.py","12/Nov/23 17:31;prabhujoseph;flink1.17-get_site_packages.py;https://issues.apache.org/jira/secure/attachment/13064350/flink1.17-get_site_packages.py","12/Nov/23 17:31;prabhujoseph;flink1.18-get_site_packages.py;https://issues.apache.org/jira/secure/attachment/13064351/flink1.18-get_site_packages.py",,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 14 02:53:32 UTC 2023,,,,,,,,,,"0|z1ljq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 02:53;dianfu;Fixed in:
 * master via 34d9594ab1e5412371b77912f120b7949c92dcdd
 * release-1.18 via 2844ea232accece60158b455079920fb2d78f448
 * release-1.17 via 01b1ce4f349315e1942b3290c0fa81ab0a6b183e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Python connector code,FLINK-33528,13557618,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,pvary,mbalassi,mbalassi,12/Nov/23 16:27,11/Mar/24 12:44,04/Jun/24 20:40,,1.18.0,,,,,,,,1.20.0,,,,API / Python,Connectors / Common,,,0,,,,"During the connector externalization effort end to end tests for the python connectors were left in the main repository under:

[https://github.com/apache/flink/tree/master/flink-python/pyflink/datastream/connectors]

These include both python connector implementation and tests. Currently they depend on a previously released version of the underlying connectors, otherwise they would introduce a circular dependency given that they are in the flink repo at the moment.

This setup prevents us from propagating any breaking change to PublicEvolving and Internal APIs used by the connectors as they lead to breaking the python e2e tests. We run into this while implementing FLINK-25857.

Note that we made the decision to turn off the Python test when merging FLINK-25857, so now we are forced to fix this until 1.19 such that we can reenable the test runs - now in the externalized connector repos.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 21 09:17:16 UTC 2023,,,,,,,,,,"0|z1ljq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 10:31;martijnvisser;[~dianfu] [~hxbks2ks] Can you share your point of view on this, given your PyFlink experience?;;;","13/Nov/23 11:10;dianfu;I think it's fair enough to externalize Python connectors.  Previously we have also found that the Java APIs of some connectors which reside in external repositories were changed in an incompatible way which breaks the Python connectors API which is located in Flink repository.;;;","13/Nov/23 12:13;martijnvisser;So externalizing the Python connectors would also help protect (given that we could test against them in the CI) against those type of incompatible changes, right?;;;","14/Nov/23 01:40;dianfu;Yes, I think so.;;;","14/Nov/23 16:02;mbalassi;[~pvary] is taking a stab at an example implementation for Kafka. We will report back to see whether it is viable or if we need assistance.;;;","16/Nov/23 03:30;dianfu;Thanks very much for the efforts (y);;;","21/Dec/23 09:06;Sergey Nuyanzin;[~pvary], [~dianfu] could you please clarify: once externalization of python code for some connector happened could we safely remove the related connector's python code from flink main repo?;;;","21/Dec/23 09:14;pvary;[~Sergey Nuyanzin]: FLINK-33762 is needed, so the connectors could release their on Python package. I would remove the code from the Flink code only after the connector packages are released.;;;","21/Dec/23 09:17;Sergey Nuyanzin;ok I see, thanks for clarification;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear all physical states after autoscaler is disabled,FLINK-33527,13557612,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,12/Nov/23 14:03,15/Nov/23 14:55,04/Jun/24 20:40,13/Nov/23 03:45,,,,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,,,,0,pull-request-available,,,"Currently, we just clear ParallelismOverrides  after autoscaler is disabled.

We should clear CollectedMetrics and ScalingHistory  as well to prevent state leak.
 * CollectedMetrics can be cleared directly.
 * ScalingHistory can be cleared based on trim logic( {color:#9876aa}VERTEX_SCALING_HISTORY_AGE){color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 15 14:17:03 UTC 2023,,,,,,,,,,"0|z1ljoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 03:45;gyfora;merged to main bf5bb94bc87295e5ecb8ace8f579a22dca440e2d;;;","15/Nov/23 14:17;gyfora;merged to main 02840b96ef3116ea95a440af4f945398900d89df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve default autoscaler configs,FLINK-33526,13557608,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,12/Nov/23 11:52,13/Nov/23 03:15,04/Jun/24 20:40,13/Nov/23 03:15,,,,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"There are a few config defaults that should be improved based on prod usage:
 * Metric window : 10 -> 15m
 * Catch up duration: 15 -> 30m
 * Restart time: 3 -> 5m
 * Utilisation boundary: 0.4 -> 0.3

These configs help make the default autoscaler behaviour smoother and less aggressive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 03:15:37 UTC 2023,,,,,,,,,,"0|z1ljns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 03:15;fanrui;Merged to master<1.7.0> via : cbcc6b67c98ddfad8bd6141edfd1a6e8c2ff00f5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate LoadSimulationPipeline in Flink K8S Operator to the new Source API,FLINK-33525,13557573,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,afedulov,afedulov,11/Nov/23 12:21,11/Nov/23 12:21,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,https://github.com/apache/flink-kubernetes-operator/blob/main/examples/autoscaling/src/main/java/autoscaling/LoadSimulationPipeline.java#L100C51-L100C65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-11 12:21:18.0,,,,,,,,,,"0|z1ljg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalJoinOperator 's judgment on late data has bug,FLINK-33524,13557555,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,zhangtao9876,zhangtao9876,11/Nov/23 03:50,11/Nov/23 07:18,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,API / DataStream,,,,0,,,,"package:
org.apache.flink.streaming.api.operators.co;
 
class: IntervalJoinOperator
 
method:
isLate
 
When data with a 1-millisecond delay enters the judgment, an incorrect value will be returned
{code:java}
private boolean isLate(long timestamp) {
    long currentWatermark = internalTimerService.currentWatermark();
    return timestamp < currentWatermark;
} {code}","Due to the Watermark calculation method :
{code:java}
public void onPeriodicEmit(WatermarkOutput output) {
            output.emitWatermark(new Watermark(maxTs - delayTime - 1L));
        }{code}
 data that was delayed by 1 millisecond in this method was incorrectly determined
{code:java}
private boolean isLate(long timestamp) {
    long currentWatermark = internalTimerService.currentWatermark();
    return timestamp < currentWatermark;
} {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,2023-11-11 03:50:58.0,,,,,,,,,,"0|z1ljc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataType ARRAY<INT NOT NULL> fails to cast into Object[],FLINK-33523,13557554,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,prabhujoseph,prabhujoseph,11/Nov/23 03:45,01/Feb/24 09:41,04/Jun/24 20:40,08/Dec/23 08:18,1.18.0,,,,,,,,1.18.1,,,,Table SQL / API,,,,0,pull-request-available,,,"When upgrading Iceberg's Flink version to 1.18, we found the Flink-related unit test case broken due to this issue. The below code used to work fine in Flink 1.17 but failed after upgrading to 1.18. DataType ARRAY<INT NOT NULL> fails to cast into Object[].

*Error:*

{code}
Exception in thread ""main"" java.lang.ClassCastException: [I cannot be cast to [Ljava.lang.Object;
at FlinkArrayIntNotNullTest.main(FlinkArrayIntNotNullTest.java:18)
{code}

*Repro:*

{code}

  import org.apache.flink.table.data.ArrayData;
  import org.apache.flink.table.data.GenericArrayData;
  import org.apache.flink.table.api.EnvironmentSettings;
  import org.apache.flink.table.api.TableEnvironment;
  import org.apache.flink.table.api.TableResult;

  public class FlinkArrayIntNotNullTest {

    public static void main(String[] args) throws Exception {

      EnvironmentSettings settings = EnvironmentSettings.newInstance().inBatchMode().build();
      TableEnvironment env = TableEnvironment.create(settings);

      env.executeSql(""CREATE TABLE filesystemtable2 (id INT, data ARRAY<INT NOT NULL>) WITH ('connector' = 'filesystem', 'path' = '/tmp/FLINK/filesystemtable2', 'format'='json')"");
      env.executeSql(""INSERT INTO filesystemtable2 VALUES (4,ARRAY [1,2,3])"");
      TableResult tableResult = env.executeSql(""SELECT * from filesystemtable2"");

      ArrayData actualArrayData = new GenericArrayData((Object[]) tableResult.collect().next().getField(1));
    }
  }

{code}

*Analysis:*

1. The code works fine with ARRAY<INT> datatype. The issue happens when using ARRAY<INT NOT NULL>.
2. The code works fine when casting into int[] instead of Object[].

",,,,,,,,,,,,,,,,,,,,,FLINK-33547,,,,,FLINK-31835,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 08 08:41:31 UTC 2023,,,,,,,,,,"0|z1ljc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Nov/23 12:43;jeyhunkarimov;Hi [~prabhujoseph] thanks for reporting the issue. 
The idea is that the type of array (ARRAY<INT NOT NULL>) enforces its elements to be NOT NULL.
Flink maps converts this to primitive types ( and not Object types) because primitive types cannot be null. 

That is why, when you eliminate NOT NULL condition this code snippet works (Flink converts this to the Object types - they can be null).

;;;","24/Nov/23 04:08;prabhujoseph;Thanks [~jeyhun]. Flink 1.17 does not have this enforcement. Flink 1.18 [FLINK-31835|https://issues.apache.org/jira/browse/FLINK-31835] has added the enforcement and has broken multiple places like [FLINK-33547 |https://issues.apache.org/jira/browse/FLINK-33547] and [Iceberg test cases|https://github.com/apache/iceberg/issues/8930]. ;;;","07/Dec/23 10:54;pvary;Created a thread about this topic on the mailing list: https://lists.apache.org/thread/m4c879y8mb7hbn2kkjh9h3d8g1jphh3j
I would appreciate if you can share your thoughts there [~prabhujoseph], [~jeyhun], [~aitozi], [~jark], [~xccui];;;","07/Dec/23 12:46;twalthr;Thanks for reporting this issue [~prabhujoseph]. I replied already on the mailing list but I will repeat my opinion here. This is a heavily breaking changes and FLINK-31835 should be reverted. It was a well-chosen decision that a NOT NULL constraint should not have an effect on the default conversion class.;;;","08/Dec/23 08:18;martijnvisser;Fixed in apache/flink

master: fcc7bc2e5e529b135aee28d732b8c5a6769afdae
release-1.18: 674d3a746417ceb36f74ccebb6a713549af59033;;;","08/Dec/23 08:41;jingge;[~martijnvisser] thanks for driving it!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Savepoint upgrade mode fails despite the savepoint succeeding,FLINK-33522,13557544,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,10/Nov/23 23:09,01/Dec/23 09:48,04/Jun/24 20:40,15/Nov/23 12:41,kubernetes-operator-1.6.0,kubernetes-operator-1.6.1,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Under certain circumstances, savepoint creation can succeed but the job fails afterwards. One example is when there are messages being distributed by the source coordinator to finished tasks. This is possibly a Flink bug although it's not clear yet how to solve the issue.

After the savepoint succeeded Flink fails the job like this:
{noformat}
Source (1/2) (cd4d56ddb71c0e763cc400bcfe2fd8ac_4081cf0163fcce7fe6af0cf07ad2d43c_0_0) switched from RUNNING to FAILED on host-taskmanager-1-1 @ ip(dataPort=36519). 
{noformat}
{noformat}
An OperatorEvent from an OperatorCoordinator to a task was lost. Triggering task failover to ensure consistency. Event: 'AddSplitEvents[[[B@722a23fa]]', targetTask: Source (1/2) - execution #0
Caused by:
org.apache.flink.runtime.operators.coordination.TaskNotRunningException: Task is not running, but in state FINISHED
   at org.apache.flink.runtime.taskmanager.Task.deliverOperatorEvent(Task.java:1502)
   at org.apache.flink.runtime.taskexecutor.TaskExecutor.sendOperatorEventToTask
{noformat}

Inside the operator this is processed as:

{noformat}
java.util.concurrent.CompletionException: org.apache.flink.runtime.scheduler.stopwithsavepoint.StopWithSavepointStoppingException: A savepoint has been created at: s3://..., but the corresponding job 1b1a3061194c62ded6e2fe823b61b2ea failed during stopping. The savepoint is consistent, but might have uncommitted transactions. If you want to commit the transaction please restart a job from this savepoint. 

          java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) 
          java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022) 
          org.apache.flink.kubernetes.operator.service.AbstractFlinkService.cancelJob(AbstractFlinkService.java:319) 
          org.apache.flink.kubernetes.operator.service.NativeFlinkService.cancelJob(NativeFlinkService.java:121) 
          org.apache.flink.kubernetes.operator.reconciler.deployment.ApplicationReconciler.cancelJob(ApplicationReconciler.java:223) 
          org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractJobReconciler.reconcileSpecChange(AbstractJobReconciler.java:122) 
         org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:163)
          org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:136) 
          org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56) 
          io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:138) 
          io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:96) 
          org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80) 
          io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:95) 
          io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139) 
          io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119) 
          io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89) 
          io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62) 
          io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:414) 
          java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) 
          java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) 
          java.lang.Thread.run(Thread.java:829) 
{noformat}

Subsequently we get the following because HA metadata is not available anymore. It has been cleared up after the terminal job failure:

{noformat}
org.apache.flink.kubernetes.operator.exception.RecoveryFailureException"",""message"":""HA metadata not available to restore from last state. It is possible that the job has finished or terminally failed, or the configmaps have been deleted. 
{noformat}

The deployment needs to be manually restored from a savepoint.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 01 09:48:58 UTC 2023,,,,,,,,,,"0|z1lj9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 12:41;gyfora;merged to main 2ce7486c7ee7ea6cb1a4160585fb5bc696025378;;;","01/Dec/23 09:48;mxm;Additional fix required via 51a91049b5f17f8a0b21e11feceb4410a97c50c1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for PythonCalc node,FLINK-33521,13557507,13556317,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jhughes,jhughes,10/Nov/23 14:37,25/Mar/24 16:29,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-11-10 14:37:46.0,,,,,,,,,,"0|z1lj1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException when running GPUDriverTest,FLINK-33520,13557503,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,rskraba,rskraba,rskraba,10/Nov/23 14:18,13/Nov/23 15:01,04/Jun/24 20:40,13/Nov/23 15:00,1.18.0,,,,,,,,1.19.0,,,,Tests,,,,0,pull-request-available,,,"I'd been running into a mysterious error running the {{flink-external-resources}} module tests:

{code}
java.io.FileNotFoundException: The gpu discovery script does not exist in path /opt/asf/flink/src/test/resources/testing-gpu-discovery.sh.
	at org.apache.flink.externalresource.gpu.GPUDriver.<init>(GPUDriver.java:98)
	at org.apache.flink.externalresource.gpu.GPUDriverTest.testGPUDriverWithInvalidAmount(GPUDriverTest.java:64)
	at
{code}

From the command line and IntelliJ, when it seems to works, it _always_ works, and when it fails it _always_ fails. I finally took a moment to figure it out: if the {{FLINK_HOME}} environment variable is set (to a valid Flink distribution of any version), this test fails.

This is a very minor irritation, but it's pretty easy to fix.

The workaround is to launch the unit test in an environment where this environment variable is not set.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 15:00:59 UTC 2023,,,,,,,,,,"0|z1lj0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 15:00;mapohl;master: [9ed3d04aa1fe638281dc3ca0983b4aa04f247eff|https://github.com/apache/flink/commit/9ed3d04aa1fe638281dc3ca0983b4aa04f247eff];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
standalone mode could not create keytab secret,FLINK-33519,13557486,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,richard.su,richard.su,10/Nov/23 11:59,21/Nov/23 03:51,04/Jun/24 20:40,,kubernetes-operator-1.3.0,kubernetes-operator-1.3.1,kubernetes-operator-1.4.0,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,kubernetes-operator-1.6.1,,,,,,,Kubernetes Operator,,,,0,,,,"when standalone build cluster, and configuration with security.kerberos.login.* configurations.

flink-kubernetes module will modify the path of security.kerberos.login.keytab configuration to /opt/kerberos/kerberos-keytab, and then create secret for job manager. the secret data from operator pod's keytab file.

after job manager created, creating task manager process will find the keytab file from the location from security.kerberos.login.keytab configuration, then it throws a exception says keytabs file not find. 

the bug is because of the configuration modified once, and reused it when create tm. Native mode didn't exist this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 03:50:21 UTC 2023,,,,,,,,,,"0|z1liww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 03:50;richard.su;actually, this is because of flink-kubernetes lib's logic, which will modify the configuration of ""security.kerberos.login.keytab"",  which I think this should be clearly wrote in flink operator doc, to explain this error could be solved by modified the path in this ""security.kerberos.login.keytab"" configuration to ""
/opt/kerberos/kerberos-keytab"". Or just create a soft link from source dir to this dir.

 

I this add doc will be fined, there is not right place to modified the code of flink operator to solved this problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for WatermarkAssigner node,FLINK-33518,13557475,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,jackylau,jackylau,10/Nov/23 09:56,11/Jan/24 09:46,04/Jun/24 20:40,11/Jan/24 09:46,1.19.0,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 09:46:00 UTC 2024,,,,,,,,,,"0|z1liug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/24 09:46;dwysakowicz;Implemented in 23629a80c574a9f998b41e258b8e656274714c9d..c233ed2599188ba63e361b1b4525d9f322965f65;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Value node,FLINK-33517,13557474,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,jackylau,jackylau,10/Nov/23 09:55,22/Feb/24 09:45,04/Jun/24 20:40,22/Feb/24 09:45,1.19.0,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 22 09:45:13 UTC 2024,,,,,,,,,,"0|z1liu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/24 09:45;dwysakowicz;Implemented in 263e7bf690ec1a7e371447460917c02766526151..aaaea64d60d90448b07ca525f757172da1222983;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create dedicated PyFlink channel,FLINK-33516,13557473,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,10/Nov/23 09:54,13/Nov/23 12:39,04/Jun/24 20:40,13/Nov/23 12:39,,,,,,,,,,,,,Documentation,,,,0,,,,"See https://lists.apache.org/thread/ynb5drhqqbd84w4o4337qv47100cp67h

1. Create new Slack channel
2. Update website ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 12:39:40 UTC 2023,,,,,,,,,,"0|z1liu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 12:39;martijnvisser;Channel created, documentation updated;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonDriver need to stream python process output to log instead of collecting it in memory,FLINK-33515,13557455,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,10/Nov/23 08:33,13/Nov/23 10:59,04/Jun/24 20:40,13/Nov/23 10:59,1.19.0,,,,,,,,1.19.0,,,,API / Python,,,,0,pull-request-available,,,PythonDriver now collects the python process output in a Stringbuilder instead of streaming it. It can cause OOM when the python process is generating huge amount of output.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 10:59:54 UTC 2023,,,,,,,,,,"0|z1liq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 10:59;gaborgsomogyi;caa324a on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkScalaKryoInstantiator class not found in KryoSerializer,FLINK-33514,13557452,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ft20082,ft20082,10/Nov/23 08:24,27/Dec/23 11:18,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,API / Core,,,09/Nov/23 00:00,0,,,,"{code:java}
16:03:13,402 INFO  org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer [] - Kryo serializer scala extensions are not available.
java.lang.ClassNotFoundException: org.apache.flink.runtime.types.FlinkScalaKryoInstantiator
    at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_341]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_341]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355) ~[?:1.8.0_341]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_341]
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_341]
    at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_341]
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:487) ~[flink-core-1.18.0.jar:1.18.0]
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:522) ~[flink-core-1.18.0.jar:1.18.0]
    at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:394) ~[flink-core-1.18.0.jar:1.18.0]
    at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:412) ~[flink-core-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:190) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:43) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:53) ~[flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.io.network.api.serialization.NonSpanningWrapper.readInto(NonSpanningWrapper.java:337) ~[flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNonSpanningRecord(SpillingAdaptiveSpanningRecordDeserializer.java:128) ~[flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.readNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:103) ~[flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:93) ~[flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:100) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:562) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807) ~[flink-streaming-java-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953) [flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932) [flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746) [flink-runtime-1.18.0.jar:1.18.0]
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.18.0.jar:1.18.0]
    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_341] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 27 11:18:23 UTC 2023,,,,,,,,,,"0|z1lipc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 10:14;martijnvisser;This sounds like you're missing a dependency. What's in your POM?;;;","22/Dec/23 20:55;Cyberness;{quote}This sounds like you're missing a dependency. What's in your POM?
{quote}
[~martijnvisser] do you mind to tell what this dependency is? It looks to me like this class disappeared long time ago that the message itself is an artifact from that times. I couldn't find any package with that class rather than old Flink runtime:

https://github.com/apache/flink/pull/6784#pullrequestreview-167040807;;;","23/Dec/23 03:11;tbnguyen1407;This could be due to outdated Kryo lib not supported by java 17+. I encountered this issue and got it working again by downgrading java to 11.

There is an open PR to upgrade the Kryo lib to support newer JDK here:

[https://github.com/apache/flink/pull/22660];;;","27/Dec/23 11:18;martijnvisser;[~Cyberness] I think this ticket lacks information in general in what's in the POM/build and what setup has been used (Java 8, 11, 17) and if this was an upgrade, or a newly started job. I'm assuming someone upgraded from a Flink version prior to 1.14 and didn't check the 1.15 release notes https://nightlies.apache.org/flink/flink-docs-master/release-notes/flink-1.15/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
