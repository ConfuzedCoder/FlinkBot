Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Incorporates),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Metastore delegation-token can be cached?,FLINK-33513,13557451,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,katty0924,katty0924,10/Nov/23 08:22,10/Nov/23 09:18,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / Hive,,,,,,0,,,,,,"Now, every time, getDelegationToken wil be called when asking for metastore, how about build a cache, we cache the token for the first time, then we can just get token from cache?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 09:18:59 UTC 2023,,,,,,,,,,"0|z1lip4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 09:18;gaborgsomogyi;If that hurts the solution is not caching but adding a token provider for metastore like HiveServer2DelegationTokenProvider.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update download link in doc of Kafka connector,FLINK-33512,13557438,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,renqs,renqs,10/Nov/23 06:54,15/Nov/23 02:42,04/Jun/24 20:40,10/Nov/23 07:20,kafka-3.0.1,,,,,,,,,,,,,,Connectors / Kafka,Documentation,,,,,0,,,,,,"Currently the download link of Kafka connector in documentations points to a non-existed version `1.18.0`:

DataStream API: [https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/kafka/]

Table API Kafka: [https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/kafka/]

Table API Upsert Kafka: [https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/upsert-kafka/]

The latest version should be 3.0.1-1.17 and 3.0.1-1.18.",,,,,,,,,,,,,,,,,,,,,,,FLINK-33401,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 15 02:42:09 UTC 2023,,,,,,,,,,"0|z1lim8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 02:42;tanyuxin;Fixed in https://issues.apache.org/jira/browse/FLINK-33401.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink SqlGateway select bigint type column get cast exception,FLINK-33511,13557432,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xiaodao,xiaodao,10/Nov/23 06:19,10/Nov/23 06:19,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Table SQL / Gateway,,,,,,0,,,,,,"when i open a beeline client connect to flink sqlgateway;

i create table like
{code:java}
//代码占位符
CREATE TABLE Orders (
    order_number BIGINT,
    price        DECIMAL(32,2),
    buyer        ROW<first_name STRING, last_name STRING>,
    order_time   TIMESTAMP(3)
) WITH (
  'connector' = 'datagen'
) {code}
and then select * from Orders；

i got exception:

java.lang.Long cannot be cast to org.apache.flink.table.data.StringData",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-10 06:19:37.0,,,,,,,,,,"0|z1likw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update plugin for SBOM generation to 2.7.10,FLINK-33510,13557424,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vinodanandan,vinodanandan,10/Nov/23 04:51,10/Nov/23 10:46,04/Jun/24 20:40,,,,,,,,,,,,,,,,Build System,,,,,,0,,,,,,Update the CycloneDX Maven plugin for SBOM generation to 2.7.10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-10 04:51:45.0,,,,,,,,,,"0|z1lij4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flaky test testNodeAffinity() in InitTaskManagerDecoratorTest.java,FLINK-33509,13557418,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yijujt2,yijujt2,10/Nov/23 04:08,15/Dec/23 05:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"When applying Nondex to the test, the NodeSelectorRequirement object shows nondeterminism. When testing, we assume that requirement would be equal to expected_requirement, both of them are the instance of NodeSelectorRequirement object. The NodeSelectorRequirement object has three attributes, including key, operator, and values ​​list.  It is possible to get values list's elements in order `[blockedNode1, blockedNode2]`, while the expected result is `[blockedNode2, blockedNode1]` which is incorrect. 

 

The root cause appeared in line 56 of `KubernetesTaskManagerTestBase.java`. (flink-kubernetes/src/test/java/org/apache/flink/kubernetes/kubeclient/KubernetesTaskManagerTestBase.java) Here we define `BLOCKED_NODES` as a new `hashSet`. In `InitTaskManagerDecoratorTest.java`, when initializing the `expected_requirement` in the test, the values ​​being passed was this`BLOCKED_NODES`, which is an **unordered Set**. Later, the code convert this **hashSet** into **arrayList**, which led to the unstable result of the values list.",Java 11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,2023-11-10 04:08:26.0,,,,,,,,,,"0|z1lihs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for wildcard paths in Flink History Server for multi cluster environment,FLINK-33508,13557415,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jjayadeep,jjayadeep,jjayadeep,10/Nov/23 03:57,13/Dec/23 08:21,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"In Cloud users typically create multiple clusters which are ephemeral and want a single history server to look at historical jobs.

To implement this history server needs to support wildcard paths and this change is to support such wildcard paths",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 13 08:21:49 UTC 2023,,,,,,,,,,"0|z1lih4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/23 08:21;anikaKelhanka;Approach to update Flink's History Server logic to enable getting logs and data from multiple directories at a time by using a path with wildcards (i.e glob pattern) from HadoopFileSystem locations:

1. Flink's {{HistoryServerArchiveFetcher}} class currently uses the HadoopFileSystem's {{listStatus}} API method which not resolve patterns/wildcards in the history server file path.
2. Introduce a new method {{globStatus(Path pathPattern)}} in {{Flink's FileSystem}} API.
3. Implement new Method in Flink's HadoopFileSystem class such that it internally calls the [Hadoop's globStatus func|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L2217].
4. Finally, point {{HistoryServerArchiveFetcher}} to the new globStatus() API instead of listStatus() for HadoopFileSystem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonToRowDataConverters can't parse zero timestamp  '0000-00-00 00:00:00',FLINK-33507,13557414,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lucas_jin,lucas_jin,10/Nov/23 03:21,24/Nov/23 09:08,04/Jun/24 20:40,,1.16.0,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,CDC,JsonFormatter,Kafka,MySQL,pull-request-available,"When I use Flink CDC to synchronize data from MySQL, Kafka is used to store data in JSON format. But when I read data from Kafka, I found that the Timestamp type data ""0000-00-00 00:00:00"" in MySQL could not be parsed by Flink, and the error was reported as follows:

Caused by: org.apache.flink.formats.json.JsonToRowDataConverters$JsonParseException: Fail to deserialize at field: data.
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$createRowConverter$ef66fe9a$1(JsonToRowDataConverters.java:354)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$wrapIntoNullableConverter$de0b9253$1(JsonToRowDataConverters.java:380)
    at org.apache.flink.formats.json.JsonRowDataDeserializationSchema.convertToRowData(JsonRowDataDeserializationSchema.java:131)
    at org.apache.flink.formats.json.canal.CanalJsonDeserializationSchema.deserialize(CanalJsonDeserializationSchema.java:234)
    ... 17 more
Caused by: org.apache.flink.formats.json.JsonToRowDataConverters$JsonParseException: Fail to deserialize at field: update_time.
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$createRowConverter$ef66fe9a$1(JsonToRowDataConverters.java:354)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$wrapIntoNullableConverter$de0b9253$1(JsonToRowDataConverters.java:380)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$createArrayConverter$94141d67$1(JsonToRowDataConverters.java:304)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$wrapIntoNullableConverter$de0b9253$1(JsonToRowDataConverters.java:380)
    at org.apache.flink.formats.json.JsonToRowDataConverters.convertField(JsonToRowDataConverters.java:370)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$createRowConverter$ef66fe9a$1(JsonToRowDataConverters.java:350)
    ... 20 more
Caused by: java.time.format.DateTimeParseException: Text '0000-00-00 00:00:00' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 0
    at java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)
    at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)
    at org.apache.flink.formats.json.JsonToRowDataConverters.convertToTimestamp(JsonToRowDataConverters.java:224)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$wrapIntoNullableConverter$de0b9253$1(JsonToRowDataConverters.java:380)
    at org.apache.flink.formats.json.JsonToRowDataConverters.convertField(JsonToRowDataConverters.java:370)
    at org.apache.flink.formats.json.JsonToRowDataConverters.lambda$createRowConverter$ef66fe9a$1(JsonToRowDataConverters.java:350)
    ... 25 more
Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 0
    at java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)
    at java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)
    at java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)
    at java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)
    at java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)
    at java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)
    at java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)
    at java.time.format.Parsed.resolveDateFields(Parsed.java:351)
    at java.time.format.Parsed.resolveFields(Parsed.java:257)
    at java.time.format.Parsed.resolve(Parsed.java:244)
    at java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)
    at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)
    at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)
    ... 29 more

Usually MySQL allows the server and client to parse this type of data and treat it as NULL, so I think Flink should also support it.",Flink 1.16.0,345600,345600,,0%,345600,345600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,english,,Fri Nov 24 09:08:40 UTC 2023,,,,,,,,,,"0|z1ligw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 10:48;martijnvisser;[~lucas_jin] There is already the option ""json.ignore-parse-errors"" for the JSON format, then it will return NULLs. So I would think this is already supported;;;","10/Nov/23 13:03;lucas_jin;[~martijnvisser] Thank you for your reply. In my scenario, I just want to recognize zero date as null, but json.ignore-parse-errors will swallow all exceptions, which will cause unknown problems in actual production, so I I hope Flink can provide a parameter similar to MySQL JDBC:
{code:java}
URL = ""jdbc:mysql://****:3306/****?zeroDateTimeBehavior=convertToNull"";{code};;;","13/Nov/23 14:05;martijnvisser;[~lucas_jin] There is also ""json.map-null-key.mode"" and ""json.map-null-key.literal"";;;","13/Nov/23 14:05;martijnvisser;Feel free to open a PR for another option in case that doesn't work for you. ;;;","24/Nov/23 09:08;lucas_jin;[~martijnvisser] 
Hi, I have submitted a PR for this issue, can you review it for me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make AWS connectors compilable with jdk17,FLINK-33506,13557390,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,09/Nov/23 21:05,25/Apr/24 08:57,04/Jun/24 20:40,25/Apr/24 08:57,,,,,,,,,,,aws-connector-4.4.0,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,Since 1.18 Flink with jdk 17 support is released it would make sense to add such support for connectors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 25 08:56:53 UTC 2024,,,,,,,,,,"0|z1libk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 08:56;dannycranmer;Merged commit [{{c6e0abb}}|https://github.com/apache/flink-connector-aws/commit/c6e0abb65a0e51b40dd218b890a111886fbf797f] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
switch away from using netty 3 based Pekko Classic Remoting,FLINK-33505,13557376,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fanningpj,fanningpj,09/Nov/23 19:29,09/Nov/23 19:29,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"It is my understanding that Flink uses the Netty 3 based Pekko Classic Remoting.

Netty 3 has a lot of security issues.

It will be months before Pekko 1.1.0 is released but that switches Classic Remoting to use Netty 4.

Akka and Pekko actually recommend that users switch to using Artery based communications.

Even if you wait for Pekko 1.1.0, the new Netty 4 based classic remoting will need to be tested.

There is also the option of dropping Pekko - FLINK-29281

If you don't want to try Artery and don't want to wait for Pekko 1.1.0, you might be able to copy over 5 classes that add Netty 4 support and update your application.conf. This would be approximately https://github.com/apache/incubator-pekko/pull/778. There is a bit more work to do in terms of debugging the test failure and it seems that this change is unlikely to be merged back to the Pekko 1.0.x line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-09 19:29:30.0,,,,,,,,,,"0|z1li8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supported maximum parallel jobs,FLINK-33504,13557361,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,09/Nov/23 17:08,17/Nov/23 22:32,04/Jun/24 20:40,,,,,,,,,,,,,,,,Build System / CI,,,,,,0,github-actions,,,,,"{quote}Up to 10 free Microsoft-hosted parallel jobs that can run for up to 360 minutes (6 hours) each time
{quote}
Azure CI allows up to 10 parallel jobs for public repos ([source|https://learn.microsoft.com/en-us/azure/devops/pipelines/licensing/concurrent-jobs?view=azure-devops&tabs=ms-hosted]).

Looks like GHA allows up to 20 parallel jobs in the free plan ([source|https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration#usage-limits]) and 5 macos jobs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-09 17:08:44.0,,,,,,,,,,"0|z1li54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Maven wrapper to 3.2.0,FLINK-33503,13557355,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,09/Nov/23 16:04,22/Nov/23 12:26,04/Jun/24 20:40,22/Nov/23 12:26,1.17.1,1.18.0,1.19.0,,,,,,,,1.19.0,,,,Build System / CI,,,,,,0,pull-request-available,,,,,"It downloads some binaries to execute
in maven-wrapper 3.2.0 there was added checksum check, should we also leverage this feature[1] before execution of downloaded binaries?
[1] https://issues.apache.org/jira/browse/MWRAPPER-75",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33501,FLINK-33607,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 22 12:26:01 UTC 2023,,,,,,,,,,"0|z1li3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/23 12:26;mapohl;* master
** [f2260a8702e9898ed8312f12de5b53483ccd5780|https://github.com/apache/flink/commit/f2260a8702e9898ed8312f12de5b53483ccd5780]
** [262c967fc0739760e0993330120e412a1715847a|https://github.com/apache/flink/commit/262c967fc0739760e0993330120e412a1715847a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase caused a fatal error,FLINK-33502,13557354,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,mapohl,mapohl,09/Nov/23 15:48,21/Dec/23 07:08,04/Jun/24 20:40,21/Dec/23 07:08,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Network,,,,,,0,pull-request-available,test-stability,,,,"[https://github.com/XComp/flink/actions/runs/6789774296/job/18458197040#step:12:9177]
{code:java}
Error: 21:21:35 21:21:35.379 [ERROR] Error occurred in starting fork, check output in log
9168Error: 21:21:35 21:21:35.379 [ERROR] Process Exit Code: 239
9169Error: 21:21:35 21:21:35.379 [ERROR] Crashed tests:
9170Error: 21:21:35 21:21:35.379 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase
9171Error: 21:21:35 21:21:35.379 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
9172Error: 21:21:35 21:21:35.379 [ERROR] Command was /bin/sh -c cd /root/flink/flink-tests && /usr/lib/jvm/jdk-11.0.19+7/bin/java -XX:+UseG1GC -Xms256m -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED -Xmx1536m -jar /root/flink/flink-tests/target/surefire/surefirebooter10811559899200556131.jar /root/flink/flink-tests/target/surefire 2023-11-07T20-32-50_466-jvmRun4 surefire6242806641230738408tmp surefire_1603959900047297795160tmp
9173Error: 21:21:35 21:21:35.379 [ERROR] Error occurred in starting fork, check output in log
9174Error: 21:21:35 21:21:35.379 [ERROR] Process Exit Code: 239
9175Error: 21:21:35 21:21:35.379 [ERROR] Crashed tests:
9176Error: 21:21:35 21:21:35.379 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase
9177Error: 21:21:35 21:21:35.379 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:532)
9178Error: 21:21:35 21:21:35.379 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:479)
9179Error: 21:21:35 21:21:35.379 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:322)
9180Error: 21:21:35 21:21:35.379 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:266)
[...] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33323,,,,,,,,"20/Nov/23 06:37;Wencong Liu;image-2023-11-20-14-37-37-321.png;https://issues.apache.org/jira/secure/attachment/13064561/image-2023-11-20-14-37-37-321.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 21 07:08:43 UTC 2023,,,,,,,,,,"0|z1li3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 09:22;mapohl;https://github.com/XComp/flink/actions/runs/6865564561/job/18670371174#step:12:8510;;;","16/Nov/23 08:32;mapohl;This issue is also appearing in the Azure CI pipeline:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54602&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8706

This is why I'm moving the issue out of the GHA work (FLINK-27075). [~Wencong Liu] can you have a look (since you replied in [FLINK-33323|https://issues.apache.org/jira/browse/FLINK-33323?focusedCommentId=17784350&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17784350])?;;;","16/Nov/23 13:00;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54512&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8639;;;","16/Nov/23 13:28;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54348&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8952;;;","16/Nov/23 13:30;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54339&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8951;;;","16/Nov/23 13:31;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54326&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8652;;;","17/Nov/23 14:13;Wencong Liu;Thank you for your reminder [~mapohl] . I would like to ask if you know any way to obtain the complete runtime logs of this ITCase? In the local IDE, we can configure _log4j2-test.properties_ to directly output INFO-level logs to the console. From the link on Github, I can only see that the process exit code is 239. Based on this information alone, I am currently unable to identify the root cause.🤔;;;","17/Nov/23 15:08;mapohl;You have to download the build artifacts for the corresponding stage (in this case \{{tests}}). The archive will contain the {{watchdog}} file which is the CI log content. Aside from that you have the JUnit fork logs {{mvn-*.log}} (4 since we have four surefire forks). I usually use {{grep -Hirn ""<testname>"" .}} to see all occurrences. Usually, the that lists the {{watchdog}} and one {{mvn-*.log}} file.

Here is the example for the last build failure:
{code:java}
$ unzip  logs-ci-test_ci_tests-1699014739.zip
$ grep -Hirn HybridShuffleITCase .
mvn-3.log:103507:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchangesRestart[enableNewHybridMode=false] is running.
mvn-3.log:104521:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchangesRestart[enableNewHybridMode=false] successfully run.
mvn-3.log:104525:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchangesRestart[enableNewHybridMode=true] is running.
mvn-3.log:105557:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchangesRestart[enableNewHybridMode=true] successfully run.
mvn-3.log:105561:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart[enableNewHybridMode=false] is running.
mvn-3.log:107414:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart[enableNewHybridMode=false] successfully run.
mvn-3.log:107418:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart[enableNewHybridMode=true] is running.
mvn-3.log:109414:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart[enableNewHybridMode=true] successfully run.
mvn-3.log:109418:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchanges[enableNewHybridMode=false] is running.
mvn-3.log:110391:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchanges[enableNewHybridMode=false] successfully run.
mvn-3.log:110395:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchanges[enableNewHybridMode=true] is running.
mvn-3.log:111388:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridFullExchanges[enableNewHybridMode=true] successfully run.
mvn-3.log:111392:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchanges[enableNewHybridMode=false] is running.
mvn-3.log:112354:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchanges[enableNewHybridMode=false] successfully run.
mvn-3.log:112358:Test org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchanges[enableNewHybridMode=true] is running.
watchdog:7740:Nov 03 12:47:49 12:47:49.161 [INFO] Running org.apache.flink.test.runtime.HybridShuffleITCase
watchdog:8567:Nov 03 13:14:12 13:14:12.059 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase
watchdog:8573:Nov 03 13:14:12 13:14:12.059 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase
watchdog:8610:Nov 03 13:14:12 13:14:12.059 [ERROR] org.apache.flink.test.runtime.HybridShuffleITCase{code}
Here you see that {{mvn-3.log}} contains the logs. The last run of {{HybridShuffleITCase}} seems to not terminate (because there is a ""is running"" but no ""successfully run"" line).

Does this help? Strangely, other tests pass afterwards which makes the logs hard to browse... :-/;;;","20/Nov/23 06:38;Wencong Liu;Thank you for your detailed reply. I am currently trying to download the build artifacts for the corresponding stage. 
However, I noticed that the log collection downloaded using the method shown in the figure is different from the logs-ci-test_ci_tests-1699014739.zip that you mentioned.
!image-2023-11-20-14-37-37-321.png|width=839,height=434!  
Could you please advise me on how to download logs-ci-test_ci_tests-1699014739.zip?;;;","20/Nov/23 06:39;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54679&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9;;;","20/Nov/23 07:44;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","20/Nov/23 11:53;mapohl;[~Wencong Liu] my description was based on Azure CI. Use one of the Azure CI links (instead of the GitHub Actions link). The GHA workflow is still in experimental stage. Sorry for the confusion. ;;;","20/Nov/23 22:30;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54732&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","23/Nov/23 06:48;Wencong Liu;Thank [~mapohl]  for your help. The fix will be merged soon.;;;","24/Nov/23 02:24;Weijie Guo;master(1.19) via 07cc8c5c7f276261b5d7c22e0b6617b6ba9666be.;;;","03/Dec/23 10:30;JunRuiLi;Hello [~Wencong Liu],

I've noticed an issue that was previously marked as closed seems to have reappeared. I encountered the problem once more, and you can find the details in the log here: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55141&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba]

Could you please take a moment to review it and confirm whether this is the same issue?

Thank you for your time and assistance.;;;","04/Dec/23 02:21;Wencong Liu;Thanks [~JunRuiLi] . I have investigated it and found that the root cause is different with this issue. But the exception caught in the outermost layer is same. I'll reopen this issue and fix it as soon as possible.;;;","04/Dec/23 07:24;JunRuiLi;[~Wencong Liu] Thank you for the clarification. I look forward to seeing your fix.;;;","06/Dec/23 09:05;Weijie Guo;master(1.19) via 6db32a3374a3fdb34c47039c6240c3845a3e1e30.;;;","12/Dec/23 12:00;mapohl;Looks like it didn't resolve the issue. There's a CI failure due to a fatal error for this test again: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55420&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8274

The following FLINK-27075-related GHA workflow run also included the fix documented above but failed with a 239 exit code in {{HybridShuffleITCase}}: https://github.com/XComp/flink/actions/runs/7176084685/job/19540677190#step:12:8755

I'm reopening the issue. [~Wencong Liu] can you have a look once more?;;;","14/Dec/23 08:44;mapohl;Another GHA workflow failure: https://github.com/XComp/flink/actions/runs/7200483902/job/19614961023#step:12:8905;;;","19/Dec/23 03:28;Wencong Liu;Sorry for the late reply. I've just identified the issue and proposed a fix; it should be stable now. [~mapohl] ;;;","21/Dec/23 07:08;Weijie Guo;master(1.19) 593e443cd85962ffa512e26d1a900eccf5f705ca.

Feel free to reopen it if this fix doesn't work, but I hope this is the last time we patch it. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rely on Maven wrapper instead of having custom Maven installation logic,FLINK-33501,13557347,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,09/Nov/23 15:03,22/Jan/24 09:57,04/Jun/24 20:40,28/Nov/23 14:24,1.17.1,1.18.0,1.19.0,,,,,,,,1.18.1,1.19.0,,,Build System / CI,,,,,,0,pull-request-available,,,,,I noticed that we could use the Maven wrapper instead of having a custom setup logic for Maven in CI.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34194,,,,,FLINK-33468,FLINK-33503,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 28 14:24:08 UTC 2023,,,,,,,,,,"0|z1li20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 14:24;mapohl;This change is not really required in general but is necessary for the GHA migration (FLINK-27075). We're probably deciding to support GHA only for 1.18 and move on with full nightly build support only with the release of 1.19 (i.e. when 1.17 is deprecated). That would reduce the extra efforts around backport CI changes. This is why this change didn't make it into the release-1.17 branch.

master: [99ba4db938ec44c048231584f451e45809fa7514|https://github.com/apache/flink/commit/99ba4db938ec44c048231584f451e45809fa7514]
1.18: [afa434d404048e5e43b49302c518e78bd07a3253|https://github.com/apache/flink/commit/afa434d404048e5e43b49302c518e78bd07a3253]


 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run storing the JobGraph an asynchronous operation,FLINK-33500,13557335,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhengzhili,mapohl,mapohl,09/Nov/23 13:52,23/Feb/24 06:50,04/Jun/24 20:40,,1.17.1,1.18.0,1.19.0,,,,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Currently, submitting a job starts with storing the JobGraph (in HA setups) in the {{{}JobGraphStore{}}}. This includes writing the file to S3 (or some other remote file system). The job submission is done in the {{{}Dispatcher{}}}'s main thread. If writing the {{JobGraph}} is slow, it would block any other operation on the {{{}Dispatcher{}}}. See [Dispatcher#persistAndRunJob|https://github.com/apache/flink/blob/52cbeb90f32ca36c59590df1daa6748995c9b7f8/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L645] as code reference.

This Jira issue is about moving the job submission into the {{ioExecutor}} as an asynchronous call.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 28 09:02:01 UTC 2023,,,,,,,,,,"0|z1lhzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Nov/23 07:36;zhengzhili;Is just using CompletableFuture to run the JobGraphWriter#putJobGraph method？ I could fix it. Please assign to me.;;;","28/Nov/23 09:02;mapohl;Thanks for volunteering, [~zhengzhili]. I assigned the issue to you.

It's more or less making the write operation being performed in the ioExecutor of the Dispatcher in an asynchronous fashion (i.e. with the use of a CompletableFuture as you suggested).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Addressing the stability of org.apache.flink.cep.operator.CEPOperatorTest.testCEPOperatorCleanupEventTime,FLINK-33499,13557333,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Invalid,,krishaudt14,krishaudt14,09/Nov/23 13:51,09/Nov/23 13:58,04/Jun/24 20:40,09/Nov/23 13:58,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,,,"I am proposing to fix the flaky behavior observed in :
{code}
org.apache.flink.cep.operator.CEPOperatorTest.testCEPOperatorCleanupEventTime
{code}

*STEPS TO REPRODUCE THE ISSUE:*

Using the [NonDex|https://github.com/TestingResearchIllinois/NonDex] plugin, the test can be executed first.

{code}
mvn -pl flink-libraries/flink-cep edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=org.apache.flink.cep.operator.CEPOperatorTest#testCEPOperatorCleanupEventTime
{code}

The following error was seen:

{code}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.561 s <<< FAILURE! - in org.apache.flink.cep.operator.CEPOperatorTest
[ERROR] org.apache.flink.cep.operator.CEPOperatorTest.testCEPOperatorCleanupEventTime  Time elapsed: 0.536 s  <<< FAILURE!
java.lang.AssertionError: expected:<SubEvent(42, foo1, 1.0, 10.0)> but was:<SubEvent(42, foo2, 1.0, 10.0)>
	at org.junit.Assert.fail(Assert.java:89)
	at org.junit.Assert.failNotEquals(Assert.java:835)
	at org.junit.Assert.assertEquals(Assert.java:120)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.flink.cep.operator.CEPOperatorTest.verifyPattern(CEPOperatorTest.java:1177)
	at org.apache.flink.cep.operator.CEPOperatorTest.testCEPOperatorCleanupEventTime(CEPOperatorTest.java:644)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   CEPOperatorTest.testCEPOperatorCleanupEventTime:644->verifyPattern:1177 expected:<SubEvent(42, foo1, 1.0, 10.0)> but was:<SubEvent(42, foo2, 1.0, 10.0)>
[INFO] 
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0
{code}

Please share your thoughts on this issue. I am working on identifying the root cause and developing a potential fix for the same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 13:58:08 UTC 2023,,,,,,,,,,"0|z1lhyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 13:58;martijnvisser;[~krishaudt14] Please reply to the comment I've left at FLINK-33443 before opening more tickets. If not, please stop opening more tickets;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not allow manually triggering incremental checkpoints with full checkpoint configured,FLINK-33498,13557302,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,09/Nov/23 11:03,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,,,,,,1.20.0,,,,Runtime / Checkpointing,,,,,,0,,,,,,"Currently, when a job is configured to run with incremental checkpoint disabled, user manually triggers an incremental checkpoint actually triggering a full checkpoint. That is because the files from full checkpoint cannot be shared with an incremental checkpoint. So we'd better throw some exception somewhere around {{CheckpointCoordinator}} and fail the request in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33723,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 01 10:16:33 UTC 2023,,,,,,,,,,"0|z1lhs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 04:32;zakelly;I'm afraid the exception should be thrown around the KeyedStateBackend, which selects the scope of checkpoint stream (exclusive or shared or both) to write during snapshot. Only KeyedStateBackend knows whether it could reuse the files from previous full checkpoints and do an incremental one. 

And later we could consider supporting this in RocksDBStateBackend, by fast duplicating / hard linking remote state files from exclusive to shared. WDYT? [~pnowojski]

 ;;;","01/Dec/23 10:16;zakelly;I notice that the incremental checkpoint is configured by 'state.backend.incremental' and the CheckpointCoordinator does not know the original value of this (It is configured by 'execution.checkpointing.xxx'). Even so, the coordinator sending the strategy of sharing files to state backends and the state backends take this only a suggestion. It is a weak contract between them and it is not easy to fail checkpoint triggering request based on incremental/full option in runtime.

I suggest we disable the incremental request from REST API first, and consider a clearer division of responsibilities between coordinator and state backends.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the Kafka connector to support DISTRIBUTED BY clause,FLINK-33497,13557296,13557293,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,tzulitai,twalthr,twalthr,09/Nov/23 10:39,25/Mar/24 08:23,04/Jun/24 20:40,,,,,,,,,,,,kafka-4.0.0,,,,Connectors / Kafka,,,,,,0,,,,,,The Kafka connector can be one of the first connectors supporting the DISTRIBUTED BY clause. The clause can be translated into 'key.fields' and 'properties.num.partitons' in the WITH clause.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 14:31:36 UTC 2023,,,,,,,,,,"0|z1lhqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 10:40;twalthr;[~tzulitai] would you like to take this? Or who in the Kafka connector community could do this?;;;","09/Nov/23 14:31;tzulitai;[~twalthr] makes sense to integrate the Kafka Connector for this as soon as the clause is ready. Thanks for the ping, I'll assign this to myself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose DISTRIBUTED BY clause via parser,FLINK-33496,13557295,13557293,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,twalthr,twalthr,09/Nov/23 10:36,19/Feb/24 12:41,04/Jun/24 20:40,19/Feb/24 12:41,,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,,,,,,Expose DISTRIBUTED BY clause via parser and TableDescriptor.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 19 12:41:18 UTC 2024,,,,,,,,,,"0|z1lhqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 12:41;twalthr;Fixed as part of FLINK-33495.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add DISTRIBUTED BY clause for CREATE TABLE,FLINK-33495,13557294,13557293,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jhughes,twalthr,twalthr,09/Nov/23 10:34,19/Feb/24 12:40,04/Jun/24 20:40,19/Feb/24 12:39,,,,,,,,,,,1.20.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"Updates:
 - CatalogTable
 - CatalogTable.Builder
 - TableDistribution
 - SupportsBucketing
 - Parser
 - Validation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 19 12:39:06 UTC 2024,,,,,,,,,,"0|z1lhq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 12:39;twalthr;Fixed in master: 6c9ac5c6ae06bf5b42d33b57a7dc1006ac2b2744;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-376: Add DISTRIBUTED BY clause,FLINK-33494,13557293,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,twalthr,twalthr,twalthr,09/Nov/23 10:24,25/Mar/24 08:22,04/Jun/24 20:40,,,,,,,,,,,,1.20.0,,,,Table SQL / API,,,,,,0,,,,,,"Many SQL vendors expose the concepts of Partitioning, Bucketing, and Clustering.

[FLIP-376|https://cwiki.apache.org/confluence/x/loxEE] proposes to introduce the concept of Bucketing to Flink.

It focuses solely on the syntax and necessary API changes to offer a native way of declaring bucketing. Whether this is supported or not during runtime should then be a connector characteristic - similar to partitioning. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-09 10:24:10.0,,,,,,,,,,"0|z1lhq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch connector ElasticsearchWriterITCase test failed,FLINK-33493,13557292,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,09/Nov/23 10:23,22/May/24 09:20,04/Jun/24 20:40,10/Nov/23 05:59,,,,,,,,,,,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,,,0,pull-request-available,,,,,"When I ran tests, the test failed. The failed reason is

{code:java}
Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-base/src/test/java/org/apache/flink/connector/elasticsearch/sink/ElasticsearchWriterITCase.java:[197,46] cannot find symbol
  symbol:   method mock(org.apache.flink.metrics.MetricGroup,org.apache.flink.metrics.groups.OperatorIOMetricGroup)
  location: class org.apache.flink.runtime.metrics.groups.InternalSinkWriterMetricGroup
Error:  /home/runner/work/flink-connector-elasticsearch/flink-connector-elasticsearch/flink-connector-elasticsearch-base/src/test/java/org/apache/flink/connector/elasticsearch/sink/ElasticsearchWriterITCase.java:[273,46] cannot find symbol
  symbol:   method mock(org.apache.flink.metrics.MetricGroup)
{code}

https://github.com/apache/flink-connector-elasticsearch/actions/runs/6809899863/job/18517273714?pr=77#step:13:134.

ElasticsearchWriterITCase called Flink ""InternalSinkWriterMetricGroup#mock"", and it is renamed in https://github.com/apache/flink/pull/23541 ([FLINK-33295|https://issues.apache.org/jira/browse/FLINK-33295] in Flink 1.19). So the test failed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33295,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 05:48:55 UTC 2023,,,,,,,,,,"0|z1lhps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 10:35;tanyuxin;In addition, the affected version can not add 4.0.x, because the version es-4.0.x does not exist currently.;;;","09/Nov/23 12:13;martijnvisser;Good find, this is something that the Elasticsearch maintainers need to address when they add compatibility with Flink 1.19;;;","10/Nov/23 03:59;tanyuxin;[~martijnvisser] Yeah. I will take a look at this issue.;;;","10/Nov/23 05:48;Weijie Guo;main via 161b615.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unavailable links in connector download page,FLINK-33492,13557275,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,liyubin117,liyubin117,09/Nov/23 08:14,23/Nov/23 08:37,04/Jun/24 20:40,23/Nov/23 08:37,1.19.0,,,,,,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"there are several unavailable connector download links (hbase, kafka, etc)

https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/downloads/",,,,,,,,,,,,,,,,,,FLINK-33567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 15:08:17 UTC 2023,,,,,,,,,,"0|z1lhm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 08:22;liyubin117;[~martijnvisser] Hi, I found kafka connector download link is [https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.18.0/flink-sql-connector-kafka-1.18.0.jar,] it should be [https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.0.1-1.18/flink-sql-connector-kafka-3.0.1-1.18.jar,] The same situation for hbase connector, I can help to fix it, WDYT?;;;","09/Nov/23 15:08;martijnvisser;[~liyubin117] I've assigned it to you, feel free to ping me. Keep in mind you'll have to fix {{sql_optional_components}} in such a way that the links can point to externalized versions for those where it's applicable, and to internal ones for the ones that are internal;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support json column validated,FLINK-33491,13557271,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,ouyangwuli,ouyangwuli,ouyangwuli,09/Nov/23 08:05,13/Nov/23 12:02,04/Jun/24 20:40,13/Nov/23 12:02,1.19.0,,,,,,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"Just like the {{is_valid_json}} function in PostgreSQL, it would be useful to have an inbuilt function to check whether a string conforms to the JSON specification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 13 12:01:52 UTC 2023,,,,,,,,,,"0|z1lhl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 15:14;martijnvisser;We already have IS JSON https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/functions/systemfunctions/#json-functions - Why would we need to add another function for this? I'm inclined to close this as a Duplicate.;;;","13/Nov/23 12:01;ouyangwuli;This is my error, I didn't find the is json function.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate the name conflicts when creating view,FLINK-33490,13557265,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,fsk119,fsk119,09/Nov/23 07:13,29/Jan/24 09:27,04/Jun/24 20:40,29/Jan/24 09:27,1.19.0,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"We should forbid 

```
CREATE VIEW id_view AS
SELECT id, uid AS id FROM id_table
```

As the SQL standards states,

If <regular view specification> is specified, then:
i) If any two columns in the table specified by the <query expression> have equivalent <column name>s, or if any column of that table has an implementation-dependent name, then a <view column list> shall be specified.
ii) Equivalent <column name>s shall not be specified more than once in the <view column list>.

Many databases also throw exception when view name conflicts, e.g. mysql, postgres.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33740,,FLINK-33928,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 04 14:35:16 UTC 2024,,,,,,,,,,"0|z1lhjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 12:06;xuyangzhong;Hi, [~fsk119] , Can I take this jira?;;;","21/Nov/23 05:11;libenchao;I'm wondering what the expected behavior for 


{code:sql}
SELECT * FROM T, T;

{code}

Note that the columns have conflicts since we are joining the same table, which means each column appear twice.;;;","22/Nov/23 06:31;xuyangzhong;Hi, [~libenchao] . 

I test the sql you provide and some more SQL. The test results are following.
{code:java}
create table t1(
  a int,
  b int,
  c int
);insert into t1 values(1,2,3); {code}
 
||Desc||SQL||Mysql 5.6||PG 9.6||Flink master||
|using query directly|select * from t1, t1|exception:
Not unique table/alias: 't1'|exception: 
relation ""t"" does not exist Position: 15|exception: 
Duplicate relation name 'T' in FROM clause|
|create view with the query|create view v as select * from t1, t1|exception: 
Duplicate column name 'a'|exception: 
table name ""t1"" specified more than once|exception: 
Duplicate relation name 't1' in FROM clause|
|using query directly by alias|select * from t1 l, t1 r|{color:#ffab00}work fine and print:
 a b c a b c
1 2 3 1 2 3{color}|{color:#ffab00}work fine and print:
 a b c a b c
1 2 3 1 2 3 {color}|{color:#ffab00}work fine and print:
 a b c  a0  b0 c0
1  2  3   1   2   3 {color}|
|create view with the query by alias|create view v as select * from t1 l, t1 r|exception:
Duplicate column name 'a'|exception:
column ""a"" specified more than once|{color:#de350b}work fine and when select from this view, print:{color}
{color:#de350b}a b c a0 b0 c0{color}
{color:#de350b}1 2 3  1   2    3{color}|

 

Please note the red part of the result. I think that's the bug this issue wants to fix.

 ;;;","23/Nov/23 04:56;libenchao;Great, thanks [~xuyangzhong] for the investigation!

The reason I'm asking this question is that name deriving happens in many cases, and they all belong to the ""query"" realm. I agree that in ""CREATE VIEW"" we can throw exception for duplicated names for better user experience. But we'd better to have a clear vision on what cases we support and what not, and put these cases into the test.

A few more come into my mind could be:

{code:sql}
SELECT *, 'hello' as new_field FROM (
 SELECT id, uid AS id FROM id_table
) 
{code}

{code:sql}
SELECT *, 'hello' as id FROM (
 SELECT id FROM id_table
) 
{code}

Besides, since this would change the public behavior, I'm not sure if we'd better to have a FLIP for it.
;;;","23/Nov/23 05:22;fsk119;+1 to clarify the behaviour. ;;;","23/Nov/23 05:42;xuyangzhong;Hi, [~libenchao] [~fsk119] .

I think we can pick some of the approaches below to describe what we support and what we don't support about the behavior for duplicated names when querying directly or creating a view.
 # Write an independent Flip for it.
 # Start a discuss topic in dev maillist for it.
 # Describe it in a separate chapter in user doc.
 # Describe it in release-note when releasing 1.19.

IMO, it is necessary for it for the work 2,3 and 4. But I'm not sure if we should do the work 1. Do you have any good ideas?;;;","24/Nov/23 05:16;libenchao;I agree mostly, I don't have a strong opinion whether to have a FLIP, I'd like to hear more from others.;;;","24/Nov/23 08:22;martijnvisser;I think it would be a good practice to have a FLIP for it;;;","24/Nov/23 15:27;lincoln.86xy;Here're my two cents: from my previous reference to the sql standard, it forbids two elements at different positions from having equivalent names, especially when created through a sql schema statement, but I didn't find any specific recommendations for the query part, this is also consistent with testing provided by the [~xuyangzhong], these rdbms are indeed allowed duplicate columns in select statement(select * from t1 l, t1 r).
So, for the create view statements involved in this issue, I would prefer to keep the behavior consistent with all the create statements (create [materialized] view / table [as query]) and follow the standard.
I'm fine with initiating a FLIP or not.
;;;","04/Dec/23 13:15;xuyangzhong;OK, it seems that a flip with aligning sql usages to list all patterns we support and do not support can be published. I will add an improvement Jira for it.;;;","21/Dec/23 03:13;fsk119;Merged into master: a4fe01cb9d678b293107b0a6278fec1a749913cc;;;","21/Dec/23 12:09;libenchao;The standard states that:
{quote}i) If any two columns in the table specified by the <query expression> have equivalent <column name>s, or if any column of that table has an implementation-dependent name, then a <view column list> shall be specified.
{quote}
Hence if we specify columns list to the view, it shall not throw exception when columns names from the query conflict. However, the current fix does not allow it, can we improve it? [~xuyangzhong] [~fsk119];;;","22/Dec/23 06:30;xuyangzhong;Hi, [~libenchao] .IIUC, do you mean we need to support the following SQL?
{code:java}
create view MyView as select t1.a, t2.a from SrcTable t1 join SrcTable t2 on t1.b = t2.b {code}
If I understand it wrong, can you give an example?;;;","22/Dec/23 07:14;libenchao;[~xuyangzhong] No, I mean 
{code:SQL}
CREATE VIEW MyView(col1, col2) AS -- Note that, I've specified the column names to the view, hence it should not throw exception even if the query's name conflicts.
SELECT a, a
FROM T
{code}
;;;","22/Dec/23 08:11;xuyangzhong;Agree with you, [~libenchao] . We shouldn't throw an exception for it. I'll improve it.;;;","28/Dec/23 12:38;martijnvisser;I'm confused about FLINK-33740 - Have we now merged this without doing a (small) FLIP? I don't understand why this ticket is now there?;;;","02/Jan/24 01:52;xuyangzhong;Hi, [~martijnvisser] . IIRC, the small Flip that we want to introduce is only used to list the supported sql patterns. Specifically, we will list the SQL syntax that we currently support and do not support one by one.

However, regarding this JIRA issue itself, we determine the need to fix it based on two criteria:
 # Whether this usage is explicitly prohibited in the SQL standard.
 # Whether this usage is explicitly prohibited in commonly used RDBMSs.

If it does not meet the requirements, we deem this syntax to be problematic and believe it requires alignment.

WDYT？;;;","02/Jan/24 09:53;martijnvisser;[~xuyangzhong] So we have a list of currently supported SQL patterns, and we want to publish a FLIP to approve that list? That doesn't feel so useful, since all of the currently supported SQL patterns have been introduced either via FLIPs or approved Jira's, right? So then wouldn't it just be a documentation thing?;;;","03/Jan/24 11:56;fsk119;Hi, [~martijnvisser]. I think this issue is just a bug fix rather than feature and the scope of the fix is clear(in previous discussions, this fix is just about aligning with the SQL standard and the behavior of other databases). Introducing a FLIP to fix a bug would not only consume a lot of our time, but it would also make the whole fixing process very lengthy. 

I think [~libenchao] 's proposal is just to sort out the current behavior supported by Flink SQL, to make Flink SQL easier for users to understand and more in line with SQL standards. However, this does not affect our bug fix, because the purpose of this bug fix itself is to make Flink SQL more standard.

WDYT? cc [~lincoln.86xy] [~xuyangzhong] [~libenchao] 

 ;;;","03/Jan/24 12:34;martijnvisser;[~fsk119] I'm not suggesting to have a FLIP for a bugfix, I'm only trying to understand the process. If the feature and scope of the fix is clear, and it doesn't require breaking anything, then it's fine to go ahead and fix it. But in that case, FLINK-33740 is not needed and not a helpful ticket, since a FLIP isn't meant to list the supported capabilities. If we need to list the supported capabilities, we should just improve the documentation and that should be sufficient. ;;;","04/Jan/24 07:23;fsk119;Hi, [~martijnvisser]. Thanks a lot for your patience and response.

To be honest, I must admit that this change will affect a small portion of users. As shown in the examples in the table above, if there are columns with the same name in a table, an error will be thrown, but we have deliberately refined the error message to show the location of the conflicting column names as clearly as possible. In fact, we scanned the impact of this fix on our internal use and found that less than 3% of jobs were affected. Moreover, this fix has been merged into the commercial branch for over a month now, and to date, we have not received any user feedback on this issue. Therefore, I believe the impact of this change is manageable.

I agree with you that FLINK-33740 is not needed; it would be better to modify this ticket to be about document improvements. cc [~xuyangzhong] ;;;","04/Jan/24 08:08;xuyangzhong;Hi, [~martijnvisser] and [~fsk119] . Thank you for your correction.

As far as FLINK-33740 is concerned, if it only lists the sql that Flink supports or does not support, there is really no need to be a separate flip.

I can change the title of it to ""Add a document to list the supported sql patterns"", what do you think?;;;","04/Jan/24 14:35;martijnvisser;Sounds good to me, thank you! ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LISTAGG with generating partial-final agg will cause wrong result,FLINK-33489,13557254,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,xuyangzhong,xuyangzhong,xuyangzhong,09/Nov/23 04:17,08/Feb/24 03:42,04/Jun/24 20:40,08/Feb/24 03:42,1.10.0,1.11.0,1.12.0,1.13.0,1.14.0,1.15.0,1.16.0,1.17.0,1.18.0,1.9.0,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Adding the following test cases in SplitAggregateITCase will reproduce this bug:

 
{code:java}
// code placeholder
@Test
def testListAggWithDistinctMultiArgs(): Unit = {
  val t1 = tEnv.sqlQuery(s""""""
                            |SELECT
                            |  a,
                            |  LISTAGG(DISTINCT c, '#')
                            |FROM T
                            |GROUP BY a
     """""".stripMargin)

  val sink = new TestingRetractSink
  t1.toRetractStream[Row].addSink(sink)
  env.execute()

  val expected = Map[String, List[String]](
    ""1"" -> List(""Hello 0"", ""Hello 1""),
    ""2"" -> List(""Hello 0"", ""Hello 1"", ""Hello 2"", ""Hello 3"", ""Hello 4""),
    ""3"" -> List(""Hello 0"", ""Hello 1""),
    ""4"" -> List(""Hello 1"", ""Hello 2"", ""Hello 3"")
  )
  val actualData = sink.getRetractResults.sorted
  println(actualData)
} {code}
The `actualData` is `List(1,Hello 0,Hello 1, 2,Hello 2,Hello 4,Hello 3,Hello 1,Hello 0, 3,Hello 1,Hello 0, 4,Hello 2,Hello 3,Hello 1)`, and the delimiter `#` will be ignored.

Let's take its plan:
{code:java}
// code placeholder
LegacySink(name=[DataStreamTableSink], fields=[a, EXPR$1])
+- GroupAggregate(groupBy=[a], partialFinalType=[FINAL], select=[a, LISTAGG_RETRACT($f3_0) AS $f1])
   +- Exchange(distribution=[hash[a]])
      +- GroupAggregate(groupBy=[a, $f3, $f4], partialFinalType=[PARTIAL], select=[a, $f3, $f4, LISTAGG(DISTINCT c, $f2) AS $f3_0])
         +- Exchange(distribution=[hash[a, $f3, $f4]])
            +- Calc(select=[a, c, _UTF-16LE'#' AS $f2, MOD(HASH_CODE(c), 1024) AS $f3, MOD(HASH_CODE(_UTF-16LE'#'), 1024) AS $f4])
               +- MiniBatchAssigner(interval=[1000ms], mode=[ProcTime])
                  +- DataStreamScan(table=[[default_catalog, default_database, T]], fields=[a, b, c]) {code}
The final `GroupAggregate` missing the delimiter args, and the default delimiter `,` will be used.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 15 11:40:13 UTC 2023,,,,,,,,,,"0|z1lhhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 04:23;xuyangzhong;For a quick fix, we can forbid generating final-partial agg with this function.

For a long term, we can only use distinct as the partial agg and then use the real agg function as the final agg. But first we must check out other agg functions.

 

I'll try to fix it.;;;","15/Nov/23 11:40;fsk119;Merged into master: 7e30e5e9fcd51382f48d48c9848bb8df14293e22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Deduplicate node,FLINK-33488,13557225,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,08/Nov/23 19:56,07/Dec/23 12:59,04/Jun/24 20:40,10/Nov/23 08:47,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:47:38 UTC 2023,,,,,,,,,,"0|z1lhaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 08:47;dwysakowicz;Implemented in 88953ba622f4d3b67b59f56e15e1983e7932b926;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the new Snowflake connector to supported list,FLINK-33487,13557220,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,morezaei00,morezaei00,08/Nov/23 19:08,10/Nov/23 08:13,04/Jun/24 20:40,09/Nov/23 12:22,1.17.1,1.18.0,,,,,,,,,,,,,Documentation,,,,,,0,,,,,,"Code was contributed in FLINK-32737.

Add this new connector to the list of supported ones in the documentation with a corresponding sub-page for the details of the connector:
https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/overview/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:13:46 UTC 2023,,,,,,,,,,"0|z1lh9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 12:21;martijnvisser;[~morezaei00] This we can't do, since the connector is not part of the ASF Flink project. It can only be listed at https://flink-packages.org/ ;;;","09/Nov/23 18:26;morezaei00;Hey [~martijnvisser] that makes sense, and thanks for the quick response and pointer.;;;","10/Nov/23 08:13;martijnvisser;[~morezaei00] My pleasure. Let me know if I can help otherwise;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar Client Send Timeout Terminates TaskManager,FLINK-33486,13557209,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,longtimer,longtimer,08/Nov/23 17:06,15/Apr/24 10:08,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,,,"Currently, when the Pulsar Producer encounters a timeout when attempting to send data, it generates an unhandled TimeoutException. This is not a reasonable way to handle the timeout. The situation should be handled in a graceful way either through additional parameters that put control of the action under the discretion of the user or through some callback mechanism that the user can work with to write code. Unfortunately, fight now, this causes a termination of the task manager which then leads to other issues.

Increasing the timeout period to avoid the issue is not really an option to ensure proper handling in the event that the situation does occur.

The exception is as follows:

org.apache.flink.util.FlinkRuntimeException: Failed to send data to Pulsar: persistent://public/default/myproducer-partition-0
        at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.throwSendingException(PulsarWriter.java:182) ~[flink-connector-pulsar-4.0.0-1.17.jar:4.0.0-1.17]
        at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.lambda$write$0(PulsarWriter.java:172) ~[flink-connector-pulsar-4.0.0-1.17.jar:4.0.0-1.17]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-dist-1.17.1.jar:1.17.1]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: org.apache.pulsar.client.api.PulsarClientException$TimeoutException: The producer myproducer- f4b1580b-1ea8-4c21-9d0b-da4d12ca6f93 can not send message to the topic persistent://public/default/myproducer-partition-0 within given timeout
        at org.apache.pulsar.client.impl.ProducerImpl.run(ProducerImpl.java:1993) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        at org.apache.pulsar.shade.io.netty.util.HashedWheelTimer$HashedWheelTimeout.run(HashedWheelTimer.java:715) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        at org.apache.pulsar.shade.io.netty.util.concurrent.ImmediateExecutor.execute(ImmediateExecutor.java:34) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        at org.apache.pulsar.shade.io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:703) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        at org.apache.pulsar.shade.io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:790) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        at org.apache.pulsar.shade.io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:503) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        at org.apache.pulsar.shade.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[pulsar-client-all-2.11.2.jar:2.11.2]
        ... 1 more

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 15 10:08:03 UTC 2024,,,,,,,,,,"0|z1lh7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Dec/23 17:56;dchristle;We also observe this issue, typically at higher QPS. When it triggers, the task will restart, but future checkpoints fail if the tolerable checkpoint failure count is greater than zero. Setting the tolerable checkpoint failure count to zero triggers a more complete restart, which fixes the issue, but it means publishing is paused for a few minutes, which is not ideal.

At least for the best effort/at-least-once delivery modes, is there some way to implement a retry when send timeout triggers? This way, we'd potentially publish a single message/batch twice, rather than triggering a full failure + republishing all messages since the last checkpoint.;;;","15/Apr/24 10:08;syhily;The retry logic is internally provided by the Pulsar client. Should we implement this feature in Flink side? I don't thinks so. But [~dchristle] sounds reasonable to me, that is add a retry logic in `at-least-once` mode and let it crash in `exactly-once` mode.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the EXISTS sub-query by Metadata RowCount,FLINK-33485,13557189,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,08/Nov/23 13:09,24/Nov/23 00:54,04/Jun/24 20:40,24/Nov/23 00:54,1.18.0,,,,,,,,,,1.19.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"If the sub-query is guaranteed to produce at least one row, just return TRUE. If the sub-query is guaranteed to produce no row, just return FALSE.

inspired by CALCITE-5117 however since there is {{FlinkSubQueryRemoveRule}} then it shold be adopted accordingly

examples
{code:sql}
SELECT * FROM T2 WHERE EXISTS (SELECT SUM(a1), COUNT(*) FROM T1 WHERE 1=2)
{code}

aggregation functions always return 1 row even if there is an empty table then we could just replace this query with 
{code:sql}
SELECT * FROM T2 
{code}

another example
{code:sql}
SELECT * FROM MyTable WHERE NOT EXISTS (SELECT a FROM MyTable LIMIT 0)
{code}

{{LIMIT 0}} means no rows so it cold be optimized to

{code:sql}
SELECT * FROM MyTable
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 24 00:54:15 UTC 2023,,,,,,,,,,"0|z1lh2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 00:54;Sergey Nuyanzin;Merged to master as [a1d4dc0a4b05164aee41c781e75c9e9aeae5d758|https://github.com/apache/flink/commit/a1d4dc0a4b05164aee41c781e75c9e9aeae5d758];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kafka Connector Offset Lag Issue with Transactional Data and Read Committed Isolation Level,FLINK-33484,13557171,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lintingbin,lintingbin,08/Nov/23 11:46,10/Nov/23 09:58,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"We have encountered an issue with the Flink Kafka connector when consuming transactional data from Kafka with the {{isolation.level}} set to {{read_committed}} ({{{}setProperty(""isolation.level"", ""read_committed""){}}}). The problem is that even when all the data from a topic is consumed, the offset lag is not 0, but 1. However, when using the Kafka Java client to consume the same data, this issue does not occur.

We suspect that this issue arises due to the way Flink Kafka connector calculates the offset. The problem seems to be in the {{KafkaRecordEmitter.java}} file, specifically in the {{emitRecord}} method. When saving the offset, the method calls {{{}splitState.setCurrentOffset(consumerRecord.offset() + 1);{}}}. While this statement works correctly in a regular Kafka scenario, it might not be accurate when the {{read_committed}} mode is used. We believe that it should be {{{}splitState.setCurrentOffset(consumerRecord.offset() + 2);{}}}, as transactional data in Kafka occupies an additional offset to store the transaction marker.

We request the Flink team to investigate this issue and provide us with guidance on how to resolve it.

Thank you for your attention and support.","Flink 1.17.1

kafka 2.5.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 09:58:06 UTC 2023,,,,,,,,,,"0|z1lgyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 15:20;martijnvisser;[~lintingbin] Please confirm which Flink Kafka connector version you've used. Was it 3.0.*-1.17 ?;;;","09/Nov/23 15:56;lintingbin;[~martijnvisser] org.apache.flink:flink-connector-kafka:1.17.1;;;","10/Nov/23 08:18;martijnvisser;[~lintingbin] Can you please use the connector version I've listed above and try again? That's a newer version with more fixes for various bugs;;;","10/Nov/23 09:58;lintingbin;[~martijnvisser] The issue remains the same with flink-connector-kafka:3.0.1-1.17. I checked the implementation of {{{}emitRecord{}}}, and it is still the same.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why is “UNDEFINED” defined in the Flink task status?,FLINK-33483,13557169,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,xinchen147,xinchen147,08/Nov/23 11:40,14/Nov/23 09:09,04/Jun/24 20:40,13/Nov/23 10:07,1.12.2,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,,,,,,"In the Flink on Yarn mode, if an unknown status appears in the Flink log, jm(jobmanager) will report the task status as undefined. The Yarn page will display the state as FINISHED, but the final status is *UNDEFINED*. In terms of business, it is unknown whether the task has failed or succeeded, and whether to retry. It has a certain impact. Why should we design UNDEFINED? Usually, this situation occurs due to zk(zookeeper) disconnection or jm abnormality, etc. Since the abnormality is present, why not use FAILED?

 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/23 11:57;xinchen147;container_e15_1693914709123_8498_01_000001_8042;https://issues.apache.org/jira/secure/attachment/13064263/container_e15_1693914709123_8498_01_000001_8042","08/Nov/23 13:31;xinchen147;reproduce.log;https://issues.apache.org/jira/secure/attachment/13064264/reproduce.log",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 14 09:09:20 UTC 2023,,,,,,,,,,"0|z1lgyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/23 11:52;xinchen147;Links to FLINK-12302. Including that issue, two scenarios were discovered in Flink-1.12.2, with the issue of reporting *UNDEFINED* to yarn resourcemanager. I have reproduced that scenario, which is when the task is completed in tm and the global terminal state (FINISHED or FAILED) is reached, the jm log shows ""Job 65ccc2410d4554553225889dbea552d7 reached global terminal state {}, kill the jm process (am) or disconnect the zk connection, which will cause a new jm to be pulled up. The new jm assigned “UNKNOWN” based on the task's status ""DONE"" in RunningJobRegistry recorded in zk, and ultimately reported “UNDEFINED”.;;;","08/Nov/23 12:02;xinchen147;But in another scenario in production practice, UNDEFINED also appears. The Jm log can be found in the file  [^container_e15_1693914709123_8498_01_000001_8042] , but I have not fully reproduced this scene. Based on the key information in the log:

{code:java}
15:00:57.657 State change: SUSPENDED
Connection to ZooKeeper suspended, waiting for reconnection.

15:00:54.754          org.apache.flink.util.FlinkException: ResourceManager leader changed to new address null
15:00:54.759          Job DataDistribution$ (281592085ed7f391ab59b83a53c40db3) switched from state RUNNING to RESTARTING.
15:00:54.771          Job DataDistribution$ (281592085ed7f391ab59b83a53c40db3) switched from state RESTARTING to SUSPENDED.
org.apache.flink.util.FlinkException: JobManager is no longer the leader.
Unable to canonicalize address zookeeper:2181 because it's not resolvable.

15:00:55.694   closing socket connection and attempting reconnect
15:00:57.657   State change: RECONNECTED
15:00:57.739  Connection to ZooKeeper was reconnected. Leader retrieval can be restarted.  
15:00:57.740   Connection to ZooKeeper was reconnected. Leader election can be restarted.  

15:00:57.741 Job 281592085ed7f391ab59b83a53c40db3 was not finished by JobManager.
15:00:57.742 Shutting down cluster because job not finished
15:00:57.742 Shutting YarnJobClusterEntrypoint down with application status UNKNOWN. Diagnostics null. 
15:00:57.767 Unregister application from the YARN Resource Manager with final status UNDEFINED.
{code}

From the logs, it can be seen that there was a disconnection of zk for a few seconds. During the disconnection period, rm(resourcemanager) was affected and the Flink task was suspended, attempting to reconnect zk. The most important thing is that after ZK reconnects, for some unknown reason, jm directly determines that the task is in an UNKNOWN state: *Shutting YarnJobClusterEntrypoint down with application status UNKNOWN. Diagnostics null*. During this process, jm did not restart, which is different from FLINK-12302. 

So how exactly is “UNKNOWN” identified here, is it also determined after reading from the RunningJobRegistry in zk?  I have also tried many times and did not reproduce this scene. The reproduction log is attached [^reproduce.log] . I think the reason for the difficulty in reproducing is : When I disconnect all zks, jm will quickly down and restart, and the log will shows an error.

{code:java}
org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token not set: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(deregisterApplication(ApplicationStatus, String))) sent to akka.tcp://flink@192.168.22.121:42347/user/rpc/resourcemanager_0 because the fencing token is null.
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:67) ~
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~
{code}
Here, I speculate that the disconnection of zk may have affected rm's leadership, leading to issues when jm unregisters to rm with ApplicationStatus: UNDEFINED. However, in actual production scenarios, jm did not fail and continued with the execution.;;;","08/Nov/23 12:41;xinchen147;Biggest confusion is why Flink needs to design UNDEFINED. From the perspective of the scenario, UNDEFINED is all due to exceptions (zk or jm exceptions，at all). Why can't we define failed? Defining FAILED allows us to determine and retry tasks, but UNDEFINED here has no meaning at all. Is there a better solution to this problem in subsequent versions of Flink, or how to better reproduce this scenario? My solution is the same as FLINK-12302, hoping to give a FALLED finalStatus to report to resourcemanager in this case, providing the user with the most clear reminder.  Even in this case, the task may have actually run successfully in TM(taskmanager), but after all, an exception (zk disconnection) has occurred. Anyway, executing a task and ultimately giving the user an UNDEFINED state can be very confusing.;;;","13/Nov/23 10:07;mapohl;Based on your comment in FLINK-12302 it appears that the issue doesn't exist in newer versions of Flink (you mentioned 1.16). As stated in [my comment|https://issues.apache.org/jira/browse/FLINK-12302?focusedCommentId=17785444&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17785444] in the very same issue, doing code investigations in old versions like 1.12 are quite tedious. The community agreed on supporting the two most-recently released versions (i.e. 1.18 and 1.17 for now). Upgrading Flink to a more recent version might be the best thing to do here as it comes with multiple other bugfixes.

I'm hesitating to pick this one up as an issue as it's 1.12-related. I'm gonna close this issue. Feel free to reopen it if you have additional information or you disagree.;;;","14/Nov/23 01:58;xinchen147;Hi [~mapohl] yes, this is not a question in recent version. But I also see some code about ""ApplicationStatus.UNKNOWN"" and ""UNDEFINED"", so it is confused whether there still be ""UNDEFINED"" scenarios in Flink?  Some other code like:

{code:java}
// ApplicationDispatcherBootstrap.java#getJobResult()
private CompletableFuture<JobResult> getJobResult(
            final DispatcherGateway dispatcherGateway,
            final JobID jobId,
            final ScheduledExecutor scheduledExecutor,
            final boolean tolerateMissingResult) {
        final Time timeout =
                Time.milliseconds(configuration.get(ClientOptions.CLIENT_TIMEOUT).toMillis());
        final Time retryPeriod =
                Time.milliseconds(configuration.get(ClientOptions.CLIENT_RETRY_PERIOD).toMillis());
        final CompletableFuture<JobResult> jobResultFuture =
                JobStatusPollingUtils.getJobResult(
                        dispatcherGateway, jobId, scheduledExecutor, timeout, retryPeriod);
        if (tolerateMissingResult) {
            // Return ""unknown"" job result if dispatcher no longer knows the actual result.
            return FutureUtils.handleException(
                    jobResultFuture,
                    FlinkJobNotFoundException.class,
                    exception ->
                            new JobResult.Builder()
                                    .jobId(jobId)
                                    .applicationStatus(ApplicationStatus.UNKNOWN)
                                    .netRuntime(Long.MAX_VALUE)
                                    .build());
        }
        return jobResultFuture;
    }



// RestClusterClient.java#requestJobResultInternal
private CompletableFuture<JobResult> requestJobResultInternal(@Nonnull JobID jobId) {
        return pollResourceAsync(
                        () -> {
                            final JobMessageParameters messageParameters =
                                    new JobMessageParameters();
                            messageParameters.jobPathParameter.resolve(jobId);
                            return sendRequest(
                                    JobExecutionResultHeaders.getInstance(), messageParameters);
                        })
                .thenApply(
                        jobResult -> {
                            if (jobResult.getApplicationStatus() == ApplicationStatus.UNKNOWN) {
                                throw new JobStateUnknownException(
                                        String.format(""Result for Job %s is UNKNOWN"", jobId));
                            }
                            return jobResult;
                        });
    }
{code}

I want to know which other scenarios the final state of the Flink task may be UNDEFINED.;;;","14/Nov/23 09:09;mapohl;I guess {{ApplicationStatus.UNKNOWN}} is used as a general fallback in a lot of cases. I'm not sure whether all the {{ApplicationStatus}} cases are properly covered in the code (which would mean that the {{UNKNOWN}} status is obsolete). That would require some more effort to analyze.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink benchmark regression check in the new machines,FLINK-33482,13557153,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zakelly,zakelly,zakelly,08/Nov/23 09:49,15/Nov/23 04:37,04/Jun/24 20:40,,,,,,,,,,,,,,,,Benchmarks,,,,,,0,pull-request-available,,,,,"After FLINK-33052, the codespeed and benchmark servers are hosted on Aliyun. We need to modify the benchmark daily monitoring scripts for the new environment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33052,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-08 09:49:44.0,,,,,,,,,,"0|z1lguw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why were checkpoints stored on zookeeper deleted when JobManager failures with Flink High Availability on yarn,FLINK-33481,13557102,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hansonhe,hansonhe,08/Nov/23 01:18,13/Nov/23 09:46,04/Jun/24 20:40,,1.13.1,,,,,,,,,,,,,,,,,,,,0,,,,,,"FlinkVersion:  1.13.1
(1) flink-conf.yaml 

high-availability.zookeeper.path.root    /flink
high-availability.zookeeper.quorum   xxxxx
state.checkpoint-storage    filesystem
state.checkpoints.dir   hdfs://xxxxx
(2) jobmanager

application_1684323088373_1744
jm_1: appattempt_1684323088373_1744_000001    Tue Oct 31 11:19:07 +0800 2023
jm_2: appattempt_1684323088373_1744_000002    Sat Nov 4 11:10:52 +0800 2023

(3) When appattempt_1684323088373_1744_000001  failures, I found 
   3.1）Completed checkpoint 5750 for job 6262e8c6a072027459f9b4eeb3e9735c stored on hdfs is successful
   3.2)  Checkpoint stored in zookeper: /flink/application_1684323088373_1744 was deleted

the logs as following: 
!image-2023-11-08-10-05-54-694.png!
!image-2023-11-08-09-40-59-889.png!

(4) After appattempt_1684323088373_1744_000001  failures, jobmanager switch to start appattempt_1684323088373_1744_000002, the logs start as following:   No checkpoint found during restore  !image-2023-11-08-09-57-17-739.png!

（5）My Question ：
       5.1）Why were checkpoints stored on zookeeper deleted when JobManager failures with Flink High Availability on yarn？It cause that  Jobmanager run to restore  without checkpoint found

       5.2）Why not directly to use successful and completed checkpoint-5750 stored on hdfs  to restore  when failed over to  jm_2:appattempt_1684323088373_1744_000002? But it still attempt to recover from ZookeeperStateHandleStore firstly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/23 01:41;hansonhe;image-2023-11-08-09-40-59-889.png;https://issues.apache.org/jira/secure/attachment/13064233/image-2023-11-08-09-40-59-889.png","08/Nov/23 01:57;hansonhe;image-2023-11-08-09-57-17-739.png;https://issues.apache.org/jira/secure/attachment/13064234/image-2023-11-08-09-57-17-739.png","08/Nov/23 02:05;hansonhe;image-2023-11-08-10-05-54-694.png;https://issues.apache.org/jira/secure/attachment/13064235/image-2023-11-08-10-05-54-694.png",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 13 09:46:54 UTC 2023,,,,,,,,,,"0|z1lgjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/23 02:55;hansonhe;@[~trohrmann]  or AnyBody
Can you or anybody help me to answer my questions. This jobmanager failedover was caused by zookeeper cluster server failures, and found some fllink jobs  consumed kafka data all from earliest after jobmanager failed over;;;","13/Nov/23 09:46;mapohl;[~hansonhe] Flink 1.13.1 reached its end-of-life already quite some time ago. I agree with you that it looks suspicious and you conclusion based on the logs you shared is correct. But it's quite tedious to investigate whether there are bugfixes for this specific issue in later versions. Can you reproduce the issue? And if yes, could you try running the scenario with newer versions of Flink to see whether it's reproducible in those versions like Flink 1.18 as well?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for GroupAggregate node,FLINK-33480,13557099,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,08/Nov/23 01:02,07/Dec/23 20:52,04/Jun/24 20:40,07/Dec/23 12:59,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 07 12:59:17 UTC 2023,,,,,,,,,,"0|z1lgiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 12:59;dwysakowicz;Implemented in 193b1c68976cdfbd66147278f23d7d427d9b5562..fac3ac786674f9b6ce5716902e74b1533ccb1c0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Rerouting support to KafkaSink,FLINK-33479,13557068,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,aeolus811tw,aeolus811tw,07/Nov/23 18:29,08/Nov/23 03:16,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"KafkaSink currently operates in a fail / success model. If it fails it will throw FlinkRuntimeException, any record that may cause failure has to be filtered by the operators preceeding KafkaSink. If there's unexpected failure that was caught, this will create continuous failure until a hotfix has been provided and issue addressed.

Relying on checkpoint to restore the process also create duplicated records produced within the interval, unless EXACTLY_ONCE is utilized.

 

Since Flink is meant to be a real-time streaming process, any downtime can create detrimental effect to the downstream consumer of the data.

I'm proposing to add an error handling injection/intercept to KafkaSink (user can define an error handler or route the error causing record to another downstream operator) to keep the process running. Only throwing RuntimeException when the interceptor was not provided.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 08 03:16:19 UTC 2023,,,,,,,,,,"0|z1lgc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 18:30;aeolus811tw;can someone assign this to me. we have idea on how this implementation can work and can take it forward if community accept the proposal;;;","08/Nov/23 03:16;shenbo;I have encountered a similar situation in JdbcSink a while ago, the job could fail and cannot recover when attempted to process a record that is too big to write to a mysql table.

In my situation, I modified Jdbc connector to skip write errors and emits error messages through logging system (plugged in a kafka appender, so the error could be consumed by external systems).

 

I am fairly new to flink, and not sure if this approach violates any design philosophy.

It would be nice if the commnuity have a proper way of managing error tolerance in sinks from a more general perspective.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to call unix_timestamp when it is in filter condition ,FLINK-33478,13557014,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,07/Nov/23 11:17,13/Nov/23 11:35,04/Jun/24 20:40,13/Nov/23 11:31,,,,,,,,,,,1.19.0,,,,Connectors / Hive,,,,,,0,pull-request-available,,,,,"It can reproduced by following sql.

select * from foo where cast(unix_timestamp() as bigint) > 0

It caused by we are not do fold expr in filter expression.

And it is supported by default hive latest version:
Code of hive: 
[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java#L3739]
[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L373]
[https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java#L1020]

https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java#L123",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 13 11:31:37 UTC 2023,,,,,,,,,,"0|z1lg00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 11:31;luoyuxia;master: 9f04df35f51d63df8d98cd49d7c2bc5090e26004;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce code redundancy in PojoSerializer,FLINK-33477,13557006,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,07/Nov/23 09:59,07/Nov/23 09:59,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,API / Type Serialization System,,,,,,0,,,,,,"We identified code duplicates in the {{PojoSerializer}} when working on FLINK-32380 (see [related thread in the PR|https://github.com/apache/flink/pull/23490#discussion_r1378509403]).

(!) We should be cautious when doing this change. This part of the code is performance-sensitive. We should monitor the change in the Flink performance tests (see [dev ML announcement|https://lists.apache.org/thread/bkw6ozoflgltwfwmzjtgx522hyssfko6] for context and link collection) if we decide to do this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-07 09:59:27.0,,,,,,,,,,"0|z1lfy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
join table throw Record is too big Exception,FLINK-33476,13557004,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,zhuyinjun,zhuyinjun,07/Nov/23 09:28,09/Nov/23 16:58,04/Jun/24 20:40,09/Nov/23 16:58,,,,,,,,,,,,,,,,,,,,,0,,,,,,"When I join a table, Flink throws a record too big Exception. What parameters should I configure to solve this problem

 

Caused by: java.io.IOException: Record is too big, it can't be added to a empty InMemoryBuffer! Record size: 1163764, Buffer: 0
    at org.apache.flink.table.runtime.util.ResettableExternalBuffer.throwTooBigException(ResettableExternalBuffer.java:193)
    at org.apache.flink.table.runtime.util.ResettableExternalBuffer.add(ResettableExternalBuffer.java:149)
    at org.apache.flink.table.runtime.operators.join.SortMergeJoinIterator.bufferMatchingRows(SortMergeJoinIterator.java:118)
    at org.apache.flink.table.runtime.operators.join.SortMergeOneSideOuterJoinIterator.nextOuterJoin(SortMergeOneSideOuterJoinIterator.java:88)
    at org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.oneSideOuterJoin(SortMergeJoinOperator.java:413)
    at org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.doSortMergeJoin(SortMergeJoinOperator.java:298)
    at org.apache.flink.table.runtime.operators.join.SortMergeJoinOperator.endInput(SortMergeJoinOperator.java:248)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.endOperatorInput(TableOperatorWrapper.java:124)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.propagateEndOperatorInput(TableOperatorWrapper.java:136)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.endOperatorInput(TableOperatorWrapper.java:130)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.propagateEndOperatorInput(TableOperatorWrapper.java:136)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.endOperatorInput(TableOperatorWrapper.java:122)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.propagateEndOperatorInput(TableOperatorWrapper.java:136)
    at org.apache.flink.table.runtime.operators.multipleinput.TableOperatorWrapper.endOperatorInput(TableOperatorWrapper.java:122)
    at org.apache.flink.table.runtime.operators.multipleinput.BatchMultipleInputStreamOperator.endInput(BatchMultipleInputStreamOperator.java:56)
    at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.endOperatorInput(StreamOperatorWrapper.java:98)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.endInput(RegularOperatorChain.java:97)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:68)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 16:58:53 UTC 2023,,,,,,,,,,"0|z1lfxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 09:33;zhuyinjun;@[~mapohl]  hi
what parameter configurations can solve the problem of exceptions when joining a table with a large  record;;;","09/Nov/23 16:58;mapohl;[~zhuyinjun] Unfortunately, I don't have an answer to your question. It's not my field of expertise. Please use the [user mailing list|https://flink.apache.org/what-is-flink/community/#mailing-lists] for questions like that. It's more likely to get an answer there than to ping individuals. Jira is preferably used for actual bugs or improvements.

I'm closing this issue as it doesn't seem to cover an actual bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn parameters cannot be set using flink.yarn.<key>,FLINK-33475,13556986,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,zhengzhili,zhengzhili,07/Nov/23 07:23,07/Nov/23 07:47,04/Jun/24 20:40,07/Nov/23 07:47,,,,,,,,,,,,,,,Command Line Client,,,,,,0,,,,,,"yarn parameters cannot be set using flink.yarn.<key>

 

!image-2023-11-07-15-22-46-793.png|width=937,height=191!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/23 07:22;zhengzhili;image-2023-11-07-15-22-46-793.png;https://issues.apache.org/jira/secure/attachment/13064223/image-2023-11-07-15-22-46-793.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 07:27:15 UTC 2023,,,,,,,,,,"0|z1lfts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 07:27;zhengzhili;Please assign it to me;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShowPlan throws undefined exception In Flink Web Submit Page,FLINK-33474,13556982,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,07/Nov/23 05:57,07/Nov/23 12:17,04/Jun/24 20:40,07/Nov/23 12:17,1.19.0,,,,,,,,,,1.17.2,1.18.1,1.19.0,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,,"The exception as shown in the figure below, meanwhile, the job plan cannot be displayed properly.

 

The root cause is that the dagreComponent is located in the nz-drawer and is only loaded when the drawer is visible, so we need to wait for the drawer to finish loading and then render the job plan.

!image-2023-11-07-13-53-08-216.png|width=400,height=190!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/23 05:53;Yu Chen;image-2023-11-07-13-53-08-216.png;https://issues.apache.org/jira/secure/attachment/13064220/image-2023-11-07-13-53-08-216.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 12:17:11 UTC 2023,,,,,,,,,,"0|z1lfsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 12:17;yunta;merged
master: 008e1916e8bbeb18c1d06c74e2797da5a439cd47
release-1.18: 2409184456aa2d07c5bbc580916370802fb3ae8e
release-1.17: 89cbd394a6cbfce1ca685362bf9ce4cf476bca7d
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Flink client to 1.18.0,FLINK-33473,13556977,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,prabhujoseph,prabhujoseph,07/Nov/23 05:24,07/Nov/23 09:38,04/Jun/24 20:40,07/Nov/23 09:38,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,Update the operator flink dependency to Flink-1.18.,,,,,,,,,,,,,,,,,,,,,,,FLINK-33403,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 09:38:14 UTC 2023,,,,,,,,,,"0|z1lfrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 05:44;fanrui;Hi [~prabhujoseph] , this Jira duplicates with FLINK-33403.

 

And Gyula feedbacked at PR comment[1], the flink client will be bumped after 1.18.1 is released.

 

[1]https://github.com/apache/flink-kubernetes-operator/pull/697#issuecomment-1786531295;;;","07/Nov/23 09:38;prabhujoseph;Thanks [~fanrui]. Will close this one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Solve the problem that the temporary file of flink-conf.yaml in S3AFileSystem cannot be uploaded,FLINK-33472,13556975,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,zhengzhili,zhengzhili,zhengzhili,07/Nov/23 03:34,27/Apr/24 10:44,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Client / Job Submission,,,,,,0,pull-request-available,,,,,Solve the problem that the temporary file of flink-conf.yaml in S3AFileSystem cannot be uploaded。,,,,,,,,,,,,,,,,,,,,,,FLINK-33569,,,,,,,,,,FLINK-29797,,,,,,,,,,"16/Nov/23 01:56;zhengzhili;image-2023-11-16-09-56-40-806.png;https://issues.apache.org/jira/secure/attachment/13064433/image-2023-11-16-09-56-40-806.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 23 07:01:03 UTC 2023,,,,,,,,,,"0|z1lfrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 07:27;zhengzhili;Please assign it to me;;;","13/Nov/23 09:34;mapohl;Thanks for bringing this up, [~zhengzhili]. Do you also have evidence (i.e. logs) that uploading the file failed? The file not being available could have multiple causes which not necessarily have something to do with Flink's code. Could you elaborate more on what the cause of the issue is?;;;","16/Nov/23 01:17;zhengzhili;[~mapohl] Thank you for your reply。

After creating the flink-conf.yaml temporary file in YarnClusterDescriptor#startAppMaster, Through YarnApplicationFileUploader# registerSingleLocalResource registered local files,

In this case, if (Utils.isRemotePath(resourcePath.toString())) is used to determine whether the file is remote based on the path.

The incoming tmpConfigurationFile#getAbsolutePath makes Utils. IsRemotePath considered remote file
When FileSystem is S3AFileSystem, the getFileStatus method displays an error indicating that the file does not exist
Incoming parameters should be new Path (tmpConfigurationFile.toURI ()) to the new Path (tmpConfigurationFile. getAbsolutePath ())   !image-2023-11-16-09-56-40-806.png|width=561,height=429!;;;","16/Nov/23 11:51;mapohl;Hi [~zhengzhili], I'm sorry but I'm struggling to follow your claim. Can you provide actual code locations (e.g. https://github.com/apache/flink/blob/2c50b4e956305426f478b726d4de4a640a16b810/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1081 for the screenshot you provided) and elaborate more on where you see a bug? Additionally, it would help to provide logs because it's still not clear to me whether it's a Flink issue.;;;","17/Nov/23 07:00;zhengzhili;My logs is at this link： [https://github.com/zhengzhili333/flinktest/blob/main/33472/flink-root-client-host109.log] . Code location is at  [https://github.com/apache/flink/blob/release-1.17.1/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1052|https://github.com/apache/flink/blob/release-1.17.1/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1050] .

 ;;;","23/Nov/23 07:01;zhengzhili;[~mapohl]   this is my pull request [https://github.com/apache/flink/pull/23782] , could you please have a look? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator supports compiling with Java 21,FLINK-33471,13556961,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,07/Nov/23 00:11,10/Nov/23 07:58,04/Jun/24 20:40,10/Nov/23 07:58,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,Since there is a new Java LTS version available (21) it would make sense to support it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 07:58:02 UTC 2023,,,,,,,,,,"0|z1lfo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 07:58;gyfora;merged to main ff4c730e0612a44fa9fc2eda09e1fe6bb7054145;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Join node,FLINK-33470,13556953,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,06/Nov/23 22:24,12/Dec/23 13:43,04/Jun/24 20:40,12/Dec/23 13:43,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33727,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 06 08:48:10 UTC 2023,,,,,,,,,,"0|z1lfmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Dec/23 15:48;dwysakowicz;Implemented in e886dfdda6cd927548c8af0a88e78171e7ba34a8..5edc7d7b18e88cc86e84d197202d8cbb40621864;;;","04/Dec/23 06:15;Sergey Nuyanzin;I'm sorry however had to revert (also confirmed by [~jhughes])
18b67b104e025b142a8321e5163edf7fbd439580 and 026bd4be9bafce86ced42d2a07e8b8820f7e6d9d

because of ci failures 
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55120&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12099
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55129&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11786
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55129&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12081
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55129&view=logs&j=32715a4c-21b8-59a3-4171-744e5ab107eb&t=ff64056b-5320-5afe-c22c-6fa339e59586&l=12064
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55129&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12099
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55136&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12099
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55137&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11779
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55160&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12189
# https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55160&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=11822


also more details at FLINK-33727;;;","04/Dec/23 06:18;Sergey Nuyanzin;a note: how to reproduce it locally 
{{ExpandRestoreTest}} should be executed before {{JoinRestoreTest}}
e.g. it could be done like here 
{quote}
I was able to find a way to reproduce it locally with 100%.
Just open IntellijIDEA and run all tests for RestoreTestBase
{quote}
;;;","06/Dec/23 08:48;dwysakowicz;Fixed in 2c7f2d4bca25f923f591b20ed7fb93a40acfc681..78b0a625f49058d0c0213de49d6337a7d4f2ab72;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Limit node ,FLINK-33469,13556944,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jhughes,jhughes,jhughes,06/Nov/23 20:14,09/Nov/23 15:49,04/Jun/24 20:40,09/Nov/23 15:49,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 15:49:53 UTC 2023,,,,,,,,,,"0|z1lfkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 15:49;dwysakowicz;Implemented in 7542b56f2abb860f42a83c4687f6e38bb82b78c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run Kubernetes IT test fails due to wrong Maven version,FLINK-33468,13556919,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,06/Nov/23 16:55,09/Nov/23 15:04,04/Jun/24 20:40,09/Nov/23 15:04,,,,,,,,,,,,,,,Build System / CI,,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6772388387/job/18405319623#step:15:4841

{code}
[...]
Error: 15:52:19 15:52:19.837 [ERROR] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion failed with message:
Nov 06 15:52:19 Detected Maven Version: 3.8.8 is not in the allowed range [3.8.6,3.8.6].
Nov 06 15:52:19 15:52:19.837 [INFO] ------------------------------------------------------------------------
Nov 06 15:52:19 15:52:19.837 [INFO] BUILD FAILURE
Nov 06 15:52:19 15:52:19.838 [INFO] ------------------------------------------------------------------------
Nov 06 15:52:19 15:52:19.839 [INFO] Total time:  9.755 s
Nov 06 15:52:19 15:52:19.839 [INFO] Finished at: 2023-11-06T15:52:19Z
Nov 06 15:52:19 15:52:19.839 [INFO] ------------------------------------------------------------------------
Error: 15:52:19 15:52:19.840 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.1.0:enforce (enforce-maven) on project flink-kubernetes: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
Error: 15:52:19 15:52:19.840 [ERROR] 
Error: 15:52:19 15:52:19.840 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Error: 15:52:19 15:52:19.840 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Error: 15:52:19 15:52:19.840 [ERROR] 
Error: 15:52:19 15:52:19.840 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Error: 15:52:19 15:52:19.840 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33501,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 15:04:06 UTC 2023,,,,,,,,,,"0|z1lfew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 08:05;mapohl;https://github.com/XComp/flink/actions/runs/6775989610/job/18417126478#step:14:7597;;;","07/Nov/23 08:08;mapohl;Moving to {{./mvnw}} in {{tools/ci/maven-utils.sh}} causes problems in {{Quickstarts Java nightly end-to-end test'}}:
{code}
Nov 07 02:26:31 Invoking mvn with '-Dmaven.repo.local=/home/runner/work/flink/flink/.m2/repository -Dmaven.wagon.http.pool=false -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat=HH:mm:ss.SSS -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn --no-snapshot-updates -B -Dflink.hadoop.version=2.10.2 -Dinclude_hadoop_aws -Dscala-2.12 -Djdk11 -Pjava11-target  --settings /home/runner/work/flink/flink/flink/tools/ci/google-mirror-settings.xml   clean package'
environment: line 7: ./mvnw: No such file or directory
/home/runner/work/flink/flink/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 72: cd: target: No such file or directory
java.nio.file.NoSuchFileException: flink-quickstart-java-0.1.jar
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
	at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:149)
	at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1764)
	at java.base/java.util.zip.ZipFile$Source.get(ZipFile.java:1414)
	at java.base/java.util.zip.ZipFile$CleanableResource.<init>(ZipFile.java:742)
	at java.base/java.util.zip.ZipFile$CleanableResource.get(ZipFile.java:859)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:257)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:186)
	at java.base/java.util.zip.ZipFile.<init>(ZipFile.java:157)
	at jdk.jartool/sun.tools.jar.Main.list(Main.java:1499)
	at jdk.jartool/sun.tools.jar.Main.run(Main.java:380)
	at jdk.jartool/sun.tools.jar.Main.main(Main.java:1680)
{code}

* https://github.com/XComp/flink/actions/runs/6778931640/job/18425437663#step:14:3088
* https://github.com/XComp/flink/actions/runs/6776239519/job/18417703242#step:14:3438
* https://github.com/XComp/flink/actions/runs/6776232956/job/18417653141#step:14:3090;;;","09/Nov/23 15:04;mapohl;Will be fixed with FLINK-33501 independently of the GHA work in FLINK-27075;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support concurrent serialization in akka,FLINK-33467,13556883,13548014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lsdy,lsdy,lsdy,06/Nov/23 11:53,06/Nov/23 12:39,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / RPC,,,,,,0,,,,,,"In OLAP high QPS scenarios, there are a large number of RPC requests for deploy task and update task status. At this time, the serialization and deserialization performed by AKKA threads become a bottleneck. By supporting parallelization of akka serialization operations, the performance of akka in accepting and processing a large number of RPC requests can be improved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-06 11:53:40.0,,,,,,,,,,"0|z1lf6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bounded Kafka source never finishes after restore from savepoint,FLINK-33466,13556877,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jwdk,jwdk,06/Nov/23 11:18,04/Apr/24 12:08,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Common,Connectors / Kafka,,,,,0,pull-request-available,,,,,"When setting up a bounded Kafka source, if the job is restored from a savepoint before the source has finished, then the Kafka source will never transition to a finished state.

This seems to be because the noMoreSplitsAssignment variable in the SourceReaderBase class is not snapshotted. Therefore, after restoring from a checkpoint/savepoint, the noMoreSplitsAssignment variable will default to false, and the first condition in the private helper method finishedOrAvailableLater() in the SourceReaderBase class will always evaluate to true.

Since this originates in the base class, the problem should hold for all source types, not just kafka.

 

Would it make sense to snapshot the noMoreSplitsAssigntments variable?

I would love to take this on as a first task if appropriate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 08 06:49:08 UTC 2023,,,,,,,,,,"0|z1lf5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 15:46;jwdk;Hi, would appreciate it if any of the committers could have a look!;;;","08/Nov/23 06:49;jwdk;See the following issues for the same problem:

FLINK-31006

FLINK-29674

It seems, that the two possible solutions are as follows:

1. Either the sources must signal the NoMoreSplitsEvent again after restoring from a snapshot or
2. the SourceReaderBase should snapshot the noMoreSplitsAssignment variable.

Which approach would be most viable?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make SingleThreadFetcherManager and FutureCompletingBlockingQueue as PublicEvolving.,FLINK-33465,13556856,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,loserwang1024,loserwang1024,06/Nov/23 08:29,26/Jan/24 03:31,04/Jun/24 20:40,26/Jan/24 03:27,1.18.0,,,,,,,,,,,,,,Connectors / Parent,,,,,,0,,,,,,"As discussed in FLINK-31324, though the {{SingleThreadFetcherManager}} is annotated as {{{}Internal{}}}, it actually acts as some-degree public API, which is widely used in many connector projects:
[flink-cdc-connector|https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlSourceReader.java#L93], [flink-connector-mongodb|https://github.com/apache/flink-connector-mongodb/blob/main/flink-connector-mongodb/src/main/java/org/apache/flink/connector/mongodb/source/reader/MongoSourceReader.java#L58] and so on.

More over, even the constructor of 
`SingleThreadMultiplexSourceReaderBase`  (which is PublicEvolving) includes the params of `SingleThreadFetcherManager`  and `FutureCompletingBlockingQueue` .That means that the `SingleThreadFetcherManager` and 
`FutureCompletingBlockingQueue`have already been exposed to users for a long time and are widely used.

 
{code:java}
public SingleThreadMultiplexSourceReaderBase(
    FutureCompletingBlockingQueue<RecordsWithSplitIds<E>> elementsQueue,
    SingleThreadFetcherManager<E, SplitT> splitFetcherManager,
    RecordEmitter<E, T, SplitStateT> recordEmitter,
    Configuration config,
    SourceReaderContext context)
{ super(elementsQueue, splitFetcherManager, recordEmitter, config, context); }
 
{code}
 
Therefore, why not make SingleThreadFetcherManager and FutureCompletingBlockingQueue PublicEvolving?",,,,,,,,,,,,,,,,,,,,,,,FLINK-34196,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 08 02:54:22 UTC 2023,,,,,,,,,,"0|z1lf0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/23 08:32;loserwang1024;[~becket_qin] , [~renqs] , [~Leonard] , [~jark] , CC, WDYT?;;;","08/Nov/23 02:54;renqs;[~loserwang1024] Thanks for raising the issue. It looks good to me to expose these two more classes as public API, considering that almost all source implementations are using them de facto. But please create a FLIP and start a discussion in the dev mailing list for public API issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinITCase.testRightOuterJoin failed due to heartbeat timeout,FLINK-33464,13556853,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,06/Nov/23 08:00,06/Nov/23 08:01,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6756936036/job/18367079822#step:12:11525

{code}
Error: 21:46:20 21:46:20.936 [ERROR] Tests run: 196, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 788.192 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase
Error: 21:46:20 21:46:20.936 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.testRightOuterJoin  Time elapsed: 68.118 s  <<< ERROR!
Nov 04 21:46:20 java.lang.RuntimeException: Failed to fetch next result
Nov 04 21:46:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:118)
Nov 04 21:46:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:89)
Nov 04 21:46:20 	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:230)
Nov 04 21:46:20 	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:132)
Nov 04 21:46:20 	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)
Nov 04 21:46:20 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:309)
Nov 04 21:46:20 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)
Nov 04 21:46:20 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:109)
Nov 04 21:46:20 	at org.apache.flink.table.planner.runtime.batch.sql.join.JoinITCase.testRightOuterJoin(JoinITCase.scala:892)
Nov 04 21:46:20 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
Nov 04 21:46:20 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
Nov 04 21:46:20 Caused by: java.io.IOException: Failed to fetch job execution result
Nov 04 21:46:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:187)
Nov 04 21:46:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:123)
Nov 04 21:46:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:115)
Nov 04 21:46:20 	... 105 more
Nov 04 21:46:20 Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)
Nov 04 21:46:20 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:185)
Nov 04 21:46:20 	... 107 more
Nov 04 21:46:20 Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
Nov 04 21:46:20 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
Nov 04 21:46:20 	at org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
Nov 04 21:46:20 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1287)
Nov 04 21:46:20 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)
Nov 04 21:46:20 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Nov 04 21:46:20 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
Nov 04 21:46:20 	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
Nov 04 21:46:20 	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)
Nov 04 21:46:20 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
Nov 04 21:46:20 	at org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)
Nov 04 21:46:20 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
Nov 04 21:46:20 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
Nov 04 21:46:20 	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
Nov 04 21:46:20 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
Nov 04 21:46:20 	at org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)
Nov 04 21:46:20 	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)
Nov 04 21:46:20 	at org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)
Nov 04 21:46:20 	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)
Nov 04 21:46:20 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
Nov 04 21:46:20 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
Nov 04 21:46:20 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)
Nov 04 21:46:20 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
Nov 04 21:46:20 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)
Nov 04 21:46:20 	... 5 more
Nov 04 21:46:20 Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
Nov 04 21:46:20 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
Nov 04 21:46:20 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:765)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:742)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
Nov 04 21:46:20 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1645)
Nov 04 21:46:20 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1144)
Nov 04 21:46:20 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1084)
Nov 04 21:46:20 	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:785)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:195)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:182)
Nov 04 21:46:20 	at org.apache.flink.runtime.scheduler.SimpleExecutionSlotAllocator$LogicalSlotHolder.release(SimpleExecutionSlotAllocator.java:203)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:152)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releasePayload(DefaultDeclarativeSlotPool.java:482)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.freeAndReleaseSlots(DefaultDeclarativeSlotPool.java:474)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.DefaultDeclarativeSlotPool.releaseSlots(DefaultDeclarativeSlotPool.java:445)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.internalReleaseTaskManager(DeclarativeSlotPoolService.java:275)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolService.releaseTaskManager(DeclarativeSlotPoolService.java:231)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:549)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.handleTaskManagerConnectionLoss(JobMaster.java:1469)
Nov 04 21:46:20 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1464)
Nov 04 21:46:20 	at org.apache.flink.runtime.heartbeat.DefaultHeartbeatMonitor.run(DefaultHeartbeatMonitor.java:158)
Nov 04 21:46:20 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
Nov 04 21:46:20 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Nov 04 21:46:20 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
Nov 04 21:46:20 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
Nov 04 21:46:20 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
Nov 04 21:46:20 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
Nov 04 21:46:20 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
Nov 04 21:46:20 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
Nov 04 21:46:20 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
Nov 04 21:46:20 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
Nov 04 21:46:20 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
Nov 04 21:46:20 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
Nov 04 21:46:20 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
Nov 04 21:46:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
Nov 04 21:46:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Nov 04 21:46:20 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
Nov 04 21:46:20 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
Nov 04 21:46:20 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
Nov 04 21:46:20 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
Nov 04 21:46:20 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
Nov 04 21:46:20 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
Nov 04 21:46:20 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
Nov 04 21:46:20 	... 5 more
Nov 04 21:46:20 Caused by: java.util.concurrent.TimeoutException: Heartbeat of TaskManager with id 0b0b8b9b-fa3b-4ce7-bd82-0bfdaf85ac79 timed out.
Nov 04 21:46:20 	... 31 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-06 08:00:47.0,,,,,,,,,,"0|z1lf08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the implementation of dynamic source tables based on the new source,FLINK-33463,13556810,13419032,Sub-task,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,05/Nov/23 09:13,01/Jun/24 02:40,04/Jun/24 20:40,,,,,,,,,,,,jdbc-3.3.0,,,,Table SQL / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 31 01:35:21 UTC 2024,,,,,,,,,,"0|z1leqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/24 01:47;fanrui;Merged to main(3.3.0) via:
 * bde28e6a92ffa75ae45bc8df6be55d299ff995a2
 * 50e8d2b16c31420bad475b629b4ab0bf4218a1f1
 * 0ed88d50ac789e3b0cf43d67416ad41e58d83699 (revert :bde28e6a92ffa75ae45bc8df6be55d299ff995a2);;;","31/May/24 01:35;fanrui;After discuss with [~RocMarshal] , it's better to support the JDBC dynamic source tables based on the new source after we think new JdbcSource is stable, so let's revert the commit first, and finish the Jira in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort out the document page about the new Jdbc source.,FLINK-33462,13556809,13419032,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,05/Nov/23 09:13,30/May/24 06:31,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 07 10:41:44 UTC 2024,,,,,,,,,,"0|z1leqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 15:15;davidradl;I am curious what we intend to document here. the main class is labelled with the internal tag in the open pr [https://github.com/apache/flink-connector-jdbc/pull/78/files#diff-1cb96d83a1a96840413da2630840a47bea8aeb07e1ec4cab6111cf2a3a013fa0] . I would not expect us to document internal classes - or should it not be tagged internal?;;;","07/May/24 10:41;RocMarshal;Hi, [~martijnvisser] 
There's a minor item to confirm, should we  synchronize the files about doc-pages to the docs-folder into the Flink main repository ? or just keep the new change in the flink-connector-jdbc repo ?
I'd appreciated with your confirmation~ :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support streaming related semantics for the new JDBC source,FLINK-33461,13556808,13419032,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,05/Nov/23 09:12,03/Jun/24 11:51,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-05 09:12:09.0,,,,,,,,,,"0|z1leq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support property authentication connection.,FLINK-33460,13556807,13419032,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,05/Nov/23 09:10,25/Apr/24 01:36,04/Jun/24 20:40,25/Apr/24 01:36,,,,,,,,,,,jdbc-3.3.0,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 25 01:30:15 UTC 2024,,,,,,,,,,"0|z1leq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/24 01:30;fanrui;Merged to main via bb43b3c18fda6a2e587b3a3941d697803fe86441;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the new source that keeps the same functionality as the original JDBC input format,FLINK-33459,13556806,13419032,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,05/Nov/23 09:08,08/Apr/24 13:06,04/Jun/24 20:40,08/Apr/24 12:07,,,,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 08 12:07:05 UTC 2024,,,,,,,,,,"0|z1leps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/23 09:16;RocMarshal;The ticket is mainly to do the two items:
1. Support the new source that keeps the same functionality as the original JDBC input format

2. Mark the old APIs as Deprecated.;;;","08/Apr/24 12:07;Sergey Nuyanzin;Merged as [ab5d6159141bdbe8aed78e24c9500a136efbfac0|https://github.com/apache/flink-connector-jdbc/commit/ab5d6159141bdbe8aed78e24c9500a136efbfac0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add env.java.opts.gateway option in flink-conf.yaml,FLINK-33458,13556792,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,agsharath,agsharath,04/Nov/23 20:31,06/Nov/23 17:09,04/Jun/24 20:40,06/Nov/23 17:09,1.18.0,,,,,,,,,,,,,,Table SQL / Gateway,,,,,,0,easyfix,,,,,"Currently, JobManager, TaskManager, HistoryServer, Client have their own configuration keys to set java options in flink-conf.yaml

However it is missing for SQL Gateway. It should be added for completeness. It can be useful, for example, to add Remote Debugging options.

I propose this config key 

 
{noformat}
env.java.opts.gateway{noformat}
 in flink-conf.yaml which will be applied to SQL Gateway. I already have a working patch. Its a pretty small one. If the community is OK, I will raise a PR",,,,,,,,,,,,,,,,,,,,,,,FLINK-33203,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 06 17:08:30 UTC 2023,,,,,,,,,,"0|z1lemo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/23 13:29;xiangyu0xf;Hi [~agsharath], thx for reporting this. However, I think this is resolved in https://issues.apache.org/jira/browse/FLINK-33203 .;;;","06/Nov/23 17:08;agsharath;Wow nice! thanks a lot for the reply and for the fix. I guess I'll close this issue.

btw, I had applied for a ASF Jira account so I can contribute some more small fixes. Plz help in the approval process if you happen to know anyone there.

Thanks!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkImageBuilder checks for Java 21,FLINK-33457,13556767,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,04/Nov/23 04:19,26/Nov/23 20:19,04/Jun/24 20:40,26/Nov/23 20:19,,,,,,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,"Currently for java 21 it fails like
{noformat}
Nov 04 03:03:08 Caused by: org.apache.flink.connector.testframe.container.ImageBuildException: Failed to build image ""flink-configured-jobmanager""
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:234)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkTestcontainersConfigurator.configureJobManagerContainer(FlinkTestcontainersConfigurator.java:65)
Nov 04 03:03:08 	... 61 more
Nov 04 03:03:08 Caused by: java.lang.IllegalStateException: Unexpected Java version: 21
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.getJavaVersionSuffix(FlinkImageBuilder.java:284)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.lambda$buildBaseImage$3(FlinkImageBuilder.java:250)
Nov 04 03:03:08 	at org.testcontainers.images.builder.traits.DockerfileTrait.withDockerfileFromBuilder(DockerfileTrait.java:19)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.buildBaseImage(FlinkImageBuilder.java:246)
Nov 04 03:03:08 	at org.apache.flink.connector.testframe.container.FlinkImageBuilder.build(FlinkImageBuilder.java:206)
Nov 04 03:03:08 	... 62 more
Nov 04 03:03:08 

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Nov 26 20:19:44 UTC 2023,,,,,,,,,,"0|z1leh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/23 20:19;Sergey Nuyanzin;Merged to master as [6c429c5450a003d6521693116e0fbb2dab543d6e|https://github.com/apache/flink/commit/6c429c5450a003d6521693116e0fbb2dab543d6e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade maven-shade-plugin to 3.5.1,FLINK-33456,13556752,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,03/Nov/23 22:12,20/Nov/23 22:02,04/Jun/24 20:40,20/Nov/23 22:02,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"Current plugin version does not support jdk20+
The support was added within MSHADE-454 and released with 3.5.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 20 22:02:45 UTC 2023,,,,,,,,,,"0|z1leds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Nov/23 22:02;Sergey Nuyanzin;Merged as [e68411a289e35633dd0e69284a07af75b21db02e|https://github.com/apache/flink/commit/e68411a289e35633dd0e69284a07af75b21db02e]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for SortLimit node,FLINK-33455,13556734,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,03/Nov/23 18:02,14/Nov/23 12:34,04/Jun/24 20:40,14/Nov/23 12:34,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 14 12:34:39 UTC 2023,,,,,,,,,,"0|z1le9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 12:34;dwysakowicz;Implemented in 32d31cb8b838451182d1949414b73ed585b13336;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding tls configuration to IngressSpec,FLINK-33454,13556721,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,ryanvanhuuksloot,dongwoo.kim,dongwoo.kim,03/Nov/23 16:07,13/Dec/23 12:01,04/Jun/24 20:40,12/Dec/23 15:56,,,,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,,,4,pull-request-available,,,,,"Hello, I want to propose new configuration parameter for IngressSpec.

Currently flink k8s operator creates ingress resource as we define but it doesn't support tls configuration to secure ingress. 

How about adding tls parameter on IngressSpec?

*IngressSpec*
tls: IngressTLS

*IngressTLSSpec*
Hosts: List<String>
SecretName: String 

If we could reach an agreement I'll be glad to take on the implementation.
Thanks in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 13 12:01:10 UTC 2023,,,,,,,,,,"0|z1le6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 15:48;gyfora;[~dongwoo.kim] , I have assigned this ticket to [~ryanvanhuuksloot] as he expressed interest about working on this.;;;","08/Dec/23 16:26;ryanvanhuuksloot;PR available. Happy to collaborate [~dongwoo.kim]. I had already written a draft of the code before I looked for an issue - we have a fork internally. Feel free to comment on the PR.;;;","12/Dec/23 15:56;gyfora;merged to main 300972547cc2d4f78434a8729f85866e64505b34;;;","13/Dec/23 12:01;dongwoo.kim;[~ryanvanhuuksloot] sorry for the late reply, thanks for your efforts :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add java21-target profile,FLINK-33453,13556711,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,03/Nov/23 14:57,21/Nov/23 22:56,04/Jun/24 20:40,21/Nov/23 22:56,,,,,,,,,,,1.19.0,,,,Build System,,,,,,0,pull-request-available,,,,,"Add a new profile analogous to the java11-target and java17-target profiles.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 21 22:56:41 UTC 2023,,,,,,,,,,"0|z1le4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Nov/23 22:56;Sergey Nuyanzin;Merged as [33dbf5043d00c3d6fed787ff62013769b38f420a|https://github.com/apache/flink/commit/33dbf5043d00c3d6fed787ff62013769b38f420a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A series of improvements of Autoscaler Standalone,FLINK-33452,13556703,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,03/Nov/23 13:56,08/Jan/24 11:03,04/Jun/24 20:40,,,,,,,,,,,,,,,,Autoscaler,,,,,,0,,,,,,This is an umbrella ticket. It aims to make a series of improvements to the Autoscaler Standalone to bring it into production.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-03 13:56:40.0,,,,,,,,,,"0|z1le2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement FileSytem based AutoScalerStateStore (e.g : HbaseAutoScalerStateStore),FLINK-33451,13556701,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,03/Nov/23 13:52,06/Nov/23 07:32,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Implement a filesystem autoscaler state store that will be useful in autoscaler support for yarn 

- HDFS support 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 14:33:14 UTC 2023,,,,,,,,,,"0|z1le2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 13:53;samrat007;i would like to work on the tasks 
[~fanrui] can you please assign me the task ?;;;","03/Nov/23 14:23;fanrui;Hi [~samrat007] , thanks for creating this JIRA. I have assigned it to you.

 

I convert this from subtask of FLINK-32723 to a saperate JIRA, because the FLIP-334 or FLINK-32723 wants to decouple autoscaler and kubernetes and support autoscaler standalone. And FLINK-32723 has almost met initial expectations.

And kubernetes 1.7.0 is about to be released, the remaining improvements will be completed gradually in 1.8 or after version.

 
{quote}Implement a filesystem autoscaler state store that will be useful in autoscaler support for yarn 
{quote}
Actually, I'm not sure whether filesystem state store is needed. Some concerns:
 * Yarn cluster always uses hdfs, but autoscaler standalone with YarnJobFetcher also can use JdbcStateStore.
 * If we support hdfs state store, it needs some hadoop dependences and hadoop conf.
 * Finally, and my biggest concern: if autoscaler manages more than 1K jobs, the state is big. How to manage the file layout?
 ** The file layout needs to care about how to update/read/write/delete the state? how to ensure the read and write performance?
 ** These are exactly the issues that Database considers.

Please correct me if my understanding is wrong, thanks~ :);;;","03/Nov/23 14:33;samrat007;Thank you [~fanrui] , for your detailed explanation, and you've raised some very valid points.

 

In the context of FLINK-33451, I initiated this Jira with these points in mind, as I believe it could lead to a valuable implementation:

In certain Hadoop clusters, there might be existing file-based databases such as HBase, and there's a need to leverage these databases for state storage. While your proposal of the JDBC StateStore offers one solution for these clusters,

I introduced this Jira to explore the potential value in implementing a file-based database for statestore option.

Your insights are greatly appreciated. Do you think this implementation would be useful?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement JdbcAutoScalerStateStore,FLINK-33450,13556698,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,03/Nov/23 13:44,22/Jan/24 02:58,04/Jun/24 20:40,22/Jan/24 02:58,,,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,,,0,pull-request-available,,,,,Design doc: https://docs.google.com/document/d/1lE4s3ZAyCfzYT4dNOVarz5dIctQ8UUAVqjPKlQ1XU1c/edit?usp=sharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 22 02:57:57 UTC 2024,,,,,,,,,,"0|z1le1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jan/24 02:57;fanrui;Merged to main(1.8.0) via :
 * c34fe4f41a2f684da01d84ab0d761ec669af83f6
 * 293067ab3fa227eb6a38ef5d1255fa4641da142c
 * 96b2aa0b6684d0b06602c128a45f8886adf9324a
 * 6e78221e6abc72ce8daeb92be1cf58fd2506b591;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support array_contains_seq function,FLINK-33449,13556688,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,leoyy,leoyy,03/Nov/23 12:24,18/Dec/23 11:07,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"support function array_contains_seq like trino contains_sequence

trino: https://trino.io/docs/current/functions/array.html?highlight=contains_sequence#contains_sequence",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 18 11:07:16 UTC 2023,,,,,,,,,,"0|z1ldzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Dec/23 03:56;xuyangzhong;Hi, [~leoyy] thanks for driving this. IMO, this type of function is very useful, and I'm +1 to support this function.

However, the function names in some database or big data system are different. Take examples of the systems you mentioned as following.

1. Clickhouse `hasAll(set, subset)`

2. Trino `contains_sequence(x, seq)`

3. Starrocks `array_contains_all(arr1, arr2)`

The function names are not unified. In Flink, there has been a function named `ARRAY_CONTAINS(haystack, needle)` to check if the given element exists in an array. What about using `ARRAY_CONTAINS_ALL` to implement the function in this JIRA?

For a more formal process, maybe a discussion mail thread should be started in dev maillist.

Look for your thought. cc [~martijnvisser] ;;;","14/Dec/23 08:10;leoyy;Hi [~xuyangzhong] Thank you for your reply

ARRAY_CONTAINS_SEQ function in StarRocks ans trino to checks whether `arr2` is a subset of `arr1` and in the same exact order. if and only if {{{}array1 = prefix + array2 + suffix{}}}.

ARRAY_CONTAINS_ALL function in StarRocks and clickhouse to checks whether `arr2` is a subset of `arr1` , but not need to in the same exact order.

Also, if we need to support ARRAY_CONTAINS_ALL , i can try to implement it in flink.

Thank you very much.;;;","15/Dec/23 02:39;xuyangzhong;Hi, [~leoyy] , thanks for your explanation.

After your description, I think the name ARRAY_CONTAINS_SEQ is OK. From a more common perspective, ARRAY_CONTAINS_ALL may be more general. But without this background, it would be difficult to distinguish them based on their names and correctly recognize their respective abilities, which is not user-friendly.

I'm neutral on starting a short discussion in dev maillist to see if others have better ideas. WDYT?;;;","18/Dec/23 10:10;leoyy;Hi [~xuyangzhong] Thank you

Pls do it.;;;","18/Dec/23 11:07;martijnvisser;I think a Dev ML discussion makes sense, I would probably vote for ARRAY_CONTAINS_SEQUENCE ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce a new configuration item 'taskmanager.load-balance.mode',FLINK-33448,13556669,13532014,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,RocMarshal,RocMarshal,RocMarshal,03/Nov/23 09:27,14/Nov/23 09:05,04/Jun/24 20:40,07/Nov/23 09:55,,,,,,,,,,,1.19.0,,,,Runtime / Task,,,,,,0,pull-request-available,,,,,"Introduce a new configuration item 'taskmanager.load-balance.mode' and make it  compatible with ""cluster.evenly-spread-out-slots""

The ticket is mainly to do three items:
 - Introduce a new configuration item 'taskmanager.load-balance.mode'
 - Make it  compatible with ""cluster.evenly-spread-out-slots""

 - Mark ""cluster.evenly-spread-out-slots"" as Deprecated",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 09:55:28 UTC 2023,,,,,,,,,,"0|z1ldvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 09:30;RocMarshal;The Jira ticket resulted from [https://github.com/apache/flink/pull/23635#discussion_r1381212518]

Hi, [~fanrui] would you help to take a look ? 
Thanks a lot~;;;","03/Nov/23 09:55;fanrui;{quote}1. Introduce a new configuration item 'taskmanager.load-balance.mode'

2. Make it  compatible with ""cluster.evenly-spread-out-slots""

3. Mark ""cluster.evenly-spread-out-slots"" as Deprecated
{quote}
It makes sense, please go ahead.

For item2, please note we only update `taskmanager.load-balance.mode` to Slots when the taskmanager.load-balance.mode isn't set and cluster.evenly-spread-out-slots is true.;;;","03/Nov/23 10:21;RocMarshal;Thanks for the reply~ :);;;","07/Nov/23 09:55;fanrui;Merged to master<1.19> via : ed07a70e45a753a259287c9fdbdae73e3415cc21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid CompiledPlan recompilation during loading,FLINK-33447,13556647,13415855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,03/Nov/23 07:51,03/Nov/23 16:35,04/Jun/24 20:40,03/Nov/23 16:35,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,{{StreamPlanner.loadPlan}} recompiles the loaded plan. This causes unnecessary computational overhead and should be removed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 16:35:00 UTC 2023,,,,,,,,,,"0|z1ldqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 16:35;twalthr;Fixed in master: 96a142866a042598bfe85407b46b0871a7b8993a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SubQueryAntiJoinTest#testMultiNotExistsWithCorrelatedOnWhere_NestedCorrelation doesn't produce the correct plan,FLINK-33446,13556624,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,fsk119,fsk119,03/Nov/23 04:00,03/Jan/24 11:36,04/Jun/24 20:40,,1.17.2,1.18.1,1.19.0,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"Although this test doesn't throw an exception, the final plan produces 3 columns rather than 2 after optimization.

{code:java}
LogicalProject(inputs=[0..1], exprs=[[$4]])
+- LogicalFilter(condition=[IS NULL($5)])
   +- LogicalJoin(condition=[AND(=($0, $2), =($1, $3))], joinType=[left])
      :- LogicalProject(exprs=[[+(2, $0), +(3, $1)]])
      :  +- LogicalTableScan(table=[[default_catalog, default_database, l, source: [TestTableSource(a, b, c)]]])
      +- LogicalProject(inputs=[0..2], exprs=[[true]])
         +- LogicalAggregate(group=[{0, 1, 2}])
            +- LogicalProject(inputs=[0..2])
               +- LogicalFilter(condition=[IS NULL($3)])
                  +- LogicalJoin(condition=[true], joinType=[left])
                     :- LogicalFilter(condition=[IS NOT NULL($0)])
                     :  +- LogicalProject(exprs=[[+($0, 1)]])
                     :     +- LogicalTableScan(table=[[default_catalog, default_database, r, source: [TestTableSource(d, e, f)]]])
                     +- LogicalProject(inputs=[0..1], exprs=[[true]])
                        +- LogicalAggregate(group=[{0, 1}])
                           +- LogicalProject(exprs=[[$3, $0]])
                              +- LogicalFilter(condition=[AND(=($1, $0), =(CAST($2):BIGINT, $3))])
                                 +- LogicalProject(exprs=[[+($0, 4), +($0, 5), +($0, 6), CAST(+($0, 6)):BIGINT]])
                                    +- LogicalTableScan(table=[[default_catalog, default_database, t, source: [TestTableSource(i, j, k)]]])

{code}

After digging, I think it's the SubQueryRemoveRule doesn't generate the Correlate but generates the Join node, which causes the failure of the decorrelation. For a quick fix, I think we should throw an exception to notify users it's not a supported feature in the Flink. 

There might exist 2 ways to fix this issue:
1. Expand subquery when converting SQL to rel.  After experimenting with calcite, I found that the Sql2RelConverter generates the correct plan.

{code:java}
LogicalProject(inputs=[0..1])
+- LogicalFilter(condition=[IS NULL($2)])
   +- LogicalCorrelate(correlation=[$cor7], joinType=[left], requiredColumns=[{0, 1}])
      :- LogicalProject(exprs=[[+(2, $0), +(3, $1)]])
      :  +- LogicalTableScan(table=[[default_catalog, default_database, l, source: [TestTableSource(a, b, c)]]])
      +- LogicalAggregate(group=[{}], agg#0=[MIN($0)])
         +- LogicalProject(exprs=[[true]])
            +- LogicalFilter(condition=[AND(=($0, $cor7.d2), IS NULL($1))])
               +- LogicalCorrelate(correlation=[$cor4], joinType=[left], requiredColumns=[{0}])
                  :- LogicalProject(inputs=[0])
                  :  +- LogicalTableScan(table=[[default_catalog, default_database, r, source: [TestTableSource(d1, e, f)]]])
                  +- LogicalAggregate(group=[{}], agg#0=[MIN($0)])
                     +- LogicalProject(exprs=[[true]])
                        +- LogicalFilter(condition=[AND(=($0, $cor4.d1), =($1, $cor4.d1), =(CAST($2):BIGINT, $cor7.d3))])
                           +- LogicalProject(exprs=[[+($0, 4), +($0, 5), +($0, 6)]])
                              +- LogicalTableScan(table=[[default_catalog, default_database, t, source: [TestTableSource(i, j, k)]]])
{code}

You can find the new plan uses a correlate node rather than a join node.

2. CALCITE-5789 has fix this problem by removing the nested correlation node.





",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29540,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 03 11:36:17 UTC 2024,,,,,,,,,,"0|z1ldlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/24 09:37;jeyhunkarimov;Hi [~fsk119] I scanned through the issue a bit. 

I can confirm that {{Sql2RelConverter}} generates the correct plan. The issue IMO is related to the {{RelDecorrelator.decorrelateQuery}} ({{FlinkDecorrelateProgram::optimize}}), specifically this line:


{code:java}
    val result = RelDecorrelator.decorrelateQuery(root)
{code}

the input plan ({{root}}) is:


{code:java}
LogicalProject(d2=[$0], d3=[$1])
  LogicalProject(d2=[$0], d3=[$1])
    LogicalFilter(condition=[IS NULL($2)])
      LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0, 1}])
        LogicalProject(d2=[+(2, $0)], d3=[+(3, $1)])
          LogicalTableScan(table=[[default_catalog, default_database, l, source: [TestTableSource(a, b, c)]]])
        LogicalAggregate(group=[{0}])
          LogicalProject(i=[true])
            LogicalProject(d1=[$0])
              LogicalFilter(condition=[AND(=($0, $cor0.d2), IS NULL($1))])
                LogicalJoin(condition=[true], joinType=[left], variablesSet=[[$cor1, $cor0]])
                  LogicalProject(d1=[+($0, 1)])
                    LogicalTableScan(table=[[default_catalog, default_database, r, source: [TestTableSource(d, e, f)]]])
                  LogicalAggregate(group=[{0}])
                    LogicalProject(i=[true])
                      LogicalFilter(condition=[AND(=($0, $cor1.d1), =($1, $cor1.d1), =(CAST($2):BIGINT, $cor0.d3))])
                        LogicalProject(d4=[+($0, 4)], d5=[+($0, 5)], d6=[+($0, 6)])
                          LogicalTableScan(table=[[default_catalog, default_database, t, source: [TestTableSource(i, j, k)]]])
{code}


and the output of the function call ({{RelDecorrelator.decorrelateQuery}}) is:


{code:java}
LogicalProject(d2=[$0], d3=[$1], d4=[$4])
  LogicalFilter(condition=[IS NULL($5)])
    LogicalJoin(condition=[AND(=($0, $2), =($1, $3))], joinType=[left])
      LogicalProject(d2=[+(2, $0)], d3=[+(3, $1)])
        LogicalTableScan(table=[[default_catalog, default_database, l, source: [TestTableSource(a, b, c)]]])
      LogicalProject(d11=[$0], $f3=[$1], d4=[$2], $f4=[true])
        LogicalAggregate(group=[{0, 1, 2}])
          LogicalProject(d11=[$0], $f3=[$1], d4=[$2])
            LogicalFilter(condition=[IS NULL($3)])
              LogicalJoin(condition=[true], joinType=[left])
                LogicalFilter(condition=[IS NOT NULL($0)])
                  LogicalProject(d1=[+($0, 1)])
                    LogicalTableScan(table=[[default_catalog, default_database, r, source: [TestTableSource(d, e, f)]]])
                LogicalProject($f3=[$0], d4=[$1], $f2=[true])
                  LogicalAggregate(group=[{0, 1}])
                    LogicalProject($f3=[$3], d4=[$0])
                      LogicalFilter(condition=[AND(=($1, $0), =(CAST($2):BIGINT, $3))])
                        LogicalProject(d4=[+($0, 4)], d5=[+($0, 5)], d6=[+($0, 6)], $f3=[CAST(+($0, 6)):BIGINT])
                          LogicalTableScan(table=[[default_catalog, default_database, t, source: [TestTableSource(i, j, k)]]])

{code}

WDYT?

;;;","03/Jan/24 11:36;fsk119;?? {{Sql2RelConverter}} generates the correct plan??

Yes I think you are right. I think it's calcite's bug and we need to upgrade calcite version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate DataSet migration guideline to Chinese,FLINK-33445,13556614,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,Wencong Liu,Wencong Liu,03/Nov/23 02:15,13/Nov/23 09:43,04/Jun/24 20:40,13/Nov/23 09:43,1.19.0,,,,,,,,,,1.19.0,,,,chinese-translation,,,,,,0,pull-request-available,starter,,,,"The [FLIINK-33041|https://issues.apache.org/jira/browse/FLINK-33041] about adding an introduction about how to migrate DataSet API to DataStream has been merged into master branch. Here is the [LINK|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/dataset_migration/] in the Flink website.

According to the [contribution guidelines|https://flink.apache.org/how-to-contribute/contribute-documentation/#chinese-documentation-translation], we should add an identical markdown file in {{content.zh/}} and translate it to Chinese. Any community volunteers are welcomed to take this task.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 13 09:43:35 UTC 2023,,,,,,,,,,"0|z1ldjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 02:34;liyubin117;[~Wencong Liu] Could you please assign this to me? thanks!;;;","03/Nov/23 02:39;Wencong Liu;Thanks [~liyubin117] ! Assigned to you. Please go ahead.;;;","13/Nov/23 09:43;Weijie Guo;master(1.19) via 753f542d175d03f8cb96b44b9afc6b0d3f1453e3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fixing the stability of ""org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.testUDTF""",FLINK-33444,13556611,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,krishaudt14,krishaudt14,03/Nov/23 01:18,09/Nov/23 13:57,04/Jun/24 20:40,09/Nov/23 13:57,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,,,"We are proposing a solution to fix potential flakiness in the following test:
{code:java}
org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase#testUDTF {code}
*STEPS TO REPRODUCE THE ISSUE:*
 * The following command can be run to execute the test with the [NonDex|https://github.com/TestingResearchIllinois/NonDex] plugin:

{code:java}
mvn -pl flink-connectors/flink-connector-hive edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase#testUDTF
{code}

 * The following error will be encountered:

{code:java}
[ERROR] Failures: 
[ERROR]   TableEnvHiveConnectorITCase.testUDTF:270->lambda$testUDTF$5:305 
expected: ""[+I[{1=a, 2=b}], +I[{3=c}]]""
 but was: ""[+I[{2=b, 1=a}], +I[{3=c}]]""
[INFO] 
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped
{code}

*ROOT CAUSE ANALYSIS:*
The test is currently flaky because of the assumption that the order of elements received in the _results_ variable will be consistent. There are currently two versions of query output that can be stored in _results_ :

# The actual order that is expected where the output of the map attribute is {1=a,2=b}
# The other order which is shown in the error extract above where the ordering of the map attribute changes to {2=b,1=a}

*POTENTIAL FIX:*
* The fix that I can suggest/have ready to raise a PR for is introducing another assertion on the second variant of the query output.
* By asserting whether the contents in _results_ are in one of the two orders, we can ascertain that the expected attributes with their contents are received as expected should the order in which they are received, not matter.

Please share your thoughts on this finding and let me know if any other potential fix is possible for this test.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 13:57:35 UTC 2023,,,,,,,,,,"0|z1ldio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 13:57;martijnvisser;See FLINK-33443 for the same comment;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Make the test ""testWriteComplexType"" stable",FLINK-33443,13556597,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Cannot Reproduce,,krishaudt14,krishaudt14,02/Nov/23 23:37,27/Nov/23 16:03,04/Jun/24 20:40,03/Nov/23 12:50,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,,,"We are proposing to make the following test stable:
{code:java}
org.apache.flink.connectors.hive.HiveRunnerITCase.testWriteComplexType{code}
*STEPS TO REPRODUCE THE ISSUE:*
 * The following command can be run to execute the test with the [NonDex|https://github.com/TestingResearchIllinois/NonDex] plugin:

{code:java}
mvn -pl flink-connectors/flink-connector-hive edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=org.apache.flink.connectors.hive.HiveRunnerITCase#testWriteComplexType {code}
 * The following error will be encountered:

{code:java}
[ERROR] Failures: 
[ERROR]   HiveRunnerITCase.testWriteComplexType:166 
expected: ""[1,2,3]	{1:""a"",2:""b""}	{""f1"":3,""f2"":""c""}""
 but was: ""[1,2,3]	{2:""b"",1:""a""}	{""f1"":3,""f2"":""c""}""
[INFO] 
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0{code}
*ROOT CAUSE ANALYSIS:*
The test is currently flaky because of the assumption that the order of elements received in the _result_ variable will be consistent. There are currently two versions of query output that can be stored in _result._
 # The actual order that is expected where the output of the map attribute is \{1: ""a"", 2: ""b""}.
 # The other order is the one shown in the error extract above where the ordering of the map attribute from the table is \{2: ""b"", 1: ""a""}.

*POTENTIAL FIX:*
 * The fix that I can suggest/have ready to raise a PR for is introducing another assertion on the second variant of the query output.
 * By asserting on whether the contents in _result_ are in one of the two orders, we can ascertain that the expected attributes with their contents are received as expected should the order in which they are received, not matter.

Please share your thoughts on this finding and let me know if any other potential fix is possible for this test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 27 16:02:19 UTC 2023,,,,,,,,,,"0|z1ldfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 12:50;martijnvisser;[~krishaudt14] There's very little/no value in tickets like these. We run CI for every PR, every merged commit, and nightly builds, and test instabilities are logged. If we can't reproduce this in our CI, then let's not spend time on these tickets but focus on the other open tickets. I'm closing this as Can't Reproduce since this requires an additional plugin to be reproduced, which we aren't using in Flink. ;;;","26/Nov/23 21:19;hannahchi;Hi Martijn : I can understand that currently this problem is not exposed in the CI, because this kind of 80% problem we detected exposed mainly because the change of the test environment (and more dangerous for lower-version JDK), while for the CI the test environment won't change. In detail, ~50% of problem we address is because the non-deterministic of the Hash (Set and Map), this is one example Apache repo [wicket|https://github.com/apache/wicket/commit/ed64e166dcba6715eafcbb7ca460d2b87e84cffc] had encountered and addressed, this is kind of problem resolved for the long-term gain :) . Here is a list of this kind of this problem : https://github.com/TestingResearchIllinois/idoft;;;","27/Nov/23 08:11;martijnvisser;[~hannahchi] Looking at the number of open tickets with actually reported test instabilities, I don't see why adding more tickets that don't have a direct reproducer is valuable for the project. It just creates noise for the maintainers who are already trying their best to keep Flink in a good shape;;;","27/Nov/23 15:49;martijnvisser;[~hannahchi] I feel like you're not actually responding to my comment, but just trying to convince me/the community that we should use your tool. I'm more then happy to have a meaningful discussion, but I don't think that's currently happening so I'll refrain from commenting, but I will close more tickets if they follow the same paradigm as I've seen recently. ;;;","27/Nov/23 16:02;hannahchi;Okay! have a good day! :D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException thrown from RocksDBIncrementalRestoreOperation,FLINK-33442,13556591,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,roman,roman,roman,02/Nov/23 21:45,03/Nov/23 10:44,04/Jun/24 20:40,03/Nov/23 10:44,1.17.1,,,,,,,,,,1.17.2,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,,,"When using the new rescaling API, it's possible to get
{code:java}
2023-10-31 18:25:05,179 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder [] - Caught unexpected exception.
java.lang.UnsupportedOperationException: null
	at java.util.Collections$1.remove(Collections.java:4714) ~[?:?]
	at java.util.AbstractCollection.remove(AbstractCollection.java:299) ~[?:?]
	at org.apache.flink.runtime.checkpoint.StateObjectCollection.remove(StateObjectCollection.java:105) ~[flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:294) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:167) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:735) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:710) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:676) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at java.lang.Thread.run(Thread.java:829) [?:?]
2023-10-31 18:25:05,182 WARN  org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Exception while restoring keyed state backend for KeyedProcessOperator_353a6b34b8b7f1c1d0fb4616d911049c_(1/2) from alternative (1/2), will retry while more alternatives are available.
org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:407) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:338) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:355) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:166) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:735) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:710) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:676) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.17.1-143.jar:1.17.1-143]
	at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.lang.UnsupportedOperationException
	at java.util.Collections$1.remove(Collections.java:4714) ~[?:?]
	at java.util.AbstractCollection.remove(AbstractCollection.java:299) ~[?:?]
	at org.apache.flink.runtime.checkpoint.StateObjectCollection.remove(StateObjectCollection.java:105) ~[flink-runtime-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithRescaling(RocksDBIncrementalRestoreOperation.java:294) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:167) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327) ~[flink-dist-1.17.1-143.jar:1.17.1-143]
	... 18 more

{code}
presumably on upscaling.
The job continues to recover (using the remote state).

The issue occurs on 1.17 and should be fixed in 1.18 and master.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-02 21:45:40.0,,,,,,,,,,"0|z1lde8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for ExecUnion node,FLINK-33441,13556582,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,02/Nov/23 19:33,05/Feb/24 13:30,04/Jun/24 20:40,12/Dec/23 13:48,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 12 13:48:49 UTC 2023,,,,,,,,,,"0|z1ldc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Dec/23 13:48;dwysakowicz;Implemented in f362dcc9d4e14cfa30a27881158ec9431dd9e274..f2460363303e49621589b1cb2b45347c8ee5dd4f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump flink version on flink-connectors-hbase,FLINK-33440,13556548,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,ferenc-csaky,ferenc-csaky,02/Nov/23 15:45,16/Apr/24 13:13,04/Jun/24 20:40,16/Apr/24 13:13,,,,,,,,,,,hbase-4.0.0,,,,Connectors / HBase,,,,,,0,pull-request-available,,,,,Follow-up the 1.18 release in the connector repo as well.,,,,,,,,,,,,,,,,,,,,,,FLINK-34003,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 16 13:13:07 UTC 2024,,,,,,,,,,"0|z1ld4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Apr/24 13:13;Sergey Nuyanzin;Merged as [08b7b69cd82acf3e8ba9af08d715b0b9616af0b0|https://github.com/apache/flink-connector-hbase/commit/08b7b69cd82acf3e8ba9af08d715b0b9616af0b0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement type inference for IN function,FLINK-33439,13556541,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,02/Nov/23 14:39,08/Nov/23 10:17,04/Jun/24 20:40,08/Nov/23 10:17,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 08 10:17:07 UTC 2023,,,,,,,,,,"0|z1ld34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/23 10:17;dwysakowicz;Implemented in 648b6bac95232c8498b392ac01e5089777553c77;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveITCase.testHiveDialect and HiveITCase.testReadWriteHive are failing,FLINK-33438,13556528,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,02/Nov/23 13:45,09/Nov/23 15:43,04/Jun/24 20:40,09/Nov/23 15:43,,,,,,,,,,,,,,,Connectors / Hive,Tests,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6729006580/job/18289544587#step:15:12706

{code}
Error: 08:09:00 08:09:00.159 [ERROR] org.apache.flink.tests.hive.HiveITCase.testHiveDialect  Time elapsed: 43.377 s  <<< FAILURE!
Nov 02 08:09:00 org.opentest4j.AssertionFailedError: Did not get expected results before timeout, actual result: null. ==> expected: <true> but was: <false>
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Nov 02 08:09:00 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
Nov 02 08:09:00 	at org.apache.flink.tests.hive.HiveITCase.checkResultFile(HiveITCase.java:204)
Nov 02 08:09:00 	at org.apache.flink.tests.hive.HiveITCase.runAndCheckSQL(HiveITCase.java:161)
Nov 02 08:09:00 	at org.apache.flink.tests.hive.HiveITCase.testHiveDialect(HiveITCase.java:131)
Nov 02 08:09:00 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
Nov 02 08:09:00 
Error: 08:09:00 08:09:00.159 [ERROR] org.apache.flink.tests.hive.HiveITCase.testReadWriteHive  Time elapsed: 37.006 s  <<< FAILURE!
Nov 02 08:09:00 org.opentest4j.AssertionFailedError: Did not get expected results before timeout, actual result: null. ==> expected: <true> but was: <false>
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
Nov 02 08:09:00 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
Nov 02 08:09:00 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
Nov 02 08:09:00 	at org.apache.flink.tests.hive.HiveITCase.checkResultFile(HiveITCase.java:204)
Nov 02 08:09:00 	at org.apache.flink.tests.hive.HiveITCase.runAndCheckSQL(HiveITCase.java:161)
Nov 02 08:09:00 	at org.apache.flink.tests.hive.HiveITCase.testReadWriteHive(HiveITCase.java:121)
Nov 02 08:09:00 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32294,FLINK-30064,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 15:43:12 UTC 2023,,,,,,,,,,"0|z1ld08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 13:46;mapohl;We see the same failure in FLINK-32294. It looks like some file permissions needed to be set.;;;","02/Nov/23 13:54;mapohl;fyi: FLINK-30064 is about removing the Hive connector which would make this issue obsolete.;;;","03/Nov/23 07:01;mapohl;* https://github.com/XComp/flink/actions/runs/6737717931/job/18316247457#step:15:13398
* https://github.com/XComp/flink/actions/runs/6737982594/job/18316903763#step:15:13621
* https://github.com/XComp/flink/actions/runs/6738075474/job/18317316061#step:16:14204;;;","09/Nov/23 15:43;mapohl;Fixed by not running the e2e tests in Docker;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink 1.17 sink commited legacy Committable state, but it was not removed from state backend",FLINK-33437,13556521,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dyccode,dyccode,dyccode,02/Nov/23 12:13,23/Jan/24 14:33,04/Jun/24 20:40,23/Jan/24 14:33,1.17.1,,,,,,,,,,1.19.0,,,,Connectors / Kafka,Runtime / Checkpointing,,,,,0,pull-request-available,,,,,"My Flink job graph: kafka source -> process -> kafka sink.

I used savepoint to upgrade Flink 1.14.5 to 1.17.1, and the program ran normally.A month later, I restarted the Flink job using a savepoint, and the job was started normally.Unfortunately, the Flink job failed every time when it did a checkpoint.For example the following scenario:
 
 # The program uses Kafka sink
 # Suspend flink job with savepoint A, and Flink Version is 1.14.x
 # Recover the job with savepoint A, and Flink Version is 1.17.1
 # Wait for time longer than {{transactional.id.expiration.ms}} + {{transaction.remove.expired.transaction.cleanup.interval.ms}}
 # Suspend flink job with savepoint B, and Flink Version is 1.17.1
 # Recover the job with savepoint B, and Flink Version is 1.17.1
 # Trigger checkpoint ,the Flink job will fail with the following error:
{code:java}
java.io.IOException: Could not perform checkpoint 1009710 for operator kafka-sink: Committer (2/2)#2.
    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1256)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:488)
    at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.triggerGlobalCheckpoint(AbstractAlignedBarrierHandlerState.java:74)
    at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlignedBarrierHandlerState.barrierReceived(AbstractAlignedBarrierHandlerState.java:66)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:118)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:96)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:75)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emit(SinkWriterOperator.java:245)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.emitCommittables(SinkWriterOperator.java:215)
    at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.prepareSnapshotPreBarrier(SinkWriterOperator.java:177)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.prepareSnapshotPreBarrier(RegularOperatorChain.java:89)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:321)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$14(StreamTask.java:1299)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1287)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1244)
    ... 22 more
Caused by: java.lang.IllegalStateException: Failed to commit KafkaCommittable{producerId=47326303, epoch=0, transactionalId=kafka-sink-xxx-1-882965}
    at org.apache.flink.streaming.runtime.operators.sink.committables.CommitRequestImpl.signalFailedWithUnknownReason(CommitRequestImpl.java:77)
    at org.apache.flink.connector.kafka.sink.KafkaCommitter.commit(KafkaCommitter.java:119)
    at org.apache.flink.streaming.runtime.operators.sink.committables.CheckpointCommittableManagerImpl.commit(CheckpointCommittableManagerImpl.java:126)
    at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmit(CommitterOperator.java:177)
    at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.commitAndEmitCheckpoints(CommitterOperator.java:161)
    at org.apache.flink.streaming.runtime.operators.sink.CommitterOperator.processElement(CommitterOperator.java:200)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:94)
    ... 33 more
Caused by: org.apache.kafka.common.errors.InvalidPidMappingException: The producer attempted to use a producer id which is not currently assigned to its transactional id.
 {code}","K8s, Flink 1.17.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,,Tue Jan 23 14:33:58 UTC 2024,,,,,,,,,,"0|z1lcyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/23 07:48;dyccode;I have already solved this problem, and tested in our cluster.Can committers assign this Jira ticket to me?;;;","06/Nov/23 08:32;masteryhx;[~dyccode] Thanks for reporting this.

Already assigned to you, please go ahead.;;;","08/Nov/23 07:28;dyccode;[~masteryhx] I have submitted the PR, can you help me review the code . Thanks.

 ;;;","23/Jan/24 14:33;masteryhx;merged efbd8c40 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation on the built-in Profiler,FLINK-33436,13556501,13554911,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,02/Nov/23 09:26,01/Mar/24 09:28,04/Jun/24 20:40,01/Mar/24 09:28,1.19.0,,,,,,,,,,1.19.0,1.20.0,,,Documentation,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 01 09:28:34 UTC 2024,,,,,,,,,,"0|z1lcu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/24 09:28;yunta;master: 5f06ce765256b375945b9e69db2f16123b53f194
release-1.19: 12ea64c0e2a56da3c5f6a656b23a2f2ac54f19d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The visualization and download capabilities of profiling history ,FLINK-33435,13556500,13554911,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,Yu Chen,Yu Chen,Yu Chen,02/Nov/23 09:24,30/Nov/23 11:57,04/Jun/24 20:40,30/Nov/23 11:57,1.19.0,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 30 11:56:32 UTC 2023,,,,,,,,,,"0|z1lcu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 11:56;Yu Chen;This subtask will be completed in FLINK-33433 and FLINK-33434. So I'll close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support invoke async-profiler on Taskmanager through REST API,FLINK-33434,13556499,13554911,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,02/Nov/23 09:21,18/Jan/24 12:26,04/Jun/24 20:40,18/Jan/24 12:26,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 18 12:26:16 UTC 2024,,,,,,,,,,"0|z1lcts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/24 12:26;yunta;merged in master: 525f6bc818eb7f15a19fa81421584920de8f8876 ... 4bee4e6e8ddb41ae9933d04bf21183223db6c2de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support invoke async-profiler on Jobmanager through REST API,FLINK-33433,13556498,13554911,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yu Chen,Yu Chen,Yu Chen,02/Nov/23 09:20,02/Jan/24 13:51,04/Jun/24 20:40,02/Jan/24 13:51,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 02 13:51:15 UTC 2024,,,,,,,,,,"0|z1lctk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jan/24 13:51;yunta;merged in master: 240494fd6169cb98b47808a003ee00804a780360...3efe9d2b09bedde89322594f0f3927004b6b1adf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build artifacts cannot be extracted smoothly,FLINK-33432,13556489,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,02/Nov/23 07:58,13/Nov/23 14:43,04/Jun/24 20:40,13/Nov/23 14:43,,,,,,,,,,,,,,,Build System / CI,,,,,,0,github-actions,,,,,"When extracting the build artifacts there is always a question to overwrite certain files:
{code}
[...]
  inflating: logs_117/6_ci  Test (module misc).txt  
replace logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y
  inflating: logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip  
[...]
  inflating: logs_117/1_ci  Test (module python).txt  
replace logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y
  inflating: logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip  
[...]
  inflating: logs_117/4_ci  Test (module connect_2).txt  
replace logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y
  inflating: logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip  
[...]
  inflating: logs_117/3_ci  Test (module connect_1).txt  
replace logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y
  inflating: logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip  
[...]  inflating: logs_117/5_ci  Test (module tests).txt  
replace logs_117/runner-diagnostic-logs/Build UnknownBuildNumber-test.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename
[...]
{code}

The file sizes indicate that these files are not the same and, therefore, should be preserved:
{code}
unzip -l logs_117.zip| grep runner
        0  2023-11-02 07:55   runner-diagnostic-logs/
    48407  2023-11-02 07:55   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:55   runner-diagnostic-logs/
    29114  2023-11-02 07:55   runner-diagnostic-logs/Build UnknownBuildNumber-Compile.zip
        0  2023-11-02 07:55   runner-diagnostic-logs/
    30963  2023-11-02 07:55   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:55   runner-diagnostic-logs/
    63016  2023-11-02 07:55   runner-diagnostic-logs/Build UnknownBuildNumber-e2e.zip
        0  2023-11-02 07:55   runner-diagnostic-logs/
    50431  2023-11-02 07:55   runner-diagnostic-logs/Build UnknownBuildNumber-e2e.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
    21525  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-Basic QA.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
    37758  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
    45392  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
        0  2023-11-02 07:56   runner-diagnostic-logs/
    31679  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
    39000  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
    41425  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-test.zip
        0  2023-11-02 07:56   runner-diagnostic-logs/
    18562  2023-11-02 07:56   runner-diagnostic-logs/Build UnknownBuildNumber-docs-404-check.zip
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/23 08:02;mapohl;logs_117.zip;https://issues.apache.org/jira/secure/attachment/13064101/logs_117.zip",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 13 14:43:47 UTC 2023,,,,,,,,,,"0|z1lcrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 08:00;mapohl;additionally, having spaces in names doesn't help browsing the directory structure:
{code}
$ find . -type d
.
./ci  Test (module tests)
./ci  Test (module connect_2)
./runner-diagnostic-logs
./ci  Test (module table)
./ci  Test (module python)
./ci  Test (module core)
./ci  Basic QA
./ci  Test packaginglicensing
./ci  Test (module connect_1)
./ci  E2E (group 2)
./ci  E2E (group 1)
./ci  Compile
./ci  Test (module misc)
./docs-404-check
./ci  Check Code modified
{code};;;","02/Nov/23 08:02;mapohl;There seem to be duplicated files?
{code}
$ grep -Hirn CheckpointAfterAllTasksFinishedITCase
ci  Test (module tests)/12_Test - tests.txt:8387:2023-11-01T19:09:57.2578029Z Nov 01 19:09:57 19:09:57.257 [INFO] Running org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase
ci  Test (module tests)/12_Test - tests.txt:8388:2023-11-01T19:10:07.6672482Z Nov 01 19:10:07 19:10:07.666 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.371 s <<< FAILURE! - in org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase
ci  Test (module tests)/12_Test - tests.txt:8389:2023-11-01T19:10:07.6687368Z Nov 01 19:10:07 19:10:07.666 [ERROR] org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished  Time elapsed: 2.171 s  <<< ERROR!
ci  Test (module tests)/12_Test - tests.txt:8393:2023-11-01T19:10:07.6696099Z Nov 01 19:10:07   at org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished(CheckpointAfterAllTasksFinishedITCase.java:121)
ci  Test (module tests)/12_Test - tests.txt:9215:2023-11-01T19:36:01.4159325Z Nov 01 19:36:01 19:36:01.412 [ERROR]   CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished:121 » Execution
5_ci  Test (module tests).txt:24772:2023-11-01T19:09:57.2578077Z Nov 01 19:09:57 19:09:57.257 [INFO] Running org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase
5_ci  Test (module tests).txt:24773:2023-11-01T19:10:07.6672547Z Nov 01 19:10:07 19:10:07.666 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.371 s <<< FAILURE! - in org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase
5_ci  Test (module tests).txt:24774:2023-11-01T19:10:07.6687697Z Nov 01 19:10:07 19:10:07.666 [ERROR] org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished  Time elapsed: 2.171 s  <<< ERROR!
5_ci  Test (module tests).txt:24778:2023-11-01T19:10:07.6696104Z Nov 01 19:10:07        at org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished(CheckpointAfterAllTasksFinishedITCase.java:121)
5_ci  Test (module tests).txt:25600:2023-11-01T19:36:01.4159372Z Nov 01 19:36:01 19:36:01.412 [ERROR]   CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished:121 » Execution
{code}

I'm gonna add the build artifacts of the [analyzed build|https://github.com/XComp/flink/actions/runs/6723279744/job/18273667605] into this Jira issue for repeatability purposes.;;;","13/Nov/23 14:43;mapohl;Update: The archive in question (in contrast to the archives that are downloaded through the build artifacts section) can be downloaded by doing the following steps:
1. Open any job log view
2. Click the settings icon on the upper right corner of the logs text pane (next to the ""Re-run this job"" button)
3. Click ""Download log archive"";;;","13/Nov/23 14:43;mapohl;Closing this issue: It's a GHA ""feature"". The relevant logs are provided through the build artifacts section.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create restore tests for ExecNodes,FLINK-33431,13556487,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,bvarghese,dwysakowicz,dwysakowicz,02/Nov/23 07:49,02/Nov/23 07:49,04/Jun/24 20:40,02/Nov/23 07:49,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,,,,,,As a follow up to FLINK-25217 we should create tests for restoring all {{ExecNodes}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-02 07:49:20.0,,,,,,,,,,"0|z1lcr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Test Failures,FLINK-33430,13556482,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dserfe,dserfe,02/Nov/23 06:51,02/Nov/23 22:49,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"This issue is to fix test failures caused by flakiness.

Steps to reproduce test failures:
1)Run the following commands

{code:bash}
mvn -pl flink-tests edu.illinois:nondex-maven-plugin:2.1.1:nondex -Dtest=org.apache.flink.test.misc.GenericTypeInfoTest#testSerializerTree
{code}

 
2) Then we found the following test failures:
{code:bash}
[ERROR] GenericTypeInfoTest.testSerializerTree:53 Expected: ""GenericTypeInfo (PojoWithCollectionGeneric)\n pojos:java.util.List\n key:int\n sqlDate:java.sql.Date\n bigInt:java.math.BigInteger\n bigDecimalKeepItNull:java.math.BigDecimal\n intVal:java.math.BigInteger\n scale:int\n scalaBigInt:scala.math.BigInt\n bigInteger:java.math.BigInteger\n mixed:java.util.List\n makeMeGeneric:org.apache.flink.test.operators.util.CollectionDataSets$PojoWithDateAndEnum\n group:java.lang.String\n date:java.util.Date\n cat:org.apache.flink.test.operators.util.CollectionDataSets$Category (is enum)\n"" but: was ""GenericTypeInfo (PojoWithCollectionGeneric)\n key:int\n bigDecimalKeepItNull:java.math.BigDecimal\n intVal:java.math.BigInteger\n scale:int\n bigInt:java.math.BigInteger\n sqlDate:java.sql.Date\n pojos:java.util.List\n scalaBigInt:scala.math.BigInt\n bigInteger:java.math.BigInteger\n makeMeGeneric:org.apache.flink.test.operators.util.CollectionDataSets$PojoWithDateAndEnum\n date:java.util.Date\n cat:org.apache.flink.test.operators.util.CollectionDataSets$Category (is enum)\n group:java.lang.String\n mixed:java.util.List\n""   {code}
The root cause of the test failure is due to assuming a consistent order of {{{}serTree{}}}, but the elements of it are actually not consistent, to fix the test failure:

_change the assertion to compare two lists, which can ignore the orders of elements_",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 22:44:38 UTC 2023,,,,,,,,,,"0|z1lcq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 14:18;mapohl;Thanks for bringing this up, [~dserfe]. I don't fully understand the intention of this Jira issue, though. We're not using {{edu.illinois:nondex-maven-plugin:2.1.1:nondex}} as part of our build system. Therefore, the test instability your mentioning isn't present. Looking through the code, it appears that the production code is also deterministically generating the String (I haven't checked the entire code, though, tbh).

Based on what I found about the {{edu.illinois:nondex-maven-plugin}} (from [this Github source|https://github.com/TestingResearchIllinois/NonDex]), it verifies that the code behaves in the same way among different JVM implementations. Is this correct?

If you think that it makes sense to add this plugin to Flink's build pipeline we should actually approach this on a more systematic level: 
1. Discussing why this plugin should be added in the first place (e.g. in the dev ML). Where are the benefits to the Flink users?
2. If the community agrees we should add the plugin to the build pipeline and fix all tests in one go.

WDYT?;;;","02/Nov/23 22:44;dserfe;Hi Matthias, thanks for your comments!

To clarify:
1) This PR is to fix a flaky test which can lead to potential test failures, the test failures may happen even when running the test suite (maybe multiple times) without NonDex. 
> it verifies that the code behaves in the same way among different JVM implementations.  Is this correct?
Yes, NonDex is designed to detect such potential issues, so the issues can be fixed to avoid test failures happening. It's fine not to add this plugin because it will have some overheads, but I believe it would be good for the builds if the test failures could be fixed.

2) Root cause of test failures:
[testSerializerTree|#L35] calls method getSerializerTree(Line 43) to return a string, and then assert it equals to an expected one. In line 291 of [getSerializerTree|#L270], it calls another method getGenericTypeTree([https://github.com/dserfe/flink/blob/a4ad86fb083f90503938a9c3d816cdda9dc22427/flink-java/src/main/java/org/apache/flink/api/java/Utils.java#L301])  that leverages API getDeclaredFields to return an array containing the names of Field objects reflecting all the declared fields. However, the elements in the array returned are not sorted and are not in any particular order (from the [documentation|[https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html]]). When iterating the array, the elements are inserted to a string and returned. As a result, testSerializerTree asserts a non-deterministic string equals a deterministic one, so the test will fail during some runs.

If you don't like the proposed fix, please let me know and we can discuss other fixes!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metric collection during stabilization phase may error due to missing metrics,FLINK-33429,13556395,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mxm,mxm,mxm,01/Nov/23 14:08,03/Nov/23 15:59,04/Jun/24 20:40,03/Nov/23 14:44,kubernetes-operator-1.7.0,,,,,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,,,,,,0,pull-request-available,,,,,"The new code for the 1.7.0 release introduces metric collection during the stabilization phase to allow sampling the observed true processing rate. Metrics might not be fully initialized during that phase, as evident through the error metrics. The following error is thrown: 

{noformat}
java.lang.RuntimeException: Could not find required metric NUM_RECORDS_OUT_PER_SEC for 667f5d5aa757fb217b92c06f0f5d2bf2 
{noformat}

To prevent these errors shadowing actual errors, we should detect and ignore this recoverable exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 15:59:50 UTC 2023,,,,,,,,,,"0|z1lc6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 14:44;fanrui;Merged master<1.7.0> via :  03c9eb6f57d5d61f68cd93ff765b663d13b01ffe;;;","03/Nov/23 15:59;mxm;Thanks for closing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flink SQL CEP support 'followed','notNext' and 'notFollowedBy' semantics",FLINK-33428,13556375,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,shaw-valjean,shaw-valjean,01/Nov/23 11:21,16/Nov/23 07:04,04/Jun/24 20:40,,1.16.0,,,,,,,,,,,,,,Table SQL / API,Table SQL / Planner,,,,,0,pull-request-available,,,,,"Currently, the cep mode of the Flink API can support next, notNext, followedBy, followedByAny, and notFollowedBy semantics, but Flink SQL only supports next semantics. The remaining notNext and followedBy semantics are implemented by other alternatives, while the notFollowedBy semantics are not currently implemented. At present, this semantics is generally implemented in business scenarios, such as judging that a user has placed an order within 15 minutes without paying. Therefore, I suggest to provide new functionality to support notFollowedBy in sql mode, along with the other three semantics

The syntax of enhanced MATCH_RECOGNIZE is proposed as follows:
{code:sql}
MATCH_RECOGNIZE (
    [ PARTITION BY <expr> [, ... ] ]
    [ ORDER BY <expr> [, ... ] ]
    [ MEASURES <expr> [AS] <alias> [, ... ] ]
    [ ONE ROW PER MATCH [ { SHOW TIMEOUT MATCHES } ] |
      ALL ROWS PER MATCH [ { SHOW TIMEOUT MATCHES } ]
    ]
    [ AFTER MATCH SKIP
          {
          PAST LAST ROW   |
          TO NEXT ROW   |
          TO [ { FIRST | LAST} ] <symbol>
          }
    ]
    PATTERN ( <pattern> )
    DEFINE <symbol> AS <expr> [, ... ]
)
{code}
* [^ <symbol>] is proposed in <pattern> to express the notNext semantic. For example, A [^B] is translated to A.notNext(B).
* \{- <symbol> -} is proposed in <pattern> to express the followedBy semantic. For example, A \{ B*? -} C is translated to A.followedBy(C).
* \{- symbol1 \-} with [^ <symbol2>] is proposed in <pattern> to express the notFollowedBy semantic. For example, A \{- B*? -} [^C] is translated to A.notFollwedBy(B).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 16 07:04:38 UTC 2023,,,,,,,,,,"0|z1lc28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/23 12:42;martijnvisser;[~shaw-valjean] This sounds very promising, but can you make sure that you propose this first on the Dev mailing list, perhaps even as part of a FLIP since it introduces new syntax? ;;;","06/Nov/23 11:46;shaw-valjean;[~martijnvisser] hi，I have already sent a [discuss] email to dev@flink.apache.org. As mentioned in the email, some of the features I mentioned are briefly described in jira(https://issues.apache.org/jira/browse/FLINK-10592), and some are not. Do I need to handle it on the previous jira?;;;","16/Nov/23 07:04;shaw-valjean;[~martijnvisser]  Could you please help me look at this problem? I have sent several emails, but have not received any reply so far;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mark new relocated autoscaler configs IGNORE in the operator,FLINK-33427,13556370,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,01/Nov/23 10:24,02/Nov/23 18:00,04/Jun/24 20:40,02/Nov/23 18:00,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"The operator currently only ignores ""kubrernetes.operator"" prefixed configs to not trigger upgrades. Autoscaler configs should also fall in this category.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 18:00:32 UTC 2023,,,,,,,,,,"0|z1lc14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 18:00;mbalassi;[{{81a2d99}}|https://github.com/apache/flink-kubernetes-operator/commit/81a2d993dffb6b193a582fcc0f08b28e5bb1073d] in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If the directory does not have the read permission, an exception cannot be thrown, when read this path.",FLINK-33426,13556363,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wenhao.yu,wenhao.yu,01/Nov/23 09:40,01/Nov/23 10:47,04/Jun/24 20:40,,1.13.5,1.17.1,,,,,,,,,1.17.1,,,,API / DataStream,,,,,,0,,,,,,"When I use StreamExecutionEnvironment.ReadFile () this API, found that while reading on HDFS directory if the directory does not give permission to read, then this exception is not thrown, task would have been run, the outside world will not perceive the task status.",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 01 10:47:24 UTC 2023,,,,,,,,,,"0|z1lbzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/23 10:47;martijnvisser;[~wenhao.yu] Have you tried using the regular FileSource, since readFile is deprecated (see https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/api/java/ExecutionEnvironment.html#readFile-org.apache.flink.api.common.io.FileInputFormat-java.lang.String-)? This probably will be a Won't Fix ticket. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL doesn't support a inline field in struct type as primary key,FLINK-33425,13556354,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elakiya,elakiya,01/Nov/23 08:10,01/Nov/23 08:10,04/Jun/24 20:40,,1.16.2,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"I have a Kafka topic named employee which uses confluent avro schema and will emit the payload as below:

{
""employee"": {
""id"": ""123456"",
""name"": ""sampleName""
}
}
I am using the upsert-kafka connector to consume the events from the above Kafka topic as below using the Flink SQL DDL statement, also here I want to use the id field as the Primary key. But I am unable to use the id field since it is inside the object and currently Flink doesn't support this feature. I am using Apache Flink 16.2 and its dependent versions

DDL Statement:
String statement = ""CREATE TABLE Employee (\r\n"" +
""  employee  ROW(id STRING, name STRING\r\n"" +
""  ),\r\n"" +
""  PRIMARY KEY ([employee.id|http://employee.id/]) NOT ENFORCED\r\n"" +
"") WITH (\r\n"" +
""  'connector' = 'upsert-kafka',\r\n"" +
""  'topic' = 'employee',\r\n"" +
""  'properties.bootstrap.servers' = 'kafka-cp-kafka:9092',\r\n"" +
""  'key.format' = 'raw',\r\n"" +
""  'value.format' = 'avro-confluent',\r\n"" +
""  'value.avro-confluent.url' = '[http://kafka-cp-schema-registry:8081|http://kafka-cp-schema-registry:8081/]',\r\n"" +
"")"";
A new feature to use the property of a Row datatype (in this case employee.id) as a primary key  would be helpful in many scenarios.
Let me know if more details are required.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-11-01 08:10:55.0,,,,,,,,,,"0|z1lbxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolved an issue in YarnClusterDescriptor where temporary files created locally by flink-conf.yaml are treated as remote files,FLINK-33424,13556350,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,zhengzhili,zhengzhili,01/Nov/23 06:53,27/Apr/24 11:49,04/Jun/24 20:40,23/Nov/23 08:57,1.17.1,,,,,,,,,,1.19.0,,,,Client / Job Submission,,,,,,0,,,,,,"After creating the flink-conf.yaml temporary file in YarnClusterDescriptor#startAppMaster, Through YarnApplicationFileUploader# registerSingleLocalResource registered local files,
{quote}fileUploader.registerSingleLocalResource(

flinkConfigKey,

new Path(tmpConfigurationFile.getAbsolutePath()),

"""",

LocalResourceType.FILE,

true,

true);
{quote}
 

In this case, if (Utils.isRemotePath(resourcePath.toString())) is used to determine whether the file is remote based on the path.
{quote}if (Utils.isRemotePath(resourcePath.toString())) {
final FileStatus fileStatus = fileSystem.getFileStatus(resourcePath);
LOG.debug(""Using remote file {} to register local resource"", fileStatus.getPath());

final YarnLocalResourceDescriptor descriptor =
YarnLocalResourceDescriptor.fromFileStatus(
key, fileStatus, LocalResourceVisibility.APPLICATION, resourceType);
addToEnvShipResourceList(whetherToAddToEnvShipResourceList, descriptor);
localResources.put(key, descriptor.toLocalResource());
return descriptor;
}
{quote}
 

The incoming tmpConfigurationFile#getAbsolutePath makes Utils. IsRemotePath considered remote file
When FileSystem is S3AFileSystem, the getFileStatus method displays an error indicating that the file does not exist

 

 

Incoming parameters should be new Path (tmpConfigurationFile.toURI ()) to the new Path (tmpConfigurationFile. getAbsolutePath ())",,,,,,,,,,,,,,,,,,FLINK-29797,,,,FLINK-29797,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 27 11:49:08 UTC 2024,,,,,,,,,,"0|z1lbwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 07:27;zhengzhili;Please assign it to me;;;","27/Apr/24 11:49;Adrian Z;[[FLINK-29797][flink-yarn] Fix fs.default-scheme will accidentally cau… by slankka · Pull Request #24733 · apache/flink (github.com)|https://github.com/apache/flink/pull/24733] And please assign to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve the problem that YarnClusterClientFactory cannot load yarn configurations,FLINK-33423,13556349,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhengzhili,zhengzhili,01/Nov/23 06:48,16/Nov/23 01:31,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Client / Job Submission,,,,,,0,,,,,,"YarnClusterClientFactory. getClusterDescriptor method   Unable to load the configuration for yarn .  The reason is that it is called HadoopUtils.getHadoopConfiguration and this method only loading HDFS configuration.

The call chain looks like this: YarnClusterClientFactory#getClusterDescriptor-->Utils#getYarnAndHadoopConfiguration-->  HadoopUtils.getHadoopConfiguration --> Hadooputills#addHadoopConfIfFound 
However, the HadoopUtils#addHadoopConfIfFound method does not load yarn configuration information

First,YarnClusterClientFactory#getClusterDescriptor This method call Utils.getYarnAndHadoopConfiguration method
{quote}private YarnClusterDescriptor getClusterDescriptor(Configuration configuration)  
Unknown macro: \{ final YarnClient yarnClient = YarnClient.createYarnClient(); final YarnConfiguration yarnConfiguration = Utils.getYarnAndHadoopConfiguration(configuration); yarnClient.init(yarnConfiguration); yarnClient.start(); return new YarnClusterDescriptor( configuration, yarnConfiguration, yarnClient, YarnClientYarnClusterInformationRetriever.create(yarnClient), false); }
{quote}
It then calls Utils# getYarnAndHadoopConfiguration method, in the call HadoopUtils# getHadoopConfiguration methods will only loading the Hadoop configuration unable to load the configuration for Yarn.
{quote}    public static YarnConfiguration getYarnAndHadoopConfiguration(
            org.apache.flink.configuration.Configuration flinkConfig) 
Unknown macro: \{         final YarnConfiguration yarnConfig = getYarnConfiguration(flinkConfig);         yarnConfig.addResource(HadoopUtils.getHadoopConfiguration(flinkConfig));         return yarnConfig;     }
{quote}
Then in HadoopUtils. GetHadoopConfiguration methods this Approach in the 3 will through HadoopUtils# addHadoopConfIfFound method to load the configuration file
{quote}    public static Configuration getHadoopConfiguration(
    
    。。。。。
    
// Approach 3: HADOOP_CONF_DIR environment variable
        String hadoopConfDir = System.getenv(""HADOOP_CONF_DIR"");
        if (hadoopConfDir != null) {
            LOG.debug(""Searching Hadoop configuration files in HADOOP_CONF_DIR: {}"", hadoopConfDir);
            foundHadoopConfiguration =
                    addHadoopConfIfFound(result, hadoopConfDir) || foundHadoopConfiguration;
        }
        。。。。。
}
{quote}
 

Finally, it calls the Hadooputills#addHadoopConfIfFound  method, which loads only the core-site and hdfs-site configuration but not the yarn-site configuration
{quote}private static boolean addHadoopConfIfFound(

Configuration configuration, String possibleHadoopConfPath)
Unknown macro: \{ boolean foundHadoopConfiguration = false; if (new File(possibleHadoopConfPath).exists()) Unknown macro}
if (new File(possibleHadoopConfPath + ""/hdfs-site.xml"").exists())
Unknown macro: \{ configuration.addResource( new org.apache.hadoop.fs.Path(possibleHadoopConfPath + ""/hdfs-site.xml"")); LOG.debug( ""Adding "" + possibleHadoopConfPath + ""/hdfs-site.xml to hadoop configuration""); foundHadoopConfiguration = true; }
}

return foundHadoopConfiguration;

}
{quote}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 02:35:25 UTC 2023,,,,,,,,,,"0|z1lbwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/23 08:54;martijnvisser;[~zhengzhili] How are you adding the Hadoop configuration to your Flink cluster? Are you using https://nightlies.apache.org/flink/flink-docs-master/docs/dev/configuration/advanced/#hadoop-dependencies ?;;;","01/Nov/23 09:15;zhengzhili;Yes, I did, but it just loaded core-site and hdfs-site without automatically loading yarn-site.
This method loads only core-site and hdfs-site:
yarnConfig.addResource(HadoopUtils.getHadoopConfiguration(flinkConfig));
However, yarn-site is not loaded;;;","01/Nov/23 09:18;martijnvisser;[~ferenc-csaky] Any thoughts?;;;","02/Nov/23 01:50;zhengzhili;[~martijnvisser] [~ferenc-csaky]
I wrote a unit test to verify this method org.apache.flink.yarn.Utils.getYarnAndHadoopConfiguration Put UtilsTest in this directory (flink-filesystems\flink-hadoop-fs\target\classes\org\apache\flink\runtime\util), Run testGetYarnAndHadoopConfiguration inside the test method.This unit test code proves that getting the configuration from the hdfs-site file is normal, but getting the configuration from the yarn-site file is problematic.  Here's a link to my unit test code :   [https://github.com/zhengzhili333/flinktest/tree/main#flinktest |https://github.com/zhengzhili333/flinktest/tree/main#flinktest]

Looking forward to your reply
 ;;;","02/Nov/23 02:09;zhengzhili;This is the result of running the unit test code:

!flinktest.png|width=760,height=407!;;;","02/Nov/23 11:26;ferenc-csaky;What the ticket states is correct, currently {{yarn-site.xml}} is not loaded automatically. At the moment only Hive loads that on its own to do some stuff according to the given YARN conf.

I am wondering why is this necessary though? YARN side configuration specifics should not be required to deploy jobs. There is a way to pass YARN specific configuration in the {{flink-conf.yaml}} via {{{}flink.yarn.<key>{}}}. More details in the [docs|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#flink-yarn-%3Ckey%3E].

I think {{HadoopUtils#getHadoopConfiguration}} should definitely not load {{{}yarn-site.xml{}}}, because that is Hadoop utility, not a YARN specific one and used by other components.;;;","02/Nov/23 19:48;zhengzhili; [~martijnvisser]   [~ferenc-csaky] I also think that this method (Hadooputills #getHadoopConfiguration) cannot load yarn-site, but new methods can be added to load it.
I am wondering why is this necessary though? – Because the client needs to link to  resoucemanager. For example, the YarnClusterDescriptor#isReadyForDeployment method requires the configuration of yarn (yarn.resourcemanager.xxx) to link to RM. If yarn is not configured, errors may occur.See the screenshot below：

!image-2023-11-03-03-45-05-640.png|width=1038,height=147!

yarn-site can be loaded in a special way, but it will be inconvenient. Just like the configuration of hdfs-site, it is better to load the configuration directory in advance. Then specify special configurations.

Can you assign this task to me? I will improve it

 ;;;","03/Nov/23 01:27;zhengzhili;We can indeed do this by adding flink.yarn.<key>, but core and hdfs can load the configuration by default, but yarn does not.Isn't that weird  !image-2023-11-03-09-27-59-792.png|width=659,height=142!;;;","03/Nov/23 02:35;zhengzhili;This is probably change train of thought, I can keep the original method, adding new method HadoopUtils#getYarnAndHadoopConfiguration. 

!image-2023-11-03-10-35-07-868.png|width=713,height=284!

Then change it to call it in org.apache.flink.yarn.utils #getYarnAndHadoopConfiguration

!image-2023-11-03-11-05-01-182.png|width=732,height=134!

This way, the original call to Utils#getHadoopConfiguration won't be affected anywhere else

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement restore tests for Calc node,FLINK-33422,13556318,13556317,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,bvarghese,bvarghese,bvarghese,31/Oct/23 21:45,30/Nov/23 14:53,04/Jun/24 20:40,02/Nov/23 09:46,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 30 14:53:43 UTC 2023,,,,,,,,,,"0|z1lbpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Nov/23 15:37;bvarghese;PR - https://github.com/apache/flink/pull/23623;;;","02/Nov/23 09:46;dwysakowicz;Implemented in 898935d75b2fc073bcee61758b3b0443cba584ca;;;","30/Nov/23 14:53;dwysakowicz;Moved classes in 645afd89e90e3f52ccc356a40c03a2b76d124931..091389e1db4b4ff7a42264681efda1e2d05ee675;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement ExecNode Restore Tests,FLINK-33421,13556317,,Improvement,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,bvarghese,bvarghese,bvarghese,31/Oct/23 21:44,08/Apr/24 14:03,04/Jun/24 20:40,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"Implement Restore Tests for various exec nodes to improve coverage

Related JIRA: https://issues.apache.org/jira/browse/FLINK-33375",,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25217,FLINK-33375,FLINK-25809,FLINK-34268,,FLINK-34513,FLINK-35012,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-31 21:44:16.0,,,,,,,,,,"0|z1lbpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run e2e test fails Intermetiently with ClientCoordinationHandler : Unhandled exception,FLINK-33420,13556301,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,samrat007,samrat007,31/Oct/23 19:25,01/Nov/23 13:11,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,Runtime / Coordination,Runtime / REST,,,,,0,test-stability,,,,,"{code}
Oct 31 08:49:37 2023-10-31 08:49:33,348 ERROR org.apache.flink.runtime.rest.handler.job.coordination.ClientCoordinationHandler [] - Unhandled exception.
Oct 31 08:49:37 org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (8528fbf0d50c0f038653f6815d56f6fd)
Oct 31 08:49:37 at org.apache.flink.runtime.dispatcher.Dispatcher.getJobMasterGateway(Dispatcher.java:1450) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
Oct 31 08:49:37 at org.apache.flink.runtime.dispatcher.Dispatcher.performOperationOnJobMasterGateway(Dispatcher.java:1465) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
Oct 31 08:49:37 at org.apache.flink.runtime.dispatcher.Dispatcher.deliverCoordinationRequestToCoordinator(Dispatcher.java:1088) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
Oct 31 08:49:37 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382] 
{code}

log Link :  https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54212&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3160

cc: [~prabhujoseph] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-31 19:25:42.0,,,,,,,,,,"0|z1lbls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port PROCTIME/ROWTIME functions to the new inference stack,FLINK-33419,13556282,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,31/Oct/23 15:26,10/Nov/23 13:43,04/Jun/24 20:40,10/Nov/23 13:43,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 13:43:43 UTC 2023,,,,,,,,,,"0|z1lbhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 13:43;dwysakowicz;Implemented in 3dd98430e5cc7ee88051d0eb6bc2c71908eb997b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayE2ECase failed due to ConnectException,FLINK-33418,13556281,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,31/Oct/23 15:25,27/Nov/23 09:31,04/Jun/24 20:40,27/Nov/23 09:31,1.18.0,1.19.0,,,,,,,,,1.18.1,1.19.0,,,Table SQL / Client,Tests,,,,,0,github-actions,pull-request-available,test-stability,,,"The container couldn't be started in [this build|https://github.com/XComp/flink/actions/runs/6696839844/job/18195926497#step:15:11765]:
{code}
Error: 20:18:40 20:18:40.111 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 110.789 s <<< FAILURE! - in org.apache.flink.table.gateway.SqlGatewayE2ECase
Error: 20:18:40 20:18:40.111 [ERROR] org.apache.flink.table.gateway.SqlGatewayE2ECase  Time elapsed: 110.789 s  <<< ERROR!
Oct 30 20:18:40 org.testcontainers.containers.ContainerLaunchException: Container startup failed for image prestodb/hdp2.6-hive:10
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:349)
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.doStart(HiveContainer.java:69)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.start(GenericContainer.java:322)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.starting(GenericContainer.java:1131)
Oct 30 20:18:40 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:28)
Oct 30 20:18:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Oct 30 20:18:40 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Oct 30 20:18:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Oct 30 20:18:40 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Oct 30 20:18:40 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Oct 30 20:18:40 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
Oct 30 20:18:40 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
Oct 30 20:18:40 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Oct 30 20:18:40 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Oct 30 20:18:40 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Oct 30 20:18:40 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Oct 30 20:18:40 Caused by: org.rnorth.ducttape.RetryCountExceededException: Retry limit hit with exception
Oct 30 20:18:40 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:88)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.doStart(GenericContainer.java:334)
Oct 30 20:18:40 	... 29 more
Oct 30 20:18:40 Caused by: org.testcontainers.containers.ContainerLaunchException: Could not create/start container
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:553)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.lambda$doStart$0(GenericContainer.java:344)
Oct 30 20:18:40 	at org.rnorth.ducttape.unreliables.Unreliables.retryUntilSuccess(Unreliables.java:81)
Oct 30 20:18:40 	... 30 more
Oct 30 20:18:40 Caused by: java.lang.RuntimeException: java.net.ConnectException: Failed to connect to /127.0.0.1:32779
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:93)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.containerIsStarted(GenericContainer.java:712)
Oct 30 20:18:40 	at org.testcontainers.containers.GenericContainer.tryStart(GenericContainer.java:532)
Oct 30 20:18:40 	... 32 more
Oct 30 20:18:40 Caused by: java.net.ConnectException: Failed to connect to /127.0.0.1:32779
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:265)
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:183)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.java:224)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.java:108)
Oct 30 20:18:40 	at okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.java:88)
Oct 30 20:18:40 	at okhttp3.internal.connection.Transmitter.newExchange(Transmitter.java:169)
Oct 30 20:18:40 	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:41)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:94)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:88)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:142)
Oct 30 20:18:40 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:117)
Oct 30 20:18:40 	at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:229)
Oct 30 20:18:40 	at okhttp3.RealCall.execute(RealCall.java:81)
Oct 30 20:18:40 	at org.apache.flink.table.gateway.containers.HiveContainer.containerIsStarted(HiveContainer.java:86)
Oct 30 20:18:40 	... 34 more
Oct 30 20:18:40 Caused by: java.net.ConnectException: Connection refused (Connection refused)
Oct 30 20:18:40 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
Oct 30 20:18:40 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
Oct 30 20:18:40 	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
Oct 30 20:18:40 	at java.base/java.net.Socket.connect(Socket.java:609)
Oct 30 20:18:40 	at okhttp3.internal.platform.Platform.connectSocket(Platform.java:130)
Oct 30 20:18:40 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:263)
Oct 30 20:18:40 	... 53 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32731,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 27 09:31:02 UTC 2023,,,,,,,,,,"0|z1lbhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 15:25;mapohl;https://github.com/XComp/flink/actions/runs/6695266358/job/18192091957#step:15:12535;;;","31/Oct/23 16:12;mapohl;https://github.com/XComp/flink/actions/runs/6707387404/job/18226444235#step:15:12068;;;","01/Nov/23 07:12;mapohl;https://github.com/XComp/flink/actions/runs/6711829097/job/18242192935#step:15:11942;;;","01/Nov/23 07:14;mapohl;https://github.com/XComp/flink/actions/runs/6714646438/job/18248611746#step:15:11379
https://github.com/XComp/flink/actions/runs/6719526710/job/18261920525#step:15:11509;;;","01/Nov/23 14:47;mapohl;[~fsk119] do you have a guess what the cause could be here? Looks like the docker container is not accessible... ;;;","01/Nov/23 15:10;mapohl;I guess there is an issue with how Github Actions handles TestContainers or more generally Docker containers which is different from how Azure CI handles it. The test seems to work locally as well.;;;","01/Nov/23 18:26;mapohl;Not sure why it works on AzureCI but it looks like using {{getHost()}} instead of {{127.0.0.1}} does the trick (see [11127da|https://github.com/XComp/flink/commit/11127da7625596fc4db6115fbdcbfdbc4fdf26b7]).;;;","02/Nov/23 07:03;mapohl;We have the {{HiveContainer}} class twice in Flink. Several subsequent builds failed due to {{HiveITCase}} which uses the other {{HiveContainer}} class. -.-
* https://github.com/XComp/flink/actions/runs/6723267607/job/18273504211#step:15:13437
* https://github.com/XComp/flink/actions/runs/6723279744/job/18273708231#step:15:13473
* https://github.com/XComp/flink/actions/runs/6724579458/job/18277691069#step:15:13803 
* https://github.com/XComp/flink/actions/runs/6727253349/job/18285012233#step:15:13613;;;","09/Nov/23 15:43;mapohl;Fixed by not running the e2e tests in Docker;;;","09/Nov/23 16:36;mapohl;I'm re-opening the issue because the fix would mean two tests that could run in Docker.;;;","27/Nov/23 09:31;mapohl;master: [f6f785fd1a927ade00f91bf0de0f96a2268aa4e5|https://github.com/apache/flink/commit/f6f785fd1a927ade00f91bf0de0f96a2268aa4e5]
1.18: [e9548a6d420ed97e0b3d50523092a6663d624ab4|https://github.com/apache/flink/commit/e9548a6d420ed97e0b3d50523092a6663d624ab4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update netty version to 4.1.83 for flink-shaded,FLINK-33417,13556234,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,31/Oct/23 11:57,15/Jan/24 12:40,04/Jun/24 20:40,03/Nov/23 09:18,shaded-16.1,,,,,,,,,,shaded-16.2,,,,BuildSystem / Shaded,,,,,,0,pull-request-available,,,,,"In our ARM environment, we encounter a compile error when using Flink 1.17.
 (The BE20xxx pic is the error when using 4.1.82. The 09DDxx pic is the pic of compiling successfully after using 4.1.83.)

Flink 1.17 depends on flink-shaded 16.1, which uses netty 4.1.82. However, flink-shaded 16.1 fails to compile in the ARM environment. As a result, we are unable to compile Flink 1.17 due to this issue.

We have tested compiling flink-shaded using netty 4.1.83 or a later version in ARM env, and it can compile successfully.

Taking into consideration the previous discussions regarding compatibility and the dependency of external connectors on this version, I propose addressing the bug by only updating flink-shaded's netty to a minor version (e.g., 4.1.83) rather than backporting FLINK-32032. 

To implement the update, maybe a new release of flink-shaded 16.2 needs to be released.

The discussion details is at https://lists.apache.org/thread/y1c8545bcsx2836d9pgfdzj65knvw7kb.
",,,,,,,,,,,,,,,,,,,FLINK-34092,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/23 09:29;tanyuxin;09DD8FAA-7701-4AA0-85F9-4ABA6C5117DF.png;https://issues.apache.org/jira/secure/attachment/13064103/09DD8FAA-7701-4AA0-85F9-4ABA6C5117DF.png","02/Nov/23 09:27;tanyuxin;BE207321-9677-4721-9415-BD3312C29824.png;https://issues.apache.org/jira/secure/attachment/13064102/BE207321-9677-4721-9415-BD3312C29824.png","03/Nov/23 08:43;tanyuxin;run.log;https://issues.apache.org/jira/secure/attachment/13064150/run.log",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 09:19:08 UTC 2023,,,,,,,,,,"0|z1lb6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Nov/23 13:08;mapohl;Thanks for adding the screenshots, [~tanyuxin]. Do we know what the reason for the difference between the two versions (or the actual cause in 4.1.82) is?;;;","02/Nov/23 13:36;mapohl;I'm still curious. We tried to reproduce it by running the following commands on a MacBook:
{code}
$ git clone git@github.com:apache/flink-shaded.git flink-shaded --branch release-16.1
$ cd flink-shaded
$ mvn package
{code}

Based on the error message it looks like the error appeared in the {{maven-antrun-plugin}} which is attached to the {{package}} phase. Do we miss anything to reproduce the error?;;;","03/Nov/23 04:06;tanyuxin;[~mapohl] Actually, as I don't possess an in-depth understanding of Netty, I haven't determined the internal cause of the error or how this can resolve the issue. But it indeed works.;;;","03/Nov/23 06:59;mapohl;I see. But can you elaborate a bit more on how to reproduce the error? That might help getting a better understanding of the issue. Or does your ""But it indeed works."" mean that you can't reproduce the error, anymore?;;;","03/Nov/23 08:43;tanyuxin;[~mapohl] ok. Thanks for the tracking. The error is on our test machine, not on the local Mac. The error can be reproduced and the reproduced log `run.log` is attached.;;;","03/Nov/23 09:15;mapohl;Thanks for sharing. AFAIU, it's being caused by intel machines with arm archtecture? I don't understand how that is actual a valid setup because I always thought that Apple's own CPUs are based on ARM and were never produced by Intel. That's a bit confusing.

Anyway, going through the netty git history reveals the following:
* [PR #12631|https://github.com/netty/netty/pull/12631] introduced a cross compilation profile in netty's build system in {{netty-4.1.80.Final}}
* [Issue #12861|https://github.com/netty/netty/issues/12861] raised the issue of libraries of a wrong classifier being used
* [PR #12865|https://github.com/netty/netty/pull/12865] fixed the issue for {{netty-4.1.83.Final}}

I guess that's what we're experiencing here.;;;","03/Nov/23 09:19;mapohl;release-16.0: 2e81185ea46261e7f6aca88fdd0a982193b4c86c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerTest failed with fatal error,FLINK-33416,13556229,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,31/Oct/23 11:29,02/Apr/24 10:11,04/Jun/24 20:40,02/Apr/24 10:11,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,github-actions,test-stability,,,,"In FLINK-33245, we reported an error of the {{ZooKeeperLeaderElectionConnectionHandlingTest}} failure due to a fatal error. The corresponding build is [this one|https://github.com/XComp/flink/actions/runs/6472726326/job/17575765131].

But the stacktrace indicates that it's actually {{FineGrainedSlotManagerTest}} which ran before the ZK-related test:
{code}
Test org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testSlotAllocationAccordingToStrategyResult[testSlotAllocationAccordingToStrategyResult()] successfully run.
================================================================================
19:30:11,463 [   pool-752-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-752-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1201ef67[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1ea6ccfa[Wrapped task = java.util.concurrent.CompletableFuture$UniHandle@36f84d94]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4642c78d[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314) ~[?:?]
        at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:951) ~[?:?]
        at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2276) ~[?:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer.allocateSlot(DefaultSlotStatusSyncer.java:138) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.allocateSlotsAccordingTo(FineGrainedSlotManager.java:722) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.checkResourceRequirements(FineGrainedSlotManager.java:645) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.lambda$checkResourceRequirementsWithDelay$12(FineGrainedSlotManager.java:603) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
        at java.lang.Thread.run(Thread.java:829) [?:?]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1201ef67[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1ea6ccfa[Wrapped task = java.util.concurrent.CompletableFuture$UniHandle@36f84d94]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@4642c78d[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:705) ~[?:?]
        at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:687) ~[?:?]
        at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:949) ~[?:?]
        ... 11 more
[...]
{code}

I leave this issue open for documentation purposes for now. ...in case the issue pops up again.",,,,,,,,,,,,,,,,,,,,,,,FLINK-34427,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 02 10:11:15 UTC 2024,,,,,,,,,,"0|z1lb5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/24 08:40;rskraba;1.19, jdk8 [https://github.com/apache/flink/actions/runs/8467681781/job/23199435037#step:10:8909]

This looks suspiciously simular in the mvn-2.log:
{code:java}
--------------------------------------------------------------------------------Test org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testSlotAllocationAccordingToStrategyResult[testSlotAllocationAccordingToStrategyResult()] successfully run.================================================================================13:18:55,524 [   pool-487-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-487-thread-1' produced an uncaught exception. Stopping the process...java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@45bd78c2 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@479ccf95[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:851) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2178) ~[?:1.8.0_392]    at org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer.allocateSlot(DefaultSlotStatusSyncer.java:138) ~[classes/:?]    at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.allocateSlotsAccordingTo(FineGrainedSlotManager.java:722) ~[classes/:?]    at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.checkResourceRequirements(FineGrainedSlotManager.java:645) ~[classes/:?]    at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.lambda$null$12(FineGrainedSlotManager.java:603) ~[classes/:?]    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_392]    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_392]    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_392]    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_392]    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_392]    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_392]    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_392]Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@45bd78c2 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@479ccf95[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_392]    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_392]    at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_392]    at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_392]    at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_392]    at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_392]    at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_392]    ... 14 more {code};;;","02/Apr/24 10:11;mapohl;This issue is addressed in FLINK-34427. I'm closing FLINK-33416 in favor of FLINK-34427 because the investigation happened there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HAJobRunOnHadoopS3FileSystemITCase failed due to NoSuchMethodError,FLINK-33415,13556226,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,mapohl,mapohl,mapohl,31/Oct/23 10:58,31/Oct/23 11:15,04/Jun/24 20:40,31/Oct/23 11:15,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,github-actions,test-stability,,,,"{code:java}
Error: 23:16:52 23:16:52.433 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.271 s <<< FAILURE! - in org.apache.flink.fs.s3hadoop.HAJobRunOnHadoopS3FileSystemITCase
37033Error: 23:16:52 23:16:52.433 [ERROR] org.apache.flink.fs.s3hadoop.HAJobRunOnHadoopS3FileSystemITCase  Time elapsed: 6.271 s  <<< ERROR!
37034Oct 10 23:16:52 java.lang.NoSuchMethodError: 'void org.apache.hadoop.security.HadoopKerberosName.setRuleMechanism(java.lang.String)'
37035Oct 10 23:16:52 	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:84)
37036Oct 10 23:16:52 	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315)
37037Oct 10 23:16:52 	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
37038Oct 10 23:16:52 	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
37039Oct 10 23:16:52 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:474)
37040Oct 10 23:16:52 	at org.apache.flink.fs.s3.common.AbstractS3FileSystemFactory.create(AbstractS3FileSystemFactory.java:129)
37041Oct 10 23:16:52 	at org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:508)
37042Oct 10 23:16:52 	at org.apache.flink.core.fs.FileSystem.get(FileSystem.java:409)
37043Oct 10 23:16:52 	at org.apache.flink.core.fs.Path.getFileSystem(Path.java:279)
37044Oct 10 23:16:52 	at org.apache.flink.runtime.blob.BlobUtils.createFileSystemBlobStore(BlobUtils.java:99)
37045Oct 10 23:16:52 	at org.apache.flink.runtime.blob.BlobUtils.createBlobStoreFromConfig(BlobUtils.java:86)
37046Oct 10 23:16:52 	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createZooKeeperHaServices(HighAvailabilityServicesUtils.java:89)
37047Oct 10 23:16:52 	at org.apache.flink.runtime.highavailability.HighAvailabilityServicesUtils.createAvailableOrEmbeddedServices(HighAvailabilityServicesUtils.java:69)
37048Oct 10 23:16:52 	at org.apache.flink.runtime.minicluster.MiniCluster$RegularHighAvailabilityServicesFactory.createHAServices(MiniCluster.java:1530)
37049Oct 10 23:16:52 	at org.apache.flink.runtime.minicluster.MiniCluster.createHighAvailabilityServices(MiniCluster.java:617)
37050Oct 10 23:16:52 	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:438)
37051Oct 10 23:16:52 	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:246)
37052Oct 10 23:16:52 	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:110)
37053Oct 10 23:16:52 	at org.apache.flink.runtime.testutils.InternalMiniClusterExtension.beforeAll(InternalMiniClusterExtension.java:72)
37054Oct 10 23:16:52 	at org.apache.flink.test.junit5.MiniClusterExtension.beforeAll(MiniClusterExtension.java:231)
[...] {code}
[Run #14|https://github.com/XComp/flink/actions/runs/6472816505/job/17575963787#step:11:37035] in the {{finegrained_resourcemanagement}} stage (see FLINK-33245)

[Run #11|https://github.com/XComp/flink/actions/runs/6471147857/job/17571310183#step:11:41740] in the {{finegrained_resourcemanagement}} stage (see FLINK-33245)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 11:15:20 UTC 2023,,,,,,,,,,"0|z1lb54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 11:11;mapohl;For the record: The two observed runs happened before switching to the currently supported minimal Hadoop version 2.10.2. So it's quite likely that this error was caused by the having the wrong Hadoop version.;;;","31/Oct/23 11:15;mapohl;I did a bit more research. The method in question is {{HadoopKerberosName.setRuleMechanism}} which is derived from {{KerberosName}}. The method was added in [d43af8b3|https://github.com/apache/hadoop/commit/d43af8b3db4743b4b240751b6f29de6c20cfd6e5]. The corresponding issue HADOOP-15996 was resolved in 2019. The Hadoop version 2.8.3 (which we used accidentally in the initial GHA runs) was released in 2018. Therefore, it makes sense to get this {{NoSuchMethodError}}.

I'm gonna resolve this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot fails due to unexpected TimeoutException,FLINK-33414,13556225,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiang Xin,mapohl,mapohl,31/Oct/23 10:45,12/Jan/24 11:48,04/Jun/24 20:40,12/Jan/24 11:48,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,github-actions,pull-request-available,test-stability,,,"We see this test instability in [this build|https://github.com/XComp/flink/actions/runs/6695266358/job/18192039035#step:12:9253].
{code:java}
Error: 17:04:52 17:04:52.042 [ERROR] Failures: 
9252Error: 17:04:52 17:04:52.042 [ERROR]   MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot:120 
9253Oct 30 17:04:52 Expecting a throwable with root cause being an instance of:
9254Oct 30 17:04:52   org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException
9255Oct 30 17:04:52 but was an instance of:
9256Oct 30 17:04:52   java.util.concurrent.TimeoutException: Timeout has occurred: 100 ms
9257Oct 30 17:04:52 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)
9258Oct 30 17:04:52 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
9259Oct 30 17:04:52 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
9260Oct 30 17:04:52 	...(27 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed) {code}
The same error occurred in the [finegrained_resourcemanager stage of this build|https://github.com/XComp/flink/actions/runs/6468655160/job/17563927249#step:11:26516] (as reported in FLINK-33245).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32846,FLINK-34070,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 12 11:48:01 UTC 2024,,,,,,,,,,"0|z1lb4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 08:09;mapohl;* https://github.com/XComp/flink/actions/runs/6774272900/job/18411630196#step:12:9254
* https://github.com/XComp/flink/actions/runs/6778931640/job/18425414988#step:12:9275;;;","20/Nov/23 07:45;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54687&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef;;;","20/Nov/23 12:22;mapohl;Ok, looks like it's a general issue. I'm converting it to a bug and move it out of FLINK-27075. Thanks for reporting, [~snuyanzin] ;;;","22/Nov/23 07:53;mapohl;https://github.com/XComp/flink/actions/runs/6944945499/job/18893668436#step:12:8893;;;","24/Nov/23 11:17;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54856&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8858;;;","29/Nov/23 07:53;mapohl;https://github.com/XComp/flink/actions/runs/7019921193/job/19098991758#step:12:8972;;;","07/Dec/23 14:52;mapohl;https://github.com/XComp/flink/actions/runs/7069011044/job/19244368664#step:12:8888;;;","15/Dec/23 08:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55530&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9217;;;","20/Dec/23 11:24;mapohl;https://github.com/XComp/flink/actions/runs/7235798862/job/19713778857#step:12:9017;;;","21/Dec/23 08:53;Jiang Xin;[~mapohl] Could you assign this issue to me?

I fould the reason is that the `FineGrainedSlotManager.checkResourceRequirementsWithDelay` would schedule a resource requirements check with a delay of 50 seconds by default. The check would also be aware of the lack of slots and notify the JobMaster so that the scheduler doesn't need to wait until timeout(100ms in the test) to fail the job. If the resource requirements check thread is not finished within 50ms, the JobMaster will fail due to a Timeout exception. So we can fix the issue by disabling the resource requirements check or adjusting the checking delay and waiting timeout to make sure the Timeout exception never be thrown.;;;","22/Dec/23 20:17;mapohl;Thanks for volunteering, [~Jiang Xin]. Unfortunately, I don't have the time to verify your conclusion right now. It looks like the issue can be reproduced locally. Feel free to create a PR. I'm happy to pick it up and review it after the holiday week. ;;;","23/Dec/23 09:23;mapohl;[~Jiang Xin] Can you link the issue that caused this instability? Or is this only caused by ""slower"" machines? The latter reason might apply since we see it most of the times on GitHub Actions. But that doesn't explain why this error also started to appear in Azure Pipelines. That is why I am wondering whether there was a change that made this instability to appear.;;;","23/Dec/23 13:56;Jiang Xin;[~mapohl] I think it is caused by this [change]([https://github.com/apache/flink/pull/23244)] about junit5 migration. In this change, the ""assertThat(e, FlinkMatchers.containsCause(NoResourceAvailableException.class));"" is replaced by "".hasRootCauseInstanceOf(NoResourceAvailableException.class)"".

The test would not fail even if the root cause is a TimeoutExcption before the change, because the NoResourceAvailableException is also in the error stack trace. However, with the change, the root cause has to be NoResourceAvailableException.;;;","08/Jan/24 18:26;mapohl;master: [f45de0c6de87c182a49f2a979884d3d2f66f870f|https://github.com/apache/flink/commit/f45de0c6de87c182a49f2a979884d3d2f66f870f];;;","09/Jan/24 07:16;Sergey Nuyanzin;I fear I have to reopen this issue since now testHandleStreamingJobsWhenNotEnoughSlot started to hang forever on AZP
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56124&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9813

{noformat}
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 09 02:01:16 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 09 02:01:16 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Jan 09 02:01:16 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 09 02:01:16 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:1050)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase.runHandleJobsWhenNotEnoughSlots(MiniClusterITCase.java:152)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase.lambda$testHandleStreamingJobsWhenNotEnoughSlot$0(MiniClusterITCase.java:119)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase$$Lambda$1927/1144737794.call(Unknown Source)
Jan 09 02:01:16 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
Jan 09 02:01:16 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
Jan 09 02:01:16 	at org.assertj.core.api.Assertions.catchThrowable(Assertions.java:1366)
Jan 09 02:01:16 	at org.assertj.core.api.Assertions.assertThatThrownBy(Assertions.java:1210)
Jan 09 02:01:16 	at org.apache.flink.runtime.minicluster.MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot(MiniClusterITCase.java:119)
Jan 09 02:01:16 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jan 09 02:01:16 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jan 09 02:01:16 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

{noformat}
[~Jiang Xin] coudl you please have a look?;;;","09/Jan/24 07:44;Jiang Xin;[~Sergey Nuyanzin] Can it be consistently reproduced on AZP? I ran this test locally for hundreds of times, but no hang happened.;;;","09/Jan/24 07:52;Sergey Nuyanzin;I don't know it appeared only today during nightly run which is scheduled once per 24 hours, 
I suspect it is a consequence of merging the related PR yesterday;;;","10/Jan/24 07:42;Sergey Nuyanzin;[~Jiang Xin] there is another reproduction from today's nightly
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56166&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10782;;;","10/Jan/24 08:50;Jiang Xin;[~mapohl] Do you have time to take a look? I went through the code but didn't find out how could it happen. By the way, I cannot reproduce it locally after running hundreds of times.;;;","10/Jan/24 14:59;mapohl;Yeah, I am not able to reproduce it locally either (with ~900 repetitions). My first suspicion would be that the slotmanager isn't reaching out to the JobMaster to inform him about the missing resources. ;;;","12/Jan/24 10:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56226&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10773;;;","12/Jan/24 10:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56285&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=10800;;;","12/Jan/24 11:23;mapohl;[~Jiang Xin] The issue only appears with the {{AdaptiveScheduler}}. You can reproduce it locally by enabling the {{AdaptiveScheduler}} in your test run: Add {{-Dflink.tests.enable-adaptive-scheduler=true}} to the VMOptions field of your Intellij run configuration for this test.

Do we use FineGrainedSlotManager;;;","12/Jan/24 11:23;mapohl;[~Jiang Xin] The issue only appears with the {{AdaptiveScheduler}}. You can reproduce it locally by enabling the {{AdaptiveScheduler}} in your test run: Add {{-Dflink.tests.enable-adaptive-scheduler=true}} to the VMOptions field of your Intellij run configuration for this test.

Can you have a look? My suspicion is that we use the slot timeout in the {{AdaptiveScheduler}} in some ways that were not considered by our initial change.;;;","12/Jan/24 11:48;mapohl;I'm closing this one again in favor of FLINK-34070. Because it's rather a new issue due to the fix of FLINK-33414, I suspect.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Avro in AWS Connectors to address CVE-2023-39410,FLINK-33413,13556223,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,31/Oct/23 10:42,31/Oct/23 13:42,04/Jun/24 20:40,31/Oct/23 13:42,aws-connector-4.1.0,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,Bump org.apache.avro:avro {{v1.11.1}} to {{v1.11.3}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 13:42:30 UTC 2023,,,,,,,,,,"0|z1lb4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 13:42;dannycranmer; merged commit [{{3f4d59b}}|https://github.com/apache/flink-connector-aws/commit/3f4d59b9cc677550e411b2086aa23812510cfc62] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement type inference for reinterpret_cast function,FLINK-33412,13556216,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,dwysakowicz,dwysakowicz,dwysakowicz,31/Oct/23 09:49,03/Nov/23 12:15,04/Jun/24 20:40,03/Nov/23 12:15,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/Reinterpret.scala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 12:15:58 UTC 2023,,,,,,,,,,"0|z1lb2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 12:15;dwysakowicz;Implemented in f6b662f83deb80572773617f7eb202fa05388198;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement type inference for window properties functions,FLINK-33411,13556215,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,31/Oct/23 09:48,17/Nov/23 09:09,04/Jun/24 20:40,17/Nov/23 09:09,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/windowProperties.scala

Functions:
* WINDOW_START
* WINDOW_END",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 17 09:09:42 UTC 2023,,,,,,,,,,"0|z1lb2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 09:09;dwysakowicz;Implemented in 873bd130c838b4b56cf9af8fdb2e0390ddbe6e4a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement type inference for Over function,FLINK-33410,13556214,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,31/Oct/23 09:46,17/Nov/23 09:01,04/Jun/24 20:40,17/Nov/23 09:01,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/overOffsets.scala

Functions:
* OVER
* CURRENT_RANGE
* CURRENT_ROW
* UNBOUNDED_ROW
* UNBOUNDED_RANGE",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 17 09:01:54 UTC 2023,,,,,,,,,,"0|z1lb2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Nov/23 09:01;dwysakowicz;Implemented in 1986cfe1cb3606fc00fd13735f7ba901b806f56f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Guava in AWS Connectors to address CVE-2020-8908/CVE-2023-2976,FLINK-33409,13556212,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,31/Oct/23 09:41,31/Oct/23 10:35,04/Jun/24 20:40,31/Oct/23 10:35,aws-connector-3.0.0,aws-connector-4.1.0,,,,,,,,,aws-connector-4.2.0,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,Bump Guava from {{32.0.0-jre}} to {{32.1.3-jre}} to mitigate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 10:34:59 UTC 2023,,,,,,,,,,"0|z1lb20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 10:34;dannycranmer; merged commit [{{23624ab}}|https://github.com/apache/flink-connector-aws/commit/23624ab0a7d36a9dc26c8f60e6b76c971c0fe662] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the snakeyaml from 1.33 to 2.0 to fix the container vulnerability.,FLINK-33408,13556207,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,danielchow,danielchow,31/Oct/23 09:00,02/Nov/23 01:46,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,pull-request-available,,,,,"Fix the container vulnerability in [CVE-2022-1471|https://github.com/advisories/GHSA-mjmj-j48q-9wg2] by upgrade the SnakeYaml dependency version in flink-kubernetes module.

Upgrade the Kubernetes Client from 6.6.2 to 6.7.0, thereby upgrading the version of snakeyaml, which the Kubernetes Client indirectly depends on, from 1.33 to 2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31815,FLINK-31997,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,https://github.com/apache/flink/pull/23631,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 00:19:59 UTC 2023,,,,,,,,,,"0|z1lb0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 09:10;danielchow;[FLINK-31815|https://issues.apache.org/jira/browse/FLINK-31815] fixed the container vulnerability from the operator side. However, we still pull the old snakeyaml for flink standalone due to the old flink fabric8 usage, and it hasn't been fixed as part of [FLINK-31997|https://issues.apache.org/jira/browse/FLINK-31997]. So I will fix it in this issue.;;;","01/Nov/23 15:51;martijnvisser;[~danielchow] Please don't ping random people for your PR. This isn't community friendly. ;;;","02/Nov/23 00:19;danielchow;[~martijnvisser] , I thought these people might be related to my ISSUE, so I mentioned them in the PR, sorry about that. Thanks for your kind reminder (y). I'll avoid it in the future. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove old expression stack leftovers for time functions,FLINK-33407,13556206,13251583,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,31/Oct/23 08:50,03/Nov/23 08:44,04/Jun/24 20:40,03/Nov/23 08:44,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Remove leftovers from https://issues.apache.org/jira/browse/FLINK-13785

There are some parts of the time functions that have not been removed e.g. https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/time.scala and some code in https://github.com/apache/flink/blob/b6000f6e589128ae1fd1e0e7d063a1b6ff1fcc20/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/PlannerExpressionConverter.scala",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 08:44:42 UTC 2023,,,,,,,,,,"0|z1lb0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 08:44;dwysakowicz;Fixed in 5da6b3eff29c14931c86b2018331c40ed256420f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Job failed due to losing connection from ZK server,FLINK-33406,13556190,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,singularityd,singularityd,31/Oct/23 06:58,31/Oct/23 07:21,04/Jun/24 20:40,,1.14.3,,,,,,,,,,,,,,API / DataStream,,,,,,0,,,,,,"We are using Flink 1.14.3 and we faced an issue when losing connection from ZK server, the flink job connecting to the target ZK server will be failed directly. This case can be reproduced 100% when you kill the connected ZK server for simulating connection refused issue. Flink jobs connect to other running ZK server keep running as expected. The log output is:
{code:java}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: java.util.concurrent.TimeoutException
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)        
            at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)        
            at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)        
            at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:812)        
            at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:246)        
            at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1054)        
            at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1132)        
            at java.security.AccessController.doPrivileged(Native Method)        
            at javax.security.auth.Subject.doAs(Subject.java:422)        
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1731)        
            at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)        
            at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1132)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException        
            at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)        
            at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)        
            at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironment.java:123)        
            at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:80)        
            at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1916){code}
 ","Flink version: 1.14.3

Zookeeper version: 3.4.10",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-31 06:58:05.0,,,,,,,,,,"0|z1lax4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProcessJoinFunction not found in Pyflink,FLINK-33405,13556169,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,dottami,dottami,31/Oct/23 03:13,31/Oct/23 03:13,04/Jun/24 20:40,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,ProcessJoinFunction doesn't exist in Pyflink. Is there a plan to add it?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-31 03:13:45.0,,,,,,,,,,"0|z1lasg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
on_timer method is missing in ProcessFunction and CoProcessFunction of Pyflink,FLINK-33404,13556168,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,dottami,dottami,31/Oct/23 03:11,31/Oct/23 03:11,04/Jun/24 20:40,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"Hello,

I find the `on_timer` method is not found in ProcessFunction and CoProcessFunction of Pyflink and it causes an error when I register a timer eg)

 ```
  ...
  File ""/home/jaehyeon/personal/flink-demos/venv/lib/python3.8/site-packages/pyflink/fn_execution/datastream/process/input_handler.py"", line 101, in process_timer
    yield from _emit_results(
  File ""/home/jaehyeon/personal/flink-demos/venv/lib/python3.8/site-packages/pyflink/fn_execution/datastream/process/input_handler.py"", line 131, in _emit_results
    for result in results:
  File ""/home/jaehyeon/personal/flink-demos/venv/lib/python3.8/site-packages/pyflink/fn_execution/datastream/process/input_handler.py"", line 114, in _on_processing_time
    yield from self._on_processing_time_func(timestamp, key, namespace)
  File ""/home/jaehyeon/personal/flink-demos/venv/lib/python3.8/site-packages/pyflink/fn_execution/datastream/process/operations.py"", line 308, in on_processing_time
    return _on_timer(TimeDomain.PROCESSING_TIME, timestamp, key)
  File ""/home/jaehyeon/personal/flink-demos/venv/lib/python3.8/site-packages/pyflink/fn_execution/datastream/process/operations.py"", line 317, in _on_timer
    return process_function.on_timer(timestamp, on_timer_ctx)
AttributeError: 'ReadingFilter' object has no attribute 'on_timer'

        at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
        at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:332)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:315)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:834)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
        ... 3 more
```

I'm working on Pyflink 1.17.1 but it would be applicable other versions. 

Can the method be added to the functions?

Cheers,
Jaehyeon",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-31 03:11:08.0,,,,,,,,,,"0|z1las8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump flink version to 1.18.1 for flink-kubernetes-operator,FLINK-33403,13556164,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,31/Oct/23 02:05,23/Jan/24 09:05,04/Jun/24 20:40,23/Jan/24 09:05,,,,,,,,,,,kubernetes-operator-1.8.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33473,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 23 09:05:08 UTC 2024,,,,,,,,,,"0|z1larc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/24 09:05;fanrui;Merged to main(1.8.0) via : 72231cf1e577b9d8d5ea2dbfdade277a368b315d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid Source Concurrency Race Condition Fixes and Related Bugs Results in Data Loss,FLINK-33402,13556143,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,varun1729dd,varun1729dd,varun1729dd,30/Oct/23 23:10,09/Feb/24 19:20,04/Jun/24 20:40,,1.16.1,,,,,,,,,,,,,,Connectors / HybridSource,,,,,,0,pull-request-available,,,,,"Hello Team,
I noticed that there is data loss when using Hybrid Source. We are reading from a series of concrete File Sources ~100. All these locations are chained together using the Hybrid source.
The issue stems from a race-condition in Flink Hybrid Source code. The Hybrid Sources switches the next source before the current source is complete. Similarly for the Hybrid Source readers. I have also shared the patch file that fixes the issue.
From the logs:

*Task Manager logs:* 
2023-10-10 17:46:23.577 [Source: parquet-source (1/2)#0|#0] INFO  o.apache.flink.connector.base.source.reader.SourceReaderBase  - Adding split(s) to reader: [FileSourceSplit: s3://REDACTED/part-1-13189.snappy [0, 94451)  hosts=[localhost] ID=0000000229 position=null] 
2023-10-10 17:46:23.715 [Source Data Fetcher for Source: parquet-source (1/2)#0|#0] INFO  org.apache.hadoop.fs.s3a.S3AInputStream  - Switching to Random IO seek policy 
2023-10-10 17:46:23.715 [Source Data Fetcher for Source: parquet-source (1/2)#0|#0] INFO  org.apache.hadoop.fs.s3a.S3AInputStream  - Switching to Random IO seek policy 
2023-10-10 17:46:24.012 [Source: parquet-source (1/2)#0|#0] INFO  o.apache.flink.connector.base.source.reader.SourceReaderBase  - Finished reading split(s) [0000000154] 
2023-10-10 17:46:24.012 [Source Data Fetcher for Source: parquet-source (1/2)#0|#0] INFO  o.a.flink.connector.base.source.reader.fetcher.SplitFetcher  - Finished reading from splits [0000000154] 
2023-10-10 17:46:24.014 [Source: parquet-source (1/2)#0|#0] INFO  o.apache.flink.connector.base.source.reader.SourceReaderBase  - Reader received NoMoreSplits event. 
2023-10-10 17:46:24.014 [Source: parquet-source (1/2)#0|#0] DEBUG o.a.flink.connector.base.source.hybrid.HybridSourceReader  - No more splits for subtask=0 sourceIndex=11 currentReader=org.apache.flink.connector.file.src.impl.FileSourceReader@59620ef8 
2023-10-10 17:46:24.116 [Source Data Fetcher for Source: parquet-source (1/2)#0|#0] INFO  org.apache.hadoop.fs.s3a.S3AInputStream  - Switching to Random IO seek policy 
2023-10-10 17:46:24.116 [Source Data Fetcher for Source: parquet-source (1/2)#0|#0] INFO  org.apache.hadoop.fs.s3a.S3AInputStream  - Switching to Random IO seek policy 
2023-10-10 17:46:24.116 [Source: parquet-source (1/2)#0|#0] INFO  o.a.flink.connector.base.source.hybrid.HybridSourceReader  - Switch source event: subtask=0 sourceIndex=12 source=org.apache.flink.connector.kafka.source.KafkaSource@7849da7e 2023-10-10 17:46:24.116 [Source: parquet-source (1/2)#0|#0] INFO  o.apache.flink.connector.base.source.reader.SourceReaderBase  - Closing Source Reader. 
2023-10-10 17:46:24.116 [Source: parquet-source (1/2)#0|#0] INFO  o.a.flink.connector.base.source.reader.fetcher.SplitFetcher  - Shutting down split fetcher 0 
2023-10-10 17:46:24.198 [Source Data Fetcher for Source: parquet-source (1/2)#0|#0] INFO  o.a.flink.connector.base.source.reader.fetcher.SplitFetcher  - Split fetcher 0 exited. 
2023-10-10 17:46:24.198 [Source: parquet-source (1/2)#0|#0] DEBUG o.a.flink.connector.base.source.hybrid.HybridSourceReader  - Reader closed: subtask=0 sourceIndex=11 currentReader=org.apache.flink.connector.file.src.impl.FileSourceReader@59620ef8

```
We identified that data from `s3://REDACTED/part-1-13189.snappy` is missing.  This is assigned to Reader with ID 0000000229. Now, we can see from the logs this split is added after the no-more splits event and is NOT read.

```



*Job Manager logs:*
2023-10-10 17:46:23.576 [SourceCoordinator-Source: parquet-source] INFO  o.a.f.c.file.src.assigners.LocalityAwareSplitAssigner  - Assigning remote split to requesting host '10': Optional[FileSourceSplit: s3://REDACTED/part-1-13189.snappy [0, 94451)  hosts=[localhost] ID=0000000229 position=null]
2023-10-10 17:46:23.576 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - Assigned split to subtask 0 : FileSourceSplit: s3://REDACTED/part-1-13189.snappy [0, 94451)  hosts=[localhost] ID=0000000229 position=null
2023-10-10 17:46:23.786 [SourceCoordinator-Source: parquet-source] INFO  o.apache.flink.runtime.source.coordinator.SourceCoordinator  - Source Source: parquet-source received split request from parallel task 1 (#0)
2023-10-10 17:46:23.786 [SourceCoordinator-Source: parquet-source] DEBUG o.a.f.c.base.source.hybrid.HybridSourceSplitEnumerator  - handleSplitRequest subtask=1 sourceIndex=11 pendingSplits={}
2023-10-10 17:46:23.786 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - Subtask 1 (on host '10.4.168.40') is requesting a file source split
2023-10-10 17:46:23.786 [SourceCoordinator-Source: parquet-source] INFO  o.a.f.c.file.src.assigners.LocalityAwareSplitAssigner  - Assigning remote split to requesting host '10': Optional[FileSourceSplit: s3://REDACTED/part-0-13127.snappy [0, 88108)  hosts=[localhost] ID=0000000045 position=null]
2023-10-10 17:46:23.786 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - Assigned split to subtask 1 : FileSourceSplit: s3://REDACTED/part-0-13127.snappy [0, 88108)  hosts=[localhost] ID=0000000045 position=null
2023-10-10 17:46:24.013 [SourceCoordinator-Source: parquet-source] INFO  o.apache.flink.runtime.source.coordinator.SourceCoordinator  - Source Source: parquet-source received split request from parallel task 0 (#0)
2023-10-10 17:46:24.013 [SourceCoordinator-Source: parquet-source] DEBUG o.a.f.c.base.source.hybrid.HybridSourceSplitEnumerator  - handleSplitRequest subtask=0 sourceIndex=11 pendingSplits={}
2023-10-10 17:46:24.013 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - Subtask 0 (on host '10.4.192.125') is requesting a file source split
2023-10-10 17:46:24.013 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - No more splits available for subtask 0
2023-10-10 17:46:24.049 [SourceCoordinator-Source: parquet-source] INFO  o.apache.flink.runtime.source.coordinator.SourceCoordinator  - Source Source: parquet-source received split request from parallel task 1 (#0)
2023-10-10 17:46:24.050 [SourceCoordinator-Source: parquet-source] DEBUG o.a.f.c.base.source.hybrid.HybridSourceSplitEnumerator  - handleSplitRequest subtask=1 sourceIndex=11 pendingSplits={}
2023-10-10 17:46:24.050 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - Subtask 1 (on host '10.4.168.40') is requesting a file source split
2023-10-10 17:46:24.050 [SourceCoordinator-Source: parquet-source] INFO  o.a.flink.connector.file.src.impl.StaticFileSplitEnumerator  - No more splits available for subtask 1
2023-10-10 17:46:24.051 [SourceCoordinator-Source: parquet-source] DEBUG o.apache.flink.runtime.source.coordinator.SourceCoordinator  - Source Source: parquet-source received custom event from parallel task 1 (#0): SourceReaderFinishedEvent\{sourceIndex=11}
2023-10-10 17:46:24.051 [SourceCoordinator-Source: parquet-source] DEBUG o.a.f.c.base.source.hybrid.HybridSourceSplitEnumerator  - handleSourceEvent SourceReaderFinishedEvent\{sourceIndex=11} subtask=1 pendingSplits={}
2023-10-10 17:46:24.051 [SourceCoordinator-Source: parquet-source] DEBUG o.a.f.c.base.source.hybrid.HybridSourceSplitEnumerator  - All readers finished, ready to switch enumerator!

The assigned split is never processed.
I traced the race conditions bug to the HybridSourceSplitEnumerator and HybridSourceSplitReader.
There are race in both the source and the reader side. The attached patch ensures that the switch from one source to another and one reader to another happen in an atomic fashion with respect to the rest of the code. All section of the code that use the currentReader or currentEnumerator are read-locked and the code for reader/enumerator switch is written lock. This ensures that no other function is executed when the switch for reader/enumerator occurs. Applying just the fixes to HybridSourceSplitEnumerator will resolve the majority of the data loss but not all. But, for complete correctness fixes are needed in both locations. Additionally, current readers also needs to be reset before proceeding.

With these fixes applied, our team using Flink, at scale of 1B+ records/hour with 180 Task Managers, did not see any data loss issue. There was also no noticeable impact on performance due to the read-write mutexes and concurrency control.

Additonally, integer comparision of objects needs to use `equals` otherwise it won't work above 128. This [issue|https://www.mail-archive.com/issues@flink.apache.org/msg647008.html] has been reported before, by another user.

If the above fixes are valid, please let me know. I would be happy to create a branch and PR against the repo. I have completed and signed the individual CLA and will be emailing it soon.","Apache Flink 1.16.1
Mac OSX, Linux etc. ",7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/23 23:08;varun1729dd;hybridSourceEnumeratorAndReaderFixes.patch;https://issues.apache.org/jira/secure/attachment/13063992/hybridSourceEnumeratorAndReaderFixes.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,English,,Mon Nov 13 16:02:54 UTC 2023,,,,,,,,,,"0|z1lamo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 17:34;varun1729dd;Similar issues [here|https://www.mail-archive.com/issues@flink.apache.org/msg716124.html];;;","31/Oct/23 21:09;varun1729dd;I had previously incorporated the fixes from [Flink-33360|https://issues.apache.org/jira/browse/FLINK-33360?jql=project%20%3D%20FLINK%20AND%20component%20%3D%20%22Connectors%20%2F%20HybridSource%22], but that does not guarantee complete correctness at high volume.;;;","09/Nov/23 00:23;varun1729dd;Created a PR: https://github.com/apache/flink/pull/23687;;;","13/Nov/23 09:53;fpaul;[~thw@apache.org] [~Leonard]  can you help here?;;;","13/Nov/23 16:02;thw;[~varun1729dd] thanks for investigating this. It would be helpful to understand better why we see the race condition as under the mailbox model this should not happen. Let's continue the discussion on the PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector has broken version,FLINK-33401,13556105,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,pavelhp,pavelhp,30/Oct/23 15:20,14/Nov/23 08:57,04/Jun/24 20:40,14/Nov/23 08:55,,,,,,,,,,,kafka-3.0.1,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"Trying to run Flink 1.18 with Kafka Connector

but official documentation has a bug  

[https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/kafka/]
{noformat}
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka</artifactId>
    <version>-1.18</version>
</dependency>{noformat}
Basically version *-1.18* doesn't exist.",,,,,,,,,,,,,,,,,,,,,,FLINK-33512,,,,,,,,,,,,,,,,,,,,"10/Nov/23 08:07;tanyuxin;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13064303/screenshot-1.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 14 08:55:58 UTC 2023,,,,,,,,,,"0|z1lae8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 03:36;tanyuxin;[~pavelhp] Thanks for reporting this issue. I will take a look at this.

Since the new version of Kafka connector has not been released, the new version can not be available even if the bug is fixed. You can use the old version (https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/kafka/) before the new connector version is released. ;;;","31/Oct/23 08:27;pavelhp;Hello, [~tanyuxin] !

Thanks for feedback!

The problem that I can't run it with 1.17.1 version
{noformat}
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka</artifactId>
    <version>1.17.1</version>
</dependency>{noformat}
I'm getting error:
 
{noformat}
Caused by: java.lang.NoClassDefFoundError: org/apache/flink/shaded/guava30/com/google/common/io/Closer at org.apache.flink.connector.kafka.sink.KafkaWriter.<init>(KafkaWriter.java:109) at org.apache.flink.connector.kafka.sink.KafkaSink.createWriter(KafkaSink.java:111) at org.apache.flink.connector.kafka.sink.KafkaSink.createWriter(KafkaSink.java:57){noformat}
 
 I'm using Java 17.;;;","01/Nov/23 00:23;tanyuxin;[~pavelhp] If you wish to try it in Flink 1.18, you may need to wait for the new release of the new version connector. (Currently, the connector that can adapt to Flink 1.18 has not been released yet.);;;","01/Nov/23 07:29;pavelhp;[~tanyuxin] do you know when KafkaConnector will be released with support Flink 1.18?

For us it is blocker which prevents migration.

Thanks!;;;","02/Nov/23 15:39;ferenc-csaky;In my opinion some restructure would be required in the documentation, because we are still highlighting data stream connectors as ""bundled"", which are not anymore: https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/overview/;;;","06/Nov/23 09:58;tanyuxin;[~pavelhp] I noticed that it has been released. You can find the release thread here: https://lists.apache.org/thread/dn8dw8551ckrm5cw6rs73v4b4zm0vy05. From the given link, the connector's download link can be found: https://www.apache.org/dyn/closer.lua/flink/flink-connector-kafka-3.0.1/flink-connector-kafka-3.0.1-src.tgz.;;;","06/Nov/23 10:36;pavelhp;[~tanyuxin] thank you! I will take a look!;;;","10/Nov/23 08:36;tanyuxin;After merging the Flink 1.19 hotfix (fa1036c73e3bcd66b57d835c7859572ca4b2250d, Remove Kafka documentation for SQL/Table API, since this is now externalized), I conducted tests on the Flink 1.19 version, and it shows the correct version.

I noticed that the hotfix has also been backported to the Flink release-1.18. Once this fix is merged into the Kafka connector repository, then 1.18 documentation will display the accurate version.

 !screenshot-1.png! ;;;","14/Nov/23 08:55;martijnvisser;Fixed in apache/flink-connector-kafka

v3.0: 18d0d770531f58df45af3921169644b6f032bd5d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector doesn't compile for Flink 1.18 due to Archunit update,FLINK-33400,13556079,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tison,martijnvisser,martijnvisser,30/Oct/23 12:39,13/Dec/23 05:41,04/Jun/24 20:40,13/Dec/23 01:58,pulsar-4.0.1,,,,,,,,,,pulsar-4.1.0,,,,Connectors / Pulsar,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 13 01:58:06 UTC 2023,,,,,,,,,,"0|z1la8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/23 01:58;tison;master(4.2) via 707e49472d557bafa58013c17e3194b64fb4b3ef

v4.1 via 417836f1903589d9f1828953848d587dba0e4b33;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support switching from batch to stream mode for KeyedCoProcessOperator and IntervalJoinOperator,FLINK-33399,13556069,13553221,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,30/Oct/23 10:32,30/Oct/23 10:32,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,Support switching from batch to stream mode for KeyedCoProcessOperator and IntervalJoinOperator.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 10:32:20.0,,,,,,,,,,"0|z1la68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support switching from batch to stream mode for one input stream operator,FLINK-33398,13556068,13553221,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,30/Oct/23 10:30,13/Dec/23 03:19,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,pull-request-available,,,,,Introduce the infra to support switching from batch to stream mode for one input stream operator.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 10:30:38.0,,,,,,,,,,"0|z1la60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-373: Support Configuring Different State TTLs using SQL Hint,FLINK-33397,13556057,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,qingyue,qingyue,30/Oct/23 09:28,21/Feb/24 06:12,04/Jun/24 20:40,30/Jan/24 01:37,1.19.0,,,,,,,,,,1.19.0,,,,Table SQL / API,,,,,,0,,,,,,"Please refer to [https://cwiki.apache.org/confluence/display/FLINK/FLIP-373%3A+Support+Configuring+Different+State+TTLs+using+SQL+Hint |https://cwiki.apache.org/confluence/display/FLINK/FLIP-373%3A+Support+Configuring+Different+State+TTLs+using+SQL+Hint] for more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33396,FLINK-33395,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 21 06:12:56 UTC 2024,,,,,,,,,,"0|z1la3k:",9223372036854775807,"This is a new feature in Apache Flink 1.19 that enhances the flexibility and user experience when managing SQL state time-to-live (TTL) settings. Users can now specify custom TTL values for regular joins and group aggregations directly within their queries by [utilizing the STATE_TTL hint](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/hints/#state-ttl-hints).

This improvement means that you no longer need to alter your compiled plan to set specific TTLs for these operators. With the introduction of STATE_TTL hints, you can streamline your workflow and dynamically adjust the TTL based on your operational requirements.",,,,,,,,,,,,,,,,,,,"03/Nov/23 08:10;xuyangzhong;Hi, [~qingyue] .I'd like to take this jira. ;;;","03/Nov/23 08:24;qingyue;Thanks [~xuyangzhong] for the volunteer!;;;","13/Nov/23 01:09;caozhen1937;hi [~qingyue], [~xuyangzhong], can I participate in the development?
 ;;;","17/Nov/23 02:34;xuyangzhong;Hi, [~caozhen1937] The work is on the way. The code is almost ready for it and just need to wait the block Jira FLINK-33395 fixed. I would be grateful if you could help with a review later. ;;;","21/Feb/24 01:35;lincoln.86xy;[~qingyue][~xuyangzhong] Should we add release notes for this ticket since it's a new feature?;;;","21/Feb/24 06:12;qingyue;I've drafted a release note for this feature. cc [~lincoln.86xy] and [~xuyangzhong] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The table alias should be removed in the final plan  when using join hints,FLINK-33396,13556054,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,xuyangzhong,xuyangzhong,xuyangzhong,30/Oct/23 09:17,04/Feb/24 19:40,04/Jun/24 20:40,,1.16.0,1.17.0,1.18.0,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"See the existent test 'NestLoopJoinHintTest#testJoinHintWithJoinHintInCorrelateAndWithAgg', the plan is 
{code:java}
HashJoin(joinType=[LeftSemiJoin], where=[=(a1, EXPR$0)], select=[a1, b1], build=[right], tryDistinctBuildRow=[true])
:- Exchange(distribution=[hash[a1]])
:  +- TableSourceScan(table=[[default_catalog, default_database, T1]], fields=[a1, b1])
+- Exchange(distribution=[hash[EXPR$0]])
   +- LocalHashAggregate(groupBy=[EXPR$0], select=[EXPR$0])
      +- Calc(select=[EXPR$0])
         +- HashAggregate(isMerge=[true], groupBy=[a1], select=[a1, Final_COUNT(count$0) AS EXPR$0])
            +- Exchange(distribution=[hash[a1]])
               +- LocalHashAggregate(groupBy=[a1], select=[a1, Partial_COUNT(a2) AS count$0])
                  +- NestedLoopJoin(joinType=[InnerJoin], where=[=(a2, a1)], select=[a2, a1], build=[right])
                     :- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2], hints=[[[ALIAS options:[T2]]]])
                     +- Exchange(distribution=[broadcast])
                        +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[a1], metadata=[]]], fields=[a1], hints=[[[ALIAS options:[T1]]]])  {code}
The hints about alias should be removed such as ""hints=[[[ALIAS options:[T1]]]]"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33397,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 30 09:19:09 UTC 2023,,,,,,,,,,"0|z1la2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 09:19;xuyangzhong;The alias hints should be removed while visiting subquery. I'll try to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The join hint doesn't work when appears in subquery,FLINK-33395,13556053,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xuyangzhong,xuyangzhong,xuyangzhong,30/Oct/23 09:12,29/Nov/23 03:43,04/Jun/24 20:40,17/Nov/23 02:15,1.16.0,1.17.0,1.18.0,,,,,,,,1.17.3,1.18.1,1.19.0,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"See the existent test 'NestLoopJoinHintTest#testJoinHintWithJoinHintInCorrelateAndWithAgg', the test plan is 
{code:java}
HashJoin(joinType=[LeftSemiJoin], where=[=(a1, EXPR$0)], select=[a1, b1], build=[right], tryDistinctBuildRow=[true])
:- Exchange(distribution=[hash[a1]])
:  +- TableSourceScan(table=[[default_catalog, default_database, T1]], fields=[a1, b1])
+- Exchange(distribution=[hash[EXPR$0]])
   +- LocalHashAggregate(groupBy=[EXPR$0], select=[EXPR$0])
      +- Calc(select=[EXPR$0])
         +- HashAggregate(isMerge=[true], groupBy=[a1], select=[a1, Final_COUNT(count$0) AS EXPR$0])
            +- Exchange(distribution=[hash[a1]])
               +- LocalHashAggregate(groupBy=[a1], select=[a1, Partial_COUNT(a2) AS count$0])
                  +- NestedLoopJoin(joinType=[InnerJoin], where=[=(a2, a1)], select=[a2, a1], build=[right])
                     :- TableSourceScan(table=[[default_catalog, default_database, T2, project=[a2], metadata=[]]], fields=[a2], hints=[[[ALIAS options:[T2]]]])
                     +- Exchange(distribution=[broadcast])
                        +- TableSourceScan(table=[[default_catalog, default_database, T1, project=[a1], metadata=[]]], fields=[a1], hints=[[[ALIAS options:[T1]]]]) {code}
but the NestedLoopJoin should broadcase left side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33397,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 15 02:29:57 UTC 2023,,,,,,,,,,"0|z1la2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 09:14;xuyangzhong;The root case is that the join hint is not propagated inside the subquery. I'll try to fix it. ;;;","30/Oct/23 09:19;qingyue;[~xuyangzhong] Thanks for reporting the issue. Assigned to you.;;;","15/Nov/23 02:29;qingyue;master: 1d08b8bf6a67c6a1874e8003868d5735248a7e45

release-1.18: 10bc43889243e3ea2f6fb1e76eca7b65ba5eb491

release-1.17: c1b5d71b4b9adf0c3b8f1c77af01f030a5c92626;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataGeneratorSourceITCase.testGatedRateLimiter fails on AZP,FLINK-33394,13556039,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,30/Oct/23 08:13,17/Apr/24 14:35,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,,,Connectors / Common,,,,,,0,test-stability,,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54054&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=22927
fails on AZP as
{noformat}
Oct 26 07:37:41 [1L, 1L, 1L, 1L, 1L, 1L]
Oct 26 07:37:41 	at org.apache.flink.connector.datagen.source.DataGeneratorSourceITCase.testGatedRateLimiter(DataGeneratorSourceITCase.java:200)
Oct 26 07:37:41 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Oct 26 07:37:41 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Oct 26 07:37:41 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Oct 26 07:37:41 	at java.lang.reflect.Method.invoke(Method.java:498)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 17 14:35:08 UTC 2024,,,,,,,,,,"0|z1l9zk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 09:43;mapohl;https://github.com/apache/flink/actions/runs/7953599343/job/21710033908#step:10:22329;;;","17/Apr/24 14:35;rskraba;This should be closed as a duplicate of FLINK-31421 (Weirdly enough, I can't link two Jira ... unless another link already exists!);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink document description error,FLINK-33393,13556033,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,caicancai,caicancai,caicancai,30/Oct/23 07:37,30/Oct/23 11:20,04/Jun/24 20:40,30/Oct/23 11:20,1.17.1,,,,,,,,,,1.19.0,,,,Documentation,,,,,,0,pull-request-available,,,,,"flink document description error, function part description error",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/23 07:41;caicancai;捕获.PNG;https://issues.apache.org/jira/secure/attachment/13063976/%E6%8D%95%E8%8E%B7.PNG",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 30 11:19:56 UTC 2023,,,,,,,,,,"0|z1l9y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 11:19;fanrui;Merged to master<1.19> via : 806147c3233a47eacaa630dca5fdfef83397ab31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the documentation page for balanced tasks scheduling,FLINK-33392,13556031,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,30/Oct/23 07:25,30/Oct/23 07:30,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:25:49.0,,,,,,,,,,"0|z1l9xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support tasks balancing at TM level for Adaptive Scheduler,FLINK-33391,13556030,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,RocMarshal,RocMarshal,30/Oct/23 07:25,30/Oct/23 07:30,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:25:18.0,,,,,,,,,,"0|z1l9xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support slot balancing at TM level for Adaptive Scheduler,FLINK-33390,13556029,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,RocMarshal,RocMarshal,30/Oct/23 07:24,30/Oct/23 07:30,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:24:39.0,,,,,,,,,,"0|z1l9xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support tasks balancing at slot level for Adaptive Scheduler,FLINK-33389,13556028,13532014,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,RocMarshal,RocMarshal,30/Oct/23 07:19,30/Oct/23 07:33,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:19:49.0,,,,,,,,,,"0|z1l9x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support tasks balancing at TM level for Default Scheduler,FLINK-33388,13556027,13532014,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,30/Oct/23 07:19,01/Nov/23 09:16,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:19:11.0,,,,,,,,,,"0|z1l9ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the abstraction and the interface about loading,FLINK-33387,13556026,13532014,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,30/Oct/23 07:17,01/Nov/23 09:16,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:17:59.0,,,,,,,,,,"0|z1l9wo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support tasks balancing at slot level for Default Scheduler,FLINK-33386,13556025,13532014,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,30/Oct/23 07:16,17/Jan/24 09:12,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Task,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 07:16:46.0,,,,,,,,,,"0|z1l9wg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink connector redshift Sink API implementation ,FLINK-33385,13556008,13533204,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,30/Oct/23 04:09,10/Nov/23 20:41,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-30 04:09:55.0,,,,,,,,,,"0|z1l9so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MySQL JDBC driver is deprecated,FLINK-33384,13555843,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,davidradl,davidradl,davidradl,27/Oct/23 13:21,02/Nov/23 15:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,Table SQL / JDBC,,,,,,0,,,,,,"I see when running tests on the JDBC connector, I get a warning

_Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary._
 

I suggest we change the class to be loaded from the old to the new non deprecated class name.

 

I am happy to implement this and do testing on it. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 15:40:35 UTC 2023,,,,,,,,,,"0|z1l8s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Oct/23 10:50;leonard;Thanks [~davidradl] for filling this ticket, assign to you.;;;","01/Nov/23 17:51;davidradl;It looks like  Oracle now recommend using a different driver for mySQL - here is the discussion  [https://blogs.oracle.com/mysql/post/mysql-connectorj-has-new-maven-coordinates.|https://blogs.oracle.com/mysql/post/mysql-connectorj-has-new-maven-coordinates]

 

I see that the mySQL tests bring in this dependancy as well.
<dependency>
<groupId>mysql</groupId>
<artifactId>mysql-connector-java</artifactId>
<version>8.0.29</version>
<scope>test</scope>
</dependency>
 
it is required to resolve an import in test code MySqlMetadata.java.
 
I could not get these tests to run in main. 
 

 

 ;;;","02/Nov/23 15:40;davidradl;note that I also see a deprecation message in the log as below: 

cat flink-davidradley-taskexecutor-0-Davids-MBP-2.out

Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-quickstart-scala.sh is not supported anymore since 1.17 but is used in flink-web helper scripts,FLINK-33383,13555830,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,27/Oct/23 12:21,30/Oct/23 08:11,04/Jun/24 20:40,,1.17.1,1.18.0,1.19.0,,,,,,,,,,,,Project Website,,,,,,0,starter,,,,,"The {{_includes/q/quickstart-scala.sh}} script does work for Flink 1.17 anymore. Analogously, keeping \{{_includes/q/quickstart-scala-SNAPSHOT.sh}} doesn't make sense. It still works for 1.17 because there were some SNAPSHOT artifacts uploaded. But it would start failing with Flink 1.18.

We might want to remove the scripts. The execution failure due to missing Maven artifacts for 1.17+:
{code:bash}
[WARNING] The POM for org.apache.flink:flink-quickstart-scala:jar:1.17.0 is missing, no dependency information available
Downloading from central: https://repo.maven.apache.org/maven2/org/apache/flink/flink-quickstart-scala/1.17.0/flink-quickstart-scala-1.17.0.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  11.050 s
[INFO] Finished at: 2023-10-27T14:13:33+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-archetype-plugin:3.2.1:generate (default-cli) on project standalone-pom: The desired archetype does not exist (org.apache.flink:flink-quickstart-scala:1.17.0) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29740,,,,,,,,,,FLINK-30768,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 30 08:11:23 UTC 2023,,,,,,,,,,"0|z1l8p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 12:23;mapohl;[~martijnvisser] do you agree that deleting these scripts is the right thing to do? One could also consider it public API because it's ""code"" that's customer/user-facing. But since it's more or less documentation it should be fine to remove these scripts to not encourage users to continue using Scala. WDYT?;;;","30/Oct/23 08:11;martijnvisser;I think we either remove the existing quickstart, or we have to create a new Quickstart that uses the Flink Java APIs in Scala;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Python Environment Manager Fails with Pip --install-option in Recent Pip Versions,FLINK-33382,13555818,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hadjinik,hadjinik,27/Oct/23 09:28,27/Oct/23 09:28,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,,,,,,0,,,,,,"I encountered an issue when running Flink jobs that use Python dependencies. The underlying problem seems to stem from Flink's use of the {{--install-option}} argument when calling {{pip install}} to manage Python dependencies. This argument has been deprecated in newer versions of pip, resulting in job failures.

This issue forces users to downgrade their {{pip}} version as a temporary workaround, which is not ideal due to potential security vulnerabilities and missing features in older {{pip}} versions.



{*}Steps to Reproduce{*}:
 # Setup a Flink job with Python dependencies.
 # Use a Python environment with a {{pip}} version >= 21.
 # Run the Flink job.
 # Observe the error: {{{}no such option: --install-option{}}}.

{*}Error Logs{*}:

 
pythonCopy code
{{no such option: --install-option...}}
(You can add more logs here if needed)

{*}Expected Behavior{*}: Flink should handle Python dependency management in a way that's compatible with newer versions of {{{}pip{}}}.



{*}Possible Solutions{*}:
 # Update Flink's Python dependency management code to remove or replace the {{--install-option}} argument.
 # Provide Flink configuration options to customize the {{pip install}} command or to skip certain options.","* Flink version: 1.17.1
 * Python version: 3.10
 * Pip version: >= 21
 * Operating System: [MacOS]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python3,,2023-10-27 09:28:43.0,,,,,,,,,,"0|z1l8mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support split big parquet file to multi InputSplits,FLINK-33381,13555805,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,27/Oct/23 08:50,27/Oct/23 08:50,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / Hive,,,,,,0,,,,,,"Currently, Flink only supports dividing a Parquet file into a split.
But in some cases, one parquet file is too big for one task.
In this case, we need to split one parquet file to multi splits.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-27 08:50:46.0,,,,,,,,,,"0|z1l8jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump flink version on flink-connectors-mongodb,FLINK-33380,13555800,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mangguozhi,jiabao.sun,jiabao.sun,27/Oct/23 08:28,31/Oct/23 12:50,04/Jun/24 20:40,31/Oct/23 12:50,mongodb-1.0.2,,,,,,,,,,mongodb-1.1.0,,,,Connectors / MongoDB,,,,,,0,pull-request-available,,,,,"As Flink 1.18 released, bump the flink version in mongodb connector .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 12:50:48 UTC 2023,,,,,,,,,,"0|z1l8ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 08:32;mangguozhi;[~jiabao.sun] This task doesn't seem to be very difficult to complete. Can you assign it to me?;;;","27/Oct/23 08:37;martijnvisser;The value that's set in https://github.com/apache/flink-connector-mongodb/blob/main/pom.xml#L56 should be set to the lowest Flink version that's supported; the only ""upgrade"" is see is validate if the CI passes and make sure that's set to the correct versions;;;","27/Oct/23 08:39;jiabao.sun;Thanks [~mangguozhi] for the warm.
Hi [~renqs], could you help assign this ticket to Tingkai?;;;","27/Oct/23 08:45;mangguozhi;hi, in [https://github.com/apache/flink-connector-mongodb/blob/main/.github/workflows/push_pr.yml]

 

flink version is 1.17-SNAPSHOT, 1.18-SNAPSHOT we should modify it.;;;","27/Oct/23 08:46;jiabao.sun;Thanks [~martijnvisser] for the suggestion.
We only need to add new versions CI check in push_pr.yml and weekly.yml.

By the way, could you help assign this ticket to [~mangguozhi].
Thanks a lot.;;;","27/Oct/23 08:46;martijnvisser;Yes, but then you should also include 1.19-SNAPSHOT;;;","27/Oct/23 08:52;mangguozhi;[~martijnvisser] Thank you for your suggestion. I will finish it.;;;","31/Oct/23 12:50;martijnvisser;Fixed in:

apache/flink-connector-mongodb:main 6636b62528a63a11b9aee3d6408ec3c440904096
apache/flink-connector-mongodb:v1.0 d6ef9663b14662c8920c51be351505612f681fad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump CI flink version on flink-connector-elasticsearch,FLINK-33379,13555798,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,liyubin117,liyubin117,liyubin117,27/Oct/23 07:56,20/Apr/24 03:41,04/Jun/24 20:40,31/Oct/23 14:21,elasticsearch-3.1.0,,,,,,,,,,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,,,0,pull-request-available,,,,,"As Flink 1.18 released, bump the flink version in es connector .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/23 08:54;liyubin117;image-2023-10-27-16-54-04-937.png;https://issues.apache.org/jira/secure/attachment/13063933/image-2023-10-27-16-54-04-937.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 20 03:41:32 UTC 2024,,,,,,,,,,"0|z1l8i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 08:18;liyubin117;[~martijnvisser] Hi, we have made heavy usage of elasticsearch connector, eager to see the upgrade take effect, Could you please assign this to me? thanks!;;;","27/Oct/23 08:35;martijnvisser;[~liyubin117] I don't think there's any upgrade needed. The value for https://github.com/apache/flink-connector-elasticsearch/blob/main/pom.xml#L53 should be set to the lowest version of Flink that's supported and the CI is already passing for Flink 1.18?;;;","27/Oct/23 08:42;liyubin117;[~martijnvisser]

now supported version in CI is ""1.17.1, 1.18-SNAPSHOT"", as the flink version released, it should be ""1.17.1, 1.18.0"", WDYT?

[https://github.com/apache/flink-connector-elasticsearch/blob/main/.github/workflows/push_pr.yml#L28] ;;;","27/Oct/23 08:45;martijnvisser;[~liyubin117] Then I think it should be ""1.17.1, 1.1.8.0, 1.19-SNAPSHOT"" and also updating the weekly tests to test for 1.19-SNAPSHOT;;;","27/Oct/23 08:49;liyubin117;[~martijnvisser] excellent idea! we also need to make CI work in the latest version 1.19-SNAPSHOT.;;;","27/Oct/23 08:57;liyubin117;[~martijnvisser] I have found some enforcer error as follow and ready to solve it.

!image-2023-10-27-16-54-04-937.png!;;;","27/Oct/23 09:04;martijnvisser;Yes, then it's a proper upgrade to 1.19-SNAPSHOT indeed;;;","31/Oct/23 14:21;martijnvisser;Fixed in apache/flink-connector-elasticsearch:main 6636b62528a63a11b9aee3d6408ec3c440904096;;;","06/Dec/23 12:03;pgodowski;Hello,

What is the expected timeline of publishing the elasticsearch connector (I pressume it would be 3.1.0), supporting Flink 1.18 please?;;;","20/Apr/24 03:41;liyubin117;[~pgodowski] sorry for the late reply, it seems that 3.1.0 branch not created yet, you could compile using master branch to support Flink 1.18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepare actions for flink version 1.18,FLINK-33378,13555795,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eskabetxe,eskabetxe,eskabetxe,27/Oct/23 07:50,26/Nov/23 23:32,04/Jun/24 20:40,26/Nov/23 23:32,,,,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,"With the release of Flink 1.18, bump flink version on connector ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Nov 26 23:32:09 UTC 2023,,,,,,,,,,"0|z1l8hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/23 23:32;Sergey Nuyanzin;Merged as [79e7509256753765813d2e2c970506414de3302a|https://github.com/apache/flink-connector-jdbc/commit/79e7509256753765813d2e2c970506414de3302a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When Flink version >= 1.15 and Flink Operator is used, there is a waste of resources when running Flink batch jobs.",FLINK-33377,13555705,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hjw,hjw,26/Oct/23 16:21,26/Oct/23 16:22,04/Jun/24 20:40,,kubernetes-operator-1.5.0,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"According to [FLINK-29376|https://issues.apache.org/jira/browse/FLINK-29376],SHUTDOWN_ON_APPLICATION_FINISH always be set false when Flink  version 1.15 and above.

However,the JobManager still exists after a Flink batch job runs normally,Is this a waste of resources？",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 26 16:22:24 UTC 2023,,,,,,,,,,"0|z1l7xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/23 16:22;hjw;[~gsomogyi] Can you take a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend Curator config option for Zookeeper configuration,FLINK-33376,13555693,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Oleksandr Nitavskyi,Oleksandr Nitavskyi,Oleksandr Nitavskyi,26/Oct/23 15:13,28/Mar/24 09:59,04/Jun/24 20:40,28/Mar/24 09:59,,,,,,,,,,,1.20.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"In certain cases ZooKeeper requires additional Authentication information. For example list of valid [names for ensemble|https://zookeeper.apache.org/doc/r3.8.0/zookeeperAdmin.html#:~:text=for%20secure%20authentication.-,zookeeper.ensembleAuthName,-%3A%20(Java%20system%20property] in order to prevent the accidental connecting to a wrong ensemble.

Curator allows to add additional AuthInfo object for such configuration. Thus it would be useful to add one more additional Map property which would allow to pass AuthInfo objects during Curator client creation.

*Acceptance Criteria:* For Flink users it is possible to configure auth info list for Curator framework client.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 28 09:59:11 UTC 2024,,,,,,,,,,"0|z1l7uo:",9223372036854775807,"Adds support for the following curator parameters: high-availability.zookeeper.client.authorization (corresponding curator parameter: authorization), high-availability.zookeeper.client.max-close-wait (corresponding curator parameter: maxCloseWaitMs), high-availability.zookeeper.client.simulated-session-expiration-percent (corresponding curator parameter: simulatedSessionExpirationPercent).",,,,,,,,,,,,,,,,,,,"26/Oct/23 15:15;Oleksandr Nitavskyi;For implementation we could add an additional Map config option and Flink users will be able to pass AuthInfo.
There is some miss-alignment, AuthInfo type is <String, byte[]> while Map is <String, String>.
As simplest workaround we get accept <String, String> on Flink config interface and use _getBytes()_ method in order to adapt interfaces.;;;","27/Oct/23 09:58;mapohl;Thanks for bringing this up. About your proposal: Keep in mind that there's a default charset defined in Flink (see [ConfigConstants#1767|https://github.com/apache/flink/blob/1354d2fae3fde2a448ce1fac5dee7859973a93e1/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java#L1767]) which we might want to use here.

AFAIU, it would require a new configuration parameter, wouldn't it? If that's the case we might want to trigger a discussion on the dev ML (since configuration parameters are public API). I understand that we didn't do this in FLINK-31780. I just want to follow the process properly this time.

On another note: Are you interested in picking this issue up and driving it?;;;","27/Oct/23 13:40;Oleksandr Nitavskyi;[~mapohl] sure, would be glad to drive this! 

Sorry, didn't know that we need to start a dev list discussion (didn't make it for several config options in the past). Will drop an email to ensure visibility with the community.;;;","27/Oct/23 13:45;mapohl;No worries. Thanks. I assigned the issue to you. There is a definition on when a FLIP should be created in the [Flink wiki|https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals#FlinkImprovementProposals-Whatisconsidereda%22majorchange%22thatneedsaFLIP?]. That states that we want to collect FLIPs for public API (which includes configuration parameter).;;;","27/Oct/23 13:56;Oleksandr Nitavskyi;Thanks for the link. If we really require a Flip, I think it would be nice to anticipate a bit more generic way to configure the Curator framework.;;;","27/Oct/23 14:48;mapohl;Sounds like a good idea. We could think about utilizing the namespaces. The FLIP could propose adding namespace support for {{curator}} -and {{zookeeper}}-. That would allow to load any parameter supported by these systems. WDYT?;;;","27/Oct/23 17:23;Oleksandr Nitavskyi;It would be really good to be able to support something generic enough to translate Flink configuration into Curator config, e.g. like in [hadoop config|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#flink-hadoop-%3Ckey%3E]. 

But since Curator uses the Builder pattern I do not see how can we make it generic enough. Probably as compromise it would be sane to consider to add support for all missing Curator configurations. 
If we go this way here is the list of configurations which Flink doesn't configure at all for now:
* [authorization|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#authorization(java.lang.String,byte%5B%5D)]
* [canBeReadOnly|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#canBeReadOnly(boolean)]
* [compressionProvider|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#compressionProvider(org.apache.curator.framework.api.CompressionProvider)]
* [defaultData|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#defaultData(byte%5B%5D)]
* [dontUseContainerParents|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#dontUseContainerParents()]/ [useContainerParentsIfAvailable|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#useContainerParentsIfAvailable()]
* [maxCloseWaitMs|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#maxCloseWaitMs(int)]
* [namespace|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#namespace(java.lang.String)]
* [runSafeService|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#runSafeService(java.util.concurrent.Executor)]
* [schemaSet|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#schemaSet(org.apache.curator.framework.schema.SchemaSet)]
* [simulatedSessionExpirationPercent|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#simulatedSessionExpirationPercent(int)]
* [waitForShutdownTimeoutMs|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#waitForShutdownTimeoutMs(int)]

;;;","30/Oct/23 17:03;mapohl;hm, good point.

About the different configuration options you mentioned ((/) should be exposed to the user, (x) should NOT be exposed to the user, (?) debatable; (!) should NOT be exposed to the user but might be useful within Flink):
 * [authorization|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#authorization(java.lang.String,byte%5B%5D)]: (/) This is the option which you want to expose to allow additional AuthInfo records as part of the connect, correct?
 * [canBeReadOnly|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#canBeReadOnly(boolean)]: (x) I'm not sure whether that's what we want. This would allow the client to read data from a ZK node that is cut off from the other nodes due to some network partition. AFAIU, we would increase the risk of ending up in an inconsistent state on Flink's side. WDYT?
 * [compressionProvider|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#compressionProvider(org.apache.curator.framework.api.CompressionProvider)]: (?) This configuration parameter can be used to specify a compression algorithm for the data that's sent. That might be useful. But generally, there's not much data written to ZK as far as I know. It's usually only a reference. The BLOB itself is stored on the FileSystem.
 * [defaultData|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#defaultData(byte%5B%5D)]: (x) I cannot think of a use-case where this is needed by the user. IIUC, it's used to specify data that's written/returned if no data is specified. Flink doesn't use this functionality and I don't see how it would be useful to the user to expose this feature.
 * [dontUseContainerParents|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#dontUseContainerParents()]/ [useContainerParentsIfAvailable|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#useContainerParentsIfAvailable()]: (!) This sounds like a property that is useful for Flink's leader election cleanup. But I don't see extra value in exposing the property to the user.
 * [maxCloseWaitMs|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#maxCloseWaitMs(int)]: (/) That might be a property that could be useful to the user. It would enable the user to adjust to different network speeds.
 * [namespace|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#namespace(java.lang.String)] (x) This one is already in use (see {{{}high-availability.zookeeper.path.root{}}})
 * [runSafeService|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#runSafeService(java.util.concurrent.Executor)]: (x) That seems to be a feature that's Flink-specific and shouldn't be handled by the user.
 * [schemaSet|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#schemaSet(org.apache.curator.framework.schema.SchemaSet)]: (!) That feels like a way to harden the internal contract between Flink and ZooKeeper. It might be nice-to-have to harden for Flink. But it shouldn't be exposed to the user, IMHO.
 * [simulatedSessionExpirationPercent|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#simulatedSessionExpirationPercent(int)]: (/) That one seems to be reasonable to be exposed.
 * [waitForShutdownTimeoutMs|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#waitForShutdownTimeoutMs(int)]: (x) This one can be used if you want Flink to wait for the resource cleanup on the ZK side, AFAIU. It feels like this is internal Flink logic and should be exposed.

I'm curious what you think about it.;;;","06/Nov/23 13:29;Oleksandr Nitavskyi;Thanks for the detailed listing. It sounds reasonable to add configuration in public Flink interface for only those options:
* [authorization|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#authorization(java.lang.String,byte%5B%5D)]
* [maxCloseWaitMs|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#maxCloseWaitMs(int)]
* [simulatedSessionExpirationPercent|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#simulatedSessionExpirationPercent(int)]

It also make sense to not touch the [compressionProvider|https://curator.apache.org/apidocs/org/apache/curator/framework/CuratorFrameworkFactory.Builder.html#compressionProvider(org.apache.curator.framework.api.CompressionProvider)] since Flink doesn't store any BLOB data in zookeeper.

In general it make sense to expose only connection related configuration (auth/timeouts) since it depends on the Flink's user environment.


Once we have aligned on the set of the options and the fact that we basically would add 3 missing options, we can start the documentation process. Am I right?;;;","20/Nov/23 14:51;mapohl;Yes, feel free to create a FLIP on that issue (sorry for the late reply).;;;","28/Mar/24 09:59;mapohl;master: [83f82ab0c865a4fa9e119c96e11e0fb3df4a5ecd|https://github.com/apache/flink/commit/83f82ab0c865a4fa9e119c96e11e0fb3df4a5ecd];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a RestoreTestBase,FLINK-33375,13555671,13415855,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,26/Oct/23 13:40,30/Jan/24 06:04,04/Jun/24 20:40,28/Oct/23 09:15,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,Add a test base class for writing restore tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33421,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 30 06:04:28 UTC 2024,,,,,,,,,,"0|z1l7ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/23 09:15;dwysakowicz;Implemented in dcce3764a4500b2006cd260677169d14c553a3eb..d863ff38c0671255df2452c79dad88fa47e2bc0c;;;","30/Jan/24 06:04;qingyue;Hi [~dwysakowicz], recently, we found an issue in some tests([a case|https://github.com/apache/flink/pull/24179]) with state retention enabled, the 
lastAccessTimestamp is persisted in the `_metadata` file as well, which leads to the restore test failure after the TTL expired. Shall we take this into consideration?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execute REMOVE JAR command failed via SQL gateway ,FLINK-33374,13555669,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,yesorno,yesorno,26/Oct/23 13:34,26/Oct/23 13:34,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Table SQL / Gateway,,,,,,0,,,,,,"Execute the below steps could reproduce the exception:

At first, I added a specified jar to the classloader via the ADD JAR command, and using the SHOW JARS command also displayed the jars. 

But the REMOVE JAR command is not supported right now.

 
{code:java}
Caused by: java.lang.UnsupportedOperationException: SQL Gateway doesn't support REMOVE JAR syntax now.
        at org.apache.flink.table.gateway.service.operation.OperationExecutor.callRemoveJar(OperationExecutor.java:550) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
        at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:442) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
        at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
        at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
        at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0] {code}
It seems the RemoveJarOperation is ignored.

https://github.com/apache/flink/blob/release-1.18.0/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/service/operation/OperationExecutor.java#L550",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-26 13:34:34.0,,,,,,,,,,"0|z1l7pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Capture build scans on ge.apache.org to benefit from deep build insights,FLINK-33373,13555662,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,clayburn,clayburn,clayburn,26/Oct/23 13:03,22/Dec/23 20:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,Build System / CI,,,,,,0,pull-request-available,,,,,"This improvement will enhance the functionality of the Flink build by publishing build scans to [ge.apache.org|https://ge.apache.org/], hosted by the Apache Software Foundation and run in partnership between the ASF and Gradle. This Develocity instance has all features and extensions enabled and is freely available for use by the Apache Flink project and all other Apache projects.

On this Develocity instance, Apache Flink will have access not only to all of the published build scans but other aggregate data features such as:
 * Dashboards to view all historical build scans, along with performance trends over time
 * Build failure analytics for enhanced investigation and diagnosis of build failures
 * Test failure analytics to better understand trends and causes around slow, failing, and flaky tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 22 20:40:52 UTC 2023,,,,,,,,,,"0|z1l7ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 07:56;mapohl;Thanks for this proposal, [~clayburn]. tbh, I'm not that familiar with Develocity. From what I found (e.g. [this blog post|https://gradle.com/blog/determine-the-root-cause-of-github-actions-failures-faster-with-gradle-enterprise/]), it connects with the CI build (in our case Azure CI) and scans the build artifacts. It provides a more organized UI for browsing test failures and determining performance bottlenecks in the build pipeline. It looks like it supports Maven projects (like Flink) along Gradle projects. Is this correct?

I have a few questions:
 * Does it provide other features along the ones I mentioned above?
 * Can we set up a test run where we could evaluate how it works with Flink before merging it to {{{}master{}}}?
 * Is this something that is advised to run on {{master}} and the release branches only? Or would this also be something that could be enabled for PRs?
 * We're currently looking into migrating from Azure CI to Github Actions (FLINK-27075). But I assume that wouldn't be such a problem for the ge.apache.org integration, would it? Would we be able to preserve the history already gathered by the Azure CI runs when moving to Github Actions?

I'm looking forward to your response.;;;","27/Oct/23 13:25;clayburn;Good questions [~mapohl]!
{quote}I'm not that familiar with Develocity

It looks like it supports Maven projects (like Flink) along Gradle projects. Is this correct?
{quote}
If it helps, Develocity was originally called Gradle Enterprise, but was recently name Develocity since it supports Maven projects as well as Gradle projects. Support for Bazel and sbt are also being implemented.
{quote}it connects with the CI build (in our case Azure CI)
{quote}
Correct, although it is also able to connect to local builds if an ASF committer opts-in by creating an [access key|https://docs.gradle.com/enterprise/maven-extension/#automated_access_key_provisioning] on their own machine.
{quote}scans the build artifacts
{quote}
I wouldn't quite say that it scans the build ""artifacts"". It is not doing deep analysis on the jar files produced by the build, for example. Essentially, it is monitoring the build for events and then uploading them as a build scan at the completion of the build, so that you can get the build scan and understand what all happened during a build.
{quote}Does it provide other features along the ones I mentioned above?
{quote}
Yes, there is much data in the build scan. In addition to what you mentioned, there is detailed information on dependency downloads and resolution, which can be helpful in investigating dependency resolution issues. There is also basic metadata, such as the infrastructure of the build machine, which become much more useful when comparing two builds against each other to see how they differ.

In addition, by aggregating all of this data across many builds, you can start to understand your build performance over time as well as the behavior of tests and the common reasons for build failures. This data aggregation can help determine how best to address any pain points in your build. One feature that has resonated with many ASF teams that we have talked to is the detection and reporting on flaky tests.

There are also performance features, such as caching, test distribution, and predictive test selection that can be future areas to explore in order to make builds faster.
{quote}Is this something that is advised to run on {{master}} and the release branches only? Or would this also be something that could be enabled for PRs?
{quote}
We recommend enabling this for all builds, including PR builds. Accumulating as much data as possiible gives the best picture of build times, test failures, etc. One caveat with PRs though is that (at least on GitHub with Actions) is that PRs may not have access to secrets when run from forks. So PR builds from contributor forks may not have the permission to upload the scan at the end of the build. We are working on ways to address this.
{quote}Can we set up a test run where we could evaluate how it works with Flink before merging it to {{{}master{}}}?
{quote}
Yes you could. We would need to get ASF Infra to create a CI user specifically for Flink. Then, whoever can do so on Azure, would need to set the key as a secret in Azure. The PR already expects a `GE_ACCESS_KEY` to exist. From there, a build should publish scans. I verified this on my Azure account. We'd be happy to help out with that test.
{quote}We're currently looking into migrating from Azure CI to Github Actions (FLINK-27075). But I assume that wouldn't be such a problem for the ge.apache.org integration, would it? Would we be able to preserve the history already gathered by the Azure CI runs when moving to Github Actions?
{quote}
Develocity is agnostic when it comes to the CI tool used, so either Azure CI or GitHub Actions will work just fine. Currently, ge.apache.org is retaining all data, so you would keep access to the Azure data. Migration is one case where Develocity can help, especially through the build scan comparison feature. It can be helpful to compare builds that ran on one CI vs the other to see what may be different about them.;;;","27/Oct/23 13:42;mapohl;[~chesnay] could you have a look at this? I think it would be interesting to set up a test run. That way we would get a feeling for what we could get out of this. My concern is that the system would struggle with out custom e2e test setup (when parsing the e2e test logs) for instance.;;;","27/Oct/23 14:00;clayburn;I have reached out to Infra to get the proper account created so you can provision the access key.;;;","11/Dec/23 18:32;clayburn;Hi [~mapohl], it was nice to connect with you on the ASF Infrastructure roundtable the other day. I see that we had some conversation on the ticket here, but let me know if there are any other questions you have about this!;;;","22/Dec/23 20:24;mapohl;Sure, thanks. We might go ahead with the Develocity integration in the context of the GitHub Actions migration ([FLIP-396|https://cwiki.apache.org/confluence/display/FLINK/FLIP-396%3A+Trial+to+test+GitHub+Actions+as+an+alternative+for+Flink%27s+current+Azure+CI+infrastructure]). I'm gonna get back to you if need. Thanks for the offer. (y);;;","22/Dec/23 20:40;clayburn;Sounds good. The ASF Infra team has made available an access key under the GE_ACCESS_TOKEN secret for all GitHub Actions workflows within the Apache organization. For example usage see here: https://github.com/apache/iotdb/blob/master/.github/workflows/unit-test.yml#L30.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cryptic exception for a sub query in a CompiledPlan,FLINK-33372,13555661,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dwysakowicz,dwysakowicz,26/Oct/23 13:01,31/Oct/23 09:20,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"SQL statements with a SUBQUERY can be compiled to a plan, but such plans can not be executed and they fail with a cryptic exception.

Example:

{code}
final CompiledPlan compiledPlan = tEnv.compilePlanSql(""insert into MySink SELECT * FROM LATERAL TABLE(func1(select c from MyTable))"");
        tEnv.loadPlan(PlanReference.fromJsonString(compiledPlan.asJsonString())).execute();
{code}

fails with:
{code}
org.apache.flink.table.planner.codegen.CodeGenException: Unsupported call: $SCALAR_QUERY() 
If you think this function should be supported, you can create an issue and start a discussion for it.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 09:20:39 UTC 2023,,,,,,,,,,"0|z1l7nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 09:20;xuyangzhong;Hi, [~dwysakowicz] . It seems related about CALCITE-4521. Since Calcite does not support this feature yet, maybe throwing a clearer exception when building LogicalTableFunctionScan is a good idea. WDYT? If we agree, I'll try to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make TestValues sinks return results as Rows,FLINK-33371,13555646,13415855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,26/Oct/23 10:52,27/Oct/23 14:05,04/Jun/24 20:40,27/Oct/23 14:05,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,Tests,,,,,0,pull-request-available,,,,,If we want to use the predicates from https://github.com/apache/flink/pull/23584 in restore tests we need to make testing sinks return Rows instead of Strings,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 14:05:07 UTC 2023,,,,,,,,,,"0|z1l7k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 14:05;dwysakowicz;Implemented in e914eb7fc3f31286ed7e33cc93e7ffbca785b731;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify validateAndParseHostsString in Elasticsearch connecter's configuration,FLINK-33370,13555620,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,26/Oct/23 08:42,24/Nov/23 02:27,04/Jun/24 20:40,24/Nov/23 02:27,,,,,,,,,,,elasticsearch-3.1.0,,,,Connectors / ElasticSearch,,,,,,0,pull-request-available,,,,,"Currently, the validateAndParseHostsString method exists in each configuration file(repeated for 3 times), but the method logic is exactly the same. We can simplify it  by introducing a common util.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 24 02:27:42 UTC 2023,,,,,,,,,,"0|z1l7eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Nov/23 02:27;Weijie Guo;main via b15b6f01d0eb46527cbdf2e41a16db381529f3aa.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount is failing on AZP with jdk17,FLINK-33369,13555610,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,chesnay,Sergey Nuyanzin,Sergey Nuyanzin,26/Oct/23 08:30,27/Oct/23 13:05,04/Jun/24 20:40,27/Oct/23 13:05,1.18.1,1.19.0,,,,,,,,,1.18.1,1.19.0,,,,,,,,,0,pull-request-available,,,,,"a number of builds start failing with that

since 25/10/2023 e.g.
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54014&view=logs&j=64debf87-ecdb-5aef-788d-8720d341b5cb&t=2302fb98-0839-5df2-3354-bbae636f81a7&l=5021]
is failing as 
{noformat}
Oct 25 03:36:24 job-cluster_1  | Starting Job Manager
Oct 25 03:36:24 job-cluster_1  | [ERROR] The execution result is empty.
Oct 25 03:36:24 job-cluster_1  | [ERROR] Could not get JVM parameters and dynamic configurations properly.
Oct 25 03:36:24 job-cluster_1  | [ERROR] Raw output from BashJavaUtils:
Oct 25 03:36:24 job-cluster_1  | Error: LinkageError occurred while loading main class org.apache.flink.runtime.util.bash.BashJavaUtils
Oct 25 03:36:24 job-cluster_1  | 	java.lang.UnsupportedClassVersionError: org/apache/flink/runtime/util/bash/BashJavaUtils has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0
Oct 25 03:36:24 Attaching to container-scripts_taskmanager_1
Oct 25 03:36:24 taskmanager_1  | Starting Task Manager
Oct 25 03:36:24 taskmanager_1  | [ERROR] The execution result is empty.
Oct 25 03:36:24 taskmanager_1  | [ERROR] Could not get JVM parameters and dynamic configurations properly.
Oct 25 03:36:24 taskmanager_1  | [ERROR] Raw output from BashJavaUtils:
Oct 25 03:36:24 taskmanager_1  | Error: LinkageError occurred while loading main class org.apache.flink.runtime.util.bash.BashJavaUtils
Oct 25 03:36:24 taskmanager_1  | 	java.lang.UnsupportedClassVersionError: org/apache/flink/runtime/util/bash/BashJavaUtils has been compiled by a more recent version of the Java Runtime (class file version 61.0), this version of the Java Runtime only recognizes class file versions up to 55.0
Oct 25 03:36:24 Checking for exceptions...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33342,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 13:05:01 UTC 2023,,,,,,,,,,"0|z1l7c8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/23 08:32;Sergey Nuyanzin;1.18 branch starts failing after this commit merged FLINK-33342
it could be that this is the reason/highlighter
looks like not everywhere during tests there jdk17;;;","27/Oct/23 08:19;chesnay;I suppose the e2e builds running on the Alibaba machines isn't running with JDK 17 because we never installed it on them.;;;","27/Oct/23 08:20;chesnay;{code}
Oct 25 03:16:33 Java and Maven version
openjdk version ""17.0.8.1"" 2023-08-24
OpenJDK Runtime Environment Temurin-17.0.8.1+1 (build 17.0.8.1+1)
OpenJDK 64-Bit Server VM Temurin-17.0.8.1+1 (build 17.0.8.1+1, mixed mode, sharing)
{code}
huh;;;","27/Oct/23 08:20;chesnay;ooooh its the _docker_ wordcount;;;","27/Oct/23 13:05;chesnay;master: f68ae194a6fda868c4d3ff3e3027cfc519c9017a
1.18: 371d9ab2f88e977b3ad0d80faacd466828b31272;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for SNI in the Flink Client,FLINK-33368,13555608,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mihai.l.lalescu,mihai.l.lalescu,26/Oct/23 08:05,30/Oct/23 08:26,04/Jun/24 20:40,30/Oct/23 08:26,,,,,,,,,,,,,,,Command Line Client,,,,,,0,,,,,,"We have Flink  clusters running on OpenShift behind a VIP that requires SNI (Server Name Information). The Flink client fails to connect to the Job Manager REST API through the VIP due to lack of SNI support in the Client. The connection was using TLS 1.2.
If required, I can provide Wireshark traces. The TLS 1.2 Client Hello package does not contain any SNI info.
I have also searched the Flink source code for the netty SniHandler class and I could not find any use of that class. 
I have not seen any SNI references here
https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/rest/RestClient.java","Flink Cluster on OpenShift
VIP requiring SNI 
Flink client running on a VM
Flink version 16.2
Java 8",,,,,,,,,,,,,,,,,,,,,FLINK-28520,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Thu Oct 26 17:28:23 UTC 2023,,,,,,,,,,"0|z1l7bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/23 10:28;martijnvisser;[~mihai.l.lalescu] Is this a duplicate of FLINK-28520? ;;;","26/Oct/23 17:28;mihai.l.lalescu;Yes this is a duplicate of the above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid Check in DefaultFileFilter,FLINK-33367,13555590,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,chiggi_dev,chiggi_dev,26/Oct/23 05:28,27/Oct/23 14:39,04/Jun/24 20:40,26/Oct/23 19:29,1.16.2,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,,,,,,"There is a null check in DefaultFileFilter:

 

if (fileName == null || fileName.length() == 0)

{   return true; }

 

So 2 questions here -

1) Can a file name ever be null?

2) What will be the behavior with return true? Should it be return false rather?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 14:38:25 UTC 2023,,,,,,,,,,"0|z1l780:",9223372036854775807,Not an actual issue.,,,,,,,,,,,,,,,,,,,"26/Oct/23 19:29;afedulov;I am closing this as there is no description of the actual issue.;;;","27/Oct/23 14:38;mapohl;Makes sense. But to answer the questions:
{quote}1) Can a file name ever be null?
{quote}
I assume not. But the JavaDoc is not precise on that one. So, it's fair to add a null check to be on the safe side, I guess.
{quote}2) What will be the behavior with return true? Should it be return false rather?
{quote}
{{DefaultFileFilter}} is meant to filter out hidden files (i.e. starting with '.' or '_'). Any other files should pass through. If you check the JavaDoc of the Java stream filter method, you will see that it filters elements that don't satisfy the passed Predicate. Considering this, returning {{true}} for files that don't have a name (because they cannot have a leading '.' or '_') seems to be reasonable.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"can not accept statement ""EXECUTE STATEMENT SET  BEGIN""",FLINK-33366,13555586,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,macdoor615,macdoor615,26/Oct/23 04:45,29/Oct/23 03:25,04/Jun/24 20:40,29/Oct/23 03:25,1.18.0,,,,,,,,,,,,,,Table SQL / JDBC,,,,,,0,,,,,,"put flink-sql-jdbc-driver-bundle-1.18.0.jar in $HIVE_HOME/lib

start beeline -u xxxx

create table
{code:java}
CREATE TABLE table_a (
  a int
) WITH (
  'connector' = 'print'
); {code}
output 
{code:java}
0: jdbc:flink://xxxx:8085> CREATE TABLE table_a (
. . . . . . . . . . . . . . . . . . . >   a int
. . . . . . . . . . . . . . . . . . . > ) WITH (
. . . . . . . . . . . . . . . . . . . >   'connector' = 'print'
. . . . . . . . . . . . . . . . . . . > );
No rows affected (1.119 seconds) {code}
execute statement ""EXECUTE STATEMENT SET  BEGIN"" and get failure output
{code:java}
0: jdbc:flink://xxx:8085> 
0: jdbc:flink://xxx:8085> EXECUTE STATEMENT SET
. . . . . . . . . . . . . . . . . . . >  BEGIN
. . . . . . . . . . . . . . . . . . . > insert into table_a values (1);
The SQL statement is incomplete.
0: jdbc:flink://hb3-dev-euler-001:8085> end;
{code}
execute statement ""BEGIN STATEMENT SET;"" and get success output
{code:java}
0: jdbc:flink://xxxx:8085> BEGIN STATEMENT SET;
No rows affected (0.118 seconds)
0: jdbc:flink://xxxx:8085> insert into table_a values (1);
No rows affected (0.114 seconds)
0: jdbc:flink://xxxxx:8085> end;
+-----------------------------------+
|              job id               |
+-----------------------------------+
| 1f34ba20cee6a35f2f8b69636ea55d29  |
+-----------------------------------+ {code}
 ","Flink 1.18.0 

Apache Hive beeline 3.2.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-26 04:45:25.0,,,,,,,,,,"0|z1l774:",9223372036854775807,"This is an issue of beeline 3.2.3, not Flink JDBC Driver's",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing filter condition in execution plan containing lookup join with mysql jdbc connector,FLINK-33365,13555585,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,davidradl,macdoor615,macdoor615,26/Oct/23 04:27,29/Feb/24 12:19,04/Jun/24 20:40,29/Feb/24 12:19,1.17.1,1.18.0,,,,,,,,,jdbc-3.1.2,jdbc-3.2.0,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,,"create table in flink with sql-client.sh
{code:java}
CREATE TABLE default_catalog.default_database.a (
  ip string, 
  proctime as proctime()
) 
WITH (
  'connector' = 'datagen'
);{code}
create table in mysql
{code:java}
create table b (
  ip varchar(20), 
  type int
);  {code}
 

Flink 1.17.1/ 1.18.0 and *flink-connector-jdbc-3.1.1-1.17.jar*

excute in sql-client.sh 
{code:java}
explain SELECT * FROM default_catalog.default_database.a left join bnpmp_mysql_test.gem_tmp.b FOR SYSTEM_TIME AS OF a.proctime on b.type = 0 and a.ip = b.ip; {code}
get the execution plan
{code:java}
...
== Optimized Execution Plan ==
Calc(select=[ip, PROCTIME_MATERIALIZE(proctime) AS proctime, ip0, type])
+- LookupJoin(table=[bnpmp_mysql_test.gem_tmp.b], joinType=[LeftOuterJoin], lookup=[ip=ip], select=[ip, proctime, ip, CAST(0 AS INTEGER) AS type, CAST(ip AS VARCHAR(2147483647)) AS ip0])
   +- Calc(select=[ip, PROCTIME() AS proctime])
      +- TableSourceScan(table=[[default_catalog, default_database, a]], fields=[ip]){code}
 
excute same sql in sql-client with Flink 1.17.1/ 1.18.0 and *flink-connector-jdbc-3.0.0-1.16.jar*

get the execution plan
{code:java}
...
== Optimized Execution Plan ==
Calc(select=[ip, PROCTIME_MATERIALIZE(proctime) AS proctime, ip0, type])
+- LookupJoin(table=[bnpmp_mysql_test.gem_tmp.b], joinType=[LeftOuterJoin], lookup=[type=0, ip=ip], where=[(type = 0)], select=[ip, proctime, ip, CAST(0 AS INTEGER) AS type, CAST(ip AS VARCHAR(2147483647)) AS ip0])
   +- Calc(select=[ip, PROCTIME() AS proctime])
      +- TableSourceScan(table=[[default_catalog, default_database, a]], fields=[ip]) {code}
with flink-connector-jdbc-3.1.1-1.17.jar,  the condition is 

*lookup=[ip=ip]*

with flink-connector-jdbc-3.0.0-1.16.jar ,  the condition is 

*lookup=[type=0, ip=ip], where=[(type = 0)]*

 

In out real world production environment, this lead incorrect data output

 

 ",Flink 1.17.1 & Flink 1.18.0 with flink-connector-jdbc-3.1.1-1.17.jar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-16024,,FLINK-34170,,,,,,,,"27/Oct/23 16:28;macdoor615;flink-connector-jdbc-3.0.0-1.16.png;https://issues.apache.org/jira/secure/attachment/13063947/flink-connector-jdbc-3.0.0-1.16.png","27/Oct/23 16:29;macdoor615;flink-connector-jdbc-3.1.1-1.17.png;https://issues.apache.org/jira/secure/attachment/13063948/flink-connector-jdbc-3.1.1-1.17.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 29 12:19:40 UTC 2024,,,,,,,,,,"0|z1l76w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 08:47;davidradl;Hi,

It would be really helpful if you could supply a test case (the tables and SQL an expected results) that shows this issue. 

 

I assume if we follow the example in [[https://nightlies.apache.org/flink/flink-table-store-docs-master/docs/development/lookup-join/](https://nightlies.apache.org/flink/flink-table-store-docs-master/docs/development/lookup-join/)] then you are using mySQL as the lookup source connector and you specify a WHERE clause that is being ignored. Am I understanding correctly?

 

 

 

 ;;;","27/Oct/23 08:55;davidradl;Or are the filters you are talking about part of the ON clause?;;;","27/Oct/23 09:16;davidradl;found the discussion in the mailing list with an example. We have recreated this situation with a getindata (lookup) connector. So I am thinking this is not just a mySQL issue.    ;;;","27/Oct/23 09:19;macdoor615;[~davidradl] yes. I found that filter “on b.type = 0” has been ignored. When I am using mySQL jdbc as the lookup source connector;;;","27/Oct/23 09:21;macdoor615;Maybe it is caused by jdbc connector.;;;","27/Oct/23 14:20;davidradl;[~macdoor615] I notice that I can run 
JdbcDynamicTableSourceITCase method , testLookupJoin in my Intellij workspace, and then choose mySQL as the implmentation.
As is this runs correctly.
 
I then change the select to something like your failing case:
 
_List<Row> collected =_
_executeQuery(_
_""SELECT S.id, S.name, D.id, D.timestamp6_col, D.decimal_col FROM value_source""_
_+ "" AS S JOIN jdbc_lookup for system_time as of S.proctime AS D ON S.id = D.id AND S.id = 2"");_
 
and rerun the test, it fails with error:
org.apache.flink.table.api.TableException: Temporal table join requires an equality condition on fields of table [default_catalog.default_database.jdbc_lookup].
 
There are some hits in google indicating that this is a restriction for temporal table joins. This Exception comes from a validate 
method in commonExecLookupJoin.java. 
 
Do you think this test is representative of your case? If not we can try other selects here, I have tried a few variants.   
 
I am going to see if this happens at the back level version of the JDBC connector.
 
 
 
 
 
 
 
 
 
 
 ;;;","27/Oct/23 15:28;macdoor615;[~davidradl] I'm not sure if your test case represents my situation. What I see is the exact same sql, using different versions of Flink JDBC connectors, getting different execution plans. Can you reproduce my situation?;;;","27/Oct/23 16:35;macdoor615;[~davidradl] I create another case

1. create a paimon table and insert 1 row of data
{code:java}
create table svc1_paimon_prod.dxx.a( ip string, proctime as proctime() );
insert into  svc1_paimon_prod.dxx.a values ('10.10.10.10'); {code}
2. create a mysql table and insert 2 rows of data
{code:java}
create table gem_tmp.b (ip varchar(20), type int);
insert into gem_tmp.b values ('10.10.10.10', 0);
insert into gem_tmp.b values ('10.10.10.10', 1); {code}
3. execute sql in sql-client.sh with flink-connector-jdbc-3.0.0-1.16.jar and. flink-connector-jdbc-3.1.1-1.17.jar 
{code:java}
SELECT * FROM svc1_paimon_prod.dxx.a left join bnpmp_mysql_test.gem_tmp.b FOR SYSTEM_TIME AS OF a.proctime on b.type = 0 and a.ip = b.ip; {code}
4. screenshot flink-connector-jdbc-3.0.0-1.16.jar, I get 1 row output ,this is correct

!flink-connector-jdbc-3.0.0-1.16.png|width=902,height=341!

5. screenshot flink-connector-jdbc-3.1.1-1.17.jar, I get 2 rows output, this is incorrect with wrong execution plan

!flink-connector-jdbc-3.1.1-1.17.png|width=970,height=228!;;;","31/Oct/23 08:56;davidradl;[~macdoor615] I am going to dig into this later this week.   ;;;","01/Nov/23 12:06;davidradl; 

[~macdoor615] I am running on a Mac and have Mariadb (I tweaked the dialect factory to allow mariadb in the jdbc url - see  https://issues.apache.org/jira/browse/FLINK-28433 ) and the latest Flink jdbc connector (master 3.2-SNAPSHOT that I built from source) and I cannot recreate the problem you are seeing (I see the filter as expected in the explain).

 

Can you try: 
 * the latest Flink JDBC connector 3.2-SNAPSHOT built from source
 * the latest mySQL JDBC driver.  I used the platform independent [https://dev.mysql.com/downloads/connector/j/]  . This seems to be recommended [https://blogs.oracle.com/mysql/post/mysql-connectorj-has-new-maven-coordinates]([https://blogs.oracle.com/mysql/post/mysql-connectorj-has-new-maven-coordinates).] I have raised issue https://issues.apache.org/jira/browse/FLINK-33384 to change to docs and code that reference the old deprecated JDBC driver. 

I hope these suggestions resolve this for you; if not I can look to setup a Linux image with mySQL; which JDBC driver do you see the issue on?;;;","01/Nov/23 14:31;davidradl;I did a further test- I rebuilt 3.1.1 (in case there was something fixing it in main) with the MariaDB change and it did not recreate the issue.  I will test with you paimon scenario ;;;","01/Nov/23 15:50;macdoor615;[~davidradl] I adopted released version in my environment, and I followed Flink docuement ([https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/jdbc/]) to get JDBC SQL Connector. 

Let's test with release version.

[https://repo.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.0.0-1.16/flink-connector-jdbc-3.0.0-1.16.jar|https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.0.0-1.16/flink-connector-jdbc-3.0.0-1.16.jar]

[https://repo.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.1-1.17/flink-connector-jdbc-3.1.1-1.17.jar|https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.1-1.17/flink-connector-jdbc-3.1.1-1.17.jar];;;","01/Nov/23 17:00;davidradl;Some good news in terms of recreations :). I have managed to recreate your paimon scenario using the latest drivers and MariaDB on the Mac. This issue is not a missing filter (as per the issue title), it seems to be incorrect execution. I can look into this but it does mismatch the issue title.   ;;;","01/Nov/23 17:36;davidradl;[~macdoor615] I switched out to use the 3.10 jar but got some compatibility issues (

*RROR] Could not execute SQL statement. Reason:*

*java.lang.ClassNotFoundException: org.apache.flink.shaded.guava30.com.google.common.collect.ImmutableMap*

). I assume you are testing that with Flink 3.16? Could you include the output of the explain when the select is working please?;;;","02/Nov/23 03:55;macdoor615;[~davidradl] I tested with flink 1.17.1 & 1.18.0. ;;;","02/Nov/23 11:56;macdoor615;[~davidradl] you need 

*flink-shaded-guava-30.1.1-jre-16.1.jar* ;;;","02/Nov/23 12:27;davidradl;[~macdoor615] thanks. ;;;","03/Nov/23 09:31;davidradl;I did not create using the back level jdbc driver. Though I am on a Mac, using MariaDB and Flink master; so that could be sufficiently different for it not to recreate for me . I am focussing on the issue I can recreate. ;;;","10/Nov/23 12:28;davidradl;an update on what I have found:

 

I have switched on DEBUG put out the rules that are being driven for my recreation. I see :

org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram [] - optimize time_indicator cost 1 ms.

optimize result:

FlinkLogicalSink(table=[*anonymous_collect$4*], fields=[ip, proctime, ip0, type])

+- FlinkLogicalCalc(select=[ip, PROCTIME_MATERIALIZE(proctime) AS proctime, ip0, type])

   +- FlinkLogicalJoin(condition=[=($0, $4)], joinType=[left])

      :- FlinkLogicalCalc(select=[ip, PROCTIME() AS proctime])

      :  +- FlinkLogicalTableSourceScan(table=[[paimon_catalog, default, a]], fields=[ip])

      +- FlinkLogicalSnapshot(period=[$cor0.proctime])

         +- FlinkLogicalCalc(select=[ip, CAST(0 AS INTEGER) AS type, CAST(ip AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS ip0])

            +- FlinkLogicalTableSourceScan(table=[[mariadb_catalog, menagerie, c, {*}filter=[=(type, 0)]]]{*}, fields=[ip, type])

 

Is removed in the next stage.  

org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram [] - optimize physical cost 3 ms.

optimize result:

Sink(table=[*anonymous_collect$4*], fields=[ip, proctime, ip0, type])

+- Calc(select=[ip, PROCTIME_MATERIALIZE(proctime) AS proctime, ip0, type])

   +- LookupJoin(table=[mariadb_catalog.menagerie.c], joinType=[LeftOuterJoin], lookup=[ip=ip], select=[ip, proctime, ip, *CAST(0 AS INTEGER)* AS type, CAST(ip AS VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"") AS ip0])

      +- Calc(select=[ip, PROCTIME() AS proctime])

         +- TableSourceScan(table=[[paimon_catalog, default, a]], fields=[ip])

 

The *CAST(0 AS INTEGER)* is in the final Optimized Execution Plan we see in the explain This cast is fine as long as the filter is there.

 

I am not an expert at this, but the comments at the top of [CommonPhysicalLookupJoin.scala]([https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/common/CommonPhysicalLookupJoin.scala)] are correct and this filter should actually be in the lookup keys. The comments says

 
_* For a lookup join query:_
_*_
_* <pre> SELECT T.id, T.content, D.age FROM T JOIN userTable FOR SYSTEM_TIME AS OF T.proctime AS D_
_* ON T.content = concat(D.name, '!') AND D.age = 11 AND T.id = D.id WHERE D.name LIKE 'Jack%'_
_* </pre>_
_*_
_* The LookupJoin physical node encapsulates the following RelNode tree:_
_*_
_* <pre> Join (l.name = r.name) / \ RelNode Calc (concat(name, ""!"") as name, name LIKE 'Jack%') |_
_* DimTable (lookup-keys: age=11, id=l.id) (age, id, name) </pre>_
_*_
_* The important member fields in LookupJoin: <ul> <li>allLookupKeys: [$0=11, $1=l.id] ($0 and $1 is_
_* the indexes of age and id in dim table)</li> <li>remainingCondition: l.name=r.name</li> <ul>_
_*_
_* The workflow of lookup join:_
_*_
_* 1) lookup records dimension table using the lookup-keys <br> 2) project & filter on the lookup-ed_
_* records <br> 3) join left input record and lookup-ed records <br> 4) only outputs the rows which_
_* match to the remainingCondition <br>_
 

I would have thought that a filter on a value on one of the sources could be pushed down a a filter to that source, rather than being in the lookup keys. Maybe that could be a subsequent optimization. We see this in: 

FlinkLogicalTableSourceScan(table=[[mariadb_catalog, menagerie, c, filter=[=(type, 0)]]], fields=[ip, type])

but this is then lost. 

It seems the table scan with the pushed down filter is being ignored during the the construction of the LookupJoin representation 

 

 

 ;;;","10/Nov/23 18:08;davidradl;I am not sure about the design intent in this area. Should the pushed down predicate be in the lookup keys as the java doc for ([https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/common/CommonPhysicalLookupJoin.scala)]

Indicates. Or should it just be honoured as a filter. If it is honoured a filter,  then does the dialect need to do anything special for the pushdown predicates, like it does for the scan case.

 

I am thinking the lookup processing should honour the pushed down predicates and they should not be in the lookup keys (unless the source does not support predicate push down). There is no current way I can see for the dialect to add in these predicates, which has been optimized in the Calcite rules processing.

Maybe a change to the lookup processing (probably in [CommonPhysicalLookupJoin|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/common/CommonPhysicalLookupJoin.scala)])  to honour the table source resolvedPredicates and pushed down params, is required.

I am new to this area and would appreciate the communities thoughts on whether I am on the right track.

WDYT?;;;","12/Nov/23 21:35;Sergey Nuyanzin;looks like the issue is incomplete (at least for the case with
{noformat}
for system_time as of {noformat}
) implementation of filter push down FLINK-16024

TC to reproduce
add this to {{org.apache.flink.connector.jdbc.table.JdbcTablePlanTest}}
{code:java}
    @Test
    public void issue33365() {
        String dataId =
                TestValuesTableFactory.registerData(
                        Arrays.asList(
                                Row.of(1L, ""Alice""),
                                Row.of(1L, ""Alice""),
                                Row.of(2L, ""Bob""),
                                Row.of(3L, ""Charlie"")));
        util.tableEnv().executeSql(
                String.format(
                        ""CREATE TABLE value_source (\n""
                                + ""`id` BIGINT,\n""
                                + ""`name` STRING,\n""
                                + ""`proctime` AS PROCTIME()\n""
                                + "") WITH (\n""
                                + ""'connector' = 'values', \n""
                                + ""'data-id' = '%s')"",
                        dataId));
        util.verifyExecPlan(
                ""SELECT S.id, S.name, D.id, D.timestamp6_col, D.double_col FROM value_source""
                        + "" AS S JOIN jdbc for system_time as of S.proctime AS D ON S.id = D.id and D.decimal_col = 0.0"");
    }
{code}
and check before and after commit [https://github.com/apache/flink/pull/20140] or same commit in jdbc connector (https://github.com/apache/flink-connector-jdbc/commit/3e3b40e8cfadfc16a8ab74d4ef6a3ab3ceafa57b) it shows different results.

At the same time there is a bunch of WA like add something meaningless or cast or some other function use, 
e.g. slightly changed query started behave as expected since math operations, cast and other functions are not going to push down (yet)
however in Flink 1.18 math operations could be simplified by newer Calcite (CALCITE-4420), so need to check case by case
like
{code:sql}
SELECT S.id, S.name, D.id, D.timestamp6_col, D.double_col 
  FROM value_source AS S 
  JOIN jdbc for system_time as of S.proctime AS D 
    ON S.id = D.id and D.decimal_col = 0.0 + 0
{code}
or with casting like
{code:sql}
SELECT S.id, S.name, D.id, D.timestamp6_col, D.double_col 
  FROM value_source AS S 
  JOIN jdbc for system_time as of S.proctime AS D 
    ON S.id = D.id and D.decimal_col = cast(0.0 as decimal)
{code}
[~macdoor615] that might help you with your queries

[~qingwei91] could you have a look here please since you are aware of current implementation;;;","13/Nov/23 02:47;libenchao;[~Sergey Nuyanzin] Thanks for the digging. It's indeed a problem from the current JDBC filter pushdown implementation, one way to mitigate this problem might be also considering the predicates in {{JdbcRowDataLookupFunction}}.;;;","13/Nov/23 12:04;qingwei91;[~Sergey Nuyanzin] , I will look into this;;;","13/Nov/23 16:27;davidradl;[~Sergey Nuyanzin] [~qingwei91] thanks for your input. The 2 circumventions do not work for me on master, on the reported case. 

It sounds like the introduction of pushdown predicate capability for jdbc, broke this case.  At the moment the JdbcRowDataLookupFunction just adds the keyNames to the conditions 
 
_this.query =_
_options.getDialect()_
_.getSelectFromStatement(options.getTableName(), fieldNames, keyNames);_
 
 I was hoping to contribute this as a fix, as I have been assigned the issue;  [~qingwei91] is this still possible- or do you plan to fix this? I see that function getLookupRuntimeProvider does have access to this.resolvedPredicates and pushdownParams it would be great if we could use these to create the condition to add to the keyNames. 
I would be great to hear your thoughts on this. 
 

 
 
 
 

 ;;;","13/Nov/23 21:36;qingwei91;Hi [~davidradl] , 

if you can work on this that's better, I agree that we need to change the LookupFunction to incorporate the information about pushdown.

 ;;;","14/Nov/23 09:21;davidradl;[~qingwei91] thank you very much for you support :) . One question I have about the design intent.

1) Would it be true to say that the Calcite rule's should dictate the general shape of the query, I can see that there are rules around predicate pushdown and lookups. Then the JDBC dialect would be minimal type mapping of the graph. 

2) The solution we are considering, because the predicate pushdown logic was done in the JDBC code, we are fixing up the query in the JDBC code to not lose the conditions.

 

Would it be true to say the 2nd approach is more tactical and short term as this is a blocker and the 1st approach would be more ideal? Or am I misunderstanding?

 

Either way I will look into option 2), as a change of the rules is much more impactful outside of JDBC. 

 

 

       

  

 

   ;;;","14/Nov/23 09:59;qingwei91;I am not familiar with Calcite, not able to comment on how it should interact with the pushdown predicate. I do agree 2nd approach is probably a smaller change and possibly safer.

I originally implemented FilterPushdown in JDBC because that's where the query is being generated and it is performing LimitPushdown, now with this bug I am not sure if that's the right way.;;;","14/Nov/23 11:18;libenchao;[~davidradl] ""lookup table source"" is a special form of ""table source"", so in theory it should/could handle all the pushed down predicate/limit/partitions, this is also how we abstracted it in Flink now. Hence to me, fixing it in JDBC Connector is a better choice for now.

Regarding the #2, I don't see a real need that one connector that supports pushdown for ""scan table source"", but not for ""lookup table source"" yet, if there is a real need for this in the future, we can revisit and add fine-grained pushdown abilities for ""scan table source"" and ""lookup table source"". WDYT?

Feel free to ping me if your PR is ready, I would be glad to help review and merge it:);;;","14/Nov/23 16:30;davidradl;[~libenchao]  and [~qingwei91] I have had a play and have some code that is working. I have put it in a draft pr [https://github.com/apache/flink-connector-jdbc/pull/79]

 

Could you feedback on it please?

 

My thinking behind the change:

The issue here is that the filters are not honoured in the loop up join when there are push down predicates.

The lookup processing has access to the lookup keys as well as the predicate pushdowns, the predicate push down information is parameterized. 

The existing getSelectStatement takes as it last parameter conditions, it takes the lookupkeys and creates an equality conditions in AbstractDialect like this 

 
_String fieldExpressions =_
_Arrays.stream(conditionFields)_
_.map(f -> format(""%s = :%s"", quoteIdentifier(f), f))_
_.collect(Collectors.joining("" AND ""));_
 

This is not appropriate for a condition like a.type = 0 containing a literal and potentially not being an equals condition.

The predicate pushdown is currently only is implemented  for simple expressions. 

The idea of the change is to resolve the conditions completely in the lookup code and pass an array of Strings to a new overloaded implementation of getSelectStatement, so we can add the appropriate conditions as a new parameter. Note that the conditions for non-predicate pushdown would end up as filters in the existing select conditions.  

 

I do not think the pr is ready yet, but I would love to hear your feedback. I intend to: 
 * to do more testing 
 ** multiple conditions including non equalities
 ** multiple lookup keys
 * ideally add test cases along the lines that have been suggested by [~Sergey Nuyanzin] 
 * look into whether I can use existing Flink classes to resolve the condition insert (I have used a replace); I see there is a 
ResolvedExpression class I might be able to use.
 * Any ideas on other tests I could try to break this?
 * We could do the resolution of the String inserts in the getSelectStatement - but this would involved passing both the predicate and the parameter in rather than resolving in the caller.  I think resolving it in the caller makes for a simpler call.   
 * Currently there is no specific logic to account for partitions (which the scan logic does), we may need to do something for partitions here. ;;;","15/Nov/23 04:57;libenchao;[~davidradl] I take a quick glance at the PR, it looks generally good, two cents from my point of view:
* We'd better to use `PreparedStatement.setXXX` to pass the literals into it, this will make us free from different SQL dialects behaviors about how to serialize literals into SQL
* It may be better to append the extra conditions in `JdbcRowDataLookupFunction` itself, instead of touching `JdbcDialect` which is a public API;;;","15/Nov/23 13:40;davidradl;[~libenchao] Thanks for the quick feedback . I agree, I was thinking the second point you make will make it more in line with the scan logic, where it appends to the select statement with the where clauses. I was not aware of the _PreparedStatement_ thanks for pointing me to it :)  ;;;","15/Nov/23 16:31;davidradl;[~libenchao] 

I have moved the code from the AbstractDialog and pushed up the change. I could not see how to get a  _PreparedStatement_ to be able to set the condition on{_}.{_} Please could you give me some pointers.

 

I have successfully tested using the supplied test tables:
 * other simple predicates work
 * multiple simple predicates work

 

I added extra tests to JdbcDynamicTableSourceITCase but cannot test those changes as I get errors locally when running the tests without my changes. Any pointers would be great I am running on a Mac.  

 

I wanted to test 2 look up keys, so I created a new table d (on MariaDB) and e (on paimon)  and set up the following tables and did the some joins with filters and a join with multiple keys. The results do not look right to me (but I may be misunderstanding) - WDYT?

select * from mariadb_catalog.menagerie.d ;

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}------------+
|op|                            ip|        type|        age|

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}------------+
|+I|                    10.10.10.10|          1|          30|
|+I|                    10.10.10.10|          2|          40|
|+I|                    10.10.10.10|          2|          50|
|+I|                    10.10.10.10|          3|          50|

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}------------+

Received a total of 4 rows

 

Flink SQL> select * from e;

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}------------------------+
|op|                            ip|        age|                proctime|

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}------------------------+
|+I|                    10.10.10.10|          40|2023-11-15 16:12:57.553|
|+I|                    10.10.10.10|          50|2023-11-15 16:12:57.554|

 

Flink SQL> SELECT * FROM e left join mariadb_catalog.menagerie.d FOR SYSTEM_TIME AS OF e.proctime on d.type = 2 and d.age = 50 and e.ip = d.ip;

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}-----------------------{-}++{-}------------------------------{-}{-}-----------{-}++{-}------------
|op|                            ip|        age|                proctime|                            ip0|        type|        age0|

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}-----------------------{-}++{-}------------------------------{-}{-}-----------{-}++{-}------------
|+I|                    10.10.10.10|          40|2023-11-15 16:08:40.973|                    10.10.10.10|          2|          50|
|+I|                    10.10.10.10|          50|2023-11-15 16:08:40.974|                    10.10.10.10|          2|          50|

 

Is this what you would expect? 

 

Also I get

SELECT * FROM e left join mariadb_catalog.menagerie.d FOR SYSTEM_TIME AS OF e.proctime on  e.age = d.age;

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}-----------------------{-}++{-}------------------------------{-}{-}-----------{-}++{-}------------
|op|                            ip|        age|                proctime|                            ip0|        type|        age0|

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}-----------------------{-}++{-}------------------------------{-}{-}-----------{-}++{-}------------

^CQuery terminated, received a total of 0 row

and 

SELECT * FROM e left join mariadb_catalog.menagerie.d FOR SYSTEM_TIME AS OF e.proctime on e.ip = d.ip;

+----+--------------------------------+-------------+-------------------------+--------------------------------+-------------+-------------+

| op |                             ip |         age |                proctime |                            ip0 |        type |        age0 |

+----+--------------------------------+-------------+-------------------------+--------------------------------+-------------+-------------+

^CQuery terminated, received a total of 0 row

and

SELECT * FROM e left join mariadb_catalog.menagerie.d FOR SYSTEM_TIME AS OF e.proctime on  e.age = d.age and d.ip = e.ip;

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}-----------------------{-}++{-}------------------------------{-}{-}-----------{-}++{-}------------
|op|                            ip|        age|                proctime|                            ip0|        type|        age0|

{+}---{-}{-}{+}------------------------------{-}++{-}-----------{-}{-}-----------------------{-}++{-}------------------------------{-}{-}-----------{-}++{-}------------

^CQuery terminated, received a total of 0 row

 

 

 

 

 

 

 

 

 

 

 

 

 ;;;","17/Nov/23 10:16;davidradl;[~libenchao] 

I fixed a bug in the fix that was stopping multiple lookup keys being processed properly.

 ;;;","21/Nov/23 04:55;libenchao;[~davidradl] Is the PR ready to review now? Since we've reached an agreement on the general idea, let's move to the PR for further discussions.;;;","18/Dec/23 09:45;davidradl;[~libenchao] [~Sergey Nuyanzin] Bringing back a discussion onto the issue. I have many of the scenarios working including the Or scenario. There are scenarios

like the one [https://lists.apache.org/thread/7r49c5ffohz1oqm3oz2g60g5lnovq0l2] if the jdbc lookup source has rows that do not match the filter, then nulls come out in the lookup join resultset. The same behaviour occurs with predicate pushdown disabled and with my current patch (when I push it up).

On the discussion thread [~ruanhang1993]  agreed that the nulls are not correct.

 

The way I am reading this is that, unless we have a reason to see these nulls, there is an issue in lookup joins in certain scenarios. It does not appear to relate to JDBC. Are we ok if I make this fix work the same as when predicate pushdown is not enabled. And raise a separate issue for the lookup join which appears not to be JDBC related.  

 

 

 ;;;","18/Dec/23 10:25;Sergey Nuyanzin;Since 
1. predicate push down from one side brings so many issues and probably there is something else which is not known yet.
2. IIRC this issue was a blocker before jdbc connector release

I tend to think 
1. to revert/disable push down commit ((https://github.com/apache/flink-connector-jdbc/commit/3e3b40e8cfadfc16a8ab74d4ef6a3ab3ceafa57b)
2. make a release
3. continue working on pushdown

what do you think?

;;;","18/Dec/23 11:01;libenchao;[~davidradl] I didn't see a problem in the example you gave in this thread: https://lists.apache.org/thread/7r49c5ffohz1oqm3oz2g60g5lnovq0l2. 

The result is correct since you are using 'LEFT JOIN', for the records from left table which does not have matching rows, it should emit the line, and pad nulls for fields in right table. Am I missing something?

The behavior should be the same when pushdown is enabled or not as you stated previously.;;;","18/Dec/23 12:05;davidradl;[~libenchao]  thanks for the quick feedback - I guess it is where the filter is applied - I was expecting the filter to be applied to the table then we join. You are saying we join and show non null values where the filter matches.  I have done some testing on sqiflddle, it looks like you are right. If I wanted the bevaviour, I was expecting I should use: 

SELECT * FROM a left join mariadb_catalog.menagerie.e FOR SYSTEM_TIME AS OF a.proctime on a.ip = e.ip _where_  e.age = 50 ;

 

 

 

 ;;;","18/Dec/23 12:19;libenchao;Yes, {{SELECT * FROM a left join mariadb_catalog.menagerie.e FOR SYSTEM_TIME AS OF a.proctime on a.ip = e.ip where e.age = 50}} should behaves as you expected. The semantic is different for filter in ON CLAUSE and WHERE CLAUSE when you are using LEFT JOIN. (For inner join, it will be the same);;;","18/Dec/23 12:20;davidradl;[~Sergey Nuyanzin] from the discussion above. I am thinking the <nulls>s issue was not really an issue, this was my misunderstanding. The options I see are: 

1) Do as you say, but wouldn't this be regression for the jdbc driver without filters, as there is likely to be a performance impact without filter pushdown

2) We could complete the fix I have. I have it working locally with the OR case. I am looking at implementing more test cases.I am struggling to make the unit test cases, to be defined in data with a for loop to execute them.  If you are ok with the style of unit tests I have, I could have this up today or tomorrow. 

3) If we do not view this as a blocker - we could release without reverting the commit and continue investigating. 

 

My preference would be option 2 - we are looking to have this fix in and would probably need to patch out code to pick it up.  But if this takes too long to get merged, option 3 would be an option.  

 

Strangely the config option to disable predicate pushdowns works for me. I was not expecting it to work as it should only effect the legacy style of predicate pushdown.  

 

WDYT?

 ;;;","19/Dec/23 11:37;Sergey Nuyanzin;glad that the issue is solved and thanks for working on it

{quote}
3) If we do not view this as a blocker - we could release without reverting the commit and continue investigating. 
{quote}
it was already mentioned in ML that it is a blocker [1] 
We could try to continue with option 2 and let's see whether we face something else or not


[1] https://lists.apache.org/thread/sdkm5qshqozow9sljz6c0qjft6kg9cwc;;;","19/Dec/23 11:51;davidradl;[~Sergey Nuyanzin] sounds great! Thank you for your support. I realise that I need to add tests that exercise the 2 types of filters, the one on the _join_ which can get nulls and the one in the _where_ that does not. I will push up new code for you to review, when I have completed the tests.   ;;;","19/Dec/23 19:34;davidradl;[~Sergey Nuyanzin] I have pushed up my latest changes including the fix for the OR case. The tests are more simplified but they are not data driven - with one method looking of test case data. I did not find a way with the existing data to drive the null case. I have tested that and the where locally but there is no automated test for it. Please review and see what more needs to be changed? I appreciate your time on this.  I added debug - as I think this will speed future diagnostics in this area. Scan logic has a similar debug. ;;;","05/Jan/24 14:35;Sergey Nuyanzin;marked as a blocker as mentioned at https://lists.apache.org/thread/pc15twvf90l0d6t4vthomh6jzv1h145d;;;","09/Jan/24 09:31;rahulchundeth;Hi [~davidradl] , Do we have any ETA for this bug fix ?;;;","09/Jan/24 09:40;Sergey Nuyanzin;[~rahulchundeth] there is an existing PR and you can help/speed it up by helping reviewing this;;;","22/Jan/24 12:50;martijnvisser;What's the status of this ticket? Are we closing in on fixing this, or will we revert the change? Flink 1.18 was announced end of October, and we've still been unable to make a JDBC connector release that's compatible with it because of this blocker. In hindsight, I think we've made a mistake with not flagging this as a blocker earlier, and we should have just reverted this change. ;;;","23/Jan/24 02:28;libenchao;[~martijnvisser]  The PR is very close to merge now, I'll try to finalize it asap, and hopefully we can resolve this blocker this week and unblock the release process.;;;","26/Jan/24 12:53;Sergey Nuyanzin;Merged to main as [5a90eb0a73ca0ac8475331a74ae8f7c1c01646bb|https://github.com/apache/flink-connector-jdbc/commit/5a90eb0a73ca0ac8475331a74ae8f7c1c01646bb];;;","26/Jan/24 14:10;Sergey Nuyanzin;Merged to 3.1.x as [6f318a5326b2d7aba554638b186bf4cc4e17511c|https://github.com/apache/flink-connector-jdbc/commit/6f318a5326b2d7aba554638b186bf4cc4e17511c];;;","09/Feb/24 07:26;rahulchundeth;Hi Team,
when we can expect next version of flink-connector-jdbc with this fix?;;;","09/Feb/24 07:39;martijnvisser;[~rahulchundeth] See the Dev mailing list for an ongoing voting thread; please help validating the release as well. https://lists.apache.org/thread/rlk5kp2vxgkmbxmq4wnco885q5vv9rtp;;;","29/Feb/24 08:01;macdoor615;I upgraded to Flink 1.18.1 and flink-connector-jdbc-3.1.2-1.18.jar. Re-run the test sql. I found it still missing filter condition 
{code:java}
create table in flink with sql-client.sh
CREATE TABLE default_catalog.default_database.a (
  ip string, 
  proctime as proctime()
) 
WITH (
  'connector' = 'datagen'
);
create table in mysql
create table b (
  ip varchar(20), 
  type int
);  {code}
excute in sql-client.sh 
{code:java}
explain SELECT * FROM default_catalog.default_database.a left join bnpmp_mysql_test.gem_tmp.b FOR SYSTEM_TIME AS OF a.proctime on b.type = 0 and a.ip = b.ip; {code}
get the execution plan
{code:java}
...
== Optimized Execution Plan ==
Calc(select=[ip, PROCTIME_MATERIALIZE(proctime) AS proctime, ip0, type])
+- LookupJoin(table=[bnpmp_mysql_test.gem_tmp.b], joinType=[LeftOuterJoin], lookup=[ip=ip], select=[ip, proctime, ip, CAST(0 AS INTEGER) AS type, CAST(ip AS VARCHAR(2147483647)) AS ip0])
   +- Calc(select=[ip, PROCTIME() AS proctime])
      +- TableSourceScan(table=[[default_catalog, default_database, a]], fields=[ip]){code}
      
still missing ""{*}type=0{*}"" filter;;;","29/Feb/24 08:23;Sergey Nuyanzin;[~macdoor615] iIRC it was discussed during PR and this is the issue for {{TableSourceScan}} how it is serialized to string and should be handled within another jira issue, however at the same time the result should be correct.
Could you please check it?;;;","29/Feb/24 11:29;libenchao;Yes, the related issue is FLINK-34170;;;","29/Feb/24 12:19;macdoor615;[~Sergey Nuyanzin] You are right. I tested the sql using data and got the correct results. There is no corresponding filter fetch in the execution plan;;;"
Introduce standard YAML for flink configuration,FLINK-33364,13555575,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,26/Oct/23 03:14,29/Jan/24 06:19,04/Jun/24 20:40,30/Nov/23 05:19,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34257,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 30 05:19:18 UTC 2023,,,,,,,,,,"0|z1l74o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/23 05:19;zhuzh;Done via d5b1afb7f67fe7da166193deb7711b8a8b163bcf;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker bases images can't run the java compiler,FLINK-33363,13555558,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Not A Problem,,henning,henning,25/Oct/23 23:28,13/Nov/23 09:00,04/Jun/24 20:40,13/Nov/23 09:00,1.17.1,,,,,,,,,,,,,,flink-docker,Table SQL / Runtime,,,,,0,,,,,,"I have set up a small cluster (Job Manager + 2 Task managers) using docker compose. When submitting a flink job that needs Calcite planning, it crashes with

{code}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unable to instantiate java compiler
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:105) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:84) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:70) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:108) ~[flink-dist-1.17.1.jar:1.17.1]
        at java.util.concurrent.CompletableFuture$AsyncSupply.run(Unknown Source) [?:?]
        at java.lang.Thread.run(Unknown Source) [?:?]
Caused by: java.lang.IllegalStateException: Unable to instantiate java compiler
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:163) ~[?:?]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.generateCompileAndInstantiate(JaninoRelMetadataProvider.java:141) ~[?:?]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.lambda$static$0(JaninoRelMetadataProvider.java:73) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.CacheLoader$FunctionToCacheLoader.load(CacheLoader.java:165) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.get(LocalCache.java:3951) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958) ~[?:?]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.revise(JaninoRelMetadataProvider.java:197) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQueryBase.revise(RelMetadataQueryBase.java:118) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:844) ~[?:?]
        at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:307) ~[?:?]
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:337) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:565) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:428) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:251) ~[?:?]
        at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:130) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:208) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:195) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:64) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:78) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59) ~[?:?]
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87) ~[?:?]
        at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:329) ~[?:?]
        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195) ~[?:?]
        at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:253) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:226) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at com.appfolio.data.kafka.consumer.PropertyListings.getDataStream(PropertyListings.java:198) ~[?:?]
        at com.appfolio.data.kafka.consumer.KafkaIcebergConsumer.run(KafkaIcebergConsumer.java:80) ~[?:?]
        at com.appfolio.data.kafka.consumer.KafkaIcebergConsumer.main(KafkaIcebergConsumer.java:32) ~[?:?]
        at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
        at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
        at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
        at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.17.1.jar:1.17.1]
        ... 7 more
Caused by: java.lang.ClassCastException: class org.codehaus.janino.CompilerFactory cannot be cast to class org.codehaus.commons.compiler.ICompilerFactory (org.codehaus.janino.CompilerFactory is in unnamed module of loader org.apache.flink.util.ChildFirstClassLoader @252ce819; org.codehaus.commons.compiler.ICompilerFactory is in unnamed module of loader 'app')
        at org.codehaus.commons.compiler.CompilerFactoryFactory.getCompilerFactory(CompilerFactoryFactory.java:129) ~[flink-table-runtime-1.17.1.jar:1.17.1]
        at org.codehaus.commons.compiler.CompilerFactoryFactory.getDefaultCompilerFactory(CompilerFactoryFactory.java:79) ~[flink-table-runtime-1.17.1.jar:1.17.1]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.compile(JaninoRelMetadataProvider.java:161) ~[?:?]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.generateCompileAndInstantiate(JaninoRelMetadataProvider.java:141) ~[?:?]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.lambda$static$0(JaninoRelMetadataProvider.java:73) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.CacheLoader$FunctionToCacheLoader.load(CacheLoader.java:165) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.get(LocalCache.java:3951) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3974) ~[?:?]
        at org.apache.flink.calcite.shaded.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4958) ~[?:?]
        at org.apache.calcite.rel.metadata.JaninoRelMetadataProvider.revise(JaninoRelMetadataProvider.java:197) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQueryBase.revise(RelMetadataQueryBase.java:118) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getPulledUpPredicates(RelMetadataQuery.java:844) ~[?:?]
        at org.apache.calcite.rel.rules.ReduceExpressionsRule$ProjectReduceExpressionsRule.onMatch(ReduceExpressionsRule.java:307) ~[?:?]
        at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:337) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:565) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:428) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:251) ~[?:?]
        at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:130) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:208) ~[?:?]
        at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:195) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:64) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:78) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59) ~[?:?]
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:156) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:156) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:156) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:154) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104) ~[flink-scala_2.12-1.17.1.jar:1.17.1]
        at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) ~[?:?]
        at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87) ~[?:?]
        at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:329) ~[?:?]
        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195) ~[?:?]
        at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:253) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:226) ~[flink-table-api-java-uber-1.17.1.jar:1.17.1]
        at com.appfolio.data.kafka.consumer.PropertyListings.getDataStream(PropertyListings.java:198) ~[?:?]
        at com.appfolio.data.kafka.consumer.KafkaIcebergConsumer.run(KafkaIcebergConsumer.java:80) ~[?:?]
        at com.appfolio.data.kafka.consumer.KafkaIcebergConsumer.main(KafkaIcebergConsumer.java:32) ~[?:?]
        at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]
        at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
        at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:?]
        at java.lang.reflect.Method.invoke(Unknown Source) ~[?:?]
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355) ~[flink-dist-1.17.1.jar:1.17.1]
        ... 7 more
2023-10-25 23:23:02,347 ERROR org.apache.flink.runtime.webmonitor.handlers.JarRunHandler   [] - Exception occurred in REST handler: Could not execute application.
{code}

As this image builds off eclipse-temurin:11-jre-jammy, this may be the problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 13 09:00:22 UTC 2023,,,,,,,,,,"0|z1l70w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 07:18;mapohl;[~snuyanzin] may you have a look at this? It looks like an issue related to Janino;;;","27/Oct/23 08:34;Sergey Nuyanzin;[~henning] what janino version is in your classpath?;;;","02/Nov/23 00:55;henning;I started a cluster with the official Flink docker images. So Flink is running within the docker image that you are publishing. I don't know what version of janino you are packaging in the docker image, but it is that one. :-) 

The error message reads ""java.lang.ClassCastException: class org.codehaus.janino.CompilerFactory cannot be cast to class org.codehaus.commons.compiler.ICompilerFactory (org.codehaus.janino.CompilerFactory is in unnamed module of loader org.apache.flink.util.ChildFirstClassLoader @252ce819; org.codehaus.commons.compiler.ICompilerFactory is in unnamed module of loader 'app')"".

This is on x86_64 (running in docker on an EC2 instance). 

;;;","02/Nov/23 09:32;Sergey Nuyanzin;[~henning] it seems same issue was recently discussed in slack [1]

{quote}
After fighting with this for too many days, I eventually added the following configuration in as a last ditch effort and it appeared to work after stumbling upon it via the docs and archived mailing list:
{noformat}
classloader.resolve-order: parent-first
{noformat}
{quote}

could you please check that it helps for your case?
Alos based on feedback on slack it seems the issue is only for jdk11 while for jdk 8 it should work

[1] https://apache-flink.slack.com/archives/C03G7LJTS2G/p1698762945955639?thread_ts=1698327026.373369&cid=C03G7LJTS2G;;;","02/Nov/23 22:54;henning;Will try. I will still keep using the official flink images (""docker pull flink:latest"")  so this may need resolution on your side. ;;;","02/Nov/23 22:59;henning;Can't do anything with the slack link, it seems that the workspace does not allow public signup and I don't have an account (and my apache.org mail address does not work).;;;","02/Nov/23 23:21;henning;So, the comment actually brought me on the right track (and here is how to fix this well without resorting to changing the configuration):

I had the ""org.apache.flink:flink-table-runtime"" as a dependency for my job. And when I packaged it up for deployment, the classes from that jar ended up in my job jar. This caused the classloader to choke.

removing this dependency (or dropping it to provided scope) fixed the problem for me.
;;;","13/Nov/23 09:00;mapohl;Thanks for getting back to us on that matter and describing the fix, [~henning]. That's much appreciated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Externalized Declarative Resource Management With Chinese,FLINK-33362,13555506,13527017,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ConradJam,ConradJam,25/Oct/23 14:03,19/Dec/23 23:03,04/Jun/24 20:40,,1.18.0,1.18.1,,,,,,,,,1.18.2,,,,Documentation,,,,,,0,pull-request-available,,,,,Document Externalized Declarative Resource Management With Chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-25 14:03:53.0,,,,,,,,,,"0|z1l6pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Java 17 compatibility to Flink Kafka connector,FLINK-33361,13555504,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,martijnvisser,martijnvisser,25/Oct/23 14:00,11/Dec/23 22:10,04/Jun/24 20:40,11/Dec/23 22:09,kafka-3.0.1,kafka-3.1.0,,,,,,,,,kafka-3.1.0,,,,Connectors / Kafka,,,,,,1,pull-request-available,,,,,"When currently trying to {{mvn clean install -Dflink.version=1.18.0 -Dscala-2.12 -Prun-end-to-end-tests -DdistDir=/Users/mvisser/Developer/flink-1.18.0 -Dflink.convergence.phase=install -Dlog4j.configurationFile=tools/ci/log4j.properties}} this fails with errors like:

{code:java}
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR] FlinkKafkaConsumerBaseMigrationTest.testRestore
[ERROR]   Run 1: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 2: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 3: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 4: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 5: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 6: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 7: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 8: Exception while creating StreamOperatorStateContext.
[ERROR]   Run 9: Exception while creating StreamOperatorStateContext.
[INFO] 
[ERROR]   FlinkKafkaConsumerBaseTest.testExplicitStateSerializerCompatibility:721 » Runtime
[ERROR]   FlinkKafkaConsumerBaseTest.testScaleDown:742->testRescaling:817 » Checkpoint C...
[ERROR]   FlinkKafkaConsumerBaseTest.testScaleUp:737->testRescaling:817 » Checkpoint Cou...
[ERROR]   UpsertKafkaDynamicTableFactoryTest.testBufferedTableSink:243 » UncheckedIO jav...
{code}

Example stacktrace:

{code:java}
Test testBufferedTableSink(org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest) failed with:
java.io.UncheckedIOException: java.io.IOException: Serializing the source elements failed: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.setOutputType(FromElementsFunction.java:162)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySetOutputType(StreamingFunctionUtils.java:84)
	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.setOutputType(StreamingFunctionUtils.java:60)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.setOutputType(AbstractUdfStreamOperator.java:146)
	at org.apache.flink.streaming.api.operators.SimpleOperatorFactory.setOutputType(SimpleOperatorFactory.java:118)
	at org.apache.flink.streaming.api.graph.StreamGraph.addOperator(StreamGraph.java:434)
	at org.apache.flink.streaming.api.graph.StreamGraph.addOperator(StreamGraph.java:402)
	at org.apache.flink.streaming.api.graph.StreamGraph.addLegacySource(StreamGraph.java:356)
	at org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.translateInternal(LegacySourceTransformationTranslator.java:66)
	at org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.translateForStreamingInternal(LegacySourceTransformationTranslator.java:53)
	at org.apache.flink.streaming.runtime.translators.LegacySourceTransformationTranslator.translateForStreamingInternal(LegacySourceTransformationTranslator.java:40)
	at org.apache.flink.streaming.api.graph.SimpleTransformationTranslator.translateForStreaming(SimpleTransformationTranslator.java:62)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:860)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:590)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.getParentInputIds(StreamGraphGenerator.java:881)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.translate(StreamGraphGenerator.java:839)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.transform(StreamGraphGenerator.java:590)
	at org.apache.flink.streaming.api.graph.StreamGraphGenerator.generate(StreamGraphGenerator.java:328)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2289)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2280)
	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:2266)
	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaDynamicTableFactoryTest.testBufferedTableSink(UpsertKafkaDynamicTableFactoryTest.java:243)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.io.IOException: Serializing the source elements failed: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.serializeElements(FromElementsFunction.java:139)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.setOutputType(FromElementsFunction.java:160)
	... 68 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:69)
	at org.apache.flink.api.java.typeutils.runtime.kryo.FlinkChillPackageRegistrar.registerSerializers(FlinkChillPackageRegistrar.java:67)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:513)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:522)
	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:348)
	at org.apache.flink.streaming.api.functions.source.FromElementsFunction.serializeElements(FromElementsFunction.java:136)
	... 69 more
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @45b4c3a9
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:67)
	... 74 more
================================================================================
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 11 22:10:00 UTC 2023,,,,,,,,,,"0|z1l6ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 14:47;Sergey Nuyanzin;[~martijnvisser] i guess this ticket is connected with FLINK-33305 and corresponding PR
the idea is that it could be fixed in 2 ways
1. rewrite complete configuration of surefire plugin
2. allos for shared utils to specify {{{}-- add-opens{}}}, {{--add-exports}} via a dedicated property like it's done for flink modules, and then on connector's level only this property should be adapted;;;","11/Dec/23 22:10;Sergey Nuyanzin;Merged as [825052f55754e401176083c121ffaf38362b7a26|https://github.com/apache/flink-connector-kafka/commit/825052f55754e401176083c121ffaf38362b7a26];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HybridSource fails to clear the previous round's state when switching sources, leading to data loss",FLINK-33360,13555492,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengjiajie,fengjiajie,fengjiajie,25/Oct/23 13:01,31/Oct/23 07:24,04/Jun/24 20:40,31/Oct/23 07:24,1.16.2,1.17.1,1.18.0,,,,,,,,1.19.0,,,,Connectors / HybridSource,,,,,,0,pull-request-available,,,,,"org.apache.flink.connector.base.source.hybrid.HybridSourceSplitEnumerator:
{code:java}
            // track readers that have finished processing for current enumerator
            finishedReaders.add(subtaskId);
            if (finishedReaders.size() == context.currentParallelism()) {
                LOG.debug(""All readers finished, ready to switch enumerator!"");
                if (currentSourceIndex + 1 < sources.size()) {
                    switchEnumerator();
                    // switch all readers prior to sending split assignments
                    for (int i = 0; i < context.currentParallelism(); i++) {
                        sendSwitchSourceEvent(i, currentSourceIndex);
                    }
                }
            } {code}
I think that *finishedReaders* is used to keep track of all the subTaskIds that have finished reading the current round of the source. Therefore, in the *switchEnumerator* function, *finishedReaders* should be cleared:

If it's not cleared, then in the next source reading, whenever any SourceReader reports a *SourceReaderFinishedEvent* (while other SourceReaders may not have finished processing in parallel), the condition *finishedReaders.size() == context.currentParallelism()* will be satisfied and it will trigger {*}sendSwitchSourceEvent{*}(i, currentSourceIndex), sending a *SwitchSourceEvent* to all SourceReaders.
If a SourceReader receives a SwitchSourceEvent before it finishes reading the previous source, it will execute {*}currentReader.close(){*}, and some data may not be fully read, resulting in a partial data loss in the source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 30 08:25:24 UTC 2023,,,,,,,,,,"0|z1l6m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 13:12;fengjiajie;pr: https://github.com/apache/flink/pull/23593;;;","26/Oct/23 16:39;leonard;Thanks [~fengjiajie] for report this issue, I assigned this ticket to you as you have raised a PR.;;;","30/Oct/23 08:25;leonard;Fixed in master(1.19): 8d21c321dda18ad20022532c861d57ee38b70fda
1.18：013b337329ff4732e62d631cac8429cb68dd4776
1.17：19637741e9e56ab231fdcafd1dec4be4087e738e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator supports compiling with Java 17,FLINK-33359,13555456,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Sergey Nuyanzin,fanrui,fanrui,25/Oct/23 08:59,26/Oct/23 15:31,04/Jun/24 20:40,26/Oct/23 15:29,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"In the voting mailing list for flink-kubernetes-operator version 1.6.1, Thomas mentioned Kubernetes operator cannot compile with java 17.

Offline discussion with [~gyfora] , we hope Kubernetes operator supports compiling with Java 17 as a critical ticket in 1.7.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 26 15:29:32 UTC 2023,,,,,,,,,,"0|z1l6e8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 14:37;Sergey Nuyanzin;i submitted a couple of PRs 
https://github.com/apache/flink-kubernetes-operator/pull/690
and 
https://github.com/apache/flink-kubernetes-operator/pull/691

it should be enough to have it compilable with jdk17;;;","26/Oct/23 15:29;gyfora;Merged to main:
d0ee0e947badcba7ed351a3ce5fdf95ee5b79847
0b6ff5a9cbfd13dddea45c82c470e44d0139ecc7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL Client fails to start in Flink on YARN,FLINK-33358,13555428,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,prabhujoseph,prabhujoseph,prabhujoseph,25/Oct/23 04:42,25/Apr/24 02:25,04/Jun/24 20:40,01/Feb/24 03:52,1.18.0,,,,,,,,,,,,,,Deployment / YARN,Table SQL / Client,,,,,0,pull-request-available,,,,,"Flink SQL Client fails to start in Flink on YARN with below error
{code:java}
flink-yarn-session -tm 2048 -s 2 -d

/usr/lib/flink/bin/sql-client.sh 

Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not read from command line.
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:221)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:179)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:121)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:114)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:169)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:118)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179)
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.client.config.SqlClientOptions
	at org.apache.flink.table.client.cli.parser.SqlClientSyntaxHighlighter.highlight(SqlClientSyntaxHighlighter.java:59)
	at org.jline.reader.impl.LineReaderImpl.getHighlightedBuffer(LineReaderImpl.java:3633)
	at org.jline.reader.impl.LineReaderImpl.getDisplayedBufferWithPrompts(LineReaderImpl.java:3615)
	at org.jline.reader.impl.LineReaderImpl.redisplay(LineReaderImpl.java:3554)
	at org.jline.reader.impl.LineReaderImpl.doCleanup(LineReaderImpl.java:2340)
	at org.jline.reader.impl.LineReaderImpl.cleanup(LineReaderImpl.java:2332)
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:626)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:194)
	... 7 more
{code}
The issue is due to the old jline jar from Hadoop (3.3.3) classpath (/usr/lib/hadoop-yarn/lib/jline-3.9.0.jar) taking first precedence. Flink-1.18 requires jline-3.21.0.jar.

Placing flink-sql-client.jar (bundled with jline-3.21) before the Hadoop classpath fixes the issue.
{code:java}
diff --git a/flink-table/flink-sql-client/bin/sql-client.sh b/flink-table/flink-sql-client/bin/sql-client.sh
index 24746c5dc8..4ab8635de2 100755
--- a/flink-table/flink-sql-client/bin/sql-client.sh
+++ b/flink-table/flink-sql-client/bin/sql-client.sh
@@ -89,7 +89,7 @@ if [[ ""$CC_CLASSPATH"" =~ .*flink-sql-client.*.jar ]]; then
 elif [ -n ""$FLINK_SQL_CLIENT_JAR"" ]; then
 
     # start client with jar
-    exec ""$JAVA_RUN"" $FLINK_ENV_JAVA_OPTS $JVM_ARGS ""${log_setting[@]}"" -classpath ""`manglePathList ""$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS:$FLINK_SQL_CLIENT_JAR""`"" org.apache.flink.table.client.SqlClient ""$@"" --jar ""`manglePath $FLINK_SQL_CLIENT_JAR`""
+    exec ""$JAVA_RUN"" $FLINK_ENV_JAVA_OPTS $JVM_ARGS ""${log_setting[@]}"" -classpath ""`manglePathList ""$CC_CLASSPATH:$FLINK_SQL_CLIENT_JAR:$INTERNAL_HADOOP_CLASSPATHS`"" org.apache.flink.table.client.SqlClient ""$@"" --jar ""`manglePath $FLINK_SQL_CLIENT_JAR`""
 
 # write error message to stderr
 else
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 01 03:52:22 UTC 2024,,,,,,,,,,"0|z1l680:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/24 03:52;fsk119;Merged into master: fa364c7c668ec2a87bcbf18ce2b80f749cc16b2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add Apache Software License 2,FLINK-33357,13555423,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Not A Problem,caicancai,caicancai,caicancai,25/Oct/23 04:12,25/Oct/23 07:47,04/Jun/24 20:40,25/Oct/23 07:38,kubernetes-operator-1.6.0,,,,,,,,,,kubernetes-operator-1.5.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Flinkdeployments.flink.apache.org - v1. Currently yml and flinksessionjobs.flink.apache.org - v1. Yml don't

add add Apache Software License 2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 04:09;caicancai;2023-10-25 12-08-58屏幕截图.png;https://issues.apache.org/jira/secure/attachment/13063839/2023-10-25+12-08-58%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 25 07:38:40 UTC 2023,,,,,,,,,,"0|z1l66w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 07:31;gyfora;These are generated files based on source code that is Apache licensed .

I think we don’t need the header.;;;","25/Oct/23 07:38;gyfora;We attach the header during the release process https://github.com/apache/flink-kubernetes-operator/blob/main/tools/releasing/create_source_release.sh#L98;;;","25/Oct/23 07:38;gyfora;https://github.com/apache/flink-kubernetes-operator/blob/main/tools/releasing/create_source_release.sh#L98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The navigation bar on Flink’s official website is messed up.,FLINK-33356,13555422,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,JunRuiLi,JunRuiLi,25/Oct/23 03:56,02/Nov/23 03:58,04/Jun/24 20:40,31/Oct/23 08:52,,,,,,,,,,,1.19.0,,,,Project Website,,,,,,1,pull-request-available,,,,,"The side navigation bar on the Flink official website at the following link: [https://nightlies.apache.org/flink/flink-docs-master/] appears to be messed up, as shown in the attached screenshot.

!image-2023-10-25-11-55-52-653.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 03:55;JunRuiLi;image-2023-10-25-11-55-52-653.png;https://issues.apache.org/jira/secure/attachment/13063838/image-2023-10-25-11-55-52-653.png","25/Oct/23 04:34;Wencong Liu;image-2023-10-25-12-34-22-790.png;https://issues.apache.org/jira/secure/attachment/13063840/image-2023-10-25-12-34-22-790.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 03:58:01 UTC 2023,,,,,,,,,,"0|z1l66o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 04:42;Wencong Liu;Hello [~JunRuiLi] , I found this case is due to the commit ""30e8b3de05c1d6b75d8f27b9188a1d34f1589ac5"", which modified the subproject commit. I think we should revert this change. Could you assign to me?

!image-2023-10-25-12-34-22-790.png!;;;","25/Oct/23 06:01;JunRuiLi;[~Wencong Liu] Thank you for your volunteering, [~wanglijie] could you help to assign this ticket to Wencong?;;;","25/Oct/23 06:09;wanglijie;[~Wencong Liu] Assigned to you.;;;","31/Oct/23 08:52;wanglijie;Fixed via master 780a673d8e2c3845d685c86d95166d9169601726;;;","01/Nov/23 07:06;JunRuiLi;[~Wencong Liu] The navigation bar on the official website seems to have not been restored yet,  could you please re-confirm this problem? Otherwise, we will need to reopen this ticket.;;;","01/Nov/23 07:20;Wencong Liu;This is because of a recent failure in the documentation build. Once the issue with the document building process is resolved, the website will return to normal.;;;","02/Nov/23 03:58;wanglijie;The website is normal now, thanks [~JunRuiLi] [~Wencong Liu] :D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can't reduce the parallelism from 'n' to '1' when recovering through a savepoint.,FLINK-33355,13555420,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,edmond_j,edmond_j,25/Oct/23 03:07,26/Oct/23 08:36,04/Jun/24 20:40,25/Oct/23 12:39,,,,,,,,,,,,,,,API / Core,,,,,,0,,,,,,"If the program includes operators with window, it is not possible to reduce the parallelism of the operators from n to 1 when restarting from a savepoint, and it will result in an error: 
{code:java}
//IllegalStateException: Failed to rollback to checkpoint/savepoint Checkpoint Metadata. Max parallelism mismatch between checkpoint/savepoint state and new program. Cannot map operator 0e059b9f403cf6f35592ab773c9408d4 with max parallelism 128 to new program with max parallelism 1. This indicates that the program has been changed in a non-compatible way after the checkpoint/savepoint. {code}",flink 1.17.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 26 08:36:40 UTC 2023,,,,,,,,,,"0|z1l668:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 04:02;yunta;Changing the max-parallelism (instead of parallelism), would break the checkpoint compatibility, which is built by design. You can refer to https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/execution/parallel/#setting-the-maximum-parallelism for more details.;;;","25/Oct/23 05:44;edmond_j;[~yunta]   I know that modifying the max parallelism is not possible, but I only changed the parallelism by using the  parameter  {*}parallelism.default{*}. In my tests, reducing it from n to m (where m < n and m > 1) is feasible. However, reducing it to 1 results in the aforementioned error. Nevertheless, I have a requirement to reduce the parallelism of a Flink process from n to 1.;;;","25/Oct/23 05:58;yunta;[~edmond_j] could you please share the code to reproduce this problem?;;;","25/Oct/23 06:37;edmond_j;[~yunta] 
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
SqlServerSourceBuilder.SqlServerIncrementalSource<String> cdcSource  = ...;
DataStreamSource<String> cdcStream = env.fromSource(cdcSource, WatermarkStrategy.noWatermarks(), ""cdc""); 
cdcStream.windowAll(TumblingProcessingTimeWindows.of(Time.seconds(10)))
        .trigger(new CustomTrigger<>())
        .apply(new CustomFunction()).print();
env.execute(); {code}
For the task mentioned above, if I reduce the parallelism from n to 1, will encounter the previously mentioned error.
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); SqlServerSourceBuilder.SqlServerIncrementalSource<String> cdcSource = ...; 
DataStreamSource<String> cdcStream = env.fromSource(cdcSource, WatermarkStrategy.noWatermarks(), ""cdc""); cdcStream.print(); 
env.execute(); {code}
But doing this doesn't result in an error, so I suspect it might be caused by window operators.

 ;;;","25/Oct/23 06:46;yunta;[~edmond_j] How did you assign the parallelism, by setting the configuration of `parallelism.default`?;;;","25/Oct/23 06:47;edmond_j;[~yunta] yes;;;","25/Oct/23 06:55;yunta;I think this is because you forgot to set the uid for each operator. Since `windowAll` operator could only have parallelism 1, all operators would chain together once you change the parallelism to 1. Please assign the operator id as doc https://nightlies.apache.org/flink/flink-docs-stable/docs/ops/state/savepoints/#assigning-operator-ids said.;;;","25/Oct/23 06:59;edmond_j;[~yunta] I've almost assigned a UID to every operator, but I couldn't find a way to specify a UID for 'windowAll.' My actual code is :
{code:java}
dataStream
        .windowAll(TumblingProcessingTimeWindows.of(Time.seconds(10)))
        .trigger(new CountTriggerWithTimeout<>(batchSize))
        .apply(new ToBatchFunction()).name(""ToBatch"").uid(""ToBatch""){code};;;","26/Oct/23 06:35;edmond_j;[~martijnvisser]  Sorry, could you please explain to me why this issue is not considered a bug?;;;","26/Oct/23 08:04;martijnvisser;See https://stackoverflow.com/questions/68618377/flink-assign-uid-to-window-function;;;","26/Oct/23 08:36;edmond_j;[~martijnvisser]  I found that the {{.with()}} method is no longer available now. Is there any other way to support reducing the parallelism of tasks involving windows from {{n}} to 1?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache TaskInformation and JobInformation to avoid deserializing duplicate big objects,FLINK-33354,13555419,13554724,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,25/Oct/23 03:03,07/Nov/23 06:31,04/Jun/24 20:40,07/Nov/23 06:31,1.17.1,1.18.0,,,,,,,,,1.19.0,,,,Runtime / Task,,,,,,0,pull-request-available,,,,,"The background is similar to FLINK-33315.

A hive table with a lot of data, and the HiveSource#partitionBytes is 281MB. When slotPerTM = 4, one TM will run 4 HiveSources at the same time.

 

How the TaskExecutor to submit a large task?
 # TaskExecutor#loadBigData will read all bytes from file to SerializedValue<TaskInformation> 
 ** The SerializedValue<TaskInformation>  has a byte[]
 ** It will cost the heap memory
 ** It will be great than 281 MB, because it not only stores HiveSource#partitionBytes, it also stores other information of TaskInformation.
 # Generate the TaskInformation from SerializedValue<TaskInformation> 
 ** TaskExecutor#submitTask calls the tdd.getSerializedTaskInformation()..deserializeValue()
 ** tdd.getSerializedTaskInformation() is SerializedValue<TaskInformation> 
 ** It will generate the TaskInformation
 ** TaskInformation includes the Configuration {color:#9876aa}taskConfiguration{color}
 ** The {color:#9876aa}taskConfiguration{color} includes StreamConfig#{color:#9876aa}SERIALIZEDUDF{color}

 

{color:#172b4d}Based on the above process, TM memory will have 2 big byte array for each task:{color}
 * {color:#172b4d}The SerializedValue<TaskInformation>{color}
 * {color:#172b4d}The TaskInformation{color}

When one TM runs 4 HiveSources at the same time, it will have 8 big byte array.

In our production environment, this is also a situation that often leads to TM OOM.
h2. Solution:

These data is totally same due to the PermanentBlobKey is same. We can add a cache for it to reduce the memory and cpu cost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 06:31:46 UTC 2023,,,,,,,,,,"0|z1l660:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 06:31;fanrui;Merged to master<1.19> via 64992471c5dd52fb4269f6de51c15e27c2a45b55 to b759794e33d68b557f4ee2d7287f5fae9a80167e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL fails because ""TimestampType.kind"" is not serialized ",FLINK-33353,13555369,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ferenc-csaky,ferenc-csaky,24/Oct/23 15:37,02/Nov/23 10:05,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"We have a custom persistent catalog store, which stores tables, views etc. in a DB. In our application, it is required to utilize the serialized formats of entities, but the same applies to the Hive, as it functions as a persistent catalog.

Take the following example SQL:

{code:sql}
CREATE TABLE IF NOT EXISTS `txn_gen` (
  `txn_id` INT,
  `amount` INT,
  `ts` TIMESTAMP(3),
   WATERMARK FOR `ts` AS `ts` - INTERVAL '1' SECOND
) WITH (
  'connector' = 'datagen',
  'fields.txn_id.min' = '1',
  'fields.txn_id.max' = '5',
  'rows-per-second' = '1'
);

CREATE VIEW IF NOT EXISTS aggr_ten_sec AS
  SELECT txn_id,
         TUMBLE_ROWTIME(`ts`, INTERVAL '10' SECOND) AS w_row_time,
         COUNT(txn_id) AS txn_count
    FROM txn_gen
    GROUP BY txn_id, TUMBLE(`ts`, INTERVAL '10' SECOND);

SELECT txn_id,
       SUM(txn_count),
       TUMBLE_START(w_row_time, INTERVAL '20' SECOND) AS total_txn_count
  FROM aggr_ten_sec
  GROUP BY txn_id, TUMBLE(w_row_time, INTERVAL '20' SECOND);
{code}

This will work without any problems when we simply execute it in a {{TableEnvironment}}, but it fails with the below error when we try to execute the query based on the serialized table metadata.
{code}
org.apache.flink.table.api.TableException: Window aggregate can only be defined over a time attribute column, but TIMESTAMP(3) encountered.
{code}

If there is a view which would require to use ROWTIME, it will be lost and we cannot recreate the same query from the serialized entites.

Currently in [TimestampType|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/TimestampType.java] the ""kind"" field is deliberatly annotated as {{@Internal}} and is not serialized, although it breaks this functionality.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 10:05:22 UTC 2023,,,,,,,,,,"0|z1l5uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 11:38;xuyangzhong;Hi, [~ferenc-csaky] . How can you serialize the tables, views and etc ?;;;","25/Oct/23 12:21;ferenc-csaky;In a nutshell we are utilizing the (deprecated) [DescriptorProperties|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java] to extract the metadata from {{CatalogBaseTable}} and store that in as part of custom DB record.;;;","02/Nov/23 10:05;ferenc-csaky;[~twalthr] any thoughts?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenAPI spec is lacking mappings for discriminator properties,FLINK-33352,13555357,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,24/Oct/23 14:31,25/Oct/23 08:50,04/Jun/24 20:40,25/Oct/23 08:50,1.17.0,,,,,,,,,,1.17.2,1.18.1,1.19.0,,Documentation,Runtime / REST,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 25 08:50:13 UTC 2023,,,,,,,,,,"0|z1l5s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 08:50;chesnay;master: 100cc20f40bb09890dac3708387cc54bdcd91654
1.18: ef950f86af3f9a3cccca585529dad788c466e437
1.17: 2aee7df2d7904ffb38a99363d8b14d3ef2f1950f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - CLONE - Vote on the release candidate,FLINK-33351,13555346,13555342,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,24/Oct/23 12:42,24/Oct/23 12:44,04/Jun/24 20:40,24/Oct/23 12:44,1.18.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 12:42:58.0,,,,,,,,,,"0|z1l5ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - CLONE - Propose a pull request for website updates,FLINK-33350,13555345,13555342,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,24/Oct/23 12:42,26/Oct/23 10:27,04/Jun/24 20:40,24/Oct/23 12:44,1.18.0,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 12:42:58.0,,,,,,,,,,"0|z1l5pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - CLONE - Stage source and binary releases on dist.apache.org,FLINK-33349,13555344,13555342,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,24/Oct/23 12:42,24/Oct/23 12:44,04/Jun/24 20:40,24/Oct/23 12:44,,,,,,,,,,,1.18.0,,,,,,,,,,0,,,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 12:42:58.0,,,,,,,,,,"0|z1l5pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - CLONE - Build and stage Java and Python artifacts,FLINK-33348,13555343,13555342,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,24/Oct/23 12:42,24/Oct/23 12:44,04/Jun/24 20:40,24/Oct/23 12:44,,,,,,,,,,,1.18.0,,,,,,,,,,0,,,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 12:42:58.0,,,,,,,,,,"0|z1l5p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - CLONE - Build Release Candidate: 1.18.0-rc3,FLINK-33347,13555342,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,24/Oct/23 12:42,24/Oct/23 12:45,04/Jun/24 20:40,24/Oct/23 12:45,1.18.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.
h4. Prerequisites

Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code:java}
RC_NUM=""3""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,FLINK-32925,FLINK-32921,FLINK-32726,FLINK-32920,,FLINK-33271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 12:42:57.0,,,,,,,,,,"0|z1l5ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherResourceCleanupTest.testFatalErrorIfJobCannotBeMarkedDirtyInJobResultStore fails on AZP,FLINK-33346,13555290,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,24/Oct/23 07:15,25/Oct/23 07:39,04/Jun/24 20:40,25/Oct/23 07:39,1.16.2,1.17.1,1.18.0,1.19.0,,,,,,,1.16.3,1.17.2,1.18.0,1.19.0,Runtime / Coordination,,,,,,0,pull-request-available,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53905&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=6800

failed with 
{noformat}
Oct 22 00:59:32 Caused by: java.io.IOException: Expected IOException.
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.DispatcherResourceCleanupTest.lambda$testFatalErrorIfJobCannotBeMarkedDirtyInJobResultStore$6(DispatcherResourceCleanupTest.java:558)
Oct 22 00:59:32 	at org.apache.flink.runtime.testutils.TestingJobResultStore.createDirtyResultAsync(TestingJobResultStore.java:81)
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.Dispatcher.createDirtyJobResultEntryAsync(Dispatcher.java:1441)
Oct 22 00:59:32 	at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createDirtyJobResultEntryIfMissingAsync$45(Dispatcher.java:1422)
Oct 22 00:59:32 	at java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:995)
Oct 22 00:59:32 	... 39 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 25 07:39:47 UTC 2023,,,,,,,,,,"0|z1l5dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/23 07:53;mapohl;The instability is caused by the 100ms. The fatal error hides the actual cause (the {{TimeoutException}}). It can be reproduced by setting the timeout of the test to 0ms. The actual cause is revealed when disabling the fatal error handling or not throwing a AssertionError in case of the fatal error.;;;","25/Oct/23 07:39;mapohl;master: [d722bf7d1fa1c267784b5103438e7a53be919d77|https://github.com/apache/flink/commit/d722bf7d1fa1c267784b5103438e7a53be919d77]
1.18: [8be1c870e395225d9aa236a7349444e8f8340459|https://github.com/apache/flink/commit/8be1c870e395225d9aa236a7349444e8f8340459]
1.17: [06db375bd3a27417f527e00dc3b1c8880508dfb3|https://github.com/apache/flink/commit/06db375bd3a27417f527e00dc3b1c8880508dfb3]
1.16: [c91a1de574de6a863cbe86a6cc47197921a64745|https://github.com/apache/flink/commit/c91a1de574de6a863cbe86a6cc47197921a64745];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sql gateway only single statement supported,FLINK-33345,13555287,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,hunterLiu,hunterLiu,24/Oct/23 07:04,24/Oct/23 07:17,04/Jun/24 20:40,,,,,,,,,,,,,,,,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,"When using sql gateway, when submitting multiple sql at one time, an error is reported: only single statement supported. I don't quite understand why only one sql must be accepted [here|https://github.com/apache/flink/blob/fa0dd3559e9697e21795a2634d8d99a0b7efdcf3/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/delegation/ParserImpl.java#L101].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 07:04:31.0,,,,,,,,,,"0|z1l5co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace Time with Duration in RpcInputSplitProvider,FLINK-33344,13555274,13268587,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,24/Oct/23 03:50,24/Oct/23 09:42,04/Jun/24 20:40,24/Oct/23 09:40,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / RPC,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 24 09:42:10 UTC 2023,,,,,,,,,,"0|z1l59s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/23 09:39;mapohl;[~jiabao.sun] Just for the future: The ""Fix Version"" is meant to be set when resolving the issue. The ""Affected Version"" is used to specify what versions are affected by the issue and should retrieve a fix. I'm gonna go ahead and move the version from ""Fix Version"" to ""Affected Version"".;;;","24/Oct/23 09:40;mapohl;master: 22c1eb44df7226c8e9045789e45c71c93668c644;;;","24/Oct/23 09:42;jiabao.sun;OK. Thanks [~mapohl].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close stale Flink PRs,FLINK-33343,13555258,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vsowrirajan,vsowrirajan,24/Oct/23 00:50,25/Oct/23 08:05,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"What is considered a stale PR? If any of the below condition is met, then the PR is considered as a stale PR
{code:java}
1. PRs that are not followed-up within 'X' number of days after a review
2. PRs that don't have a passing build and/or don't follow contribution guidelines after 'X' number of days.
3. PRs that have merge conflicts after 'X' number of days.
{code}
We are yet to decide on what is 'X' yet? This can be done as part of the PR and retroactively updating the same in the JIRA.

To see the complete set of conversations on this topic, see [here|https://lists.apache.org/thread/pml95msx21sdc539404xs9tk209sdd55]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-24 00:50:41.0,,,,,,,,,,"0|z1l568:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDK 17 CI run doesn't set java17-target profile,FLINK-33342,13555205,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,23/Oct/23 13:35,27/Oct/23 08:01,04/Jun/24 20:40,24/Oct/23 11:27,1.18.0,1.19.0,,,,,,,,,1.18.1,1.19.0,,,Build System / Azure Pipelines,Build System / CI,,,,,0,pull-request-available,,,,,"In contrast to the jdk11 CI run which has the java11-target profile set (see [tools/azure-pipelines/build-apache-repo.yml:138|https://github.com/apache/flink/blob/9b63099964b36ad9d78649bb6f5b39473e0031bd/tools/azure-pipelines/build-apache-repo.yml#L138]), it's missing for the jdk17 CI run (see [tools/azure-pipelines/build-apache-repo.yml:149|https://github.com/apache/flink/blob/9b63099964b36ad9d78649bb6f5b39473e0031bd/tools/azure-pipelines/build-apache-repo.yml#L149]).

The profile for the source version (i.e. {{java11}} and {{java17}}) are automatically activated through the JDK version of the run.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33369,,,,,,,,,FLINK-32380,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 24 11:27:26 UTC 2023,,,,,,,,,,"0|z1l4ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/23 11:27;mapohl;master: [6ebd4dc3e4f902701d64d3709d47d971f1ac4753|https://github.com/apache/flink/commit/6ebd4dc3e4f902701d64d3709d47d971f1ac4753]
1.18: [dc5bc811de1b4bce9e8858080e234865e431fbff|https://github.com/apache/flink/commit/dc5bc811de1b4bce9e8858080e234865e431fbff];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use available local keyed state for rescaling,FLINK-33341,13555200,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,srichter,srichter,23/Oct/23 13:03,11/Dec/23 09:54,04/Jun/24 20:40,27/Oct/23 14:49,,,,,,,,,,,1.19.0,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,,,"Local state is currently only used for recovery. However, it would make sense to also use available local state in rescaling scenarios to reduce the amount of data to download from remote storage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 14:49:51 UTC 2023,,,,,,,,,,"0|z1l4tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 13:23;yunta;Current FLIP-291 already supports local recovery with the adaptive scheduler, what's the case here?;;;","23/Oct/23 13:47;srichter;Exactly, we support local state for recovery - but not for rescaling, yet.;;;","24/Oct/23 14:08;Yanfei Lei;Hi [~srichter]

IIUC, only subtasks that are assigned to the original physical machine can use the local state.

For a subtask, will part of its state come from the local state and part from the remote state? Does this require the scheduler to do anything else?;;;","24/Oct/23 14:52;srichter;[~Yanfei Lei], yes only the previous local state is available to be used in rescaling, so we might still need to download additional state from remote. But oftentimes we don't need to download everything from remote, in particular if we scale out we will often find the complete state locally on some machines and just need to drop some key-groups. And for scale-in, we should at least find one piece of the state locally. There is no good reason not to opportunistically use local state also in rescaling scenarios. No change to the scheduler will be needed.;;;","24/Oct/23 15:02;srichter;FYI, here is a link to the development branch: https://github.com/apache/flink/compare/master...StefanRRichter:flink:srichter-local-rescaling-FLINK-33341;;;","27/Oct/23 14:49;srichter;merged in a4ad86f ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump Jackson to 2.15.3,FLINK-33340,13555181,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,23/Oct/23 10:39,30/Nov/23 11:08,04/Jun/24 20:40,20/Nov/23 17:40,,,,,,,,,,,1.19.0,shaded-18.0,,,BuildSystem / Shaded,,,,,,0,pull-request-available,,,,,"Among others there is a number of improvements regarding parsing of numbers (jackson-core)
https://github.com/FasterXML/jackson-core/blob/2.16/release-notes/VERSION-2.x
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 20 17:40:19 UTC 2023,,,,,,,,,,"0|z1l4p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Nov/23 01:06;Sergey Nuyanzin;Merged to flink-shaded master as [/9fef53a5532d394bc853ba92088d23c98005da98|https://github.com/apache/flink-shaded/commit/9fef53a5532d394bc853ba92088d23c98005da98];;;","20/Nov/23 17:40;Sergey Nuyanzin;flink main repo master [316c08b0d962c2c7344785ad845c0e7bc465c094|https://github.com/apache/flink/commit/316c08b0d962c2c7344785ad845c0e7bc465c094];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Guava to 32.1.3,FLINK-33339,13555168,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Oct/23 09:21,31/Oct/23 12:02,04/Jun/24 20:40,31/Oct/23 12:02,,,,,,,,,,,shaded-18.0,,,,BuildSystem / Shaded,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 12:02:43 UTC 2023,,,,,,,,,,"0|z1l4m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 12:02;martijnvisser;Fixed in apache/flink-shaded:master 8eb41f5926d731c6b2c4ee4c0be29eabda2af7fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump FRocksDB version,FLINK-33338,13555163,13526295,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,roman,pnowojski,pnowojski,23/Oct/23 09:04,15/Jan/24 14:51,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,,,"We need to bump RocksDB in order to be able to use new IngestDB and ClipDB commands.

If some of the required changes haven't been merged to Facebook/RocksDB, we should cherry-pick and include them in our FRocksDB fork.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 15 14:51:03 UTC 2024,,,,,,,,,,"0|z1l4l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 12:32;pnowojski;The problem is that some of the required by us changes, haven't been merged yet. And some of the other changes, are very new and not part of any RocksDB release yet. https://issues.apache.org/jira/browse/FLINK-33337 tracks the state and the list of the related PRs against facebook/rocksdb repository that are required by FLINK-31238;;;","24/Nov/23 06:49;mayuehappy;[~pnowojski] Update:  Now All the changes we need are released in *8.9.0(11/17/203) [HISTORY.md|https://github.com/facebook/rocksdb/blob/main/HISTORY.md]*;;;","05/Dec/23 12:09;Yanfei Lei;[~mayuehappy]/[~pnowojski]  Some updates:

I first released a [version|https://s01.oss.sonatype.org/content/repositories/releases/io/github/fredia/frocksdbjni/8.6.7-ververica-test-1.0/] (based on 8.6.7) for [benchmarking|http://jenkins.flink-speed.xyz/]. This version does not include {{ppcle}} and {{windows}}  platform.  And I encountered some problems when cherry-pick https://issues.apache.org/jira/browse/FLINK-19710 (because this part of the code has been updated many times). 

I also ran microbenchmarks based on 8.6.7 version, and there were some big [performance regressions|http://flink-speed.xyz/comparison/?exe=1%2BL%2Bbenchmark-request%2C3%2BL%2Bmaster&ben=148%2C150%2C202%2C152%2C154%2C156%2C158%2C160%2C162%2C164%2C195%2C166%2C168%2C170%2C172%2C174%2C176%2C178%2C180%2C182%2C352%2C353&env=3&hor=true&bas=3%2BL%2Bmaster&chart=normal+bars] compared to 6.20.3.;;;","08/Dec/23 04:05;mayuehappy;[~Yanfei Lei] Thank you very much for your help. What I want to know is whether the performance regression is due to the fact that the commit https://issues.apache.org/jira/browse/FLINK-19710 has not been cherry-picked? I am willing to help analyze the reasons for performance regression.;;;","08/Dec/23 09:58;pnowojski;From what I've heard from an offline discussion, the suspicion is that the regression might be caused by FLINK-19710. From the look at the performance regression in FLINK-19710 (up to ~10%) and here (up to ~20%), it suggests that there might be more than just that. But definitely the next step would be trying to re-implement FLINK-19710 on top of FRocksDB 8.6.7 and:
 * check if that solves the problem (At least mostly)
 * maybe during this one can actually spot something else that might be responsible for the regression

If you could help with this [~mayuehappy] , that would be a tremendous help for us. AFAIK [~Yanfei Lei] won't be able to tackle this in the next couple of months. Me or my colleagues might be available to help sooner, but most likely not sooner than January 2024.  ;;;","15/Jan/24 14:51;pnowojski;[~roman], let's make sure that this latest bug fix is also included in our FRocksDB release:
https://issues.apache.org/jira/browse/FLINK-33337?focusedCommentId=17805364&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17805364;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose IngestDB and ClipDB in the official RocksDB API,FLINK-33337,13555162,13526295,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mayuehappy,pnowojski,pnowojski,23/Oct/23 09:02,15/Jan/24 14:51,04/Jun/24 20:40,07/Nov/23 10:33,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,,,"Remaining open PRs:

None :)

Already merged PRs:


https://github.com/facebook/rocksdb/pull/11646
https://github.com/facebook/rocksdb/pull/11868
https://github.com/facebook/rocksdb/pull/11811
https://github.com/facebook/rocksdb/pull/11381
https://github.com/facebook/rocksdb/pull/11379
https://github.com/facebook/rocksdb/pull/11378",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/24 04:03;mayuehappy;image-2024-01-11-12-03-14-308.png;https://issues.apache.org/jira/secure/attachment/13065889/image-2024-01-11-12-03-14-308.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 15 14:51:29 UTC 2024,,,,,,,,,,"0|z1l4kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/23 04:54;mayuehappy;Update: [https://github.com/facebook/rocksdb/pull/11868] merged in [https://github.com/facebook/rocksdb/commit/8e1adab5cecad129131a4eceabe645b9442acb9c];;;","07/Nov/23 03:36;mayuehappy;Update:

[https://github.com/facebook/rocksdb/pull/11646] merged in [{{19768a9}}|https://github.com/facebook/rocksdb/commit/19768a923a814a7510423b57329c50587362541e].;;;","07/Nov/23 07:59;pnowojski;Thanks for the update [~mayuehappy]! Does it mean that all of the required changes were merged into facebook/rocksdb?;;;","08/Nov/23 10:43;mayuehappy;[~pnowojski] yes ~ all of the required changes were merged into facebook/rocksdb. I plan to create a version of frocksdb with the latest rocksdb code to test the functionality and performance of ingestdb;;;","11/Jan/24 04:03;mayuehappy;Update: https://github.com/facebook/rocksdb/pull/12219
This PR fixes a bug that may cause incorrect ClipDB results ;;;","15/Jan/24 14:51;pnowojski;Thanks [~mayuehappy] for the update!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ASM to 9.6,FLINK-33336,13555153,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,23/Oct/23 07:57,23/Oct/23 09:27,04/Jun/24 20:40,23/Oct/23 09:27,,,,,,,,,,,shaded-18.0,,,,BuildSystem / Shaded,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 09:27:39 UTC 2023,,,,,,,,,,"0|z1l4iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 09:27;martijnvisser;Fixed in apache/flink-shaded:master 4b62d5fdbe1488549ba289ab49c0ecd241b50e2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reactivate missing e2e tests,FLINK-33335,13555073,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,21/Oct/23 14:41,29/Oct/23 11:42,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"FLINK-17375 removed _run-pre-commit-tests.sh_ in Flink 1.12 [1]. Since then the following tests are not executed anymore:
_test_state_migration.sh_
_test_state_evolution.sh_
_test_streaming_kinesis.sh_
_test_streaming_classloader.sh_
_test_streaming_distributed_cache_via_blob.sh_

[1]   https://github.com/apache/flink/pull/12268/files#diff-39f0aea40d2dd3f026544bb4c2502b2e9eab4c825df5f2b68c6d4ca8c39d7b5e",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 25 10:13:02 UTC 2023,,,,,,,,,,"0|z1l414:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/23 14:46;afedulov;[~rmetzger] [~chesnay] 
Could you please confirm that disabling of the aforementioned tests was intentional?
I would like to understand if we can simply drop _org.apache.flink.test.StatefulStreamingJob_ without moving it to a FLIP-27 source.

 ;;;","24/Oct/23 13:28;rmetzger;It's a long time ago since we worked on this :) I don't recall the details, and there's no explicit comment about removing those tests.
My gut feeling is that removing these tests was not intentional, and if it is not very difficult to re-activate them, I would do so.;;;","25/Oct/23 10:13;afedulov;[~rmetzger] thanks for the clarification! I am changing the name of the ticket
Remove unused e2e tests -> Reactivate missing e2e tests  :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A number of json plan tests fail with comparisonfailure,FLINK-33334,13555023,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 22:10,30/Oct/23 14:19,04/Jun/24 20:40,30/Oct/23 14:19,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"for instance
{noformat}
[ERROR] org.apache.flink.table.planner.plan.nodes.exec.stream.SortJsonPlanTest.testSort  Time elapsed: 0.037 s  <<< FAILURE!
org.junit.ComparisonFailure: 
expected:<...alse"",
            ""[table-sink-class"" : ""DEFAULT"",
            ""connector"" : ""values]""
          }
      ...> but was:<...alse"",
            ""[connector"" : ""values"",
            ""table-sink-class"" : ""DEFAULT]""
          }
      ...>
	at org.junit.Assert.assertEquals(Assert.java:117)
	at org.junit.Assert.assertEquals(Assert.java:146)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyJsonPlan(TableTestBase.scala:846)
	at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyJsonPlan(TableTestBase.scala:813)
	at org.apache.flink.table.planner.plan.nodes.exec.stream.SortJsonPlanTest.testSort(SortJsonPlanTest.java:64)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 30 14:19:31 UTC 2023,,,,,,,,,,"0|z1l3q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 14:19;qingyue;Fixed in master: cc62044efc054b057d02838a02356fe7c9c9d7a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TableSinkJsonPlanTest fails as JsonParseException: Unexpected character ('""' (code 34)): was expecting comma ",FLINK-33333,13555022,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 21:57,29/Nov/23 08:12,04/Jun/24 20:40,29/Nov/23 08:10,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"{{org.apache.flink.table.planner.plan.nodes.exec.stream.TableSinkJsonPlanTest#testCdcWithNonDeterministicFuncSinkWithDifferentPk}} fails as
{noformat}
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParseException: Unexpected character ('""' (code 34)): was expecting comma to separate Object entries
 at [Source: (String)""{
  ""flinkVersion"" : """",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`users`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""user_id"",
              ""dataType"" : ""VARCHAR(2147483647) NOT NULL""
            }, {
              ""name"" : ""user_name"",
              ""dataType"" : ""VARCHAR(2147483647)""
         ""[truncated 4287 chars]; line: 34, column: 14]

	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2418)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:749)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:673)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipComma(ReaderBasedJsonParser.java:2459)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextFieldName(ReaderBasedJsonParser.java:950)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer._deserializeContainerNoRecursion(JsonNodeDeserializer.java:537)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:98)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:23)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4706)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:2989)
	at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonParser.readValueAsTree(JsonParser.java:2392)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 29 08:12:32 UTC 2023,,,,,,,,,,"0|z1l3ps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Nov/23 08:09;mapohl;This issue was created by accident.;;;","29/Nov/23 08:12;Sergey Nuyanzin;yes, as I realized it was a result of local running with incomplete fix for FLINK-33334;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available",FLINK-33332,13555021,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 21:43,21/Oct/23 22:13,04/Jun/24 20:40,21/Oct/23 22:13,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"A couple of tests in flink-python  fail like

{noformat}
java.lang.AssertionError: Exception in test: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available
	at org.apache.flink.table.runtime.arrow.ArrowReaderWriterTestBase.testBasicFunctionality(ArrowReaderWriterTestBase.java:86)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,FLINK-33331,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Oct 21 22:12:48 UTC 2023,,,,,,,,,,"0|z1l3pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Oct/23 22:12;Sergey Nuyanzin;now it seems it is a duplicate of FLINK-33331;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade netty to 4.1.93+,FLINK-33331,13555020,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 21:41,16/Nov/23 13:55,04/Jun/24 20:40,12/Nov/23 17:10,,,,,,,,,,,1.19.0,shaded-18.0,,,API / Python,,,,,,0,pull-request-available,,,,,"A number of tests fails like e.g.for 
{{org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOpe}}
{noformat}
[ERROR] ratorTest.testFinishBundleTriggeredByTime  Time elapsed: 0.031 s  <<< ERROR!
java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.runtime.arrow.serializers.ArrowSerializer
	at org.apache.flink.table.runtime.operators.python.scalar.arrow.ArrowPythonScalarFunctionOperator.open(ArrowPythonScalarFunctionOperator.java:72)
	at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.open(AbstractStreamOperatorTestHarness.java:681)
	at org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperatorTestBase.testFinishBundleTriggeredByTime(PythonScalarFunctionOperatorTestBase.java:156)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
...
Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.RuntimeException: Arrow depends on DirectByteBuffer.<init>(long, int) which is not available. Please set the system property 'io.netty.tryReflectionSetAccessible' to 'true'. [in thread ""ForkJoinPool-3-worker-1""]
    at org.apache.flink.table.runtime.arrow.ArrowUtils.checkArrowUsable(ArrowUtils.java:184)
    at org.apache.flink.table.runtime.arrow.serializers.ArrowSerializer.<clinit>(ArrowSerializer.java:44)
    at org.apache.flink.table.runtime.utils.PassThroughPythonAggregateFunctionRunner.<init>(PassThroughPythonAggregateFunctionRunner.java:96)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest$PassThroughBatchArrowPythonGroupWindowAggregateFunctionOperator.createPythonFunctionRunner(BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java:414)
    at org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.open(AbstractExternalPythonFunctionOperator.java:56)
    at org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator.open(AbstractStatelessFunctionOperator.java:92)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.AbstractArrowPythonAggregateFunctionOperator.open(AbstractArrowPythonAggregateFunctionOperator.java:89)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.AbstractBatchArrowPythonAggregateFunctionOperator.open(AbstractBatchArrowPythonAggregateFunctionOperator.java:82)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperator.open(BatchArrowPythonGroupWindowAggregateFunctionOperator.java:119)
    at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.open(AbstractStreamOperatorTestHarness.java:681)
    at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount(BatchArrowPythonGroupWindowAggregateFunctionOperatorTest.java:140)
    ... 57 more
{noformat}


UPDATE
The reason is that since JDK21 there was removed this constructor within https://bugs.openjdk.org/browse/JDK-8303083
 and corresponding changes in Netty are done at https://github.com/netty/netty/pull/13366 which is a part of 4.1.93.Final",,,,,,,,,,,,,,,,,,,,,,FLINK-33332,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Nov 12 17:09:52 UTC 2023,,,,,,,,,,"0|z1l3pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 09:27;martijnvisser;Fixed in apache/flink-shaded:master c2e27e8188613b53a283deced8667b16db302b23;;;","05/Nov/23 14:36;Sergey Nuyanzin;Sorry, probably I wasn't clear enough
Need to reopen this because to fix we need to update Netty in Flink main repo as well (for some reason flink-python doesn't use the version from flink-shaded). Also since flink-python uses arrow depending on netty, arrow also should be updated.

Without this updates the error in description still appears

I will submit a PR for Flink main repo;;;","12/Nov/23 17:09;Sergey Nuyanzin;Merged to flink main repo as 
[c488adfbcdca30320b8721c028f9289f1563ae05|https://github.com/apache/flink/commit/c488adfbcdca30320b8721c028f9289f1563ae05];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump zookeeper  to address CVE-2023-44981,FLINK-33330,13555007,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 19:39,27/Nov/23 14:36,04/Jun/24 20:40,23/Oct/23 09:00,,,,,,,,,,,shaded-18.0,,,,BuildSystem / Shaded,,,,,,0,pull-request-available,,,,,"There is a [CVE-2023-44981|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-44981] which is fixed in 3.7.2, 3.8.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 09:00:06 UTC 2023,,,,,,,,,,"0|z1l3mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 09:00;Sergey Nuyanzin;Merged as [18e315a25bb699263861146fd06b0552f0e9e355|https://github.com/apache/flink-shaded/commit/18e315a25bb699263861146fd06b0552f0e9e355];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump commons-compress to address CVEs,FLINK-33329,13554991,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 17:58,18/Jan/24 15:09,04/Jun/24 20:40,27/Dec/23 13:44,,,,,,,,,,,1.19.0,jdbc-3.2.0,kafka-3.1.0,,,,,,,,0,pull-request-available,,,,,current version has CVE-2023-42503,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 18 15:09:40 UTC 2024,,,,,,,,,,"0|z1l3iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 09:06;Sergey Nuyanzin;Merged as [d59196cc23e58ca7d86d7ce2924defb8ba289c3e|https://github.com/apache/flink/commit/d59196cc23e58ca7d86d7ce2924defb8ba289c3e];;;","27/Dec/23 13:44;martijnvisser;Re-opened to also add this for:
apache/flink-connector-jdbc@main be5150b32367a33c99faa5ed29be5f6dd8af3193;;;","18/Jan/24 15:09;martijnvisser;Also fixed for: apache/flink-connector-kafka@main 8d531898d7df775535084e0413442c86a4c497ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow TwoPhaseCommittingSink WithPreCommitTopology to alter the type of the Committable,FLINK-33328,13554943,13426150,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,pvary,pvary,pvary,20/Oct/23 12:51,06/Feb/24 11:22,04/Jun/24 20:40,06/Feb/24 11:22,,,,,,,,,,,,,,,API / DataStream,Connectors / Common,,,,,0,pull-request-available,,,,,"In case of the Iceberg Sink, we would like to use the _WithPreCommitTopology_ to aggregate the writer results and create a single committable from them. So we would like to change both the type, and the number of the messages. Using the current _WithPreCommitTopology_ interface we can work around the issue by using a Tuple, or POJO where some of the fields are used only before the _addPreCommitTopology_ method, and some of the fields are only used after the method, but this seems more like abusing the interface than using it.

This is a more generic issue where the _WithPreCommitTopology_ should provide a way to transform not only the data, but the type of the data channelled through it.",,,,,,,,,,,,,,,,,,,,,,,FLINK-33972,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 06 11:22:08 UTC 2024,,,,,,,,,,"0|z1l388:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/24 11:22;pvary;Solved as part of FLINK-33972;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window TVF column expansion does not work with an INSERT INTO,FLINK-33327,13554941,13549560,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,20/Oct/23 12:46,20/Oct/23 15:58,04/Jun/24 20:40,20/Oct/23 15:58,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"If we have an {{INSERT INTO}} with an explicit column list and a {{TUMBLE}} function, the explicit column expansion fails with {{NullPointerException}}.

Test to reproduce:

{code}
    @Test
    public void testExplicitTableWithinTableFunctionWithInsertIntoNamedColumns() {
        tableEnv.getConfig()
                .set(
                        TABLE_COLUMN_EXPANSION_STRATEGY,
                        Collections.singletonList(EXCLUDE_DEFAULT_VIRTUAL_METADATA_COLUMNS));

        tableEnv.executeSql(
                ""CREATE TABLE sink (\n""
                        + ""  a STRING,\n""
                        + ""  c BIGINT\n""
                        + "") WITH (\n""
                        + "" 'connector' = 'values',""
                        + "" 'sink-insert-only' = 'false'""
                        + "")"");

        tableEnv.explainSql(
                ""INSERT INTO sink(a, c) ""
                        + ""SELECT t3_s, COUNT(t3_i) FROM ""
                        + "" TABLE(TUMBLE(TABLE t3, DESCRIPTOR(t3_m_virtual), INTERVAL '1' MINUTE)) ""
                        + ""GROUP BY t3_s;"");
    }
{code}

Exception:

{code}
org.apache.flink.table.api.ValidationException: SQL validation failed. SQL validation failed. null

	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:189)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:115)
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:282)
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.explainSql(TableEnvironmentImpl.java:696)
	at org.apache.flink.table.api.TableEnvironment.explainSql(TableEnvironment.java:976)
	at org.apache.flink.table.planner.plan.stream.sql.ColumnExpansionTest.testExplicitTableWithinTableFunctionWithInsertIntoNamedColumns(ColumnExpansionTest.java:279)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:189)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:115)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validateRichSqlInsert(FlinkPlannerImpl.scala:294)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:177)
	... 34 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.performUnconditionalRewrites(FlinkCalciteSqlValidator.java:213)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1288)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.performUnconditionalRewrites(FlinkCalciteSqlValidator.java:204)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1288)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.performUnconditionalRewrites(FlinkCalciteSqlValidator.java:204)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.performUnconditionalRewrites(SqlValidatorImpl.java:1288)
	at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.performUnconditionalRewrites(FlinkCalciteSqlValidator.java:204)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:985)
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:741)
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:185)
	... 37 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 15:58:43 UTC 2023,,,,,,,,,,"0|z1l37s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 15:58;dwysakowicz;Fixed in c7beda0da81ffc4bbb01befafd2eed08b7b35854;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove usage of FromElementsFunction,FLINK-33326,13554917,13449984,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,20/Oct/23 10:37,20/Oct/23 10:37,04/Jun/24 20:40,,,,,,,,,,,,,,,,API / Core,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-20 10:37:26.0,,,,,,,,,,"0|z1l32g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-375: Built-in cross-platform powerful java profiler,FLINK-33325,13554911,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,Yu Chen,Yu Chen,Yu Chen,20/Oct/23 09:57,02/Mar/24 06:24,04/Jun/24 20:40,02/Mar/24 06:24,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / REST,Runtime / Web Frontend,,,,,0,,,,,,"This is an umbrella JIRA of [FLIP-375|https://cwiki.apache.org/confluence/x/64lEE]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 11 05:36:36 UTC 2024,,,,,,,,,,"0|z1l314:",9223372036854775807,"Since Flink 1.19, we support profiling the JobManager/TaskManager process interactively with [async-profiler](https://github.com/async-profiler/async-profiler) via the Flink Web UI, which allows users to create a profiling instance with arbitrary intervals and event modes, e.g ITIMER, CPU, Lock, Wall-Clock and Allocation.
Flink users can complete the profiling submission and result export via Flink Web UI conveniently.

For example, 
- First, users should find out the candidate TaskManager/JobManager with performance bottleneck for profiling, and switch to the corresponding TaskManager/JobManager page (profiler tab).
- Users can submit a profiling instance with a specified period and mode by simply clicking on the button `Create Profiling Instance`. (The description of the profiling mode will be shown when hovering over the corresponding mode.)
- Once the profiling instance is complete, the user can easily download the interactive HTML file by clicking on the link.

**More Information**
- [Documents](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/ops/debugging/profiler/)
- [FLIP-375: Built-in cross-platform powerful java profiler](https://cwiki.apache.org/confluence/x/64lEE)",,,,,,,,,,,,,,,,,,,"09/Jan/24 06:59;Zhanghao Chen;Looking forward to the feature~;;;","11/Jan/24 05:36;Yu Chen;Hi [~Zhanghao Chen] , thanks for your attention, the feature will be available to everyone soon!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add flink managed timeout mechanism for backend restore operation,FLINK-33324,13554900,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,dongwoo.kim,dongwoo.kim,20/Oct/23 08:38,27/Oct/23 15:48,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Checkpointing,Runtime / State Backends,,,,,0,,,,,,"Hello community, I would like to share an issue our team recently faced and propose a feature to mitigate similar problems in the future.
h2. Issue

Our Flink streaming job encountered consecutive checkpoint failures and subsequently attempted a restart. 
This failure occurred due to timeouts in two subtasks located within the same task manager. 
The restore operation for this particular task manager also got stuck, resulting in an ""initializing"" state lasting over an hour. 
Once we realized the hang during the restore operation, we terminated the task manager pod, resolving the issue.

!image-2023-10-20-15-16-53-324.png|width=683,height=604!

The sequence of events was as follows:

1. Checkpoint timed out for subtasks within the task manager, referred to as tm-32.
2. The Flink job failed and initiated a restart.
3. Restoration was successful for 282 subtasks, but got stuck for the 2 subtasks in tm-32.
4. While the Flink tasks weren't fully in running state, checkpointing was still being triggered, leading to consecutive checkpoint failures.
5. These checkpoint failures seemed to be ignored, and did not count to the execution.checkpointing.tolerable-failed-checkpoints configuration. 
     As a result, the job remained in the initialization phase for very long period.
6. Once we found this, we terminated the tm-32 pod, leading to a successful Flink job restart.
h2. Suggestion

I feel that, a Flink job remaining in the initializing state indefinitely is not ideal. 
To enhance resilience, I think it would be helpful if we could add timeout feature for restore operation. 
If the restore operation exceeds a specified duration, an exception should be thrown, causing the job to fail. 
This way, we can address restore-related issues similarly to how we handle checkpoint failures.
h2. Notes

Just to add, I've made a basic version of this feature to see if it works as expected. 
I've attached a picture from the Flink UI that shows the timeout exception happened during restore operation. 
It's just a start, but I hope it helps with our discussion. 
(I've simulated network chaos, using [litmus|https://litmuschaos.github.io/litmus/experiments/categories/pods/pod-network-latency/#destination-ips-and-destination-hosts] chaos engineering tool.)
!image-2023-10-20-17-42-11-504.png|width=940,height=317!

 

Thank you for considering my proposal. I'm looking forward to hear your thoughts. 
If there's agreement on this, I'd be happy to work on implementing this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 06:16;dongwoo.kim;image-2023-10-20-15-16-53-324.png;https://issues.apache.org/jira/secure/attachment/13063739/image-2023-10-20-15-16-53-324.png","20/Oct/23 08:42;dongwoo.kim;image-2023-10-20-17-42-11-504.png;https://issues.apache.org/jira/secure/attachment/13063740/image-2023-10-20-17-42-11-504.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 15:48:11 UTC 2023,,,,,,,,,,"0|z1l2yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 08:59;pnowojski;Could you share a code for the change, how complicated it is? 

This issue might be also handled via monitoring and alerts in whatever system you are using to monitor Flink's state/metrics. You could just very simply implement an alert if job is stuck in initialising for too long. That alert would require a manual action from a human being the same way, how you would respond to a job stuck in a failover loop (I would guess you already have an alert/monitoring for that). A narrow advantage of the timeout is that Flink could re-try restoring once again, if this is an intermittent issue caused by some silently broken connection.;;;","20/Oct/23 09:01;zakelly;Hi [~dongwoo.kim] ,

Thanks for reporting this! Before we think about this feature, may I ask have you investigated why those two subtasks got stuck? Are they making any progress? I'm asking because in our production environment, we sometimes encounter a slow restoration lasting over more than 30min, especially for jobs with big state or when there's a heavy load on the HDFS. But the job can eventually restore successfully. Thus a timeout and retry won't make this better. I suggest we figure out what happend on the stuck subtask and find out if there is a bug. WDYT?;;;","20/Oct/23 10:09;roman;Thanks for the proposal [~dongwoo.kim]

I have some concerns below.

 

First, simply retrying the recovery might not solve the issue. In your case, a TM needed to be replaced for recovery to succeed.

And killing a TM on which the recovery timed out might affect other jobs (btw we'll also lose local state on that TM, so the next recovery might be slower).

OTH, if the goal is to notify a human operator about some number of failed recovery attempts, then it can be achieved via external monitoring as [~pnowojski] said.

 

Second, in some cases like the one [~Zakelly] mentioned, this might actually worsen the situation by always timing out slow recovery; or timing out transactions in external systems like Kafka.;;;","20/Oct/23 13:09;dongwoo.kim;Hi, [~pnowojski] 

Thanks for the opinion. 
First about the code, I just simply wrapped the main logic code [here|https://github.com/apache/flink/blob/72e302310ba55bb5f35966ed448243aae36e193e/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/BackendRestorerProcedure.java#L94] with callable object and combined with future.get(timeout). Please consider that it was just initial check for feasibility without a deep dive into the Flink code.

When considering manual action from human, I agree solving this issue with alert system seem practical. 
However, our goal for handling the failover loop was to automate operations using the failure-rate restart strategy and a cronJob that monitors the Flink job's status. 
Instead of adding ambiguous conditions in the cronJob, treating an unusually long restore operation as a failure simplifies our process. 
Yet, I understand from the feedback that this approach might fit more to our team's unique needs and might not be as helpful for everyone else.;;;","20/Oct/23 13:11;dongwoo.kim;Hi, [~Zakelly] 



Thanks for sharing your opinion and experience.
Unfortunately we failed to discover the root cause. 
However, we noticed that during the problem, our hdfs wasn't under any unusual load, in fact, it was below its average. 
Additionally, our state size isn't particularly large.
All other tm completed the restore process, except one that got stuck for over an hour.
After removing that specific tm, everything went back to normal. 

Your feedback made me thinking enforcing timeout for restore operation might have hugh side effect. 
Perhaps I was too focused on our specific situation. 

What if we keep the default setting without a timeout, but allow developers the option to set one if needed? 
Depending on their specific needs and SLO, long hanging restore could be considered as failure. WDYT?;;;","20/Oct/23 13:13;dongwoo.kim;Hi, [~roman] 

Thanks for considering my suggestion.
For some background, we are leveraging failure-rate restart strategy and monitoring cronJob to manage flink application.


By marking long hanging restore operation as a failure retry can be initiated.

And if the retries don't resolve the issue, the job should ultimately fail.
This way, the cronJob monitoring the Flink application can quickly detect it and redeploy the job from its last state.

(In this scenario new task manager pod is created so this specific issue could be solved)

 

As you pointed out, introducing a timeout might have various side effects. 
Whether one can tolerate a lengthy restore or prefers quicker retries and redeployments might vary based on operational needs. 
What if we make this an optional feature? By default, there would be no timeout, but developers could configure it if desired.

Thanks in advance.;;;","23/Oct/23 12:37;roman;Thanks for clarifying [~dongwoo.kim]

it seems to me that the key part of your solution is the external cron job that resubmits the Flink job using a new set of pods.

 

Given that other users don't have such a job, I think it would make sense to implement slow recovery detection in cron job too.

And if it requires ""adding ambiguous conditions in the cronJob"" as you said earlier, we could rather improve Flink observability to make this detection easier.

That way, more users would benefit from the change.

 

WDYT?

Could you also describe why is it difficult to detect slow recovery externally?;;;","27/Oct/23 15:48;dongwoo.kim;Thanks for the opnion [~roman] , and sorry for the late reply. 
I've been thinking about ways to detect the slow restore issue in Flink, and here are a couple of methods I've considered

*1. Alerts Based on Checkpoint Failures.* 
Since flink tries to trigger checkpoint even though all the subtasks are not in running state it always fails.
While this indicates a possible issue, it doesn't specifically tell us if it's due to slow recovery

*2. Using the REST API*
Another approach could be to leveraging Flink's REST API to check the status of subtasks. I found that even if some subtasks are initializing state the *""/jobs/:jobid/status""* endpoint often shows them as {*}running{*}. This was unexpected. 
However, we can look at the *""/jobs/:jobid/vertices/:vertexid/taskmanagers""* endpoint to identify subtasks that are in the *initializing* state. 
A limitation here is that this endpoint just shows the current state and doesn't provide the duration for which a subtask has been in that state. 
So, some additional logic might be needed to track how long subtasks have been 'stuck'.

Considering the above, if we're looking to enhance Flink's observability, what about introducing a metric that shows restore time? 
Perhaps something named {*}lastDurationOfRestore{*}? WDYT?

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase fails with produced an uncaught exception in FatalExitExceptionHandler,FLINK-33323,13554894,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Wencong Liu,Sergey Nuyanzin,Sergey Nuyanzin,20/Oct/23 07:57,09/Nov/23 15:49,04/Jun/24 20:40,09/Nov/23 15:46,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Network,,,,,,0,pull-request-available,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53853&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=9166
fails with
{noformat}
01:15:38,516 [blocking-shuffle-io-thread-4] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'blocking-shuffle-io-thread-4' produced an uncaught exception. Stopping the process...
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4275bb45[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@488dd035[Wrapped task = org.apache.fl
ink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskIOScheduler$$Lambda$2561/0x0000000801a2f728@464a3754]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@22747816[Shutting down, pool size = 10, active threads = 9,
 queued tasks = 1, completed tasks = 1]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562) ~[?:?]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskIOScheduler.run(DiskIOScheduler.java:151) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskIOScheduler.lambda$triggerScheduling$0(DiskIOScheduler.java:308) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) [?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
        at java.lang.Thread.run(Thread.java:833) [?:?]

{noformat}

also logs are attached

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33502,,,,,,,,,,"20/Oct/23 07:58;Sergey Nuyanzin;mvn-3.zip;https://issues.apache.org/jira/secure/attachment/13063736/mvn-3.zip",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 15:46:36 UTC 2023,,,,,,,,,,"0|z1l2xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 07:59;Sergey Nuyanzin;it might be worth to note that the problem happened while testing against jdk 17;;;","20/Oct/23 08:00;Sergey Nuyanzin;[~tanyuxin],  [~Weijie Guo]  could you please have a look
I think it is related to FLINK-33185;;;","26/Oct/23 07:18;tanyuxin;[~Wencong Liu] Could you please take a look at this issue?;;;","30/Oct/23 07:55;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54155&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8642;;;","30/Oct/23 07:59;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54144&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8952;;;","02/Nov/23 10:02;Weijie Guo;master(1.19) via cb3a69c967a978bf8e8bbe287e15113740a2766a.;;;","09/Nov/23 09:36;mapohl;[~Wencong Liu] thanks for looking into the issue. I'm working on Github actions for Flink (FLINK-27075) and this issue popped up there: [https://github.com/XComp/flink/actions/runs/6789774296/job/18458197040#step:12:9221]

I verified that your change of [cb3a69c|https://github.com/apache/flink/commit/cb3a69c967a978bf8e8bbe287e15113740a2766a] was included in the build.

Additionally, (just as a reminder) please remember to resolve a Jira issue by not only adding the branch and commit in a comment but also update the {{{}Fix Version{}}}. :);;;","09/Nov/23 10:18;Wencong Liu;Thanks for your reminder! [~mapohl] . Could you please help or teach me get the complete log during the running phase of HybridShuffleITCase like `mvn-3.zip` file in this jira? I've taken a look at your issue and it seems that the phenomenon doesn't match the one described in this Jira. Therefore, I would require additional logs to further investigate. 😄;;;","09/Nov/23 15:46;mapohl;Oh, you're right. I got misguided by the ""FATAL"" error and ignored the rest of the logs. Sorry for that. I'm gonna create a separate issue (FLINK-33502) and will continue monitoring whether it's related to my GHA infra work or some test instability. I have to work on collecting the build artifacts properly still. Closing the issue again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProjectWatermarkAssignerTransposeRule support nested nested projection push down in transpose,FLINK-33322,13554874,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,lijingwei.5018,lijingwei.5018,lijingwei.5018,20/Oct/23 02:34,20/Oct/23 02:39,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"ProjectWatermarkAssignerTransposeRule support nested projection push down in transpose

!image-2023-10-20-10-33-55-649.png|width=757,height=183!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 02:33;lijingwei.5018;image-2023-10-20-10-33-55-649.png;https://issues.apache.org/jira/secure/attachment/13063729/image-2023-10-20-10-33-55-649.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-20 02:34:27.0,,,,,,,,,,"0|z1l2sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VertexFlameGraphFactoryTest#verifyRecursively fails on Java 21,FLINK-33321,13554831,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,19/Oct/23 18:42,24/Oct/23 19:27,04/Jun/24 20:40,20/Oct/23 07:34,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"With jdk 21 it fails as
{noformat}
java.lang.AssertionError: No lambdas encountered in the test, cleanup functionality was not tested

	at org.apache.flink.runtime.webmonitor.threadinfo.VertexFlameGraphFactoryTest.testLambdaClassNamesCleanUp(VertexFlameGraphFactoryTest.java:54)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 07:33:48 UTC 2023,,,,,,,,,,"0|z1l2jc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 07:33;Sergey Nuyanzin;Merged as [d00f031785867d465d4a32fd937fe2cbd6722ca3|https://github.com/apache/flink/commit/d00f031785867d465d4a32fd937fe2cbd6722ca3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Dynamic Logger Level Adjustment,FLINK-33320,13554794,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,RocMarshal,RocMarshal,RocMarshal,19/Oct/23 14:11,09/Nov/23 13:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / REST,,,,,,0,REST_API,runtime,,,,"During the process of routine program debugging or troubleshooting, analyzing system logs is a common approach. 

Comprehensive and detailed system logs contribute to improved visibility of internal system execution information and also enhance the efficiency of program debugging or issue troubleshooting.However, comprehensive and detailed log settings can lead to the following issues:
 # A sharp increase in log volume, accelerating disk occupancy.
 # Potential risks of system performance degradation due to a large volume of log printing.
 # The need to simplify log configuration subsequently.

     Therefore, introducing a mechanism to dynamically adjust the online log output level in the event of diagnosing online issues or debugging programs could be meaningful. 

This mechanism should ideally provide the following two basic capabilities:
 # Dynamically adjust log levels.
 # Query the current log levels of the JM/TM in the cluster.

 

The proposal doc:  https://docs.google.com/document/d/1s2XQzet_8oPhMs3WyDhP_pPhAE3d1Gdw_qR4W0nKtlY/edit?usp=sharing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 08:47:49 UTC 2023,,,,,,,,,,"0|z1l2b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 01:36;RocMarshal;Hi, [~fanrui] Thank you for the assign~ I'm willing to contribute to this ticket.

Although this change is relatively minor, I still want to confirm if there is any other discussion needed before starting the contribution. Such as discussion or FLIP. Thank you very much~;;;","20/Oct/23 01:52;fanrui;The FLIP is necessary due to some of new rest apis are introduced.;;;","20/Oct/23 08:47;martijnvisser;And please don't use a Google Doc for proposal, but create a FLIP page indeed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add AverageTime metric to measure delta change in GC time,FLINK-33319,13554777,13551327,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,ouyangwuli,gyfora,gyfora,19/Oct/23 12:06,14/Mar/24 07:36,04/Jun/24 20:40,,,,,,,,,,,,1.20.0,,,,Runtime / Metrics,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 10 12:04:01 UTC 2024,,,,,,,,,,"0|z1l27c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/24 02:20;ouyangwuli;[~gyfora]  Could you assign this ticket to me？;;;","10/Jan/24 12:04;gyfora;done :) ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create TimeMsPerSec metrics for each collector and add metrics across all collectors,FLINK-33318,13554775,13551327,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,gyfora,gyfora,gyfora,19/Oct/23 12:05,22/Oct/23 16:34,04/Jun/24 20:40,22/Oct/23 16:34,,,,,,,,,,,1.19.0,,,,Runtime / Metrics,,,,,,0,pull-request-available,,,,,"Add the new TimeMsPerSec GC metrics per collector.

Add a new `All` group and aggregate GC metrics across collectors",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 22 16:34:10 UTC 2023,,,,,,,,,,"0|z1l26w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/23 16:34;gyfora;merged to master 76dda5d3cf7c4ecf255370679b27e11dd974a293;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add cleaning mechanism for StreamConfig#SERIALIZEDUDF,FLINK-33317,13554735,13554724,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,19/Oct/23 08:53,07/Nov/23 09:41,04/Jun/24 20:40,07/Nov/23 09:41,1.17.0,1.18.0,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"See FLINK-33315 for details.

This Jira focus on avoid unnecessary memory usage, it can optimize the memory cost of Replica_3 in FLINK-33315.

 

Solution:
 * After getStreamOperatorFactory,  the StreamConfig#SERIALIZEDUDF can be cleaned

In general, we don't clean any configuration. However, the SERIALIZED_UDF may be very large when operator includes some large objects.

{@link #getStreamOperatorFactory}

is used to create a StreamOperator and usually only needs to be called once.

Callers can clean it to reduce the memory after calling the \{@link #getStreamOperatorFactory}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 09:40:54 UTC 2023,,,,,,,,,,"0|z1l1y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 09:40;fanrui;Merged master<1.19> via: c83792b0f3fb6ae18dfabd52a89dc42c074ee73c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid unnecessary heavy getStreamOperatorFactory,FLINK-33316,13554734,13554724,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,19/Oct/23 08:44,26/Oct/23 13:31,04/Jun/24 20:40,19/Oct/23 12:41,1.17.0,1.18.0,,,,,,,,,1.17.2,1.18.1,1.19.0,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"See FLINK-33315 for details.

This Jira focus on avoid unnecessary heavy getStreamOperatorFactory, it can optimize the memory and cpu cost of Replica_2 in FLINK-33315.

Solution: We can store the serializedUdfClassName at StreamConfig, and using the getStreamOperatorFactoryClassName instead of the heavy getStreamOperatorFactory in OperatorChain#getOperatorRecordsOutCounter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 02:57:00 UTC 2023,,,,,,,,,,"0|z1l1xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/23 12:41;lindong;Merged to the apache/flink master branch at a2681f6a85aaad21179f91e03a91b4a05158841e;;;","20/Oct/23 02:46;fanrui;Hi [~lindong] , this extra memory cost is introduced since 1.17, and it will affect the stability of large batch jobs. Do you think it's needed for 1.17 and 1.18?

 ;;;","20/Oct/23 03:46;lindong;[~fanrui] I think it makes sense to cherry-pick this to the next Flink 1.17/1.18 minor release. But I am not sure it is necessary to include this patch in the ongoing Flink 1.18.0 release at the risk of further delaying this release.

Note that this issue increases the memory usage during Flink job initialization for large batch jobs, which strictly speaking has limited impact because very few users currently use Flink for such scenarios. For those pioneer/advanced users (like your team), I think it is reasonable for them to patch their own internal copy of Flink repo to get this issue fixed.

Also note that when Flink is (close to) production ready for large batch jobs in the future, we should have stress/performance tests during release testing to catch issues like this. The fact that we don't have this also signals that Flink is not really production ready for large-scale batch jobs yet.

 ;;;","20/Oct/23 03:51;fanrui;Thanks for your explanation!

I will cherry-pick it to release-1.17 and 1.18 later, and don't block the release of 1.18.0.

 ;;;","23/Oct/23 02:57;fanrui;The change is subtle, so I push this commit directly.

Merged 1.19: a2681f6a85aaad21179f91e03a91b4a05158841e and 0388b760fc66975c70f797ad07f2e073738a7171

Merged 1.17: 024fa4776d0246a283a70743f1ce3c04981daeb9

Merged 1.18: 0dd3b4ce9f0b9f193926445bf9c1f8579fa86161

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize memory usage of large StreamOperator,FLINK-33315,13554724,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,19/Oct/23 08:28,20/Nov/23 06:21,04/Jun/24 20:40,20/Nov/23 06:21,1.17.0,1.18.0,,,,,,,,,1.19.0,,,,Runtime / Configuration,Runtime / Task,,,,,0,,,,,,"Some of our batch jobs are upgraded from flink-1.15 to flink-1.17, and TM always fail with java.lang.OutOfMemoryError: Java heap space.

 

Here is a example: a hive table with a lot of data, and the HiveSource#partitionBytes is 281MB.

After analysis, the root cause is that TM maintains the big object with 3 replicas:
 * Replica_1: SourceOperatorFactory (it's necessary for running task)
 * Replica_2: Temporarily generate the duplicate SourceOperatorFactory object.
 ** It's introduced in FLINK-30536 (1.17), it's not necessary. ([code link|https://github.com/apache/flink/blob/c2e14ff411e806f9ccf176c85eb8249b8ff12e56/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java#L646])
 ** When creating a successor operator to a SourceOperator, the call stack is:
 *** OperatorChain#createOperatorChain ->
 *** wrapOperatorIntoOutput ->
 *** getOperatorRecordsOutCounter ->
 *** operatorConfig.getStreamOperatorFactory(userCodeClassloader)
 ** It will generate the SourceOperatorFactory temporarily and just check whether it's SinkWriterOperatorFactory
 * Replica_3: The value of StreamConfig#{color:#9876aa}SERIALIZEDUDF {color}
 ** It is used to generate SourceOperatorFactory.
 ** Now the value is always maintained in heap memory.
 ** However, after generating we can release it or store it in the disk if needed.
 *** We can define a threshold, when the value size is less than threshold, the release strategy doesn't take effect.
 ** If so, we can save a lot of heap memory.

These three replicas use about 800MB of memory. Please note that this is just a subtask. Since each TM has 4 slots, it will run 4 HiveSources at the same time, so 12 replicas are maintained in the TM memory, it's about 3.3 GB.

These large objects in the JVM cannot be recycled, causing TM to frequently OOM.

This JIRA focus on optimizing Replica_2 and Replica_3.

 

!image-2023-10-19-16-28-16-077.png!

 

!https://f.haiserve.com/download/130f436613b52b321bd9bd0211dd109f0b0102000020e860f292a13c0702016976850466192b?userid=146850&token=4e7b7352b30d6e5d2dd2bb7a7479fc93!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 02:41;fanrui;130f436613b52b321bd9bd0211dd109f0b0102000020e860f292a13c0702016976850466192b.png;https://issues.apache.org/jira/secure/attachment/13063837/130f436613b52b321bd9bd0211dd109f0b0102000020e860f292a13c0702016976850466192b.png","19/Oct/23 08:28;fanrui;image-2023-10-19-16-28-16-077.png;https://issues.apache.org/jira/secure/attachment/13063680/image-2023-10-19-16-28-16-077.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 20 06:21:06 UTC 2023,,,,,,,,,,"0|z1l1vk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/23 09:02;fanrui;Thanks [~zhongwei] for the analysis together.:);;;","19/Oct/23 10:12;fanrui;Hi [~pnowojski] , you may be interested in this optimization. :);;;","20/Nov/23 06:21;fanrui;3 subtasks have been merged for some time, and after these improvement our Flink Batch job with large operator run very well, so I close this JIRA.

Many thanks to everyone who helped with the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the named parameter example in window tvf,FLINK-33314,13554717,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,19/Oct/23 07:55,23/Oct/23 08:58,04/Jun/24 20:40,23/Oct/23 08:58,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"CALCITE-4171 adds support for named params in window function, but the new param name OFFSET conflicts with the reserved keyword, so it can't be used directly, users must use it with double quotes(`OFFSET`) otherwise it will cause a validation error.  
In Flink SQL, we should fix the example in the documentation first (also add some tests). 
When the Calcite community's discussion on the renaming ( https://issues.apache.org/jira/browse/CALCITE-6060 ) has come to a conclusion, we can do the followup change in Flink.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 08:58:02 UTC 2023,,,,,,,,,,"0|z1l1u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 08:58;lincoln.86xy;Fixed in master: 3596a94fe415fdc834cec346cc3910dd775f5239;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RexNodeExtractor fails to extract conditions with binary literal,FLINK-33313,13554712,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,FrankZou,FrankZou,FrankZou,19/Oct/23 07:49,09/Dec/23 15:21,04/Jun/24 20:40,20/Oct/23 09:40,,,,,,,,,,,1.17.3,1.18.1,1.19.0,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"An exception will be thrown when we try to extract conditions with binary literal in RexNodeExtractor. Here is a test I add in `RexNodeExtractorTest` which could reproduce the case(Also add a field 'blob' with bytes type in RexNodeTestBase).
{code:java}
  @Test
  def testExtractConditionWithBinaryLiteral(): Unit = {
    // blob
    val t0 = rexBuilder.makeInputRef(allFieldTypes.get(5), 5)

    // X'616263'
    val t1 = rexBuilder.makeBinaryLiteral(ByteString.of(""616263"", 16))

    // blob = X'616263'
    val a = rexBuilder.makeCall(SqlStdOperatorTable.EQUALS, t0, t1)

    val relBuilder: RexBuilder = new FlinkRexBuilder(typeFactory)
    val (convertedExpressions, unconvertedRexNodes) =
      extractConjunctiveConditions(a, -1, allFieldNames, relBuilder, functionCatalog)

    val expected: Array[Expression] = Array($""blob"" === Array[Byte](97, 98, 99))
    assertExpressionArrayEquals(expected, convertedExpressions)
    assertEquals(0, unconvertedRexNodes.length)
  }
{code}
And here is the exception stack:
{code:java}
org.apache.flink.table.api.ValidationException: Data type 'BINARY(3) NOT NULL' with conversion class '[B' does not support a value literal of class 'org.apache.calcite.avatica.util.ByteString'.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:294)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:79)
	at org.apache.flink.table.expressions.ApiExpressionUtils.valueLiteral(ApiExpressionUtils.java:251)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:503)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitLiteral(RexNodeExtractor.scala:393)
	at org.apache.calcite.rex.RexLiteral.accept(RexLiteral.java:1217)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.$anonfun$visitCall$3(RexNodeExtractor.scala:509)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:509)
	at org.apache.flink.table.planner.plan.utils.RexNodeToExpressionConverter.visitCall(RexNodeExtractor.scala:393)
	at org.apache.calcite.rex.RexCall.accept(RexCall.java:189)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.$anonfun$extractConjunctiveConditions$2(RexNodeExtractor.scala:158)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:157)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractor$.extractConjunctiveConditions(RexNodeExtractor.scala:119)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.extractConjunctiveConditions(RexNodeExtractorTest.scala:785)
	at org.apache.flink.table.planner.plan.utils.RexNodeExtractorTest.testExtractConditionWithBinaryLiteral(RexNodeExtractorTest.scala:160)
{code}
IMO, we should cast `ByteString` to `byte[]` in RexNodeToExpressionConverter#visitLiteral",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Dec 09 15:21:49 UTC 2023,,,,,,,,,,"0|z1l1sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 09:39;libenchao;merged: master(1.19.0) : 4d37b8c34ff062b7505ab8c0ca8f2181768aab60
;;;","20/Oct/23 09:53;libenchao;[~FrankZou] Do this also affect 1.18 and 1.17, if yes, we also need to fix for these versions, could you help to prepare the corresponding PRs?;;;","23/Oct/23 02:34;FrankZou;[~libenchao] OK, I will do it.;;;","08/Dec/23 08:58;libenchao;1.18.1： 265d81741654ff70836485e8e8d31ce33a87e960;;;","09/Dec/23 15:21;libenchao;1.17.3: cfc7881ab5f1c27d7a9a7781b255fa94989201b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingFileWriterTest.testCommitFileWhenPartitionIsCommittableByPartitionTime fails on java 21,FLINK-33312,13554659,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 23:11,13/Dec/23 10:51,04/Jun/24 20:40,13/Dec/23 10:51,,,,,,,,,,,,,,,,,,,,,0,,,,,,"it fails as

{noformat}
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.222 s <<< FAILURE! - in org.apache.flink.connector.file.table.stream.StreamingFileWriterTest
[ERROR] org.apache.flink.connector.file.table.stream.StreamingFileWriterTest.testCommitFileWhenPartitionIsCommittableByPartitionTime  Time elapsed: 0.043 s  <<< FAILURE!
org.opentest4j.AssertionFailedError: 

Expecting value to be true but was false
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.connector.file.table.stream.StreamingFileWriterTest.testCommitFileWhenPartitionIsCommittableByPartitionTime(StreamingFileWriterTest.java:276)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 13 10:51:13 UTC 2023,,,,,,,,,,"0|z1l1h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/23 11:17;Sergey Nuyanzin;This looks likes a very floating one
I faced it only a couple of times, and it could happen it disappears with 21.0.1, will keep an eye on it
will close in case there will not be new cases;;;","13/Dec/23 10:51;Sergey Nuyanzin;No more new occurrences for almost 2 months

For that reason I'm closing this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSecurityManagerITCase fails on java 21,FLINK-33311,13554658,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 22:41,22/Nov/23 11:09,04/Jun/24 20:40,22/Nov/23 11:09,,,,,,,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,"There are 2 tests {{testForcedJVMExit}} and {{testIgnoredJVMExit}} failing like 
{noformat}

expected: 222
 but was: 1
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.runtime.util.FlinkSecurityManagerITCase.testForcedJVMExit(FlinkSecurityManagerITCase.java:51)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)

{noformat}

and
{noformat}

expected: 0
 but was: 1
	at java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)
	at org.apache.flink.runtime.util.FlinkSecurityManagerITCase.testIgnoredJVMExit(FlinkSecurityManagerITCase.java:65)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 22 11:09:40 UTC 2023,,,,,,,,,,"0|z1l1gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 11:38;chesnay;I assume these failed after FLINK-33309?;;;","20/Oct/23 23:19;Sergey Nuyanzin;yep, you're right;;;","03/Nov/23 17:42;Sergey Nuyanzin;It seems I found the root cause.
[~chesnay] I submitted a PR, would be great if you have a look.
Also below there is a description of the problem

In {{org.apache.flink.runtime.testutils.TestJvmProcess#startProcess}} there is such code snippet
{code:java}
String[] cmd =
        new String[] {
            javaCommandPath,
            ""-Dlog.level=DEBUG"",
            ""-Dlog4j.configurationFile=file:"" + log4jConfigFilePath,

            ""-Xms"" + jvmMemoryInMb + ""m"",
            ""-Xmx"" + jvmMemoryInMb + ""m"",

            ""-classpath"",
            getCurrentClasspath(),

            ""-XX:+IgnoreUnrecognizedVMOptions"",
            getEntryPointClassName()
        };

String[] jvmArgs = getJvmArgs();

if (jvmArgs != null && jvmArgs.length > 0) {
    cmd = ArrayUtils.addAll(cmd, jvmArgs);
}

final String moduleConfig = System.getProperty(""surefire.module.config"");
if (moduleConfig != null) {
    cmd = ArrayUtils.addAll(cmd, moduleConfig.split("" ""));
}
{code}
the problem with this code that {{surefire}} properties are after entry point classname...
Also probably {{jvmArgs}} is a very confusing name since in fact it is about args for main method and not for jvm...

;;;","22/Nov/23 11:09;chesnay;master: 905d5998093127b145c9aa7d32a1667c4b45e850;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala before 2.12.18 doesn't compile on Java 21,FLINK-33310,13554655,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 22:20,22/Oct/23 20:00,04/Jun/24 20:40,22/Oct/23 20:00,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"It fails with

{noformat}
[ERROR] error:
[INFO]   bad constant pool index: 0 at pos: 48445
[INFO]      while compiling: <no file>
[INFO]         during phase: globalPhase=<no phase>, enteringPhase=<some phase>
[INFO]      library version: version 2.12.15
[INFO]     compiler version: version 2.12.15
...
[INFO]   last tree to typer: EmptyTree
[INFO]        tree position: <unknown>
[INFO]             tree tpe: <notype>
[INFO]               symbol: null
[INFO]            call site: <none> in <none>
[INFO] 
[INFO] == Source file context for tree position ==

{noformat}

based on release notes 2.12.18 - the first 2.12.x supporting jdk21
https://github.com/scala/scala/releases/tag/v2.12.18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 22 20:00:16 UTC 2023,,,,,,,,,,"0|z1l1g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/23 20:00;Sergey Nuyanzin;Merged as [6cdfca2a6aba1a387b866d9358df9c1ee7f1c138|https://github.com/apache/flink/commit/6cdfca2a6aba1a387b866d9358df9c1ee7f1c138];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 java.security.manager is disallowed by default starting in java18,FLINK-33309,13554648,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 20:53,03/Nov/23 11:52,04/Jun/24 20:40,03/Nov/23 11:51,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The change was done within [https://openjdk.org/jeps/411]

{quote}
Starting in Java 18, the default value of java.security.manager will be disallow if not otherwise set via java -D.... As a result, applications and libraries that call System::setSecurityManager may fail due to an unexpected UnsupportedOperationException. In order for System::setSecurityManager to work as before, the end user will have to set java.security.manager to allow on the command line (java -Djava.security.manager=allow ...).
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 11:51:54 UTC 2023,,,,,,,,,,"0|z1l1eo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 11:51;chesnay;master: 7295c3bcf92c0e106d3e91c57b6492030c760a25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade lombok to 1.8.30,FLINK-33308,13554645,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 20:33,20/Oct/23 11:39,04/Jun/24 20:40,20/Oct/23 11:39,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"Current lombok version is incompatible with jdk21
this issue was solved within [https://github.com/projectlombok/lombok/issues/3393]

and fix is available since 1.8.30",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 11:39:50 UTC 2023,,,,,,,,,,"0|z1l1e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 11:39;chesnay;master: 9471f81a2fb53f5c8274bbbe99161d8f1b27bafc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable spotless on Java 21,FLINK-33307,13554643,13552124,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 20:27,20/Oct/23 11:40,04/Jun/24 20:40,20/Oct/23 11:40,,,,,,,,,,,1.19.0,,,,Build System,,,,,,0,pull-request-available,,,,,The current spotless version doesn't run on Java 21. An upgrade is at this time impossible because compatible versions no longer run on Java 8.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 11:40:44 UTC 2023,,,,,,,,,,"0|z1l1dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 11:40;chesnay;master: 72e302310ba55bb5f35966ed448243aae36e193e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use observed true processing rate when source metrics are incorrect,FLINK-33306,13554594,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,18/Oct/23 14:23,03/May/24 13:50,04/Jun/24 20:40,24/Oct/23 15:28,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"The aim is to address the cases when Flink incorrectly reports low busy time (high idleness) for sources that are in fact cannot keep up due to the slowness of the reader/fetchers. As the metrics cannot be generally fixed on the Flink - connector side we have to detect this and handle it when collecting the metrics.

The main symptom of this problem is overestimation of the true processing rate and not triggering scaling even if lag is building up as the autoscaler thinks it will be able to keep up.

To tackle this we differentiate two different methods of TPR measurement:
 # *Busy-time based TPR* (this is the current approach in the autoscaler) : computed from incoming records and busy time
 # *Observed TPR* : computed from incoming records and back pressure, measurable only when we assume full processing throughput (i.e during catch-up)

h3. Current behaviour

The operator currently always uses a busy-time based TPR calculation which is very flexible and allows for scaling up / down but is susceptible to overestimation due to the broken metrics.
h3. Suggested new behaviour

Instead of using the busy-time based TPR we detect when TPR is overestimated (busy-time too low) and switch to observed TPR.

To do this, whenever we there is lag for a source (during catchup, or lag-buildup) we measure both busy-time and observed TPR.

If the avg busy-time based TPR is off by a configured amount we switch to observed TPR for this source during metric evaluation.

*Why not use observed TPR all the time?*
Observed TPR can only be measured when we are catching up (during stabilization) or when cannot keep up. This makes it harder to scale down or to detect changes in source throughput over time (before lag starts to build up). Instead of using observed TPR we switch to it only when we detect a problem with the busy-time (this is a rare case overall), to hopefully get the best of both worlds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32127,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 24 15:28:58 UTC 2023,,,,,,,,,,"0|z1l12o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/23 15:28;gyfora;merged to main cc680e142bb8d52c4db215658ee7f4c4159a0fe4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to customize arg line for connector's surefire plugin,FLINK-33305,13554583,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 13:53,04/Dec/23 16:48,04/Jun/24 20:40,04/Dec/23 16:48,,,,,,,,,,,connector-parent-1.1.0,,,,Build System,Connectors / Parent,,,,,0,pull-request-available,,,,,"Currently in case there is a need to add {{add opens}} {{add exports}} it is impossible to do without complete rewriting of surefire configuration...

 

The idea is to apply same approach as for flink and it's modules to allow customization via {{surefire.module.config}} property",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 04 16:48:12 UTC 2023,,,,,,,,,,"0|z1l108:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/23 16:48;Sergey Nuyanzin;Merged as [ac09d18d874691f00629c90726458011034ea106|https://github.com/apache/flink-connector-shared-utils/commit/ac09d18d874691f00629c90726458011034ea106];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Atomicity of RowMutations would broken when Delete and Put on same columnFamily/column/row,FLINK-33304,13554582,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanjialiang,tanjialiang,tanjialiang,18/Oct/23 13:52,03/Nov/23 15:14,04/Jun/24 20:40,03/Nov/23 15:14,hbase-3.0.1,,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,,,0,pull-request-available,,,,,"Current we put the {{Mutation}} into {{BufferedMutator}} directly, when {{Mutation}} have a {{Delete}} followed by {{Put}} to same column family or columns or rows, only the {{Delete}} is happening while the {{Put}} is ignored so atomicity of {{Mutation}} is broken for such cases.

See https://issues.apache.org/jira/browse/HBASE-8626.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 03 15:14:01 UTC 2023,,,,,,,,,,"0|z1l100:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Oct/23 09:24;tanjialiang;Hi [~MartijnVisser], can you help me request a reviewer?;;;","03/Nov/23 15:14;martijnvisser;Fixed in apache/flink-connector-hbase

main: e0971c3888db03243b08e5684b7690150276ef2c 
v3.0: 3116aa4a2f6aa423cd8f03a634aefba4b6d373f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flink-avro] Union type not inheriting type conversions,FLINK-33303,13554568,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,leibovski@rogers.com,leibovski@rogers.com,18/Oct/23 12:49,20/Oct/23 10:58,04/Jun/24 20:40,20/Oct/23 10:58,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,pull-request-available,,,,,"Avro version bump is required to address [AVRO-3536|https://issues.apache.org/jira/browse/AVRO-3536]",,,,,,,,,,,,,,,,,,FLINK-33238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 10:58:53 UTC 2023,,,,,,,,,,"0|z1l0ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 10:58;martijnvisser;Fixed via FLINK-33238;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to customize jdk version for connectors,FLINK-33302,13554559,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 12:07,08/Dec/23 12:19,04/Jun/24 20:40,08/Dec/23 12:19,,,,,,,,,,,connector-parent-1.1.0,,,,Build System,Connectors / Parent,,,,,0,pull-request-available,,,,,This will allow to enable jdk 17 per connector once it's ready to compile and run with jdk17 for instance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 08 12:19:14 UTC 2023,,,,,,,,,,"0|z1l0uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Dec/23 12:19;Sergey Nuyanzin;Merged as [835ba9613a0f488172cf71a8fa4dfcd2ded529a5|https://github.com/apache/flink-connector-shared-utils/commit/835ba9613a0f488172cf71a8fa4dfcd2ded529a5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Java and Maven version checks in the bash script of Flink release process,FLINK-33301,13554551,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,jingge,jingge,jingge,18/Oct/23 10:42,26/Nov/23 09:19,04/Jun/24 20:40,26/Nov/23 09:19,1.18.0,1.19.0,,,,,,,,,,,,,Release System,,,,,,0,pull-request-available,,,,,"During the release, Flink requires specific version of Java and Maven[1]. It makes sense to check those versions at the very beginning of some bash scripts to let it fail fast and therefore improve the efficiency.

 

[1][https://lists.apache.org/thread/fbdl2w6wjmwk55y94ml91bpnhmh4rnm0|https://lists.apache.org/thread/fbdl2w6wjmwk55y94ml91bpnhmh4rnm0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33291,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 24 15:40:09 UTC 2023,,,,,,,,,,"0|z1l0t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 10:59;mapohl;Isn't this only required when building the artifacts? The corresponding checks will be executed right at the start (as long as {[-Prelease}}) is specified. That would make the release manager aware of the fact that he/she has to use {{./mvnw}} and the specific Java version. Changing the Java version would fix the issue and the release manager could proceed. WDYT? ;;;","18/Oct/23 11:01;mapohl;On another note: Shouldn't the Fix Version only be specified if a fix was provided? Or what's the reason why you added 1.19.0 as a {{Fix Version}}?;;;","18/Oct/23 12:39;jingge;The document said -Prelease was one optional step. For folks who did the release will skip the preparation and just jump to the bash scripts.

For the fix version, I have a fix on my local machine and want to create a pr soon. THB, I am not sure if it is a feasible idea and happen to invite you to review it.;;;","18/Oct/23 13:23;mapohl;{{-Prelease}} is also activated when building the binaries. You can find the maven command in [apache/flink:tools/releasing/create_binary_release.sh:74|https://github.com/apache/flink/blob/50cb4ee8c545cd38d0efee014939df91c2c9c65f/tools/releasing/create_binary_release.sh#L74]. That should be enough to verify the check, right? WDYT?;;;","19/Oct/23 07:59;jingge;Afaik, create_source_release.sh (and better deploy_staging_jars.sh, since people might run it individually) should have Java and maven versions checks too, according to https://lists.apache.org/thread/fv9hqvdwz3qbybh4cng6hg9l8r8jofy9;;;","20/Oct/23 14:41;mapohl;[apache/flink:tools/releasing/deploy_staging_jars.sh:44|https://github.com/apache/flink/blob/d80d4876e3e4f1ff5a9b63082df55c82db04ebca/tools/releasing/deploy_staging_jars.sh#L44] does have the {{release}} profile enabled. Why do we need this for {{tools/releasing/create_source_release.sh}}? It doesn't build the artifacts from the sources as far as I can see?

-for the record: the link to the ML thread doesn't work for me. Therefore, I cannot follow what thread you're referring to.-

What was the purpose of referencing the 1.18.0-rc2 ML thread? I don't find any information about what scripts are affected in there.;;;","23/Oct/23 09:13;jingge;There are some pre-built jars. Let me copy the reply done by [~yunta] from [https://lists.apache.org/thread/fv9hqvdwz3qbybh4cng6hg9l8r8jofy9]:

""I checked the new pre-built binary release, and it works fine now. However, I then checked the pre-built jar packages and found they are also built with JDK17, e.g., flink-runtime.jar [1]. I think we also need to update all artifacts to be deployed. [1] [https://repository.apache.org/content/repositories/orgapacheflink-1658/org/apache/flink/flink-runtime/1.18.0/flink-runtime-1.18.0.jar] Best, Yun Tang""

 

 ;;;","23/Oct/23 14:33;mapohl;Aren't those the ones that were created and deployed with {{tools/releasing/deploy_staging_jars.sh}}?;;;","24/Oct/23 12:55;jingge;You are right. I didn't read through all the source code, but from the naming used, I am expecting deploy_staging_jars.sh only do deploy, just like the name tells us, i.e. SRP with no hiding magics. The jars should be built before that script.;;;","24/Oct/23 13:04;jingge;Commonly speaking, as a release manager, I'd like to literally know at very beginning that I am using the right version of Java or Maven or not before I do anything wrt the release. Therefore it would be good to fail at the first script I call instead of in the middle of the release process. Because if it happens, I have to make deep dive to understand everything about how the release process is implemented to check if I have to repeat any previous steps after I fixed Java and Maven version issues.;;;","24/Oct/23 13:24;mapohl;I see where you're coming from. I feel like the check has a reasonable output to let the release manager immediately know what's wrong. There are no consequences for the release because the release manager could just adapt his/her configuration accordingly based on the error message and keep going (i.e. no need to go through code to understand what's the issue imho). But you could add the following call earlier on in the release documentation if you want to still do it:
{code}
$ mvn -q -Prelease -pl flink-annotations validate
{code}

It will generate the following output (if the wrong Maven and Java version is used):
{code}[ERROR] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion failed with message:
Detected Maven Version: 3.6.3 is not in the allowed range [3.8.6,3.8.6].
[ERROR] Rule 1: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:
Detected JDK Version: 11.0.20 is not in the allowed range [1.8.0,1.8.1).
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:3.1.0:enforce (enforce-maven) on project flink-annotations: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}

I am hesitant to do that because it adds one more location for documentation that doesn't bring much value to the process. But I understand that it's less distracting for the release manager if the process works flawlessly without interruptions.;;;","24/Oct/23 14:00;jingge;Thanks for sharing your thoughts. I am trying to share my experience and feeling that most release managers will be facing the same issues. Following are my thoughts:
 # Yes, using pom to control the versions centrally is a good idea. But the current Flink release is not pure maven based, it contains many scripts beyond Maven. If someone like me has wrong versions of Java or Maven, it can only be failed at the time when maven -Prelease is called.
 # During Flink 1.18 release I ran the release process 4 times and I have other works in between. Since Flink is an open source project, I guess what I was facing is very common, release managers will have other works in parallel. Java and Maven versions will be switched back and forth depending on which project the release manager is working on. Each time, when I switched back to Flink release, I would not always call mvn -Prelease, because I already did the preparation. I just jumped into the ""Build a release candidate"" section.  All scripts described on the wiki page will be executed. And the fact is that I created my own protocol/program to execute the release in order to move faster without read the long content. At that time, my only focus was those scripts. I had the version issues twice more in addition to the one detected by rc2. As a release manager, I am keen to have the check at script level, because it is the entry point of my work. I might have my own script on top of the official scripts to save even more time. Like I did with the PR to have the following error messages(as example) at very beginning with any script I might call:

{code:java}
Java version is incorrect. Required version: 1.8, but it is 17.0.8

Maven version is incorrect. Required version: 3.8.6, but it is 3.9.1{code}
 Since the output is controlled by us, we can tell release managers anything we want to help them work more efficiently.

This is my personal experience, I want to save all future release managers' time I could have saved for myself. I am not sure if Maven could help me the way I required.

 

 ;;;","24/Oct/23 14:52;mapohl;{quote}
Yes, using pom to control the versions centrally is a good idea. But the current Flink release is not pure maven based, it contains many scripts beyond Maven. If someone like me has wrong versions of Java or Maven, it can only be failed at the time when maven -Prelease is called.
{quote}
That is why we have the script. So that you don't have to call {{mvn -Prelease}} on your own.

{quote}
During Flink 1.18 release I ran the release process 4 times and I have other works in between. Since Flink is an open source project, I guess what I was facing is very common, release managers will have other works in parallel. Java and Maven versions will be switched back and forth depending on which project the release manager is working on. Each time, when I switched back to Flink release, I would not always call mvn -Prelease, because I already did the preparation. I just jumped into the ""Build a release candidate"" section.  All scripts described on the wiki page will be executed. And the fact is that I created my own protocol/program to execute the release in order to move faster without read the long content. At that time, my only focus was those scripts. I had the version issues twice more in addition to the one detected by rc2. As a release manager, I am keen to have the check at script level, because it is the entry point of my work. I might have my own script on top of the official scripts to save even more time. Like I did with the PR to have the following error messages(as example) at very beginning with any script I might call:
{quote}

Just for the record: the only value you're adding is that the error is raised when calling {{tools/releasing/create_source_release.sh}}. The other two script do print the error already (because they call Maven's enforcement plugin). Additionally, you're introducing a helper script that can be used if you're not familiar with the Maven features for calling the version enforcement correctly. 

In contrast to that, you're adding one additional artifact where we have to maintain the Maven and Java version. You could adapt your script in a way that it uses the Maven command, I shared in my comment above. This way, we don't introduce new code locations with the version being hard-coded but rely on the Maven configuration. That keeps maintainability at the current level.

You could even grep for the relevant output if you're concerned about the verbose ERROR output: {{mvn -q -Prelease -pl flink-annotations validate | grep -v ERROR}}. WDYT?;;;","24/Oct/23 15:40;jingge;The value(my intention) was to fail faster before maven is called, like I mentioned, the entry point is script not maven. IMHO, anything we built to improve the efficiency will need to be maintained. That is the cost. The key point is how often the built thing will be used vs. how often it need to be maintained. Afaik, the versions of Java or maven might need to be changed/maintained once every 2 years or even longer. 

 

But, I got your point. Your thoughts are rational too. Let's stay with your solution and we could still come back after we got more feeling/feedback about it. WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update testcontainers dependency to v1.19.1,FLINK-33300,13554525,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,18/Oct/23 08:30,05/Nov/23 14:53,04/Jun/24 20:40,05/Nov/23 14:53,,,,,,,,,,,1.19.0,,,,Test Infrastructure,,,,,,0,pull-request-available,,,,,"One of the main features is life simplification for SELinux users (I'm one of those)
https://github.com/testcontainers/testcontainers-java/pull/6294
https://github.com/testcontainers/testcontainers-java/pull/7187

Also more release notes are at
https://github.com/testcontainers/testcontainers-java/releases/tag/1.19.1
https://github.com/testcontainers/testcontainers-java/releases/tag/1.19.0
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Nov 05 14:53:20 UTC 2023,,,,,,,,,,"0|z1l0nc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Nov/23 14:53;Sergey Nuyanzin;Merged as https://github.com/apache/flink/commit/fccf26bb449b90b617c6523db17eab98a544c0a7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Add checks to guarantee the non-deprecated options not conflicting with standard YAML,FLINK-33299,13554490,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,,JunRuiLi,JunRuiLi,18/Oct/23 03:14,18/Oct/23 03:18,04/Jun/24 20:40,18/Oct/23 03:15,,,,,,,,,,,,,,,Runtime / Configuration,Tests,,,,,0,,,,,,"To support the standard YAML parser, we add suffixes to all necessary options in FLINK-29372. However, this cannot guarantee that newly added options would still obey this rule. Thus, we should add test checks to guarantee this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 03:18:33 UTC 2023,,,,,,,,,,"0|z1l0fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 03:18;JunRuiLi;I'm sorry for mistakenly creating an invalid clone task. I have closed this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Add a suffix to keys that violate YAML spec,FLINK-33298,13554489,13554488,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,chesnay,JunRuiLi,JunRuiLi,18/Oct/23 03:14,18/Oct/23 03:18,04/Jun/24 20:40,18/Oct/23 03:15,,,,,,,,,,,,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"We have a few options where the key is a prefix of other options (e.g., {{high-availability}} and {{high-availability.cluster-id}}.

Add a suffix to these options and keep the old key as deprecated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 03:18:18 UTC 2023,,,,,,,,,,"0|z1l0fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 03:18;JunRuiLi;I'm sorry for mistakenly creating an invalid clone task. I have closed this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-366: Support standard YAML for FLINK configuration,FLINK-33297,13554488,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,18/Oct/23 03:14,05/Feb/24 02:31,04/Jun/24 20:40,05/Feb/24 02:31,,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,2.0-related,pull-request-available,,,,Support standard YAML for FLINK configuration,,,,,,,,,,,FLINK-34114,,,,FLINK-33677,,,,,,,,,,,,,,,FLINK-34116,,,,FLINK-23620,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Jan 27 03:59:24 UTC 2024,,,,,,,,,,"0|z1l0f4:",9223372036854775807,"Starting with Flink-1.19, Flink has officially introduced full support for the standard YAML 1.2 syntax (FLIP-366). The default configuration file has been changed to `config.yaml` and placed in the `conf/` directory. Users should directly modify this file to configure Flink.

If users want to use the legacy configuration file `flink-conf.yaml`, users just need to copy this file into the `conf/` directory. Once the legacy configuration file `flink-conf.yaml` is detected, Flink will prioritize using it as the configuration file.
And in the upcoming Flink 2.0, the `flink-conf.yaml` configuration file will no longer work.

More details could be found at Flink website (https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#flink-configuration-file)",,,,,,,,,,,,,,,,,,,"26/Jan/24 13:20;lincoln.86xy;[~JunRuiLi] Is all the work on FLIP-366 done? If so, we can close the jira.;;;","26/Jan/24 16:01;JunRuiLi;[~lincoln.86xy] Not yet, sorry for the misleading information. I missed creating the JIRA for the document this FLIP.  have now added that. Once the documentation is completed, I will close this JIRA.;;;","27/Jan/24 03:59;lincoln.86xy;[~JunRuiLi] Thank you for the update.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Split flink-dist.jar into independent dependency packages,FLINK-33296,13554485,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Won't Fix,,hejiefang,hejiefang,18/Oct/23 02:08,18/Oct/23 06:43,04/Jun/24 20:40,18/Oct/23 06:43,,,,,,,,,,,,,,,,,,,,,0,,,,,,"Flink currently uses the shade package of flink-dist. Can it be split into independent dependency packages？

To facilitate the replacement of dependencies individually",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 06:43:53 UTC 2023,,,,,,,,,,"0|z1l0eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 06:43;martijnvisser;[~hejiefang] No, because it causes classpath issues with common user provided dependencies like Guava, Jackson etc. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Separate SinkV2 and SinkV1Adapter tests,FLINK-33295,13554434,13426150,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,pvary,pvary,pvary,17/Oct/23 15:06,10/Nov/23 08:09,04/Jun/24 20:40,08/Nov/23 16:57,,,,,,,,,,,1.19.0,,,,API / DataStream,Connectors / Common,,,,,0,pull-request-available,,,,,"Current SinkV2 tests are based on the sink generated by the _org.apache.flink.streaming.runtime.operators.sink.TestSink_ test class. This test class does not generate the SinkV2 directly, but generates a SinkV1 and wraps in with a _org.apache.flink.streaming.api.transformations.SinkV1Adapter._ While this tests the SinkV2, but only as it is aligned with SinkV1, and the SinkV1Adapter.

We should have tests where we create a SinkV2 directly and the functionality is tested without the adapter.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33493,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:09:00 UTC 2023,,,,,,,,,,"0|z1l034:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/23 16:57;mbalassi;[{{92951a0}}|https://github.com/apache/flink/commit/92951a05127f1e0e2ab0ea04ae022659fc5276ab] in master;;;","09/Nov/23 13:56;martijnvisser;[~pvary] [~mbalassi] Just as an FYI, this does break some connectors at this moment for 1.19 compatibility, see FLINK-33493. Since it's an Internal class, should this be made public or did we miss something along the way?;;;","10/Nov/23 06:21;pvary;_InternalSinkWriterMetricGroup_ is an initial class, so in theory connectors should not use it.
 * How much effort would it be to enable the annotation check for the connectors?
 * We can expose the _MetricsGroupTestUtils_ in a test jar, if we see that the connectors would like use it for testing.

Thanks for the heads-up!

 

Peter;;;","10/Nov/23 08:09;martijnvisser;It's used for testing only, see https://github.com/apache/flink-connector-elasticsearch/pull/81 which fixed it for Elasticsearch. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink minicluster is shut down after every job completion,FLINK-33294,13554431,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mfatihaktas,mfatihaktas,17/Oct/23 14:46,18/Oct/23 06:45,04/Jun/24 20:40,,,,,,,,,,,,,,,,Test Infrastructure,,,,,,0,,,,,,"*What*

A new PyFlink minicluster is created for each job. This significantly slows down execution.

We tried to configure the minicluster to prevent it from shutting down after every job completion, as shown [here|https://github.com/mfatihaktas/flink-playground/blob/main/tests/test_pyflink_mini_cluster.py#L17]. This, however, did not seem to make any change.

*Why*

Our project relies on testing many PyFlink jobs on Github Actions. This issue is causing our testing process to take an unfeasibly long time.

*How to reproduce*
 * Clone [https://github.com/mfatihaktas/flink-playground]
 * {{make env; make install}} (dependencies: direnv, make)
 * Run {{run_tests.sh}}
 * Find the directory in which PyFlink writes the logs
 * 
{code:python}
python -c ""import pyflink;import os;print(os.path.dirname(os.path.abspath(pyflink.__file__))+'/log')"" {code}

 * Search in `flink-***.log` for “minicluster shutting down/created”. An example log is attached here for convenience: [^pyflink.log] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/23 14:43;mfatihaktas;pyflink.log;https://issues.apache.org/jira/secure/attachment/13063610/pyflink.log",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 06:45:19 UTC 2023,,,,,,,,,,"0|z1l02g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 06:45;martijnvisser;[~mfatihaktas] I've lowered the priority for this ticket given that this is not a blocker for the Flink project. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data loss with Kafka Sink,FLINK-33293,13554415,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jredzepovic,jredzepovic,17/Oct/23 13:16,16/Nov/23 17:38,04/Jun/24 20:40,,1.16.1,1.16.2,1.17.1,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"More info in Slack discussion: [https://www.linen.dev/s/apache-flink/t/15851877/hi-all-it-s-me-again-based-on-https-apache-flink-slack-com-a#e102fa98-dcd7-40e8-a2c4-f7b4a83234e1]

 

*TLDR:*

(in Flink version 1.15 I was unable to reproduce the issue, but in 1.16 and 1.17 I can reproduce it)

I have created a sink topic with 8 partitions, a replication factor of 3, and a minimum in-sync replicas of 2. The consumer properties are set to their default values.

For the producer, I made changes to the delivery.timeout.ms and request.timeout.ms properties, setting them to *5000ms* and *4000ms* respectively. (acks are set to -1 by default, which is equals to _all_ I guess)

KafkaSink is configured with an AT_LEAST_ONCE delivery guarantee. The job parallelism is set to 1 and the checkpointing interval is set to 2000ms. I started a Flink Job and monitored its logs. Additionally, I was consuming the __consumer_offsets topic in parallel to track when offsets are committed for my consumer group.

The problematic part occurs during checkpoint 5. Its duration was 5009ms, which exceeds the delivery timeout for Kafka (5000ms). Although it was marked as completed, I believe that the output buffer of KafkaSink was not fully acknowledged by Kafka. As a result, Flink proceeded to trigger checkpoint 6 but immediately encountered a Kafka {_}TimeoutException: Expiring N records{_}.

I suspect that this exception originated from checkpoint 5 and that checkpoint 5 should not have been considered successful. The job then failed but recovered from checkpoint 5. Some time after checkpoint 7, consumer offsets were committed to Kafka, and this process repeated once more at checkpoint 9.

Since the offsets of checkpoint 5 were committed to Kafka, but the output buffer was only partially delivered, there has been data loss. I confirmed this when sinking the topic to the database.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/23 13:19;jredzepovic;job.log;https://issues.apache.org/jira/secure/attachment/13063609/job.log","19/Oct/23 15:04;jredzepovic;job_1_16_2_run1.log;https://issues.apache.org/jira/secure/attachment/13063710/job_1_16_2_run1.log","19/Oct/23 15:04;jredzepovic;job_1_16_2_run2.log;https://issues.apache.org/jira/secure/attachment/13063712/job_1_16_2_run2.log","19/Oct/23 15:04;jredzepovic;job_1_17_1_run1.log;https://issues.apache.org/jira/secure/attachment/13063709/job_1_17_1_run1.log","19/Oct/23 15:04;jredzepovic;job_1_17_1_run2.log;https://issues.apache.org/jira/secure/attachment/13063711/job_1_17_1_run2.log",,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Thu Nov 16 17:38:59 UTC 2023,,,,,,,,,,"0|z1kzyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 13:23;jredzepovic;[~martijnvisser] fyi;;;","17/Oct/23 19:20;tzulitai;[~jredzepovic] was this actually reproduced with versions 1.16.2 and 1.17.1? I see the ticket tagged with these versions, but in the linked Slack thread it seems like it was only reproduced with version 1.16.1.

I'm asking because, at a first glance, checkpoints incorrectly succeeding in the case of flush failures seems to be caused by FLINK-31305, which is fixed via Flink Kafka connector versions 1.16.2, 1.17.1, and 3.0.0-1.17 (the externalized Kafka connector).

Prior to FLINK-31305, the KafkaSink was not correctly checking that the flush was fully successful before acknowledging checkpoint complete.

If you did not test against those versions, could you try that and report back?;;;","18/Oct/23 11:17;jredzepovic;Hi [~tzulitai] , thank you for the quick response. :)

These are the Flink versions I've tested the issue against:
*1.15.4* worked today without data loss (/)
*1.16.1* experienced data loss today (x) 
*1.16.2* I couldn't reproduce it today, but I'm pretty sure I did before ~3weeks, today it worked without data loss (!)
*1.17.1* experienced data loss today (x);;;","18/Oct/23 18:43;mason6345;Hi, this case should be handled by FLINK-31305 as [~tzulitai] mentioned. On delivery timeout, the flush should fail and restart from the previous checkpoint. I have confirmed that this is correct from your logs (KafkaSink threw exception, failed the checkpoint, and subsequently restarted the job). 

 

However, there is something suspicious with the restart, unrelated to the KafkaSink.

```

21:46:12,793 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - Restoring job f543de153e861f822176132218942c8d from Checkpoint 5 @ 1695671165746 for f543de153e861f822176132218942c8d located at <checkpoint-not-externally-addressable>. 21:46:12,817 INFO org.apache.flink.runtime.checkpoint.CheckpointCoordinator [] - No master state to restore

```

Is it possible that the job did not recover from checkpoint on the restart? That would violate at least once semantics.;;;","19/Oct/23 09:28;jredzepovic;Forgot to mention that attached log is from running the job with version 1.16.1. (sry :)) I will generate new log file for the version 1.17.1 now.

Regarding job not recovering from checkpoint, I think that wouldn't cause data loss, because job is stateless ETL and I guess checkpoint is just storing source offsets for Kafka topic. Not recovering the offsets from checkpoint would make the job continue from the committed offsets from Kafka which would introduce duplication, but I'm fine with that.;;;","19/Oct/23 15:11;jredzepovic;I managed to reproduce data loss using 1.16.2 and 1.17.1 versions. Log files are in the attachment of this ticket.

The test I did is the same as described above. When testing, the _first run_ would stuck in the KafkaException loop, after which I stopped the job and run again ({_}second run{_}) from the offsets committed in Kafka. After 2 runs, all data from the topic was processed, but when examining processed data in database I detected data loss.;;;","30/Oct/23 10:25;martijnvisser;[~jredzepovic] Did you test 3.0.0-1.17?;;;","03/Nov/23 15:12;jredzepovic;Not before, but did that test now.

*flinkVersion: 1.17.0*

connectorVersion: 1.17.0 and 3.0.0-1.17 (did 2 tests, one for each connector version, just to be sure :D)

 

{*}Results for connectorVersion 1.17.0{*}: It only required one job run to process all messages from Kafka topic. There were some Kafka TimeoutExceptions, but no restart was required - job managed to recover from it and managed to write checkpoints successfully. After all messages were processed, consumer lag on topic was 0, but after examining processed data in database I detected data loss.

 

{*}Results for connectorVersion 3.0.0-1.17{*}: Did the same test as on 19/Oct with 2 job runs. All messages were processed, consumer lag on topic was 0, but after examining processed data in database I detected data loss.;;;","03/Nov/23 15:17;martijnvisser;[~tzulitai] WDYT?;;;","16/Nov/23 17:38;tzulitai;We have a new ticket whose description seems to break my understanding / assumption of how flushing has been working in the KafkaWriter: https://issues.apache.org/jira/browse/FLINK-33545

If what that ticket describes is indeed true, that would also answer the mystery here ...
i.e. in at-least-once mode, flushing the producer doesn't actually flush everything + ensure all records are successfully written before the checkpoint barrier is processed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenSearch connector release,FLINK-33292,13554412,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Information Provided,,rkth,rkth,17/Oct/23 12:49,17/Oct/23 13:02,04/Jun/24 20:40,17/Oct/23 13:02,,,,,,,,,,,,,,,Connectors / Opensearch,,,,,,0,,,,,,"We had a request for supporting OAuth in the OpenSearch connector earlier this year, and the problem was resolved about 4 months ago right after the last release, but there is no new release that we can use. We have forked the repository and created our own package that we use to hopefully have a plug-and-play solution when the next release is out. Do you have any plans on when the next release will be out?

Here is our last issue https://issues.apache.org/jira/browse/FLINK-31856?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 17 13:02:59 UTC 2023,,,,,,,,,,"0|z1kzy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 13:02;martijnvisser;[~rkth] Thanks for opening the ticket, but that's not the purpose of Jira. Questions like these should be asked on the User or Dev mailing list.

I'm expecting that we'll do a new connector release with Flink 1.18 coming up, so in the next couple of weeks. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The release profile of Flink does include enforcing the Java version only in a ""soft"" way",FLINK-33291,13554410,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,17/Oct/23 12:36,20/Oct/23 10:29,04/Jun/24 20:40,20/Oct/23 10:27,1.16.2,1.17.1,1.18.0,1.19.0,,,,,,,1.16.3,1.17.2,1.18.0,1.19.0,Build System,,,,,,0,pull-request-available,,,,,"The vote on 1.18.0-rc2 [revealed|https://lists.apache.org/thread/fbdl2w6wjmwk55y94ml91bpnhmh4rnm0] that we don't do proper JDK version enforcement in {{release}} profile of the Maven build.

This is added to ensure that Java 8 is used when releasing the artifacts. The current configuration sets the version as a ""soft"" requirement (see [documentation on version|https://cwiki.apache.org/confluence/display/MAVENOLD/Dependency+Mediation+and+Conflict+Resolution#DependencyMediationandConflictResolution-DependencyVersionRanges] which is referenced in the [Maven Enforcer Plugin website|https://maven.apache.org/enforcer/maven-enforcer-plugin/]).

We should use square brackets to enforce the JDK version. The same can be applied to the Maven version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33301,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 10:27:57 UTC 2023,,,,,,,,,,"0|z1kzxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 10:27;mapohl;master: [36caa02bf73bb4e1644451f71ca07860360ec0d8|https://github.com/apache/flink/commit/36caa02bf73bb4e1644451f71ca07860360ec0d8]
1.18: [2f66dd4cfa5c7d0eb0f2ff0f03977034e8d0df75|https://github.com/apache/flink/commit/2f66dd4cfa5c7d0eb0f2ff0f03977034e8d0df75]
1.17: [8e33eb6d5e23fcbf35bb347061f72e770641a3f7|https://github.com/apache/flink/commit/8e33eb6d5e23fcbf35bb347061f72e770641a3f7]
1.16: [a33d5326bd86562975426724a1ff76f3193a24a1|https://github.com/apache/flink/commit/a33d5326bd86562975426724a1ff76f3193a24a1];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom counters to capture encoding/decoding failure in flink,FLINK-33290,13554389,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,chiggi_dev,chiggi_dev,17/Oct/23 10:06,17/Oct/23 10:06,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Metrics,,,,,,0,,,,,,"I need to get the difference between records that are collected by the source and the records that are emitted. (To capture deserialization failure)

Similarly, the difference between the records that have been received by sink and the records sent out of the sink. (To capture serialization failures)


For e.g. - If deserialization fails while reading records from kafka, in that case, I want to expose the difference between records collected from Kafka Broker and records emitted from Kafka operator after deserialization as a metric.

But I think flink does not provide any such metrics.

In Kafka Source I can have a workaround to get this metric:

I can override the open method from KafkaRecordDeserializationSchema where a metric can be added to show decoding failures:

    @Override
    public void open(DeserializationSchema.InitializationContext context) throws Exception {
    context.getMetricGroup().gauge(""decodingFailures"", new Gauge<Integer>()
        {
            @Override
            public Integer getValue()
            {
                return decodingFailures;
            }
        });
    }

and at the time of deserialization I can increment that counter as below:

    @Override
    public void deserialize(ConsumerRecord<byte[], byte[]> record, Collector<T> out)
    {
        try
        {
            //deserialize
        }
        catch (IOException | MMException e)
        {
            logger.error(String.format(""Error received while decoding, in partition [%d] for topic [%s] at offset [%d]: %s"",            
                partition, topic, offset, e.toString()));
           
            decodingFailures++;
}

*But there is no such way to implement this in FileSource, as SimpleStreamFormat/Reader does not provide access to Context in any way.*

*Similarly, I did not find any way to expose serialization related metrics in any of the sinks as well.*

Would it be possible to provide a way to implement custom counters to count serialization/deserialization failures in all Flink connectors (sinks & sources)?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-17 10:06:49.0,,,,,,,,,,"0|z1kzt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Gateway supports IPv6 addresses.,FLINK-33289,13554383,13548014,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsdy,guoyangze,guoyangze,17/Oct/23 09:30,18/Oct/23 07:51,04/Jun/24 20:40,18/Oct/23 07:51,1.18.0,,,,,,,,,,1.19.0,,,,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 07:51:17 UTC 2023,,,,,,,,,,"0|z1kzrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 07:51;guoyangze;master: 1182d15ec0d1047375f007ead26c09e86425b02c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Empty directory residue with appid name in HA(highly-available) related directory of hdfs, not cleaned",FLINK-33288,13554353,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,xinchen147,xinchen147,17/Oct/23 07:00,18/Oct/23 01:13,04/Jun/24 20:40,17/Oct/23 12:27,1.16.2,1.17.1,,,,,,,,,,,,,Runtime / Configuration,,,,,,0,,,,,,"When I submitted a large number of tasks in Flink-on-Yarn mode and successfully executed, I unexpectedly found a large number of empty directories left in the directory related to 'high availability.storageDir' on hdfs, with appids as shown below. I believe this must be cleared! However, after verification in the environments of 1.16.2 and 1.17.1, it was proven that neither of them solved this problem.

my flink-conf.yaml about 'high availability.storageDir':

{code:java}
high-availability.storageDir: hdfs://hdfsHACluster/flink/recovery
{code}

 !screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,,,FLINK-30513,,,,,,,,,,,,,,,,,,,"17/Oct/23 08:43;xinchen147;image-2023-10-17-16-43-07-859.png;https://issues.apache.org/jira/secure/attachment/13063598/image-2023-10-17-16-43-07-859.png","17/Oct/23 07:00;xinchen147;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13063596/screenshot-1.png","17/Oct/23 07:17;xinchen147;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13063597/screenshot-2.png",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 01:13:47 UTC 2023,,,,,,,,,,"0|z1kzl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 07:06;xinchen147;This is a serious bug. Specifically, when adding the following configuration to the namenode：
{code:java}
<property>
<name>dfs.namenode.fs-limits.max-directory-items</name>
<value>50</value>
</property>
{code}
Once the number of empty subdirectories remaining in 'high availability. storageDir' exceeds 50, Flink can not continue submitting tasks, with an error as follows:

{code:java}
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /flink/recovery/ is exceeded: limit=50 items=50
{code}
;;;","17/Oct/23 07:41;xinchen147;In the code, it can be seen that after the task is completed, there is an action to clear the data under the HA directory. If an exception occurs during the cleaning process, a warn-level log will be printed, which includes 'high availability StorageDir'.
 !screenshot-2.png! 

But in reality, 
{code:java}
removeJob (jobId, cleanupJobState)
{code}
 only deleted the blob subdirectory(/flink/recovery/application_1694077753088_0009/blob) of the appid-directory under that directory, as well as deleted znode and configmap in k8s, but there was no action to delete the parent directory(/flink/recovery/application_1694077753088_0009).;;;","17/Oct/23 07:52;Zhanghao Chen;Hi [~xinchen147], thanks for reporting it, this is the same issue as is reported in [FLINK-30513|https://issues.apache.org/jira/browse/FLINK-30513]. The originally proposed solution is to simply delete the parent dir on cluster termination. However, it was later found that the quick fix broke the job result store feature when {{job-result-store.delete-on-commit}} is set to false and the job result store path is not overridden. We can restart the discussion on a proper fix there.;;;","17/Oct/23 08:07;xinchen147;Hi [~Zhanghao Chen] , so 'Broking the job result store feature' is the reason why originally proposed solution did not incorporate 1.16 and 1.17？Thank you for your reply. I will carefully refer to that solution.;;;","17/Oct/23 08:13;Zhanghao Chen;[~xinchen147] Yes, the solution proposed in FLINK-30513 should be fine if you don't need to keep the job results after the job reached a clean state （which is also the default behavior of Flink, as job-result-store.delete-on-commit is default to true ）;;;","17/Oct/23 08:35;xinchen147;Thank you very much! [~Zhanghao Chen]. I have hardly heard of 'job-result-store', and I am not quite sure what scenario this parameter is specifically used for. Do you know which version of Flink that was added? Because we used Flink 1.12.2 before, we couldn't find this parameter in it, but we can saw it in 16 and 17.

And directory leak also exists in 'job-result-store.storage-path' in Flink 1.16-1.17:

 !image-2023-10-17-16-43-07-859.png! ;;;","17/Oct/23 08:39;Zhanghao Chen;Job Result Store is added in Flink 1.15. Check [https://cwiki.apache.org/confluence/display/FLINK/FLIP-194%3A+Introduce+the+JobResultStore] for more details.;;;","17/Oct/23 08:57;Zhanghao Chen;[~xinchen147] Indeed, it seems that the job result store path itself also has leakage issue that shall be fixed;;;","17/Oct/23 09:09;xinchen147;[~Zhanghao Chen] Yes, and I'm still a bit confused. You can see that job-result-store path and HA-storage-appid path is  parallel, such as:

{code:java}
/flink/recovery/application_1697525535591_0001
/flink/recovery/job-result-store/application_1697525535591_0001
{code}

and Your code:

{code:java}
final Path clusterHaStoragePath =
                HighAvailabilityServicesUtils.getClusterHighAvailableStoragePath(configuration);
{code}
'clusterHaStoragePath ' can be '/flink/recovery/application_1697525535591_0001', we can delete that. Deleting it every time does not seem to affect the job-result-store-path: '/flink/recovery/job-result-store/application_1697525535591_0001'! 

So why it can broke the job result store feature?
;;;","17/Oct/23 09:55;Zhanghao Chen;[~xinchen147] you are right. But since the path to job result storage is configurable, if it is configured to be a child path of HA storage path, we might get into trouble.;;;","17/Oct/23 10:14;xinchen147;yeah，I see now. [~Zhanghao Chen] I have incorporated your modifications and verified that after the task is executed normally, the directory can be deleted. However, in abnormal scenarios, such as when the task is running on Yarn and I kill it, both the directory and subdirectory blob will still exist. Perhaps the community does not consider abnormal scenarios, as we have previously added a hook to clear the appid directory under '/user/hadoop/. flink', It includes jar packages uploaded by Flink when submitting tasks.

But this hook involves Hadoop and it also needs to be modified. In the hook code, it needs to provide us with feedback about the status of the task. I think it may be due to the interaction between multiple communities, and the Flink community did not actively consider clearing in this abnormal situation.;;;","17/Oct/23 11:34;mapohl;Hi [~xinchen147], thanks for reporting this issue and [~Zhanghao Chen] for jumping into the discussion.

The job cleanup should deal with all the artifacts of a job, i.e. all its artifacts should be removed if the job reaches a globally-terminal state (which means that it wouldn't be recovered if the Flink cluster restarts). This doesn't apply to the scenario where {{job-result-store.delete-on-commit}} is disabled because it would leave entry files in the {{JobResultStore}} folder. In that case, Flink has handed over the ownership of those entry files to the user (i.e. he/she is in charge of (re)moving the files). This is desired behavior to enable recovery in application-mode clusters.

This keeps me wondering whether we should do a recursive deletion of empty folders within the HA storage folder. That would cover the case where all the artifacts are removed (when all jobs reached globally-terminal state) and protect the scenarios where artifacts are retained (with {{job-result-store.delete-on-commit}} being disabled or some jobs not having reached globally-terminal state or some other issue occurred that prevented the deletion of artifacts). WDYT?;;;","17/Oct/23 12:19;Zhanghao Chen;HI [~mapohl], thanks for joining the discussion here. +1 for adding recursive deletion of empty folders within the HA storage folder + the JRS storage folder. Actually, only one-level of recursion should be enough in the two cases here. For HA storage folder cleanup, we just need to add an additional check if there exist some other files/subfolders before delete the dir based on FLINK-30513's solution. For cleaning the JRS storage folder, we'll need to implement a cleanup method for JobResultStore, which is called on cluster termination and takes responsibility of cleaning up any empty folders. I'd be willing to pick up from where I left in FLINK-30513 to fix the leakage issue.;;;","17/Oct/23 12:27;mapohl;Sounds good to me. (y) Let's move the discussion into FLINK-30513 as you already suggested. I'm gonna close this issue here in favor of FLINK-30513. Thanks again for reporting it, anyway, [~xinchen147] and for your detailed description. That's much appreciated.;;;","18/Oct/23 01:13;xinchen147;[~mapohl][~Zhanghao Chen] Thank you for your discussion. I'll move to pay attention to  FLINK-30513. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using the flink shaded jackson for flink-autoscaler,FLINK-33287,13554350,13556703,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,17/Oct/23 06:17,24/Jan/24 10:55,04/Jun/24 20:40,24/Jan/24 10:54,,,,,,,,,,,kubernetes-operator-1.8.0,,,,Autoscaler,,,,,,0,pull-request-available,,,,,"FLINK-33098 still using the jackson version of {{flink-kubernetes-operator}} instead of flink shaded version. I want to update it after {{flink-1.18}} released.

The reason is : current autoscaler is using the {{loaderOptions}} to limit the serialized size.
 * The shaded jackson version of {{flink-1.17}} is {{{}2.13.4-16.1{}}}, it doesn't support the {{{}loaderOptions{}}}.
 * The shaded jackson version of {{flink-1.18}} is {{{}2.14.2-17.0{}}}, it supports the {{{}loaderOptions{}}}.

 

For details can be get from this comment[1].

 

[1] https://github.com/apache/flink-kubernetes-operator/pull/677#discussion_r1336571925",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 24 10:54:50 UTC 2024,,,,,,,,,,"0|z1kzkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 08:49;martijnvisser;I don't think the Operator should use flink-shaded, but use its own. We're decoupling flink-shaded from all other Flink repos, like connectors, so why would we tightly couple these here?;;;","19/Oct/23 03:46;fanrui;Thanks [~martijnvisser] for this comment!

{quote}We're decoupling flink-shaded from all other Flink repos, like connectors{quote}

May I know why decoupling flink-shaded from connectors? As I understand, it's because we want the same connector version to support different versions of Flink, right?

If yes, it's reasonable for connectors.

Let me explain why we prefer flink-autoscaler uses the flink shaded jackson:
* flink-autoscaler only depends on the latest version of flink, it will not depends on older versions of flink.
* flink-autoscaler is using the jackson version of operator for now, this version isn't shaded.
* Both non-shaded and shaded versions are fine so now.
* flink-autoscaler has been decoupled from the kubernetes operator and may be moved to a separate repo in the future. So depending on kubernetes operator jackson may not be a good choice.
* Currently flink-autoscaler only depends on the latest flink, so there will be no conflicts with other shaded jackson.
* The shaded version is enough for now, so we prefer bundle it with the flink client. And we can re-shade it if it doesn't work well in the future. 

WDYT? 

Please refer to [comment|https://github.com/apache/flink-kubernetes-operator/pull/677#discussion_r1336571925] for more background and details. 
;;;","19/Oct/23 07:57;martijnvisser;[~fanrui] My concern is on this part:

bq. Currently flink-autoscaler only depends on the latest flink, so there will be no conflicts with other shaded jackson.

If at a point in time you want to add support for multiple versions of Flink, you're going to run into the same problem as with the connectors. If it still works for Autoscaler, fine with me, as long as we won't have issues brought up in the future where we're saying we can't update a Flink-Shaded dependency because it would break the Autoscaler. ;;;","19/Oct/23 08:38;fanrui;{quote} If at a point in time you want to add support for multiple versions of Flink
{quote}
Actually, the flink-autoscaler supports muitiple versions for now, its interaction with multiple flink versions is based on rest api.

Why does flink-autoscaler need to depend on flink, it just needs some Utils or common class, such as: JobStatus, JobVertex, RestClusterClient, etc.

Also, if we need to depend on multiple versions in the future, we can re-shade it at that time. It really isn't needed for now, WDYT?;;;","24/Jan/24 10:54;fanrui;Merged to main(1.8.0) via : ca5587445e1cde945e26bd6aa65b9bf10bad41a8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataGeneratorSource should support automatic return type detection,FLINK-33286,13554305,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,afedulov,afedulov,afedulov,16/Oct/23 21:24,17/Oct/23 10:03,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,,,,,,0,,,,,,"Currently, DataGeneratorSource requires both GeneratorFunction<Long, OUT> and 
TypeInformation<OUT> to be passed during its construction. Given that the generator function has a fixed API, it should be possible to reliably extract the OUT type automatically for both generator functions passed both as lambda and as objects.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 21:25:43 UTC 2023,,,,,,,,,,"0|z1kzag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 21:25;afedulov;Connectors / DataGen component is missing. If you read this comment and have the required permissions, please create.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e 1 stage: Wordcount on Docker test (custom fs plugin),FLINK-33285,13554211,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,16/Oct/23 09:34,06/Nov/23 17:19,04/Jun/24 20:40,06/Nov/23 17:19,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"Might be caused by TCPServer not being able to be started due to address already in use error

[https://github.com/XComp/flink/actions/runs/6525957588/job/17719349973#step:13:5859]
{code:java}
 Oct 15 20:44:18 Retry 1/5 exited 128, retrying in 1 seconds...
5829Traceback (most recent call last):
5830  File ""/root/flink/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
5831    httpd = socketserver.TCPServer(("""", 9999), handler)
5832  File ""/usr/lib/python3.5/socketserver.py"", line 440, in __init__
5833    self.server_bind()
5834  File ""/usr/lib/python3.5/socketserver.py"", line 454, in server_bind
5835    self.socket.bind(self.server_address)
5836OSError: [Errno 98] Address already in use{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 06 17:19:39 UTC 2023,,,,,,,,,,"0|z1kypk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:38;mapohl;https://github.com/XComp/flink/actions/runs/6525933101/job/17719343177#step:13:6025;;;","16/Oct/23 09:40;mapohl;https://github.com/XComp/flink/actions/runs/6525266999/job/17718044295#step:13:6008;;;","16/Oct/23 09:41;mapohl;https://github.com/XComp/flink/actions/runs/6525150610/job/17717689177#step:15:6266;;;","16/Oct/23 09:44;mapohl;https://github.com/XComp/flink/actions/runs/6525041368/job/17717462067#step:15:5987;;;","01/Nov/23 07:10;mapohl;https://github.com/XComp/flink/actions/runs/6707387404/job/18226443754#step:15:7318;;;","01/Nov/23 07:10;mapohl;https://github.com/XComp/flink/actions/runs/6711829097/job/18242192715#step:15:5977;;;","02/Nov/23 07:52;mapohl;* https://github.com/XComp/flink/actions/runs/6719526710/job/18261920352#step:15:5470
* https://github.com/XComp/flink/actions/runs/6723267607/job/18273503924#step:15:6014
* https://github.com/XComp/flink/actions/runs/6723279744/job/18273707884#step:15:6141
* https://github.com/XComp/flink/actions/runs/6724579458/job/18277690835#step:15:6738
* https://github.com/XComp/flink/actions/runs/6727253349/job/18285012122#step:15:6559
* https://github.com/XComp/flink/actions/runs/6729006580/job/18289544480#step:15:6546;;;","03/Nov/23 07:09;mapohl;without docker-compose installation:
* https://github.com/XComp/flink/actions/runs/6737717931/job/18316247284#step:15:4435
* https://github.com/XComp/flink/actions/runs/6737982594/job/18316903575#step:15:5325

w/ docker-compose dependency being installed:
* https://github.com/XComp/flink/actions/runs/6738075474/job/18317315889#step:16:6160
* https://github.com/XComp/flink/actions/runs/6738562074/job/18318911234#step:16:5197

Next error in logs:
{code}
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Nov 02 22:03:49 Retry 1/5 exited 128, retrying in 1 seconds...
Traceback (most recent call last):
  File ""/root/flink/flink-end-to-end-tests/test-scripts/python3_fileserver.py"", line 26, in <module>
    httpd = socketserver.TCPServer(("""", 9999), handler)
  File ""/usr/lib/python3.5/socketserver.py"", line 440, in __init__
    self.server_bind()
  File ""/usr/lib/python3.5/socketserver.py"", line 454, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Nov 02 22:03:50 Retry 2/5 exited 128, retrying in 2 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Nov 02 22:03:52 Retry 3/5 exited 128, retrying in 4 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Nov 02 22:03:56 Retry 4/5 exited 128, retrying in 8 seconds...
fatal: destination path 'flink-docker' already exists and is not an empty directory.
Nov 02 22:04:04 Retry 5/5 exited 128, no more retries left.
{code};;;","06/Nov/23 17:19;mapohl;This issue was actually caused by the e2e tests running in a container. Moving the test execution into the runner (analogously to how we do it for the AzureCI pipeline) solved the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
core stage: HistoryServerStaticFileServerHandlerTest.testRespondWithFile(Path),FLINK-33284,13554208,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,16/Oct/23 09:24,27/Nov/23 08:43,04/Jun/24 20:40,27/Nov/23 08:41,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6525957588/job/17719339666#step:10:12209

{code}
Error: 20:06:13 20:06:13.081 [ERROR] org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.testRespondWithFile(Path)  Time elapsed: 1.981 s  <<< FAILURE!
Oct 15 20:06:13 org.opentest4j.AssertionFailedError: 
Oct 15 20:06:13 
Oct 15 20:06:13 expected: 200
Oct 15 20:06:13  but was: 404
Oct 15 20:06:13 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Oct 15 20:06:13 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Oct 15 20:06:13 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Oct 15 20:06:13 	at org.apache.flink.runtime.webmonitor.history.HistoryServerStaticFileServerHandlerTest.testRespondWithFile(HistoryServerStaticFileServerHandlerTest.java:70)
Oct 15 20:06:13 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33283,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 27 08:41:37 UTC 2023,,,,,,,,,,"0|z1kyow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:26;mapohl;FLINK-33284 and FLINK-33283 failed in the same job;;;","16/Oct/23 09:36;mapohl;https://github.com/XComp/flink/actions/runs/6525933101/job/17719334124#step:10:12214;;;","16/Oct/23 09:39;mapohl;https://github.com/XComp/flink/actions/runs/6525266999/job/17718032103#step:10:12283;;;","16/Oct/23 11:22;chesnay;Same cause as FLINK-32283.;;;","27/Nov/23 08:41;mapohl;The issue didn't reappear for a while. I'm closing it as not-reproducible to track that there's no effort put into investigating it further for now. We can re-open this Jira again if the problem re-occurs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
core stage: WebFrontendBootstrapTest.testHandlersMustBeLoaded,FLINK-33283,13554207,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,16/Oct/23 09:23,27/Nov/23 08:41,04/Jun/24 20:40,27/Nov/23 08:41,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"[https://github.com/XComp/flink/actions/runs/6525957588/job/17719339666#step:10:12279]
{code:java}
 Error: 20:06:13 20:06:13.132 [ERROR] org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrapTest.testHandlersMustBeLoaded  Time elapsed: 2.298 s  <<< FAILURE!
12279Oct 15 20:06:13 org.opentest4j.AssertionFailedError: 
12280Oct 15 20:06:13 
12281Oct 15 20:06:13 expected: 404
12282Oct 15 20:06:13  but was: 200
12283Oct 15 20:06:13 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
12284Oct 15 20:06:13 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
12285Oct 15 20:06:13 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
12286Oct 15 20:06:13 	at org.apache.flink.runtime.webmonitor.utils.WebFrontendBootstrapTest.testHandlersMustBeLoaded(WebFrontendBootstrapTest.java:89)
12287Oct 15 20:06:13 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33284,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 27 08:41:25 UTC 2023,,,,,,,,,,"0|z1kyoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:36;mapohl;https://github.com/XComp/flink/actions/runs/6525933101/job/17719334124#step:10:12288;;;","16/Oct/23 09:39;mapohl;https://github.com/XComp/flink/actions/runs/6525266999/job/17718032103#step:10:12214;;;","16/Oct/23 11:20;chesnay;Probably caused by the WebUI never being built.;;;","16/Oct/23 15:01;mapohl;hm, but it's also not built in the Azure CI runs (see [this one|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53758&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=3998]) but the tests are succeeding. We also have two Github Action runs (e.g. [this one|https://github.com/XComp/flink/actions/runs/6529754573/job/17728222254]) where {{core}} didn't fail.;;;","17/Oct/23 09:28;chesnay;Azure CI builds it in the compile step. But it seems the GHA CI also did that in the failed runs :/;;;","31/Oct/23 15:19;mapohl;I did a test run where I repeatedly run {{WebFrontendBootstrapTest.testHandlersMustBeLoaded}} only (see [2a0627e|https://github.com/XComp/flink/blob/2a0627e9d6cc5f12462cfc7de2c58acb85e666fd/.github/workflows/flink-ci-template.yml#L286-L290]. I wasn't able to reproduce the error (grep over the build's log artifacts):
{code:bash}
$ grep -c ""Tests run: 1"" *module*/*_Test*txt      
ci  Test (module connect_1)/12_Test - connect_1.txt:5536
ci  Test (module connect_2)/12_Test - connect_2.txt:6136
ci  Test (module core)/12_Test - core.txt:4866
ci  Test (module misc)/12_Test - misc.txt:5602
ci  Test (module python)/12_Test - python.txt:5922
ci  Test (module table)/12_Test - table.txt:5522
ci  Test (module tests)/12_Test - tests.txt:5806
$ grep -c ""Tests run: 1, Failures: 0"" *module*/*_Test*txt
ci  Test (module connect_1)/12_Test - connect_1.txt:5536
ci  Test (module connect_2)/12_Test - connect_2.txt:6136
ci  Test (module core)/12_Test - core.txt:4866
ci  Test (module misc)/12_Test - misc.txt:5602
ci  Test (module python)/12_Test - python.txt:5922
ci  Test (module table)/12_Test - table.txt:5522
ci  Test (module tests)/12_Test - tests.txt:5806
{code}
The number of test runs matched the number of test runs w/o a failure.;;;","27/Nov/23 08:41;mapohl;The issue didn't reappear for a while. I'm closing it as not-reproducible.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
core stage: 137 exit code,FLINK-33282,13554205,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,16/Oct/23 09:21,16/Nov/23 16:46,04/Jun/24 20:40,16/Nov/23 16:46,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,https://github.com/XComp/flink/actions/runs/6529672916/job/17728011881#step:12:8547,,,,,,,,,,,,,,,,,,,,,,,FLINK-31278,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 01 18:39:29 UTC 2023,,,,,,,,,,"0|z1kyo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:26;Sergey Nuyanzin;I wonder whether this could be related to each other FLINK-31278;;;","16/Oct/23 12:15;mapohl;yes, could be. I didn't do research on whether it's actually caused by a actual Flink issue or whether it's GHA-related.;;;","01/Nov/23 12:56;mapohl;https://github.com/XComp/flink/actions/runs/6719520497/job/18261736667#step:12:8565;;;","01/Nov/23 18:39;mapohl;https://github.com/XComp/flink/actions/runs/6723279744/job/18273666083#step:12:8547;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e 2 stage: PyFlink end-to-end test,FLINK-33281,13554200,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,mapohl,mapohl,16/Oct/23 09:18,27/Nov/23 08:42,04/Jun/24 20:40,27/Nov/23 08:41,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"[https://github.com/XComp/flink/actions/runs/6529754573/job/17728244982#step:15:7938]
{code:java}
 Oct 16 08:02:13 pyflink.util.exceptions.TableException: org.apache.flink.table.api.TableException: Failed to execute sql
7868Oct 16 08:02:13 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1048)
7869Oct 16 08:02:13 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:864)
7870Oct 16 08:02:13 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1097)
7871Oct 16 08:02:13 	at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:59)
7872Oct 16 08:02:13 	at org.apache.flink.table.api.Table.executeInsert(Table.java:1074)
7873Oct 16 08:02:13 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
7874Oct 16 08:02:13 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
7875Oct 16 08:02:13 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
7876Oct 16 08:02:13 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
7877Oct 16 08:02:13 	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
7878Oct 16 08:02:13 	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
7879Oct 16 08:02:13 	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
7880Oct 16 08:02:13 	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
7881Oct 16 08:02:13 	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
7882Oct 16 08:02:13 	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
7883Oct 16 08:02:13 	at java.base/java.lang.Thread.run(Thread.java:829)
7884Oct 16 08:02:13 Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'insert-into_default_catalog.default_database.Results'.
7885Oct 16 08:02:13 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2253)
7886Oct 16 08:02:13 	at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:189)
7887Oct 16 08:02:13 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:110)
7888Oct 16 08:02:13 	at org.apache.flink.table.executor.python.ChainingOptimizingExecutor.executeAsync(ChainingOptimizingExecutor.java:88)
7889Oct 16 08:02:13 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1020)
7890Oct 16 08:02:13 	... 15 more
7891Oct 16 08:02:13 Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
7892Oct 16 08:02:13 	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$12(RestClusterClient.java:479)
7893Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986)
7894Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970)
7895Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
7896Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
7897Oct 16 08:02:13 	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$6(FutureUtils.java:272)
7898Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
7899Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
7900Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
7901Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610)
7902Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085)
7903Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
7904Oct 16 08:02:13 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
7905Oct 16 08:02:13 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
7906Oct 16 08:02:13 	... 1 more
7907Oct 16 08:02:13 Caused by: org.apache.flink.runtime.rest.util.RestClientException: [File upload failed.]
7908Oct 16 08:02:13 	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:646)
7909Oct 16 08:02:13 	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$6(RestClient.java:626)
7910Oct 16 08:02:13 	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)
7911Oct 16 08:02:13 	... 4 more{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 27 08:41:09 UTC 2023,,,,,,,,,,"0|z1kyn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:22;mapohl;https://github.com/XComp/flink/actions/runs/6529672916/job/17728036397#step:15:7712;;;","16/Oct/23 09:34;mapohl;https://github.com/XComp/flink/actions/runs/6525957588/job/17719350016#step:13:7923;;;","16/Oct/23 09:38;mapohl;https://github.com/XComp/flink/actions/runs/6525933101/job/17719343238#step:13:7692;;;","16/Oct/23 09:41;mapohl;https://github.com/XComp/flink/actions/runs/6525266999/job/17718044376#step:13:7623;;;","16/Oct/23 09:42;mapohl;https://github.com/XComp/flink/actions/runs/6525150610/job/17717689225#step:15:7662;;;","16/Oct/23 09:45;mapohl;https://github.com/XComp/flink/actions/runs/6525041368/job/17717462135#step:15:8164;;;","27/Nov/23 08:41;mapohl;The issue didn't reappear for a while. I'm closing it as not-reproducible to track that there's no effort put into investigating it further for now. We can re-open this Jira again if the problem re-occurs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tests stage: HybridShuffleITCase.testHybridSelectiveExchangesRestart due to timeout,FLINK-33280,13554199,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,16/Oct/23 09:17,31/Oct/23 10:49,04/Jun/24 20:40,18/Oct/23 12:09,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Network,,,,,,0,github-actions,test-stability,,,,"[https://github.com/XComp/flink/actions/runs/6529754573/job/17728223099#step:12:9120]
{code:java}
 Oct 16 09:13:15 java.lang.AssertionError: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=10, backoffTimeMS=0)
8959Oct 16 09:13:15 	at org.apache.flink.test.runtime.JobGraphRunningUtil.execute(JobGraphRunningUtil.java:59)
8960Oct 16 09:13:15 	at org.apache.flink.test.runtime.BatchShuffleITCaseBase.executeJob(BatchShuffleITCaseBase.java:118)
8961Oct 16 09:13:15 	at org.apache.flink.test.runtime.HybridShuffleITCase.testHybridSelectiveExchangesRestart(HybridShuffleITCase.java:77)
8962Oct 16 09:13:15 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
8963Oct 16 09:13:15 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
8964Oct 16 09:13:15 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
8965Oct 16 09:13:15 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
8966Oct 16 09:13:15 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
8967Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
8968Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
8969Oct 16 09:13:15 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
8970Oct 16 09:13:15 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
8971Oct 16 09:13:15 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtension.java:94)
8972Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
8973Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
8974Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
8975Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
8976Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
8977Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
8978Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
8979Oct 16 09:13:15 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
8980Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
8981Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
8982Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
8983Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
8984Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
8985Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
8986Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
8987Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
8988Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
8989Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
8990Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
8991Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
8992Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
8993Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
8994Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.submit(ForkJoinPoolHierarchicalTestExecutorService.java:118)
8995Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:226)
8996Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask$DefaultDynamicTestExecutor.execute(NodeTestTask.java:204)
8997Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:142)
8998Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.lambda$execute$2(TestTemplateTestDescriptor.java:110)
8999Oct 16 09:13:15 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
9000Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
9001Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)
9002Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
9003Oct 16 09:13:15 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
9004Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
9005Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)
9006Oct 16 09:13:15 	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
9007Oct 16 09:13:15 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
9008Oct 16 09:13:15 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
9009Oct 16 09:13:15 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
9010Oct 16 09:13:15 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
9011Oct 16 09:13:15 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
9012Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
9013Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:274)
9014Oct 16 09:13:15 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
9015Oct 16 09:13:15 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
9016Oct 16 09:13:15 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
9017Oct 16 09:13:15 	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
9018Oct 16 09:13:15 	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
9019Oct 16 09:13:15 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
9020Oct 16 09:13:15 	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
9021Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:110)
9022Oct 16 09:13:15 	at org.junit.jupiter.engine.descriptor.TestTemplateTestDescriptor.execute(TestTemplateTestDescriptor.java:44)
9023Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
9024Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
9025Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
9026Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
9027Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
9028Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
9029Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
9030Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
9031Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
9032Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
9033Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
9034Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
9035Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
9036Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
9037Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
9038Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
9039Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
9040Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
9041Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
9042Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
9043Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
9044Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
9045Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
9046Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
9047Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
9048Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
9049Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
9050Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
9051Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
9052Oct 16 09:13:15 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
9053Oct 16 09:13:15 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
9054Oct 16 09:13:15 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
9055Oct 16 09:13:15 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
9056Oct 16 09:13:15 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
9057Oct 16 09:13:15 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
9058Oct 16 09:13:15 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
9059Oct 16 09:13:15 Caused by: org.apache.flink.runtime.JobException: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=10, backoffTimeMS=0)
9060Oct 16 09:13:15 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)
9061Oct 16 09:13:15 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)
9062Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)
9063Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)
9064Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)
9065Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:765)
9066Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:742)
9067Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:51)
9068Oct 16 09:13:15 	at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(DefaultExecutionGraph.java:1645)
9069Oct 16 09:13:15 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1144)
9070Oct 16 09:13:15 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1084)
9071Oct 16 09:13:15 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:923)
9072Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultExecutionOperations.markFailed(DefaultExecutionOperations.java:43)
9073Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.handleTaskDeploymentFailure(DefaultExecutionDeployer.java:330)
9074Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignAllResourcesAndRegisterProducedPartitions$2(DefaultExecutionDeployer.java:169)
9075Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
9076Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
9077Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
9078Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
9079Oct 16 09:13:15 	at org.apache.flink.runtime.jobmaster.slotpool.PendingRequest.failRequest(PendingRequest.java:88)
9080Oct 16 09:13:15 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:371)
9081Oct 16 09:13:15 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.timeoutPendingSlotRequest(DeclarativeSlotPoolBridge.java:342)
9082Oct 16 09:13:15 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.checkBatchSlotTimeout(DeclarativeSlotPoolBridge.java:497)
9083Oct 16 09:13:15 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
9084Oct 16 09:13:15 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
9085Oct 16 09:13:15 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRunAsync$4(PekkoRpcActor.java:451)
9086Oct 16 09:13:15 	at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
9087Oct 16 09:13:15 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRunAsync(PekkoRpcActor.java:451)
9088Oct 16 09:13:15 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:218)
9089Oct 16 09:13:15 	at org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)
9090Oct 16 09:13:15 	at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)
9091Oct 16 09:13:15 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)
9092Oct 16 09:13:15 	at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)
9093Oct 16 09:13:15 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
9094Oct 16 09:13:15 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
9095Oct 16 09:13:15 	at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)
9096Oct 16 09:13:15 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)
9097Oct 16 09:13:15 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
9098Oct 16 09:13:15 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)
9099Oct 16 09:13:15 	at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)
9100Oct 16 09:13:15 	at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)
9101Oct 16 09:13:15 	at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)
9102Oct 16 09:13:15 	at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)
9103Oct 16 09:13:15 	at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)
9104Oct 16 09:13:15 	at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)
9105Oct 16 09:13:15 	at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)
9106Oct 16 09:13:15 	at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)
9107Oct 16 09:13:15 	... 5 more
9108Oct 16 09:13:15 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Pending slot request with SlotRequestId{9204458405f867d3ac077179bf43a756} has been released.
9109Oct 16 09:13:15 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:226)
9110Oct 16 09:13:15 	... 37 more
9111Oct 16 09:13:15 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Pending slot request with SlotRequestId{9204458405f867d3ac077179bf43a756} has been released.
9112Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
9113Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
9114Oct 16 09:13:15 	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:632)
9115Oct 16 09:13:15 	... 35 more
9116Oct 16 09:13:15 Caused by: org.apache.flink.util.FlinkException: org.apache.flink.util.FlinkException: Pending slot request with SlotRequestId{9204458405f867d3ac077179bf43a756} has been released.
9117Oct 16 09:13:15 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:373)
9118Oct 16 09:13:15 	... 31 more
9119Oct 16 09:13:15 Caused by: java.util.concurrent.TimeoutException: java.util.concurrent.TimeoutException: Pending slot request timed out in slot pool.
9120Oct 16 09:13:15 	... 31 more{code}",,,,,,,,,,,,,,,,,,,,,,,FLINK-33185,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 13:14:39 UTC 2023,,,,,,,,,,"0|z1kymw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:44;mapohl;https://github.com/XComp/flink/actions/runs/6525041368/job/17717450228#step:12:43025;;;","18/Oct/23 12:08;tanyuxin;The reason is the same as https://issues.apache.org/jira/browse/FLINK-33185. The fix has been merged.

Feel free to re-open this issue if it‘s also reproducible.;;;","18/Oct/23 13:14;mapohl;Thanks for looking into it [~tanyuxin];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisStream e2e tests running/failing on PR ,FLINK-33279,13554195,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,16/Oct/23 08:57,19/Oct/23 08:16,04/Jun/24 20:40,19/Oct/23 08:16,,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,,,"We recently added e2e tests to hit the real KDS (AWS) endpoints on CI. These tests should only run on push/nightly. The tests are running on PR and failing, since the credentils are not present. [https://github.com/apache/flink-connector-aws/actions/runs/6524289454/job/17731091860?pr=105]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 19 08:16:25 UTC 2023,,,,,,,,,,"0|z1kym0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Oct/23 08:16;dannycranmer;Merged commit [{{a1eb725}}|https://github.com/apache/flink-connector-aws/commit/a1eb725e98b950dea51d91e2c16f782bffaf3903] into apache:main
Merged commit [{{c3395b7}}|https://github.com/apache/flink-connector-aws/commit/c3395b771e361d35da5059aa4b297a004048cbd0] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable fails on AZP,FLINK-33278,13554189,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,Sergey Nuyanzin,Sergey Nuyanzin,16/Oct/23 07:46,23/Oct/23 15:51,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,Runtime / RPC,,,,,,0,test-stability,,,,,"This build [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53740&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=6563]
fails as

{noformat}
Oct 15 01:02:20 Multiple Failures (1 failure)
Oct 15 01:02:20 -- failure 1 --
Oct 15 01:02:20 [Any cause is instance of class 'class org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException'] 
Oct 15 01:02:20 Expecting any element of:
Oct 15 01:02:20   [java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:38231/user/rpc/8c211f34-41e5-4efe-93bd-8eca6c590a7f] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
Oct 15 01:02:20 	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
Oct 15 01:02:20 	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
Oct 15 01:02:20 	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
Oct 15 01:02:20 	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),

...

{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/23 07:27;jiabao.sun;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13063795/screenshot-1.png","23/Oct/23 07:28;jiabao.sun;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13063796/screenshot-2.png","23/Oct/23 07:29;jiabao.sun;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13063797/screenshot-3.png","23/Oct/23 15:46;jiabao.sun;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13063808/screenshot-4.png",,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 15:51:57 UTC 2023,,,,,,,,,,"0|z1kyko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 12:23;mapohl;That's a suspicious one. I'm not able to reproduce it locally with over 25000 repetitions. It's hard to tell what's the cause of this issue due to the lack of debug logs. I'm wondering whether we should enable debug logging for the pekko module using {{LogLevelExtension}} to get a bit more out of the test run in the future. Enabling it for all modules (even when written to its own log files like we do it for the ZooKeeper dependency) might be a bit of an overkill considering how many RPC calls are sent.

I would assume that it's not only a 1.19.0 issue but also affects 1.18.0 since we migrated to Pekko.

[~chesnay] do you have any other suggestions on that one? I couldn't find any error like that in Jira (considering Akka instead of Pekko as a substring).;;;","17/Oct/23 12:23;mapohl;
On the other hand: There seems to be a hickup in the logs between 01:02:10.686 and 01:02:19.074 which might have been the cause.
{code}
[...]
Oct 15 01:02:09 01:02:09.137 [INFO] Running org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest
Oct 15 01:02:09 01:02:09.535 [INFO] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.478 s - in org.apache.flink.runtime.rpc.pekko.PekkoRpcActorTest
Oct 15 01:02:09 01:02:09.549 [INFO] Running org.apache.flink.runtime.rpc.pekko.TimeoutCallStackTest
Oct 15 01:02:09 01:02:09.797 [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.243 s - in org.apache.flink.runtime.rpc.pekko.TimeoutCallStackTest
Oct 15 01:02:09 01:02:09.823 [INFO] Running org.apache.flink.runtime.rpc.pekko.RpcSerializedValueTest
Oct 15 01:02:09 01:02:09.937 [INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.111 s - in org.apache.flink.runtime.rpc.pekko.RpcSerializedValueTest
Oct 15 01:02:09 01:02:09.957 [INFO] Running org.apache.flink.runtime.rpc.pekko.RobustActorSystemTest
Oct 15 01:02:09 01:02:09.968 [INFO] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.371 s - in org.apache.flink.runtime.rpc.pekko.ContextClassLoadingSettingTest
Oct 15 01:02:09 01:02:09.993 [INFO] Running org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceTest
Oct 15 01:02:10 [INFO] [10/15/2023 01:02:09.993] [ForkJoinPool-41-worker-25] [CoordinatedShutdown(pekko://testSystem)] Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Oct 15 01:02:10 [INFO] [10/15/2023 01:02:10.049] [ForkJoinPool-41-worker-25] [CoordinatedShutdown(pekko://testSystem)] Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Oct 15 01:02:10 [INFO] [10/15/2023 01:02:10.153] [ForkJoinPool-41-worker-25] [CoordinatedShutdown(pekko://testSystem)] Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Oct 15 01:02:10 [INFO] [10/15/2023 01:02:10.179] [ForkJoinPool-41-worker-25] [CoordinatedShutdown(pekko://testSystem)] Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Oct 15 01:02:10 [INFO] [10/15/2023 01:02:10.213] [ForkJoinPool-41-worker-25] [CoordinatedShutdown(pekko://testSystem)] Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
Oct 15 01:02:10 01:02:10.219 [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.257 s - in org.apache.flink.runtime.rpc.pekko.RobustActorSystemTest
Oct 15 01:02:10 01:02:10.686 [INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.689 s - in org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceTest
Oct 15 01:02:19 01:02:19.074 [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.618 s - in org.apache.flink.runtime.rpc.pekko.MessageSerializationTest
Oct 15 01:02:19 01:02:19.847 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.694 s <<< FAILURE! - in org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest
Oct 15 01:02:19 01:02:19.847 [ERROR] org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable  Time elapsed: 10.238 s  <<< FAILURE!
[...]
{code};;;","23/Oct/23 07:32;jiabao.sun;Hold the thread at ClassLoadingUtils.guardCompletionWithContextClassLoader more than 10 seconds will reproduce this problem.
Increase the RPC timeout mays solve this problem.
Perhaps there are other problems that cause this problem, should investigate further.

!screenshot-1.png!
!screenshot-2.png! 
!screenshot-3.png! ;;;","23/Oct/23 15:09;mapohl;thanks for looking into this, [~jiabao.sun]. I am not able to follow what you're doing. Stopping the code execution at the lines that you suggest in your screenshots doesn't make the test fail for me. Generally speaking, if you stop the execution at the ""right"" place in the code it becomes quite likely that you generate a timeout. 

That's also what we most likely have observed in the logs where there the machine didn't continue processing for some time (based on the logged timestamps).;;;","23/Oct/23 15:51;jiabao.sun;The hickup interval is nearly 10 seconds in the logs between 01:02:10.686 and 01:02:19.074, so I suspected that the RPC timeout.
As for the specific reason, it may need further survey.

The break entrance:
 !screenshot-4.png! 

Stack trace:
{code:java}
1825 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils [] - Trying to start actor system, external address localhost:0, bind address 0.0.0.0:0.
3007 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.event.slf4j.Slf4jLogger [] - Slf4jLogger started
3041 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.remote.RemoteActorRefProvider [] - Pekko Cluster not in use - enabling unsafe features anyway because `pekko.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
3042 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.remote.Remoting [] - Starting remoting
3218 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.remote.Remoting [] - Remoting started; listening on addresses :[pekko.tcp://flink@localhost:57012]
3534 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils [] - Actor system started at pekko.tcp://flink@localhost:57012
3571 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils [] - Trying to start actor system, external address localhost:0, bind address 0.0.0.0:0.
3610 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.event.slf4j.Slf4jLogger [] - Slf4jLogger started
3616 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider [] - Pekko Cluster not in use - enabling unsafe features anyway because `pekko.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
3617 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.Remoting [] - Starting remoting
3627 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.Remoting [] - Remoting started; listening on addresses :[pekko.tcp://flink@localhost:57013]
3665 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils [] - Actor system started at pekko.tcp://flink@localhost:57013
3690 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.util.TestLoggerExtension [] - 
================================================================================
Test org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable[failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable()] is running.
--------------------------------------------------------------------------------
3696 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils [] - Trying to start actor system, external address localhost:0, bind address 0.0.0.0:0.
3726 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.event.slf4j.Slf4jLogger [] - Slf4jLogger started
3730 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider [] - Pekko Cluster not in use - enabling unsafe features anyway because `pekko.remote.use-unsafe-remote-features-outside-cluster` has been enabled.
3730 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.Remoting [] - Starting remoting
3746 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.Remoting [] - Remoting started; listening on addresses :[pekko.tcp://flink@localhost:57014]
3789 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcServiceUtils [] - Actor system started at pekko.tcp://flink@localhost:57014
3831 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.rpc.pekko.PekkoRpcActorTest$SerializedValueRespondingEndpoint at pekko://flink/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5 .
8669 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Stopping Pekko RPC service.
8772 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.actor.CoordinatedShutdown [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
8801 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
8801 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.
8864 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.
8892 [flink-pekko.actor.default-dispatcher-20] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Stopped Pekko RPC service.
22759 [flink-pekko.actor.default-dispatcher-6] WARN  org.apache.pekko.remote.transport.netty.NettyTransport [] - Remote connection to [null] failed with java.net.ConnectException: Connection refused: localhost/127.0.0.1:57014
22760 [flink-pekko.actor.default-dispatcher-6] WARN  org.apache.pekko.remote.ReliableDeliverySupervisor [] - Association with remote system [pekko.tcp://flink@localhost:57014] has failed, address is now gated for [50] ms. Reason: [Association failed with [pekko.tcp://flink@localhost:57014]] Caused by: [java.net.ConnectException: Connection refused: localhost/127.0.0.1:57014]
22829 [ForkJoinPool-1-worker-1] ERROR org.apache.flink.util.TestLoggerExtension [] - 
--------------------------------------------------------------------------------
Test org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable[failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable()] failed with:
org.assertj.core.error.AssertJMultipleFailuresError: 
Multiple Failures (1 failure)
-- failure 1 --
[Any cause is instance of class 'class org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException'] 
Expecting any element of:
  [java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
    java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
    org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
]
to satisfy the given assertions requirements but none did:

java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)

java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)

org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.

error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
at FlinkAssertions.lambda$anyCauseMatches$3(FlinkAssertions.java:89)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable(RemotePekkoRpcActorTest.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec$$$capture(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)

================================================================================

java.lang.AssertionError: [Any cause is instance of class 'class org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException'] 
Expecting any element of:
  [java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
    java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
    org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
]
to satisfy the given assertions requirements but none did:

java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)

java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)

org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.

error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.


	at org.apache.flink.core.testutils.FlinkAssertions.lambda$anyCauseMatches$3(FlinkAssertions.java:89)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable(RemotePekkoRpcActorTest.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec$$$capture(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)


org.assertj.core.error.AssertJMultipleFailuresError: 
Multiple Failures (1 failure)
-- failure 1 --
[Any cause is instance of class 'class org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException'] 
Expecting any element of:
  [java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
    java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed),
    org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
]
to satisfy the given assertions requirements but none did:

java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at java.util.concurrent.CompletableFuture.reportJoin(CompletableFuture.java:375)
	at java.util.concurrent.CompletableFuture.join(CompletableFuture.java:1947)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	...(63 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)

java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  java.util.concurrent.TimeoutException: Invocation of [RemoteRpcInvocation(SerializedValueRespondingGateway.getSerializedValue())] at recipient [pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5] timed out. This is usually caused by: 1) Pekko failed sending the message silently, due to problems like oversized payload or serialization failures. In that case, you should find detailed error information in the logs. 2) The recipient needs more time for responding, due to problems like slow machines or network jitters. In that case, you can try to increase pekko.ask.timeout.
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.lambda$failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable$1(RemotePekkoRpcActorTest.java:168)
	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63)
	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892)
	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)

org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.

error: 
Expecting actual throwable to be an instance of:
  org.apache.flink.runtime.rpc.exceptions.RecipientUnreachableException
but was:
  org.apache.pekko.pattern.AskTimeoutException: Ask timed out on [Actor[pekko.tcp://flink@localhost:57014/user/rpc/079e1e41-10aa-4e64-95fb-1ce2a21c11b5#314813027]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
at FlinkAssertions.lambda$anyCauseMatches$3(FlinkAssertions.java:89)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at org.apache.flink.runtime.rpc.pekko.RemotePekkoRpcActorTest.failsRpcResultImmediatelyIfRemoteRpcServiceIsNotAvailable(RemotePekkoRpcActorTest.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec$$$capture(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1067)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1703)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:172)

22850 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Stopping Pekko RPC service.
22850 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Stopping Pekko RPC service.
22869 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.actor.CoordinatedShutdown [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
22869 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.actor.CoordinatedShutdown [] - Running CoordinatedShutdown with reason [ActorSystemTerminateReason]
22885 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
22885 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
22893 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.
22893 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.
22902 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.
22903 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remoting shut down.
22913 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Stopped Pekko RPC service.
22920 [flink-pekko.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.rpc.pekko.PekkoRpcService [] - Stopped Pekko RPC service.
public String getFoo()
{
    return foo;
}
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Upgrading to actions/checkout@v4 requires GLIBC 2.25, 2.27, or 2.28 to be installed, apparently",FLINK-33277,13554153,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,mapohl,mapohl,15/Oct/23 19:02,21/Dec/23 10:02,04/Jun/24 20:40,21/Dec/23 10:01,,,,,,,,,,,,,,,,,,,,,0,github-actions,,,,,https://github.com/XComp/flink/actions/runs/6525835575/job/17718926296#step:5:64,,,,,,,,,,,,,,,,,,,,,,FLINK-33923,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 21 10:01:48 UTC 2023,,,,,,,,,,"0|z1kyco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 11:08;chesnay;Is there a strong benefit in using v4? Connector CI is using v3.;;;","16/Oct/23 12:13;mapohl;None that I know of. I haven't looked into it, yet but just created the issue to have this covered as a subtask for now.;;;","18/Oct/23 09:54;Sergey Nuyanzin;Based one error description
{noformat}
##[debug]Running JavaScript Action with default external tool: node20
/__e/node20/bin/node: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /__e/node20/bin/node)
/__e/node20/bin/node: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.25' not found (required by /__e/node20/bin/node)
{noformat}
in case of actions/checkout@v4 default nodejs is nodejs20 which requires such dependencies.
At the same time flink-runtime-web depends on nodejs 16.13.2, 

based on that i wonder whether changing of preinstalled node version helps to cope with this
based on https://github.com/actions/setup-node
it should be possible like 
{code:yaml}
steps:
- uses: actions/checkout@v3
- uses: actions/setup-node@v3
  with:
    node-version: 16
{code} 
;;;","21/Dec/23 10:01;mapohl;I'm closing this one and created a duplicate issue under the FLINK-33901 umbrella where it's a valid subtask as part of the actual migration efforts (in contrast to FLINK-27075 which is handled as the experimenting Jira issue).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reorganize CI stages,FLINK-33276,13554044,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,13/Oct/23 16:00,15/Nov/23 17:35,04/Jun/24 20:40,15/Nov/23 17:35,1.18.0,1.19.0,,,,,,,,,1.18.1,1.19.0,,,Build System / CI,,,,,,0,pull-request-available,,,,,{{connect_2}} stage became obsolete due to the externalization of the connectors. We can merge {{connect_1}} and {{connect_2}} again into a single {{connect}} stage (and maybe rename it into something more meaningful?),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 15 12:01:04 UTC 2023,,,,,,,,,,"0|z1kxog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 12:01;mapohl;master: [e06267f293cfa7f708fa8a45b914b275017b2110|https://github.com/apache/flink/commit/e06267f293cfa7f708fa8a45b914b275017b2110]
1.18: [e53acef4374f97aeb2ac5ac491bd58c15347606c|https://github.com/apache/flink/commit/e53acef4374f97aeb2ac5ac491bd58c15347606c];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Vote on the release candidate,FLINK-33275,13554032,13554028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,13/Oct/23 13:40,24/Oct/23 12:43,04/Jun/24 20:40,24/Oct/23 12:43,1.18.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"Once you have built and individually reviewed the release candidate, please share it for the community-wide review. Please review foundation-wide [voting guidelines|http://www.apache.org/foundation/voting.html] for more information.

Start the review-and-vote thread on the dev@ mailing list. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [VOTE] Release 1.2.3, release candidate #3

Hi everyone,
Please review and vote on the release candidate #3 for the version 1.2.3, as follows:
[ ] +1, Approve the release
[ ] -1, Do not approve the release (please provide specific comments)

The complete staging area is available for your review, which includes:
 * JIRA release notes [1],
 * the official Apache source release and binary convenience releases to be deployed to dist.apache.org [2], which are signed with the key with fingerprint FFFFFFFF [3],
 * all artifacts to be deployed to the Maven Central Repository [4],
 * source code tag ""release-1.2.3-rc3"" [5],
 * website pull request listing the new release and adding announcement blog post [6].

The vote will be open for at least 72 hours. It is adopted by majority approval, with at least 3 PMC affirmative votes.

Thanks,
Release Manager

[1] link
[2] link
[3] [https://dist.apache.org/repos/dist/release/flink/KEYS]
[4] link
[5] link
[6] link
{quote}
*If there are any issues found in the release candidate, reply on the vote thread to cancel the vote.* There’s no need to wait 72 hours. Proceed to the Fix Issues step below and address the problem. However, some issues don’t require cancellation. For example, if an issue is found in the website pull request, just correct it on the spot and the vote can continue as-is.

For cancelling a release, the release manager needs to send an email to the release candidate thread, stating that the release candidate is officially cancelled. Next, all artifacts created specifically for the RC in the previous steps need to be removed:
 * Delete the staging repository in Nexus
 * Remove the source / binary RC files from dist.apache.org
 * Delete the source code tag in git

*If there are no issues, reply on the vote thread to close the voting.* Then, tally the votes in a separate email. Here’s an email template; please adjust as you see fit.
{quote}From: Release Manager
To: dev@flink.apache.org
Subject: [RESULT] [VOTE] Release 1.2.3, release candidate #3

I'm happy to announce that we have unanimously approved this release.

There are XXX approving votes, XXX of which are binding:
 * approver 1
 * approver 2
 * approver 3
 * approver 4

There are no disapproving votes.

Thanks everyone!
{quote}
 
----
h3. Expectations
 * Community votes to release the proposed candidate, with at least three approving PMC votes

Any issues that are raised till the vote is over should be either resolved or moved into the next release (if applicable).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-13 13:40:45.0,,,,,,,,,,"0|z1kxls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Propose a pull request for website updates,FLINK-33274,13554031,13554028,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,13/Oct/23 13:40,24/Oct/23 15:14,04/Jun/24 20:40,24/Oct/23 12:43,1.18.0,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"The final step of building the candidate is to propose a website pull request containing the following changes:
 # update [apache/flink-web:_config.yml|https://github.com/apache/flink-web/blob/asf-site/_config.yml]
 ## update {{FLINK_VERSION_STABLE}} and {{FLINK_VERSION_STABLE_SHORT}} as required
 ## update version references in quickstarts ({{{}q/{}}} directory) as required
 ## (major only) add a new entry to {{flink_releases}} for the release binaries and sources
 ## (minor only) update the entry for the previous release in the series in {{flink_releases}}
 ### Please pay notice to the ids assigned to the download entries. They should be unique and reflect their corresponding version number.
 ## add a new entry to {{release_archive.flink}}
 # add a blog post announcing the release in _posts
 # add a organized release notes page under docs/content/release-notes and docs/content.zh/release-notes (like [https://nightlies.apache.org/flink/flink-docs-release-1.15/release-notes/flink-1.15/]). The page is based on the non-empty release notes collected from the issues, and only the issues that affect existing users should be included (e.g., instead of new functionality). It should be in a separate PR since it would be merged to the flink project.

(!) Don’t merge the PRs before finalizing the release.

 
----
h3. Expectations
 * Website pull request proposed to list the [release|http://flink.apache.org/downloads.html]
 * (major only) Check {{docs/config.toml}} to ensure that
 ** the version constants refer to the new version
 ** the {{baseurl}} does not point to {{flink-docs-master}}  but {{flink-docs-release-X.Y}} instead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 10:33:56 UTC 2023,,,,,,,,,,"0|z1kxlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 10:33;jingge;Release notes PR: [https://github.com/apache/flink/pull/23527]

Web PR: https://github.com/apache/flink-web/pull/680;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Stage source and binary releases on dist.apache.org,FLINK-33273,13554030,13554028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,13/Oct/23 13:40,13/Oct/23 13:42,04/Jun/24 20:40,13/Oct/23 13:42,,,,,,,,,,,1.18.0,,,,,,,,,,0,,,,,,"Copy the source release to the dev repository of dist.apache.org:
# If you have not already, check out the Flink section of the dev repository on dist.apache.org via Subversion. In a fresh directory:
{code:bash}
$ svn checkout https://dist.apache.org/repos/dist/dev/flink --depth=immediates
{code}
# Make a directory for the new release and copy all the artifacts (Flink source/binary distributions, hashes, GPG signatures and the python subdirectory) into that newly created directory:
{code:bash}
$ mkdir flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
$ mv <flink-dir>/tools/releasing/release/* flink/flink-${RELEASE_VERSION}-rc${RC_NUM}
{code}
# Add and commit all the files.
{code:bash}
$ cd flink
flink $ svn add flink-${RELEASE_VERSION}-rc${RC_NUM}
flink $ svn commit -m ""Add flink-${RELEASE_VERSION}-rc${RC_NUM}""
{code}
# Verify that files are present under [https://dist.apache.org/repos/dist/dev/flink|https://dist.apache.org/repos/dist/dev/flink].
# Push the release tag if not done already (the following command assumes to be called from within the apache/flink checkout):
{code:bash}
$ git push <remote> refs/tags/release-${RELEASE_VERSION}-rc${RC_NUM}
{code}

 
----
h3. Expectations
 * Maven artifacts deployed to the staging repository of [repository.apache.org|https://repository.apache.org/content/repositories/]
 * Source distribution deployed to the dev repository of [dist.apache.org|https://dist.apache.org/repos/dist/dev/flink/]
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-13 13:40:45.0,,,,,,,,,,"0|z1kxlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Build and stage Java and Python artifacts,FLINK-33272,13554029,13554028,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,13/Oct/23 13:40,13/Oct/23 13:42,04/Jun/24 20:40,13/Oct/23 13:42,,,,,,,,,,,1.18.0,,,,,,,,,,0,,,,,,"# Create a local release branch ((!) this step can not be skipped for minor releases):
{code:bash}
$ cd ./tools
tools/ $ OLD_VERSION=$CURRENT_SNAPSHOT_VERSION NEW_VERSION=$RELEASE_VERSION RELEASE_CANDIDATE=$RC_NUM releasing/create_release_branch.sh
{code}
 # Tag the release commit:
{code:bash}
$ git tag -s ${TAG} -m ""${TAG}""
{code}
 # We now need to do several things:
 ## Create the source release archive
 ## Deploy jar artefacts to the [Apache Nexus Repository|https://repository.apache.org/], which is the staging area for deploying the jars to Maven Central
 ## Build PyFlink wheel packages
You might want to create a directory on your local machine for collecting the various source and binary releases before uploading them. Creating the binary releases is a lengthy process but you can do this on another machine (for example, in the ""cloud""). When doing this, you can skip signing the release files on the remote machine, download them to your local machine and sign them there.
 # Build the source release:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_source_release.sh
{code}
 # Stage the maven artifacts:
{code:bash}
tools $ releasing/deploy_staging_jars.sh
{code}
Review all staged artifacts ([https://repository.apache.org/]). They should contain all relevant parts for each module, including pom.xml, jar, test jar, source, test source, javadoc, etc. Carefully review any new artifacts.
 # Close the staging repository on Apache Nexus. When prompted for a description, enter “Apache Flink, version X, release candidate Y”.
Then, you need to build the PyFlink wheel packages (since 1.11):
 # Set up an azure pipeline in your own Azure account. You can refer to [Azure Pipelines|https://cwiki.apache.org/confluence/display/FLINK/Azure+Pipelines#AzurePipelines-Tutorial:SettingupAzurePipelinesforaforkoftheFlinkrepository] for more details on how to set up azure pipeline for a fork of the Flink repository. Note that a google cloud mirror in Europe is used for downloading maven artifacts, therefore it is recommended to set your [Azure organization region|https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/change-organization-location] to Europe to speed up the downloads.
 # Push the release candidate branch to your forked personal Flink repository, e.g.
{code:bash}
tools $ git push <remote> refs/heads/release-${RELEASE_VERSION}-rc${RC_NUM}:release-${RELEASE_VERSION}-rc${RC_NUM}
{code}
 # Trigger the Azure Pipelines manually to build the PyFlink wheel packages
 ## Go to your Azure Pipelines Flink project → Pipelines
 ## Click the ""New pipeline"" button on the top right
 ## Select ""GitHub"" → your GitHub Flink repository → ""Existing Azure Pipelines YAML file""
 ## Select your branch → Set path to ""/azure-pipelines.yaml"" → click on ""Continue"" → click on ""Variables""
 ## Then click ""New Variable"" button, fill the name with ""MODE"", and the value with ""release"". Click ""OK"" to set the variable and the ""Save"" button to save the variables, then back on the ""Review your pipeline"" screen click ""Run"" to trigger the build.
 ## You should now see a build where only the ""CI build (release)"" is running
 # Download the PyFlink wheel packages from the build result page after the jobs of ""build_wheels mac"" and ""build_wheels linux"" have finished.
 ## Download the PyFlink wheel packages
 ### Open the build result page of the pipeline
 ### Go to the {{Artifacts}} page (build_wheels linux -> 1 artifact)
 ### Click {{wheel_Darwin_build_wheels mac}} and {{wheel_Linux_build_wheels linux}} separately to download the zip files
 ## Unzip these two zip files
{code:bash}
$ cd /path/to/downloaded_wheel_packages
$ unzip wheel_Linux_build_wheels\ linux.zip
$ unzip wheel_Darwin_build_wheels\ mac.zip{code}
 ## Create directory {{./dist}} under the directory of {{{}flink-python{}}}:
{code:bash}
$ cd <flink-dir>
$ mkdir flink-python/dist{code}
 ## Move the unzipped wheel packages to the directory of {{{}flink-python/dist{}}}:
{code:java}
$ mv /path/to/wheel_Darwin_build_wheels\ mac/* flink-python/dist/
$ mv /path/to/wheel_Linux_build_wheels\ linux/* flink-python/dist/
$ cd tools{code}

Finally, we create the binary convenience release files:
{code:bash}
tools $ RELEASE_VERSION=$RELEASE_VERSION releasing/create_binary_release.sh
{code}
If you want to run this step in parallel on a remote machine you have to make the release commit available there (for example by pushing to a repository). 
*This is important: the commit inside the binary builds has to match the commit of the source builds and the tagged release commit.* 
When building remotely, you can skip gpg signing by setting {{{}SKIP_GPG=true{}}}. You would then sign the files manually after downloading them to your machine:
{code:bash}
$ for f in flink-*-bin*.tgz; do gpg --armor --detach-sig $f; done
$ gpg --armor --detach-sig apache-flink-*.tar.gz
{code}
The release manager need to make sure the PyPI project {{apache-flink}} and {{apache-flink-libraries}} has enough available space for the python artifacts. The remaining space must be larger than the size of {{{}tools/releasing/release/python{}}}. Login with the PyPI admin account ([account info|https://lists.apache.org/thread.html/8273a5e8834b788d8ae552a5e177b69e04e96c0446bb90979444deee@%3Cprivate.flink.apache.org%3E] is only available to PMC members) and check the remaining space in [project settings|http://pypi.org/manage/project/apache-flink-libraries/settings].

Request an increase if there's not enough space. Note, it could take some days for PyPI to review our request.

 
----
h3. Expectations
 * Check hashes (e.g. shasum -c *.sha512)
 * Check signatures (e.g. {{{}gpg --verify flink-1.2.3-source-release.tar.gz.asc flink-1.2.3-source-release.tar.gz{}}})
 * {{grep}} for legal headers in each file.
 * If time allows check the NOTICE files of the modules whose dependencies have been changed in this release in advance, since the license issues from time to time pop up during voting. See [Verifying a Flink Release|https://cwiki.apache.org/confluence/display/FLINK/Verifying+a+Flink+Release] ""Checking License"" section.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-13 13:40:45.0,,,,,,,,,,"0|z1kxl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE - Build Release Candidate: 1.18.0-rc2,FLINK-33271,13554028,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jingge,jingge,jingge,13/Oct/23 13:40,24/Oct/23 12:42,04/Jun/24 20:40,24/Oct/23 12:41,1.18.0,,,,,,,,,,,,,,,,,,,,0,,,,,,"The core of the release process is the build-vote-fix cycle. Each cycle produces one release candidate. The Release Manager repeats this cycle until the community approves one release candidate, which is then finalized.
h4. Prerequisites

Set up a few environment variables to simplify Maven commands that follow. This identifies the release candidate being built. Start with {{RC_NUM}} equal to 1 and increment it for each candidate:
{code:java}
RC_NUM=""2""
TAG=""release-${RELEASE_VERSION}-rc${RC_NUM}""
{code}",,,,,,,,,,,,FLINK-32925,FLINK-32921,FLINK-32726,FLINK-32920,FLINK-33347,FLINK-32932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-13 13:40:45.0,,,,,,,,,,"0|z1kxkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Think about installing Java instead of providing it through Docker,FLINK-33270,13554013,13438065,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,13/Oct/23 11:02,31/Oct/23 10:48,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,github-actions,,,,,"The [GitHub Actions docs|https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows] suggest to {{setup-java}} action as it comes with the Maven dependency caching right away. See [actions/setup-java docs|https://github.com/actions/setup-java#caching-packages-dependencies].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 17 10:01:50 UTC 2023,,,,,,,,,,"0|z1kxhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 10:01;mapohl;This issue is not meant as a final decision. There are also good reasons on keeping the docker setup (e.g. reducing the processing time by not installing 3 different Java versions for each job). I created the subtask to document the reasons for (or against) one of the two solutions in a proper way. The argument on caching can be (and is currently) resolved by using {{actions/cache}} on its own.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
light and dark scss files  do not have apache licenses at the top.,FLINK-33269,13554005,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,davidradl,davidradl,davidradl,13/Oct/23 10:24,17/Oct/23 10:18,04/Jun/24 20:40,17/Oct/23 10:18,,,,,,,,,,,,,,,Documentation,Runtime / Web Frontend,,,,,0,,,,,,"I notice that 

[https://github.com/apache/flink-web/blob/asf-site/docs/assets/_code-dark.scss]

and 

[https://github.com/apache/flink-web/blob/asf-site/docs/assets/_code-light.scss]

 

recently added for https://issues.apache.org/jira/browse/FLINK-33046 do not have the Apache  license at the top like the other scss files. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 17 10:18:09 UTC 2023,,,,,,,,,,"0|z1kxfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/23 10:25;davidradl;I can work on this;;;","13/Oct/23 10:38;davidradl;Created pr https://github.com/apache/flink-web/pull/687;;;","17/Oct/23 10:18;mapohl;asf-site: 5731453d9e89a145dce02660a52495ff053aa0ce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink REST API response parsing throws exception on new fields,FLINK-33268,13553979,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,13/Oct/23 07:54,07/May/24 07:33,04/Jun/24 20:40,15/Jan/24 08:55,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,"At the moment Flink is not ignoring unknown fields when parsing REST responses. An example for such a class is JobDetailsInfo but this applies to all others. It would be good to add this support to increase compatibility.

The real life use-case is when the Flink k8s operator wants to handle 2 jobs with 2 different Flink versions where the newer version has added a new field to any REST response. Such case the operator gets an exception when for example it tries to poll the job details with the additional field.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-35302,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 07 07:33:46 UTC 2024,,,,,,,,,,"0|z1kxa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jan/24 08:55;gaborgsomogyi;[{{19cb9de}}|https://github.com/apache/flink/commit/19cb9de5c54b9535be15ca850f5e1ebd2e21c244] on master;;;","28/Apr/24 02:52;Juntao Hu;Sorry to comment on a closed issue, but I wonder why ignoring unknown properties wasn't applied to REST server?

I assume that there can still be a problem, when a newer version client sends requests to an old version server with additional fields in RequestBody that server does not recognize.;;;","06/May/24 07:32;gaborgsomogyi;> I assume that there can still be a problem, when a newer version client sends requests to an old version server with additional fields in RequestBody that server does not recognize.

That's correct. This change is not solving the complete set of combinations just making the client side more flexible. The main use-case what we wanted to fix is the client usage in the operator code. In short the operator uses a client and when received a new feature like slot sharing group information which was added lately then it was blowing up. To overcome this we needed to copy some things from Flink code which is ugly and in mid-long term must be removed, for example: https://github.com/apache/flink-kubernetes-operator/blob/e73363f3486ed9e1df5cc05c9d0baec7c8c3a37f/flink-autoscaler/src/main/java/org/apache/flink/runtime/rest/messages/job/JobDetailsInfo.java#L295;;;","07/May/24 07:33;Juntao Hu;[~gaborgsomogyi]  thanks for replying! I've created an issue FLINK-35302 and [PR|https://github.com/apache/flink/pull/24759] , it would be great if you can review it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PreparedStatement in JDBC Driver,FLINK-33267,13553962,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,FrankZou,FrankZou,FrankZou,13/Oct/23 03:39,23/Oct/23 08:55,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,A PreparedStatement is used to execute parameterized SQL queries. It would be helpful to implement PreparedStatement in JDBC Driver.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 08:54:28 UTC 2023,,,,,,,,,,"0|z1kx68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/23 08:54;libenchao;[~FrankZou] Thanks for your contribution, since this relates to the public API, we may need a FLIP for it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support plan cache for DQL in SQL Gateway,FLINK-33266,13553961,13417633,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,FrankZou,FrankZou,FrankZou,13/Oct/23 03:28,07/Dec/23 06:15,04/Jun/24 20:40,07/Dec/23 06:15,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"In OLAP scenarios, running a single query typically cost hundreds of milliseconds or a few seconds, of which it takes about tens of milliseconds to parse, validate, optimize, and translate it to Flink transformations. Adding cache to cache the transformations corresponding to queries is meaningful for scenarios where certain queries are often executed repeatedly.

We focus on submitting DQL through SQL gateway in this ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 07 06:15:08 UTC 2023,,,,,,,,,,"0|z1kx60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Dec/23 06:15;libenchao;Fixed via b6380d54f11ebb062397d775764f28f20fd35b69 (1.19.0 master)

[~FrankZou] Thanks for your work!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support source parallelism setting for Kafka connector,FLINK-33265,13553953,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,sudewei.sdw,Zhanghao Chen,Zhanghao Chen,13/Oct/23 02:38,05/Feb/24 07:11,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"The kafka connector adaptation work for [FLIP-367: Support Setting Parallelism for Table/SQL Sources - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=263429150].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33261,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 05 07:11:23 UTC 2024,,,,,,,,,,"0|z1kx48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/24 06:25;sudewei.sdw;Hi [~lincoln.86xy]  , I’m willing to doing this, shall I have a try ? ;;;","30/Jan/24 06:35;lincoln.86xy;[~sudewei.sdw] I'm not sure if [~Zhanghao Chen] has started this part, but you can work together, whether it's the code part or the review work, welcome for contributing :);;;","05/Feb/24 06:44;Zhanghao Chen;Hi [~lincoln.86xy], SuDewei will be doing this. Could you help assign it to him?;;;","05/Feb/24 07:11;lincoln.86xy;[~Zhanghao Chen] Assigned to him.  [~sudewei.sdw] Welcome for contributing!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support source parallelism setting for DataGen connector,FLINK-33264,13553952,13553946,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,13/Oct/23 02:38,26/Jan/24 13:34,04/Jun/24 20:40,26/Jan/24 13:34,,,,,,,,,,,1.19.0,,,,Connectors / Parent,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 26 13:34:12 UTC 2024,,,,,,,,,,"0|z1kx40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 07:03;Zhanghao Chen;[~libenchao] Could you assign the task to me?;;;","31/Oct/23 07:09;libenchao;[~Zhanghao Chen] Assigned to you~;;;","26/Jan/24 13:34;libenchao;Implemented via 862a7129d2730b4c70a21826a5b858fc541a4470 (master)

[~Zhanghao Chen] Thanks for the contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement ParallelismProvider for sources in the table planner,FLINK-33263,13553950,13553946,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,sudewei.sdw,Zhanghao Chen,Zhanghao Chen,13/Oct/23 02:33,04/Mar/24 13:19,04/Jun/24 20:40,26/Jan/24 02:17,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jan 26 02:17:29 UTC 2024,,,,,,,,,,"0|z1kx3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 13:06;yunta;[~Zhanghao Chen] Do we still have some specific planner called {{Blink}} planner currently? There is only one table planner now.;;;","15/Nov/23 04:43;Zhanghao Chen;[~yunta] Thanks for pointing out. I'll rephrase it as simply "" the table planner"".;;;","15/Nov/23 08:12;yunta;[~Zhanghao Chen] Thanks for the update. Looking forward to the PR.;;;","10/Jan/24 11:48;sudewei.sdw;Hi [~libenchao] 

I’m willing to doing this, shall I have a try ? ;;;","10/Jan/24 14:36;libenchao;[~sudewei.sdw] Welcome contribution, assigned to you.;;;","26/Jan/24 02:17;libenchao;Implemented via 42b7e74ab20785289b62f5dd68d566995ba9dcfc (master);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend source provider interfaces with the new parallelism provider interface,FLINK-33262,13553947,13553946,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,13/Oct/23 02:32,09/Nov/23 05:16,04/Jun/24 20:40,09/Nov/23 05:16,,,,,,,,,,,1.19.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 05:16:04 UTC 2023,,,,,,,,,,"0|z1kx2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 07:03;Zhanghao Chen;[~libenchao] Could you assign the task to me?;;;","31/Oct/23 07:08;libenchao;[~Zhanghao Chen] assigned to you~;;;","09/Nov/23 05:16;libenchao;merged via 55162dcc5cca6db6aeedddb30d80dd9f9b8d5202 (1.19.0);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-367: Support Setting Parallelism for Table/SQL Sources,FLINK-33261,13553946,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Zhanghao Chen,Zhanghao Chen,13/Oct/23 02:22,22/Feb/24 16:23,04/Jun/24 20:40,01/Feb/24 02:39,,,,,,,,,,,1.19.0,,,,Table SQL / API,,,,,,0,,,,,,"Umbrella issue for [FLIP-367: Support Setting Parallelism for Table/SQL Sources - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=263429150].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33265,FLINK-34500,FLINK-34305,,FLINK-25582,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 01 02:39:43 UTC 2024,,,,,,,,,,"0|z1kx2o:",9223372036854775807,"Scan table sources can now be set a custom parallelism for performance tuning via the ""scan.parallelism"" option. Currently, only the DataGen connector has been adapted to support that, Kafka on the way. Check https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#scan-table-source on how to adapt your custom connectors to it.",,,,,,,,,,,,,,,,,,,"30/Jan/24 03:28;lincoln.86xy;[~Zhanghao Chen]  IIUC, the work on the table/sql part of this flip is already done, can the kafka connector adaptation work be followed up separately (no longer as a subtask, but related one) since the connector repo has already been externalized and has its own release schedule for adpating to new table interfaces/capabilities?
Also we can add the relevant documentation for datagen users and connector developers in this 1.19 release.;;;","30/Jan/24 05:13;Zhanghao Chen;[~lincoln.86xy] Thanks for the attention. I'll transfer the kafka connector adaptation work as a separate task to follow up, and the user-faced doc for datagen is already updated. As for connector deveoper facing docs, I'm not quite familiar with this part of the doc, could you kindly guide in which relevant section of the document we shall update docs?;;;","30/Jan/24 05:38;lincoln.86xy;[~Zhanghao Chen] Thank you for the quick reply!  For the documentation, the 'extension points' on User-defined Sources & Sinks page (https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sourcessinks/#extension-points) maybe a proper place to put, WDYT?;;;","01/Feb/24 02:39;libenchao;Marking the umbrella issue ""resolved"" since all the subtasks has been finished.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Error Handling for Kinesis EFO Consumer,FLINK-33260,13553910,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,iemre,dannycranmer,dannycranmer,12/Oct/23 18:18,04/Jan/24 13:39,04/Jun/24 20:40,04/Jan/24 13:39,aws-connector-4.2.0,,,,,,,,,,aws-connector-4.3.0,,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,,,"Background

The Kinesis Consumer exposes various configuration that allows the user to define retry and backoff strategies when dealing with errors. However, the configuration does not allow the user to configure which errors are retryable, or different strategies for different errors. The error handling logic is hard coded within the connector. Over time we discover errors that should be retryable that are not, for example KDS throwing 500 on SubscribeToShare or transient DNS issues. When these arise the user can either fork-fix the connector or log an issue and wait for the next version.
h3. Scope

Add the ability for the user to define retry/backoff strategy per error. This could be achieved using flexible configuration keys, or allowing the user to register their own retry strategies on the connector

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33991,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 04 13:37:50 UTC 2024,,,,,,,,,,"0|z1kwuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/23 08:29;iemre;Hi, can you please assign this to me?;;;","13/Oct/23 10:19;iemre;Looking at the existing Flink connectors, most don't offer custom retry/error handling per error type. One exception is the Elasticsearch connector:

[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/elasticsearch/] which allows the user to provide a custom class for handling errors.

 ;;;","17/Oct/23 08:59;iemre;The current connector offers configurations like below:

 
{code:java}
flink.shard.getrecords.maxretries
flink.list.shards.maxretries
{code}
 

 

In a similar fashion, I am thinking about a new set of config keys:

 
{code:java}
flink.shard.getrecords.customexceptions=java.lang.Exception;com.mycompany.CustomException
flink.list.shards.customexceptions=java.lang.Exception;com.mycompany.CustomException
 
{code}
 

This would allow the users to re-use the existing retry mechanism with a custom list of exceptions to retry. Note that the user would need to specify the exceptions per operation.

 

If we are going for a custom retry mechanism per exception type, I believe a configuration like below could be a nice user experience:

 

 
{code:java}
flink.shard.getrecords.customexceptions.0.exception=java.lang.Exception
flink.shard.getrecords.customexceptions.0.maxretries=5
flink.shard.getrecords.customexceptions.0.backoff.max=20000
 
flink.shard.getrecords.customexceptions.1.exception=java.lang.Exception
flink.shard.getrecords.customexceptions.1.maxretries=5
flink.shard.getrecords.customexceptions.1.backoff.max=20000
 
{code}
 ;;;","17/Oct/23 12:38;iemre;There appears to be an existing mechanism to distinguish between retriable errors and recoverable errors: [https://github.com/awslabs/amazon-kinesis-connector-flink/blob/master/amazon-kinesis-connector-flink/src/main/java/software/amazon/kinesis/connectors/flink/internals/publisher/fanout/FanOutRecordPublisher.java#L152]

 
 * *Recoverable* errors are retried indefinitely
 * *Retriable* errors use the retry configuration (backoff, no of retries etc)

 

One way to deliver this feature fast/small is to focus on FanOutRecordPublisher, and have 2 configurations along the lines

 

 
{code:java}
custom.recoverable_errors: java.lang.Exception; com.other.CustomException;
custom.retriable_errors: some.AccessDeniedException; some.OtherException
 
{code}
 

Where *retriable* errors would make use of the retry configurations currently present in the codebase, and *recoverable* errors would be retried indefinitely (structure for this is already in place).

 

I'm inclined towards this as the structure for it is already in place allowing faster delivery. And potentially avoiding waste i.e. implementing a more sophisticated approach (previous comment) that users might not actually need.

I will make an attempt at implementing above, focusing initially on Fan Out.

 

 ;;;","07/Nov/23 23:00;dannycranmer;Merged commit [{{4c2fdd9}}|https://github.com/apache/flink-connector-aws/commit/4c2fdd94e27312282e11b17e12f4ec3e5adc113e] into apache:main ;;;","07/Nov/23 23:01;dannycranmer;[~iemre] I have merged the EFO support. Will you be contributing support for polling consumer too? We should use the same config to drive both;;;","22/Nov/23 09:50;iemre;Yes we've merged the EFO support. I will work on the polling consumer too. Thanks [~dannycranmer] for the code review for EFO.;;;","04/Jan/24 13:37;iemre;We can resolve this in favour of https://issues.apache.org/jira/browse/FLINK-33991;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-aws should use/extend the common connector workflow,FLINK-33259,13553905,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,a.pilipenko,hong,hong,12/Oct/23 17:20,16/Apr/24 10:33,04/Jun/24 20:40,,aws-connector-3.0.0,aws-connector-4.2.0,aws-connector-4.3.0,,,,,,,,aws-connector-4.4.0,,,,Connectors / AWS,,,,,,0,,,,,,"We should use the common ci github workflow.
[https://github.com/apache/flink-connector-shared-utils/blob/ci_utils/.github/workflows/ci.yml]

 

Example used in flink-connector-elasticsearch

[https://github.com/apache/flink-connector-elasticsearch/blob/main/.github/workflows/push_pr.yml]

 

This improves our operational stance because we will now inherit any improvements/changes to the main ci workflow file",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-12 17:20:38.0,,,,,,,,,,"0|z1kwtk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Kafka/Gelly module,FLINK-33258,13553880,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,12/Oct/23 14:47,31/Oct/23 10:48,04/Jun/24 20:40,16/Oct/23 09:29,,,,,,,,,,,,,,,,,,,,,0,github-actions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 09:29:12 UTC 2023,,,,,,,,,,"0|z1kwo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:29;mapohl;Change was merged (/);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support filter pushdown in MongoDB connector,FLINK-33257,13553849,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,jiabao.sun,jiabao.sun,jiabao.sun,12/Oct/23 11:10,25/Jan/24 16:16,04/Jun/24 20:40,25/Jan/24 16:16,,,,,,,,,,,mongodb-1.1.0,,,,Connectors / MongoDB,,,,,,0,pull-request-available,,,,,Support filter pushdown in MongoDB connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 25 16:16:53 UTC 2024,,,,,,,,,,"0|z1kwh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 11:11;jiabao.sun;Hi [~Leonard], could you help assign this ticket to me?;;;","12/Oct/23 11:47;leonard;👍🏻，assigned to you [~jiabao.sun];;;","25/Jan/24 16:16;leonard;flink-connector-mongodb(main): fbabbd6b2d535c1c3fe37636e476f1a52990fc48a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fatal error due to dubious ownership in docs-404-check,FLINK-33256,13553848,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,12/Oct/23 11:09,31/Oct/23 10:48,04/Jun/24 20:40,12/Oct/23 11:22,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6468655160/job/17561122928#step:5:5

{code}
fatal: detected dubious ownership in repository at '/__w/flink/flink'
{code}

This doesn't seem to be an issue in other checkouts (e.g. [compile in the same build|https://github.com/XComp/flink/actions/runs/6468655160/job/17561123488#step:5:30] where the directory is added as safe directory temporarily).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 11:22:06 UTC 2023,,,,,,,,,,"0|z1kwgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 11:10;mapohl;There's a issue for that in GitHub actions that describes a workaround: 
https://github.com/actions/checkout/issues/1169;;;","12/Oct/23 11:22;mapohl;This is fixed after the workaround was added to the GHA yaml config of the doc-404-check job: https://github.com/XComp/flink/actions/runs/6470473080/job/17566762062;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate argument count during type inference,FLINK-33255,13553847,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,12/Oct/23 11:08,25/Oct/23 11:26,04/Jun/24 20:40,25/Oct/23 11:26,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Currently we do not validate the argument count in {{TypeInferenceOperandInference}} which results in bugs like e.g. [FLINK-33248]. We do run the check already in {{TypeInferenceUtil}} when running inference for Table API so we should do the same in {{TypeInferenceOperandInference}} case.

We could expose {{TypeInferenceUtil#validateArgumentCount}} and call it. If the check fails, we should not adapt {{operandTypes}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 25 11:26:18 UTC 2023,,,,,,,,,,"0|z1kwgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Oct/23 11:26;dwysakowicz;Improved in b1bbafddc98c76653a5733ec345f79b1ee4eee71;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve speed of compile step,FLINK-33254,13553846,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,12/Oct/23 11:03,31/Oct/23 10:48,04/Jun/24 20:40,15/Oct/23 14:36,,,,,,,,,,,,,,,,,,,,,0,github-actions,,,,,There were issues with the compilation step where I initially thought that it's due to the parallelization of the compilation (which is odd). This issue is about investigating how to do the compilation and forwarding the artifacts in the right way to the downstream jobs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 06:03:33 UTC 2023,,,,,,,,,,"0|z1kwgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 11:43;mapohl;I brought back two threads compilation with [68d67ae|https://github.com/XComp/flink/commit/68d67aee2205f4f76e9df42ad2926f9c48f2053d] in [https://github.com/XComp/flink/actions/runs/6493741592]

It appears that all jobs are failing now within 1-2.;;;","13/Oct/23 07:08;mapohl;I'm analyzing the {{test-compile}} approach again:

{{flink-clients}} failed initially due to {{flink-clients-test-utils not being being built yet and, therefore, couldn't be copied.
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.2.0:copy-dependencies (copy-dependencies) on project flink-clients: Artifact has not been packaged yet. When used on reactor artifact, copy should be executed after packaging: see MDEP-187. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :flink-clients {code};;;","13/Oct/23 10:45;mapohl;Next error was caused by optional dependencies being not included in the build:
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:testCompile (default-testCompile) on project flink-csv: Compilation failure                                                                                                                           
[ERROR] /home/mapohl/workspace/flink-xcomp/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFormatFilesystemStatisticsReportTest.java:[51,29] cannot access org.apache.calcite.schema.Statistic                                                                      
[ERROR]   class file for org.apache.calcite.schema.Statistic not found  {code};;;","15/Oct/23 14:27;mapohl;The issues above are fixed. I tried to introduce caching of the local Maven repository as well to speed up the compilation. This was a bit tricky because the cache action didn't pick up the local folder. The reason for this is the working directory that's used in a Github Actions container setup: By default it will use \{{/__w}} as the working directory within the container which will be mapped to {{/home/runner/work}} in the runner system. We need to use {{/__w}} to refer to the Maven repository within the container to make the caching work (a bit more context can be found [here|https://josh-ops.com/posts/github-container-jobs/]).;;;","16/Oct/23 06:03;mapohl;I also reverted copying the build artifacts and checked the build times:
||Run||Licence Checker||e2e1 compile||e2e2 compile||python||table||connect1||tests||M2 Caching||Build artifact||
|​[Run @5437c3b|https://github.com/XComp/flink/actions/runs/6510559714]|1h 40m​|34m 45s|58m 3s|<error>|<error>|<error>|<error>|(x)|(/)|
|...| | | | | | | | | |
|[Run @54eefec|https://github.com/XComp/flink/actions/runs/6524656877]|40m 19s|7m 40s|8m 17s|<error>|<error>|<error>|<error>|(/)|(/)|
|[Run @4706d3|https://github.com/XComp/flink/actions/runs/6524929361]|56m 50s|6m 46s|7m 45s|<error>|<error>|<error>|<error>|(/)|(/)|
|[Run @d0a31ae|https://github.com/XComp/flink/actions/runs/6525041368]|43m 42s|6m 53s|8m 22s|1h 23m 23s|2h 13m 21s|30m 36s|1h 17m 54s|(/)|(/)|
|[Run @d5e9188|https://github.com/XComp/flink/actions/runs/6525150610]|38m 38s|7m 18s|7m 39s|1h 25m 21s|2h 1m 13s|35m 54s|1h 29m 37s|(/)|(/)|
|[Run @f9d5d1e|https://github.com/XComp/flink/actions/runs/6525266999]|<error>|15m 17s|15m 42s|1h 42m 18s|2h 0m 3s|48m 45s|1h 14m 51s|(/)|(x)|
|[Run @8c71a90|https://github.com/XComp/flink/actions/runs/6525933101]|51m 5s|15m 57s|14m 28s|1h 29m 18s|2h 17m 27s|40m 35s|1h 23m 35s|(/)|(x)|
|[Run @b0ca2d0|https://github.com/XComp/flink/actions/runs/6525957588]|46m 41s|14m 11s|15m 39s|1h 34m 5s|2h 14m 53s|43m 9s|1h 52m 23s|(/)|(x)|
|[Run @f879cf4|https://github.com/XComp/flink/actions/runs/6529672916]|48m 22s|7m 30s|8m 35s|1h 56m 25s|2h 7m 33s|36m 12s|1h 12m 6s|(/)|(/)|
|[Run @21aceb7|https://github.com/XComp/flink/actions/runs/6529754573]|44m 26s|8m 38s|7m 55s|1h 49m 37s|2h 28m 51s|38m 43s|2h 39m 57s|(/)|(/)|

This feature has some effects on the e2e tests, at least. The runtime of the other test modules were to volatile to come up with a conclusion. But it doesn't hurt to utilize the build artifacts there as well. ¯{_}(ツ){_}/¯;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_pyflink.sh fails: zip is missing in Docker image,FLINK-33253,13553845,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,12/Oct/23 11:01,20/Dec/23 12:10,04/Jun/24 20:40,12/Oct/23 11:21,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6459928600/job/17538354449#step:14:7360
{code}
/root/flink/flink-end-to-end-tests/test-scripts/test_pyflink.sh: line 107: zip: command not found
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33904,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 11:21:00 UTC 2023,,,,,,,,,,"0|z1kwg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 11:21;mapohl;Previously, it failed to install the {{.conda}} directory from the {{${TEST_DATA_DIR}/venv.zip}} archive.

This is fixed. The build went on with extracting the folder and starting a Flink cluster: https://github.com/XComp/flink/actions/runs/6468655160/job/17563965337#step:14:7965;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LicenseChecker fails in GHA but succeeds in Azure,FLINK-33252,13553837,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,12/Oct/23 10:04,31/Oct/23 10:47,04/Jun/24 20:40,16/Oct/23 09:07,,,,,,,,,,,,,,,Build System / CI,,,,,,0,github-actions,test-stability,,,,"Both builds are based on [master@011b6b44|https://github.com/apache/flink/commit/011b6b44]:
 * [GitHub Actions|https://github.com/XComp/flink/actions/runs/6487689661/job/17620650207#step:8:41307]
 * [Azure CI|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53624&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=43193]

The GitHub Actions run reports 12 severe issues where it's unclear where they are coming from:
{code:java}
23:30:45,534 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - Found a total of 12 severe license issues {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 09:07:04 UTC 2023,,,,,,,,,,"0|z1kweg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 13:56;mapohl;There are, indeed, 12 issues in GHA which do not show up in the AzureCI run:
{code:java}
13:16:01,120 INFO  org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Problems were detected for a NOTICE file.
    flink-s3-fs-hadoop:
	 These issue are legally problematic and MUST be fixed: 
		Dependency org.apache.hadoop:hadoop-annotations:2.8.3 is not listed.
		Dependency org.apache.hadoop:hadoop-auth:2.8.3 is not listed.
		Dependency org.apache.directory.server:apacheds-kerberos-codec:2.0.0-M15 is not listed.
		Dependency org.apache.directory.server:apacheds-i18n:2.0.0-M15 is not listed.
		Dependency org.apache.directory.api:api-asn1-api:1.0.0-M20 is not listed.
		Dependency org.apache.directory.api:api-util:1.0.0-M20 is not listed.
[...]
13:16:01,125 INFO  org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Problems were detected for a NOTICE file.
    flink-s3-fs-presto:
	 These issue are legally problematic and MUST be fixed: 
		Dependency org.apache.hadoop:hadoop-annotations:2.8.3 is not listed.
		Dependency org.apache.hadoop:hadoop-auth:2.8.3 is not listed.
		Dependency org.apache.directory.server:apacheds-kerberos-codec:2.0.0-M15 is not listed.
		Dependency org.apache.directory.server:apacheds-i18n:2.0.0-M15 is not listed.
		Dependency org.apache.directory.api:api-asn1-api:1.0.0-M20 is not listed.
		Dependency org.apache.directory.api:api-util:1.0.0-M20 is not listed.{code};;;","13/Oct/23 13:07;mapohl;Updating the hadoop version (as suggested by [~chesnay]) didn't help (but should have been the fix). The step still fails. I couldn't reproduce it locally until I noticed that I have Maven 3.6.3 installed but GHA uses the Maven wrapper (i.e. 3.8.6). Switching to the Maven wrapper helped me reproduce it locally.

With Maven 3.8.6 {{fs.hadoopshaded.version}} isn't used for overwriting the hadoop dependency version which then clashes with the version that's listed in the NOTICE file.

Strangely though, we don't run into the same issue with AzureCI where we use the Maven wrapper as well but the step doesn't fail.;;;","13/Oct/23 13:32;mapohl;A renaming of {{hadoop.version}} into {{flink.hadoop.version}} happened in FLINK-28196 that wasn't addressed in the GHA experimental branch. But this doesn't fix the issue, yet, that the dependency version of the filesystem modules isn't overwritten.;;;","16/Oct/23 09:07;mapohl;Fixing the variable name and updating the hadoop version to 2.10.2 was good enough for the LicenseChecker along the one pom file change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client query execution aborts after a few seconds: ConnectTimeoutException,FLINK-33251,13553834,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rmoff,rmoff,12/Oct/23 09:56,28/May/24 10:27,04/Jun/24 20:40,,1.17.1,1.18.0,,,,,,,,,,,,,Table SQL / Client,,,,,,0,,,,,,"If I run a streaming query from an unbounded connector from the SQL Client, it bombs out after ~15 seconds.

{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:52596
{code}

This *doesn't* happen on 1.16.2. It *does* happen on *1.17.1* and *1.18* that I have just built locally (git repo hash `9b837727b6d`). 

The corresponding task's status in the Web UI shows as `CANCELED`. 

---

h2. To reproduce

Launch local cluster and SQL client

{code}
➜  flink-1.18-SNAPSHOT ./bin/start-cluster.sh 
Starting cluster.
Starting standalonesession daemon on host asgard08.
Starting taskexecutor daemon on host asgard08.
➜  flink-1.18-SNAPSHOT ./bin/sql-client.sh
[…]
Flink SQL>
{code}

Set streaming mode and result mode

{code:sql}
Flink SQL> SET 'execution.runtime-mode' = 'STREAMING';
[INFO] Execute statement succeed.

Flink SQL> SET 'sql-client.execution.result-mode' = 'changelog';
[INFO] Execute statement succeed.
{code}

Define a table to read data from CSV files in a folder

{code:sql}
CREATE TABLE firewall (
  event_time STRING,
  source_ip  STRING,
  dest_ip    STRING,
  source_prt INT,
  dest_prt   INT
) WITH (
  'connector' = 'filesystem',
  'path' = 'file:///tmp/firewall/',
  'format' = 'csv',
  'source.monitor-interval' = '1' -- unclear from the docs what the unit is here
);
{code}

Create a CSV file to read in

{code:bash}
$ mkdir /tmp/firewall

$ cat > /tmp/firewall/data.csv <<EOF
2018-05-11 00:19:34,151.35.34.162,125.26.20.222,2014,68
2018-05-11 22:20:43,114.24.126.190,21.68.21.69,379,1619
EOF
{code}

Run a streaming query 

{code}
SELECT * FROM firewall;
{code}

You will get results showing (and if you add another data file it will show up) - but after ~30 seconds the query aborts and throws an error back to the user at the SQL Client prompt

{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:58470

Flink SQL>
{code}","Macbook Pro 

Apple M1 Max
 
{code:java}
$ uname -a
Darwin asgard08 23.0.0 Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:43 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T6000 arm64
{code}

{code:bash}
$ java --version
openjdk 11.0.20.1 2023-08-24
OpenJDK Runtime Environment Homebrew (build 11.0.20.1+0)
OpenJDK 64-Bit Server VM Homebrew (build 11.0.20.1+0, mixed mode)
$ mvn --version
Apache Maven 3.9.5 (57804ffe001d7215b5e7bcb531cf83df38f93546)
Maven home: /opt/homebrew/Cellar/maven/3.9.5/libexec
Java version: 11.0.20.1, vendor: Homebrew, runtime: /opt/homebrew/Cellar/openjdk@11/11.0.20.1/libexec/openjdk.jdk/Contents/Home
Default locale: en_GB, platform encoding: UTF-8
OS name: ""mac os x"", version: ""14.0"", arch: ""aarch64"", family: ""mac""
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/23 09:55;rmoff;log.zip;https://issues.apache.org/jira/secure/attachment/13063512/log.zip","25/May/24 16:16;leekeiabstraction;reproduce_FLINK-33251.java;https://issues.apache.org/jira/secure/attachment/13069109/reproduce_FLINK-33251.java",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 28 10:18:06 UTC 2024,,,,,,,,,,"0|z1kwds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 10:57;Sergey Nuyanzin;I followed the steps on linux (Fedora 38) with jdk8 for flink master (1.19-SNAPSHOT) and it works
I tend to think that this is your env related issues like Mac M1 or something like that;;;","12/Oct/23 12:18;martijnvisser;[~rmoff] Are you on VPN by any chance? ;;;","09/Nov/23 16:35;jcaberio;I was able to replicate this issue on my Mac Mini M2 running Flink 1.17.1

{code}
$ uname -a
Darwin Joricks-Mini.bbrouter 22.5.0 Darwin Kernel Version 22.5.0: Thu Jun  8 22:21:34 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T8112 arm64
{code}


{code:sql}
CREATE TABLE total_amount_table (
  `transaction_id` STRING,
  `transaction_datetime` STRING,
  `amount` DOUBLE
) WITH (
  'connector' = 'kafka',
  'topic' = 'total_amount_table',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'total_amount_table',
  'scan.startup.mode' = 'latest-offset',
  'format' = 'json'
);
{code}

{code:sql}
INSERT INTO total_amount_table VALUES ('txn1', '2023-11-09T16:10:25Z', 125125.125);
{code}

{code:sql}
SELECT * FROM total_amount_table;
{code}


{code:sql}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:50358
{code}
;;;","10/Nov/23 08:17;martijnvisser;[~jcaberio] Are you also on a VPN by any chance? ;;;","10/Nov/23 12:03;jcaberio;No;;;","11/Nov/23 11:05;jcaberio;FWIW I tried replicating this in 1.16 and 1.18.

1.16 works as expected while 1.18 has the same issue.;;;","14/Nov/23 04:15;fsk119;I can not reproduce this problem in my local machine with the steps above.
{code:java}
Darwin B-QB5MMD6M-0305.local 19.6.0 Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64 x86_64
{code}

Could you modify the log level of  FLINK_HOME/conf/log4j-cli.properties to TRACE and then upload the log file?


{code:java}
rootLogger.level = TRACE
{code}
;;;","23/Apr/24 10:53;rmetzger;I'm having this problem with 1.19.0 as well, on a M1 MBP.
The problem is tricky to reproduce (e.g. it doesn't happen all the times)

{code}
2024-04-23 12:51:14,317 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Shutting down rest endpoint.
2024-04-23 12:51:14,317 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache [] - Freed 2 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1
2024-04-23 12:51:14,318 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown complete.
2024-04-23 12:51:14,318 TRACE org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop [] - instrumented a special java.util.Set into: sun.nio.ch.KQueueSelectorImpl@c1d225b
2024-04-23 12:51:14,318 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest client endpoint started.
2024-04-23 12:51:14,318 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.job.coordination.ClientCoordinationRequestBody to localhost:8081/v1/jobs/5d8d1b8ef7dc49381a3855ae10a18ec5/coordinators/b728d985904d42b0fdd945a9e3253fca
2024-04-23 12:51:14,320 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Received response {""serializedCoordinationResult"":""rO0ABXNyAExvcmcuYXBhY2hlLmZsaW5rLnN0cmVhbWluZy5hcGkub3BlcmF0b3JzLmNvbGxlY3QuQ29sbGVjdENvb3JkaW5hdGlvblJlc3BvbnNlAAAAAAAAAAECAANKABZsYXN0Q2hlY2twb2ludGVkT2Zmc2V0TAARc2VyaWFsaXplZFJlc3VsdHN0ABBMamF2YS91dGlsL0xpc3Q7TAAHdmVyc2lvbnQAEkxqYXZhL2xhbmcvU3RyaW5nO3hwAAAAAAAAAABzcgATamF2YS51dGlsLkFycmF5TGlzdHiB0h2Zx2GdAwABSQAEc2l6ZXhwAAAAAHcEAAAAAHh0ACQ3MzgyMjA5Ni0wODE2LTQ5NTMtODA4NC1kMDJhZTg0ZjNhNWU=""}.
2024-04-23 12:51:14,321 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Shutting down rest endpoint.
2024-04-23 12:51:14,321 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache [] - Freed 3 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1
2024-04-23 12:51:14,321 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown complete.
2024-04-23 12:51:14,390 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to localhost:62113/v2/sessions/88b98272-be33-4303-a649-942acd213e84/heartbeat
2024-04-23 12:51:14,391 TRACE org.apache.flink.runtime.rest.FileUploadHandler              [] - Received request. URL:/v2/sessions/88b98272-be33-4303-a649-942acd213e84/heartbeat Method:POST
2024-04-23 12:51:14,391 TRACE org.apache.flink.table.gateway.rest.handler.session.TriggerSessionHeartbeatHandler [] - Received request /v2/sessions/88b98272-be33-4303-a649-942acd213e84/heartbeat.
2024-04-23 12:51:14,391 TRACE org.apache.flink.table.gateway.rest.handler.session.TriggerSessionHeartbeatHandler [] - Starting request processing.
2024-04-23 12:51:14,391 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Received response {}.
2024-04-23 12:51:14,425 TRACE org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop [] - instrumented a special java.util.Set into: sun.nio.ch.KQueueSelectorImpl@1436813
2024-04-23 12:51:14,425 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest client endpoint started.
2024-04-23 12:51:14,426 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to localhost:8081/v1/jobs/5d8d1b8ef7dc49381a3855ae10a18ec5/status
2024-04-23 12:51:14,434 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Received response {""status"":""RUNNING""}.
2024-04-23 12:51:14,435 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Shutting down rest endpoint.
2024-04-23 12:51:14,435 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache [] - Freed 2 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1
2024-04-23 12:51:14,435 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown complete.
2024-04-23 12:51:14,435 TRACE org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop [] - instrumented a special java.util.Set into: sun.nio.ch.KQueueSelectorImpl@64f0cb77
2024-04-23 12:51:14,435 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest client endpoint started.
2024-04-23 12:51:14,436 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.job.coordination.ClientCoordinationRequestBody to localhost:8081/v1/jobs/5d8d1b8ef7dc49381a3855ae10a18ec5/coordinators/b728d985904d42b0fdd945a9e3253fca
2024-04-23 12:51:21,111 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to localhost:62113/v2/sessions/88b98272-be33-4303-a649-942acd213e84/operations/32015c0c-d543-48fc-bec5-2ec8f5ae59e1/close
2024-04-23 12:51:21,113 TRACE org.apache.flink.runtime.rest.FileUploadHandler              [] - Received request. URL:/v2/sessions/88b98272-be33-4303-a649-942acd213e84/operations/32015c0c-d543-48fc-bec5-2ec8f5ae59e1/close Method:DELETE
2024-04-23 12:51:21,113 TRACE org.apache.flink.table.gateway.rest.handler.operation.CloseOperationHandler [] - Received request /v2/sessions/88b98272-be33-4303-a649-942acd213e84/operations/32015c0c-d543-48fc-bec5-2ec8f5ae59e1/close.
2024-04-23 12:51:21,113 TRACE org.apache.flink.table.gateway.rest.handler.operation.CloseOperationHandler [] - Starting request processing.
2024-04-23 12:51:21,113 DEBUG org.apache.flink.table.gateway.service.operation.OperationManager [] - Convert operation 32015c0c-d543-48fc-bec5-2ec8f5ae59e1 from FINISHED to CLOSED.
2024-04-23 12:51:21,114 TRACE org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop [] - instrumented a special java.util.Set into: sun.nio.ch.KQueueSelectorImpl@5679090f
2024-04-23 12:51:21,114 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest client endpoint started.
2024-04-23 12:51:21,114 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to localhost:8081/v1/jobs/5d8d1b8ef7dc49381a3855ae10a18ec5/status
2024-04-23 12:51:21,115 WARN  org.apache.flink.streaming.api.operators.collect.CollectResultFetcher [] - An exception occurred when fetching query results
java.lang.InterruptedException: null
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:386) ~[?:?]
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073) ~[?:?]
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sendRequest(CollectResultFetcher.java:173) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:131) [flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:126) [flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:100) [flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:247) [flink-table-planner_833723f8-2501-4b26-9e61-bfe84f51b9ca.jar:1.19.0]
	at org.apache.flink.table.gateway.service.result.ResultStore$ResultRetrievalThread.run(ResultStore.java:155) [flink-sql-gateway-1.19.0.jar:1.19.0]
2024-04-23 12:51:21,117 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Received response {""status"":""RUNNING""}.
2024-04-23 12:51:21,117 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Shutting down rest endpoint.
2024-04-23 12:51:21,117 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache [] - Freed 2 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1
2024-04-23 12:51:21,117 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown complete.
2024-04-23 12:51:21,118 TRACE org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop [] - instrumented a special java.util.Set into: sun.nio.ch.KQueueSelectorImpl@7de456e9
2024-04-23 12:51:21,118 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest client endpoint started.
2024-04-23 12:51:21,119 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to localhost:8081/v1/jobs/5d8d1b8ef7dc49381a3855ae10a18ec5?mode=cancel
2024-04-23 12:51:21,119 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Received response {""status"":""CLOSED""}.
2024-04-23 12:51:21,125 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Received response {}.
2024-04-23 12:51:21,125 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Shutting down rest endpoint.
2024-04-23 12:51:21,125 DEBUG org.apache.flink.shaded.netty4.io.netty.buffer.PoolThreadCache [] - Freed 2 thread-local buffer(s) from thread: flink-rest-client-netty-thread-1
2024-04-23 12:51:21,125 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Rest endpoint shutdown complete.
2024-04-23 12:51:21,140 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Error while retrieving result.
	at org.apache.flink.table.client.gateway.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:84) ~[flink-sql-client-1.19.0.jar:1.19.0]
Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Failed to get response for the operation 32015c0c-d543-48fc-bec5-2ec8f5ae59e1.
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:489) ~[flink-sql-client-1.19.0.jar:1.19.0]
	at org.apache.flink.table.client.gateway.ExecutorImpl.access$200(ExecutorImpl.java:108) ~[flink-sql-client-1.19.0.jar:1.19.0]
	at org.apache.flink.table.client.gateway.ExecutorImpl$RowDataInfoIterator.fetchResults(ExecutorImpl.java:392) ~[flink-sql-client-1.19.0.jar:1.19.0]
	at org.apache.flink.table.client.gateway.ExecutorImpl$RowDataInfoIterator.hasNext(ExecutorImpl.java:376) ~[flink-sql-client-1.19.0.jar:1.19.0]
	at org.apache.flink.table.client.gateway.StatementResult.hasNext(StatementResult.java:106) ~[flink-sql-client-1.19.0.jar:1.19.0]
	at org.apache.flink.table.client.gateway.result.CollectResultBase$ResultRetrievalThread.run(CollectResultBase.java:80) ~[flink-sql-client-1.19.0.jar:1.19.0]
Caused by: org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:62113
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe$1.run(AbstractNioChannel.java:261) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:153) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[flink-dist-1.19.0.jar:1.19.0]
	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist-1.19.0.jar:1.19.0]
	at java.lang.Thread.run(Thread.java:840) ~[?:?]
2024-04-23 12:51:21,141 DEBUG org.apache.flink.runtime.rest.RestClient                     [] - Sending request of class class org.apache.flink.runtime.rest.messages.EmptyRequestBody to localhost:62113/v2/sessions/88b98272-be33-4303-a649-942acd213e84
2024-04-23 12:51:21,142 TRACE org.apache.flink.runtime.rest.FileUploadHandler              [] - Received request. URL:/v2/sessions/88b98272-be33-4303-a649-942acd213e84 Method:GET
2024-04-23 12:51:21,142 TRACE org.apache.flink.table.gateway.rest.handler.session.GetSessionConfigHandler [] - Received request /v2/sessions/88b98272-be33-4303-a649-942acd213e84.
2024-04-23 12:51:21,143 TRACE org.apache.flink.table.gateway.rest.handler.session.GetSessionConfigHandler [] - Starting request processing.
{code}

;;;","20/May/24 08:19;leekeiabstraction;I've also experience similar issue so adding my observation here. Similar to Robert's observation, it does not happen all the time.

 

Exception seen:

{quote}[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.shaded.netty4.io.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:54658{quote}

Flink environment:

{quote}
Flink SQL Client Version: 1.18.1
Command used to start SQL Client: bin/sql-client.sh --init sqlClientDemoInit

Flink Version: 1.18.1
Command used to start JM: bin/jobmanager.sh start-foreground
Command used to start TM: bin/taskmanager.sh start-foreground{quote}

Machine:
{quote}
  Model Name:    MacBook Pro
  Model Identifier:    MacBookPro18,1
  Model Number:    Z14V000GDB/A
  Chip:    Apple M1 Pro
  Total Number of Cores:    10 (8 performance and 2 efficiency)
  Memory:    32 GB
  System Firmware Version:    10151.101.3
  OS Loader Version:    8422.141.2.700.1
  Serial Number (system):    DW1GQ06D7F
  Hardware UUID:    E41F9063-CF2B-539A-89D4-05E36577A809
  Provisioning UDID:    00006000-000E69EC3602401E
  Activation Lock Status:    Disabled{quote}
;;;","22/May/24 13:25;kwall;I see this issue too on flink-1.19.0.

uname -a
Darwin Oslo.local 22.6.0 Darwin Kernel Version 22.6.0: Mon Feb 19 19:48:53 PST 2024; root:xnu-8796.141.3.704.6~1/RELEASE_X86_64 x86_64

 ;;;","25/May/24 08:35;leekeiabstraction;It seems that reproducing this on M1 Mac is not difficult, also can confirm that Amazon Linux 2023 does NOT run into same issue.

To reproduce, just start sqlClient and run the following. You may or may not get timeout when the DQL initialises, however, if you wait long enough while viewing the results, the issue occurs eventually.

```

CREATE TABLE Orders 
 (order_number BIGINT,price DECIMAL(32,2),
 buyer ROW<first_name STRING, last_name STRING>,
 order_time TIMESTAMP(3)) 
WITH ('connector' = 'datagen');

 

SELECT * FROM Orders;

```

 ;;;","25/May/24 16:18;leekeiabstraction;Adding IntelliJ scratch file that can reliably reproduce: [^reproduce_FLINK-33251.java]
To use this, your run configuration 


1. should use flink-sql-client as classpath module.
2. should define environment variable: FLINK_CONF_DIR=/<path_to_flink>/flink-1.18.1/conf/;;;","28/May/24 10:18;leekeiabstraction;The timeout could come from any of:

1. SQL Gateway not responding/in time to connection

2. Bug in ExecutorImpl

3. Bug in Flink's RestClient

4. Bug in Netty's Bootstrap (unlikely as it is).

I've ran the following looped cURL commands to rule out SQL Gateway and can confirm that connection timeouts are NOT seen.
{quote}SESSION_HANDLE=$(curl --request POST http://localhost:8083/v1/sessions | jq -r .sessionHandle)
OPERATION_HANDLE=$(curl --request POST http://localhost:8083/v1/sessions/${SESSION_HANDLE}/statements/ --data '\{""statement"": ""CREATE TABLE Orders (order_number BIGINT, order_time TIMESTAMP(3)) WITH ('""'connector'""' = '""'datagen'""');""}' | jq -r .operationHandle)
curl --request GET http://localhost:8083/v1/sessions/${SESSION_HANDLE}/operations/${OPERATION_HANDLE}/result/0 | jq .

OPERATION_HANDLE=$(curl --request POST http://localhost:8083/v1/sessions/${SESSION_HANDLE}/statements/ --data '\{""statement"": ""SELECT * FROM Orders;""}' | jq -r .operationHandle)
NEXT_RESULT_URI=$(curl --request GET http://localhost:8083/v1/sessions/${SESSION_HANDLE}/operations/${OPERATION_HANDLE}/result/0 | jq -r .nextResultUri)

while true; do echo $NEXT_RESULT_URI; NEXT_RESULT_URI_TEMP=$(curl --request GET http://localhost:8083/${NEXT_RESULT_URI} | jq -r .nextResultUri); NEXT_RESULT_URI=$NEXT_RESULT_URI_TEMP; done
{quote}
{{So I think the bug is in either ExecutorImpl or RestClient or Netty's bootstrap class.}}

{{{}One interesting observation I have is the fetchResultWithInterval flag which controls if a 100ms sleep is added{}}}{{{}: [https://github.com/apache/flink/blob/master/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/ExecutorImpl.java#L469-L471]{}}}

 
{quote}{{    private FetchResultsResponseBody getFetchResultResponse(}}
{{            OperationHandle operationHandle,}}
{{            long token,}}
{{            boolean fetchResultWithInterval,}}
{{            Function<InterruptedException, SqlExecutionException> interruptedExceptionHandler) {}}
{{        try {}}
{{            if (fetchResultWithInterval) {}}
{{                Thread.sleep(100);}}

}
{quote}
 

{{[~fsk119] can you elaborate on why the interval is necessary? As the other observation that I have is that the ConnectTimeoutException seems to correlate with how quickly we call getFetchResultResponse.}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase Connector should directly depend on 3rd-party libs instead of flink-shaded repo,FLINK-33250,13553805,13552952,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,12/Oct/23 07:51,12/Oct/23 13:38,04/Jun/24 20:40,12/Oct/23 13:38,,,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 13:38:06 UTC 2023,,,,,,,,,,"0|z1kw7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 13:38;martijnvisser;Fixed in apache/flink-connector-hbase@main 5e81d4b788bb0b5c8a9f0115723a30a0754ab97a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
comment should be parsed by StringLiteral() instead of SqlCharStringLiteral to avoid parsing failure,FLINK-33249,13553782,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,12/Oct/23 03:38,19/Oct/23 11:30,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"this problem is also recorded in calcite

 

https://issues.apache.org/jira/browse/CALCITE-6046

 

Hi, I found this problem when I used below code to split SQL statements. the process is SQL string -> SqlNode -> SQL String
{code:java}
// code placeholder
SqlParser.Config parserConfig = getCurrentSqlParserConfig(sqlDialect);
SqlParser sqlParser = SqlParser.create(sqlContent, parserConfig);
SqlNodeList sqlNodeList = sqlParser.parseStmtList(); 

sqlParser.parse(sqlNodeList.get(0));{code}
the Dialect/ SqlConformance is a costumed one:

[https://github.com/apache/flink/blob/master/flink-table/flink-sql-parser/src/main/java/org/apache/flink/sql/parser/validate/FlinkSqlConformance.java]

 

 

then I found below SQL
{code:java}
// code placeholder
CREATE TABLE source (
    a BIGINT
) comment '测试test'
WITH (
  'connector' = 'test'
);  {code}
transformed to
{code:java}
// code placeholder
CREATE TABLE `source` (
  `a` BIGINT
)
COMMENT u&'\5218\51eftest' WITH (
  'connector' = 'test'
)  {code}
 

and the SQL parser template is like
{code:java}
// code placeholder
SqlCreate SqlCreateTable(Span s, boolean replace, boolean isTemporary) :
{
    final SqlParserPos startPos = s.pos();
    boolean ifNotExists = false;
    SqlIdentifier tableName;
    List<SqlTableConstraint> constraints = new ArrayList<SqlTableConstraint>();
    SqlWatermark watermark = null;
    SqlNodeList columnList = SqlNodeList.EMPTY;
   SqlCharStringLiteral comment = null;
   SqlTableLike tableLike = null;
    SqlNode asQuery = null;

    SqlNodeList propertyList = SqlNodeList.EMPTY;
    SqlNodeList partitionColumns = SqlNodeList.EMPTY;
    SqlParserPos pos = startPos;
}
{
    <TABLE>

    ifNotExists = IfNotExistsOpt()

    tableName = CompoundIdentifier()
    [
        <LPAREN> { pos = getPos(); TableCreationContext ctx = new TableCreationContext();}
        TableColumn(ctx)
        (
            <COMMA> TableColumn(ctx)
        )*
        {
            pos = pos.plus(getPos());
            columnList = new SqlNodeList(ctx.columnList, pos);
            constraints = ctx.constraints;
            watermark = ctx.watermark;
        }
        <RPAREN>
    ]
    [ <COMMENT> <QUOTED_STRING> {
        String p = SqlParserUtil.parseString(token.image);
        comment = SqlLiteral.createCharString(p, getPos());
    }]
    [
        <PARTITIONED> <BY>
        partitionColumns = ParenthesizedSimpleIdentifierList()
    ]
    [
        <WITH>
        propertyList = TableProperties()
    ]
    [
        <LIKE>
        tableLike = SqlTableLike(getPos())
        {
            return new SqlCreateTableLike(startPos.plus(getPos()),
                tableName,
                columnList,
                constraints,
                propertyList,
                partitionColumns,
                watermark,
                comment,
                tableLike,
                isTemporary,
                ifNotExists);
        }
    |
        <AS>
        asQuery = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY)
        {
            return new SqlCreateTableAs(startPos.plus(getPos()),
                tableName,
                columnList,
                constraints,
                propertyList,
                partitionColumns,
                watermark,
                comment,
                asQuery,
                isTemporary,
                ifNotExists);
        }
    ]
    {
        return new SqlCreateTable(startPos.plus(getPos()),
            tableName,
            columnList,
            constraints,
            propertyList,
            partitionColumns,
            watermark,
            comment,
            isTemporary,
            ifNotExists);
    }
} {code}
will give a exception :

Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered ""u&\'\\5218\\51eftest\'"" at line 4, column 9.
Was expecting:
    <QUOTED_STRING> ...

 

so I think all the SqlCharStringLiteral should be replaced by StringLiteral()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-6046,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 19 11:30:07 UTC 2023,,,,,,,,,,"0|z1kw28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 06:52;zhoujira86;I'd like to take this ticket;;;","12/Oct/23 08:43;martijnvisser;[~zhoujira86] This should only be fixed when it's fixed in Calcite, not first in Flink and then Calcite later;;;","12/Oct/23 10:49;zhoujira86;[~martijnvisser]  Hi Martin, I am not sure whether this is something to be fixed in calcite. As 
SqlCreateTable
template is in flink parser. 

I have attached a url, would you please have a glance at it?;;;","12/Oct/23 11:18;martijnvisser;I would wait until the Calcite community has given a conclusion to the ticket you opened ;;;","12/Oct/23 11:21;martijnvisser;[~zhoujira86] The PR wouldn't get merged anyway, since it doesn't have a test for it. It would be better if you could have a test that displays the issue, and then you create a proposal to fix it;;;","12/Oct/23 12:10;zhoujira86;[~martijnvisser] Sure thing, Let me provide a test to demonstrate the issue.;;;","19/Oct/23 05:40;zhoujira86;Hi [~martijnvisser]

https://issues.apache.org/jira/browse/CALCITE-6001 will improve CALCITE to omit the charset from the generated literal when it is the default charset of the DIALECT. maybe wait for the CALCITE future version and set the FLINK DIALECT to use UTF-8;;;","19/Oct/23 11:30;martijnvisser;[~zhoujira86] Thanks, let's wait for the Calcite feature indeed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling CURRENT_WATERMARK without parameters gives IndexOutOfBoundsException,FLINK-33248,13553759,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,bvarghese,bvarghese,bvarghese,11/Oct/23 21:23,12/Oct/23 10:50,04/Jun/24 20:40,12/Oct/23 10:50,,,,,,,,,,,1.19.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"Create a table
{code:java}
Flink SQL> CREATE TABLE T (ts TIMESTAMP(3)) WITH ('connector'='values', 'bounded'='true');
[INFO] Execute statement succeed. {code}
Select CURRENT_WATERMARK without parameters
{code:java}
Flink SQL> SELECT ts, CURRENT_WATERMARK() FROM T;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IndexOutOfBoundsException: Index 0 out of bounds for length 0 {code}
Logs from sql-client:
{code:java}
2023-10-11 14:58:20,576 ERROR org.apache.flink.table.gateway.service.operation.OperationManager [] - Failed to execute the operation cc8d29d3-e6ad-428a-92ea-b0a2a72c25f9.
org.apache.flink.table.api.ValidationException: SQL validation failed. Unexpected error in type inference logic of function 'CURRENT_WATERMARK'. This is a bug.
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:200) ~[?:?]
    at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:117) ~[?:?]
    at org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:261) ~[?:?]
    at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:187) ~[flink-sql-gateway-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
    at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
    at java.lang.Thread.run(Thread.java:829) [?:?] {code}
 

{*}Expected Behavior{*}:
It should return a SqlValidatorException: No match found for function signature CURRENT_WATERMARK()

This is inline with other functions which expects a parameter
Example:
{code:java}
Flink SQL> SELECT ARRAY_JOIN();
[ERROR] Could not execute SQL statement. Reason:
org.apache.calcite.sql.validate.SqlValidatorException: No match found for function signature ARRAY_JOIN(){code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 10:50:47 UTC 2023,,,,,,,,,,"0|z1kvx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 10:50;dwysakowicz;Fixed in master: 35a2257af5d0cf9e671460b9770f1363b7a3d60c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalArgumentException in NoticeFileChecker,FLINK-33247,13553730,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,11/Oct/23 17:13,31/Oct/23 10:47,04/Jun/24 20:40,12/Oct/23 09:58,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"{code}
2023-10-11T15:55:54.7616189Z 15:55:54.760 [INFO] --- exec-maven-plugin:3.1.0:java (default-cli) @ flink-ci-tools ---
2023-10-11T15:55:55.2153082Z 15:55:55,212 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - THIS UTILITY IS ONLY CHECKING FOR COMMON LICENSING MISTAKES. A MANUAL CHECK OF THE NOTICE FILES, DEPLOYED ARTIFACTS, ETC. IS STILL NEEDED!
2023-10-11T15:55:55.2217611Z 15:55:55,221 DEBUG org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Loaded 3 items from resource modules-defining-excess-dependencies.modulelist
2023-10-11T15:55:58.1540870Z 15:55:58,153 INFO  org.apache.flink.tools.ci.licensecheck.NoticeFileChecker     [] - Extracted 128 modules that were deployed and 174 modules which bundle dependencies with a total of 174 dependencies
2023-10-11T15:55:58.1700608Z 15:55:58.162 [WARNING] 
2023-10-11T15:55:58.1701401Z java.lang.IllegalArgumentException
2023-10-11T15:55:58.1702263Z     at sun.nio.fs.UnixPath.getName (UnixPath.java:334)
2023-10-11T15:55:58.1702685Z     at sun.nio.fs.UnixPath.getName (UnixPath.java:43)
2023-10-11T15:55:58.1703278Z     at org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.lambda$findNoticeFiles$10 (NoticeFileChecker.java:362)
2023-10-11T15:55:58.1703907Z     at java.util.stream.ReferencePipeline$2$1.accept (ReferencePipeline.java:176)
2023-10-11T15:55:58.1704412Z     at java.util.stream.ReferencePipeline$3$1.accept (ReferencePipeline.java:195)
2023-10-11T15:55:58.1704877Z     at java.util.Iterator.forEachRemaining (Iterator.java:133)
2023-10-11T15:55:58.1705356Z     at java.util.Spliterators$IteratorSpliterator.forEachRemaining (Spliterators.java:1801)
2023-10-11T15:55:58.1705902Z     at java.util.stream.AbstractPipeline.copyInto (AbstractPipeline.java:484)
2023-10-11T15:55:58.1706457Z     at java.util.stream.AbstractPipeline.wrapAndCopyInto (AbstractPipeline.java:474)
2023-10-11T15:55:58.1706975Z     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential (ReduceOps.java:913)
2023-10-11T15:55:58.1707485Z     at java.util.stream.AbstractPipeline.evaluate (AbstractPipeline.java:234)
2023-10-11T15:55:58.1708005Z     at java.util.stream.ReferencePipeline.collect (ReferencePipeline.java:578)
2023-10-11T15:55:58.1708667Z     at org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.findNoticeFiles (NoticeFileChecker.java:366)
2023-10-11T15:55:58.1709374Z     at org.apache.flink.tools.ci.licensecheck.NoticeFileChecker.run (NoticeFileChecker.java:91)
2023-10-11T15:55:58.1710011Z     at org.apache.flink.tools.ci.licensecheck.LicenseChecker.main (LicenseChecker.java:42)
2023-10-11T15:55:58.1710548Z     at org.codehaus.mojo.exec.ExecJavaMojo$1.run (ExecJavaMojo.java:279)
2023-10-11T15:55:58.1710929Z     at java.lang.Thread.run (Thread.java:829)
{code}
https://github.com/XComp/flink/actions/runs/6483465871/job/17607225139#step:8:41046",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 11:25:28 UTC 2023,,,,,,,,,,"0|z1kvqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 11:25;mapohl;This is fixed by filtering out resources that less than two parent folders: https://github.com/XComp/flink/actions/runs/6485902212/job/17614369586#step:8:41045;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add RescalingIT case that uses checkpoints and resource requests,FLINK-33246,13553714,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,srichter,srichter,11/Oct/23 14:40,02/Feb/24 05:10,04/Jun/24 20:40,11/Dec/23 09:55,,,,,,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,"RescalingITCase currently uses savepoints and cancel/restart for rescaling. We should add a test that also tests rescaling from checkpoints under changing resource requirements, i.e. without cancelation of the job.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34200,FLINK-34336,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 11 09:55:12 UTC 2023,,,,,,,,,,"0|z1kvn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Dec/23 09:55;yunta; merged in master 98e4610f09f35a942e55472b5d358ebe113b0dba;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
finegrained_resource_management module failed,FLINK-33245,13553713,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,mapohl,mapohl,11/Oct/23 14:31,31/Oct/23 10:47,04/Jun/24 20:40,16/Oct/23 09:28,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"||Run||Runtime||Outcome||
|[run #15|https://github.com/XComp/flink/actions/runs/6473584177/job/17581943111]|3h 58m 17s|* timed out with thread dump
|
|[run #14|https://github.com/XComp/flink/actions/runs/6472816505/job/17575963787]|3h 36m 56s|* {{NoSuchMethodError}} in {{HAJobRunOnHadoopS3FileSystemITCase}}
|
|[run #13|https://github.com/XComp/flink/actions/runs/6472726326/job/17575765131]|58m 8s|* fatal error in {{ZooKeeperLeaderElectionConnectionHandlingTest}}
|
|[run #12|https://github.com/XComp/flink/actions/runs/6471693368/job/17575340696]|3h 58m 20s|* timed out with thread dump
|
|[run #11|https://github.com/XComp/flink/actions/runs/6471147857/job/17571310183]|3h 48m 54s|* {{NoSuchMethodError}} in {{HAJobRunOnHadoopS3FileSystemITCase}}
|
|[run #10|https://github.com/XComp/flink/actions/runs/6470473080/job/17569261725]|3h 58m 20s|* timed out with thread dump
|
|[run #9|https://github.com/XComp/flink/actions/runs/6468655160/job/17563927249]|1h 52m 35s|* 100ms timeout occurred in {{MiniClusterITCase.testHandleStreamingJobsWhenNotEnoughSlot}}
|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 09:28:43 UTC 2023,,,,,,,,,,"0|z1kvmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 09:28;mapohl;This won't be investigated. The dedicated {{finegraned_resource_management}} stage was removed because the fine-grained ResourceManager became the default implementation in FLINK-31448.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not Able To Pass the Configuration On Flink Session,FLINK-33244,13553706,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,ZhenqiuHuang,amar1509,amar1509,11/Oct/23 13:49,19/Feb/24 15:28,04/Jun/24 20:40,,kubernetes-operator-1.5.0,,,,,,,,,,1.17.1,,,,flink-contrib,Kubernetes Operator,,,,,0,,,,,,"Hi 
I have tried configuring the flink run -D like 

-Dmetrics.reporter=promgateway\
-Dmetrics.reporter.promgateway.jobName: flink_test_outside

these configuration .

And Same is for FLink Kubernetive Operator
Not able to Configure KuberConfiguraton using 


apiVersion: flink.apache.org/v1beta1
kind: FlinkSessionJob
metadata:
name: flink-job-test
spec:
deploymentName: flink-session-cluster
restartNonce: 11
flinkConfiguration:
# Flink Config Overrides
kubernetes.operator.job.restart.failed: ""true""
metrics.reporters: ""promgateway""
metrics.reporter.promgateway.jobName: ""flink_test_outside""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 19 11:43:53 UTC 2024,,,,,,,,,,"0|z1kvlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Feb/24 11:43;ZhenqiuHuang;[~amar1509]
Is it still an issue for the latest operator version?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.scalafmt.conf cannot be found in Test packaging/licensing job,FLINK-33243,13553705,13438065,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,11/Oct/23 13:46,31/Oct/23 10:47,04/Jun/24 20:40,11/Oct/23 17:11,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,https://github.com/XComp/flink/actions/runs/6473584177/job/17581941684#step:8:4327,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 14:37:17 UTC 2023,,,,,,,,,,"0|z1kvl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 14:37;mapohl;Initial proposed command didn't work in the runner (even though it worked locally using {{bash}}):
{code}
mv ./{*,.[^.]* {{ env.CHECKOUT_DIR }}
{code}  https://github.com/XComp/flink/actions/runs/6483187711;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
misc module: YARN tests are flaky,FLINK-33242,13553700,13438065,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,mapohl,mapohl,mapohl,11/Oct/23 13:29,27/Nov/23 08:42,04/Jun/24 20:40,27/Nov/23 08:40,,,,,,,,,,,,,,,,,,,,,0,github-actions,test-stability,,,,"https://github.com/XComp/flink/actions/runs/6473584177/job/17581942919

{code}
2023-10-10T23:16:09.3548634Z Oct 10 23:16:09 23:16:09.354 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.664 s <<< FAILURE! - in org.apache.flink.yarn.YarnPrioritySchedulingITCase
2023-10-10T23:16:09.3564980Z Oct 10 23:16:09 23:16:09.354 [ERROR] org.apache.flink.yarn.YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority  Time elapsed: 1.226 s  <<< ERROR!
2023-10-10T23:16:09.3565608Z Oct 10 23:16:09 java.lang.RuntimeException: Runner failed with exception.
2023-10-10T23:16:09.3566290Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase.startWithArgs(YarnTestBase.java:949)
2023-10-10T23:16:09.3566954Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnPrioritySchedulingITCase.lambda$yarnApplication_submissionWithPriority_shouldRespectPriority$0(YarnPrioritySchedulingITCase.java:45)
2023-10-10T23:16:09.3567646Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:09.3568447Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority(YarnPrioritySchedulingITCase.java:41)
2023-10-10T23:16:09.3569187Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:09.3569805Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:09.3570485Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:09.3571052Z Oct 10 23:16:09 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:09.3571527Z Oct 10 23:16:09 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:09.3572075Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:09.3572716Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:09.3573350Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:09.3573954Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:09.3574665Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:09.3575378Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:09.3576139Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:09.3576852Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:09.3577539Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:09.3578225Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:09.3578898Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:09.3579568Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:09.3580243Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:09.3580917Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:09.3581584Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3582276Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:09.3582952Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:09.3583651Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:09.3584282Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:09.3584914Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3585589Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3586368Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3587030Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3587749Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3588550Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3589237Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3590109Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3591187Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:09.3592081Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:09.3592840Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3593551Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3594182Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3594898Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3595600Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3596324Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3596994Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3597858Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3598950Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:09.3599832Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:09.3600565Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3601268Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3601893Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3602519Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3603281Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3604234Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3604926Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3605850Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3606648Z Oct 10 23:16:09 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:09.3607160Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:09.3607702Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:09.3608332Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:09.3608855Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:09.3609416Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:09.3610332Z Oct 10 23:16:09 Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
2023-10-10T23:16:09.3611024Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:479)
2023-10-10T23:16:09.3611663Z Oct 10 23:16:09 	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:604)
2023-10-10T23:16:09.3612282Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase$Runner.run(YarnTestBase.java:1154)
2023-10-10T23:16:09.3613022Z Oct 10 23:16:09 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:09.3613849Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:09.3614501Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:09.3615153Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:472)
2023-10-10T23:16:09.3615632Z Oct 10 23:16:09 	... 2 more
2023-10-10T23:16:09.3615862Z Oct 10 23:16:09 
2023-10-10T23:16:10.1130633Z Oct 10 23:16:10 23:16:10.112 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-10-10T23:16:11.1935140Z Oct 10 23:16:11 23:16:11.192 [INFO] Running org.apache.flink.yarn.YARNApplicationITCase
2023-10-10T23:16:17.2611617Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2612300Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2612901Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2613468Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2023-10-10T23:16:17.2614031Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2614540Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2615047Z Oct 10, 2023 11:16:13 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2615720Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2616208Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2617123Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2617787Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2618489Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2619296Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2619952Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2620582Z Oct 10, 2023 11:16:14 PM com.google.inject.servlet.GuiceFilter setPipeline
2023-10-10T23:16:17.2621470Z WARNING: Multiple Servlet injectors detected. This is a warning indicating that you have more than one GuiceFilter running in your web application. If this is deliberate, you may safely ignore this message. If this is NOT deliberate however, your application may not work as expected.
2023-10-10T23:16:17.2622217Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2622759Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
2023-10-10T23:16:17.2623380Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2623919Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2624481Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2625294Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2625850Z Oct 10, 2023 11:16:14 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2626484Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2627039Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2627774Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2628487Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2629143Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2629800Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2630486Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2631081Z Oct 10, 2023 11:16:14 PM com.google.inject.servlet.GuiceFilter setPipeline
2023-10-10T23:16:17.2631822Z WARNING: Multiple Servlet injectors detected. This is a warning indicating that you have more than one GuiceFilter running in your web application. If this is deliberate, you may safely ignore this message. If this is NOT deliberate however, your application may not work as expected.
2023-10-10T23:16:17.2632613Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2633238Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
2023-10-10T23:16:17.2633853Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2634431Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2638068Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2638660Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2639242Z Oct 10, 2023 11:16:14 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2639820Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2640381Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2641137Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2641897Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2642619Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2643219Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2644145Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2646774Z Oct 10 23:16:17 Formatting using clusterid: testClusterID
2023-10-10T23:16:18.7266553Z Oct 10 23:16:18 23:16:18.726 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 7.525 s <<< FAILURE! - in org.apache.flink.yarn.YARNApplicationITCase
2023-10-10T23:16:18.7276855Z Oct 10 23:16:18 23:16:18.726 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion  Time elapsed: 0.242 s  <<< ERROR!
2023-10-10T23:16:18.7278287Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7279023Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7279745Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7280549Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1(YARNApplicationITCase.java:72)
2023-10-10T23:16:18.7281269Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7282068Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion(YARNApplicationITCase.java:70)
2023-10-10T23:16:18.7282892Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7283646Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7284329Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7284895Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7285448Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7286066Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7286781Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7287482Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7288165Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7288983Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7289779Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7290658Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7291445Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7292193Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7292954Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7293781Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7294515Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7295309Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7296068Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7296820Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7298389Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7299074Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7299710Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7300336Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7300981Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7301613Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7302172Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7302727Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7303360Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7304014Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7304630Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7305425Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7306495Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7307582Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7308509Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7309149Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7309816Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7310378Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7310936Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7311567Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7312301Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7312921Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7313708Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7314698Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7315498Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7316174Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7316814Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7317364Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7317914Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7318538Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7319185Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7319804Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7320583Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7321309Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7321766Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7322238Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7335962Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7336477Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7336970Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7337680Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7338554Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7339149Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7339757Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7340170Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7340437Z Oct 10 23:16:18 
2023-10-10T23:16:18.7344548Z Oct 10 23:16:18 23:16:18.732 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar  Time elapsed: 0.554 s  <<< ERROR!
2023-10-10T23:16:18.7345418Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7346043Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7346765Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7347401Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithRemoteUserJar$2(YARNApplicationITCase.java:86)
2023-10-10T23:16:18.7347951Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7348549Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar(YARNApplicationITCase.java:84)
2023-10-10T23:16:18.7349174Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7349713Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7350446Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7350937Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7351416Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7351965Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7352606Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7353236Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7353840Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7354480Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7355203Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7355951Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7356667Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7357354Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7358040Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7358719Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7359434Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7360111Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7360797Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7361504Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7362189Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7362863Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7363762Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7364405Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7365043Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7365673Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7366233Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7366788Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7367690Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7368345Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7368964Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7369752Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7370829Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7371914Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7372743Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7373385Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7374007Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7374561Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7375120Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7375746Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7376395Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7377016Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7377868Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7378859Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7379707Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7380337Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7380964Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7381523Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7382136Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7382756Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7383402Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7384019Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7384791Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7385545Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7386000Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7386477Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7386946Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7399865Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7400385Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7401100Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7401860Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7402463Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7403060Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7404050Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7404263Z Oct 10 23:16:18 
2023-10-10T23:16:18.7404867Z Oct 10 23:16:18 23:16:18.737 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion  Time elapsed: 0.006 s  <<< ERROR!
2023-10-10T23:16:18.7405830Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7406446Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7407060Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7407923Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0(YARNApplicationITCase.java:62)
2023-10-10T23:16:18.7408561Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7409274Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion(YARNApplicationITCase.java:60)
2023-10-10T23:16:18.7410056Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7410601Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7411193Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7411685Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7412290Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7412842Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7413472Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7414100Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7414702Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7415340Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7416112Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7416861Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7417556Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7418232Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7418920Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7419588Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7420253Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7420937Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7421621Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7422293Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7422964Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7423638Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7424278Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7424956Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7425602Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7426235Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7426860Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7427420Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7428035Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7428687Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7429351Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7430139Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7431226Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7432309Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7433117Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7433795Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7434496Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7435049Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7435604Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7436234Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7436874Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7437499Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7438284Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7439272Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7440071Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7440681Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7441310Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7441865Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7442412Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7443068Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7448970Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7449605Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7450536Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7451260Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7451723Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7452269Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7452735Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7453187Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7453670Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7454381Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7455128Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7455775Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7456391Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7456805Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7457010Z Oct 10 23:16:18 
2023-10-10T23:16:19.0886760Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0887052Z Oct 10 23:16:19 23:16:19.087 [INFO] Results:
2023-10-10T23:16:19.0887292Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0887538Z Oct 10 23:16:19 23:16:19.087 [ERROR] Errors: 
2023-10-10T23:16:19.0890525Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion:70->YarnTestBase.runTest:303->lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1:72->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0892075Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion:60->YarnTestBase.runTest:303->lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0:62->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0893304Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithRemoteUserJar:84->YarnTestBase.runTest:303->lambda$testApplicationClusterWithRemoteUserJar$2:86->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0894114Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNITCase.testPerJobWithProvidedLibDirs » Remote File /flink-provided-lib/fli...
2023-10-10T23:16:19.0895062Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority:41->YarnTestBase.runTest:303->lambda$yarnApplication_submissionWithPriority_shouldRespectPriority$0:45->YarnTestBase.startWithArgs:949 » Runtime
2023-10-10T23:16:19.0895643Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0895944Z Oct 10 23:16:19 23:16:19.087 [ERROR] Tests run: 27, Failures: 0, Errors: 5, Skipped: 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 27 08:40:42 UTC 2023,,,,,,,,,,"0|z1kvk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 14:22;mapohl;The tests are not consistently failing:
||Run||Runtime||Outcome||
|[run #15|https://github.com/XComp/flink/actions/runs/6473584177/job/17581942919]|1h 13m 58s| * YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion
 * YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion
 * YARNApplicationITCase.testApplicationClusterWithRemoteUserJar
 * YARNITCase.testPerJobWithProvidedLibDirs
 * YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority|
|[run #14|https://github.com/XComp/flink/actions/runs/6472816505/job/17575963464]|1h 0m 17s| * YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion
 * YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion
 * YARNApplicationITCase.testApplicationClusterWithRemoteUserJar
 * YARNITCase.testPerJobWithProvidedLibDirs|
|[run #13|https://github.com/XComp/flink/actions/runs/6472726326/job/17575764870]|5h 36m 52s| * Timed out without any logs being preserved|
|[run #12|https://github.com/XComp/flink/actions/runs/6471693368/job/17575340475]|1h 5m 24s| * Successfully completed run|
|[run #11|https://github.com/XComp/flink/actions/runs/6471147857/job/17571309709]|56m 20s| * Successfully completed run|
|[run #10|https://github.com/XComp/flink/actions/runs/6470473080/job/17569261115]|5h 9m 51s| * Timed out without any logs being preserved|
|[run #9|https://github.com/XComp/flink/actions/runs/6468655160/job/17563926990]|1h 19m 18s| * YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion|;;;","16/Oct/23 09:37;mapohl;https://github.com/XComp/flink/actions/runs/6525933101/job/17719334431#step:10:28465;;;","16/Oct/23 09:37;mapohl;https://github.com/XComp/flink/actions/runs/6525957588/job/17719339975#step:10:28037;;;","16/Oct/23 09:40;mapohl;https://github.com/XComp/flink/actions/runs/6525266999/job/17718032661#step:10:28137;;;","16/Oct/23 09:41;mapohl;https://github.com/XComp/flink/actions/runs/6525150610/job/17717679179#step:12:27655;;;","16/Oct/23 09:43;mapohl;https://github.com/XComp/flink/actions/runs/6525041368/job/17717450135#step:12:27614;;;","31/Oct/23 15:34;mapohl;Interestingly, we have a build where the yarn tests didn't fail: https://github.com/XComp/flink/actions/runs/6707387404/job/18226397885;;;","27/Nov/23 08:40;mapohl;The issue didn't reappear for a while. I'm closing it as not-reproducible to track that there's no effort put into investigating it further for now. We can re-open this Jira again if the problem re-occurs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align config option generation documentation for Flink's config documentation,FLINK-33241,13553680,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,mapohl,mapohl,11/Oct/23 11:58,20/Feb/24 11:41,04/Jun/24 20:40,20/Feb/24 11:41,1.19.0,,,,,,,,,,1.19.0,,,,Documentation,,,,,,0,pull-request-available,starter,,,,"The configuration parameter docs generation is documented in two places in different ways:
[docs/README.md:62|https://github.com/apache/flink/blob/5c1e9f3b1449cb77276d578b344d9a69c7cf9a3c/docs/README.md#L62] and [flink-docs/README.md:44|https://github.com/apache/flink/blob/7bebd2d9fac517c28afc24c0c034d77cfe2b43a6/flink-docs/README.md#L44].

We should remove the corresponding command from {{docs/README.md}} and refer to {{flink-docs/README.md}} for the documentation. That way, we only have to maintain a single file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 20 11:41:49 UTC 2024,,,,,,,,,,"0|z1kvfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 03:09;JunRuiLi;Hi, [~mapohl], thanks for reporting this issue, I'd like to take this ticket. Could you help to assign it to me?;;;","12/Oct/23 07:53;mapohl;I assigned the issue to you. Thanks for your help.;;;","20/Feb/24 11:41;zhuzh;Fixed via 8fe005cf1325b7477d7e1808a46bd80798165029;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate docs for deprecated options as well,FLINK-33240,13553678,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,Zhanghao Chen,Zhanghao Chen,11/Oct/23 11:20,05/Feb/24 06:46,04/Jun/24 20:40,05/Feb/24 06:46,,,,,,,,,,,,,,,Documentation,,,,,,0,,,,,,"Currently, Flink will skip doc generation for deprecated options (See 
{{{}ConfigOptionsDocGenerator#{}}}{{{}shouldBeDocumented{}}}). As a result, the deprecated options can no longer be found in the new version of Flink document. This might confuse users upgrading from an older version of Flink and they have to either carefully read the release notes or check the source code for upgrading guidance on deprecated options. I suggest generating doc for deprecated options as well, and we should scan through the code to make sure that proper upgrading guidance is provided for the deprecated options.",,,,,,,,,,,,,,,,,,,,,,,FLINK-30862,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 05 06:45:48 UTC 2024,,,,,,,,,,"0|z1kvf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 11:51;mapohl;I think that it makes sense to keep the deprecated options documented. This way we could add additional information on the deprecation. We might need to label these entries (maybe adding a ""deprecated"" warning next to the name) in a specific way, though.;;;","11/Oct/23 11:54;Zhanghao Chen;[~mapohl] Exactly. I'm thinking of adding a ""(deprecated)"" tag at the beginning of the option description. Also, we might better reach out to more devs and users for more feedbacks on it. I'll be writing a discussion email. WDYT?;;;","11/Oct/23 11:59;mapohl;Sounds good. (y) Thanks [~Zhanghao Chen];;;","02/Dec/23 14:33;jingge;please check FLINK-30862 for some objections.;;;","05/Feb/24 06:45;Zhanghao Chen;Closing it for duplication. The effort is postponed due to objections.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After enabling exactly-once in the Flink Kafka sink, the Kafka broker's memory keeps increasing, eventually causing the Kafka broker to crash.",FLINK-33239,13553672,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,lintingbin,lintingbin,11/Oct/23 10:47,11/Oct/23 11:39,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"We are using Flink version 1.17.1 and Kafka server version 2.8.2. After enabling exactly-once, in order to allow downstream consumers to read data from Kafka as soon as possible, we set the checkpoint interval to 5 seconds. Approximately three days after writing to the Kafka cluster, the Kafka JVM's memory is exhausted. We printed the memory consumption and found that the main consumption is on the {{kafka.log.ProducerStateEntry}} object.

Currently, in the exactly-once Kafka sink, a new producer is created every time a checkpoint is executed. The {{kafka.log.ProducerStateEntry}} object seems to store the producer's state, so it keeps increasing. We'd like to ask: Is this normal? If it's normal, do we need to allocate a large amount of memory for our Kafka cluster? If it's not normal, how should we solve this problem?

!image-2023-10-11-18-47-32-712.png!","flink 1.17.1

kafka server 2.8.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 10:47;lintingbin;image-2023-10-11-18-47-32-712.png;https://issues.apache.org/jira/secure/attachment/13063481/image-2023-10-11-18-47-32-712.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-11 10:47:59.0,,,,,,,,,,"0|z1kvds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade org.apache.avro:avro to 1.11.3 to mitigate CVE-2023-39410,FLINK-33238,13553665,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,11/Oct/23 09:47,23/Oct/23 06:29,04/Jun/24 20:40,23/Oct/23 06:29,,,,,,,,,,,1.18.1,1.19.0,kafka-3.1.0,,Connectors / Kafka,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,0,pull-request-available,,,,,"We should update AVRO to 1.11.3 to avoid false-positives on CVE-2023-39410
",,,,,,,,,,,,,,,,,,,FLINK-33303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 11:00:20 UTC 2023,,,,,,,,,,"0|z1kvc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 19:57;martijnvisser;Fixed in apache/flink-connector-kafka:main

c829d7a81ae0792af19525d257226c84fab7370b;;;","14/Oct/23 15:02;leibovski@rogers.com;Hi [~martijnvisser] I noticed this Jira after I opened my [PR|https://github.com/apache/flink/pull/23524] for the same thing but for a different issue. I think we should consolidate?

Avro 1.11.2 was a broken release, so closed my previous [PR|https://github.com/apache/flink/pull/23157]

Do you think there is still time to get this into Flink 1.18 ?

 ;;;","20/Oct/23 11:00;martijnvisser;[~leibovski@rogers.com] I'll backport this to 1.18 for 1.18.1

Fixed in apache/flink:

master: a58a35e2b1abda28ae7d884a433ca26624c7a4c4
release-1.18: a85832fb8ff1b1c55fa3e532645d208749b803d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the data type of JobStatus#state from String to Enum,FLINK-33237,13553650,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,11/Oct/23 08:20,11/Oct/23 08:20,04/Jun/24 20:40,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"As discuss at comment[1], the type of {{org.apache.flink.kubernetes.operator.api.status.JobStatus#state}} be changed from {{String}} to {{org.apache.flink.api.common.JobStatus.}}

 

[1] https://github.com/apache/flink-kubernetes-operator/pull/677#discussion_r1354340358

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-11 08:20:48.0,,,,,,,,,,"0|z1kv8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate the unused high-availability.zookeeper.path.running-registry option,FLINK-33236,13553619,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Zhanghao Chen,Zhanghao Chen,11/Oct/23 05:34,16/Oct/23 11:43,04/Jun/24 20:40,16/Oct/23 11:43,1.18.0,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"The running registry subcomponent of Flink HA has been removed in FLINK-25430 and the ""high-availability.zookeeper.path.running-registry"" option is of no use after that. We should deprecate the option and regenerate the config doc to avoid user confusion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 11:43:48 UTC 2023,,,,,,,,,,"0|z1kv20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 05:42;Zhanghao Chen;Hi [~mapohl], I've prepared a quick cleanup PR for it. Could you help take a review when you are free?;;;","16/Oct/23 11:43;mapohl;master: 935188f06a8a94b3a2c991a8aa6e48b2bfaeee70;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the Quickstart guide for Flink OLAP and translate to Chinese,FLINK-33235,13553615,13548014,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangyu0xf,xiangyu0xf,xiangyu0xf,11/Oct/23 05:14,02/Nov/23 02:19,04/Jun/24 20:40,02/Nov/23 02:19,,,,,,,,,,,1.19.0,,,,Documentation,,,,,,0,pull-request-available,,,,,"1, Follow the [https://flink.apache.org/how-to-contribute/documentation-style-guide/,|https://flink.apache.org/how-to-contribute/documentation-style-guide/] use 'active voice' and 'you' instead of 'passive voice' and 'we' in the doc;

2, Add architecture graphs in the doc;

3, Fix typos and misconfigured options in the doc;

4, Add new introduced options;

5, Translate to Chinese;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 02:17:55 UTC 2023,,,,,,,,,,"0|z1kv14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 08:46;martijnvisser;[~xiangyu0xf] I don't think we should do this as we don't do it for any feature that hasn't been released yet but only exists in master. We should just document features in general and have a generic introduction on how you could build master (which already exists).;;;","12/Oct/23 11:32;xiangyu0xf;[~martijnvisser] Hi Martjin, thx for your comment. I see your point now and I agree with you on this. However, I still want to improve this doc in following ways:

1, After checking the [https://flink.apache.org/how-to-contribute/documentation-style-guide/,|https://flink.apache.org/how-to-contribute/documentation-style-guide/] we should use 'active voice' and 'you'  instead of 'passive voice' and 'we' in the doc;

2, Add architecture graphs in this doc;

3, Fix typos and misconfigured options in this doc;

4, Add notes to explain that some OLAP features may have not released yet, user can choose to build from the master branch;

I will change the tittle and description accordingly, WDYT?

 ;;;","12/Oct/23 11:40;martijnvisser;I'm all for step 1 to 3, but not for 4. If the documentation is in master, but not in a release branch, there's a notification shown that the user is looking at documentation that might not be released. That's the generic solution for the whole of Flink. ;;;","12/Oct/23 11:50;xiangyu0xf;[~martijnvisser] I see that now. When this documentation is released, all the referenced feature should also be released. I should only update this document in next release cycle, right?;;;","12/Oct/23 11:55;martijnvisser;I'm not sure what you mean with ""update this documentation in next release cycle"". ;;;","12/Oct/23 12:07;xiangyu0xf;[~martijnvisser] Sorry I didn't make myself clear. I mean this document might be released in 1.19 along within its relying features. After then, we might add new features to master branch and then we should update this documents accordingly. ;;;","13/Oct/23 10:33;xiangyu0xf;Hi [~martijnvisser], excuse me for a minutes. I will take your advice and continue this work if this is ok for you.;;;","02/Nov/23 02:17;guoyangze;master: ff5d8f08662bd50479039914e7a8b85ca539e6c1

817e3f2b964fa5d86e207d6aa3065f139ec84402;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump used Guava version in Kafka E2E tests,FLINK-33234,13553544,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,10/Oct/23 14:08,11/Oct/23 09:45,04/Jun/24 20:40,11/Oct/23 09:45,,,,,,,,,,,kafka-3.1.0,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,To resolve existing Dependabot PRs: https://github.com/apache/flink-connector-kafka/security/dependabot?q=package%3Acom.google.guava%3Aguava+manifest%3Aflink-connector-kafka-e2e-tests%2Fflink-end-to-end-tests-common-kafka%2Fpom.xml+has%3Apatch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 09:45:02 UTC 2023,,,,,,,,,,"0|z1kulc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 09:45;martijnvisser;Fixed in apache/flink-connector-kafka:main 

Bump Guava 89343bb5d226e7e6d52e0ff18b172cf2156087a7
Exclude Gauva from Kafka AVRO Serializer and Schema Registry Client 26ab532b376d143f721d3ee89f64500577dfb84a
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null point exception when non-native udf used in join condition,FLINK-33233,13553541,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,yunfanfighting@foxmail.com,10/Oct/23 13:49,24/Jan/24 04:03,04/Jun/24 20:40,24/Jan/24 04:03,1.17.0,,,,,,,,,,1.17.3,1.18.2,1.19.0,,Connectors / Hive,,,,,,0,pull-request-available,,,,,"Any non-native udf used in hive-parser join condition. 

It will caused NullPointException.

It can reproduced by follow code by adding this test to 
{code:java}
org.apache.flink.connectors.hive.HiveDialectQueryITCase{code}
 
{code:java}
// Add follow code to org.apache.flink.connectors.hive.HiveDialectQueryITCase
@Test
public void testUdfInJoinCondition() throws Exception {
    List<Row> result = CollectionUtil.iteratorToList(tableEnv.executeSql(
            ""select foo.y, bar.I from bar join foo on hiveudf(foo.x) = bar.I where bar.I > 1"").collect());
    assertThat(result.toString())
            .isEqualTo(""[+I[2, 2]]"");
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 24 04:03:09 UTC 2024,,,,,,,,,,"0|z1kuko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 06:21;luoyuxia;master: ea3e80c0984a61589458c0672e3f0a73495203ed;;;","09/Jan/24 07:31;mapohl;Out of curiosity: Is this really an improvement? It sounds more like a bug to me. If that's the case, it might be worth backporting it to 1.18 and 1.17 as well (since the affected version is stated as 1.17.0 in this Jira issue). [~luoyuxia] [~yunfanfighting@foxmail.com] WDYT?;;;","10/Jan/24 01:42;luoyuxia;[~mapohl] Thanks for reminder. Haven't been aware of it. [~yunfanfighting@foxmail.com] Could you please back port to 1.17, 1.18?;;;","19/Jan/24 09:12;yunfanfighting@foxmail.com;[~luoyuxia] [~mapohl] Hello. I have submitted the PR which back port to release-1.18 and release-1.17 ;;;","24/Jan/24 04:03;luoyuxia;1.17: https://github.com/apache/flink/commit/44a697c2537de02b96ca1044498e3f930dd6fdc7

1.18:https://github.com/apache/flink/commit/369fae70399798c1afa5041dec02d666b6d98008;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetive Operator Not Able to Take Other Python paramters While Submitting Job Deployment,FLINK-33232,13553539,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,amar1509,amar1509,10/Oct/23 13:40,05/Dec/23 12:18,04/Jun/24 20:40,,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,,,,,,,1.17.1,,,,Kubernetes Operator,,,,,,0,,,,,,"Flink Operator Is not Able to Read the Python Cmd like -pyFiles.
While apply using Kubernetive on a Flink Session Cluster. The PyFiles are mounted using EFS, Not able to Read the EFS files and apply it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 05 12:18:48 UTC 2023,,,,,,,,,,"0|z1kuk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Dec/23 12:18;richard.su;I think this can be done by change the jobSpec.args, could you please offer a example yaml so that I can check the logic for you. [~amar1509] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in KafkaSourceReader if no data in consumed topic,FLINK-33231,13553537,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,lauri.suurvali,lauri.suurvali,10/Oct/23 13:29,11/Oct/23 07:08,04/Jun/24 20:40,11/Oct/23 07:08,1.17.1,,,,,,,,,,kafka-3.0.1,kafka-3.1.0,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"*Problem description*

Our Flink streaming job TaskManager heap gets full when the job has nothing to consume and process.

It's a simple streaming job with KafkaSource -> ProcessFunction -> KafkaSink. When there are no messages in the source topic the TaskManager heap usage starts increasing until the job exits after receiving a SIGTERM signal. We are running the job on AWS EMR with YARN.

The problems with the TaskManager heap usage do not occur when there is data to process. It's also worth noting that sending a single message to the source topic of a streaming job that has been sitting idle and suffers from the memory leak will cause the heap to be cleared. However it does not resolve the problem since the heap usage will start increasing immediately after processing the message.

!Screenshot 2023-10-10 at 12.49.37.png!

TaskManager heap used percentage is calculated by 

 
{code:java}
flink.taskmanager.Status.JVM.Memory.Heap.Used * 100 / flink.taskmanager.Status.JVM.Memory.Heap.Max{code}
 

 

 I was able to take heap dumps of the TaskManager processes during a high heap usage percentage. Heap dump analysis detected 912,355 instances of java.util.HashMap empty collections retaining >= 43,793,040 bytes.

!Screenshot 2023-10-09 at 14.13.43.png!

The retained heap seemed to be located at:

 
{code:java}
org.apache.flink.connector.kafka.source.reader.KafkaSourceReader#offsetsToCommit{code}
 

!Screenshot 2023-10-09 at 13.02.34.png!

 

*Possible hints:*

An empty HashMap is added during the snapshotState method to offsetsToCommit map if it does not already exist for the given checkpoint. [KafkaSourceReader line 107|https://github.com/apache/flink-connector-kafka/blob/b09928d5ef290f2a046dc1fe40b4c5cebe76f997/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java#L107]

 
{code:java}
Map<TopicPartition, OffsetAndMetadata> offsetsMap =
        offsetsToCommit.computeIfAbsent(checkpointId, id -> new HashMap<>()); {code}
 

If the startingOffset for the given split is >= 0 then a new entry would be added to the map from the previous step. [KafkaSourceReader line 113|https://github.com/apache/flink-connector-kafka/blob/b09928d5ef290f2a046dc1fe40b4c5cebe76f997/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/KafkaSourceReader.java#L113]
{code:java}
if (split.getStartingOffset() >= 0) {
    offsetsMap.put(
        split.getTopicPartition(),
        new OffsetAndMetadata(split.getStartingOffset()));
}{code}
If the starting offset is smaller than 0 then this would leave the offsetMap created in step 1 empty. We can see from the logs that the startingOffset is -3 when the splits are added to the reader.

 
{code:java}
Adding split(s) to reader: [[Partition: source-events-20, StartingOffset: 1, StoppingOffset: -9223372036854775808], [Partition: source-events-44, StartingOffset: -3, StoppingOffset: -9223372036854775808], [Partition: source-events-12, StartingOffset: -3, StoppingOffset: -9223372036854775808], [Partition: source-events-36, StartingOffset: 1, StoppingOffset: -9223372036854775808], [Partition: source-events-4, StartingOffset: -3, StoppingOffset: -9223372036854775808], [Partition: source-events-28, StartingOffset: -3, StoppingOffset: -9223372036854775808]]{code}
 

 

The offsetsToCommit map is cleaned from entries once they have been committed to Kafka which happens during the callback function that is passed to the KafkaSourceFetcherManager.commitOffsets method in KafkaSourceReader.notifyCheckpointComplete method.

However if the committedPartitions is empty for the given checkpoint, then the KafkaSourceFetcherManager.commitOffsets method returns.  [KafkaSourceFetcherManager line 78|https://github.com/apache/flink-connector-kafka/blob/b09928d5ef290f2a046dc1fe40b4c5cebe76f997/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/fetcher/KafkaSourceFetcherManager.java#L78]
{code:java}
if (offsetsToCommit.isEmpty()) {
    return;
} {code}
We can observe from the logs that indeed an empty map is encountered at this step:
{code:java}
Committing offsets {}{code}
*Conclusion*

It seems that an empty map gets added per each checkpoint to offsetsToCommit map. Since the startingOffset in our case is -3 then the empty map never gets filled. During the offset commit phase the offsets for these checkpoints are ignored, since there is nothing to commit, however there isn't any cleanup either so the empty maps keep accumulating. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/23 13:26;lauri.suurvali;Screenshot 2023-10-09 at 13.02.34.png;https://issues.apache.org/jira/secure/attachment/13063454/Screenshot+2023-10-09+at+13.02.34.png","10/Oct/23 13:25;lauri.suurvali;Screenshot 2023-10-09 at 14.13.43.png;https://issues.apache.org/jira/secure/attachment/13063455/Screenshot+2023-10-09+at+14.13.43.png","10/Oct/23 13:25;lauri.suurvali;Screenshot 2023-10-10 at 12.49.37.png;https://issues.apache.org/jira/secure/attachment/13063456/Screenshot+2023-10-10+at+12.49.37.png",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 07:08:27 UTC 2023,,,,,,,,,,"0|z1kujs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 15:58;tzulitai;[~lauri.suurvali] great debugging!

I think the fix is basically, in KafkaSourceFetcherManager#commitOffsets, if the provided offsetsToCommitMap is empty, the callback (where the logic for truncating the map) should be used as well. Currently, it just returns without calling the callback at all. Code link: https://github.com/apache/flink-connector-kafka/blob/main/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/source/reader/fetcher/KafkaSourceFetcherManager.java#L78-L80;;;","10/Oct/23 16:03;tzulitai;Making this a blocker for the upcoming Kafka connector releases.;;;","10/Oct/23 18:58;lauri.suurvali;[~tzulitai] thank you for the comment!
Would removing the code that you linked, which returns in case of an empty offsetsToCommit, be an option to solve this issue? The remaining code would end up in org.apache.kafka.clients.consumer.internals.ConsumerCoordinator#sendOffsetCommitRequest which would return a successful response locally in case of an empty offsets map. [kafka/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java at master · a0x8o/kafka (github.com)|https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L1267-L1269]

Since the request is handled locally then perhaps this is a good way to ensure that the callback function isn't discarded. Would this sort of an approach bring any additional overhead that we would like to avoid or perhaps I am missing something?;;;","10/Oct/23 19:18;tzulitai;[~lauri.suurvali] I think that would work, but the issue is that in the callback, on success we log that a commit was successful, and also source reader metrics is bumped. Which can be confusing if no offsets were actually committed. Moreoever, with that approach we would be relying on internal details of the Kafka client that is hard to cover with tests (i.e. things might silently change such that a remote request is issued even if provided offsets are empty, which is not ideal).

So, I think we can be a bit cleaner by short-cutting the {{notifyCheckpointComplete}} method such that is the offsets for a checkpoint is empty, we don't even attempt to use the fetcher manager to try to commit offsets.;;;","11/Oct/23 07:08;martijnvisser;Fixed in apache/flink-connector-kafka

main b0f15f279915a3862d353c9d2726a52741c248ba
v3.0 b316d8ce25cc63bc6cdb858831e8566164a16b81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Expanding ExecutionGraph to StreamGraph in Web UI,FLINK-33230,13553522,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,Yu Chen,Yu Chen,Yu Chen,10/Oct/23 10:41,13/Jan/24 01:39,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,"Flink Web shows users the ExecutionGraph (i.e., chained operators), but in some cases, we would like to know the structure of the chained operators as well as the necessary metrics such as the inputs and outputs of data, etc.

 

Thus, we propose to show the stream graphs and some related metrics such as numberRecordInand numberRecordOut on the Flink Web (As shown in the Figure).

 

!image-2023-10-10-18-52-38-252.png|width=750,height=263!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/23 10:52;Yu Chen;image-2023-10-10-18-52-38-252.png;https://issues.apache.org/jira/secure/attachment/13063449/image-2023-10-10-18-52-38-252.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 02 11:55:13 UTC 2024,,,,,,,,,,"0|z1kugg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 01:48;lsy;[~Yu Chen] Hi, it is a good proposal, I have one question: How do you get the StreamGraph during runtime?;;;","12/Oct/23 06:04;Yu Chen;Hi [~lsy]. 

We can store the Json String of the StreamGraph into the ArchiveExecutionGraph in a similar way as JsonPlan.
Actually, the execution graph shown in the Web UI was also extracted from the JsonPlan.

You can refer to the following code path:
StreamingJobGraphGenerator->createJobGraph-[DefaultExecutionGraphBuilder]->executionGraph -> ArchivedExecutionGraph;;;","13/Oct/23 06:11;JunRuiLi;Hi [~Yu Chen] , Because this proposal involves modifications to the public interface, specifically the Web-UI, it is necessary to discuss this on the Dev mailing list.;;;","16/Oct/23 12:50;Yu Chen;Hi [~JunRuiLi] ,

Sure, I'll illustrate the details of the implementation as a FLIP and create the discussion in the dev mailing group.;;;","02/Jan/24 11:55;wangm92;[~Yu Chen] it's a valuable function, looking forward to it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moving Java FlinkRecomputeStatisticsProgram from scala package to java package,FLINK-33229,13553491,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lsy,lsy,lsy,10/Oct/23 08:31,12/Oct/23 09:52,04/Jun/24 20:40,12/Oct/23 09:34,1.19.0,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 09:52:14 UTC 2023,,,,,,,,,,"0|z1ku9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 08:47;martijnvisser;[~lincoln.86xy] Should this ticket not be closed, as the PR is merged?;;;","12/Oct/23 09:33;lincoln.86xy;[~martijnvisser] thanks for the reminder :) We don't need do any backport after confirmed with [~lsy], I'll close it.;;;","12/Oct/23 09:33;lincoln.86xy;fixed in master: 521e742bfc034d66168e151e15e65e29a23b56c9;;;","12/Oct/23 09:52;martijnvisser;Awesome, thanks :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the total current resource calculation when fulfilling requirements,FLINK-33228,13553482,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiangyu0xf,xiangyu0xf,xiangyu0xf,10/Oct/23 08:02,11/Oct/23 01:59,04/Jun/24 20:40,11/Oct/23 01:59,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Currently, the `totalCurrentResources` calculation in `DefaultResourceAllocationStrategy#tryFulfillRequirements` is wrong.

`ResourceProfile.merge` will not change the original `ResourceProfile`.

!image-2023-10-10-16-09-23-635.png|width=564,height=286!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15959,,,,,,,,,,"10/Oct/23 08:09;xiangyu0xf;image-2023-10-10-16-09-23-635.png;https://issues.apache.org/jira/secure/attachment/13063442/image-2023-10-10-16-09-23-635.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 01:58:53 UTC 2023,,,,,,,,,,"0|z1ku7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 08:28;xiangyu0xf;Hi [~guoyangze], I've fixed this issue. Would u kindly assign this Jira to me?;;;","11/Oct/23 01:58;guoyangze;master: 5269631af525a01d944cfa9994a116fb27b80b1b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL CEP support 'notFollowedBy' semantics,FLINK-33227,13553455,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,shaw-valjean,shaw-valjean,10/Oct/23 04:04,01/Nov/23 09:58,04/Jun/24 20:40,,1.16.0,,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"Currently, the cep mode of the Flink API can support next, notNext, followedBy, followedByAny, and notFollowedBy semantics, but Flink SQL only supports next semantics. The remaining notNext and followedBy semantics are implemented by other alternatives, while the notFollowedBy semantics are not currently implemented. At present, this semantics is generally implemented in business scenarios, such as judging that a user has placed an order within 15 minutes without paying. Therefore, I suggest to provide new functionality to support notFollowedBy in sql mode, along with the other three semantics

 

The syntax of enhanced MATCH_RECOGNIZE is proposed as follows:
{code:sql}
MATCH_RECOGNIZE (
    [ PARTITION BY <expr> [, ... ] ]
    [ ORDER BY <expr> [, ... ] ]
    [ MEASURES <expr> [AS] <alias> [, ... ] ]
    [ ONE ROW PER MATCH [ { SHOW TIMEOUT MATCHES } ] |
      ALL ROWS PER MATCH [ { SHOW TIMEOUT MATCHES } ]
    ]
    [ AFTER MATCH SKIP
          {
          PAST LAST ROW   |
          TO NEXT ROW   |
          TO [ { FIRST | LAST} ] <symbol>
          }
    ]
    PATTERN ( <pattern> )
    DEFINE <symbol> AS <expr> [, ... ]
)
{code}
 * [^ <symbol>] is proposed in <pattern> to express the notNext semantic. For example, A [^B] is translated to A.notNext(B).
 * {- <symbol> -} is proposed in <pattern> to express the followedBy semantic. For example, A \{ B*? -} C is translated to A.followedBy(C).
 * {- symbol1 \-} with [^ <symbol2>] is proposed in <pattern> to express the notFollowedBy semantic. For example, A {- B*? -} [^C] is translated to A.notFollwedBy(B).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-10 04:04:02.0,,,,,,,,,,"0|z1ku1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forbid to drop current database,FLINK-33226,13553448,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ferenc-csaky,fsk119,fsk119,10/Oct/23 01:41,17/Oct/23 11:47,04/Jun/24 20:40,17/Oct/23 11:47,1.19.0,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"PG or MySql both doesn't support to drop the current database. PG throws the following exception.


{code:java}
test=# drop database
test-# test;
ERROR:  cannot drop the currently open database
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 17 11:46:48 UTC 2023,,,,,,,,,,"0|z1ku00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 08:10;ferenc-csaky;Hi! I bumped into this a while back and I think this makes sense. Since it is a fairly small change, I opened a PR.;;;","17/Oct/23 11:46;fsk119;Merged into master: a29a320187a45ca2579a4cdeddeb99a974d9cf2d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python API incorrectly passes `JVM_ARGS` as single argument,FLINK-33225,13553421,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,deepyaman,deepyaman,deepyaman,09/Oct/23 17:31,22/Nov/23 12:32,04/Jun/24 20:40,22/Nov/23 12:32,1.17.1,1.18.0,1.18.1,,,,,,,,1.18.1,1.19.0,,,,,,,,,0,github-pullrequest,pull-request-available,,,,"In the same vein as https://issues.apache.org/jira/browse/FLINK-31915, `JVM_ARGS` need to be passed as an array. For example, the current behavior of export `JVM_ARGS='-XX:CompressedClassSpaceSize=100M -XX:MaxMetaspaceSize=200M'` is:

{{>               raise RuntimeError(}}
{{                    ""Java gateway process exited before sending its port number.\nStderr:\n""}}
{{                    + stderr_info}}
{{                )}}
{{E               RuntimeError: Java gateway process exited before sending its port number.}}
{{E               Stderr:}}
{{E               Improperly specified VM option 'CompressedClassSpaceSize=100M -XX:MaxMetaspaceSize=200M'}}
{{E               Error: Could not create the Java Virtual Machine.}}
{{E               Error: A fatal exception has occurred. Program will exit.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 22 12:32:23 UTC 2023,,,,,,,,,,"0|z1ktu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Nov/23 12:32;dianfu;Merged to
- master via b1bfd70ad8a9e4e1a710dc5775837ba7102d4b70
- release-1.18 via 73273ae1669c9f01b61667dedb85a7e745d6bbe2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink  Batch Job does not complete when using Kafka topics with empty partitions,FLINK-33224,13553410,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ekanthi,ekanthi,09/Oct/23 14:44,12/Oct/23 20:17,04/Jun/24 20:40,,1.14.6,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"Kafka Source Connector no longer throws an exception when running a flink job in batch mode when using Kafka topics with empty partitions. However, flink does not emit the final watermark of Long.MAX_VALUE even if all the messages have been read. Note that we are using bounded sources, since that is a requirement for batch mode. It appears that the Kafka consumer continues to run even though the source is bounded.

Same is the case with an empty topic. We do have optional sources, where data need not be present all the time. However if we mark the source as bounded, the Kafka connector should recognize empty partitions and mark it as completed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Thu Oct 12 20:17:56 UTC 2023,,,,,,,,,,"0|z1ktrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 08:49;martijnvisser;[~ekanthi] Can you verify this in a supported version of Flink, since there have been many changes in both Flink and the Flink Kafka connector since 1.14?;;;","12/Oct/23 19:48;ekanthi;[~martijnvisser] I am talking about Apache Flink 1.14.6. Is this not supported anymore? What is the smallest version of flink that you support? https://flink.apache.org/2022/09/28/apache-flink-1.14.6-release-announcement/;;;","12/Oct/23 20:17;martijnvisser;The Flink community only supports the last two minor releases, so currently 1.16 and 1.17 and it will be 1.17 and 1.18 shortly;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MATCH_RECOGNIZE AFTER MATCH clause can not be deserialised from a compiled plan,FLINK-33223,13553409,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,09/Oct/23 14:10,10/Oct/23 06:44,04/Jun/24 20:40,10/Oct/23 06:42,,,,,,,,,,,1.18.1,1.19.0,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"{code}
        String sql =
                ""insert into MySink""
                        + "" SELECT * FROM\n""
                        + "" MyTable\n""
                        + ""   MATCH_RECOGNIZE(\n""
                        + ""   PARTITION BY vehicle_id\n""
                        + ""   ORDER BY `rowtime`\n""
                        + ""   MEASURES \n""
                        + ""       FIRST(A.`rowtime`) as startTime,\n""
                        + ""       LAST(A.`rowtime`) as endTime,\n""
                        + ""       FIRST(A.engine_temperature) as Initial_Temp,\n""
                        + ""       LAST(A.engine_temperature) as Final_Temp\n""
                        + ""   ONE ROW PER MATCH\n""
                        + ""   AFTER MATCH SKIP TO FIRST B\n""
                        + ""   PATTERN (A+ B)\n""
                        + ""   DEFINE\n""
                        + ""       A as LAST(A.engine_temperature,1) is NULL OR A.engine_temperature > LAST(A.engine_temperature,1),\n""
                        + ""       B as B.engine_temperature < LAST(A.engine_temperature)\n""
                        + ""   )MR;"";
        util.verifyJsonPlan(String.format(sql, afterClause));
{code}

fails with:

{code}
Could not resolve internal system function '$SKIP TO LAST$1'. This is a bug, please file an issue. (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[3]->org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch[""matchSpec""]->org.apache.flink.table.planner.plan.nodes.exec.spec.MatchSpec[""after""])
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 06:42:14 UTC 2023,,,,,,,,,,"0|z1ktrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:42;dwysakowicz;Fixed in 
* master
** 53ece12c25579497338ed59a7aebe70f2b3d9ed6
* 1.18.1
** 9b837727b6d369af9029c73a02bf5c43f0ce6201;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator rollback app when it should not,FLINK-33222,13553405,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nfraison.datadog,nfraison.datadog,09/Oct/23 13:35,10/Oct/23 08:54,04/Jun/24 20:40,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"The operator can decide to rollback when an update of the job spec is performed on 
savepointTriggerNonce or initialSavepointPath if the app has been deployed since more than KubernetesOperatorConfigOptions.DEPLOYMENT_READINESS_TIMEOUT.
 
This is due to the objectmeta generation being [updated|https://github.com/apache/flink-kubernetes-operator/blob/release-1.6/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L169] when changing those spec and leading to the lastReconcileSpec not being aligned with the stableReconcileSpec while those spec are well ignored when checking for upgrade diff
 
Looking at the main branch we should still face the same issue as the same [update|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L169] is performed at the end of the reconcile loop",Flink operator 1.6 - Flink 1.17.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 08:54:58 UTC 2023,,,,,,,,,,"0|z1ktqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 08:54;nfraison.datadog;So I was wrong the release 1.7-snapshot is not affected by this bug thanks to [https://github.com/apache/flink-kubernetes-operator/pull/681] patch.

Indeed deploying the app with an {{{}initialSavepointPath{}}}:
 * lastReconciledSpec get the update of generation from N to N+1 while stable spec generation stay at N. But no rollback detected as the [update|https://github.com/apache/flink-kubernetes-operator/pull/681/files#diff-29ea38a50cac5b4432dd0969bc3e2177e29a5507f8c7bb01b80f605a8740de41R169] is done after the [rollback|https://github.com/apache/flink-kubernetes-operator/pull/681/files#diff-29ea38a50cac5b4432dd0969bc3e2177e29a5507f8c7bb01b80f605a8740de41R146] check
deployment is consider as DEPLOYED

 * then on second reconcile loop the stable spec generation is also updated from N to N+1 (in [patchAndCacheStatus|[https://github.com/apache/flink-kubernetes-operator/blob/release-1.6/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/controller/FlinkDeploymentController.java#L135]])and the deployment is consider as STABLE

But this look quite brittle to me as just changing the position of the shouldRollBack or ReconciliationUtils.updateReconciliationMetadata could lead to that bad behaviour again.

 

I'm wondering if we could not take in account the generation field in the [isLastReconciledSpecStable|https://github.com/apache/flink-kubernetes-operator/blob/release-1.6/flink-kubernetes-operator-api/src/main/java/org/apache/flink/kubernetes/operator/api/status/ReconciliationStatus.java#L91];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add config options for administrator JVM options,FLINK-33221,13553390,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,09/Oct/23 11:41,31/Jan/24 06:23,04/Jun/24 20:40,22/Jan/24 03:31,,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"We encounter similar issues described in SPARK-23472. Users may need to add JVM options to their Flink applications (e.g. to tune GC options). They typically use {{env.java.opts.x}} series of options to do so. We also have a set of administrator JVM options to apply by default, e.g. to enable GC log, tune GC options, etc. Both use cases will need to set the same series of options and will clobber one another.

In the past, we generated and pretended to the administrator JVM options in the Java code for generating the starting command for JM/TM. However, this has been proven to be difficult to maintain.

Therefore, I propose to also add a set of default JVM options for administrator use that prepends the user-set extra JVM options. We can mark the existing {{env.java.opts.x}} series as user-set extra JVM options and add a set of new {{env.java.opts.x.default}} options for administrator use.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 22 03:31:04 UTC 2024,,,,,,,,,,"0|z1ktnc:",9223372036854775807,"A set of administrator JVM options are available, which prepend the user-set extra JVM options for platform-wide JVM tuning.",,,,,,,,,,,,,,,,,,,"22/Jan/24 03:31;fanrui;Merged to master(1.19.0) via : b4eb8ac503f41fd793db1ac662fbedc46af92fd5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink support for Datagen connector,FLINK-33220,13553372,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liu.chong,liu.chong,09/Oct/23 09:53,21/Mar/24 01:24,04/Jun/24 20:40,,,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,This is a simple Jira to propose the support of Datagen in PyFlink datastream API as a built-in source connector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 21 01:24:29 UTC 2024,,,,,,,,,,"0|z1ktjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 05:21;liu.chong;we've got the code ready for adding datagen to pyflink, would you like to give some comments if we should move forward and submit the PR? [~dianfu] ;;;","21/Mar/24 01:24;dianfu;[~liu.chong]  Sorry, I missed this ticket. Feel free to submit the PR and ping me~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka connector has architecture test violation against Flink 1.18,FLINK-33219,13553371,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,renqs,renqs,renqs,09/Oct/23 09:52,25/Oct/23 13:13,04/Jun/24 20:40,09/Oct/23 13:32,kafka-4.0.0,,,,,,,,,,kafka-3.0.1,kafka-3.1.0,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"FLINK-31804 modified architecture rule about MiniCluster, which changes error message and leads to Kafka connector's architecture test fail:

 [https://github.com/apache/flink-connector-kafka/actions/runs/6453974340/job/17518572377]",,,,,,,,,,,,,,,,,,,FLINK-33017,,,,FLINK-33104,,,,,,FLINK-31804,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 13:32:59 UTC 2023,,,,,,,,,,"0|z1ktj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 09:56;renqs;As FLINK-31804 was not back-ported to 1.17, the error message of violation is different in 1.17 and 1.18. If we re-freeze violations against one version, the other one will fail then. 

One solution is that we keep two versions of error message in the same violation file. ;;;","09/Oct/23 13:32;martijnvisser;Fixed in:

apache/flink-connector-kafka@main b09928d5ef290f2a046dc1fe40b4c5cebe76f997;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First Steps - error when run with zsh,FLINK-33218,13553370,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,rmoff,rmoff,rmoff,09/Oct/23 09:46,16/Oct/23 19:22,04/Jun/24 20:40,,,,,,,,,,,,,,,,Documentation,,,,,,0,pull-request-available,,,,,"If a user of zsh (the default on MacOS) runs the literal command that's given under ""Browsing the project directory"" they get an error: 


{code:java}
$ cd flink-* && ls -l
cd: string not in pwd: flink-1.17.1
{code}
 

This is because the behaviour of `cd` is different under zsh than bash and the glob triggers this. I've written up [an explanation|https://rmoff.net/2023/10/04/cd-string-not-in-pwd/] for those interested.

IMO the fix is to hardcode the version in the instructions. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-09 09:46:06.0,,,,,,,,,,"0|z1ktiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL: UNNEST fails with on LEFT JOIN with NOT NULL type in array,FLINK-33217,13553365,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,rmetzger,rmetzger,09/Oct/23 09:36,22/Dec/23 07:42,04/Jun/24 20:40,,1.15.3,1.18.0,1.19.0,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"Steps to reproduce:

Take a column of type 

{code:java}
business_data ARRAY<STRING NOT NULL>
{code}

Take this query

{code:java}
select bd_name from reproduce_unnest LEFT JOIN UNNEST(reproduce_unnest.business_data) AS exploded_bd(bd_name) ON true
{code}

And get this error

{code:java}
Caused by: java.lang.AssertionError: Type mismatch:
rowtype of rel before registration: RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL ARRAY business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" bd_name) NOT NULL
rowtype of rel after registration: RecordType(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL ARRAY business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL f0) NOT NULL
Difference:
bd_name: VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" -> VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL

	at org.apache.calcite.util.Litmus$1.fail(Litmus.java:32)
	at org.apache.calcite.plan.RelOptUtil.equal(RelOptUtil.java:2206)
	at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:275)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1270)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:598)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:613)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.changeTraits(VolcanoPlanner.java:498)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:315)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
{code}
I have implemented a small test case, which fails against Flink 1.15, 1.8 and the latest master branch.

Workarounds:
1. Drop ""NOT NULL"" in array type
2. Drop ""LEFT"" from ""LEFT JOIN"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31830,,,,,,,,"09/Oct/23 13:31;rmetzger;UnnestNullErrorTest.scala;https://issues.apache.org/jira/secure/attachment/13063441/UnnestNullErrorTest.scala",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 22 07:42:36 UTC 2023,,,,,,,,,,"0|z1kths:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 11:42;Sergey Nuyanzin;Hi [~rmetzger]
thanks for raising the issue

It looks like the problem consists of several parts
1. there is an error in SQL provided in description
   it complains about type mismatch (in trace) like 
{noformat}
RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL ARRAY updateEvent) business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" ue_name) NOT NULL
RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL ARRAY updateEvent) business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL
{noformat}

the first discrepancy is naimng: in one column it is {{ue_name}} while in another it is {{name}}, 
also keep in mind type 
{code:sql}
business_data ROW<`id` STRING, `updateEvent` ARRAY<ROW<`name` STRING NOT NULL> NOT NULL>> 
{code}
where it is called {{name}}

so the more correct SQL should look like 
{code:sql}
select id, name from reproduce_unnest LEFT JOIN UNNEST(reproduce_unnest.business_data.updateEvent) AS exploded_ue(name) ON true;
{code}

2. However this will also fail with 
{noformat}
set type is RecordType(RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" id, RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL ARRAY updateEvent) business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" name) NOT NULL
expression type is RecordType(RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" id, RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL ARRAY updateEvent) business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL
set is rel#104:LogicalCorrelate.NONE.any.None: 0.[NONE].[NONE](left=HepRelVertex#98,right=HepRelVertex#103,correlation=$cor0,joinType=left,requiredColumns={0})
expression is LogicalCorrelate(correlation=[$cor0], joinType=[left], requiredColumns=[{0}])
  LogicalTableScan(table=[[default_catalog, default_database, reproduce_unnest, source: [CollectionTableSource(business_data)]]])
  LogicalTableFunctionScan(invocation=[$UNNEST_ROWS$1($cor0.business_data.updateEvent)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) name)])

{noformat}

Now if look at type comparison in trace
{noformat}
RecordType(RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" id, RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL ARRAY updateEvent) business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" name) NOT NULL
RecordType(RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" id, RecordType:peek_no_expand(VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL ARRAY updateEvent) business_data, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" NOT NULL name) NOT NULL
{noformat}
we can see that the difference is that for the first type column {{name}} is nullable and for the second is not and this is probably should be fixed (not sure whether it could be fixed with FLIP-154 or not, will try to double check it later)

the WA is to ensure that there is the same type on both sides of the join e.g. nullable for both
like
{code:sql}
CREATE TABLE reproduce_unnest (
 business_data ROW<`id` STRING, `updateEvent` ARRAY<ROW<`name` STRING> NOT NULL>>
) WITH (
 'connector' = 'COLLECTION',
 'is-bounded' = 'false'
)
{code}
and
{code:sql}
select id, name from reproduce_unnest LEFT JOIN UNNEST(reproduce_unnest.business_data.updateEvent) AS exploded_ue(name) ON true
{code};;;","09/Oct/23 13:29;rmetzger;Thanks a lot for your response. You are right that I made some mistakes in the original bug report. The reproducer was buggy and can be simplified. I've just uploaded a new version which hopefully addresses your concerns. I've also updated the original issue description to make it easier to understand.

You can either use the workaround you've provided or alternatively you can turn the ""LEFT JOIN"" into a ""JOIN"", ofc the result will be wrong/different because of this, but you can at least get the query to compile for now.
In my case, I have a number of queries and an immutable input schema, so making the type nullable is not an option.

I'm pretty sure this is a bug in Calcite, as this is an internal assertion failing while planning the query. But I didn't want to open a ticket in Calcite right away, before having collected the opinion from folks with experience around Flink SQL.
;;;","09/Oct/23 13:29;rmetzger;Actually, you can make the reproducer even simpler:

{code}business_data ARRAY<STRING NOT NULL>{code}
also fails with the same error. I updated the code & description again.;;;","16/Oct/23 13:01;Sergey Nuyanzin;after looking a bit deeper it looks like it is not Calcite issue
e.g.

this query

{code:sql}
with Book as (
   SELECT *
     FROM
          (
        VALUES
              ROW (array[1, 2])
            , ROW (array[11])
            , ROW (array[22])
        )  Book (authorId)
        )  select * from Book b left join unnest(b.authorId) on true
{code}
works ok with Calcite while is failing on Flink with similar issue.

I think the reason is that  In Calcite there is a dedicated {{UNNEST}} operator [1] which for some reason is not used in Flink... Instead there is LogicalUnnestRule [2] which tries to translates result of unnest as a table scan and this is the place where the error happens... 
Based on the code of this rule 
{code:scala}
      relNode match {
        case rs: HepRelVertex =>
          convert(getRel(rs))

          case f: LogicalProject =>
           ...
          case f: LogicalFilter =>
           ...

          case uc: Uncollect =>
          ...
    }
{code}
there could be  4 different types of cases failing with same or similar error while join unnest.

Current thoughts about how to fix it 
1. Move to Calcite's Unnest operator (however that's still not clear what was the reason to not use it...)
2. Since while parsing and building AST and while also convertion Calcite converts {{LEFT JOIN}} to something that has nullable type pn the left and this is also the reason, we could add convertion to not do it for {{LEFT JOIN UNNEST}}
3. We could try handling this in {{LogicalUnnestRule}} by making types broader like force nullables... however it could lead wrong final types (e.g. nullable instead of not nullable)

[1] https://github.com/apache/calcite/blob/bf56743554ea27d250d41db2eb83806f9e626b55/core/src/main/java/org/apache/calcite/sql/SqlUnnestOperator.java#L34
[2] https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/LogicalUnnestRule.scala#L52;;;","16/Oct/23 13:06;xuyangzhong;This may be caused by the specific struct type in Flink 'PEEK_FIELDS_NO_EXPAND'. I'll try to find the way to fix it.;;;","31/Oct/23 07:57;xuyangzhong;This issue is related to FLINK-31830. The node 'unnest' will be convert to LogicalTableFunctionScan in LogicalUnnestRule and its row type changes to `ROW(VARCHAR not null) not null`. Because the LEFT join, the type row is forced nullable, and the row type is changes to `ROW(VARCHAR not null)` by `FlinkTypeFactory#createTypeWithNullability` that overrides its super class. And the diff about nullable cases this failure.

In the latest calcite version, if the ROW changes into nullable, the fields in ROW should also changes into nullable (CALCITE-2464). If we also support that feature in Flink in FLINK-31830, this bug will be fixed.;;;","04/Dec/23 14:19;rmetzger;Thank you so much for analyzing the issue. I will keep a close eye on FLINK-31830.;;;","22/Dec/23 07:42;qingyue;Sorry for being late.

Hi, [~xuyangzhong], thanks for the exploration. While you mentioned that
{quote}Because the LEFT join, the type row is forced nullable, and the row type is changes to `ROW(VARCHAR not null)` by `FlinkTypeFactory#createTypeWithNullability` that overrides its super class. And the diff about nullable cases this failure.
{quote}
Actually, the desired nullability change for the type`ROW(VARCHAR not null) not null` after a LEFT OUTER JOIN should be `ROW(VARCHAR)`, rather than `ROW(VARCHAR not null)`.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
turn off the public access of the s3 bucket flink-nightly,FLINK-33216,13553363,13553357,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,09/Oct/23 09:21,09/Oct/23 09:21,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Build System,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-09 09:21:17.0,,,,,,,,,,"0|z1kthc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella] use https://nightlies.apache.org/flink/ as the flink artifact server for connector nightly build ,FLINK-33215,13553357,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,jingge,jingge,09/Oct/23 08:54,09/Oct/23 09:38,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Build System / CI,,,,,,0,,,,,,"# Flink nightly build artifact should be uploaded to [https://nightlies.apache.org/flink/]
 # all externalized connectors should download the Flink artifact from [https://nightlies.apache.org/flink/] instead of S3 bucket flink-nightly
 # the s3 bucket will not be publicly accessible.",,,,,,,,,,,,FLINK-33214,,,,,,,,,,,,,,,,,,,,FLINK-33175,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-09 08:54:44.0,,,,,,,,,,"0|z1ktg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Moving away from the Ververica-owned S3 bucket and use Apache Infra's nightly SVN instead,FLINK-33214,13553356,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,09/Oct/23 08:47,09/Oct/23 09:38,04/Jun/24 20:40,,,,,,,,,,,,,,,,Build System,,,,,,0,,,,,,"Currently, we're deploying snapshot versions of Flink to a Ververica-owned S3 bucket. Instead, it's recommended to use Apache Infra resources with https://nightlies.apache.org/flink/, instead.

It was verified with Apache Infra that the capacity of the repository would work for Flink's nightly (see [Slack post|https://the-asf.slack.com/archives/CBX4TSBQ8/p1696357867690409] and follow-ups):
{quote}
Martijn Visser: Hi all, I wanted to double check if it's safe to use https://nightlies.apache.org/flink/ for storing nightly builds of Apache Flink, which are created after a successful merge. We use these in weekly runs of Apache Flink connectors (that all reside in their own repository), where we test that connector against the snapshot version that's available. We cache the binaries in Flink connector runs, to avoid overly downloading them.
---
Gavin McDonald: sure, hopefully size isnt excessive
---
Martijn Visser: flink-1.18-SNAPSHOT-bin-scala_2.12.tgz (size: 478MB) :sweat_smile:
---
Gavin McDonald: let me look into it further, remind myself what space we have etc. will report back tomorrow
---
Chesnay Schepler: FWIW we'd need (exactly) 3 of these files at any given time, right 
@Martijn Visser? Last 2 versions + master.
---
Martijn Visser: Yes, correct
---
Gavin McDonald: ok I think we should be fine
{quote}

(!) Moving to SVN for nightly storage also includes updating the release documentation: We have to clean up deprecated minor version snapshots to keep the memory footprint low.",,,,,,,,,,,,,,,FLINK-33215,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33175,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-09 08:47:39.0,,,,,,,,,,"0|z1ktfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL MinMax aggregations without retract messages when `where` condition exist,FLINK-33213,13553342,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,paryshevsergey,paryshevsergey,paryshevsergey,09/Oct/23 07:56,12/Nov/23 17:08,04/Jun/24 20:40,12/Nov/23 17:08,1.12.7,1.13.6,1.14.6,1.15.4,1.16.2,1.17.1,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Currently Flink didn't take account `where` condition when calculate SqlMonotonicity for Calc/FlinkLogicalCalc/StreamPhysicalCalc.

 

To reproduce bug (put into org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase):
{code:java}
@Test
def testMaxRetract(): Unit = {
  env.setParallelism(1)
  val data = new mutable.MutableList[(Int, Int)]
  data.+=((1, 10))
  data.+=((1, 10))
  data.+=((2, 5))
  data.+=((1, 10))

  val t = failingDataSource(data).toTable(tEnv, 'id, 'price)
  tEnv.createTemporaryView(""T"", t)

  val sql =
    """"""
      |SELECT MAX(price) FROM(
      |   SELECT id, count(*) as c, price FROM T GROUP BY id, price)
      |WHERE c > 0 and c < 3"""""".stripMargin

  val sink = new TestingRetractSink
  tEnv.sqlQuery(sql).toRetractStream[Row].addSink(sink)
  env.execute()

  val expected = List(""5"")
  assertEquals(expected.sorted, sink.getRetractResults.sorted)
} {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Nov 12 17:08:26 UTC 2023,,,,,,,,,,"0|z1ktco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 08:00;paryshevsergey;[~martijnvisser] [~trohrmann] 

Hi, please, assign me to this task;;;","12/Oct/23 08:50;martijnvisser;[~snuyanzin] Any thoughts on this one?;;;","12/Nov/23 17:08;Sergey Nuyanzin;Merged as [29e49482a82e8c1cd404e42c3aae0944188d956e|https://github.com/apache/flink/commit/29e49482a82e8c1cd404e42c3aae0944188d956e];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce job status changed listener for lineage,FLINK-33212,13553329,13526635,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hpeter,zjureel,zjureel,09/Oct/23 05:22,29/May/24 02:16,04/Jun/24 20:40,,1.20.0,,,,,,,,,,,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,"Introduce job status changed listener relevant interfaces and its implementation. The job listeners will be registered in runtime and also client side pipeline executors, including localExecutor, embeddedExecutor for application mode, and  abstract session cluster executor.  When job submission is successfully, the job created event will be created with lineage graph info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-09 05:22:02.0,,,,,,,,,,"0|z1kt9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement table lineage graph,FLINK-33211,13553327,13526635,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,hpeter,zjureel,zjureel,09/Oct/23 05:19,24/May/24 10:57,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,,,Implement table lineage graph,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-09 05:19:15.0,,,,,,,,,,"0|z1kt9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce lineage graph relevant interfaces ,FLINK-33210,13553326,13526635,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,zjureel,zjureel,zjureel,09/Oct/23 05:17,26/Apr/24 16:55,04/Jun/24 20:40,06/Mar/24 01:59,1.19.0,,,,,,,,,,1.20.0,,,,API / DataStream,Table SQL / API,,,,,0,pull-request-available,,,,,"Introduce LineageGraph, LineageVertex and LineageEdge interfaces",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 06 01:59:20 UTC 2024,,,,,,,,,,"0|z1kt94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/24 01:59;zjureel;Closed by 6b9c282b5090720a02b941586cb7f5389691dba9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate Flink OLAP quick start guide to Chinese,FLINK-33209,13553314,13548014,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,xiangyu0xf,xiangyu0xf,xiangyu0xf,09/Oct/23 02:07,12/Oct/23 11:56,04/Jun/24 20:40,12/Oct/23 11:56,,,,,,,,,,,,,,,chinese-translation,,,,,,0,,,,,,Translate Flink OLAP quick start guide to Chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32755,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 11:56:23 UTC 2023,,,,,,,,,,"0|z1kt6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 03:30;shengbo;Hi xiangyu,  I am a Flink developer beginner and a native Chinese speaker.   Would you please assign this subtask to me？;;;","09/Oct/23 03:47;xiangyu0xf;Hi [~shengbo], thanks for your volunteering. Currently we are still improving this document both in English and Chinese version since OLAP is still in fast progress, so we'd rather keeping this work in house. Is that working for you?;;;","12/Oct/23 11:56;xiangyu0xf;duplicate with FLINK-33235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support the writable metadata timestamp for hbase connector,FLINK-33208,13553291,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanjialiang,tanjialiang,tanjialiang,08/Oct/23 13:45,14/Nov/23 14:37,04/Jun/24 20:40,14/Nov/23 14:37,hbase-3.0.1,,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,,,0,pull-request-available,,,,,"Currently, the hbase sink does not support write data with `timestamp`, which may cause the data to be written out of order. I suggest to support the timestamp writable metadata for hbase connector so that we can set the `timestamp` when we writing.
{code:java}
CREATE TABLE hTable (
 rowkey INT,
 family1 ROW<q1 INT>,
 version TIMESTAMP_LTZ(3) METADATA FROM 'timestamp',
 PRIMARY KEY (rowkey) NOT ENFORCED
) WITH (
 'connector' = 'hbase-2.2',
 'table-name' = 'mytable',
 'zookeeper.quorum' = 'localhost:2181'
);{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 14 14:37:42 UTC 2023,,,,,,,,,,"0|z1kt1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/23 14:37;martijnvisser;Fixed in apache/flink-connector-hbase

main: ce15c1baea93107a6324c87c2081dc1daae5978b
v3.0: f6023ed6a78d83d2000a207bcebee971f80ad7b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan hbase table will throw error when table is empty,FLINK-33207,13553280,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanjialiang,tanjialiang,tanjialiang,08/Oct/23 09:43,10/Nov/23 10:10,04/Jun/24 20:40,10/Nov/23 10:10,hbase-3.0.1,,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,,,0,pull-request-available,stale-blocker,,,,"When i scan the empty hbase table, it will throw an error when createInputSplits, we should return empty split instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 10:10:39 UTC 2023,,,,,,,,,,"0|z1ksyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Nov/23 10:10;martijnvisser;Fixed in apache/flink-connector-hbase 

main: 4b33c32a7f40b7e4fb469facf436017f2cdd8485
v3.0: 870ff0749d617ba03ecbd48632107793112e594e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verify the existence of hbase table before read/write,FLINK-33206,13553277,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanjialiang,tanjialiang,tanjialiang,08/Oct/23 08:59,10/Nov/23 08:23,04/Jun/24 20:40,10/Nov/23 08:23,hbase-3.0.1,,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,,,0,pull-request-available,stale-blocker,,,,"Currently, we do not verify the existence of hbase table before read/write, and the error would make the user confused.

The `HBaseSinkFunction` throws `TableNotFoundException` when do flush.
The `inputFormat` throws not obvious enough.
!image-2023-10-08-16-54-05-917.png!

So i think we should verify the existence of hbase table when call `open` function.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/23 08:54;tanjialiang;image-2023-10-08-16-54-05-917.png;https://issues.apache.org/jira/secure/attachment/13063414/image-2023-10-08-16-54-05-917.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:23:08 UTC 2023,,,,,,,,,,"0|z1ksy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Nov/23 08:23;martijnvisser;Fixed in apache/flink-connector-hbase

main: 00143773ba3f647099b7f53c17133fef99ab8fed
v3.0: db4f3389a791e954bdbca0f5fbaa3a09cdc95148;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Replace Akka with Pekko in the description of ""pekko.ssl.enabled""",FLINK-33205,13553270,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,08/Oct/23 04:57,09/Oct/23 05:57,04/Jun/24 20:40,09/Oct/23 05:57,1.18.0,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"Current description: ""Turns on SSL for Akka’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true."" ""Akka"" should be replaced with Pekko"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 05:57:36 UTC 2023,,,,,,,,,,"0|z1kswo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/23 05:32;Zhanghao Chen;[~chesnay] I've prepared a quick fix on it, could you help review it when you are available? Much appreciated;;;","09/Oct/23 05:57;zjureel;Fixed by d652238e84037a25eb0cef137f34edb37d9e1ac2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add description for missing options in the all jobmanager/taskmanager options section in document,FLINK-33204,13553269,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,08/Oct/23 04:42,09/Oct/23 07:18,04/Jun/24 20:40,09/Oct/23 07:18,1.17.0,1.18.0,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"There are 4 options which are excluded from the all jobmanager/taskmanager options section in the configuration document:
 # taskmanager.bind-host
 # taskmanager.rpc.bind-port
 # jobmanager.bind-host
 # jobmanager.rpc.bind-port

We should add them to the document under the all  jobmanager/taskmanager options section for completeness.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 07:18:09 UTC 2023,,,,,,,,,,"0|z1kswg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Oct/23 05:56;Zhanghao Chen;Hi [~xtsong], I've prepared a quick fix, could you help review it when you are free? Much appreciated.;;;","09/Oct/23 07:18;xtsong;master (1.19): 011b6b44074bad6b5f6db416f77a15c83a47ccc2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-374: Adding a separate configuration for specifying Java Options of the SQL Gateway,FLINK-33203,13553226,13548014,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,07/Oct/23 07:28,07/Nov/23 02:19,04/Jun/24 20:40,16/Oct/23 09:59,,,,,,,,,,,1.19.0,,,,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,"{color:#000000}The SQL Gateway is an essential component of Flink in OLAP scenarios, and its performance and stability determine the SLA of Flink as an OLAP service. Just like other components in Flink, we propose adding a separate configuration option to specify the Java options for the SQL Gateway. This would allow users to fine-tune the memory settings, garbage collection behavior, and other relevant Java parameters specific to the SQL Gateway, ensuring optimal performance and stability in production environments.{color}",,,,,,,,,,,,,,,,,,,,,,FLINK-33458,FLINK-32985,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 09:57:24 UTC 2023,,,,,,,,,,"0|z1ksmw:",9223372036854775807,"Flink introduces ""env.java.opts.sql-gateway"" for specifying the Java options for the SQL Gateway, which allows you to fine-tune the memory settings, garbage collection behavior, and other relevant Java parameters.",,,,,,,,,,,,,,,,,,,"16/Oct/23 09:57;guoyangze;master: aaaa07a614b387817b9443ae8d8c64eb7cfcfc4c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-327: Support switching from batch to stream mode to improve throughput when processing backlog data,FLINK-33202,13553221,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,xuannan,xuannan,07/Oct/23 06:11,03/Jun/24 10:57,04/Jun/24 20:40,,,,,,,,,,,,1.20.0,,,,Runtime / Task,,,,,,0,pull-request-available,,,,,"Umbrella issue for [https://cwiki.apache.org/confluence/display/FLINK/FLIP-327%3A+Support+switching+from+batch+to+stream+mode+to+improve+throughput+when+processing+backlog+data|https://cwiki.apache.org/confluence/display/FLINK/FLIP-309%3A+Support+using+larger+checkpointing+interval+when+source+is+processing+backlog]
h4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 03 10:57:02 UTC 2024,,,,,,,,,,"0|z1ksls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jun/24 10:57;lda-dima;[~xuannan] 

Hello.

I apologize for writing so late.

As I understand it, according to the description of FLIP-327, switching between batch and stream will take place by means of backlog data (depending on the type). Judging by the number of changes in the project code, this is probably not the best way. We tried to implement our own version of mode switching:
 * created an Event similar to EndOfDataEvent
 * similarly with endOfData intercept it in the operator
 * after receiving the event, the operator is switched to STREAM mode (initially the operator works in BATCH mode)

Note: It seems to me that the most likely case to use this feature is to process the accumulated data in batch mode and then fly to stream (by event).

Is it really need to do it through backlog data?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in CachingTopicSelector,FLINK-33201,13553218,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,qbx2,qbx2,07/Oct/23 05:46,13/Feb/24 02:50,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"Pull Request available at: https://github.com/apache/flink-connector-kafka/pull/55

 

In the CachingTopicSelector, a memory leak may occur when the internal logic fails to check the cache size due to a race condition. ([https://github.com/apache/flink-connector-kafka/blob/d89a082180232bb79e3c764228c4e7dbb9eb6b8b/flink-connector-kafka/src/main/java/org/apache/flink/connector/kafka/sink/KafkaRecordSerializationSchemaBuilder.java#L287-L289)]

 

By analyzing a Java heap dump, I identified a memory leak in the CachingTopicSelector. As in the screenshot, cache has 47,769 elements. If the internal logic were functioning correctly, the number of elements should be less than or equal to CACHE_RESET_SIZE (which is 5).","I am out of office now, so this is what I remember (the flink version may not be correct). Because I already identified the cause, this should not matter anyway.

EKS 1.24, x86_64, Bottlerocket OS, flink 1.14, scala 2.12",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/23 05:40;qbx2;273084767-29bc0d8a-7445-4a74-a6e1-7c836775c7b1.png;https://issues.apache.org/jira/secure/attachment/13063395/273084767-29bc0d8a-7445-4a74-a6e1-7c836775c7b1.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,"In the CachingTopicSelector, a memory leak may occur when the internal logic fails to check the cache size due to a race condition.",false,,,,,,,,,,,https://github.com/apache/flink-connector-kafka/pull/55,,,,,,,,,,,9223372036854775807,,,Java,,2023-10-07 05:46:02.0,,,,,,,,,,"0|z1ksl4:",9223372036854775807,Fix memory leak in CachingTopicSelector.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ItemAt Expression validation fail in Table API due to type mismatch,FLINK-33200,13553200,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,ZhenqiuHuang,ZhenqiuHuang,06/Oct/23 23:46,12/Oct/23 09:22,04/Jun/24 20:40,,1.17.1,1.18.0,,,,,,,,,,,,,Table SQL / API,,,,,,0,,,,,,"The table schema is defined as below:

public static final DataType DATA_TYPE = DataTypes.ROW(
            DataTypes.FIELD(""id"", DataTypes.STRING()),
            DataTypes.FIELD(""events"", DataTypes.ARRAY(DataTypes.MAP(DataTypes.STRING(), DataTypes.STRING())))
        );

public static final Schema SCHEMA = Schema.newBuilder().fromRowDataType(DATA_TYPE).build();


inTable.select(Expressions.$(""events"").at(1).at(""eventType"").as(""firstEventType"")

The validation fail as ""eventType"" is inferred as BasicTypeInfo.STRING_TYPE_INFO, the table key internally is a StringDataTypeInfo. The validation fail at 

case mti: MapTypeInfo[_, _] =>
        if (key.resultType == mti.getKeyTypeInfo) {
          ValidationSuccess
        } else {
          ValidationFailure(
            s""Map entry access needs a valid key of type "" +
              s""'${mti.getKeyTypeInfo}', found '${key.resultType}'."")
        }




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/23 19:31;andyglow;IssueDemo.java;https://issues.apache.org/jira/secure/attachment/13063463/IssueDemo.java",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 09:22:32 UTC 2023,,,,,,,,,,"0|z1ksh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/23 23:49;ZhenqiuHuang;[~lzljs3620320]

As StringDataTypeInfo is the default key type in Flink table, shall we loose the validation to 
 
if (key.resultType == StringDataTypeInfo || key.resultType == BasicTypeInfo.STRING_TYPE_INFO);;;","09/Oct/23 01:55;lzljs3620320;CC [~lincoln.86xy] ;;;","09/Oct/23 15:04;lincoln.86xy;[~ZhenqiuHuang] What flink version are you using? ;;;","09/Oct/23 15:30;ZhenqiuHuang;We tried both 1.17.1 and 1.18-SNAPSHOT;;;","09/Oct/23 19:39;andyglow;Hello, team. Here is a bit of the summary to this bug
h3. Given

​
Working on projection over dataset that contains data represented as array of maps of
string to string `array[map[string, string]]` several issues was identified.
h3. Data Schema

{{DataTypes.ROW(}}
   DataTypes.FIELD(""id"", DataTypes.STRING()),
   DataTypes.FIELD(""events"", DataTypes.ARRAY(DataTypes.MAP(DataTypes.STRING(), DataTypes.STRING())))
{{{}){}}}​
h3. Issues
h4. Issue 1. Extraction of map values by index and key.

 
{{inTable.select(}}
{{  Expressions.$(""events"").at(1).at(""eventType"").as(""firstEventType"")}}
{{).execute().print();}}
 
 
results in
 
{{Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Map entry access needs a valid key of type 'StringData', found 'String'.}}
 at org.apache.flink.table.planner.expressions.PlannerTypeInferenceUtilImpl.validateArguments(PlannerTypeInferenceUtilImpl.java:111)
at org.apache.flink.table.planner.expressions.PlannerTypeInferenceUtilImpl.runTypeInference(PlannerTypeInferenceUtilImpl.java:69)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.runLegacyTypeInference(ResolveCallByArgumentsRule.java:284)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.lambda$visit$2(ResolveCallByArgumentsRule.java:164)
at java.base/java.util.Optional.orElseGet(Optional.java:369)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:164)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:98)
at org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:37)
at org.apache.flink.table.expressions.UnresolvedCallExpression.accept(UnresolvedCallExpression.java:97)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:147)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule$ResolvingCallVisitor.visit(ResolveCallByArgumentsRule.java:98)
at org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:37)
at org.apache.flink.table.expressions.UnresolvedCallExpression.accept(UnresolvedCallExpression.java:97)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.lambda$apply$0(ResolveCallByArgumentsRule.java:92)
at java.base/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:271)
at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655)
at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)
at org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.apply(ResolveCallByArgumentsRule.java:93)
at org.apache.flink.table.expressions.resolver.ExpressionResolver.lambda$concatenateRules$2(ExpressionResolver.java:247)
at java.base/java.util.function.Function.lambda$andThen$1(Function.java:88)
at org.apache.flink.table.expressions.resolver.ExpressionResolver.resolve(ExpressionResolver.java:210)
at org.apache.flink.table.operations.utils.OperationTreeBuilder.projectInternal(OperationTreeBuilder.java:199)
at org.apache.flink.table.operations.utils.OperationTreeBuilder.project(OperationTreeBuilder.java:174)
at org.apache.flink.table.api.internal.TableImpl.select(TableImpl.java:142)
at IssueDemo.main(IssueDemo.java:73)}}
h4. Issue 2. Extraction of entire map by index.

{{inTable.select(}}
{{  Expressions.$(""events"").at(1).as(""firstEvent"")}}
{{).execute().print();}}
​
results in
 
{{Query schema: [firstEvent: MAP<STRING, STRING>]}}
{{Sink schema: [firstEvent: MAP<RAW('org.apache.flink.table.data.StringData', ?), RAW('org.apache.flink.table.data.StringData', ?)>]}}
 at org.apache.flink.table.planner.connectors.DynamicSinkUtils.createSchemaMismatchException(DynamicSinkUtils.java:1005)
at org.apache.flink.table.planner.connectors.DynamicSinkUtils.validateSchemaAndApplyImplicitCast(DynamicSinkUtils.java:349)
at org.apache.flink.table.planner.connectors.DynamicSinkUtils.validateSchemaAndApplyImplicitCast(DynamicSinkUtils.java:307)
at org.apache.flink.table.planner.connectors.DynamicSinkUtils.convertSinkToRel(DynamicSinkUtils.java:268)
at org.apache.flink.table.planner.connectors.DynamicSinkUtils.convertCollectToRel(DynamicSinkUtils.java:154)
at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:230)
at org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:194)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
at scala.collection.Iterator.foreach(Iterator.scala:937)
at scala.collection.Iterator.foreach$(Iterator.scala:937)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
at scala.collection.IterableLike.foreach(IterableLike.scala:70)
at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at scala.collection.TraversableLike.map(TraversableLike.scala:233)
at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:194)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1805)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:947)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1424)
at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
at IssueDemo.main(IssueDemo.java:80)}}
​
h3. Issue Reproduction

Full `IssueDemo.java` source code
```
{{import lombok.AllArgsConstructor;}}
{{import org.apache.flink.api.common.typeinfo.TypeInformation;}}
{{import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;}}
{{import org.apache.flink.table.api.DataTypes;}}
{{import org.apache.flink.table.api.Expressions;}}
{{{}import org.apache.flink.table.api.Schema;{}}}{{{}import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;{}}}
{{import org.apache.flink.table.data.*;}}
{{import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;}}
{{{}import org.apache.flink.table.types.DataType;{}}}{{{}​{}}}
{{import java.util.List;}}
{{import java.util.Map;}}
{{import java.util.stream.Collectors;}}
{{{}​{}}}{{{}public class IssueDemo {{}}}{{{}​{}}}
{{  @AllArgsConstructor}}
    public static class DataEnvelope {
{{    public final String id;}}
{{{}    public final List<Map<String, String>> events;{}}}{{{}​{}}}
{{    public RowData toRowData() {}}
{{      final var row = new GenericRowData(2);}}
{{      row.setField(0, StringData.fromString(id));}}
{{      row.setField(1, eventsMapData());}}
{{      return row;}}
{{{}    }{}}}{{{}​{}}}
{{    private ArrayData eventsMapData() {}}
{{      return new GenericArrayData(events.stream()}}
{{        .map(event -> }}new GenericMapData(event
{{          .entrySet()}}
{{          .stream()}}
{{          .collect(Collectors.toMap(}}
{{            e -> StringData.fromString(e.getKey()),}}
{{            e -> StringData.fromString(e.getValue())}}
{{        ))))}}
{{        .toArray(Object[]::new)}}
{{      );}}
{{{}    }{}}}{{{}​{}}}
{{    public static final DataType DATA_TYPE = DataTypes.ROW(}}
{{      DataTypes.FIELD(""id"", DataTypes.STRING()),}}
{{      DataTypes.FIELD(""events"", DataTypes.ARRAY(DataTypes.MAP(DataTypes.STRING(), DataTypes.STRING())))}}
{{{}    );{}}}{{{}​{}}}
{{{}    public static final TypeInformation<RowData> TYPE_INFO = InternalTypeInfo.of(DATA_TYPE.getLogicalType());{}}}{{{}​{}}}
{{    public static final Schema SCHEMA = Schema.newBuilder().fromRowDataType(DATA_TYPE).build();}}
{{{}  }{{}}}}{{{}​{}}}
{{  public static final List<DataEnvelope> TEST_DATA = List.of(}}
{{    new DataEnvelope(""1"", List.of(Map.of(""eventType"", ""enter""))),}}
{{    new DataEnvelope(""2"", List.of(Map.of(""eventType"", ""dialog""), Map.of(""eventType"", ""exit"")))}}
{{{}  );{}}}{{{}​{}}}
{{  public static void main(String[] args) throws Exception {}}
{{    final var env = StreamExecutionEnvironment.getExecutionEnvironment();}}
{{{}    final var tenv = StreamTableEnvironment.create(env);{}}}{{{}​{}}}
{{    final var inRows = TEST_DATA.stream().map(DataEnvelope::toRowData).collect(Collectors.toList());}}
{{    final var inStream = env.fromCollection(inRows, DataEnvelope.TYPE_INFO);}}
{{    final var inTable = tenv.fromDataStream(inStream, DataEnvelope.SCHEMA);}}
{{    // print test data}}
{{{}    ​{}}}{{{}inTable.execute().print();{}}}{{{}​{}}}
      // issue #1
{{      inTable.select(}}
{{      Expressions.$(""events"").at(1).at(""eventType"").as(""firstEventType"")}}
{{{}    ).execute().print();{}}}{{{}​{}}}
      // issue #2
{{      inTable.select(}}
{{      Expressions.$(""events"").at(1).as(""firstEvent"")}}
{{{}    ).execute().print();{}}}{{{}​{}}}
{{    env.close();}}

  }
{{{}}{}}}​
h3. Test

I also tried to work out a test, but might need some guidance from people with better knowledge of the subject.
​
One of the obvious places to test it (from novice perspective) is `ExpressionResolverTest` class.
This route didn't work though. The reason is that `FunctionLookup` `Mock` is pretty limited around 
supported function definitions. Adding `PlanerExpression`, which is used in production code
(see `org.apache.flink.table.expressions.resolver.rules.ResolveCallByArgumentsRule.ResolvingCallVisitor#runLegacyTypeInference`),
to the picture doesn't really work as it introduces circular dependency between (`flink-table-api-java` and `flink-table-plannner_2.12`).​
h3. Diagnosis

​
`AT` function definition does have `MISSING` output type stragegy, which, according to code in `ResolveCallByArgumentsRule.getOptionalTypeInference` makes
flow fall to `ResolveCallByArgumentsRule.runLegacyTypeInference`. So first question is (again, from novice point of view) - do we really want to fall to
legacy procedures here. The legacy procedure, which is mainly specified as `PlannerTypeInferenceUtilImpl -> TypeConversions.fromLegacyInfoToDataType -> LegacyTypeInfoDataTypeConverter.toDataType`
chain call seems to having no ideas about `StringData` type, which leads to usage of legacy wrapper for `StringData` here `org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter#createLegacyType`.
So the second question is: if the legacy logical branch here is intentional, should we add proper support for `StringData` there?;;;","10/Oct/23 19:36;andyglow;Seems like I have caught several more issues related to `AT` expression and type resolution.
 # accessing array of primitives doesn't work if at data level arrays represented as ArrayData. What I got is: `Unsupported conversion from data type 'ARRAY<INT>' (conversion class: org.apache.flink.table.data.ArrayData) to type information. Only data types that originated from type information fully support a reverse conversion.`
 # accessing `Map<Long, String>` throws too. `map.at(0L)` gets failed with Incompatible types for sink column
{{Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Column types of query result and sink for '*anonymous_collect$3*' do not match.}}
{{Cause: Incompatible types for sink column 'longData_at_0' at position 0.}}
{{Query schema: [longData_at_0: STRING]}}
{{Sink schema: [longData_at_0: RAW('org.apache.flink.table.data.StringData', ?)]}}

All issues are reflected in `IssueDemo.java` (see attachments).;;;","11/Oct/23 06:06;xuyangzhong;Hi, [~andyglow] . I think the issue 1 and 2 are caused by different bugs. 

 

The issue 1 is caused by that the field 'events' refs from the table, and its actual type is StringData, converted from the origin type DataTypes.STRING (). But the key of the 'AT' named 'eventType' is DataType.STRING, not StringData. So the difference about these two types causes the failure about validating 'AT'.

 

The issue 2 is caused by that if the planner can't infer the result type of the table expression (that means the result type is Polymorphic, depending on the type of args), the legacy rule is used to infer the result type. But the legacy rules wrongly create a LegacyTypeInformation for StringData. Not only 'AT', but also 'COLLECT' (inTable.select(Expressions.$(""id"").collect()).execute().print()) will cause this bug.

 

I'll try to fix all of them.;;;","11/Oct/23 08:50;xuyangzhong;Hi, [~ZhenqiuHuang] , can you provide the code caused this issue?

From the example provided by [~andyglow] , I found that an internal TypeInformation 'InternalTypeInfo' which should not be used actually is used. After replacing 'InternalTypeInfo' with 'ExternalTypeInfo', issue 1 and 2 all work fine. The new example can be found following. The main changes is:
 # use 'ExternalTypeInfo.of(DATA_TYPE)' instead of 'InternalTypeInfo.of(DATA_TYPE.getLogicalType())'
 # remove 'ArrayData', 'GenericMapData' and 'StringData'
 # use 'Row' instead of 'RowData'

 
{code:java}
public class DemoTest {

    public static class DataEnvelope {
        public final String id;
        public final List<Map<String, String>> events;

        public DataEnvelope(String id, List<Map<String, String>> events) {
            this.id = id;
            this.events = events;
        }

        public Row toRowData() {
            final Row row = new Row(2);
            row.setField(0, id);
            //            row.setField(1, eventsMapData());
            row.setField(1, events.toArray());

            return row;
        }

        public static final DataType DATA_TYPE =
                DataTypes.ROW(
                        DataTypes.FIELD(""id"", DataTypes.STRING()),
                        DataTypes.FIELD(
                                ""events"",
                                DataTypes.ARRAY(
                                        DataTypes.MAP(DataTypes.STRING(), DataTypes.STRING()))));
        public static final TypeInformation<RowData> TYPE_INFO = ExternalTypeInfo.of(DATA_TYPE);

        public static final Schema SCHEMA = Schema.newBuilder().fromRowDataType(DATA_TYPE).build();
    }

    public static final List<DataEnvelope> TEST_DATA =
            Lists.newArrayList(
                    new DataEnvelope(
                            ""1"", Lists.newArrayList(Maps.newHashMap(""eventType"", ""enter""))),
                    new DataEnvelope(
                            ""2"",
                            Lists.newArrayList(
                                    Maps.newHashMap(""eventType"", ""dialog""),
                                    Maps.newHashMap(""eventType"", ""exit""))));

    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        final StreamTableEnvironment tenv = StreamTableEnvironment.create(env);
        final List inRows =
                TEST_DATA.stream().map(DataEnvelope::toRowData).collect(Collectors.toList());
        final DataStreamSource inStream = env.fromCollection(inRows, DataEnvelope.TYPE_INFO);
        final Table inTable = tenv.fromDataStream(inStream, DataEnvelope.SCHEMA);
        // issue #1
        inTable.select(Expressions.$(""events"").at(1).at(""eventType"").as(""firstEventType""))
                .execute()
                .print();
        // issue #2
        inTable.select(Expressions.$(""events"").at(1).as(""firstEvent"")).execute().print();
        env.close();
    }
}
 {code}
 

 ;;;","11/Oct/23 17:35;andyglow;Hello, [~xuyangzhong]. Thanks for your analysis!

Can I get some guidance regarding Row vs RowData, String vs StringData, Timestamp/LocalDateTime vs TimestampData, T[] vs ArrayData, etc

I think I saw some comments saying that `*Data` struct types are the perspective API that is going to be used going forward. May be I misinterpret it. How would you define a guidline?

Thank you;;;","12/Oct/23 02:19;xuyangzhong;Hi, [~andyglow] , maybe this flip[1] can be referred.

[1][https://cwiki.apache.org/confluence/display/FLINK/FLIP-37%3A+Rework+of+the+Table+API+Type+System]

I think it's ok for user to use StringData directly with tag '@PublicEvolving', and the root problem is the misuse of 'InternalTypeInfo' which is tagged as '@Internal'.;;;","12/Oct/23 02:21;xuyangzhong;cc [~twalthr] ;;;","12/Oct/23 09:22;twalthr;We should not use `PlannerTypeInferenceUtilImpl` anymore. There are still roughly 20 functions left that use the old inference. Upgrading those functions should also fix this issue here. The new stack works purely on logical types and the class does not matter anymore. The old stack was based on TypeInformation where the class clearly mattered.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArchitectureTests should test for fully qualified class names instead of Class objects,FLINK-33199,13553165,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,afedulov,afedulov,afedulov,06/Oct/23 16:01,13/Oct/23 09:08,04/Jun/24 20:40,12/Oct/23 12:33,,,,,,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,"Currently architecture tests rely on importing such classes as MiniClusterExtension. This introduces a production scope dependency on flink-test-utils which in turn depends on flink-streaming-java. This is problematic because adding architecture tests to any direct or transitive dependency of flink-streaming-java creates a dependency cycle.

Example: https://github.com/apache/flink/pull/22850#discussion_r1243343382

In general, since architecture tests are supposed to be used freely in any submodule, it is desirable to reduce its dependency surface as much as possible to prevent such cycles. 

This can be achieved by moving away from using Class objects and employing fully qualified type names checks instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 12 12:33:44 UTC 2023,,,,,,,,,,"0|z1ks9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 12:33;chesnay;master: 71c407a0b94022339319807032982e7ee604cc7e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add timestamp with local time zone support in Avro converters,FLINK-33198,13553154,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,ZhenqiuHuang,ZhenqiuHuang,ZhenqiuHuang,06/Oct/23 14:24,11/Apr/24 08:47,04/Jun/24 20:40,24/Jan/24 02:45,,,,,,,,,,,1.19.0,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,1,pull-request-available,,,,,"Currently, RowDataToAvroConverters doesn't handle with LogicType TIMESTAMP_WITH_LOCAL_TIME_ZONE. We should add the corresponding conversion.",,,,,,,,,,,,,,,,,,,,,,,FLINK-30483,,,,,,,,,FLINK-34250,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 24 02:45:30 UTC 2024,,,,,,,,,,"0|z1ks6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 08:56;martijnvisser;Changed the fixVersion to 1.19.0 since this isn't a bug in Flink 1.18 so can't be addressed in a patch version;;;","15/Nov/23 01:14;james.johnston.thumbtack;The proposal in FLIP-378 sounds good to me.;;;","24/Jan/24 02:45;leonard;Implemented in master(1.19): 533ead6ae946cbc77525d276b6dea965d390181a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink support for ByteArraySchema,FLINK-33197,13553110,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,liu.chong,liu.chong,06/Oct/23 09:17,10/Oct/23 05:21,04/Jun/24 20:40,,1.17.0,,,,,,,,,,,,,,API / Python,,,,,,0,,,,,,"Currently in Python Flink API, when reading messages from a Kafka source, only SimpleStringSchema is available.
If the data is in arbitary binary format(e.g. marshalled Protocol Buffer msg) it may not be decodable with the default 'utf-8' encoding. 
There's currently a workaround which is to manually set the encoding to 'ISO-8859-1' which supports all possible byte combinations. 
However this is not an elegant solution.
We should support ByteArraySchema which outputs a raw byte array for subsequent unmarshalling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 05:21:32 UTC 2023,,,,,,,,,,"0|z1krx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 05:21;liu.chong;we've got the code ready for adding ByteArraySchema to pyflink, would you like to give some comments if we should move forward and submit the PR? [~dianfu] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support DB2 JDBC driver ,FLINK-33196,13553032,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,davidradl,davidradl,05/Oct/23 16:11,21/Feb/24 01:46,04/Jun/24 20:40,06/Oct/23 08:22,,,,,,,,,,,jdbc-3.2.0,,,,Connectors / JDBC,,,,,,0,,,,,,"The [docs|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/jdbc/] show the currently supported JDBC drivers. I propose that we add DB2 to this list. 

 

I'm happy to provide the implementation. It looks like I need to create a new DB2 dialect in [https://github.com/apache/flink-connector-jdbc/tree/main/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/databases|http://example.com/] , with appropriate DB2 specific tests and update the documentation. 

 ",,,,,,,,,,FLINK-14102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 06 08:21:55 UTC 2023,,,,,,,,,,"0|z1krfs:",9223372036854775807,duplicate of https://issues.apache.org/jira/browse/FLINK-14102,,,,,,,,,,,,,,,,,,,"05/Oct/23 16:19;Sergey Nuyanzin;I tend to think that this is a duplicate of https://issues.apache.org/jira/browse/FLINK-14102

[~davidradl] can you please clarify whether you really need a separate DB2 jdbc driver (then not clear how should it look like) or just support of DB2 dialect for jdbc connector?;;;","06/Oct/23 08:21;davidradl;[~Sergey Nuyanzin] thanks I had not seen that. Great spot. I will close this out as a duplicate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticSearch Connector should directly depend on 3rd-party libs instead of flink-shaded repo,FLINK-33195,13552985,13552952,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,jingge,jingge,05/Oct/23 09:10,02/Dec/23 13:42,04/Jun/24 20:40,02/Dec/23 13:42,1.18.0,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-05 09:10:36.0,,,,,,,,,,"0|z1kr5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWS Connector should directly depend on 3rd-party libs instead of flink-shaded repo,FLINK-33194,13552963,13552952,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jingge,jingge,05/Oct/23 06:05,27/Oct/23 09:17,04/Jun/24 20:40,27/Oct/23 09:17,1.18.0,,,,,,,,,,,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 09:17:16 UTC 2023,,,,,,,,,,"0|z1kr0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 09:17;hong;merged commit [{{c244bb3}}|https://github.com/apache/flink-connector-aws/commit/c244bb30240399f67557bbaf8e718d5eb158199d] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC Connector should directly depend on 3rd-party libs instead of flink-shaded repo,FLINK-33193,13552962,13552952,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,jingge,jingge,05/Oct/23 06:04,02/Dec/23 13:42,04/Jun/24 20:40,02/Dec/23 13:42,1.18.0,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-05 06:04:53.0,,,,,,,,,,"0|z1kr08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State memory leak in the Window Operator due to unregistered cleanup timer,FLINK-33192,13552955,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,ym,vkalvakunta,vkalvakunta,05/Oct/23 05:07,03/Jun/24 14:02,04/Jun/24 20:40,,1.14.6,1.15.4,1.16.2,1.17.1,,,,,,,,,,,Runtime / State Backends,,,,,,0,easyfix,,,,,"I have encountered a state memory leak issue in the default window operator. The cleanup timer for a window is not registered when it does not emit a result if it’s fired immediately after creation. The window is added to the window state and as the cleanup timer isn't registered, it's never cleaned up, allowing it to live forever.

*Steps to Reproduce:*
 # Write a custom trigger that triggers for every element.
 # Write a custom aggregate function that never produces a result.
 # Use a default tumbling event time window with this custom trigger and aggregate function.
 # Publish events spanning multiple time windows.
 # The window state will contain all the windows even after their expiry/cleanup time.

*Code with the bug:*

[https://github.com/apache/flink/blob/cd95b560d0c11a64b42bf6b98107314d32a4de86/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java#L398-L417|https://github.com/apache/flink/blob/cd95b560d0c11a64b42bf6b98107314d32a4de86/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperator.java#L399-L417]
{code:java}
windowState.setCurrentNamespace(window);
windowState.add(element.getValue());

if (triggerResult.isFire()) {
    ACC contents = windowState.get();
    if (contents == null) {
        continue;
    }
    emitWindowContents(window, contents);
}

if (triggerResult.isPurge()) {
    windowState.clear();
}
registerCleanupTimer(window);{code}
 

*Expected Result:*

The cleanup timer should be registered for every window that's added to the window state regardless of it emitting a result after it’s fired.

*Actual Result:*

The cleanup timer is not registered for a window when it does not emit a result after it’s fired, causing the window state that is already created to live on indefinitely.

*Impact:*

This issue led to a huge state memory leak in our applications and was very challenging to identify.

 

*Fix:*

There are two ways to fix this issue. I'm willing to create a PR with the fix if approved.

1. Register the cleanup timer immediately after a window is added to the state.
{code:java}
windowState.setCurrentNamespace(window);
windowState.add(element.getValue());
registerCleanupTimer(window);{code}
2. Emit the results when the contents are not null and remove the continue statement.
{code:java}
if (triggerResult.isFire()) {
    ACC contents = windowState.get();
    if (contents != null) {         
        emitWindowContents(window, contents);
    }
} {code}
 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-05 05:07:25.0,,,,,,,,,,"0|z1kqyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Connector should directly depend on 3rd-party libs instead of flink-shaded repo,FLINK-33191,13552953,13552952,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,jingge,jingge,05/Oct/23 05:03,11/Oct/23 16:06,04/Jun/24 20:40,11/Oct/23 16:06,kafka-3.0.0,,,,,,,,,,kafka-3.0.1,kafka-3.1.0,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 16:06:29 UTC 2023,,,,,,,,,,"0|z1kqy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 16:06;tzulitai;main
bd260f14dd280f464e5dcea76b3735782312b4ae

v3.0
58e500378193b76fe2262818de3703208a028728;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Umbrella]Externalized Connectors should directly depend on 3rd-party libs instead of shaded repo ,FLINK-33190,13552952,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,jingge,jingge,05/Oct/23 05:01,02/Dec/23 13:42,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Build System,,,,,,0,,,,,,"Connectors shouldn't depend on flink-shaded.
The overhead and/or risks of doing/supporting that right now far
outweigh the benefits.
( Because we either have to encode the full version for all dependencies
into the package, or accept the risk of minor/patch dependency clashes)
Connectors are small enough in scope that depending directly on
guava/jackson/etc. is a fine approach, and they have plenty of other
dependencies that they need to manage anyway; let's treat these the same
way.

 

https://lists.apache.org/thread/mtypmprz2b5p20gj064d0wsz3k0ofpco",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Dec 02 13:42:08 UTC 2023,,,,,,,,,,"0|z1kqy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 14:46;martijnvisser;[~jingge] I checked the connectors and I believe that JDBC, AWS and Elasticsearch don't rely on Shaded in their {{main}} branch;;;","02/Dec/23 13:42;jingge;[~martijnvisser] thanks for the hint!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FsCompletedCheckpointStorageLocation#disposeStorageLocation non-recursively deletes a directory,FLINK-33189,13552941,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,vladov,vladov,04/Oct/23 23:39,05/Oct/23 06:37,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,,,,,,"FsCompletedCheckpointStorageLocation attempts to non-recursively delete a directory
[https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsCompletedCheckpointStorageLocation.java#L74]

However, per the documentation of Flink's FileSystem Interface, such attempts are explicitly expected to fail. 
{code:java}
     * @param recursive if path is a directory and set to <code>true</code>, the directory is
     *     deleted else throws an exception. In case of a file the recursive can be set to either
     *     <code>true</code> or <code>false</code>{code}
[https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/core/fs/FileSystem.java#L689]

I am sure there is a non-negligible chance I am missing some flink internals here considering the class has not been touched since 2018 but my read is the above is either a bug, or it would be nice to update the FileSystem#delete docs.

Thanks for taking a look :) !",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 05 06:37:41 UTC 2023,,,,,,,,,,"0|z1kqvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 23:50;vladov;For posterity, I was able to locate
https://issues.apache.org/jira/browse/FLINK-13856 and https://issues.apache.org/jira/browse/FLINK-17860 that seemed somewhat similar but also broader in scope. https://issues.apache.org/jira/browse/FLINK-31685 seems most directly related.

However, the first two are both fairly old and I think some guidance on how a pluggable File System should implement #delete is independently warranted of the fix made here.;;;","05/Oct/23 06:37;mapohl;Hi [~vladov], thanks for bringing this to the community's attention. You're right: The JavaDoc is misleading. This part of the code isn't my field of expertise. But looking at the code, it's most likely that Flink's FileSystem interface was derived from [Hadoop's FileSystem interface|https://github.com/apache/hadoop/blob/rel/release-2.10.2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L1551] (which has the exact same requirement stated in the JavaDoc).

Flink's general {{FileSystem}} implementations do not seem to follow this contract properly (the {{FileSystem}} contract is generally not well implemented based on my experience). [LocalFileSystem|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java#L174] only fails if the directory is not empty or doesn't exist. [Flink's HadoopFileSystem|https://github.com/apache/flink/blob/0e4a6661deefa06039f422148164b8f765eafc98/flink-filesystems/flink-hadoop-fs/src/main/java/org/apache/flink/runtime/fs/hdfs/HadoopFileSystem.java] relies on [Hadoop's S3AFileSystem|https://github.com/apache/hadoop/blob/rel/release-2.10.2/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L1449] where the delete call also only fails if the directory is not empty. The customized delete functionality of the [FlinkS3PrestoFileSystem|https://github.com/apache/flink/blob/4907f5fb7033b1f918c4e721a1ccf95878f980a2/flink-filesystems/flink-s3-fs-presto/src/main/java/org/apache/flink/fs/s3presto/FlinkS3PrestoFileSystem.java#L62] doesn't do the content check but rather relies on the [delete functionality of PrestoS3FileSystem|https://github.com/prestodb/presto/blob/0.272/presto-hive/src/main/java/com/facebook/presto/hive/s3/PrestoS3FileSystem.java#L475]. {{PrestoS3FileSystem}} actually follows the contract and throws an {{IOException}} if {{false}} is passed (without checking whether the directory is empty; even though the error message of the {{IOException}} states something else).

This analysis proves my point that the {{FileSystem}} contract is not properly followed. Generally, the Flink 2.0 efforts would be a great opportunity to align the {{FileSystem}} contract.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink MapState with Types.ROW() throws exception,FLINK-33188,13552936,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,elkhand,elkhand,04/Oct/23 20:47,05/Oct/23 18:28,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,API / Python,API / Type Serialization System,,,,,0,,,,,,"I'm trying to use MapState, where the value will be a list of <class 'pyflink.common.types.Row'> type elements.
 
Wanted to check if anyone else faced the same issue while trying to use MapState in PyFlink with complex types.
 
Here is the code:
 
from pyflink.common import Time
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import (
    KeyedCoProcessFunction,
    KeySelector,
    RuntimeContext,
)
from pyflink.datastream.state import (
    MapStateDescriptor,
    StateTtlConfig,
    ValueStateDescriptor,
    ListStateDescriptor
)
from pyflink.table import DataTypes, StreamTableEnvironment


class MyKeyedCoProcessFunction(KeyedCoProcessFunction):
    def __init__(self):
        self.my_map_state = None

    def open(self, runtime_context: RuntimeContext):
        state_ttl_config = (
            StateTtlConfig.new_builder(Time.seconds(1))
            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite)
            .disable_cleanup_in_background()
            .build()
        )

        my_map_state_descriptor = MapStateDescriptor(
            ""my_map_state"",
            Types.SQL_TIMESTAMP(),
            Types.LIST(Types.ROW([
                Types.STRING(), 
                Types.STRING(), 
                Types.STRING(), 
                Types.STRING(), 
                Types.STRING(), 
                Types.STRING(), 
                Types.STRING(), 
                Types.STRING(), 
                Types.SQL_TIMESTAMP(), 
                Types.SQL_TIMESTAMP(), 
                Types.SQL_TIMESTAMP(), 
                Types.BIG_INT() 
            ]))
        )
        my_map_state_descriptor.enable_time_to_live(state_ttl_config)
        self.my_map_state = runtime_context.get_map_state(my_map_state_descriptor)
 
But while running this code, it fails with this exception at job startup (at runtime_context.get_map_state(my_map_state_descriptor)), even without trying to add anything to the state.
 
File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 249, in pyflink.fn_execution.beam.beam_operations_fast.StatefulFunctionOperation.__init__
File""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 132, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.__init__
File""/usr/local/lib64/python3.9/site-packages/pyflink/fn_execution/datastream/process/operations.py"", line 127, in open
self.open_func()
File""/usr/local/lib64/python3.9/site-packages/pyflink/fn_execution/datastream/process/operations.py"", line 296, in open_func
process_function.open(runtime_context)
File""/tmp/ipykernel_83481/1603226134.py"", line 57, in open
File""/usr/local/lib64/python3.9/site-packages/pyflink/fn_execution/datastream/process/runtime_context.py"", line 125, in get_map_state
map_coder = from_type_info(state_descriptor.type_info) # type: MapCoder
File""/usr/local/lib64/python3.9/site-packages/pyflink/fn_execution/coders.py"", line 812, in from_type_info
from_type_info(type_info._key_type_info), from_type_info(type_info._value_type_info))
File""/usr/local/lib64/python3.9/site-packages/pyflink/fn_execution/coders.py"", line 809, in from_type_info
returnGenericArrayCoder(from_type_info(type_info.elem_type))
File""/usr/local/lib64/python3.9/site-packages/pyflink/fn_execution/coders.py"", line 819, in from_type_info
[f for f in type_info.get_field_names()])
File""/usr/local/lib64/python3.9/site-packages/pyflink/common/typeinfo.py"", line 377, in get_field_names
j_field_names = self.get_java_type_info().getFieldNames()
File""/usr/local/lib64/python3.9/site-packages/pyflink/common/typeinfo.py"", line 391, in get_java_type_info
j_types_array = get_gateway()\
File""/usr/local/lib64/python3.9/site-packages/pyflink/java_gateway.py"", line 62, in get_gateway
_gateway = launch_gateway()
File""/usr/local/lib64/python3.9/site-packages/pyflink/java_gateway.py"", line 86, in launch_gateway
raise Exception(""It's launching the PythonGatewayServer during Python UDF execution ""
Exception: It's launching the PythonGatewayServer during Python UDF execution which is unexpected. It usually happens when the job codes are in the top level of the Python script file and are not enclosed in a `if name == 'main'` statement.If I switch from Tupes.ROW to Types.TUPLE() it works without any exception.
 
This works:
 
my_map_state_descriptor = MapStateDescriptor(
            ""my_map_state"",
            Types.SQL_TIMESTAMP(),
            Types.LIST(Types.TUPLE([
                Types.STRING(),
                Types.STRING(),
                Types.STRING(),
                Types.STRING(),
                Types.STRING(),
                Types.STRING(),
                Types.STRING(),
                Types.STRING(),
                Types.SQL_TIMESTAMP(),
                Types.SQL_TIMESTAMP(),
                Types.SQL_TIMESTAMP(),
                Types.BIG_INT()
            ]))
        )",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python3,,Thu Oct 05 18:28:10 UTC 2023,,,,,,,,,,"0|z1kqug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Oct/23 18:28;elkhand;After digging into the flink-python code, It seems if `PYFLINK_GATEWAY_DISABLED` is set to false in an environment variable, then using Types.LIST(Types.ROW([...])) does not have any issue, once Java Gateway is launched.
 
It was unexpected for Flink local run to set this flag to false explicitly.
 
This is a workaround for this issue:

 
 def open(self, runtime_context: RuntimeContext):
        state_ttl_config = (
            StateTtlConfig.new_builder(Time.seconds(1))
            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite)
            .disable_cleanup_in_background()
            .build()
        )
        import os
        os.environ[""PYFLINK_GATEWAY_DISABLED""] = ""0"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't record duplicate event if no change,FLINK-33187,13552924,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,claraxiong,claraxiong,claraxiong,04/Oct/23 17:57,24/Oct/23 18:11,04/Jun/24 20:40,09/Oct/23 06:13,1.17.1,,,,,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,,,,,,0,pull-request-available,,,,,"Problem:
Some events are recorded repeatedly such as ScalingReport when autoscaling is not enable,  which consists 99% of all events in our prod env. This wastes resources and causes performance  downstream.

Proposal:
Suppress duplicate event within an interval defined by a new operator config ""scaling.report.interval"" in second, defaulted to 1800.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 24 18:11:11 UTC 2023,,,,,,,,,,"0|z1kqrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 06:13;gyfora;merged to main 305498a9ab2e04ab71a4c2d87f2edb746373df1a;;;","19/Oct/23 15:23;claraxiong;Anyone have opinions on the default interval? Is 30 min a reasonable duration?;;;","24/Oct/23 18:11;gyfora;merged to main faaff564e1bb3d8ca51c939d34dd416585a3de74;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished fails on AZP,FLINK-33186,13552919,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,Jiang Xin,Sergey Nuyanzin,Sergey Nuyanzin,04/Oct/23 17:21,31/May/24 13:20,04/Jun/24 20:40,,1.18.1,1.19.0,,,,,,,,,,,,,Runtime / Checkpointing,,,,,,0,test-stability,,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53509&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8762
fails as
{noformat}
Sep 28 01:23:43 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task local checkpoint failure.
Sep 28 01:23:43 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:550)
Sep 28 01:23:43 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2248)
Sep 28 01:23:43 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2235)
Sep 28 01:23:43 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$null$9(CheckpointCoordinator.java:817)
Sep 28 01:23:43 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Sep 28 01:23:43 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Sep 28 01:23:43 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
Sep 28 01:23:43 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
Sep 28 01:23:43 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Sep 28 01:23:43 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Sep 28 01:23:43 	at java.lang.Thread.run(Thread.java:748)

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 31 13:20:47 UTC 2024,,,,,,,,,,"0|z1kqqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 17:22;Sergey Nuyanzin;[~Jiang Xin], [~lindong] since it is very similar to FLINK-32996 could you please have a look?;;;","05/Oct/23 05:00;mapohl;Isn't that a blocker as long it's not determined what the cause is? Just asking because the {{CheckpointAfterAllTasksFinishedITCase}}-related issues FLINK-32996 and FLINK-32907 have a fix version 1.18.0.;;;","05/Oct/23 13:02;lindong;cc [~xtsong] ;;;","05/Oct/23 14:46;Jiang Xin;I'll take a look at it.;;;","10/Oct/23 06:52;Jiang Xin;[~Sergey Nuyanzin] I don't think the failure is related to FLINK-28386 or FLINK-32996.

The test code triggered a savepoint after some subtasks were completed, but from the logs, it can be seen that the subtask `passA ->Sink: sinkA (2/4)` went to the finished state right after initiating the savepoint. Then, TaskExecutor believed that it had received a checkpoint request for an unknown task, thus it failed. So I think it is an existing concurrency issue.
{code:java}
01:23:40,823 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - passA -> Sink: sinkA (1/4) (3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_0_0) switc
hed from RUNNING to FINISHED.
01:23:40,823 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - passA -> Sink: sinkA (3/4) (3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_2_0) switc
hed from RUNNING to FINISHED.
01:23:40,826 [flink-pekko.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Triggering cancel-with-savepoint for job c82d241f9952e043dfed65318f0d962a.
01:23:40,828 passA -> Sink: sinkA (2/4)#0 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - passA -> Sink: sinkA (2/4)#0 (3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_1_0) switched from RUNNING to FINISHED.
01:23:40,828 passA -> Sink: sinkA (2/4)#0 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for passA -> Sink: sinkA (2/4)#0 (3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_1_0).
01:23:40,828 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task passA -> Sink: sinkA (2/4)#0 3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_1_0.
01:23:40,829 [flink-pekko.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - passA -> Sink: sinkA (2/4) (3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_1_0) switched from RUNNING to FINISHED.
01:23:40,829 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 (type=SavepointType{name='Savepoint', postCheckpointAction=NONE, formatType=CANONICAL}) @ 1695864220828 for job c82d241f9952e043dfed65318f0d962a.
01:23:40,917 [flink-pekko.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering Checkpoint 2 for job c82d241f9952e043dfed65318f0d962a failed due to org.apache.flink.runtime.checkpoint.CheckpointException: TaskManager received a checkpoint request for unknown task 3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_1_0. Failure reason: Task local checkpoint failure.
01:23:40,918 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 2 for job c82d241f9952e043dfed65318f0d962a. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: TaskManager received a checkpoint request for unknown task 3146192e86ef62554451af0e39df80b5_51397532e2d9c7a21097a30d590b3114_1_0. Failure reason: Task local checkpoint failure.
        at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:1046) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309) ~[?:?]
        at org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-core-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307) ~[?:?]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222) ~[?:?]
        at org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168) ~[?:?]
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33) ~[?:?]
        at org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29) ~[?:?]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[scala-library-2.12.7.jar:?]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[scala-library-2.12.7.jar:?]
        at org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29) ~[?:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[scala-library-2.12.7.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[scala-library-2.12.7.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[scala-library-2.12.7.jar:?]
        at org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547) ~[?:?]
        at org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545) ~[?:?]
        at org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229) ~[?:?]
        at org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590) ~[?:?]
        at org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557) ~[?:?]
        at org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280) ~[?:?]
        at org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241) ~[?:?]
        at org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253) ~[?:?]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_292]
01:23:40,929 [ForkJoinPool-1-worker-25] INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Shutting down Flink Mini Cluster
  {code};;;","02/Nov/23 08:40;mapohl;This issue popped up twice in the efforts around moving to Github Actions (FLINK-27075):
* https://github.com/XComp/flink/actions/runs/6724579458/job/18277659778#step:12:8453
* https://github.com/XComp/flink/actions/runs/6723279744/job/18273667605#step:12:8453;;;","24/Jan/24 07:50;mapohl;1.18: https://github.com/XComp/flink/actions/runs/7632434711/job/20792993223#step:10:8585

I add 1.18 as well because the GHA workflow was based on release-1.18. That also backs [~Jiang Xin] claim that it's not newly introduced in 1.19.;;;","25/Jan/24 10:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56823&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8426;;;","12/Feb/24 08:30;mapohl;https://github.com/apache/flink/actions/runs/7866453155/job/21460933108#step:10:7710;;;","26/Feb/24 07:32;mapohl;https://github.com/apache/flink/actions/runs/8027473900/job/21931656512#step:10:7665

{code}
Error: 02:29:11 02:29:11.608 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.619 s <<< FAILURE! -- in org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase
Error: 02:29:11 02:29:11.609 [ERROR] org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished -- Time elapsed: 1.601 s <<< ERROR!
Feb 24 02:29:11 java.util.concurrent.ExecutionException: org.apache.flink.runtime.checkpoint.CheckpointException: Task local checkpoint failure.
Feb 24 02:29:11 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
Feb 24 02:29:11 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
Feb 24 02:29:11 	at org.apache.flink.test.checkpointing.CheckpointAfterAllTasksFinishedITCase.testRestoreAfterSomeTasksFinished(CheckpointAfterAllTasksFinishedITCase.java:124)
Feb 24 02:29:11 	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
Feb 24 02:29:11 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Feb 24 02:29:11 Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task local checkpoint failure.
Feb 24 02:29:11 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:554)
Feb 24 02:29:11 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2260)
Feb 24 02:29:11 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2247)
Feb 24 02:29:11 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$triggerCheckpointRequest$9(CheckpointCoordinator.java:817)
Feb 24 02:29:11 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
Feb 24 02:29:11 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
Feb 24 02:29:11 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
Feb 24 02:29:11 	at java.base/java.lang.Thread.run(Thread.java:833)
{code};;;","19/Mar/24 13:34;rskraba;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58399&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8063] (1.20);;;","21/Mar/24 10:52;mapohl;https://github.com/apache/flink/actions/runs/8304570591/job/22730524813#step:10:7494;;;","21/Mar/24 11:05;mapohl;https://github.com/apache/flink/actions/runs/8320416262/job/22765302151#step:10:7608;;;","21/Mar/24 11:13;mapohl;https://github.com/apache/flink/actions/runs/8369823390/job/22916375709#step:10:7894;;;","17/Apr/24 14:49;rskraba;1.20 test_cron_hadoop313 https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58958&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8314
1.20 Java 8: Test (module: tests) https://github.com/apache/flink/actions/runs/8719280474/job/23918749100#step:10:8028
;;;","14/May/24 12:10;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=59529&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8036;;;","16/May/24 08:42;rskraba;* 1.18 AdaptiveScheduler / Test (module: tests) https://github.com/apache/flink/actions/runs/9088951392/job/24979573762#step:10:7852;;;","23/May/24 12:58;rskraba;* 1.19 Default (Java 8) / Test (module: tests) https://github.com/apache/flink/actions/runs/9201085836/job/25309084773#step:10:8471;;;","31/May/24 13:20;rskraba;* 1.20 Default (Java 8) / Test (module: test) https://github.com/apache/flink/actions/runs/9304086727/job/25608401253#step:10:8187
* 1.20 Java 11 / Test (module: tests) https://github.com/apache/flink/actions/runs/9295906525/job/25583844657#step:10:7960;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase fails with TimeoutException: Pending slot request timed out in slot pool on AZP,FLINK-33185,13552917,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanyuxin,Sergey Nuyanzin,Sergey Nuyanzin,04/Oct/23 17:14,24/Nov/23 02:37,04/Jun/24 20:40,18/Oct/23 12:04,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Network,,,,,,0,pull-request-available,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53519&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8641
fails as 
{noformat}
Sep 29 05:13:54 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Pending slot request with SlotRequestId{b6e57c09274f4edc50697300bc8859a8} has been released.
Sep 29 05:13:54 	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:226)
Sep 29 05:13:54 	... 36 more
Sep 29 05:13:54 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Pending slot request with SlotRequestId{b6e57c09274f4edc50697300bc8859a8} has been released.
Sep 29 05:13:54 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
Sep 29 05:13:54 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
Sep 29 05:13:54 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
Sep 29 05:13:54 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
Sep 29 05:13:54 	... 34 more
Sep 29 05:13:54 Caused by: org.apache.flink.util.FlinkException: org.apache.flink.util.FlinkException: Pending slot request with SlotRequestId{b6e57c09274f4edc50697300bc8859a8} has been released.
Sep 29 05:13:54 	at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.releaseSlot(DeclarativeSlotPoolBridge.java:373)
Sep 29 05:13:54 	... 30 more
Sep 29 05:13:54 Caused by: java.util.concurrent.TimeoutException: java.util.concurrent.TimeoutException: Pending slot request timed out in slot pool.
Sep 29 05:13:54 	... 30 more

{noformat}",,,,,,,,,,,,,,,,,,,FLINK-33184,,,FLINK-33280,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 18 12:04:35 UTC 2023,,,,,,,,,,"0|z1kqq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 20:31;Sergey Nuyanzin;[~tanyuxin], [~Weijie Guo] could you please have a look
it might be related to FLINK-33103;;;","16/Oct/23 05:49;guoyangze;Another instance [~Weijie Guo] 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53749&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba;;;","18/Oct/23 03:57;tanyuxin;[~Sergey Nuyanzin] Sorry for the late reply. I will take a look.;;;","18/Oct/23 12:04;Weijie Guo;master via 166aab3ac4d83b0078496b41483d469158332b91.

Feel free to re-open this if it‘s also reproducible.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridShuffleITCase fails with exception in resource cleanup of task Map on AZP,FLINK-33184,13552913,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,04/Oct/23 16:53,25/Jan/24 03:56,04/Jun/24 20:40,24/Nov/23 02:38,1.19.0,,,,,,,,,,,,,,Runtime / Network,,,,,,0,test-stability,,,,,"This build fails https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53548&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8710
{noformat} 
Map (5/10)#0] ERROR org.apache.flink.runtime.taskmanager.Task                    [] - FATAL - exception in resource cleanup of task Map (5/10)#0 (159f887fbd200ea7cfa4aaeb1127c4ab_0a448493b4782967b150582570326227_4_0)
.
java.lang.IllegalStateException: Leaking buffers.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManagerImpl.release(TieredStorageMemoryManagerImpl.java:236) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_292]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageResourceRegistry.clearResourceFor(TieredStorageResourceRegistry.java:59) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartition.releaseInternal(TieredResultPartition.java:195) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.ResultPartition.release(ResultPartition.java:262) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.releasePartition(ResultPartitionManager.java:88) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:284) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:1004) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.releaseResources(Task.java:990) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:838) [flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
01:17:22,375 [flink-pekko.actor.default-dispatcher-5] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Task Sink: Unnamed (3/10)#0 is already in state CANCELING
01:17:22,375 [        Map (5/10)#0] ERROR org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - FATAL - exception in resource cleanup of task Map (5/10)#0 (159f887fbd200ea7cfa4aaeb1127c4ab_0a448493b4782967b150582570326227_4_0)
.
java.lang.IllegalStateException: Leaking buffers.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-core-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageMemoryManagerImpl.release(TieredStorageMemoryManagerImpl.java:236) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_292]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageResourceRegistry.clearResourceFor(TieredStorageResourceRegistry.java:59) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartition.releaseInternal(TieredResultPartition.java:195) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.ResultPartition.release(ResultPartition.java:262) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.releasePartition(ResultPartitionManager.java:88) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:284) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:1004) ~[flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.releaseResources(Task.java:990) [flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:838) [flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) [flink-runtime-1.19-SNAPSHOT.jar:1.19-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]

{noformat}",,,,,,,,,,,,,,,,,,FLINK-33185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 25 03:56:44 UTC 2024,,,,,,,,,,"0|z1kqpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 17:01;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53535&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=8823;;;","04/Oct/23 17:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53530&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=9039;;;","04/Oct/23 17:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53525&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8664;;;","04/Oct/23 17:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53519&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae;;;","04/Oct/23 17:17;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53509&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8993;;;","04/Oct/23 17:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53389&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9183;;;","04/Oct/23 20:31;Sergey Nuyanzin;[~tanyuxin], [~Weijie Guo] could you please have a look
it might be related to FLINK-33103;;;","05/Oct/23 10:11;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53560&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8993;;;","09/Oct/23 07:52;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53588&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=8993;;;","09/Oct/23 07:55;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53602&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8858;;;","11/Oct/23 07:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53624&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=9043;;;","16/Oct/23 07:19;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53744&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=b78d9d30-509a-5cea-1fef-db7abaa325ae&l=8809;;;","16/Oct/23 07:54;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53727&view=logs&j=baf26b34-3c6a-54e8-f93f-cf269b32f802&t=8c9d126d-57d2-5a9e-a8c8-ff53f7b35cd9&l=8989;;;","17/Oct/23 02:18;xuannan;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53752&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=9031;;;","24/Oct/23 07:10;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53899&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=11711;;;","24/Nov/23 02:36;Weijie Guo;It sees that this code path(i.e. IllegalStateException: Leaking buffers) has been fixed via FLINK-33185. Feel free to open a new ticket if something related to this test still exists.;;;","25/Jan/24 03:56;xccui;Just hit a similar issue in Flink 1.18.1. If [https://github.com/apache/flink/pull/23532] solved the issue, it's better to backport it.
{code:java}
ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Error in the task canceler for task KeyedProcess (112/128)#1.
java.lang.IllegalStateException: Leaking buffers.
    at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.SubpartitionDiskCacheManager.release(SubpartitionDiskCacheManager.java:113) ~[flink-dist-1.18.1.jar:1.18.1]
    at java.util.Spliterators$ArraySpliterator.forEachRemaining(Unknown Source) ~[?:?]
    at java.util.stream.ReferencePipeline$Head.forEach(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskCacheManager.release(DiskCacheManager.java:128) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.io.network.partition.hybrid.tiered.tier.disk.DiskTierProducerAgent.releaseResources(DiskTierProducerAgent.java:222) ~[flink-dist-1.18.1.jar:1.18.1]
    at java.util.ArrayList.forEach(Unknown Source) ~[?:?]
    at org.apache.flink.runtime.io.network.partition.hybrid.tiered.storage.TieredStorageResourceRegistry.clearResourceFor(TieredStorageResourceRegistry.java:59) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.io.network.partition.hybrid.tiered.shuffle.TieredResultPartition.releaseInternal(TieredResultPartition.java:195) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.io.network.partition.ResultPartition.release(ResultPartition.java:262) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.releasePartition(ResultPartitionManager.java:88) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.io.network.partition.ResultPartition.fail(ResultPartition.java:284) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.taskmanager.Task.failAllResultPartitions(Task.java:1004) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.taskmanager.Task.access$100(Task.java:139) ~[flink-dist-1.18.1.jar:1.18.1]
    at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1677) [flink-dist-1.18.1.jar:1.18.1]
    at java.lang.Thread.run(Unknown Source) [?:?]
2024-01-25 03:44:21 [KeyedProcess (112/128)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - KeyedProcess (112/128)#1 (7bb761e84f2d7957d3b927e49a6b28b3_e0d77c22cedd08ffffdc719831d914bf_111_1) switched from CANCELING to CANCELED. {code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable metadata columns in NduAnalyzer with retract if non-virtual,FLINK-33183,13552900,13447630,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,twalthr,twalthr,04/Oct/23 14:24,07/May/24 02:32,04/Jun/24 20:40,,,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"Currently, the NduAnalyzer is very strict about metadata columns in updating sources. Compared to append and upsert sources (see also FLINK-33182), retract sources are tricky. And the analyzer is actually correct.

However, for retract sources we should expose more functionality to the user and add a warning to the documentation that retract mode could potentially cause NDU problems if not enough attention is paid. We should only throw an error on virtual metadata columns. Persisted metadata columns can be considered “safe“. When a metadata column is persisted, we can assume that an upstream Flink job fills its content thus likely also fills its correct retraction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 07 02:32:20 UTC 2024,,,,,,,,,,"0|z1kqmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 14:26;twalthr;[~lincoln.86xy] what is you opinion here? This might be more controversial than FLINK-33182. But I think that the behavior actually depends on VIRTUAL vs non-VIRTUAL in order to cause issues.;;;","03/May/24 10:03;martijnvisser;[~lincoln.86xy] Friendly ping, what do you think of this ticket?;;;","07/May/24 02:32;lincoln.86xy;[~martijnvisser] Thanks for the reminder and sorry for my late response! I've thought about this issue many times, but I still think determine a column's determinism by whether a column is virtual or not is too subtle, also I don't want to introduce a new option(which maybe difficult for users to understand) to control the logic of determining here. Perhaps what we need more is a way to deal with the challenges posed by metadata columns, but I don't have a perfect solution yet.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow metadata columns in NduAnalyzer with ChangelogNormalize,FLINK-33182,13552896,13447630,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,twalthr,twalthr,04/Oct/23 14:10,20/Feb/24 08:18,04/Jun/24 20:40,20/Feb/24 08:18,,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Currently, the NduAnalyzer is very strict about metadata columns in updating sources. However, for upsert sources (like Kafka) that contain an incomplete changelog, the planner always adds a ChangelogNormalize node. ChangelogNormalize will make sure that metadata columns can be considered deterministic. So the NduAnalyzer should be satisfied in this case. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 20 08:18:18 UTC 2024,,,,,,,,,,"0|z1kqlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 14:10;twalthr;[~lincoln.86xy] do you agree with this assumption?;;;","06/Oct/23 14:33;lincoln.86xy;[~twalthr] I've thought about this carefully, and for upsert sources, while the non-determinism of the metadata column doesn't affect the +U message, if there is a -D message, there is no guarantee of correctness if we relax the strict restriction on the metadata (since the metadata column in the -D message is carried by the message itself, not as the changelognormalize node is handled). 
So, if we want to relax the restriction, we need to additionally make sure that the source does not contain -D message, WDYT?;;;","09/Oct/23 10:20;twalthr;[~lincoln.86xy] not sure if I understand your comment correctly. I understand that -D is dangerous and could cause non-determinism. However, ChangelogNormalize does not use the message value of -D. It reconstructs the -D from the previously stored row as can be seen in `DeduplicateFunctionHelper#processLastRowOnChangelog`. So metadata columns cannot affect determinism in this case.

Note that only the primary key is used for equality. Metadata columns are not supported as primary key columns.;;;","09/Oct/23 15:15;lincoln.86xy;[~twalthr] You're right, I checked the current `DeduplicateFunctionHelper#processLastRowOnChangelog` processing, it does avoid the non-determinism of the -D message, so I agree it's safe in this case with the guarantee that metadata columns are not allowed as primary key.
By the way, when you mention NduAnalyzer here, do you actually mean StreamNonDeterministicUpdatePlanVisitor (because NDUAnalyzer  relies on it, and we should actually modify the visitor)?;;;","09/Oct/23 15:20;twalthr;Great that we are on the same page. Yes I meant the `StreamNonDeterministicUpdatePlanVisitor`. Do you want to take this ticket? I guess you are the expert in this class.;;;","10/Oct/23 14:07;lincoln.86xy;[~twalthr] Of course, I'll put it on my worklist for the next release.;;;","11/Oct/23 09:26;twalthr;Thanks [~lincoln.86xy]. Let me know if I can help with a review.;;;","18/Jan/24 08:07;lincoln.86xy;[~twalthr] I've submitted a pr for this, if you could help with the review, that would be great.;;;","20/Feb/24 08:18;twalthr;Fixed in master: dc25bcabb5abd98780e0d5a086e417d5c0c90451;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table using `kinesis` connector can not be used for both read & write operations if it's defined with unsupported sink property,FLINK-33181,13552861,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,khanhvu,khanhvu,khanhvu,04/Oct/23 09:46,19/Oct/23 11:18,04/Jun/24 20:40,19/Oct/23 11:18,1.15.4,aws-connector-4.1.0,,,,,,,,,aws-connector-4.2.0,,,,Connectors / Kinesis,Table SQL / Runtime,,,,,0,pull-request-available,,,,,"First, I define a table which uses `kinesis` connector with an unsupported property for sink, e.g. `scan.stream.initpos`:
{code:sql}
%flink.ssql(type=update)
– Create input
DROP TABLE IF EXISTS `kds_input`;
CREATE TABLE `kds_input` (
`some_string` STRING,
`some_int` BIGINT,
`time` AS PROCTIME()
) WITH (
'connector' = 'kinesis',
'stream' = 'ExampleInputStream',
'aws.region' = 'us-east-1',
'scan.stream.initpos' = 'LATEST',
'format' = 'csv'
);
{code}

I can read from my table (kds_input) without any issue, but it will throw exception if I try to write to the table:

{code:sql}
%flink.ssql(type=update)
– Use to generate data in the input table

DROP TABLE IF EXISTS connector_cve_datagen;
CREATE TABLE connector_cve_datagen(
`some_string` STRING,
`some_int` BIGINT
) WITH (
'connector' = 'datagen',
'rows-per-second' = '1',
'fields.some_string.length' = '2');
INSERT INTO kds_input SELECT some_string, some_int from connector_cve_datagen
{code}

Exception observed:
{code:java}
Caused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'kinesis'.

Unsupported options:

scan.stream.initpos

Supported options:

aws.region
connector
csv.allow-comments
csv.array-element-delimiter
csv.disable-quote-character
csv.escape-character
csv.field-delimiter
csv.ignore-parse-errors
csv.null-literal
csv.quote-character
format
property-version
sink.batch.max-size
sink.fail-on-error
sink.flush-buffer.size
sink.flush-buffer.timeout
sink.partitioner
sink.partitioner-field-delimiter
sink.producer.collection-max-count (deprecated)
sink.producer.collection-max-size (deprecated)
sink.producer.fail-on-error (deprecated)
sink.producer.record-max-buffered-time (deprecated)
sink.requests.max-buffered
sink.requests.max-inflight
stream
at org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:624)
at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:914)
at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validate(FactoryUtil.java:978)
at org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validateExcept(FactoryUtil.java:938)
at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.validateExcept(FactoryUtil.java:978)
at org.apache.flink.connector.kinesis.table.KinesisDynamicTableSinkFactory.createDynamicTableSink(KinesisDynamicTableSinkFactory.java:65)
at org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:259)
... 36 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 19 11:18:12 UTC 2023,,,,,,,,,,"0|z1kqds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Oct/23 09:34;martijnvisser;[~danny.cranmer] [~Hong Teoh] Is this one on your radar?;;;","12/Oct/23 12:34;dannycranmer;Hey [~martijnvisser]. Yep, [~khanhvu]  is on our team.;;;","16/Oct/23 07:33;chalixar;Hi [~khanhvu], Could you please describe the use case more. It feels like an anti-pattern to use the same stream as source and sink. Kinesis Table API source and sink implementations were intentionally separated post 1.15.;;;","19/Oct/23 11:17;dannycranmer;Merged commit [{{76d791f}}|https://github.com/apache/flink-connector-aws/commit/76d791fe72c328fc80f3efe26f57a33e8b67dc23] into apache:main;;;","19/Oct/23 11:18;dannycranmer;[~chalixar] I have used it before for load testing, without requiring a separate data generator :D ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement support for processing subset of  shards/traffic,FLINK-33180,13552789,13404461,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,cprakash,cprakash,03/Oct/23 19:58,02/Feb/24 10:12,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,Support processing of subset of shards/traffic. This feature is require to implement canary / onebox.,,,,,,,,,,,,FLINK-34339,,,FLINK-31989,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-03 19:58:36.0,,,,,,,,,,"0|z1kpyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve reporting serialisation issues,FLINK-33179,13552740,13415855,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,03/Oct/23 12:41,10/Oct/23 06:45,04/Jun/24 20:40,10/Oct/23 06:45,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,FLINK-33158 shows that serialisation exceptions are not reported in a helpful manner. We should improve error reporting so that it gives more context what went wrong.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 06:45:48 UTC 2023,,,,,,,,,,"0|z1kpns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:45;dwysakowicz;Fixed in master: 6b52a4107db7521a25f4f308891095c5ba33cca0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Highly parallel apps suffer from bottleneck in NativePRNG ,FLINK-33178,13552724,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,iemre,iemre,03/Oct/23 10:42,16/Oct/23 08:41,04/Jun/24 20:40,16/Oct/23 08:41,1.13.2,1.17.1,,,,,,,,,,,,,Runtime / Network,,,,,,0,,,,,,"I observed the below thread dumps that highlighted a potential bottleneck in Flink/Netty/JDK. The application (Flink 1.13) from which I took the thread dumps had very high parallelism and was distributed on nodes with >150GB random access memory.

It appears that there is a call to ""Arrays.copyOfRange"" in a syncrhonized block in ""sun.security.provider.NativePRNG"", which blocks other threads waiting for the lock to the same synchronized block. This appears to be a problem only with highly parallel applications. I don't know exactly at what parallelism it starts becoming a problem, and how much of a bottleneck it actually is.

I was also slightly hesitant about creating a Flink ticket as the improvement could well be made in Netty or even JDK. But I believe we should have a record of the issue in Flink Jira.

Related: [https://bugs.openjdk.org/browse/JDK-8278371]

 

 
{code:java}
""Flink Netty Server (6121) Thread 43"" #930 daemon prio=5 os_prio=0 cpu=2298176.43ms elapsed=44352.31s allocated=155G defined_classes=0 tid=0x00007f0a3397f800 nid=0x519 waiting for monitor entry  [0x00007efc5d549000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at sun.security.provider.NativePRNG$RandomIO.implNextBytes(java.base@11.0.18/NativePRNG.java:544)
        - waiting to lock <0x00007f0b62c2eee8> (a java.lang.Object)
        at sun.security.provider.NativePRNG.engineNextBytes(java.base@11.0.18/NativePRNG.java:220)
        at java.security.SecureRandom.nextBytes(java.base@11.0.18/SecureRandom.java:751)
        at sun.security.ssl.SSLCipher$T11BlockWriteCipherGenerator$BlockWriteCipher.encrypt(java.base@11.0.18/SSLCipher.java:1498)
        at sun.security.ssl.OutputRecord.t10Encrypt(java.base@11.0.18/OutputRecord.java:441)
        at sun.security.ssl.OutputRecord.encrypt(java.base@11.0.18/OutputRecord.java:345)
        at sun.security.ssl.SSLEngineOutputRecord.encode(java.base@11.0.18/SSLEngineOutputRecord.java:287)
        at sun.security.ssl.SSLEngineOutputRecord.encode(java.base@11.0.18/SSLEngineOutputRecord.java:189)
        at sun.security.ssl.SSLEngineImpl.encode(java.base@11.0.18/SSLEngineImpl.java:285)
        at sun.security.ssl.SSLEngineImpl.writeRecord(java.base@11.0.18/SSLEngineImpl.java:231)
        at sun.security.ssl.SSLEngineImpl.wrap(java.base@11.0.18/SSLEngineImpl.java:136)
        - eliminated <0x00007f0b6aab70c8> (a sun.security.ssl.SSLEngineImpl)
        at sun.security.ssl.SSLEngineImpl.wrap(java.base@11.0.18/SSLEngineImpl.java:116)
        - locked <0x00007f0b6aab70c8> (a sun.security.ssl.SSLEngineImpl)
        at javax.net.ssl.SSLEngine.wrap(java.base@11.0.18/SSLEngine.java:522)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1071)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:843)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.wrapAndFlush(SslHandler.java:811)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.flush(SslHandler.java:792)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:742)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:728)
        at org.apache.flink.shaded.netty4.io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:125)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:765)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:808)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1025)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:294)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:246)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:110)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:173)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
        at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
        at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:89)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue$$Lambda$1875/0x00007efc8a13b4b0.run(Unknown Source)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(java.base@11.0.18/Thread.java:829)""Flink Netty Server (6121) Thread 42"" #929 daemon prio=5 os_prio=0 cpu=2371118.12ms elapsed=44352.38s allocated=162G defined_classes=0 tid=0x00007f0a3397e800 nid=0x518 runnable  [0x00007f0a1997c000]
   java.lang.Thread.State: RUNNABLE
        at java.util.Arrays.copyOfRange(java.base@11.0.18/Arrays.java:4030)
        at sun.security.provider.NativePRNG$RandomIO.implNextBytes(java.base@11.0.18/NativePRNG.java:554)
        - locked <0x00007f0b62c2eee8> (a java.lang.Object)
        at sun.security.provider.NativePRNG.engineNextBytes(java.base@11.0.18/NativePRNG.java:220)
        at java.security.SecureRandom.nextBytes(java.base@11.0.18/SecureRandom.java:751)
        at sun.security.ssl.SSLCipher$T11BlockWriteCipherGenerator$BlockWriteCipher.encrypt(java.base@11.0.18/SSLCipher.java:1498)
        at sun.security.ssl.OutputRecord.t10Encrypt(java.base@11.0.18/OutputRecord.java:441)
        at sun.security.ssl.OutputRecord.encrypt(java.base@11.0.18/OutputRecord.java:345)
        at sun.security.ssl.SSLEngineOutputRecord.encode(java.base@11.0.18/SSLEngineOutputRecord.java:287)
        at sun.security.ssl.SSLEngineOutputRecord.encode(java.base@11.0.18/SSLEngineOutputRecord.java:189)
        at sun.security.ssl.SSLEngineImpl.encode(java.base@11.0.18/SSLEngineImpl.java:285)
        at sun.security.ssl.SSLEngineImpl.writeRecord(java.base@11.0.18/SSLEngineImpl.java:231)
        at sun.security.ssl.SSLEngineImpl.wrap(java.base@11.0.18/SSLEngineImpl.java:136)
        - eliminated <0x00007f183702cb78> (a sun.security.ssl.SSLEngineImpl)
        at sun.security.ssl.SSLEngineImpl.wrap(java.base@11.0.18/SSLEngineImpl.java:116)
        - locked <0x00007f183702cb78> (a sun.security.ssl.SSLEngineImpl)
        at javax.net.ssl.SSLEngine.wrap(java.base@11.0.18/SSLEngine.java:522)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1071)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:843)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.wrapAndFlush(SslHandler.java:811)
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.flush(SslHandler.java:792)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:742)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:728)
        at org.apache.flink.shaded.netty4.io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:125)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:750)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:765)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:808)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1025)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:294)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.writeAndFlushNextMessageIfPossible(PartitionRequestQueue.java:246)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.enqueueAvailableReader(PartitionRequestQueue.java:110)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.userEventTriggered(PartitionRequestQueue.java:173)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
        at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
        at org.apache.flink.shaded.netty4.io.netty.channel.ChannelInboundHandlerAdapter.userEventTriggered(ChannelInboundHandlerAdapter.java:117)
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.userEventTriggered(ByteToMessageDecoder.java:365)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireUserEventTriggered(AbstractChannelHandlerContext.java:324)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.userEventTriggered(DefaultChannelPipeline.java:1428)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:346)
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeUserEventTriggered(AbstractChannelHandlerContext.java:332)
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireUserEventTriggered(DefaultChannelPipeline.java:913)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue.lambda$notifyReaderNonEmpty$0(PartitionRequestQueue.java:89)
        at org.apache.flink.runtime.io.network.netty.PartitionRequestQueue$$Lambda$1875/0x00007efc8a13b4b0.run(Unknown Source)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(java.base@11.0.18/Thread.java:829) {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,FLINK-11579,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 08:41:23 UTC 2023,,,,,,,,,,"0|z1kpk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 18:38;martijnvisser;[~iemre] Thanks for opening the ticket, but Flink 1.13 is more then 2 years old and not supported in the Flink community anymore. Given the number of changes that have gone into the runtime, I think we have to close this unless the issue can be reproduced in the latest version of Flink;;;","04/Oct/23 09:42;iemre;[~martijnvisser] Thanks for the comment. You're right to point that out.

Just double-checked this, I can see the same code path to the same Netty call in the master branch: [https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestQueue.java#L320C21-L320C28]

Unless Netty has done something to improve it, I believe the bottleneck might still be there. Looking at the code [https://github.com/netty/netty/blob/d773f37e3422b8bc38429bbde94583173c3b7e4a/handler/src/main/java/io/netty/handler/ssl/SslHandler.java#L809] I can see the same call to SSLEngine. So I believe the same bottleneck might be there in the current master head.

It also depends on the JDK being used, so users could work around the issue by using an upgraded/patched version. I've linked an OpenJDK ticket for the issue that's still open, I don't know if a patched version exists.

I am happy to report the issue. But please feel free to close the ticket if you think the issue is best addressed elsewhere, or if you think we should observe it in practice in newer Flink versions. Judging by the codepath, I believe we would see the issue in newer versions too.;;;","05/Oct/23 08:27;iemre;Adding 1.17.1 to affected versions based on my above ""static analysis"".

Please feel free to remove it and/or close the ticket if you have reservations.;;;","13/Oct/23 10:18;pnowojski;I have a feeling this ticket is a duplicate of FLINK-11579 and is a ""known non-FLink issue"".

[~iemre], can you try using OpenSSL instead of JDK's ?;;;","16/Oct/23 08:41;iemre;Hi [~pnowojski] thanks a lot for pointing that out. I was not able to find the issue when I did a little search.
I believe that would do the job. I am closing this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in MockStreamingRuntimeContext,FLINK-33177,13552653,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,rskraba,rskraba,02/Oct/23 15:06,02/Oct/23 15:06,04/Jun/24 20:40,,,,,,,,,,,,,,,,Tests,,,,,,0,,,,,,"(I noticed this when fixing FLINK-33018)

The three-argument constructor for [MockStreamingRuntimeContext|https://github.com/apache/flink/blob/ab26175a82a836da9edfaea6325038541e492a3e/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/MockStreamingRuntimeContext.java#L42] has a memory leak due to a MockEnvironment being created and never closed.

You can reproduce this by running any test that uses this constructor in IntelliJ with a mode set to ""Repeat until fail"". After about 16K runs:
{code:java}
#
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f5f814c1000, 16384, 0) failed; error='Not enough space' (errno=12)
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 16384 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /home/ryan.skraba/working/apache/flink-connector-gcp-pubsub/flink-connector-gcp-pubsub/hs_err_pid214687.log
[154.974s][warning][os,thread] Failed to start the native thread for java.lang.Thread ""IOManager reader thread #1""
Exception in thread ""Thread-48747"" java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached
    at java.base/java.lang.Thread.start0(Native Method)
    at java.base/java.lang.Thread.start(Thread.java:802)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync.<init>(IOManagerAsync.java:97)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync.<init>(IOManagerAsync.java:66)
    at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync.<init>(IOManagerAsync.java:57)
    at org.apache.flink.runtime.operators.testutils.MockEnvironmentBuilder.build(MockEnvironmentBuilder.java:173)
    at org.apache.flink.streaming.util.MockStreamingRuntimeContext.<init>(MockStreamingRuntimeContext.java:52)
    at org.apache.flink.streaming.connectors.gcp.pubsub.PubSubConsumingTest.lambda$createSourceThread$0(PubSubConsumingTest.java:186)
    at java.base/java.lang.Thread.run(Thread.java:833)
[154.977s][warning][os,thread] Attempt to deallocate stack guard pages failed (0x00007f5f816c1000-0x00007f5f816c5000).
[thread 214689 also had an error]
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f69994e0000, 65536, 0) failed; error='Not enough space' (errno=12)
Disconnected from the target VM, address: '127.0.0.1:40395', transport: 'socket' {code}
or
{code:java}
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f01232ab000, 16384, 0) failed; error='Not enough space' (errno=12)
[thread 330183 also had an error]
[21.295s][warning][os,thread] Failed to start thread ""Unknown thread"" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.
[21.295s][warning][os,thread] Failed to start the native thread for java.lang.Thread ""IOManager reader thread #1""
#
# If you would like to submit a bug report, please visit:
#   https://bugzilla.redhat.com/enter_bug.cgi?product=Fedora&component=java-17-openjdk-portable&version=37
# {code}
This obviously isn't a big deal, since the tests that use this mock are only intended to be run once.  These errors can be fixed by using the four argument version of the constructor and explicitly closing the MockEnvironment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-10-02 15:06:12.0,,,,,,,,,,"0|z1kp4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinesis source throws NullPointerException in Table API on ignored parsing errors,FLINK-33176,13552635,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,a.pilipenko,a.pilipenko,a.pilipenko,02/Oct/23 12:24,04/Oct/23 16:00,04/Jun/24 20:40,04/Oct/23 16:00,1.15.4,aws-connector-4.1.0,,,,,,,,,aws-connector-4.2.0,,,,Connectors / Kinesis,,,,,,0,pull-request-available,,,,,"Using following example table:

 
{code:java}
CREATE TABLE source (
  text STRING,
  `arrival_time` TIMESTAMP(3) METADATA FROM 'timestamp' VIRTUAL
) WITH (
  'connector' = 'kinesis',
  'stream' = 'test',
  'aws.region' = 'us-east-1',
  'json.ignore-parse-errors' = 'true',
  'format' = 'json'
) {code}
Connector throws NullPointerException when source consumes malformed json message:
{code:java}
java.lang.NullPointerException
    at org.apache.flink.streaming.connectors.kinesis.table.RowDataKinesisDeserializationSchema.deserialize(RowDataKinesisDeserializationSchema.java:137)
    at org.apache.flink.streaming.connectors.kinesis.table.RowDataKinesisDeserializationSchema.deserialize(RowDataKinesisDeserializationSchema.java:44)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.deserializeRecordForCollectionAndUpdateState(ShardConsumer.java:202)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.lambda$run$0(ShardConsumer.java:126)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:118)
    at org.apache.flink.streaming.connectors.kinesis.internals.publisher.polling.PollingRecordPublisher.run(PollingRecordPublisher.java:102)
    at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:264)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829) {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 04 16:00:29 UTC 2023,,,,,,,,,,"0|z1kp0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 16:00;dannycranmer;merged commit [{{5105d48}}|https://github.com/apache/flink-connector-aws/commit/5105d48bf7334503b314d188462c3542f438f1d6] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nightly builds from S3 are not available for download, breaking all connector tests",FLINK-33175,13552606,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,martijnvisser,martijnvisser,02/Oct/23 06:50,09/Oct/23 21:03,04/Jun/24 20:40,09/Oct/23 09:22,,,,,,,,,,,,,,,Connectors / Common,,,,,,0,,,,,,"All downloads of Flink binaries fail with:

{code:java}
Run wget -q -c https://s3.amazonaws.com/flink-nightly/flink-1.18-SNAPSHOT-bin-scala_2.12.tgz -O - | tar -xz

gzip: stdin: unexpected end of file
tar: Child returned status 1
tar: Error is not recoverable: exiting now
Error: Process completed with exit code 2.
{code}

This goes for 1.18, but also 1.17 and 1.16",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33215,,,,,,FLINK-33214,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 15:26:30 UTC 2023,,,,,,,,,,"0|z1kou8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 18:09;martijnvisser;It looks like uploads are still happening, like in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53520&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b&t=2f31de30-3dd6-5bd6-d86d-dcdc2d7cc3de&l=26 but that the files aren't accessible anymore;;;","05/Oct/23 09:42;mapohl;I looked into the issue with [~Sergey Nuyanzin]. The [aws connector's nightly builds|https://github.com/apache/flink-connector-aws/actions/workflows/nightly.yml] help to narrow down the time when the issue started to appear: There must have been a change on September 18, 2023. The last successful build happened in the morning of that day. 

Coincidentally, [1.18.0-rc0 was announced|https://lists.apache.org/thread/5x28rp3zct4p603hm4zdwx6kfr101w38] on that very same day (not sure whether that's related). [~jingge] do you remember what you did for the rc creation? ...just to rule out that it's related.;;;","06/Oct/23 20:18;jingge;[~mapohl] the file is ok and not public accessible: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53296&view=l[…]-51bc-8163-03e54ba9ce5b&t=2f31de30-3dd6-5bd6-d86d-dcdc2d7cc3de|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53296&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b&t=2f31de30-3dd6-5bd6-d86d-dcdc2d7cc3de]

I just followed the release guideline and didn't change anything else. Do you know why the file can be downloaded from S3 previously? How is the credential configured?;;;","09/Oct/23 02:15;renqs;I took a glance at the Github Action of flink-connector-kafka and I didn't find any S3 / AWS related credentials in configuration. It just uses a plain wget to download the Flink binary from S3, so I guess the assumption on connector side is that the binary should be public accessible (please correct me if I'm wrong [~martijnvisser]). [~jingge] is there any changes on the permission of the S3 bucket or object these days?;;;","09/Oct/23 06:50;martijnvisser;[~renqs] Indeed, the artifact was always publicly available. We can also change it from S3 to an ASF-owned artifact. ;;;","09/Oct/23 07:00;mapohl;{quote}
It just uses a plain wget to download the Flink binary from S3, so I guess the assumption on connector side is that the binary should be public accessible (please correct me if I'm wrong Martijn Visser).
{quote}

That was also my conclusion. Based on [~rmetzger]'s memory the corresponding S3 Bucket is a legacy resource that might be owned by Ververica. I would think that one should connect to the S3 bucket and check the permissions.

I talked to [~mdedetrich] (from Apache Pekko) about nightly builds: The conclusion was that we might want to use the nightly SVN repo (https://nightlies.apache.org/flink/) which is provided by Apache INFRA. Apparently, that's the intended way of deploying nightlies in the Apache world. We're using it already for the docs nightlies. In that case, infra would be hosting the storage. That might reduce the burden on Ververica's/Alibaba's side. But we might want to discuss it on the mailing list first and create a follow-up issue for that one.;;;","09/Oct/23 07:12;martijnvisser;I already checked with ASF Infra if they're OK with that on the ASF Slack channel, see https://the-asf.slack.com/archives/CBX4TSBQ8/p1696357867690409

I do want to get this situation unblocked rather quickly: we currently are on the verge of releasing 1.18 and there's no connector that can currently check if they're compatible with that. ;;;","09/Oct/23 07:42;jingge;[~mapohl] exactly, I just raised the same thought wrt [https://nightlies.apache.org/flink|https://nightlies.apache.org/flink/] on the ML at the same time.;;;","09/Oct/23 08:10;jingge;Ververica has changed the files to be publicly accessible. Could anyone of you trigger any builds to test? Thanks! [~mapohl] [~martijnvisser] [~Sergey Nuyanzin] [~hlteoh37] [~danny.cranmer] ;;;","09/Oct/23 08:13;renqs;[~jingge] Thanks for the effort. I manually triggered the weekly build in flink-connector-kafka repo. From the log it has passed the binary downloading step and started compiling already, so I think it should work now. 

[https://github.com/apache/flink-connector-kafka/actions/runs/6453974340/job/17518572643];;;","09/Oct/23 08:13;Sergey Nuyanzin;Thanks [~jingge]
I rerun it for hbase https://github.com/apache/flink-connector-hbase/actions/runs/6210320267
;;;","09/Oct/23 08:18;martijnvisser;[~jingge] Thanks, it indeed looks to be resolved :);;;","09/Oct/23 08:49;mapohl;Thanks everyone. I created a follow-up issue FLINK-33214 covering the nightly SVN repository efforts.;;;","09/Oct/23 08:58;jingge;Thank you all! Appreciate it!

[~mapohl] I just created another umbrella one  https://issues.apache.org/jira/browse/FLINK-33215 at the same time. -May I convert the ticket you created to be the subtask of mine?-  Never mind, I made my task be blocked by yours.;;;","09/Oct/23 13:07;hong;Verified the flink-connector-aws one succeeds as well. Thank you for looking into this, everyone!

 

https://github.com/apache/flink-connector-aws/actions/runs/6450772599/job/17520032785;;;","09/Oct/23 15:26;jingge;[~hong] Thanks for the effort and feedback!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
enabling tablesample bernoulli in flink,FLINK-33174,13552560,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,01/Oct/23 07:07,09/Oct/23 13:26,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"I'd like to introduce a table sample function to enable fast sampling to streamings. 

this is enlighted by https://issues.apache.org/jira/browse/CALCITE-5971",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 13:26:06 UTC 2023,,,,,,,,,,"0|z1kok0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/23 07:09;zhoujira86;[~lsy] [~libenchao] Hi Bros, would you please help review the suggestion?;;;","07/Oct/23 11:20;libenchao;It sounds good to me. Since Calcite has supported this syntax and has rules to convert it to {{Fitler}} on top of {{Scan}}, the work should be minimal to support this in Flink, but requires to upgrade to Calcite 1.36.0.

BTW, Flink is a unified engine of streaming and batch, we should do it for both of them.;;;","07/Oct/23 14:13;zhoujira86;[~libenchao] Hi Benchao, Would you please assign the issue to me? 

We can see whether to wait for the calcite bumping or find someway else;;;","08/Oct/23 02:20;libenchao;[~zhoujira86] Since this is a new feature, which can be seen as a new public API, according to our [bylaw|https://cwiki.apache.org/confluence/display/FLINK/Flink+Improvement+Proposals#FlinkImprovementProposals-Whatisconsidereda%22majorchange%22thatneedsaFLIP?], we may need a FLIP for new APIs.

[~martijnvisser] What's your opinion on this? (Sorry to ping you directly, but I see that you are watching this issue);;;","08/Oct/23 11:37;lsy;[~zhoujira86] Thanks for your proposal, is supporting this function currently a more urgent and important requirement? If not, I'd suggest waiting a little while for upgrade calcite.;;;","09/Oct/23 07:16;martijnvisser;[~libenchao] I think it needs a FLIP, especially for the syntax topic. I don't think we have an equivalent for this yet. And [~lsy] makes a good comment that this would also require upgrading from Calcite 1.32 to 1.36, so there's definitely some work that needs to be completed first. I do think that we should continue with the upgrades of Calcite.;;;","09/Oct/23 13:26;zhoujira86;[~libenchao] [~lsy] [~martijnvisser]  Thanks all for your comments, Let me prepare a FLIP first and wait for the calcite upgrading;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support currentFetchEventTimeLag metrics for KafkaSource,FLINK-33173,13552531,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tanjialiang,tanjialiang,30/Sep/23 12:54,20/Oct/23 02:23,04/Jun/24 20:40,,kafka-3.0.0,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"In the connector specifications of [FLIP-33|https://cwiki.apache.org/confluence/display/FLINK/FLIP-33], {{currentFetchEventTimeLag}} metrics is the time in milliseconds from the record event timestamp to the timestamp Flink fetched the record. It can reflect the consupition latency before deserialization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 02:23:44 UTC 2023,,,,,,,,,,"0|z1kodk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Oct/23 06:05;tanjialiang;We need to make SourceReaderBase provide an interface for implementations to deliver FetchTime all the way to the place extracting EventTime in SourceOutput. Then we can continue working on this.;;;","18/Oct/23 06:34;martijnvisser;[~tanjialiang] Do you want to prepare a FLIP and open a discussion on the Dev mailing list for that?;;;","20/Oct/23 02:23;tanjialiang;[~martijnvisser] It's my pleasure, i will try it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump numpy version,FLINK-33172,13552530,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,30/Sep/23 12:18,01/Oct/23 11:12,04/Jun/24 20:40,01/Oct/23 11:12,1.19.0,,,,,,,,,,1.19.0,,,,API / Python,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 01 11:12:26 UTC 2023,,,,,,,,,,"0|z1kodc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Oct/23 11:12;gaborgsomogyi;ab26175 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consistent implicit type coercion support for equal and non-equal comparisons for codegen,FLINK-33171,13552528,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fengjiajie,fengjiajie,fengjiajie,30/Sep/23 11:25,07/Nov/23 01:50,04/Jun/24 20:40,07/Nov/23 01:49,1.17.1,1.18.0,,,,,,,,,1.17.2,1.18.1,1.19.0,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"When executing the following SQL:
{code:sql}
SELECT
    time1,
    time1 = '2023-09-30 18:22:42.123' AS eq1,
    NOT (time1 = '2023-09-30 18:22:42.123') AS notEq1
FROM table1;
{code}
the result is as follows:
{code:java}
+----+-------------------------+--------+--------+
| op |                   time1 |    eq1 | notEq1 |
+----+-------------------------+--------+--------+
| +I | 2023-09-30 18:22:42.123 |   TRUE |   TRUE |
| +I | 2023-09-30 18:22:42.124 |  FALSE |   TRUE |
+----+-------------------------+--------+--------+
2 rows in set
{code}
The ""notEq1"" in the first row should be FALSE.

Here is the reproducing code:
{code:java}
import org.apache.flink.api.common.functions.RichMapFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.typeutils.RowTypeInfo;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.types.Row;

public class TimePointNotEqualTest {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env =
                StreamExecutionEnvironment.getExecutionEnvironment(new Configuration());
        env.setParallelism(1);

        DataStreamSource<Long> longDataStreamSource = env.fromSequence(0, 1);
        RowTypeInfo rowTypeInfo =
                new RowTypeInfo(new TypeInformation[] {Types.LONG}, new String[] {""time1""});
        SingleOutputStreamOperator<Row> map =
                longDataStreamSource.map(new RichMapFunction<Long, Row>() {
                    @Override
                    public Row map(Long value) {
                        Row row = new Row(1);
                        row.setField(0, 1696069362123L + value);
                        return row;
                    }
                }, rowTypeInfo);

        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
        Schema schema = Schema.newBuilder()
                .column(""time1"", DataTypes.TIMESTAMP_LTZ(3).bridgedTo(Long.class))
                .build();
        tableEnv.createTemporaryView(""table1"", map, schema);

        tableEnv.sqlQuery(""SELECT ""
                + ""time1,"" // 2023-09-30 18:22:42.123
                + ""time1 = '2023-09-30 18:22:42.123' AS eq1,"" // expect TRUE
                + ""NOT (time1 = '2023-09-30 18:22:42.123') AS notEq1 "" // expect FALSE but TRUE
                + ""FROM table1"").execute().print();
    }
}
{code}
I would like to attempt to fix this issue. If possible, please assign the issue to me. Thank you.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 06 06:24:33 UTC 2023,,,,,,,,,,"0|z1kocw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/23 12:06;fengjiajie;I have attempted to submit a pull request: [https://github.com/apache/flink/pull/23478];;;","07/Oct/23 08:38;qingyue;Thanks for reporting this issue. I can help to review.;;;","07/Oct/23 11:41;libenchao;Since Flink does not support implicit casting yet, I would prefer to take this query as invalid (comparing between timestamp and varchar). There is a related issue CALCITE-5907, which I think it's a bug in Calcite.;;;","08/Oct/23 01:56;qingyue;[~libenchao] Correct me if I'm wrong, but I think this issue cannot be fixed even if CALCITE-5907 is fixed. ScalarOperatorGens already supports equal comparison between timestamp/date/time and character, so it'd better to align the behavior for non-equi comparison.;;;","08/Oct/23 02:12;libenchao;[~qingyue] Yes, CALCITE-5907 is not going to fix this, but will forbid this usage, that says, the validator should not allow implicit type coercion unless we enable it explicitly after FLIP-154. 

I'm not strongly against fixing it temporarily in Flink, but we'd better not guarantee the implicit type coercion for comparisons for users for now. Since there will be more corner cases, such as the one mentioned in FLINK-27402.;;;","08/Oct/23 08:41;qingyue;[~libenchao], Thanks for the detailed explanation. 

If I understand correctly, the current situation is that although FlinkPlanner turns off implicit type coercion by default, there are still issues related to it in practice. Some queries may fail to run, some may run but produce incorrect results, and some may run correctly (due to code generation support). In reality, there is no consistent behavior regarding implicit type coercion.

In the ideal state, after FLIP-154 is completed, if implicit type coercion is disabled, the validator should throw an exception directly. If type coercion is enabled, the type passed into code generation should already be the type after the implicit cast. In either case, ScalarOperatorGens may need to be refactored. Considering this situation, I actually think that including the discovered corner cases in the test suite would be helpful for future refactoring. What do you think? Also, I would like to ping [~snuyanzin] and ask about the progress of the [Revival of FLIP-154|https://www.mail-archive.com/dev@flink.apache.org/msg68233.html].;;;","08/Oct/23 11:49;libenchao;[~qingyue]  I agree with you.;;;","09/Oct/23 10:32;Sergey Nuyanzin;[~qingyue] thanks for ping
Currently I postponed a bit activity about FLIP-154 since I'm participating in 1.18.0 release activities. 
I'm going to continue work on it once 1.18.0 is released;;;","10/Oct/23 07:08;qingyue;Thanks, [~libenchao] and [~Sergey Nuyanzin]. If there are no other objections, I suggest assigning the ticket to [~fengjiajie].;;;","10/Oct/23 08:39;libenchao;[~qingyue] assigned to [~fengjiajie], thanks for driving this!;;;","13/Oct/23 08:53;lincoln.86xy;[~fengjiajie] [~libenchao] [~qingyue] Similar type handling differences were found in multiset and generic types in the pr review, so I think it's reasonable to change this jira's scope to fix all these difference between eq and non-eq comparison, for implementation wise we need to ensure that we don't modify the current three-value logic, e.g.,  'null = null' return false and 'null <> null' also return false. WDYT?;;;","13/Oct/23 09:10;libenchao;Regarding three-value logic, generally, it should return 'null' for 'null = null' and 'null <> null'. Whether we'll take 'null' as 'false' depends on what the sql block we are in, e.g., in 'where' and 'join on', we usually take 'unknow as false' strategy; but in 'select', we should keep 'null'.;;;","13/Oct/23 09:25;lincoln.86xy;[~libenchao] Yes, you're right. The description above may not make it clear, at the expression level we just need to make sure that unknown[1] is returned correctly, and the final result for the user is false because the we takes unknown as false strategy. Glad we're on the same page.

What are your thoughts on fixing the similar processing difference on multiset and generic types?

1. https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/#comparison-functions;;;","13/Oct/23 10:38;libenchao;bq. What are your thoughts on fixing the similar processing difference on multiset and generic types?

[~lincoln.86xy] Generally I'm not against fixing any corner cases encountered so far.;;;","16/Oct/23 10:06;qingyue;Hi, [~lincoln.86xy] and [~libenchao], sorry for the late reply. +1 to align the behavior of ScalarOperatorCodegens for both equi- and non-equi comparisons.

Hi [~fengjiajie], since the scope is enlarged, would you mind changing the Jira title to describe the issue better?

E.g., Consistent implicit type coercion support for equal and non-equi comparison for codegen?;;;","06/Nov/23 06:24;qingyue;master: 04b98e6e80ca3a68324775166dedb7d58ae4d60b

release-1.18: 68b49a3cbcd95c04bb237f3106121961bd80e61d

release-1.17: e18672adff49d5c16af23cee25fc9e29cb4db1e4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HybridSourceSplitEnumerator causes dropped records in Hybrid Sources with 3+ sources,FLINK-33170,13552501,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,robertjira,robertjira,30/Sep/23 02:07,03/Oct/23 18:55,04/Jun/24 20:40,,1.15.4,1.16.2,1.17.1,1.18.1,1.19.0,,,,,,,,,,Connectors / HybridSource,,,,,,0,,,,,,"Possibly related to FLINK-27916.

Priority labeled critical because this issue can cause major data loss, in our experience order GBs to TBs.

 

In all versions since 1.15.x there's a subtle bug in {{HybridSourceSplitEnumerator}} when determining if it's time to move on to the next source:
{code:java}
finishedReaders.add(subtaskId);
if (finishedReaders.size() == context.currentParallelism()) {
  // move on to the next source if it exists
{code}
This snippet is correct, but when changing to the next source, {{finishedReaders}} is never cleared. So when processing the second source, the {{finishedReaders.size()}} check will return true when the _first_ subtask finishes.** The hybrid source moves on to the next source if one exists, so any records remaining to be read and sent in the other {{numSubtasks - 1}} subtasks will get dropped.

 

{{**}} if each of the sources in the hybrid source has the same parallelism. If any source except the last has lower parallelism then I suspect that the source will never move on: it's impossible for {{finishedReaders.size()}} to shrink.

 

Concrete example with three sources, two subtasks each:
 # subtask 0 finishes with the first source. {{finishedReaders}} has size 1
 # subtask 1 finishes with the first source. {{finishedReaders}} has size 2 now, and moves on to the second source
 # subtask 1 finishes with the first source. {{finishedReaders.add(1)}} doesn't change the set; {{finishedReaders}} still has size 2. So the hybrid source moves on to the third source.
 # subtask 0 wasn't finished with the second source, but receives the notification to move on. Any unsent records are lost. *Data loss!*
 # this continues to the last source. The source doesn't change over if at the last source so the race condition in step 3 never happens

 

So step 3 results in the race condition that will drop records indeterminately for all but the first source and last source.

In production this issue caused the loss of GBs to TBs of data when a hybrid source had the following:
 * 3-5 underlying sources, each of which should emit 100 GB to 10 TB worth of records
 * all sources had the same number of splits, around 64-256

We fixed it in a private fork by clearing the {{finishedReaders}} set when changing to the next source.

Existing tests don't catch this data race because, as far as I understand them:
 * use two mock sources, whereas this bug manifests for 3+ sources
 * have sources with parallelism 1, while this bug manifests when the sources have parallelism > 1",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 03 18:54:55 UTC 2023,,,,,,,,,,"0|z1ko6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/23 04:31;robertjira;If someone can point me to how to Flink's policies on submitting a PR I can take a shot at fixing this. Otherwise if someone else wants to tackle it, iirc the fix was to clear the set [near the end|https://github.com/apache/flink/blob/b25b57c55d903e4fdd2b666de49c90bfbad8fa99/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/hybrid/HybridSourceSplitEnumerator.java#L305] of the {{switchEnumerator}} method.;;;","03/Oct/23 18:54;martijnvisser;[~robertjira] Have a look at https://flink.apache.org/how-to-contribute/overview/ - If you run into issues, let me know;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Window TVFs don't consider column expansion,FLINK-33169,13552463,13549560,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,29/Sep/23 16:02,30/Sep/23 01:29,04/Jun/24 20:40,30/Sep/23 01:29,,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,Window TVFs don't consider the column expansion. The reason for this is that `TABLE t` is expanded by a custom logic in the parser. The expansion logic should consider the descriptor in this case.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Sep 30 01:29:09 UTC 2023,,,,,,,,,,"0|z1knyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Sep/23 01:29;twalthr;Fixed in master: b25b57c55d903e4fdd2b666de49c90bfbad8fa99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"An error occurred when executing sql, java.lang.NoSuchFieldError: operands",FLINK-33168,13552382,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Invalid,zhengyunhong97,macdoor615,macdoor615,29/Sep/23 06:34,11/Oct/23 05:12,04/Jun/24 20:40,11/Oct/23 04:14,1.18.0,,,,,,,,,,,,,,Table SQL / Client,,,,,,0,,,,,,"Environment:

 
{code:java}
Linux hb3-prod-hadoop-006 4.18.0-477.27.1.el8_8.x86_64 #1 SMP Thu Sep 21 06:49:25 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux
openjdk version ""1.8.0_382""
OpenJDK Runtime Environment (build 1.8.0_382-b05)
OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)
flink-1.18.0-RC1 , https://github.com/apache/flink/releases/tag/release-1.18.0-rc1
{code}
 

I execute the following sql in sql-client.sh.

 
{code:java}
insert into svc1_paimon_prod.cq.b_customer_ecus
select
  rcus.id id,
  if(cus.id is not null, cus.id, try_cast(NULL as string)) cus_id,    
  if(cus.id is null and cus_rownum = 1, rcus.id, try_cast(NULL as string)) newCus_id,
  companyID,
  customerProvinceNumber,
  mobilePhone,
  oprCode,
  customerNum,
  staffName,
  location,
  staffNumber,
  extendInfo,
  customerName,
  case when companyID='000' then '名称1'
       when companyID='002' then '名称2'
       else '新名称'
       end prov,
  row (
    accessToken,
    busType,
    cutOffDay,
    domain,
    envFlag,
    routeType,
    routeValue,
    sessionID,
    sign,
    signMethod,
    org_timeStamp,
    transIDO,
    userPartyID,
    version
  ) raw_message,
  named_struct(
    'id', cus.id,
    'name', cus.name,
    'code', cus.code,
    'customerlevel', cus.customerlevel,
    'prov', cus.prov,
    'container', cus.container,
    'crtime', cus.crtime,
    'updtime', cus.updtime
  ) existing_cus,
  cus_rownum,
  to_timestamp(org_timeStamp, 'yyyyMMddHHmmss') as org_timeStamp,
  raw_rowtime,
  localtimestamp as raw_rowtime1,
  dt
from svc1_paimon_prod.raw_data.abscustinfoserv_content_append_cq
  /*+ OPTIONS('consumer-id' = '创建新客户id') */
  rcus
left join svc1_mysql_test.gem_svc1_vpn.bv_customer
FOR SYSTEM_TIME AS OF rcus.proctime AS cus on rcus.customerNum=cus.code
{code}
There are the following jar files in the flink/lib directory.
{code:java}
commons-cli-1.5.0.jar
flink-cep-1.18.0.jar
flink-connector-files-1.18.0.jar
flink-connector-jdbc-3.1.1-1.17.jar
flink-csv-1.18.0.jar
flink-dist-1.18.0.jar
flink-json-1.18.0.jar
flink-orc-1.18.0.jar
flink-parquet-1.18.0.jar
flink-scala_2.12-1.18.0.jar
flink-sql-avro-1.18.0.jar
flink-sql-avro-confluent-registry-1.18.0.jar
flink-sql-connector-elasticsearch7-3.0.0-1.16.jar
flink-sql-connector-hive-3.1.3_2.12-1.18.0.jar
flink-sql-connector-kafka-3.0.0-1.17.jar
flink-sql-orc-1.18.0.jar
flink-sql-parquet-1.18.0.jar
flink-table-api-java-uber-1.18.0.jar
flink-table-api-scala_2.12-1.18.0.jar
flink-table-api-scala-bridge_2.12-1.18.0.jar
flink-table-planner_2.12-1.18.0.jar
flink-table-runtime-1.18.0.jar
jline-reader-3.23.0.jar
jline-terminal-3.23.0.jar
kafka-clients-3.5.1.jar
log4j-1.2-api-2.17.1.jar
log4j-api-2.17.1.jar
log4j-core-2.17.1.jar
log4j-slf4j-impl-2.17.1.jar
mysql-connector-j-8.1.0.jar
paimon-flink-1.18-0.6-20230929.002044-11.jar{code}
Works correctly in version 1.17.1, but produces the following error in 1.18.0-RC1

 
{code:java}
2023-09-29 14:04:11,438 ERROR org.apache.flink.table.gateway.service.operation.OperationManager [] - Failed to execute the operation fe1b0a58-b822-49c0-b1ae-ce73d16f92da.
java.lang.NoSuchFieldError: operands
at org.apache.calcite.plan.RelOptRule.operand(RelOptRule.java:129) ~[flink-sql-connector-hive-3.1.3_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.<init>(SimplifyFilterConditionRule.scala:36) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<init>(SimplifyFilterConditionRule.scala:94) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<clinit>(SimplifyFilterConditionRule.scala) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<init>(FlinkStreamRuleSets.scala:35) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<clinit>(FlinkStreamRuleSets.scala) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram$.buildProgram(FlinkStreamProgram.scala:57) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.$anonfun$optimizeTree$1(StreamCommonSubGraphBasedOptimizer.scala:169) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at scala.Option.getOrElse(Option.scala:189) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:169) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:324) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_382]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_382]
at java.lang.Thread.run(Thread.java:750) [?:1.8.0_382]
2023-09-29 14:04:11,505 ERROR org.apache.flink.table.gateway.service.SqlGatewayServiceImpl [] - Failed to fetchResults.
org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation fe1b0a58-b822-49c0-b1ae-ce73d16f92da.
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]
at java.lang.Thread.run(Thread.java:750) [?:1.8.0_382]
Caused by: java.lang.NoSuchFieldError: operands
at org.apache.calcite.plan.RelOptRule.operand(RelOptRule.java:129) ~[flink-sql-connector-hive-3.1.3_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.<init>(SimplifyFilterConditionRule.scala:36) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<init>(SimplifyFilterConditionRule.scala:94) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<clinit>(SimplifyFilterConditionRule.scala) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<init>(FlinkStreamRuleSets.scala:35) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<clinit>(FlinkStreamRuleSets.scala) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram$.buildProgram(FlinkStreamProgram.scala:57) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.$anonfun$optimizeTree$1(StreamCommonSubGraphBasedOptimizer.scala:169) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at scala.Option.getOrElse(Option.scala:189) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:169) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:324) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
... 7 more
2023-09-29 14:04:11,508 ERROR org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler [] - Unhandled exception.
org.apache.flink.table.gateway.api.utils.SqlGatewayException: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:85) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:84) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196) ~[flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist-1.18.0.jar:1.18.0]
at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_382]
at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.18.0.jar:1.18.0]
at java.lang.Thread.run(Thread.java:750) [?:1.8.0_382]
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation fe1b0a58-b822-49c0-b1ae-ce73d16f92da.
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]
... 1 more
Caused by: java.lang.NoSuchFieldError: operands
at org.apache.calcite.plan.RelOptRule.operand(RelOptRule.java:129) ~[flink-sql-connector-hive-3.1.3_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.<init>(SimplifyFilterConditionRule.scala:36) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<init>(SimplifyFilterConditionRule.scala:94) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<clinit>(SimplifyFilterConditionRule.scala) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<init>(FlinkStreamRuleSets.scala:35) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<clinit>(FlinkStreamRuleSets.scala) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram$.buildProgram(FlinkStreamProgram.scala:57) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.$anonfun$optimizeTree$1(StreamCommonSubGraphBasedOptimizer.scala:169) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at scala.Option.getOrElse(Option.scala:189) ~[flink-scala_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:169) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:324) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182) ~[flink-table-planner_2.12-1.18.0.jar:1.18.0]
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862) ~[flink-table-api-java-uber-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258) ~[flink-sql-gateway-1.18.0.jar:1.18.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]
... 1 more
2023-09-29 14:04:11,517 WARN  org.apache.flink.table.client.cli.CliClient                  [] - Could not execute SQL statement.
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to get response for the operation fe1b0a58-b822-49c0-b1ae-ce73d16f92da.
at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:488) ~[flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:448) ~[flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:309) ~[flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.cli.parser.SqlMultiLineParser.parse(SqlMultiLineParser.java:113) ~[flink-sql-client-1.18.0.jar:1.18.0]
at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2994) ~[jline-reader-3.23.0.jar:?]
at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3812) ~[jline-reader-3.23.0.jar:?]
at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:689) ~[jline-reader-3.23.0.jar:?]
at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:194) [flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.cli.CliClient.executeFile(CliClient.java:243) [flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.cli.CliClient.executeInNonInteractiveMode(CliClient.java:131) [flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:171) [flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.SqlClient.start(SqlClient.java:118) [flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228) [flink-sql-client-1.18.0.jar:1.18.0]
at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179) [flink-sql-client-1.18.0.jar:1.18.0]
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., <Exception on server side:
org.apache.flink.table.gateway.api.utils.SqlGatewayException: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:85)
at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:84)
at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52)
at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196)
at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83)
at java.util.Optional.ifPresent(Optional.java:159)
at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45)
at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80)
at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49)
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115)
at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94)
at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55)
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208)
at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69)
at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.flink.table.gateway.api.utils.SqlGatewayException: Failed to fetchResults.
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.fetchResults(SqlGatewayServiceImpl.java:229)
at org.apache.flink.table.gateway.rest.handler.statement.FetchResultsHandler.handleRequest(FetchResultsHandler.java:83)
... 48 more
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Failed to execute the operation fe1b0a58-b822-49c0-b1ae-ce73d16f92da.
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.processThrowable(OperationManager.java:414)
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:267)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
... 1 more
Caused by: java.lang.NoSuchFieldError: operands
at org.apache.calcite.plan.RelOptRule.operand(RelOptRule.java:129)
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule.<init>(SimplifyFilterConditionRule.scala:36)
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<init>(SimplifyFilterConditionRule.scala:94)
at org.apache.flink.table.planner.plan.rules.logical.SimplifyFilterConditionRule$.<clinit>(SimplifyFilterConditionRule.scala)
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<init>(FlinkStreamRuleSets.scala:35)
at org.apache.flink.table.planner.plan.rules.FlinkStreamRuleSets$.<clinit>(FlinkStreamRuleSets.scala)
at org.apache.flink.table.planner.plan.optimize.program.FlinkStreamProgram$.buildProgram(FlinkStreamProgram.scala:57)
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.$anonfun$optimizeTree$1(StreamCommonSubGraphBasedOptimizer.scala:169)
at scala.Option.getOrElse(Option.scala:189)
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:169)
at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:324)
at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:182)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1277)
at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:862)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:513)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:426)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:207)
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
... 7 more
 
End of exception on server side>]
at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:646) ~[flink-dist-1.18.0.jar:1.18.0]
at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$6(RestClient.java:626) ~[flink-dist-1.18.0.jar:1.18.0]
at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966) ~[?:1.8.0_382]
at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940) ~[?:1.8.0_382]
at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]
at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog mysql_bnpmp closing
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog mysql_service1 closing
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog svc1_mysql_test closing
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog bnpmp_mysql_prod closing
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog mysql_test closing
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog bnpmp_mysql_test closing
2023-09-29 14:04:11,528 INFO  org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog  [] - Catalog svc1_mysql_prod closing
2023-09-29 14:04:11,536 INFO  org.apache.flink.table.catalog.hive.HiveCatalog              [] - Close connection to Hive metastore
2023-09-29 14:04:11,546 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Shutting down rest endpoint.
2023-09-29 14:04:11,555 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Shut down complete.
2023-09-29 14:04:12,547 INFO  org.apache.flink.table.gateway.rest.SqlGatewayRestEndpoint   [] - Shutting down rest endpoint.
{code}
 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 05:12:55 UTC 2023,,,,,,,,,,"0|z1kngg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Oct/23 01:43;luoyuxia;[~macdoor615] Hi, could you please try using flink-table-planer-loader.jar replace flink-table-planer.jar?;;;","11/Oct/23 04:14;luoyuxia;I tried with puting table-planner-loader.jar in /lib and it works. Since it's expected in Flink 1.18 as you can see in FLINK-31575, we recommend to not to swap table-planner-loader and table-planner, I would like to close it. Feel free to open it again when you still meet problems. 

The reason may be a a little of complex, we include {{org/apache/calcite/plan/RelOptRule.class}} in flink-sql-connector-hive 3.1.3, the reason can be seen in [here|https://github.com/apache/flink/blob/5269631af525a01d944cfa9994a116fb27b80b1b/flink-connectors/flink-sql-connector-hive-3.1.3/pom.xml#L198]. 
Then, the planner will load the class {{RelOptRule}} in flink-sql-connector-hive 3.1.3, but the class RelOptRule in complied in flink-sql-connector-hive which shade {{com.google}} to {{{}org.apache.flink.hive.shaded.com.google{}}}.
But {{RelOptRule}} will refer to {{{}com.google.common.collect.ImmutableList{}}}, afte complie, it will then become 
to refer to Field {{{}org/apache/calcite/plan/RelOptRuleOperandChildren.operands:Lorg/apache/flink/hive/shaded/com/google/common/collect/ImmutableList{}}}.

But RelOptRuleOperandChildren is compiled in flink-table-planner which shade {{com.google}} to {{{}org.apache.flink.calcite.shaded.com.google{}}}, so it will only contain a field {{{}org/apache/flink/calcite/shaded/com/google/common/collect/ImmutableList{}}}.
Then it'll casue {{{}java.lang.NoSuchFieldError: operands{}}}.

It's simliar to the issue FLINK-32286.;;;","11/Oct/23 04:19;macdoor615;[~luoyuxia]  table-planner-loader.jar works. Docs should be updated.;;;","11/Oct/23 05:12;luoyuxia;I believe the doc has been updated in FLINK-32982. And the release note will also include it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run IT tests against Kinesalite if AWS credentials are not present,FLINK-33167,13552293,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hong,hong,28/Sep/23 10:40,28/Sep/23 10:40,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / AWS,Connectors / Kinesis,,,,,0,,,,,,"*What*

We want to run Kinesis IT tests against Kinesalite if there are no AWS credentials present. 

 

*Why*

We want maximum test coverage (e.g. on PR build, we don't have AWS creds, so we run against Kinesalite to eagerly find mistakes in PRs)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-28 10:40:44.0,,,,,,,,,,"0|z1kmwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support setting root logger level by config,FLINK-33166,13552262,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,28/Sep/23 05:59,04/Jan/24 10:54,04/Jun/24 20:40,11/Oct/23 09:28,,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"Users currently cannot change logging level by config and have to modify the cumbersome logger configuration file manually. We'd better provide a shortcut and support setting root logger level by config. There're a number configs already to set logger configurations, like {{env.log.dir}} for logging dir, {{env.log.max}} for max number of old logging file to save. We can name the new config {{{}env.log.level{}}}.

Multiple loggers are configured in the configuration files, some with different logging level from the root logger to reduce irrelevant logs. In most cases, only the root logger is relevant. We can make {{env.log.level}} applies to the root logger only for simplicity.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33988,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 11 09:28:22 UTC 2023,,,,,,,,,,"0|z1kmps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/23 06:03;Zhanghao Chen;[~fangyong] Could you assign it to me if you think it reasonable?;;;","11/Oct/23 09:28;zjureel;Fixed by b14411e7062c235cb28582a37e37decc7c425476;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink UI stack trace popup continually displayed when a job is deleted,FLINK-33165,13552197,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,davidradl,davidradl,27/Sep/23 13:43,28/Sep/23 11:07,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Runtime / Web Frontend,,,,,,0,,,,,,"If you run a job and view that running job in the Flink dashboard, then delete the job, the UI displays a pop-up with an information icon and a message and stack trace . This is repeatedly displayed every second or so. If we must display a popup is should be once without the stack (which implies there is an error - the stack trace is not useful to the user). We should look into whether we need a popup in this case. It would be better to not display a popup, as this should be just a state change.    

An example popup content  is:

*Server Response Message:*

org.apache.flink.runtime.rest.NotFoundException: Job 4e332318d164d86e76a239eabd49bf93 not found

at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99)

at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(Unknown Source)

at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109)

at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(Unknown Source)

at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:260)

at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(Unknown Source)

at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1298)

at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$1(ClassLoadingUtils.java:93)

at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)

at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)

at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)

at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(Unknown Source)

at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:45)

at akka.dispatch.OnComplete.internal(Future.scala:299)

at akka.dispatch.OnComplete.internal(Future.scala:297)

at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)

at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)

at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)

at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)

at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)

at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)

at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)

at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)

at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:622)

at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:25)

at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)

at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)

at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)

at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)

at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)

at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)

at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)

at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)

at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)

at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)

at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)

at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)

at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)

at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)

at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)

at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)

at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)

Caused by: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (4e332318d164d86e76a239eabd49bf93)

at org.apache.flink.runtime.dispatcher.Dispatcher.requestExecutionGraphInfo(Dispatcher.java:905)

at jdk.internal.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)

at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)

at java.base/java.lang.reflect.Method.invoke(Unknown Source)

at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309)

at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)

at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307)

at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222)

at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84)

at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168)

at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)

at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)

at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)

at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)

at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)

at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)

at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)

at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)

at akka.actor.Actor.aroundReceive(Actor.scala:537)

at akka.actor.Actor.aroundReceive$(Actor.scala:535)

at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579)

at akka.actor.ActorCell.invoke(ActorCell.scala:547)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)

at akka.dispatch.Mailbox.run(Mailbox.scala:231)

at akka.dispatch.Mailbox.exec(Mailbox.scala:243)

... 5 more

 ",MacOS M1 . ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 28 11:06:05 UTC 2023,,,,,,,,,,"0|z1kmbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/23 13:55;Sergey Nuyanzin;[~davidradl] thanks for the raising the issue
could you please clarify whether it is correct that you want to fix it in 1.7.3 which is not supported for long time or is it just a mistake?

Also it's better to specify affected version rather than fixed version (fixed version should be specified when fix has been merged or you are committing that you will provide fix before this version is released);;;","28/Sep/23 11:06;davidradl;[~Sergey Nuyanzin] thanks - good spot. It was a mistake I have removed that version. I have added the latest version as the affect version. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase connector support ignore null value for partial update,FLINK-33164,13552126,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanjialiang,tanjialiang,tanjialiang,27/Sep/23 07:11,10/Nov/23 08:24,04/Jun/24 20:40,10/Nov/23 08:24,hbase-3.0.0,,,,,,,,,,hbase-3.0.1,,,,Connectors / HBase,,,,,,0,pull-request-available,,,,,"Sometimes, user want to write data and ignore null value to achieve partial update. So i suggest adding an options: sink.ignore-null-value.

 
{code:java}
CREATE TABLE hTable (
 rowkey STRING,
 cf1 ROW<q1 STRING, q2 STRING>,
 PRIMARY KEY (rowkey) NOT ENFORCED
) WITH (
 'connector' = 'hbase-2.2',
 'table-name' = 'default:test',
 'zookeeper.quorum' = 'localhost:2181',
 'sink.ignore-null-value' = 'true' -- default is false, true is enabled
);

INSERT INTO hTable VALUES('1', ROW('10', 'hello, world'));
INSERT INTO hTable VALUES('1', ROW('30', CAST(NULL AS STRING))); -- null value to cf1.q2

-- when sink.ignore-null-value is false
// after first insert
{rowkey: ""1"", ""cf1"": {q1: ""10"", q2: ""hello, world""}} 
// after second insert, cf1.q2 update to null
{rowkey: ""1"", ""cf1"": {q1: ""30"", q2: ""null""}} 


-- when sink.ignore-null-value is true
// after first insert 
{rowkey: ""1"", ""cf1"": {q1: ""10"", q2: ""hello, world""}}
// after second insert, cf1.q2 is still the old value 
{rowkey: ""1"", ""cf1"": {q1: ""30"", q2: ""hello, world""}} {code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:24:36 UTC 2023,,,,,,,,,,"0|z1klvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/23 15:53;martijnvisser;Fixed in apache/flink-connector-hbase

main: 298d8164495732f59d18c54d4d40b601b6d44f21
v3.0: 654469780f0ac85f156525575c8b522227559f03;;;","10/Nov/23 08:24;martijnvisser;Re-opened to add documentation in apache/flink-connector-base:

main: 2dcaa6a13f6eb76337c2c28a6685a8759fe890a1
v3.0: 87912baee1f75c43a6380867984c91715f201b99;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Java 21 (LTS),FLINK-33163,13552124,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,maver1ck,maver1ck,27/Sep/23 06:40,09/Feb/24 11:20,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,Based on https://issues.apache.org/jira/browse/FLINK-15736 we should have similar ticket for Java 21 LTS.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-27 06:40:10.0,,,,,,,,,,"0|z1klv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
seperate the executor in DefaultDispatcherResourceManagerComponentFactory for MetricFetcher and webMonitorEndpoint,FLINK-33162,13552122,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zhoujira86,zhoujira86,27/Sep/23 06:11,11/Mar/24 12:44,04/Jun/24 20:40,,1.13.1,,,,,,,,,,1.20.0,,,,Runtime / REST,,,,,,0,,,,,,"when starting a job with large number of taskmanagers, the jobmanager of the job failed to respond to and rest request. when look into the jstack we found all the 4 threads are server metrics fetcher.
{code:java}
// code placeholder
""Flink-DispatcherRestEndpoint-thread-4"" #91 daemon prio=5 os_prio=0 tid=0x00007f17e7823000 nid=0x246 waiting for monitor entry [0x00007f178e9fe000]   java.lang.Thread.State: BLOCKED (on object monitor)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.addAll(MetricStore.java:81)	- waiting to lock <0x00000003d5f62638> (a org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.lambda$queryMetrics$5(MetricFetcherImpl.java:244)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl$$Lambda$1590/569188012.accept(Unknown Source)	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000003ce80d8f0> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""Flink-DispatcherRestEndpoint-thread-3"" #88 daemon prio=5 os_prio=0 tid=0x00007f17e88af000 nid=0x243 waiting for monitor entry [0x00007f1790dfe000]   java.lang.Thread.State: BLOCKED (on object monitor)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.addAll(MetricStore.java:81)	- waiting to lock <0x00000003d5f62638> (a org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.lambda$queryMetrics$5(MetricFetcherImpl.java:244)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl$$Lambda$1590/569188012.accept(Unknown Source)	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000003ce80df88> (a java.util.concurrent.ThreadPoolExecutor$Worker) 

""Flink-DispatcherRestEndpoint-thread-2"" #79 daemon prio=5 os_prio=0 tid=0x00007f1793473800 nid=0x23a runnable [0x00007f17922fd000]   java.lang.Thread.State: RUNNABLE	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.add(MetricStore.java:216)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.addAll(MetricStore.java:82)	- locked <0x00000003d5f62638> (a org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.lambda$queryMetrics$5(MetricFetcherImpl.java:244)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl$$Lambda$1590/569188012.accept(Unknown Source)	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000003ce811120> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""Flink-DispatcherRestEndpoint-thread-1"" #76 daemon prio=5 os_prio=0 tid=0x00007f17a56f5000 nid=0x237 waiting for monitor entry [0x00007f1792cfd000]   java.lang.Thread.State: BLOCKED (on object monitor)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore.addAll(MetricStore.java:81)	- waiting to lock <0x00000003d5f62638> (a org.apache.flink.runtime.rest.handler.legacy.metrics.MetricStore)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl.lambda$queryMetrics$5(MetricFetcherImpl.java:244)	at org.apache.flink.runtime.rest.handler.legacy.metrics.MetricFetcherImpl$$Lambda$1590/569188012.accept(Unknown Source)	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748)
   Locked ownable synchronizers:	- <0x00000003ce8115f0> (a java.util.concurrent.ThreadPoolExecutor$Worker){code}
 

I suggest to enable a policy to reject unhandlable request executor
{code:java}
// code placeholder

Executors.newScheduledThreadPool( numThreads, new ExecutorThreadFactory.Builder() .setThreadPriority(threadPriority) .setPoolName(""Flink-"" + componentName) .build());


final MetricFetcher metricFetcher =
        updateInterval == 0
                ? VoidMetricFetcher.INSTANCE
                : MetricFetcherImpl.fromConfiguration(
                        configuration,
                        metricQueryServiceRetriever,
                        dispatcherGatewayRetriever,
                        executor);

webMonitorEndpoint =
        restEndpointFactory.createRestEndpoint(
                configuration,
                dispatcherGatewayRetriever,
                resourceManagerGatewayRetriever,
                blobServer,
                executor,
                metricFetcher,
                highAvailabilityServices.getClusterRestEndpointLeaderElectionService(),
                fatalErrorHandler);
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-27 06:11:48.0,,,,,,,,,,"0|z1kluo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[benchmark] Java17 profile for benchmarks,FLINK-33161,13552112,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,27/Sep/23 03:17,09/Oct/23 02:08,04/Jun/24 20:40,09/Oct/23 02:08,,,,,,,,,,,1.18.1,,,,Benchmarks,,,,,,0,pull-request-available,,,,,"Flink has supported java 17, however, the benchmark does not support running with java 17 out of the box. The runner complains:
{code:java}
Caused by: java.lang.RuntimeException: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @14899482
01:39:16 at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:69)
01:39:16 at org.apache.flink.api.java.typeutils.runtime.kryo.FlinkChillPackageRegistrar.registerSerializers(FlinkChillPackageRegistrar.java:67)
01:39:16 at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.getKryoInstance(KryoSerializer.java:513)
01:39:16 at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.checkKryoInitialized(KryoSerializer.java:522)
01:39:16 at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.serialize(KryoSerializer.java:348)
01:39:16 at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)
01:39:16 at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)
01:39:16 at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)
01:39:16 at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:151)
01:39:16 at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
01:39:16 at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
01:39:16 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)
01:39:16 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)
01:39:16 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)
01:39:16 at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)
01:39:16 at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
01:39:16 at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
01:39:16 at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:425)
01:39:16 at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:520)
01:39:16 at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:110)
01:39:16 at org.apache.flink.benchmark.ContinuousFileReaderOperatorBenchmark$MockSourceFunction.run(ContinuousFileReaderOperatorBenchmark.java:101)
01:39:16 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:114)
01:39:16 at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:71)
01:39:16 at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:338)
01:39:16 Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object[] java.util.Arrays$ArrayList.a accessible: module java.base does not ""opens java.util"" to unnamed module @14899482
01:39:16 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
01:39:16 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
01:39:16 at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
01:39:16 at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
01:39:16 at com.twitter.chill.java.ArraysAsListSerializer.<init>(ArraysAsListSerializer.java:67)
01:39:16 ... 23 more
 {code}
To resolve this, we should add a target profile for java 17 in pom.xml of benchmark project and provide some cli arg like ""--add-opens java.base/java.util=ALL-UNNAMED"" for the execution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-15736,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 02:08:24 UTC 2023,,,,,,,,,,"0|z1klsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 02:08;Yanfei Lei;Merged via 60e3522d3920391ae75234b67af7d3c774f4c7b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log the remote address when an exception occurs in the PartitionRequestQueue,FLINK-33160,13552109,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,lsdy,lsdy,27/Sep/23 02:18,02/Nov/23 02:29,04/Jun/24 20:40,02/Nov/23 02:29,,,,,,,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"Add the information of the remote address in the exception handling logs of the PartitionRequestQueue, so that network issues can be located through the network quintuple.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 02:29:56 UTC 2023,,,,,,,,,,"0|z1klrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/23 02:23;Zhanghao Chen;This is a useful feature for debugging network issues, +1;;;","08/Oct/23 03:24;huweihua;[~lsdy] makes sense to me, would you like prepare a PR for it?;;;","24/Oct/23 13:15;lsdy;[~huweihua] I have sent a pull request. Could you please take a look at it? ;;;","02/Nov/23 02:29;huweihua;resolved in master: 32652e1ea10d2578f583f37aeb329e34ad0464c7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No functional change - Use the variables instead of multiple literals for java and Maven version checks in the pom file ,FLINK-33159,13552022,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,davidradl,davidradl,26/Sep/23 11:03,09/Oct/23 08:50,04/Jun/24 20:40,,,,,,,,,,,,,,,,Build System,,,,,,0,pull-request-available,,,,,"Use the variables for java and Maven version checks in the pom file.

In the pom file I notice line

_<requireJavaVersion>_
                   _<version>1.8.0</version>_
 _</requireJavaVersion>_

This should use the variable like this:

  \{_}<version>{_}${target.java.version}{_}</version>{_}

I also see 2 instanced of _<target.java.version>1.8</target.java.version>_

The second instance should refer to the variable

also the Maven versions should use variables.

 

I will provide a pr for this.

 

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 27 08:53:12 UTC 2023,,,,,,,,,,"0|z1kl8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/23 11:46;martijnvisser;[~davidradl] It's a necessity since we have to enforce Java8 when creating a release. Until Java 8 support is dropped, we can't change this;;;","26/Sep/23 16:40;davidradl;Hi [~martijnvisser] , I see this change as cleaning up the pom file to use the variables - it should not change anything functionally. Do you think this introduces some change in behaviour somehow?

 

The pr is here - [https://github.com/apache/flink/pull/23469] hopefully this does not concern you. Sorry if my wording of this Jira was misleading.;;;","27/Sep/23 08:53;davidradl;Re opening - as this change is not changing the versions to check. It is moving to consistently use Maven and Java variables in the pom file. Currently Java version has multiple literals of 1.8 - this proposal is to change to use one variable with value 1.8 that is then used for the other java version references. It introduces Maven variables for the 2 Maven versions  currently referenced in the pom file.

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cryptic exception when there is a StreamExecSort in JsonPlan,FLINK-33158,13552017,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,26/Sep/23 10:40,03/Oct/23 12:51,04/Jun/24 20:40,03/Oct/23 12:43,1.16.2,1.17.1,,,,,,,,,1.18.1,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"{code}
CREATE TABLE MyTable (
   a bigint,
   b int not null,
   c varchar,
   d timestamp(3)
with (
   'connector' = 'values',
   'bounded' = 'false'
)

insert into MySink SELECT a, a from MyTable order by b
{code}

fails with:

{code}
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: For input string: ""null"" (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[2])
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 03 12:43:31 UTC 2023,,,,,,,,,,"0|z1kl7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 12:43;dwysakowicz;Fixed in:
* master
** 4afd09245823a1cf2d849dbd84c1b6d5ab58c875
* 1.18.1
** 2c8207a11a1850b11b06edfe10f3c8eecaf8d641;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support AWS SDK V2 for Flink's S3 FileSystem,FLINK-33157,13551994,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,mimaomao,mimaomao,26/Sep/23 08:00,28/Sep/23 15:58,04/Jun/24 20:40,28/Sep/23 15:58,,,,,,,,,,,,,,,Connectors / FileSystem,,,,,,0,,,,,,"Currently, Flink's S3 FileSystem only supports AWS SDK V1. It's recommended by AWS to use AWS SDK V2 for improvements and longer maintenance support. To complete this work, the following modules need to be updated:
 * flink-s3-fs-base
 * flink-s3-fs-hadoop
 * flink-s3-fs-presto

The Hadoop has already added support for AWS SDK V2, but this is not the case for Presto. To unblock component flink-s3-fs-presto for AWS SDK V2 support, we might have the following options:
 # Flink can have a copy of Presto's S3 FileSystem with AWS SDK V2 support added.
 # Add AWS SDK V2 support in Presto's S3 FileSystem in Presto and use new Presto version with this feature in Flink.
 # Use Trino's S3FileSystem, which has already supported AWS SDK V2, instead of Presto's S3FileSystem.",,,,,,,,,,,,,,,,,,,,,,FLINK-30975,FLINK-30975,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 27 06:27:04 UTC 2023,,,,,,,,,,"0|z1kl28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/23 11:35;martijnvisser;[~mimaomao] Do you want to work on this?;;;","27/Sep/23 06:27;samrat007;I think there exists Jira for sdk v1 to v2 migration 
https://issues.apache.org/jira/browse/FLINK-30975;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove flakiness from tests in OperatorStateBackendTest.java,FLINK-33156,13551980,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,asha-boyapati,asha-boyapati,asha-boyapati,26/Sep/23 05:51,29/Sep/23 14:46,04/Jun/24 20:40,,1.17.1,,,,,,,,,,1.17.1,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,,,"This issue is similar to:
https://issues.apache.org/jira/browse/FLINK-32963

We are proposing to make the following tests stable:

{quote}org.apache.flink.runtime.state.OperatorStateBackendTest#testSnapshotRestoreSync
org.apache.flink.runtime.state.OperatorStateBackendTest#testSnapshotRestoreAsync{quote}

The tests are currently flaky because the order of elements returned by iterators is non-deterministic.

The following PR fixes the flaky test by making them independent of the order of elements returned by the iterator:
https://github.com/apache/flink/pull/23464

We detected this using the NonDex tool using the following commands:

{quote}mvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -pl flink-runtime -DnondexRuns=10 -Dtest=org.apache.flink.runtime.state.OperatorStateBackendTest#testSnapshotRestoreSync

mvn edu.illinois:nondex-maven-plugin:2.1.1:nondex -pl flink-runtime -DnondexRuns=10 -Dtest=org.apache.flink.runtime.state.OperatorStateBackendTest#testSnapshotRestoreAsync{quote}

Please see the following Continuous Integration log that shows the flakiness:
https://github.com/asha-boyapati/flink/actions/runs/6193757385

Please see the following Continuous Integration log that shows that the flakiness is fixed by this change:
https://github.com/asha-boyapati/flink/actions/runs/6194044449",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-26 05:51:44.0,,,,,,,,,,"0|z1kkz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink ResourceManager continuously fails to start TM container on YARN when Kerberos enabled,FLINK-33155,13551879,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wangyang0918,wangyang0918,25/Sep/23 11:33,01/Oct/23 06:57,04/Jun/24 20:40,,,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,,,,,,"When Kerberos enabled(with key tab) and after one day(the container token expired), Flink fails to create the TaskManager container on YARN due to the following exception.

 
{code:java}
2023-09-25 16:48:50,030 INFO  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Worker container_1695106898104_0003_01_000069 is terminated. Diagnostics: Container container_1695106898104_0003_01_000069 was invalid. Diagnostics: [2023-09-25 16:48:45.710]token (token for hadoop: HDFS_DELEGATION_TOKEN owner=hadoop/master-1-1.c-5ee7bdc598b6e1cc.cn-beijing.emr.aliyuncs.com@EMR.C-5EE7BDC598B6E1CC.COM, renewer=, realUser=, issueDate=1695196431487, maxDate=1695801231487, sequenceNumber=12, masterKeyId=3) can't be found in cache
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (token for hadoop: HDFS_DELEGATION_TOKEN owner=xxxx, renewer=, realUser=, issueDate=1695196431487, maxDate=1695801231487, sequenceNumber=12, masterKeyId=3) can't be found in cache
    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
    at org.apache.hadoop.ipc.Client.call(Client.java:1491)
    at org.apache.hadoop.ipc.Client.call(Client.java:1388)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
    at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:907)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:431)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362)
    at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1666)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1573)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1588)
    at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:269)
    at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)
    at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)
    at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
    at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:243)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:236)
    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:224)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750) {code}
The root cause might be that we are reading the delegation token from JM local file[1]. It will expire after one day. When the old TaskManager container crashes and ResourceManager tries to create a new one, the YARN NodeManager will use the expired token to localize the resources for TaskManager and then fail.

Instead, we could read the latest valid token from UGI and then set it in the ContainerLaunchContext.

 

[1]. [https://github.com/apache/flink/blob/master/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java#L434]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 01 06:57:58 UTC 2023,,,,,,,,,,"0|z1kkco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/23 12:18;gaborgsomogyi;Not updating UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION is a known limitation of YARN.

If the mentioned code runs on the JM side and delegation tokens are enabled then it makes sense since the JM keeps it's tokens up-to-date all the time.

Couple of notes:
 * Changing the default behavior from file to UGI can be a breaking change to users which are depending on that some way
 * DT handling is a single threaded operation but as I see TM creation uses multiple threads which may end-up in undefined behavior;;;","25/Sep/23 12:58;wangyang0918;Thanks for your comments.

 

> Changing the default behavior from file to UGI can be a breaking change to users which are depending on that some way

What I mean is to get the delegation token from UGI instead of reading from file, just like we have already done in the {{{}YarnClusterDescriptor{}}}[1]. I am not sure why this will be a breaking change because the tokens in the {{ContainerLaunchContext}} are just same.

 

[1]. [https://github.com/apache/flink/blob/master/flink-yarn/src/main/java/org/apache/flink/yarn/YarnClusterDescriptor.java#L1334];;;","27/Sep/23 09:05;gaborgsomogyi;I would say green light on this :) Just saying that getting tokens from different source where the content is different is relatively a big change. W/o the mentioned change this is not going to work properly...
;;;","30/Sep/23 04:17;Feng Yuan;The root cause might be that we are reading the delegation token from JM local file[1] ， as i know token will renew every 1 day by Yarn rm until reach 7days limit, token in your log maxDate is ‘2023-09-27 15:53:51.487’， error ocurr is '2023-9-25', it is say: althought jm use token in local file, it is also can be sucesseful. If the token be renewed in 2023-9-25 sometime by yarn rm. So i think if this is a yarn issue? [~wangyang0918], hope your reply! ;;;","01/Oct/23 06:57;gaborgsomogyi;The fact that YARN is not able to renew the token after 7 days (with default config) is a known for long time. If this is the issue what one wants to solve then I would say that must be solved on YARN side and not hack around in Flink. We've had the same issue when we've formed the Spark authentication framework back in the days and we've had the same conclusion there...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink on k8s，An error occurred during consuming rocketmq,FLINK-33154,13551873,,Technical Debt,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,monody,monody,25/Sep/23 10:58,23/Oct/23 07:46,04/Jun/24 20:40,,kubernetes-operator-1.1.0,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,flink-kubernetes-operator,RocketMQ,,,,"The following error occurs when flink consumes rocketmq. The flink job is running on k8s, and the projects used are:
The projects used by flink to consume rocketmq are:
The flink job runs normally on yarn, and no abnormality is found on the rocketmq server. Why does this happen? and how to solve it?
!https://user-images.githubusercontent.com/47728686/265662530-231c500c-fd64-4679-9b0f-ff4a025dd766.png!","flink-kubernetes-operator：https://github.com/apache/flink-kubernetes-operator#current-api-version-v1beta1
rocketmq-flink：https://github.com/apache/rocketmq-flink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,scala,Mon Oct 23 07:46:42 UTC 2023,,,,,,,,,,"0|z1kkbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/23 00:49;martijnvisser;I would think this isn't necessarily a Kubernetes Operator problem, but given that there's no Flink project owned/supported RocketMQ connector, I would wonder if the issue actually resides in the connector. The operator doesn't do anything special when it comes to connector artifacts. ;;;","23/Oct/23 03:41;monody;[~martijnvisser]  hi~ Thank you for paying attention to this issue. I'm using [rocketmq-flink connector|https://github.com/apache/rocketmq-flink], It is production ready.

This leaves me wondering where the problem might be, do you have any possible guesses?;;;","23/Oct/23 06:37;martijnvisser;[~monody] But that connector isn't owned by the ASF Flink project, but by the ASF RocketMQ project. I'm seeing similar issues filed on the RocktMQ project like this one https://github.com/apache/rocketmq-flink/issues/100 so I'm suspecting this is really a RocketMQ problem. ;;;","23/Oct/23 07:27;monody;[~martijnvisser] That issue was also initiated by me:(;;;","23/Oct/23 07:37;martijnvisser;Ah. It could be a dependency issue on Netty, I can't really tell. But I really think this is a connector issue, not a Flink issue;;;","23/Oct/23 07:46;monody;[~martijnvisser] OK. thank you~ ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka using latest-offset maybe missing data,FLINK-33153,13551864,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Duplicate,,tanjialiang,tanjialiang,25/Sep/23 09:25,18/Dec/23 09:31,04/Jun/24 20:40,25/Sep/23 10:51,,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"When Kafka start with the latest-offset strategy, it does not fetch the latest snapshot offset and specify it for consumption. Instead, it sets the startingOffset to -1 (KafkaPartitionSplit.LATEST_OFFSET, which makes currentOffset = -1, and call the KafkaConsumer's  seekToEnd API). The currentOffset is only set to the consumed offset + 1 when the task consumes data, and this currentOffset is stored in the state during checkpointing. If there are very few messages in Kafka and a partition has not consumed any data, and I stop the task with a savepoint, then write data to that partition, and start the task with the savepoint, the task will resume from the saved state. Due to the startingOffset in the state being -1, it will cause the task to miss the data that was written before the recovery point.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 25 10:53:30 UTC 2023,,,,,,,,,,"0|z1kk9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/23 10:53;tanjialiang;It relate to https://issues.apache.org/jira/browse/FLINK-28303, so i close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus Sink Connector - Integration tests,FLINK-33152,13551858,13551769,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nicusX,nicusX,25/Sep/23 08:33,25/Sep/23 08:50,04/Jun/24 20:40,,,,,,,,,,,,prometheus-connector-1.0.0,,,,Connectors / Prometheus,,,,,,0,,,,,,Integration tests against containerised Prometheus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-25 08:33:46.0,,,,,,,,,,"0|z1kk80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus Sink Connector - Create Github Repo,FLINK-33151,13551857,13551769,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,25/Sep/23 08:22,25/Sep/23 08:54,04/Jun/24 20:40,25/Sep/23 08:54,,,,,,,,,,,prometheus-connector-1.0.0,,,,Connectors / Prometheus,,,,,,0,,,,,,Create the \{{flink-connector-prometheus}} repo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 25 08:54:39 UTC 2023,,,,,,,,,,"0|z1kk7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/23 08:54;dannycranmer;https://github.com/apache/flink-connector-prometheus;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 add the processing logic for the long type,FLINK-33150,13551856,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,wenhao.yu,wenhao.yu,25/Sep/23 08:21,25/Sep/23 15:40,04/Jun/24 20:40,,1.15.4,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,,"The AvroToRowDataConverters class has a convertToDate method that will report an error when it encounters time data represented by the long type, so add a code to handle the long type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,2023-09-25 08:21:27.0,,,,,,,,,,"0|z1kk7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump snappy-java to 1.1.10.4,FLINK-33149,13551854,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,rskraba,rskraba,25/Sep/23 08:18,26/Jan/24 12:56,04/Jun/24 20:40,23/Nov/23 09:58,1.16.3,1.17.2,1.18.0,,,,,,,,1.16.3,1.17.2,1.18.0,kafka-3.1.0,API / Core,Connectors / AWS,Connectors / HBase,Connectors / Kafka,Stateful Functions,,0,pull-request-available,,,,,"Xerial published a security alert for a Denial of Service attack that [exists on 1.1.10.1|https://github.com/xerial/snappy-java/security/advisories/GHSA-55g7-9cwv-5qfv].

This is included in flink-dist, but also in flink-statefun, and several connectors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 23 09:02:59 UTC 2023,,,,,,,,,,"0|z1kk74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/23 18:49;mapohl;Thanks for looking into it. I did a code investigation to see where we use snappy in flink core.

Snappy was introduced for the state backend and used in [SnappyStreamCompressionDecorator.java:25-26|https://github.com/apache/flink/blob/116f297478f2d443178510565b1cd5a2f387e241/flink-runtime/src/main/java/org/apache/flink/runtime/state/SnappyStreamCompressionDecorator.java#L25]. The classes that are affected by this vulnerability ({{SnappyInputStream}} and {{SnappyOutputStream}}) are not used. Flink uses {{SnappyFramedInputStream}} and {{SnappyFramedOutputStream}}. Therefore, it's not critical and priority Major makes sense. But it's still good to have this fixed considering the alerts that might pop up in security scanners.

I also did a brief analysis of a few connector implementations:
{code}
➜  workspace for c in $(ls -d flink-connector*); do echo $c; grep --include=pom.xml -Hirn snappy $c; done
flink-connector-aws
flink-connector-aws/pom.xml:254:                <groupId>org.xerial.snappy</groupId>
flink-connector-aws/pom.xml:255:                <artifactId>snappy-java</artifactId>
flink-connector-cassandra
flink-connector-elasticsearch
flink-connector-gcp-pubsub
flink-connector-hbase
flink-connector-hbase/pom.xml:245:                                              <groupId>org.xerial.snappy</groupId>
flink-connector-hbase/pom.xml:246:                                              <artifactId>snappy-java</artifactId>
flink-connector-hive
flink-connector-jdbc
flink-connector-kafka
flink-connector-kafka/pom.xml:70:        <snappy-java.version>1.1.8.3</snappy-java.version>
flink-connector-kafka/pom.xml:231:                <groupId>org.xerial.snappy</groupId>
flink-connector-kafka/pom.xml:232:                <artifactId>snappy-java</artifactId>
flink-connector-kafka/pom.xml:233:                <version>${snappy-java.version}</version>
flink-connector-mongodb
flink-connector-opensearch
flink-connector-pulsar
flink-connector-rabbitmq
flink-connector-redis-streams
{code}

Only {{flink-connector-kafka}} and {{flink-connector-aws}} have this dependency listed. None of them actually uses any classes from within the {{xerial}} package:
{code}
for c in $(ls -d flink-connector*); do echo $c; grep --include=""*java"" -Hirn xerial $c; done
flink-connector-aws
flink-connector-cassandra
flink-connector-elasticsearch
flink-connector-gcp-pubsub
flink-connector-hbase
flink-connector-hive
flink-connector-jdbc
flink-connector-kafka
flink-connector-mongodb
flink-connector-opensearch
flink-connector-pulsar
flink-connector-rabbitmq
flink-connector-redis-streams
{code}

Would it be worth removing the dependency from the connectors entirely? WDYT?;;;","25/Sep/23 19:03;mapohl;I think my previous investigation is not enough: It seems to be used by {{flink-avro}}, {{flink-parquet}} and {{flink-presto}}, as well.;;;","27/Sep/23 15:48;Sergey Nuyanzin;Merged to flink main repo 
master: [ec6ebe2d22d15883f7236895387a45a533cfefe0|https://github.com/apache/flink/commit/ec6ebe2d22d15883f7236895387a45a533cfefe0]

1.18: [f4093fcfcaf0dea7b497bf3c69d6608616de7092|https://github.com/apache/flink/commit/f4093fcfcaf0dea7b497bf3c69d6608616de7092]
1.17: [14c2a23d73e8f17a1741248d1362af20852e9f2b|https://github.com/apache/flink/commit/14c2a23d73e8f17a1741248d1362af20852e9f2b]

1.16: [b309293561a1881fcc127d2d7a712eb8b03edba5|https://github.com/apache/flink/commit/b309293561a1881fcc127d2d7a712eb8b03edba5];;;","28/Sep/23 07:30;rskraba;Thanks for taking a look!  I'm going through the Flink connector repos as well (I also did the flink-statefun repo, but so did Dependabot :D )

Kafka is currently bumping their implementation and it looks like a safe ""drop-in"" replacement (with some minor testing issues).

Avro *just missed* the upgrade with their last 1.11.3 release, but I've confirmed locally that it's safe to override the snappy dependency there as well.  It will be present in their next release.

I'll include a summary here about the connectors when I've finished taking a look.;;;","10/Oct/23 13:25;martijnvisser;Merged in:

apache/flink-connector-kafka:main 73f761fa73d4200d18f628eef7c79cf91dd1a0bc;;;","17/Oct/23 10:23;mapohl;Reverted the fixVersion change back to 1.18.0 (and added the other fix version). The issue is resolved in 1.18.0 already. This Jira issues isn't closed, yet, due to the open work on the connectors as far as I understand.;;;","23/Nov/23 03:45;yunta;[~mapohl] When can we close this ticket? ;;;","23/Nov/23 07:52;mapohl;[~rskraba] wanted to work on bringing this update to the connectors as well. Ryan: Could you create dedicated tickets for that so that we could close this one? I guess that would make sense because 1.18.0 was already released.;;;","23/Nov/23 09:02;rskraba;Hello!  Thanks for bringing this back.  I've checked all of the connectors (as well as the Hive connector currently being externalized) and the versions of Snappy are 1.1.10.4 or 1.1.10.5.

I created FLINK-33627 for flink-statefun, however, so this JIRA can be closed for Flink core and connectors.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update Web UI to adopt the new ""endpoint"" field in REST API",FLINK-33148,13551829,13551827,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,25/Sep/23 05:02,15/Nov/23 03:36,04/Jun/24 20:40,15/Nov/23 03:36,1.18.0,,,,,,,,,,1.19.0,,,,Runtime / Web Frontend,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 15 03:36:08 UTC 2023,,,,,,,,,,"0|z1kk1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/23 03:36;huweihua;resolved in master: 5f9bef4288f59f428d4b2036f6eb9b30adf08287, c4f2da4d133690c0eb9933b1eb1d53b6aa5a3187;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Introduce a new ""endpoint"" field in REST API to represent TaskManager endpoint (host + port) and deprecate the ""host"" field",FLINK-33147,13551828,13551827,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,25/Sep/23 05:00,09/Nov/23 11:19,04/Jun/24 20:40,09/Nov/23 11:19,1.18.0,,,,,,,,,,1.19.0,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 09 11:19:53 UTC 2023,,,,,,,,,,"0|z1kk1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Nov/23 11:19;huweihua; resolved in master: f77dbdfd0c7d510ae32ce932c7b84ed968496130;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-363: Unify the Representation of TaskManager Location in REST API and Web UI,FLINK-33146,13551827,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,Zhanghao Chen,Zhanghao Chen,Zhanghao Chen,25/Sep/23 04:57,15/Nov/23 04:55,04/Jun/24 20:40,15/Nov/23 04:55,1.18.0,,,,,,,,,,1.19.0,,,,Runtime / REST,Runtime / Web Frontend,,,,,0,,,,,,"Umbrella ticket for [FLIP-363: Unify the Representation of TaskManager Location in REST API and Web UI|https://cwiki.apache.org/confluence/display/FLINK/FLIP-363%3A+Unify+the+Representation+of+TaskManager+Location+in+REST+API+and+Web+UI]. This is a continuation of FLINK-25371. Separate tickets are created as the scope is enlarged after discussion.",,,,,,,,,,,,,,,,,,,,,FLINK-25371,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-25 04:57:20.0,,,,,,,,,,"0|z1kk14:",9223372036854775807,"Unify the representation of TaskManager location in REST API and Web UI. The ""host"" field is deprecated in favor of the newly introduced ""endpoint"" field that includes both the host and port infomation to distinguish multiple TaskManagers on the same host.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exposes the FileLifeCycleListener interface,FLINK-33145,13551819,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,wenhao.yu,wenhao.yu,25/Sep/23 03:24,01/Nov/23 09:26,04/Jun/24 20:40,,1.17.1,,,,,,,,,,1.15.4,,,,Connectors / FileSystem,,,,,,0,,,,,,"Monitors the life cycle of written files. Users can customize operations after the file is

committed. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,2023-09-25 03:24:06.0,,,,,,,,,,"0|z1kjzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Iteration API in DataStream,FLINK-33144,13551811,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,,Wencong Liu,Wencong Liu,25/Sep/23 02:19,12/Oct/23 07:35,04/Jun/24 20:40,27/Sep/23 09:04,1.19.0,,,,,,,,,,1.19.0,,,,API / DataStream,,,,,,0,pull-request-available,,,,,"[FLIP-357: Deprecate Iteration API of DataStream - Apache Flink - Apache Software Foundation|https://cwiki.apache.org/confluence/display/FLINK/FLIP-357%3A+Deprecate+Iteration+API+of+DataStream] has decided to deprecate the Iteration API of DataStream and remove it completely in the next major version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 27 09:04:37 UTC 2023,,,,,,,,,,"0|z1kjxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Sep/23 09:04;xtsong;master (1.19): 5525d85e70bfd53fb47a459b2e637e3d8a2bff54;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrongly throw error while temporary table join with invalidScan lookup source and selected as view,FLINK-33143,13551808,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Resolved,,337361684@qq.com,337361684@qq.com,25/Sep/23 01:57,26/Sep/23 02:37,04/Jun/24 20:40,26/Sep/23 02:36,1.19.0,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,Wrongly throw error while temporary table join with invalidScan lookup source and selected as view.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 26 02:35:52 UTC 2023,,,,,,,,,,"0|z1kjww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/23 02:35;337361684@qq.com;This fix way cannot cover all sql pattern, like subQuery, so this issue will be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus Sink Connector - Update Documentation,FLINK-33142,13551775,13551769,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nicusX,nicusX,24/Sep/23 07:01,24/Sep/23 07:01,04/Jun/24 20:40,,,,,,,,,,,,,,,,Documentation,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-24 07:01:59.0,,,,,,,,,,"0|z1kjpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promentheus Sink Connector - Amazon Managed Prometheus Request Signer,FLINK-33141,13551774,13551769,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,,nicusX,nicusX,24/Sep/23 07:01,09/Oct/23 13:30,04/Jun/24 20:40,09/Oct/23 13:30,,,,,,,,,,,,,,,Connectors / AWS,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33138,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 13:30:36 UTC 2023,,,,,,,,,,"0|z1kjpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 13:30;hong;Resolved by https://issues.apache.org/jira/browse/FLINK-33138;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus Sink Connector - E2E example on AWS,FLINK-33140,13551773,13551769,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nicusX,nicusX,24/Sep/23 06:59,25/Sep/23 08:32,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"End-to-end example application, deployable on Amazon Managed Service for Apache Flink, and writing to Amazon Managed Prometheus",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-24 06:59:52.0,,,,,,,,,,"0|z1kjp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus Sink Connector - Table API support,FLINK-33139,13551772,13551769,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,nicusX,nicusX,24/Sep/23 06:59,25/Sep/23 08:50,04/Jun/24 20:40,,,,,,,,,,,,prometheus-connector-1.0.0,,,,Connectors / Prometheus,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-24 06:59:05.0,,,,,,,,,,"0|z1kjow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus Connector Sink - DataStream API implementation,FLINK-33138,13551770,13551769,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,nicusX,nicusX,nicusX,24/Sep/23 06:58,15/May/24 08:05,04/Jun/24 20:40,,,,,,,,,,,,prometheus-connector-1.0.0,,,,Connectors / Prometheus,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33141,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-24 06:58:32.0,,,,,,,,,,"0|z1kjog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-312: Prometheus Sink Connector,FLINK-33137,13551769,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,nicusX,nicusX,nicusX,24/Sep/23 06:56,25/Sep/23 08:49,04/Jun/24 20:40,,,,,,,,,,,,prometheus-connector-1.0.0,,,,Connectors / Prometheus,,,,,,0,Connector,,,,,"Umbrella Jira for implementation of Prometheus Sink Connector
https://cwiki.apache.org/confluence/display/FLINK/FLIP-312:+Prometheus+Sink+Connector",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-24 06:56:39.0,,,,,,,,,,"0|z1kjo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Pulsar Connector RoundRobinTopicRouter Generates Invalid Error Message,FLINK-33136,13551753,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,longtimer,longtimer,23/Sep/23 17:56,15/Apr/24 10:11,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,,,"The RoundRobinTopicRouter class generates the runtime error ""You should provide topics for routing topic by message key hash."" when no partitions are set. This error is a direct copy of the error in the KeyHashTopicRouter but is nonsensical to a RoundRobinTopicRouter since hashing is not involved in route selection.

More importantly however, this error should be detected at deploy time when the PulsarSink is built with the builder since the list of topics is supplied via the setTopics() method of the builder.

Additionally, the wording of the error is not clear in any case and could be improved to something like: ""No partition routing topics were provided to allow for topic routing""

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 15 10:11:49 UTC 2024,,,,,,,,,,"0|z1kjko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/24 10:11;syhily;Yep, I think you are right. We should use more perspective words to describe the issue that no topics is present. BTW, add the check in the {{PulsarSink}}. PR is welcome.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Pulsar Connector Attempts Partitioned Routing on Unpartitioned Topic,FLINK-33135,13551752,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,longtimer,longtimer,23/Sep/23 17:40,23/Sep/23 21:18,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,,,"For a non-partitioned topic, the Flink Pulsar connector creates a TopicRouter, ensures that the list of partitions is empty during deployment and then complains when the list of partitions supplied to it is empty at runtime. The default TopicRouter that is created is the RoundRobinTopicRouter and it provides a nonsensical error for this type of TopicRouter. This error message issue is raised in ticket https://issues.apache.org/jira/browse/FLINK-33136.

The connector should not be applying a topic router to nonpartitioned topics or should treat the nonpartitioned topic as a special/different case. Currently, the following error is raised even though the setTopics() method is called on the PulsarSink.builder() with a single topic.

Caused by: java.lang.IllegalArgumentException: You should provide topics for routing topic by message key hash.
        at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
        at org.apache.flink.connector.pulsar.sink.writer.router.RoundRobinTopicRouter.route(RoundRobinTopicRouter.java:56)
        at org.apache.flink.connector.pulsar.sink.writer.PulsarWriter.write(PulsarWriter.java:147)
        at org.apache.flink.streaming.runtime.operators.sink.SinkWriterOperator.processElement(SinkWriterOperator.java:158)
        at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
        ... 30 more

The distinctTopics() method of the TopicNameUtils class is what ensures the list of partitions is empty for a nonpartitioned topic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-23 17:40:28.0,,,,,,,,,,"0|z1kjkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Connector redshift E2E test ,FLINK-33134,13551750,13533204,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,23/Sep/23 17:24,07/Nov/23 10:08,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-23 17:24:05.0,,,,,,,,,,"0|z1kjk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Document Update ,FLINK-33133,13551749,13533204,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,23/Sep/23 17:23,07/Nov/23 10:08,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-23 17:23:44.0,,,,,,,,,,"0|z1kjjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Connector Redshift Implementation (Table-API),FLINK-33132,13551748,13533204,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,samrat007,samrat007,23/Sep/23 17:22,05/May/24 10:12,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / AWS,,,,,,0,in-progress,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-23 17:22:54.0,,,,,,,,,,"0|z1kjjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Grafana dashboard for flink k8 operator metrics,FLINK-33131,13551747,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,,,gaurav726,gaurav726,23/Sep/23 16:38,23/Sep/23 16:38,04/Jun/24 20:40,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"I have created a grafana dashboard for flink k8 operator, can I commit same in flink k8 operator repo..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-23 16:38:20.0,,,,,,,,,,"0|z1kjjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't show the sources and sinks io metrics correctly at flink webui,FLINK-33130,13551723,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,xiebin,xiebin,23/Sep/23 05:12,21/May/24 12:43,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,pull-request-available,,,,,"Now the sources and sinks io metrics(Bytes/Records) is not showed correctly at flink webui。

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-23 05:12:07.0,,,,,,,,,,"0|z1kje0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't create RowDataToAvroConverter for LocalZonedTimestampType logical type,FLINK-33129,13551696,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,neuscottshao,neuscottshao,22/Sep/23 18:21,29/Sep/23 01:03,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,,"While creating converter using `RowDataToAvroConverters.createConverter` with LocalZonedTimestampType logical type, the method will throw exception. This is because the switch clause is missing a clause for `LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZON`.

Code: [https://github.com/apache/flink/blob/master/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/RowDataToAvroConverters.java#L75]

 

We can convert the value to `LocalDateTime` and then `TimestampData` using method below. Then we can apply the same converter as 
TIMESTAMP_WITHOUT_TIME_ZONE? 
 
`TimestampData fromLocalDateTime(LocalDateTime dateTime)`

Can Flink team help adding the support for this logical type and logical type root?

This is now a blocker for creating Flink Iceberg consumer with Avro GenericRecord when IcebergTable has `TimestampTZ` type field which will be converted to LocalZonedTimestampType.

See error below:
 Unsupported type: TIMESTAMP_LTZ(6) 
        stack: [ [-] 
          org.apache.flink.formats.avro.RowDataToAvroConverters.createConverter(RowDataToAvroConverters.java:186) 
          java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195) 
          java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1655) 
          java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) 
          java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) 
          java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:550) 
          java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) 
          java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:517) 
          org.apache.flink.formats.avro.RowDataToAvroConverters.createRowConverter(RowDataToAvroConverters.java:224) 
          org.apache.flink.formats.avro.RowDataToAvroConverters.createConverter(RowDataToAvroConverters.java:178) 
          org.apache.iceberg.flink.source.RowDataToAvroGenericRecordConverter.<init>(RowDataToAvroGenericRecordConverter.java:46) 
          org.apache.iceberg.flink.source.RowDataToAvroGenericRecordConverter.fromIcebergSchema(RowDataToAvroGenericRecordConverter.java:60) 
          org.apache.iceberg.flink.source.reader.AvroGenericRecordReaderFunction.lazyConverter(AvroGenericRecordReaderFunction.java:93) 
          org.apache.iceberg.flink.source.reader.AvroGenericRecordReaderFunction.createDataIterator(AvroGenericRecordReaderFunction.java:85) 
          org.apache.iceberg.flink.source.reader.DataIteratorReaderFunction.apply(DataIteratorReaderFunction.java:39) 
          org.apache.iceberg.flink.source.reader.DataIteratorReaderFunction.apply(DataIteratorReaderFunction.java:27) 
          org.apache.iceberg.flink.source.reader.IcebergSourceSplitReader.fetch(IcebergSourceSplitReader.java:74) 
          org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58) 
          org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162) 
          org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114) 
          java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) 
          java.util.concurrent.FutureTask.run(FutureTask.java:264) 
          java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) 
          java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) 
          java.lang.Thread.run(Thread.java:829)",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,java,,Tue Sep 26 22:00:03 UTC 2023,,,,,,,,,,"0|z1kj80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Sep/23 02:33;neuscottshao;[~danny0405] can you help take a look? as I saw you are the original author ;;;","26/Sep/23 01:54;neuscottshao;In DataStructureConverters, there is similar implementation already using         
```
putConverter(
                LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
                Integer.class,
                constructor(LocalZonedTimestampIntConverter::new));
```

[https://github.com/apache/flink/blob/9b2b4e3f194467aae0d299b3b403e0ca60c42ef0/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/data/conversion/DataStructureConverters.java#L134]

We can use same/smiliar approach to support TIMESTAMP_WITH_LOCAL_TIME_ZONE in `RowDataToAvroConverters.createConverter`;;;","26/Sep/23 22:00;neuscottshao;Hi [~jadireddi] thanks for working on the converter for `TIMESTAMP_LTZ`

Looks like you already had a PR for https://issues.apache.org/jira/browse/FLINK-30483.

Do you mind addressing this issue together with your PR? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestValuesRuntimeFunctions$TestValuesLookupFunction does not call open() on converter,FLINK-33128,13551671,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jgagnon1,jgagnon1,jgagnon1,22/Sep/23 13:58,26/Oct/23 15:41,04/Jun/24 20:40,26/Oct/23 15:41,1.19.0,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"When using the TestValues connector with nested Row values relying on BinaryArrayWriter the following exception happen : 
{code:java}
java.lang.NullPointerException: Cannot invoke ""org.apache.flink.table.data.writer.BinaryArrayWriter.getNumElements()"" because ""this.reuseWriter"" is null
    at org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.allocateWriter(ArrayObjectArrayConverter.java:140)
    at org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toBinaryArrayData(ArrayObjectArrayConverter.java:114)
    at org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:93)
    at org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:40)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:90)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:90)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:75)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at org.apache.flink.table.runtime.connector.source.DataStructureConverterWrapper.toInternal(DataStructureConverterWrapper.java:51)
    at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions$TestValuesLookupFunction.lambda$indexDataByKey$0(TestValuesRuntimeFunctions.java:626)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions$TestValuesLookupFunction.indexDataByKey(TestValuesRuntimeFunctions.java:624)
    at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions$TestValuesLookupFunction.open(TestValuesRuntimeFunctions.java:601)
    at LookupFunction$370.open(Unknown Source)
    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.open(LookupJoinRunner.java:67)
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner.open(LookupJoinWithCalcRunner.java:51)
    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
    at org.apache.flink.streaming.api.operators.ProcessOperator.open(ProcessOperator.java:56)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:731)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:706)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:672)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550){code}
 

This is happening because open() is being not called from TestValuesLookupFunction.open() and the underlying converter writer never gets initialized.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 26 15:40:56 UTC 2023,,,,,,,,,,"0|z1kj2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Oct/23 15:40;gaoyunhaii;Merged on master via f31770fcf5769052f1ac32a6529de979eaf339a4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HeapKeyedStateBackend: use buffered I/O to speed up local recovery,FLINK-33127,13551476,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,zhangyy91,zhangyy91,zhangyy91,21/Sep/23 03:08,25/Sep/23 09:22,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / State Backends,,,,,,0,,,,,,"Recently, I observed a slow restore case in local recovery using hashmap statebackend.

It took 147 seconds to restore from a 467MB snapshot, 9 times slower than that (16s) when restore from remote fs.

The thread dump show that It read local snapshot file directly by unbuffered FileInputStream / fs.local.LocalDataInputStream.

!thread_dump.png!

Maybe we can wrap with BufferInputStream to speed up local recovery.",,,,,,,,,,,,,,,,,,,,,,,FLINK-19911,FLINK-26586,,,,,,,,,,,,,,,,,,"21/Sep/23 02:52;zhangyy91;thread_dump.png;https://issues.apache.org/jira/secure/attachment/13063084/thread_dump.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 25 09:22:27 UTC 2023,,,,,,,,,,"0|z1khv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/23 03:49;masteryhx;IIUC, it's duplicated one with FLINK-26586 and FLINK-19911.

So just kindly ping, [~Matthias Schwalbe] Are you still working on FLINK-26586 ?;;;","25/Sep/23 09:06;Matthias Schwalbe;[~masteryhx] : I actually want to finish FLINK-26585 first before I can start  FLINK-26586 (capacity)

FLINK-26585 is somewhat hung in approval of PR without any progress for a couple of weeks.

(will ping you on that ticket in a second)

Thias;;;","25/Sep/23 09:22;masteryhx;Actually I have taked a second review last month but not received your response until now.

Of course, I'm fine that we focused on FLINK-26585 firstly.

Just Kindly ping about the duplicated ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix EventTimeAllWindowCheckpointingITCase jobName typo,FLINK-33126,13551409,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mayuehappy,mayuehappy,mayuehappy,20/Sep/23 12:47,21/Sep/23 06:01,04/Jun/24 20:40,21/Sep/23 06:01,1.17.1,,,,,,,,,,1.19.0,,,,Tests,,,,,,0,pull-request-available,,,,,Fix EventTimeAllWindowCheckpointingITCase jobName Typo ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 21 06:01:57 UTC 2023,,,,,,,,,,"0|z1khg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/23 06:01;huweihua;master: 4f09bbb39239f5523da8ecb7c7cd5ac9fd34c0e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade JOSDK to 4.4.4,FLINK-33125,13551357,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nfraison.datadog,nfraison.datadog,20/Sep/23 07:58,20/Nov/23 12:29,04/Jun/24 20:40,21/Sep/23 12:42,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"JOSDK [4.4.4|https://github.com/operator-framework/java-operator-sdk/releases/tag/v4.4.4] contains fix for leader election issue we face in our environment

Here are more information on the [issue|https://github.com/operator-framework/java-operator-sdk/issues/2056] faced",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 21 12:42:37 UTC 2023,,,,,,,,,,"0|z1kh4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/23 12:42;gyfora;merged to main fd68924f8db5872092177fc77ab07349d688d0fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Connector not working for table,FLINK-33124,13551354,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Cannot Reproduce,,aarsh33,aarsh33,20/Sep/23 07:34,16/Oct/23 07:30,04/Jun/24 20:40,16/Oct/23 07:30,1.16.0,,,,,,,,,,,,,,Connectors / Kafka,Table SQL / Runtime,,,,,0,,,,,,"The kafka table connector is not working for table 
[As mentioned here|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/connectors/table/kafka/#security], I am providing the mechanism, the protocol and the jaas config, but the server doesn't have a truststore, so is it mandatory to provide the truststore? 
And if not, what am I missing 
This is my command

CREATE TABLE IF NOT EXISTS `xyz` (
proctime AS PROCTIME()
) WITH (
'connector' = 'kafka',
'topic' = 'xyz',
'scan.startup.mode' = 'group-offsets',
'properties.auto.offset.reset' = 'earliest',
'properties.security.protocol' = 'SASL_SSL',
'properties.sasl.mechanism' = 'SCRAM-SHA-256',
'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.scram.ScramLoginModule required username=""xyz"" password=""xyz"";',
'properties.bootstrap.servers' = 'xyz',
'properties.group.id' = 'xyz',
'format' = 'protobuf',
'protobuf.message-class-name' = 'xyz'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 29 01:05:58 UTC 2023,,,,,,,,,,"0|z1kh40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Sep/23 01:05;martijnvisser;[~aarsh33] This all depends on what's necessary in your configuration to authorize towards your Kafka cluster. If you have a setup that doesn't require a truststore, you don't have to provide it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong dynamic replacement of partitioner from FORWARD to REBLANCE for autoscaler and adaptive scheduler,FLINK-33123,13551349,,Bug,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,dmvk,Zhanghao Chen,Zhanghao Chen,20/Sep/23 07:06,17/Jan/24 13:26,04/Jun/24 20:40,,1.17.0,1.18.0,,,,,,,,,,,,,Autoscaler,Runtime / Coordination,,,,,0,,,,,,"*Background*

https://issues.apache.org/jira/browse/FLINK-30213 reported that the edge is wrong when the parallelism is changed for a vertex with a FORWARD edge, which is used by both the autoscaler and adaptive scheduler where one can change the vertex parallelism dynamically. Fix is applied to dynamically replace partitioner from FORWARD to REBLANCE on task deployment in {{{}StreamTask{}}}: 
 
!image-2023-09-20-15-09-22-733.png|width=560,height=221!
*Problem*

Unfortunately, the fix is still buggy in two aspects:
 # The connections between upstream and downstream tasks are determined by the distribution type of the partitioner when generating execution graph on the JM side. When the edge is FORWARD, the distribution type is POINTWISE, and Flink will try to evenly distribute subpartitions to all downstream tasks. If one want to change it to REBALANCE, the distribution type has to be changed to ALL_TO_ALL to make all-to-all connections between upstream and downstream tasks. However, the fix did not change the distribution type which makes the network connections be set up in a wrong way.
 # The FOWARD partitioner will be replaced if environment.getWriter(outputIndex).getNumberOfSubpartitions() equals to the task parallelism. However, the number of subpartitions here equals to the number of downstream tasks of this particular task, which is also determined by the distribution type of the partitioner when generating execution graph on the JM side.  When ceil(downstream task parallelism / upstream task parallelism) = upstream task parallelism, we will have the number of subpartitions = task parallelism. For example, for a topology A (parallelism 2) -> B (parallelism 5), we will have 1 A task having 2 subpartitions, 1 A task having 3 subpartition, and hence 1 task will have its number of subpartitions equals to the task parallelism 2 and skip partitioner replacement. As a result, that task will only send data to only one downstream task as the FORWARD partitioner always send data to the first subpartition. In fact, for a normal job with a FORWARD edge without any autoscaling action, you will find that the partitioner is changed to REBALANCE internally as the number of subpartitions always equals to 1 in this case.

!image-2023-09-20-15-14-04-679.png|width=892,height=301!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30213,,,,,,,,,,,,,"20/Sep/23 07:09;Zhanghao Chen;image-2023-09-20-15-09-22-733.png;https://issues.apache.org/jira/secure/attachment/13063048/image-2023-09-20-15-09-22-733.png","20/Sep/23 07:14;Zhanghao Chen;image-2023-09-20-15-14-04-679.png;https://issues.apache.org/jira/secure/attachment/13063049/image-2023-09-20-15-14-04-679.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 21 06:21:56 UTC 2023,,,,,,,,,,"0|z1kh2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 07:35;Zhanghao Chen;Hi [~gyfora] [~mxm] , please help take a look. I personally think that dynamically replacing partitioner on task deployment time is a wrong direction to follow and we'd better dynamically change the partitioner info on the jobgraph side. However, the currently way of encoding partitioner info makes it difficult to do so and may require a large-scale refactoring. Maybe we can apply a quick fix first and consider the refactoring later:
 * If we just want to change FORWARD to RESCALE, the easiest fix would be to always replace FORWARD partitioner by RESCALE partitioner in StreamTask. When the number of subpartitions = 1, the behavior of FORWARD and RESCALE partitioner is actually the same.
 * If we just want to change FORWARD to REBALANCE, the easiest fix would be to always replace FORWARD partitioner by REBALANCE partitioner in StreamTask + changing the distribution type to ALL_TO_ALL in jobgraph.

Looking forward to your opinions on it.;;;","20/Sep/23 15:26;fanrui;Hi [~Zhanghao Chen] , thanks for your report!

Your analysis is correct, and I also found this bug last month. I didn't report it because I don't know how to fix it properly and the FORWARD partitioner between 2 tasks aren't common(most of operator are chained directly).

First of all, I prefer changing it from FORWARD to REBALANCE, and I said the reason at [code review|https://github.com/apache/flink/pull/21443#discussion_r1042919428] of FLINK-30213.

Why I don't know how to fix it properly?

As you said: we should changing the distribution type to ALL_TO_ALL in jobgraph. Here is some issues here:
 * issue1: The jobGraph is just generated once for Adaptive Scheduler. And the jobgraph isn't changed even if the parallelism is changed(Just update the ExecutionGraph).
 * If the issue1 is solved, the issue2 is how to handle the case that from REBALANCE to FORWARD?
 ** Assume a job has taskA and taskB, the parallelism of them are 3, and user uses the FORWARD partitioner
 ** Time1: the parallelism of taskA is changed to 2, we should do 2 things:
 *** replace FORWARD partitioner by REBALANCE partitioner in StreamTask
 *** changing the distribution type to ALL_TO_ALL in jobgraph.
 ** Time2: the parallelism of taskB is changed to 2, we should do 2 things:
 *** Using the FORWARD partitioner
 *** Using the POINTWISE distribution type
 ** The case is fine.
 ** However, assume a job has taskA and taskB, the parallelism of them are 3, and user uses the *REBALANCE* partitioner.
 *** This case, user choose the REBALANCE partitioner even if it can use FORWARD partitioner here.
 *** For this case, we should still keep the REBALANCE partitioner and ALL_TO_ALL for time1 and time2.
 ** So time2 needs to consider should we update it to FORWARD partitioner and POINTWISE distribution type.

I'm not sure whether these 2 issues are clear.

Please let me know if I'm wrong, thanks~;;;","21/Sep/23 06:21;Zhanghao Chen;Hi [~fanrui], thanks for sharing the issues. For issue 1: directly updating the execution graph should also work. For issue 2: this is indeed an issue and I don't know if there is a good solution to it if we would use REBALANCE. But I think it would be better to use RESCALE with some help from the auotscaling algorithm side. We choose to change it from FORWARD to RESCALE instead internally for the following reasons:
 * REBALANCE takes quite a few network memory which can lead to memory issue after rescaling, esp. if the parallelism is big. It may also introduce performance degrading due to the extra shuffle.
 * FORWARD and RESCALE are actually interchangeable, they share the same shuffle behavior under the same upstream and downstream parallelism setting. This avoids the issues mentioned in issue2 here.
 * To address the issue that RESCALE might lead to imbalanced data on the downstream side, we introduced an improvement on the autoscaling algo side to make the upstream and downstream task parallelism to be multiples of each other.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Benchmark] Null checkpoint directory in rescaling benchmarks,FLINK-33122,13551344,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,20/Sep/23 06:51,20/Oct/23 09:37,04/Jun/24 20:40,20/Oct/23 09:37,,,,,,,,,,,1.19.0,,,,Benchmarks,,,,,,0,pull-request-available,,,,,"Currently, when setting up a rescaling benchmark, a local checkpoint storage is created based on a local path configured by ""benchmark.state.data-dir"". When user does not provide value for this option, an exception is thrown. In this case, the right behavior should be to create a temporary directory for checkpoint, just like the _StateBackendBenchmarkUtils#createKeyedStateBackend_
 does for local data directory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 09:36:44 UTC 2023,,,,,,,,,,"0|z1kh1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/23 09:36;masteryhx;merged into master via ddaafee5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed precondition in JobExceptionsHandler due to concurrent global failures,FLINK-33121,13551331,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,pgaref,pgaref,pgaref,20/Sep/23 05:54,28/Mar/24 17:06,04/Jun/24 20:40,28/Mar/24 17:06,1.18.0,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"We make the assumption that Global Failures (with null Task name) may only be RootExceptions and and Local/Task exception may be part of concurrent exceptions List (see {{{}JobExceptionsHandler#createRootExceptionInfo{}}}).
However, when the Adaptive scheduler is in a Restarting phase due to an existing failure (that is now the new Root) we can still, in rare occasions, capture new Global failures, violating this condition (with an assertion is thrown as part of {{{}assertLocalExceptionInfo{}}}) seeing something like:
{code:java}
The taskName must not be null for a non-global failure.  {code}
We want to ignore Global failures while being in a Restarting phase on the Adaptive scheduler until we properly support multiple Global failures in the Exception History as part of https://issues.apache.org/jira/browse/FLINK-34922

Note: DefaultScheduler does not suffer from this issue as it treats failures directly as HistoryEntries (no conversion step)",,,,,,,,,,,,,,,,,,FLINK-34922,,,,,,,,,,,,,,FLINK-33565,FLINK-34922,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 28 15:20:06 UTC 2024,,,,,,,,,,"0|z1kgyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 07:17;chesnay;??there are rare cases where multiple concurrent global failures are triggered??

Can you give us an example of when this might happen?;;;","20/Sep/23 17:46;chesnay;Had an offline chat; our suspicion is that something is calling the failure handling logic of the scheduler without running this call in the main thread.

We'll try to confirm this theory.;;;","21/Nov/23 06:40;fanrui;Hey [~pgaref] , thanks for reporing this Jira and fixing it. :)

I didn't understand this JIRA clearly. I'd like to check with you first, assertLocalExceptionInfo is called in 2 places,
 * First one is `JobExceptionsHandler#createExceptionInfo` calls `assertLocalExceptionInfo` [1]
 * Second one is `JobExceptionsHandler#createRootExceptionInfo` calls `assertLocalExceptionInfo` [2]

You meet an exception in the first caller, right?

Would you mind offering the exception information in this JIRA? Does it happen in Flink CI or production environment? Looking forward to your feedback, thanks~

[1] https://github.com/apache/flink/blob/278504a2787a154faf6f6401028d4bbadafbba0a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/JobExceptionsHandler.java#L252

[2] https://github.com/apache/flink/blob/278504a2787a154faf6f6401028d4bbadafbba0a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/JobExceptionsHandler.java#L236;;;","28/Mar/24 15:20;pgaref; Closing in favor of https://issues.apache.org/jira/browse/FLINK-34922;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-361: Improve GC Metrics,FLINK-33120,13551327,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,gyfora,gyfora,gyfora,20/Sep/23 05:48,11/Mar/24 12:44,04/Jun/24 20:40,,,,,,,,,,,,1.20.0,,,,Runtime / Metrics,,,,,,0,,,,,,"Umbrella ticket for FLIP-361: Improve GC Metrics

[https://cwiki.apache.org/confluence/display/FLINK/FLIP-361%3A+Improve+GC+Metrics]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-20 05:48:15.0,,,,,,,,,,"0|z1kgy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The pojo result returned be procedure should be Row of fields in the pojo instead of the whole pojo object,FLINK-33119,13551316,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,luoyuxia,luoyuxia,20/Sep/23 04:16,22/Sep/23 11:43,04/Jun/24 20:40,22/Sep/23 11:43,,,,,,,,,,,1.18.0,1.19.0,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"If a procedure return a Pojo object, as a table result of Row, Now, Flink will consider it as Row.of(pojo) instead of Row.of(f1, f2, ..) where f1, f2 are the fields of the Pojo object. The current behavior for procedure is not align to function. So, we should consider it as Row.of(f1, f2, ..).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 22 08:29:21 UTC 2023,,,,,,,,,,"0|z1kgvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Sep/23 08:29;luoyuxia;1.18: deb07e99560b45033a629afc3f90666ad0a32feb

master: 9b2b4e3f194467aae0d299b3b403e0ca60c42ef0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the PythonBridgeUtils,FLINK-33118,13551310,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,20/Sep/23 02:05,25/Sep/23 05:21,04/Jun/24 20:40,25/Sep/23 05:21,,,,,,,,,,,ml-2.4.0,,,,Library / Machine Learning,,,,,,0,pull-request-available,,,,,"We added org.apache.flink.ml.python.PythonBridgeUtils.java before to workaround the 
FLINK-30168 and FLINK-29477. Now they are fixed so we can remove the class along with its dependencies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-20 02:05:11.0,,,,,,,,,,"0|z1kgu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a wrong scala example in udfs page,FLINK-33117,13551213,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,cuiyanxiang,cuiyanxiang,19/Sep/23 10:00,20/Sep/23 01:40,04/Jun/24 20:40,,1.17.1,1.18.0,1.19.0,,,,,,,,,,,,Documentation,,,,,,0,,,,,,"problem：scala syntax error

!image-2023-09-19-17-58-42-102.png!

!image-2023-09-19-17-59-52-174.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 09:59;cuiyanxiang;image-2023-09-19-17-58-42-102.png;https://issues.apache.org/jira/secure/attachment/13063028/image-2023-09-19-17-58-42-102.png","19/Sep/23 10:00;cuiyanxiang;image-2023-09-19-17-59-52-174.png;https://issues.apache.org/jira/secure/attachment/13063027/image-2023-09-19-17-59-52-174.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 20 01:40:39 UTC 2023,,,,,,,,,,"0|z1kg8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 10:25;jiabao.sun;Thanks [~cuiyanxiang] to report this.
Are you going to submit a PR to fix this?;;;","20/Sep/23 01:34;cuiyanxiang;sure, I have created a PR for this [~jiabao.sun] 

https://github.com/apache/flink/pull/23439;;;","20/Sep/23 01:40;cuiyanxiang;cc [~jark] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientTest.testCancelExecutionInteractiveMode fails with NPE on AZP,FLINK-33116,13551208,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabao.sun,Sergey Nuyanzin,Sergey Nuyanzin,19/Sep/23 09:30,20/Oct/23 10:50,04/Jun/24 20:40,20/Oct/23 10:50,1.19.0,,,,,,,,,,1.16.3,1.17.2,1.18.1,1.19.0,Table SQL / Client,,,,,,0,pull-request-available,stale-critical,test-stability,,,"This build [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53309&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12264]

fails as
{noformat}
Sep 18 02:26:15 02:26:15.743 [ERROR] org.apache.flink.table.client.cli.CliClientTest.testCancelExecutionInteractiveMode  Time elapsed: 0.1 s  <<< ERROR!
Sep 18 02:26:15 java.lang.NullPointerException
Sep 18 02:26:15 	at org.apache.flink.table.client.cli.CliClient.closeTerminal(CliClient.java:284)
Sep 18 02:26:15 	at org.apache.flink.table.client.cli.CliClient.close(CliClient.java:108)
Sep 18 02:26:15 	at org.apache.flink.table.client.cli.CliClientTest.testCancelExecutionInteractiveMode(CliClientTest.java:314)
Sep 18 02:26:15 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/23 04:05;jiabao.sun;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13063501/screenshot-1.png","12/Oct/23 04:05;jiabao.sun;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13063502/screenshot-2.png","12/Oct/23 04:05;jiabao.sun;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13063503/screenshot-3.png","12/Oct/23 04:05;jiabao.sun;screenshot-4.png;https://issues.apache.org/jira/secure/attachment/13063504/screenshot-4.png","12/Oct/23 04:05;jiabao.sun;screenshot-5.png;https://issues.apache.org/jira/secure/attachment/13063505/screenshot-5.png",,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 10:50:47 UTC 2023,,,,,,,,,,"0|z1kg7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","04/Oct/23 10:46;mapohl;Looks like the {{CliClient}} is not thread-safe: The close call fails with a {{NullPointerException}} because {{CliClient.close()}} calls {{closeTerminal()}} if a terminal set. But the test finishes the {{client.executeInInteractiveMode()}} in a separate thread which calls {{closeTerminal()}} in the end as well. [~fsk119] can you delegate this issue?;;;","12/Oct/23 04:05;jiabao.sun;Steps to reproduce:
thread1 -> CliClient.executeInInteractiveMode
thread1 -> CliClient.closeTerminal
main    -> CliClient.close
main    -> CliClient.closeTerminal
thread1 -> CliClient.closeTerminal# { terminal = null; }
main    -> CliClient.closeTerminal# { terminal.close(); } // NPE will cause

 !screenshot-1.png! 
 !screenshot-2.png! 
 !screenshot-3.png! 
 !screenshot-4.png! 
 !screenshot-5.png! 

;;;","20/Oct/23 10:50;mapohl;master: [c1c780225cd5c622aa6f2f052d5b926fc4c74dbb|https://github.com/apache/flink/commit/c1c780225cd5c622aa6f2f052d5b926fc4c74dbb]
1.18: [7eedd4238e107c3991139c6ce6aac7d2b3b956d8|https://github.com/apache/flink/commit/7eedd4238e107c3991139c6ce6aac7d2b3b956d8]
1.17: [4ce41e14798ccc3320a194ab7ab8eb5cb9ba5ed1|https://github.com/apache/flink/commit/4ce41e14798ccc3320a194ab7ab8eb5cb9ba5ed1]
1.16: [e1114181eeb766c0fb7aea9c8dfe14660c62aca0|https://github.com/apache/flink/commit/e1114181eeb766c0fb7aea9c8dfe14660c62aca0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractHadoopRecoverableWriterITCase is hanging with timeout on AZP,FLINK-33115,13551207,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,samrat007,Sergey Nuyanzin,Sergey Nuyanzin,19/Sep/23 09:24,17/Jan/24 07:48,04/Jun/24 20:40,,1.17.3,1.18.0,1.19.0,,,,,,,,,,,,Tests,,,,,,0,stale-critical,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53281&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9&l=14239
is failing as
{noformat}
Sep 15 11:33:02 ==============================================================================
Sep 15 11:33:02 Process produced no output for 900 seconds.
Sep 15 11:33:02 ==============================================================================

...
Sep 15 11:33:03 	at java.io.DataInputStream.read(DataInputStream.java:149)
Sep 15 11:33:03 	at org.apache.flink.runtime.fs.hdfs.HadoopDataInputStream.read(HadoopDataInputStream.java:96)
Sep 15 11:33:03 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
Sep 15 11:33:03 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
Sep 15 11:33:03 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
Sep 15 11:33:03 	- locked <0x00000000bfa7a760> (a java.io.InputStreamReader)
Sep 15 11:33:03 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
Sep 15 11:33:03 	at java.io.BufferedReader.fill(BufferedReader.java:161)
Sep 15 11:33:03 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
Sep 15 11:33:03 	- locked <0x00000000bfa7a760> (a java.io.InputStreamReader)
Sep 15 11:33:03 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
Sep 15 11:33:03 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.getContentsOfFile(AbstractHadoopRecoverableWriterITCase.java:387)
Sep 15 11:33:03 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testResumeAfterMultiplePersist(AbstractHadoopRecoverableWriterITCase.java:377)
Sep 15 11:33:03 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWriterITCase.testResumeAfterMultiplePersistWithMultiPartUploads(AbstractHadoopRecoverableWriterITCase.java:330)
Sep 15 11:33:03 	at org.apache.flink.runtime.fs.hdfs.AbstractHadoopRecoverableWri
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,FLINK-33574,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 17 07:48:19 UTC 2024,,,,,,,,,,"0|z1kg7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 09:25;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53280&view=logs&j=4eda0b4a-bd0d-521a-0916-8285b9be9bb5&t=2ff6d5fa-53a6-53ac-bff7-fa524ea361a9;;;","03/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","22/Nov/23 15:52;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54801&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=13288;;;","22/Nov/23 15:53;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54803&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=13625;;;","07/Jan/24 23:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55861&view=logs&j=a1c9cb25-90c0-571d-f799-17f22465fffe&t=d3dfb133-675e-59f9-fc92-7ec7d710e0e1&l=14321;;;","07/Jan/24 23:36;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55882&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=14777;;;","07/Jan/24 23:39;Sergey Nuyanzin;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55883&view=logs&j=7e3d33c3-a462-5ea8-98b8-27e1aafe4ceb&t=ef77f8d1-44c8-5ee2-f175-1c88f61de8c0&l=40965;;;","07/Jan/24 23:50;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55930&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=13667;;;","07/Jan/24 23:52;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55932&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=13667;;;","08/Jan/24 00:27;Sergey Nuyanzin;1.18: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55982&view=logs&j=93ebd72a-004d-5a68-6295-7ace4ad889cd&t=35e92294-2840-51f1-1753-ae015c24c41f&l=13365;;;","08/Jan/24 00:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55980&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=13189;;;","08/Jan/24 06:20;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56089&view=logs&j=93ebd72a-004d-5a68-6295-7ace4ad889cd&t=35e92294-2840-51f1-1753-ae015c24c41f&l=13514;;;","16/Jan/24 14:15;mapohl;Same pipeline run:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56348&view=logs&j=93ebd72a-004d-5a68-6295-7ace4ad889cd&t=35e92294-2840-51f1-1753-ae015c24c41f&l=13705]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56348&view=logs&j=54dde87f-1e30-5a5e-7727-3286687a13f7&t=4653b7d3-60bf-5409-5154-679ed6d8688e&l=13448] ;;;","16/Jan/24 14:24;mapohl;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56359&view=logs&j=93ebd72a-004d-5a68-6295-7ace4ad889cd&t=35e92294-2840-51f1-1753-ae015c24c41f&l=14874]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56359&view=logs&j=b6f8a893-8f59-51d5-fe28-fb56a8b0932c&t=095f1730-efbe-5303-c4a3-b5e3696fc4e2&l=13818];;;","16/Jan/24 14:26;mapohl;1.18: [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56360&view=logs&j=a1c9cb25-90c0-571d-f799-17f22465fffe&t=d3dfb133-675e-59f9-fc92-7ec7d710e0e1&l=13655];;;","16/Jan/24 14:35;mapohl;* [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=2e8cb2f7-b2d3-5c62-9c05-cd756d33a819&t=2dd510a3-5041-5201-6dc3-54d310f68906&l=14862]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=56416&view=logs&j=20977292-985a-55fa-bea5-c9f02afad127&t=a1f924dc-43f6-5215-f225-22135f5ba121&l=13440] ;;;","16/Jan/24 14:47;mapohl;[~samrat007] can you have a look at it? I noticed that you worked on FLINK-28513 that was merged close to when this issue started to appear. You touched {{S3RecoverableFsDataOutputStream}} which also appears in the thread dump:
{code:java}
[...]
Jan 16 01:52:14 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartUploadToComplete(RecoverableMultiPartUploadImpl.java:233)
Jan 16 01:52:14 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.awaitPendingPartsUpload(RecoverableMultiPartUploadImpl.java:223)
Jan 16 01:52:14 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetRecoverable(RecoverableMultiPartUploadImpl.java:152)
Jan 16 01:52:14 	at org.apache.flink.fs.s3.common.writer.RecoverableMultiPartUploadImpl.snapshotAndGetRecoverable(RecoverableMultiPartUploadImpl.java:56)
Jan 16 01:52:14 	at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.persist(S3RecoverableFsDataOutputStream.java:167)
[...] {code}
To be fair, the initial stack trace seems to look differently.;;;","17/Jan/24 05:17;samrat007;Hi [~mapohl] , 

i will look into it . 

can you please  assign me the ticket ? ;;;","17/Jan/24 07:48;mapohl;Thanks for offering your help. I assigned the issue to you. It would be helpful to fix it due to the frequency of failures in the recent builds.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RightOuterJoinTaskTest.testCancelRightOuterJoinTaskWhileBuilding:232 Task thread did not finish within 60 seconds,FLINK-33114,13551205,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,,,Sergey Nuyanzin,Sergey Nuyanzin,19/Sep/23 09:19,07/Feb/24 08:40,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,API / Core,,,,,,0,stale-critical,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53250&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8
failed as 
{noformat}

Sep 14 22:34:21 		at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Sep 14 22:34:21 		at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
Sep 14 22:34:21 		at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
Sep 14 22:34:21 		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeMethodInExtensionContext(ClassBasedTestDescriptor.java:520)
Sep 14 22:34:21 		at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$synthesizeAfterEachMethodAdapter$24(ClassBasedTestDescriptor.java:510)
Sep 14 22:34:21 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAfterEachMethods$10(TestMethodTestDescriptor.java:243)
Sep 14 22:34:21 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$13(TestMethodTestDescriptor.java:276)
Sep 14 22:34:21 		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Sep 14 22:34:21 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$14(TestMethodTestDescriptor.java:276)
Sep 14 22:34:21 		at java.util.ArrayList.forEach(ArrayList.java:1259)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-34405,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 03 22:35:12 UTC 2023,,,,,,,,,,"0|z1kg6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARNSessionFIFOITCase.checkForProhibitedLogContents:  Interrupted waiting to send RPC request to server,FLINK-33113,13551197,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,19/Sep/23 08:31,04/Oct/23 09:38,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,test-stability,,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53197&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=27683

fails as 

{code}
Sep 14 05:52:35 2023-09-14 05:51:31,456 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeat
Sep 14 05:52:35 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Sep 14 05:52:35 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Sep 14 05:52:35 	at org.apache.hadoop.ipc.Client.call(Client.java:1461) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.ipc.Client.call(Client.java:1403) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]
Sep 14 05:52:35 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-2.10.2.jar:?]
Sep 14 05:52:35 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Sep 14 05:52:35 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Sep 14 05:52:35 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Sep 14 05:52:35 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Sep 14 05:52:35 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]
Sep 14 05:52:35 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:297) ~[hadoop-yarn-client-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:274) [hadoop-yarn-client-2.10.2.jar:?]
Sep 14 05:52:35 Caused by: java.lang.InterruptedException
Sep 14 05:52:35 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Sep 14 05:52:35 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Sep 14 05:52:35 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1177) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	at org.apache.hadoop.ipc.Client.call(Client.java:1456) ~[hadoop-common-2.10.2.jar:?]
Sep 14 05:52:35 	... 17 more
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-19 08:31:30.0,,,,,,,,,,"0|z1kg54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support placement constraint,FLINK-33112,13551174,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,zuston,zuston,19/Sep/23 03:48,23/Oct/23 03:47,04/Jun/24 20:40,,,,,,,,,,,,,,,,Deployment / YARN,,,,,,0,,,,,,"Yarn placement constraint is introduced in hadoop3.2.0 , which is useful for specify affinity or anti-affinity or colocation with K8s",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 23 03:47:16 UTC 2023,,,,,,,,,,"0|z1kg00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/23 02:48;zuston;PTAL [~guoyangze] ;;;","09/Oct/23 06:02;guoyangze;Not quite familiar with the placement constraint mechanism of YARN. Could you elaborate more on your proposal?;;;","23/Oct/23 03:47;zuston;[https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/PlacementConstraints.html] The detailed info is here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Pulsar Connector to Pulsar Client Version Mismatch,FLINK-33111,13551163,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,longtimer,longtimer,19/Sep/23 02:14,20/Sep/23 14:11,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,,,"In the documentation for the Flink Pulsar Connector, ([https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/pulsar/]) it indicates that 2.10.0 and above versions of the pulsar client are supported ""You can use the connector with the Pulsar 2.10.0 or higher"" and the pom file entry references the 4.0.0-1.17 version of the connector which points to the 2.11.0 version of the Pulsar client. However, when using Pulsar Client 2.10.4 or 2.10.5, the following error is generated:
 
java.lang.NoSuchMethodError: 'org.apache.pulsar.client.api.ClientBuilder org.apache.pulsar.client.api.ClientBuilder.connectionMaxIdleSeconds(int)'
    at org.apache.flink.connector.pulsar.common.config.PulsarClientFactory.createClient(PulsarClientFactory.java:127)
    at org.apache.flink.connector.pulsar.source.reader.PulsarSourceReader.create(PulsarSourceReader.java:266)
    at org.apache.flink.connector.pulsar.source.PulsarSource.createReader(PulsarSource.java:137)
    at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:312)
    at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:93)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Thread.java:829)
 
The referenced method 'connectionMaxIdleSeconds' is only available in the Pulsar 2.11 client when looking at the source code. I am not sure whether the documentation is wrong and the Flink Pulsar Connector 2.11 is the intended Pulsar version. However, my understanding is that Pulsar 2.11 is targeted toward java 17. This would create the need for mixed Java 11 and Java 17 deployment unless the Pulsar client code is compiled for 2.11.
 
Documentation cleanup and a reference to the appropriate Java versions is needed. A fix to the 1.17.1 Flink pulsar connector may alternatively be required.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 20 14:11:36 UTC 2023,,,,,,,,,,"0|z1kfxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 12:30;martijnvisser;[~Tison] WDYT?;;;","20/Sep/23 14:11;tison;We upgrade the Pulsar client version in https://github.com/apache/flink-connector-pulsar/pull/25 which brings benefits for the new version.

Perhaps we should update the document and people who use Pulsar 2.10.x can use 3.x connector.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Array content gets replaced with last element duplicates,FLINK-33110,13551104,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,romanlupiichuk,romanlupiichuk,18/Sep/23 15:28,20/Sep/23 12:36,04/Jun/24 20:40,,1.15.4,,,,,,,,,,,,,,,,,,,,0,,,,,,"After upgrade from Flink 1.14.0 to 1.15.4 a bunch of our tests started to fail.

I've striped down one of the failing test to following (it's in Kotlin)
{code:java}
import org.apache.flink.configuration.Configuration
import org.apache.flink.table.annotation.DataTypeHint
import org.apache.flink.table.annotation.FunctionHint
import org.apache.flink.table.annotation.InputGroup
import org.apache.flink.table.api.TableEnvironment
import org.apache.flink.table.functions.ScalarFunction
import org.apache.flink.table.planner.factories.TestValuesTableFactory
import org.apache.flink.types.Row
import org.junit.jupiter.api.Test

@FunctionHint(output = DataTypeHint(""ARRAY<ROW<fieldName STRING NOT NULL>>""))
object TestArrayFunc : ScalarFunction() {
    fun eval(@DataTypeHint(inputGroup = InputGroup.ANY) vararg values: Any): Array<Row> =
        values
            .map { data ->
                val casted = data as Map<String, String?>
                Row.of(casted[""fieldName""])
            }
            .toTypedArray()
}

class ArrayFieldTest {
    @Test
    fun test() {
        val tableEnv = TableEnvironment.create(
            Configuration().also {
                it.setString(""table.exec.resource.default-parallelism"", ""1"")
            },
        )
        tableEnv.createTemporarySystemFunction(""TO_FIELDS_ARRAY"", TestArrayFunc)

        val dataId = TestValuesTableFactory.registerData(
            listOf(
                TestValuesTableFactory.changelogRow(
                    ""+I"",
                    ""123""
                )
            )
        )
        tableEnv.executeSql(
            """"""
                CREATE TABLE events
                (
                    id STRING
                ) WITH (
                    'connector' = 'values',
                    'data-id' = '$dataId'
                )
            """"""
        )
        tableEnv.executeSql(
            """"""
                CREATE TABLE results
                (
                    fields ARRAY<ROW<fieldName STRING>>,
                    event_time TIMESTAMP
                ) WITH (
                    'connector' = 'print'
                )
            """"""
        )

        tableEnv.executeSql(
            """"""
                INSERT INTO results (fields, event_time)
                SELECT
                    TO_FIELDS_ARRAY(
                       MAP['fieldName', 'foo'],
                       MAP['fieldName', 'hello']
                    ),
                    NOW()
                FROM events
            """"""
        )
    }
}
 {code}
In Flink 1.14.0 it produces
{code:java}
+I[[+I[foo], +I[hello]], 2023-09-18T08:18:55.278]{code}
That's correct and expected output.

But in Flink 1.15.4 the output is
{code:java}
+I[[+I[hello], +I[hello]], 2023-09-18T08:21:12.569]{code}
As one can see all elements in the array were replaced with the last element duplicates.

The issue goes away if I
 # either remove NOT NULL constraint from function hint
 # or remove TIMESTAMP field from the sink table

There is also no issue in regular Flink cluster, only in MiniCluster which is used in testing.",,,,,,,,,,,,,,,,,,,,,,FLINK-28591,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,,Wed Sep 20 12:36:30 UTC 2023,,,,,,,,,,"0|z1kfkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 12:36;martijnvisser;[~romanlupiichuk] Can you please verify if this issue still exists with Flink 1.17?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Watermark alignment not applied after recovery from checkpoint,FLINK-33109,13551077,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,YordanPavlov,YordanPavlov,18/Sep/23 12:49,16/May/24 01:38,04/Jun/24 20:40,16/May/24 01:38,1.17.1,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"I am observing a problem where after recovery from a checkpoint the Kafka source watermarks would start to diverge not honoring the watermark alignment setting I have applied.

I have a Kafka source which reads a topic with 32 partitions. I am applying the following watermark strategy:


{code:java}
new EventAwareWatermarkStrategy[KeyedKafkaSourceMessage]](msg => msg.value.getTimestamp)
      .withWatermarkAlignment(""alignment-sources-group"", time.Duration.ofMillis(sourceWatermarkAlignmentBlocks)){code}
 

This works great up until my job needs to recover from checkpoint. Once the recovery takes place, no alignment is taking place any more. This can best be illustrated by looking at the watermark metrics for various operators in the image:

!image-2023-09-18-15-40-06-868.png!

 

You can see how the watermarks disperse after the recovery. Trying to debug the problem I noticed that before the failure there would be calls in

 
{code:java}
SourceCoordinator::announceCombinedWatermark() 
{code}
after the recovery, no calls get there, so no value for 
{code:java}
watermarkAlignmentParams.getMaxAllowedWatermarkDrift(){code}
is ever read. I can manually fix the problem If I stop the job, clear all state from Zookeeper and then manually start Flink providing the last checkpoint with 
{code:java}
'–fromSavepoint'{code}
 flag. This would cause the SourceCoordinator to be constructed properly and watermark drift to be checked. Once recovery manually watermarks would again converge to the allowed drift as seen in the metrics:

!image-2023-09-18-15-46-16-106.png!

 

Let me know If I can be helpful by providing any more information.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/23 15:43;YordanPavlov;WatermarkTest-1.scala;https://issues.apache.org/jira/secure/attachment/13063390/WatermarkTest-1.scala","18/Sep/23 12:40;YordanPavlov;image-2023-09-18-15-40-06-868.png;https://issues.apache.org/jira/secure/attachment/13062992/image-2023-09-18-15-40-06-868.png","18/Sep/23 12:46;YordanPavlov;image-2023-09-18-15-46-16-106.png;https://issues.apache.org/jira/secure/attachment/13062991/image-2023-09-18-15-46-16-106.png",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 16 01:38:09 UTC 2024,,,,,,,,,,"0|z1kfeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 14:16;fanrui;Hi [~YordanPavlov] , thanks for your report. I'm not sure whether this bug has been fixed. Could you try the 1.17.2 or 1.18.0? Note: these 2 versions aren't released so far. You need to try it locally or build a version by yourself.

I want you try the 1.17.2 or 1.18.0, because we have fixed a series of bugs over the past few months, you can get detailed information from FLINK-32548.

If your job doesn't work well with the latest code, it may be a new bug. Would you like to fix it? If yes, I can assign this ticket to you and I can help you review. If no, I'm be happy to analysis this bug and fix it.

 

BTW, FLINK-32548 (Flink-1.18) marks the watermark alignment ready for production use, so I hope it can be fixed asap if it's indeed a bug. It's better to fix this bug before 1.18.0 is released, thanks :);;;","18/Sep/23 16:22;YordanPavlov;I'll give a try the 1.17.2 and 1.18.0 codebase and come back to you;;;","20/Sep/23 06:26;YordanPavlov;Hi [~fanrui] can I ask for your help in testing the latest builds. I have modified the Dockerfile found at:

https://github.com/apache/flink-docker/blob/master/1.17/scala_2.12-java11-ubuntu/Dockerfile

so that it builds from source code. The branch I am using is `release-1.17`. Here is the full Dockerfile I am using, in it I am also applying my user Flink job code.
[https://gist.github.com/YordanPavlov/b9d2f08370dadb5ab18a2dc096b21481]

 

However, switching a running job (from checkpoint) from the official Flink 1.17.1 image to the one I've built from source gives me the error:


{code:java}
from INITIALIZING to FAILED on 10.42.209.30:44171-33acd8 @ 10-42-209-30.xrp-extractor-v4-flink-taskmanager-7.flink.svc.cluster.local (dataPort=46037).
java.lang.Exception: Exception while creating StreamOperatorStateContext.
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:256)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:256)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:734)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:709)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for WindowOperator_439a9d1f894a5a8c69e615cc793a5c7b_(11/32) from any of the 1 provided restore options.
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:160)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
    ... 11 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:407)
    at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:512)
    at org.apache.flink.contrib.streaming.state.EmbeddedRocksDBStateBackend.createKeyedStateBackend(EmbeddedRocksDBStateBackend.java:99)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
    ... 13 common frames omitted
Caused by: java.lang.NoSuchMethodError: 'java.util.List org.apache.flink.runtime.state.IncrementalKeyedStateHandle.getSharedStateHandles()'
    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restorePreviousIncrementalFilesStatus(RocksDBIncrementalRestoreOperation.java:212)
    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restoreWithoutRescaling(RocksDBIncrementalRestoreOperation.java:188)
    at org.apache.flink.contrib.streaming.state.restore.RocksDBIncrementalRestoreOperation.restore(RocksDBIncrementalRestoreOperation.java:169)
    at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:327){code}
Can you hint me in what I might be doing wrong, thanks!;;;","20/Sep/23 14:05;fanrui;Could you try this image? It's the offcial image based on release-1.17 branch.

[https://github.com/apache/flink-docker/pkgs/container/flink-docker/128728529?tag=1.17-SNAPSHOT-scala_2.12-java11-debian]

Also, would you mind sharing a flink job demo that can reproduce your bug? If so, I can try it with the latest code.;;;","06/Oct/23 15:39;YordanPavlov;Hi [~fanrui] 

sorry for the delay, running this image gives me the following error, shortly after start
{code:java}
2023-10-06 14:54:20.725 [AsyncOperations-thread-1] INFO  o.a.flink.streaming.runtime.tasks.AsyncCheckpointRunnable  - TumblingEventTimeWindows (10/32)#0 - asynchronous part of checkpoint 1 could not be completed.
java.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: 'void org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.<init>(java.util.UUID, org.apache.flink.runtime.state.KeyGroupRange, long, java.util.List, java.util.List, org.apache.flink.runtime.state.StreamStateHandle, long)'
    at java.base/java.util.concurrent.FutureTask.report(Unknown Source)
    at java.base/java.util.concurrent.FutureTask.get(Unknown Source)
    at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:544)
    at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:54)
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191)
    at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.NoSuchMethodError: 'void org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.<init>(java.util.UUID, org.apache.flink.runtime.state.KeyGroupRange, long, java.util.List, java.util.List, org.apache.flink.runtime.state.StreamStateHandle, long)'
    at org.apache.flink.contrib.streaming.state.snapshot.RocksNativeFullSnapshotStrategy$RocksDBNativeFullSnapshotOperation.get(RocksNativeFullSnapshotStrategy.java:198)
    at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:91)
    at org.apache.flink.runtime.state.SnapshotStrategyRunner$1.callInternal(SnapshotStrategyRunner.java:88)
    at org.apache.flink.runtime.state.AsyncSnapshotCallable.call(AsyncSnapshotCallable.java:78)
    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
    at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:541)
    ... 6 common frames omitted{code}
so I couldn't test if has the problem I am facing.

Attached you can find a simple program which would exhibit the problem with the latest stable image 1.17.1:  [^WatermarkTest-1.scala]
what I would observe is that different sources would converge shortly after start having a small watermark alignment value. However if I kill the task manager and restart from a checkpoint they would start to diverge as if no watermark alignment is present.;;;","08/Oct/23 04:28;fanrui;Hi [~YordanPavlov] , I guess this bug has been fixed in 1.17.2 and 1.18.

I have tested it with a watermark alignment job[1], the job will fails after some times during snapshot, after failover:
 * Watermark alignment doesn't work well with flink-1.17.1
 * Watermark alignment works well with flink-master code

{quote}java.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: 'void org.apache.flink.runtime.state.IncrementalRemoteKeyedStateHandle.<init>(java.util.UUID, org.apache.flink.runtime.state.KeyGroupRange, long, java.util.List, java.util.List, org.apache.flink.runtime.state.StreamStateHandle, long)'
{quote}
About your exception, it's caused by maven dependency conflict. I guess you includes some flink code into your jar, the flink version of your jar isn't same with the flink image. You can try to provided them, and try again.

[1]https://github.com/1996fanrui/fanrui-learning/blob/29e363f3d327fee159bd176ca6669dc1453ac7bc/module-flink/src/main/java/com/dream/flink/kafka/alignment/KafkaAlignmentDemo.java#L20;;;","15/May/24 11:50;pnowojski;Hi!

{quote}
I guess this bug has been fixed in 1.17.2 and 1.18.
{quote}

Has this been confirmed?  [~YordanPavlov], has this issue been fixed for you?;;;","15/May/24 20:29;YordanPavlov;Hi [~pnowojski] , yes we do not observe the problem in 1.18;;;","16/May/24 01:38;fanrui;Thanks [~YordanPavlov] for the feedback, let me close this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error during error status handling,FLINK-33108,13551046,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,gaborgsomogyi,gaborgsomogyi,18/Sep/23 10:02,19/Sep/23 08:31,04/Jun/24 20:40,,,,,,,,,,,,,,,,Kubernetes Operator,,,,,,0,,,,,,"e2e_ci (v1_13, flink, native, test_multi_sessionjob.sh) failed with the following issue:
{code:java}
Error: m2023-09-18 08:26:41,813 i.j.o.p.e.ReconciliationDispatcher [ERROR][flink/session-cluster-1] Error during error status handling.
org.apache.flink.kubernetes.operator.exception.StatusConflictException: Status have been modified externally in version 1374 Previous: {""jobStatus"":{""jobName"":null,""jobId"":null,""state"":null,""startTime"":null,""updateTime"":null,""savepointInfo"":{""lastSavepoint"":null,""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[],""lastPeriodicSavepointTimestamp"":0},""checkpointInfo"":{""lastCheckpoint"":null,""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""lastPeriodicCheckpointTimestamp"":0}},""error"":null,""lifecycleState"":""STABLE"",""clusterInfo"":{""total-cpu"":""0.25"",""flink-version"":""1.13.6"",""flink-revision"":""b2ca390 @ 2022-02-03T14:54:22+01:00"",""total-memory"":""1073741824""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1695025410957,""lastReconciledSpec"":""{\""spec\"":{\""job\"":null,\""restartNonce\"":null,\""flinkConfiguration\"":{\""high-availability\"":\""org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\"",\""high-availability.storageDir\"":\""file:///opt/flink/volume/flink-ha\"",\""state.checkpoints.dir\"":\""file:///opt/flink/volume/flink-cp\"",\""state.savepoints.dir\"":\""file:///opt/flink/volume/flink-sp\"",\""taskmanager.numberOfTaskSlots\"":\""2\""},\""image\"":\""flink:1.13\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""flink\"",\""flinkVersion\"":\""v1_13\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""name\"":\""pod-template\""},\""spec\"":{\""containers\"":[{\""name\"":\""flink-main-container\"",\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""2048Mi\""},\""requests\"":{\""ephemeral-storage\"":\""2048Mi\""}},\""volumeMounts\"":[{\""mountPath\"":\""/opt/flink/volume\"",\""name\"":\""flink-volume\""}]}],\""volumes\"":[{\""name\"":\""flink-volume\"",\""persistentVolumeClaim\"":{\""claimName\"":\""session-cluster-1-pvc\""}}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":\""native\""},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":2},\""firstDeployment\"":true}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":null,\""restartNonce\"":null,\""flinkConfiguration\"":{\""high-availability\"":\""org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\"",\""high-availability.storageDir\"":\""file:///opt/flink/volume/flink-ha\"",\""state.checkpoints.dir\"":\""file:///opt/flink/volume/flink-cp\"",\""state.savepoints.dir\"":\""file:///opt/flink/volume/flink-sp\"",\""taskmanager.numberOfTaskSlots\"":\""2\""},\""image\"":\""flink:1.13\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""flink\"",\""flinkVersion\"":\""v1_13\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""name\"":\""pod-template\""},\""spec\"":{\""containers\"":[{\""name\"":\""flink-main-container\"",\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""2048Mi\""},\""requests\"":{\""ephemeral-storage\"":\""2048Mi\""}},\""volumeMounts\"":[{\""mountPath\"":\""/opt/flink/volume\"",\""name\"":\""flink-volume\""}]}],\""volumes\"":[{\""name\"":\""flink-volume\"",\""persistentVolumeClaim\"":{\""claimName\"":\""session-cluster-1-pvc\""}}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":\""native\""},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":2},\""firstDeployment\"":true}}"",""state"":""DEPLOYED""},""taskManager"":null} Latest: {""jobStatus"":{""jobName"":null,""jobId"":null,""state"":null,""startTime"":null,""updateTime"":null,""savepointInfo"":{""lastSavepoint"":null,""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""savepointHistory"":[],""lastPeriodicSavepointTimestamp"":0},""checkpointInfo"":{""lastCheckpoint"":null,""triggerId"":null,""triggerTimestamp"":null,""triggerType"":null,""formatType"":null,""lastPeriodicCheckpointTimestamp"":0}},""error"":null,""lifecycleState"":""STABLE"",""clusterInfo"":{""flink-revision"":""b2ca390 @ 2022-02-03T14:54:22+01:00"",""flink-version"":""1.13.6"",""total-cpu"":""0.5"",""total-memory"":""2147483648""},""jobManagerDeploymentStatus"":""READY"",""reconciliationStatus"":{""reconciliationTimestamp"":1695025410957,""lastReconciledSpec"":""{\""spec\"":{\""job\"":null,\""restartNonce\"":null,\""flinkConfiguration\"":{\""high-availability\"":\""org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\"",\""high-availability.storageDir\"":\""file:///opt/flink/volume/flink-ha\"",\""state.checkpoints.dir\"":\""file:///opt/flink/volume/flink-cp\"",\""state.savepoints.dir\"":\""file:///opt/flink/volume/flink-sp\"",\""taskmanager.numberOfTaskSlots\"":\""2\""},\""image\"":\""flink:1.13\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""flink\"",\""flinkVersion\"":\""v1_13\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""name\"":\""pod-template\""},\""spec\"":{\""containers\"":[{\""name\"":\""flink-main-container\"",\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""2048Mi\""},\""requests\"":{\""ephemeral-storage\"":\""2048Mi\""}},\""volumeMounts\"":[{\""mountPath\"":\""/opt/flink/volume\"",\""name\"":\""flink-volume\""}]}],\""volumes\"":[{\""name\"":\""flink-volume\"",\""persistentVolumeClaim\"":{\""claimName\"":\""session-cluster-1-pvc\""}}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":\""native\""},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":2},\""firstDeployment\"":true}}"",""lastStableSpec"":""{\""spec\"":{\""job\"":null,\""restartNonce\"":null,\""flinkConfiguration\"":{\""high-availability\"":\""org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\"",\""high-availability.storageDir\"":\""file:///opt/flink/volume/flink-ha\"",\""state.checkpoints.dir\"":\""file:///opt/flink/volume/flink-cp\"",\""state.savepoints.dir\"":\""file:///opt/flink/volume/flink-sp\"",\""taskmanager.numberOfTaskSlots\"":\""2\""},\""image\"":\""flink:1.13\"",\""imagePullPolicy\"":null,\""serviceAccount\"":\""flink\"",\""flinkVersion\"":\""v1_13\"",\""ingress\"":{\""template\"":\""/{{namespace}}/{{name}}(/|$)(.*)\"",\""className\"":\""nginx\"",\""annotations\"":{\""nginx.ingress.kubernetes.io/rewrite-target\"":\""/$2\""}},\""podTemplate\"":{\""apiVersion\"":\""v1\"",\""kind\"":\""Pod\"",\""metadata\"":{\""name\"":\""pod-template\""},\""spec\"":{\""containers\"":[{\""name\"":\""flink-main-container\"",\""resources\"":{\""limits\"":{\""ephemeral-storage\"":\""2048Mi\""},\""requests\"":{\""ephemeral-storage\"":\""2048Mi\""}},\""volumeMounts\"":[{\""mountPath\"":\""/opt/flink/volume\"",\""name\"":\""flink-volume\""}]}],\""volumes\"":[{\""name\"":\""flink-volume\"",\""persistentVolumeClaim\"":{\""claimName\"":\""session-cluster-1-pvc\""}}]}},\""jobManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":1,\""podTemplate\"":null},\""taskManager\"":{\""resource\"":{\""cpu\"":0.25,\""memory\"":\""1024m\"",\""ephemeralStorage\"":null},\""replicas\"":null,\""podTemplate\"":null},\""logConfiguration\"":null,\""mode\"":\""native\""},\""resource_metadata\"":{\""apiVersion\"":\""flink.apache.org/v1beta1\"",\""metadata\"":{\""generation\"":2},\""firstDeployment\"":true}}"",""state"":""DEPLOYED""},""taskManager"":null}
{code}
Link: https://github.com/apache/flink-kubernetes-operator/actions/runs/6219937225/job/16879006709?pr=676#logs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 08:31:48 UTC 2023,,,,,,,,,,"0|z1kf7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 10:07;gaborgsomogyi;After some digging the following value changed from:
{code:java}
    ""clusterInfo"": {
        ""flink-revision"": ""b2ca390 @ 2022-02-03T14:54:22+01:00"",
        ""flink-version"": ""1.13.6"",
        ""total-cpu"": ""0.25"",
        ""total-memory"": ""1073741824""
    },
{code}
to:
{code:java}
    ""clusterInfo"": {
        ""flink-revision"": ""b2ca390 @ 2022-02-03T14:54:22+01:00"",
        ""flink-version"": ""1.13.6"",
        ""total-cpu"": ""0.5"",
        ""total-memory"": ""2147483648""
    },
{code}
;;;","18/Sep/23 10:11;gaborgsomogyi;The issue comes only occasionally and not consistent so seems like a race.;;;","18/Sep/23 11:25;gaborgsomogyi;I'm not telling that this causes the issue but the problematic fields has been added here: https://github.com/apache/flink-kubernetes-operator/pull/558;;;","18/Sep/23 12:18;mbalassi;The total resources used in the clusterInfo of the status is updated by the operator in the reconcile loop. We are relying on this for application mode metrics.

I suspect that the issue is that the two session jobs are adding now resources (taskmanagers) concurrently and this leads to this issue. One possible solution that we could consider is simply removing these from the session cluster and only keeping them at the individual job level. Otherwise we need to solve for the concurrency of these updates.;;;","19/Sep/23 08:31;gaborgsomogyi;After quite some investment I still don't see what is actually doing the race. I would say it would be good to keep this feature at the individual job level for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update stable-spec upgrade mode on reconciled-spec change,FLINK-33107,13551042,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gyfora,gyfora,gyfora,18/Sep/23 09:30,25/Sep/23 07:13,04/Jun/24 20:40,25/Sep/23 07:13,kubernetes-operator-1.6.0,kubernetes-operator-1.7.0,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Since now the rollback mechanism uses the regular upgrade flow, we need to ensure that the lastStableSpec upgrade mode is kept in sync with the lastReconciled spec to ensure correct stateful upgrades.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 25 07:13:16 UTC 2023,,,,,,,,,,"0|z1kf6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/23 07:13;gyfora;merged to main 662fa612a8ab352e43ab8a99fa61aadfbe41e4d7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyflink脚本通过python命令运行正常，但部署至Flink（单机）运行时报错,FLINK-33106,13551040,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Invalid,,palosky,palosky,18/Sep/23 09:26,18/Sep/23 12:33,04/Jun/24 20:40,18/Sep/23 12:33,1.17.0,,,,,,,,,,,,,,API / Python,Connectors / FileSystem,FileSystems,,,,0,easyfix,,,,,"尊敬的开发老师，你们好。最近遇到一个问题，经搜索资料，仍无法解决，因此在这里反馈。

 

【问题描述】：

我用pyflink写了一个demo程序（参考附件《csv_batch_source_csv.py》），读取一个csv并打print。这个程序用python来运行是正常的，参考附件《python_cmd.txt》，但是通过flink run提交至flink环境执行时却报错，无法正确执行，参考附件《flink_cmd.txt》。

 

我怀疑是一个bug，但尚不确定，请老师评估，谢谢。

 

 ",Flink 1.17.0（单机），Python 3.9.10。,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/23 09:22;palosky;csv_batch_source_csv.py;https://issues.apache.org/jira/secure/attachment/13062987/csv_batch_source_csv.py","18/Sep/23 09:29;palosky;flink_cmd.txt;https://issues.apache.org/jira/secure/attachment/13062988/flink_cmd.txt","18/Sep/23 09:22;palosky;python_cmd.txt;https://issues.apache.org/jira/secure/attachment/13062985/python_cmd.txt",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Java,Python,Mon Sep 18 12:33:47 UTC 2023,,,,,,,,,,"0|z1kf68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 12:33;martijnvisser;[~palosky] Please translate this ticket to English, so it can be re-opened ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate operator error log entry: Exception while fetching cluster info,FLINK-33105,13551023,13505006,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,18/Sep/23 07:56,19/Sep/23 09:07,04/Jun/24 20:40,19/Sep/23 09:04,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 09:04:01 UTC 2023,,,,,,,,,,"0|z1kf2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 09:04;gaborgsomogyi;3211549 on main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nightly run for Flink Kafka connector fails due to architecture tests failing,FLINK-33104,13551009,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Duplicate,,martijnvisser,martijnvisser,18/Sep/23 06:50,09/Oct/23 13:33,04/Jun/24 20:40,09/Oct/23 13:33,kafka-3.1.0,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,,,,,,"{code:java}
2023-09-17T00:29:07.1675694Z [WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 9, Time elapsed: 308.532 s - in org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerMigrationTest
2023-09-17T00:29:07.5171608Z [INFO] 
2023-09-17T00:29:07.5172360Z [INFO] Results:
2023-09-17T00:29:07.5172773Z [INFO] 
2023-09-17T00:29:07.5173139Z [ERROR] Failures: 
2023-09-17T00:29:07.5174181Z [ERROR]   Architecture Violation [Priority: MEDIUM] - Rule 'ITCASE tests should use a MiniCluster resource or extension' was violated (13 times):
2023-09-17T00:29:07.5176050Z org.apache.flink.connector.kafka.sink.FlinkKafkaInternalProducerITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5177452Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5179831Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5181277Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5182154Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5182951Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5183906Z org.apache.flink.connector.kafka.sink.KafkaSinkITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5184769Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5185812Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5186880Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5187929Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5189073Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5190076Z org.apache.flink.connector.kafka.sink.KafkaTransactionLogITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5190946Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5191983Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5192845Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5193532Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5194300Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5195091Z org.apache.flink.connector.kafka.sink.KafkaWriterITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5195938Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5197014Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5197882Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5198708Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5199526Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5200429Z org.apache.flink.connector.kafka.source.KafkaSourceITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5201301Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5202422Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5203294Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5203980Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5204750Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5205567Z org.apache.flink.connector.kafka.source.KafkaSourceLegacyITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5206438Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5207478Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5208345Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5209438Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5210190Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5211069Z org.apache.flink.streaming.connectors.kafka.FlinkKafkaInternalProducerITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5211998Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5213039Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5213907Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5214592Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5215353Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5216499Z org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5217439Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5218537Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5219405Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5220147Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5220897Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5221665Z org.apache.flink.streaming.connectors.kafka.KafkaITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5222482Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5223483Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5224321Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5224987Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5225735Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5226572Z org.apache.flink.streaming.connectors.kafka.KafkaProducerAtLeastOnceITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5227455Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5228461Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5229301Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5229947Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5244058Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5244923Z org.apache.flink.streaming.connectors.kafka.KafkaProducerExactlyOnceITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5245874Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5247073Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5247979Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5251868Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5252843Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5254009Z org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleExactlyOnceITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5255758Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5257289Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5258312Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5259141Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5260667Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5262234Z org.apache.flink.streaming.connectors.kafka.shuffle.KafkaShuffleITCase does not satisfy: only one of the following predicates match:
2023-09-17T00:29:07.5265345Z * reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
2023-09-17T00:29:07.5267243Z * reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
2023-09-17T00:29:07.5269228Z * reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
2023-09-17T00:29:07.5272222Z * reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
2023-09-17T00:29:07.5274887Z  or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
2023-09-17T00:29:07.5278016Z [INFO] 
2023-09-17T00:29:07.5278347Z [ERROR] Tests run: 386, Failures: 1, Errors: 0, Skipped: 31
{code}

https://github.com/apache/flink-connector-kafka/actions/runs/6210384981/job/16858550802",,,,,,,,,,,,,,,,,,,,,,FLINK-33219,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 06 10:33:08 UTC 2023,,,,,,,,,,"0|z1kezc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 10:43;jiabao.sun;Hi [~martijnvisser].
Could you help assign this ticket to me?;;;","20/Sep/23 18:53;mason6345;Do we want to ignore the violations for now? or do we want to migrate tests to JUnit5? I can help do some of the refactoring.

Moreover, it seems like any arch unit rule addition can easily break builds. I don't know if there is anything to make this situation more stable since we rely on the SNAPSHOT artifacts;;;","28/Sep/23 18:38;mason6345;[~martijnvisser] do we need any help with updating the particular unit tests to unblock this? I have some bandwidth. It seems arch test error report is valid and affects important tests so I wouldn't pursue an alternative method to ignore the errors

cc [~tzulitai] ;;;","28/Sep/23 23:53;martijnvisser;[~mason6345] I don't think we have to update to JUnit5 in order to get this fixed. IIRC, we had a similar problem with the Cassandra connector on this topic and we managed to get it fixed there too. It's more because Flink 1.18 has an updated version of Archunit, while the rules that we have are still build for a lower version of Archunit in general. [~echauchot] do you recall this situation we had with the Cassandra connector?;;;","29/Sep/23 08:48;echauchot;[~martijnvisser] actually it was never fixed in cassandra (see FLINK-32353): the problem is that when archunit rules change we need to update the archunit violation store. And there is only a single violation store. As the nightly tests the connector against several versions of flink, there will be failures if not all these versions have the same archunit rules. To fix this problem we need to skip archunit tests on Flink versions that are not the main one (the one the connector is built against) so that the single arunit violation store contains the violations for the main flink version. I proposed some change in the github action script [here|https://issues.apache.org/jira/browse/FLINK-32563]  for which I was waiting for your feedback.;;;","05/Oct/23 21:10;tzulitai;[~echauchot] this seems to be the conclusion, correct?: [https://lists.apache.org/thread/pr0g812olzpgz21d9oodhc46db9jpxo3]

If that's the case, I'll do the same for the Kafka connector as well then. i.e.
 * for now only run archunit tests when building against 1.17.1
 * once 1.18 drops, update the violation store and switch to run archunit tests only when building against 1.18.0;;;","06/Oct/23 10:33;echauchot;[~tzulitai] yes this is exactly what I wanted to do for Cassandra but it requires a change in the github actions scripts and/or on main build proposed [here|https://issues.apache.org/jira/browse/FLINK-32563] but I received no feedback. Among the 3 proposed changes in that ticket, I prefer the one that links the dep convergence tests and the archunit tests into a single maven profile (like architecture tests). I'll do this change in a PR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hybrid shuffle ITCase supports the new mode,FLINK-33103,13551005,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,18/Sep/23 06:24,19/Sep/23 06:35,04/Jun/24 20:40,19/Sep/23 06:35,1.18.1,,,,,,,,,,1.19.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"Currently, the Hybrid shuffle ITCase only supports the legacy mode. The new mode should also be verified, so we should improve it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 06:35:19 UTC 2023,,,,,,,,,,"0|z1keyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 06:35;Weijie Guo;master(1.19) via cc33ce175c1b3953577cd26e2416cb2eef94e176.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document the standalone autoscaler,FLINK-33102,13550991,13545642,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,18/Sep/23 02:59,10/Nov/23 08:06,04/Jun/24 20:40,10/Nov/23 08:06,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:06:18 UTC 2023,,,,,,,,,,"0|z1kevc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 08:06;gyfora;merged to main e36819820627dedaca66d33f85687166ac829395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the integration test for standalone autoscaler,FLINK-33101,13550990,13556703,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,samrat007,fanrui,fanrui,18/Sep/23 02:59,03/Nov/23 14:02,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-18 02:59:00.0,,,,,,,,,,"0|z1kev4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement YarnJobFetcher for Standalone Autoscaler,FLINK-33100,13550988,13556703,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,18/Sep/23 02:54,04/Jan/24 05:14,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-18 02:54:58.0,,,,,,,,,,"0|z1keuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the Standalone Autoscaler and support flink cluster,FLINK-33099,13550987,13545642,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,18/Sep/23 02:53,07/Nov/23 03:07,04/Jun/24 20:40,07/Nov/23 03:07,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 03:02:05 UTC 2023,,,,,,,,,,"0|z1keug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/23 03:02;fanrui;Merged master<1.7.0> via : a309af647c1eab5bd3ae0b4f056e53a43d01b607 and c9dde4e76e63acec611f84bbe1421d8001a15da5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support kubernetes autoscaler using generic interface,FLINK-33098,13550986,13545642,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,18/Sep/23 02:52,17/Oct/23 15:35,04/Jun/24 20:40,17/Oct/23 14:28,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,,,,,0,,,,,," # Moving all classes aren't related to kubernetes to flink-autoscaler module
 # Support kubernetes autoscaler using generic interface (The most important commit)
 ## Migrate all generic classes from flink-kubernetes-operator-autoscaler module to autoscaler moudle
 ## Migrate all kubernetes implementation classes from flink-kubernetes-operator-autoscaler module to flink-kubernetes-operator moudle
 ## Support all old tests
 ## Removing the flink-kubernetes-operator-autoscaler module
 # Removing the option prefix(kubernetes.operator.) for all options and update the doc(All old option names are marked with withDeprecatedKeys to ensure the compatibility.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 17 15:35:47 UTC 2023,,,,,,,,,,"0|z1keu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 14:28;gyfora;Merged to main 305498a9ab2e04ab71a4c2d87f2edb746373df1a..b1c40e3578120d5b892a6c80fe0dba8ebca58b6a;;;","17/Oct/23 15:35;fanrui;Thanks for the review and merging!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Initialize the generic autoscaler module and interfaces,FLINK-33097,13550985,13545642,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,18/Sep/23 02:37,13/Nov/23 13:29,04/Jun/24 20:40,17/Oct/23 15:22,,,,,,,,,,,kubernetes-operator-1.7.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 17 15:21:48 UTC 2023,,,,,,,,,,"0|z1keu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Oct/23 15:21;fanrui;Merged main via
5c76d01c79a91a255b40e0f08d384d7b0326bea3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink on k8s，if one taskmanager pod was crashed，the whole flink job will be failed,FLINK-33096,13550969,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,wawa,wawa,17/Sep/23 13:45,21/Sep/23 14:52,04/Jun/24 20:40,,1.14.3,,,,,,,,,,,,,,Deployment / Kubernetes,,,,,,0,,,,,,"The Flink version is 1.14.3, and the job is submitted to Kubernetes using the Native Kubernetes application mode. During the scheduling process, when a TaskManager pod crashes due to an exception, Kubernetes will attempt to start a new TaskManager pod. However, the scheduling process is halted immediately, resulting in the entire Flink job being terminated. On the other hand, if the JobManager pod crashes, Kubernetes is able to successfully schedule a new JobManager pod. This observation was made during application usage. Can you please help analyze the underlying issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,English,,Thu Sep 21 14:52:52 UTC 2023,,,,,,,,,,"0|z1keqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Sep/23 11:27;huweihua;[~wawa] I think it's because the job is failed after some TM pod crashed. You need pick the restart strategy for your job. Please refer to [https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/ops/state/task_failure_recovery/#restart-strategies];;;","19/Sep/23 05:54;wawa;[~huweihua] Restart-strategy has been set. Here are the details:
|*restart-strategy*|fixed-delay|
|*restart-strategy.fixed-delay.attempts*|5|
|*restart-strategy.fixed-delay.delay*|10 s|;;;","21/Sep/23 14:52;wawa;So sorry，correct the previous description: 

 there was an exception during the scheduling of the taskManager, and the restart strategy took effect. As a result, the Duration value was reset and started counting again.

However, due to pod memory overuse, the taskManager pod was killed and evicted by Kubernetes. When trying to schedule a new taskManager pod, it failed, resulting in the failure of the entire Flink job.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job jar related issue should be reported as BAD_REQUEST instead of INTERNAL_SERVER_ERROR,FLINK-33095,13550915,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,surendralilhore,surendralilhore,16/Sep/23 07:49,05/Oct/23 07:47,04/Jun/24 20:40,,1.16.0,,,,,,,,,,,,,,Runtime / REST,,,,,,0,pull-request-available,,,,,"When submitting a job with incorrect parameters, such as an invalid entry class, the current response is an internal server error.

To enhance the user experience and consistency, it is recommended to throw a Rest exception and return a BAD_REQUEST response code in such cases.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/23 07:50;surendralilhore;image-2023-09-16-13-20-15-052.png;https://issues.apache.org/jira/secure/attachment/13062964/image-2023-09-16-13-20-15-052.png","16/Sep/23 07:51;surendralilhore;image-2023-09-16-13-21-24-634.png;https://issues.apache.org/jira/secure/attachment/13062965/image-2023-09-16-13-21-24-634.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Sep 16 07:51:34 UTC 2023,,,,,,,,,,"0|z1keeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/23 07:51;surendralilhore;Current response from server :

!image-2023-09-16-13-20-15-052.png!

==========

It should be something like this :

!image-2023-09-16-13-21-24-634.png!

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kubernetes.decorator.kerberos-mount.enabled does not work,FLINK-33094,13550864,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,seung-min,seung-min,15/Sep/23 15:12,15/Sep/23 16:15,04/Jun/24 20:40,,,,,,,,,,,,,,,,Deployment / Kubernetes,Kubernetes Operator,,,,,0,,,,,,"I deployed _[basic-example|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.5/docs/try-flink-kubernetes-operator/quick-start/]_ job using flink-kubernetes-operator(v1.5.0) and flink (v1.17.1)

I do not need to mount kerberos (keytab), so I set {color:#de350b}_kubernetes.decorator.kerberos-mount.enabled_{color} as _{color:#de350b}false{color}_

And checked that above option set as _{color:#de350b}false{color}_ in log of job manager and task manager.

!image-2023-09-16-00-08-56-204.png|width=1419,height=25!

 

Then as I understand, when {color:#de350b}_kubernetes.decorator.kerberos-mount.enabled_{color} is {_}{color:#de350b}false{color}{_}, it should not create secret.
(https://github.com/apache/flink/blob/2750d5c26b1ff8e3706a7c0d7cb2c8955247f538/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/factory/KubernetesJobManagerFactory.java#L83-L95)

But It create secret {color:#de350b}_kerberos-keytab-basic-example_{color}

!image-2023-09-16-00-12-12-793.png|width=697,height=147!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/23 15:08;seung-min;image-2023-09-16-00-08-56-204.png;https://issues.apache.org/jira/secure/attachment/13062950/image-2023-09-16-00-08-56-204.png","15/Sep/23 15:12;seung-min;image-2023-09-16-00-12-12-793.png;https://issues.apache.org/jira/secure/attachment/13062949/image-2023-09-16-00-12-12-793.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-15 15:12:23.0,,,,,,,,,,"0|z1ke34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SHOW FUNCTIONS throw exception with unset catalog,FLINK-33093,13550850,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,15/Sep/23 14:25,29/Nov/23 12:51,04/Jun/24 20:40,19/Sep/23 10:39,1.18.0,,,,,,,,,,1.19.0,,,,Table SQL / API,,,,,,0,pull-request-available,,,,,"A test like this throw an exception. It should instead return only built-in functions
{code}
    @Test
    public void testUnsetCatalogWithShowFunctions() throws Exception {
        TableEnvironment tEnv = TableEnvironment.create(ENVIRONMENT_SETTINGS);

        tEnv.useCatalog(null);

        TableResult table = tEnv.executeSql(""SHOW FUNCTIONS"");
        final List<Row> functions = CollectionUtil.iteratorToList(table.collect());

        // check it has some built-in functions
        assertThat(functions).hasSizeGreaterThan(0);
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 10:39:16 UTC 2023,,,,,,,,,,"0|z1ke00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 10:39;dwysakowicz;Fixed in 05b0b61c62434c73cd819750c0d56b1070a2b0f2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the resource-stabilization-timeout mechanism when rescale a job for Adaptive Scheduler,FLINK-33092,13550802,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,fanrui,fanrui,fanrui,15/Sep/23 07:19,01/Feb/24 04:57,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,,"!image-2023-09-15-14-43-35-104.png|width=916,height=647!
h1. 1. Propose

The above is the state transition graph when rescale a job in Adaptive Scheduler.

In brief, when we trigger a rescale, the job will wait _*resource-stabilization-timeout*_ in WaitingForResources State when it has sufficient resources and it doesn't have the desired resource.

If the _*resource-stabilization-timeout mechanism*_ is moved into the Executing State, the rescale downtime will be significantly reduced.
h1. 2. Why the downtime is long?

Currently, when rescale a job:
 * The Executing will transition to Restarting
 * The Restarting will cancel this job first.
 * The Restarting will transition to WaitingForResources after the whole job is terminal.
 * When this job has sufficient resources and it doesn't have the desired resource, the WaitingForResources needs to wait  _*resource-stabilization-timeout*_ .
 * WaitingForResources will transition to CreatingExecutionGraph after  resource-stabilization-timeout.

The problem is the job isn't running during the resource-stabilization-timeout phase.
h1. 3. How to reduce the downtime?

We can move the _*resource-stabilization-timeout mechanism*_ into the Executing State when trigger a rescale. It means:
 * When this job has desired resources, the Executing can rescale directly.
 * When this job has sufficient resources and it doesn't have the desired resource, we can rescale after _*resource-stabilization-timeout.*_
 * The WaitingForResources will ignore the resource-stabilization-timeout after this improvement.

The resource-stabilization-timeout works before cancel job, so the rescale downtime will be significantly reduced.

 

Note: the resource-stabilization-timeout still works in WaitingForResources when start a job. It's just changed when rescale a job.",,,,,,,,,,,,,,,,,,,,,,FLINK-34318,,,,,,,,,,,,,,,,,,,,"31/Jan/24 16:43;dmvk;FLINK-FLIP-XXX_ Refining AdaptiveScheduler timeouts and rescale triggering-310124-163851.pdf;https://issues.apache.org/jira/secure/attachment/13066380/FLINK-FLIP-XXX_+Refining+AdaptiveScheduler+timeouts+and+rescale+triggering-310124-163851.pdf","15/Sep/23 06:43;fanrui;image-2023-09-15-14-43-35-104.png;https://issues.apache.org/jira/secure/attachment/13062920/image-2023-09-15-14-43-35-104.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 01 04:57:20 UTC 2024,,,,,,,,,,"0|z1kdpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/23 07:25;fanrui;Hi [~chesnay]  [~dmvk] , would you mind helping take a look this improvement? I can go ahead if you think it makes sense.;;;","31/Jan/24 10:07;gyfora;Sounds like a great idea [~fanrui] ;;;","31/Jan/24 16:18;mxm;+1 waiting on resources in the Executing state.

I think we need to just change the ScalingControler to delay triggering the actual rescale process: [https://github.com/apache/flink/blob/cb9e220c2291088459f0281aa8e8e8584436a9b2/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adaptive/scalingpolicy/RescalingController.java#L37]

Right now, it triggers immediately on parallelism change. [~dmvk] can probably answer this.;;;","31/Jan/24 16:42;dmvk;This is something we've discussed internally as well. I think overall we should take a bigger stab on the whole AdaptiveScheduler configuration to make the whole story more round.

 
 * We can probably get rid of RescalingController (it exists for historical reasons)
 * Redefine how cooldown periods work (they shouldn't restart every time something changes), especially in combination with bringing _resource-stabilization-timeout_ into executing state{_}.{_}
 * Allowing rescaling to wait for next complete checkpoint (this is what triggered us to look into the whole configuration story)

 

I've also already done some work in the direction, it would be great to align on it if you have some time this week (the first steps around fixing the cooldown periods and consolidating AS settings are already merged).

 

 ;;;","31/Jan/24 16:43;dmvk;I've attached some notes I had (not very structured) to the ticket;;;","01/Feb/24 04:57;fanrui;Thank you all for the lively discussion.
{quote} * Redefine how cooldown periods work (they shouldn't restart every time something changes), especially in combination with bringing _resource-stabilization-timeout_ into executing state{_}.{_}{quote}
It make sense, current restart mechanism is a little complex.
{quote} * Allowing rescaling to wait for next complete checkpoint (this is what triggered us to look into the whole configuration story){quote}
Rescaling after a checkpoint is definitely a good idea as well. It can avoid the processing for repetitive data.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit on outgoing connections to 64 seems unnecessary,FLINK-33091,13550756,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,roganmorrowrokt,roganmorrowrokt,14/Sep/23 21:55,14/Sep/23 21:57,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,,,,,,0,,,,,,"We have a job that results in a node having more than 64 outputs. However submitting the job fails with error ""Cannot currently handle nodes with more than 64 outputs."". The error originates from this line:

[https://github.com/apache/flink/blob/e7eeea033a68e1ff6bf82132b5a59eb0a5a2d0ed/flink-optimizer/src/main/java/org/apache/flink/optimizer/dag/OptimizerNode.java#L355]

There is no explanation in the code for why this arbitrary limit is set, so I'm wondering what the purpose of it is.

After forking Flink and removing the above line so that there is no limit, the job works. So it seems that the limit is unnecessary and can be removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-14 21:55:25.0,,,,,,,,,,"0|z1kdf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointsCleaner clean individual checkpoint states in parallel,FLINK-33090,13550727,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yigress,yigress,yigress,14/Sep/23 17:39,25/Oct/23 18:25,04/Jun/24 20:40,25/Oct/23 14:00,1.17.1,,,,,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,"Currently CheckpointsCleaner clean multiple checkpoints in parallel with JobManager's ioExecutor, however each checkpoint states is cleaned sequentially. With thousands of StateObjects to clean this can take long time on some checkpoint storage, if longer than the checkpoint interval this prevents new checkpointing.

The proposal is to use the same ioExecutor to clean up each checkpoints states in parallel as well. From my local testing, with default settings for ioExecutor thread pool for xK state files this can reduce clean up time from 10 minutes to <1 minute. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26590,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 25 18:24:28 UTC 2023,,,,,,,,,,"0|z1kd8o:",9223372036854775807,"Now when disposing of no longer needed checkpoints, every state handle/state file will be disposed in parallel by the ioExecutor, vastly improving disposing speed of a single checkpoint (for large checkpoints the disposal time can be improved from 10 minutes to < 1 minute) . The old behaviour can be restored by setting `state.checkpoint.cleaner.parallel-mode` to `false`. ",,,,,,,,,,,,,,,,,,,"15/Sep/23 02:43;masteryhx;Thanks for the proposal.

We also saw related problem in FLINK-26590 which I just linked.

Would you like to contribute your codes ?;;;","15/Sep/23 05:38;yigress;Yes I would like to contribute, I will send a PR for review.;;;","15/Sep/23 09:50;yigress;[~masteryhx] Can you help review?  I am new to Flink and greatly appreciate guidance and advices, thank you!;;;","18/Sep/23 02:48;masteryhx;Sure. Thanks for the pr.
I will take a look asap.;;;","27/Sep/23 23:51;yigress;some of our users jobs having large number of state files, this is an annoying issue for them, really appreciate if can resolve this. [@rkhachatryan|https://github.com/rkhachatryan]  can you kindly help review as requested? thank you!;;;","25/Oct/23 14:00;pnowojski;merged commit 4de0f76 into apache:master;;;","25/Oct/23 18:24;yigress; [~masteryhx],  [~pnowojski] really appreciate your patient and constructive reviews and guidance on working with Flink code, thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop Flink 1.13 and 1.14 support for the operator,FLINK-33089,13550710,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,14/Sep/23 15:23,01/Feb/24 15:35,04/Jun/24 20:40,28/Oct/23 20:00,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"As agreed with the community we will only support the last 4 stable Flink minor versions.
With Flink 1.18 already out, we should drop 1.13 and 1.14 support from the operator.

This includes any special codepaths required and we should probably throw a validation error and short-circuit reconciliation on unsupported versions to signal to users and avoid any accidental deployment problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Oct 28 20:00:41 UTC 2023,,,,,,,,,,"0|z1kd4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Oct/23 20:00;gyfora;merged to main e1c3192fdda1b22373a0e5821856c028bd4183b8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NullPointerException in RemoteTierConsumerAgent of tiered storage,FLINK-33088,13550683,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,14/Sep/23 11:56,15/Sep/23 09:38,04/Jun/24 20:40,15/Sep/23 09:38,1.18.0,,,,,,,,,,,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"Currently, when getting a buffer from RemoteTierConsumerAgent of tiered storage, a NullPointerException may be thrown, we should fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 09:38:39 UTC 2023,,,,,,,,,,"0|z1kcyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/23 09:38;tanyuxin;master(1.19): 1774b84a74f5d348a94bf7dbd5694122150881dd
release-1.18: 0501d03658495b86b33ea57913c4baa131287388;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkSql unable to parse column comment,FLINK-33087,13550640,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,heigebupahei,heigebupahei,14/Sep/23 07:27,14/Sep/23 07:51,04/Jun/24 20:40,14/Sep/23 07:49,1.16.2,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,"If use flinksql to create a table:
{code:java}
CREATE TABLE `test_table` (
    `id` STRING PRIMARY KEY NOT ENFORCED COMMENT 'test_comment', 
    `gender` STRING COMMENT 'test_comment', )
COMMENT 'test'
 WITH (
...
); {code}
We cannot get the comment  of the column

org.apache.flink.table.planner.operations.SqlCreateTableConverter#createCatalogTable
The comment of the column is not parsed. This is related to FLIP-164
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-14 07:27:05.0,,,,,,,,,,"0|z1kcpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protect failure enrichment against unhandled exceptions,FLINK-33086,13550604,13529017,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pgaref,pgaref,pgaref,14/Sep/23 01:50,14/Sep/23 15:32,04/Jun/24 20:40,14/Sep/23 15:32,,,,,,,,,,,1.18.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"Existing [labelFailure|https://github.com/apache/flink/blob/603181da811edb47c0d573492639a381fbbedc28/flink-runtime/src/main/java/org/apache/flink/runtime/failure/FailureEnricherUtils.java#L175] async logic is expecting FailureEnricher future to never fail (or do their own exception handling) however there is no way to enforce that as they are loaded as they are implemented as pluggable components. This could result to throwing away labels from other enrichers that successfully completed. 

A better solution would be to handle the failures and LOG the errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 14 15:32:34 UTC 2023,,,,,,,,,,"0|z1kchc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/23 15:32;chesnay;master: e58a718d04e3ec6e2a43da8a868e5515916c0eea
1.18: 206609e822a8029a78245e6eef7ab5d88a0f370b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message when the invalidate lookupTableSource without primary key is used as temporal join table,FLINK-33085,13550524,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,337361684@qq.com,337361684@qq.com,13/Sep/23 12:14,11/Mar/24 12:44,04/Jun/24 20:40,,1.19.0,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,Improve the error message when the invalidate lookupTableSource without primary key is used as temporal join table.  This pr can check the legality of temporary table join syntax in sqlToRel phase and make the thrown error clearer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-13 12:14:18.0,,,,,,,,,,"0|z1kbzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate globalJobParameter in ExecutionConfig to configuration instance,FLINK-33084,13550499,,Technical Debt,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,JunRuiLi,JunRuiLi,JunRuiLi,13/Sep/23 10:25,30/Oct/23 07:52,04/Jun/24 20:40,30/Oct/23 07:52,,,,,,,,,,,1.19.0,,,,Runtime / Configuration,,,,,,0,pull-request-available,,,,,"Currently, the globalJobParameter field in ExecutionConfig has not been migrated to the Configuration. Considering the goal of unifying configuration options, it is necessary to migrate it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 30 07:52:40 UTC 2023,,,,,,,,,,"0|z1kbu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Oct/23 07:52;zhuzh;master: b946ecc668342d48c2c0193ad4eff1897c75b68f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SupportsReadingMetadata is not applied when loading a CompiledPlan,FLINK-33083,13550488,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dwysakowicz,dwysakowicz,dwysakowicz,13/Sep/23 09:38,29/Nov/23 12:51,04/Jun/24 20:40,19/Sep/23 15:45,1.16.2,1.17.1,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"If a few conditions are met, we can not apply ReadingMetadata interface:
# source overwrites:
 {code}
    @Override
    public boolean supportsMetadataProjection() {
        return false;
    }
 {code}
# source does not implement {{SupportsProjectionPushDown}}
# table has metadata columns e.g.
{code}
CREATE TABLE src (
  physical_name STRING,
  physical_sum INT,
  timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL
)
{code}
# we query the table {{SELECT * FROM src}}

It fails with:
{code}
Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)
{code}

The reason is {{SupportsReadingMetadataSpec}} is created only in the {{PushProjectIntoTableSourceScanRule}}, but the rule is not applied when 1 & 2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 15:45:50 UTC 2023,,,,,,,,,,"0|z1kbrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 13:12;337361684@qq.com;It looks like this is a planner bug, and we don't have related tests to cover the situation that connector implement 

SupportsReadingMetadata and supportsMetadataProjection return false:
{code:java}
default boolean supportsMetadataProjection() {
    return false;
}{code}
 ;;;","19/Sep/23 15:45;dwysakowicz;fixed in 065107edbea6d1afa26af72d8d0bf536104521b8..35f13ea50c1c99d4b4751b33410fb5f5241094c2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure Pipelines 4 is not responding on AZP,FLINK-33082,13550484,,Bug,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,13/Sep/23 09:31,14/Dec/23 08:26,04/Jun/24 20:40,,1.17.2,,,,,,,,,,,,,,Build System / CI,,,,,,0,test-stability,,,,,"it impacts this build for 1.17 [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53163&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=6e8542d7-de38-5a33-4aca-458d6c87066d]

{noformat}
##[error]We stopped hearing from agent Azure Pipelines 4. Verify the agent machine is running and has a healthy network connection. Anything that terminates an agent process, starves it for CPU, or blocks its network access can cause this error. For more information, see: https://go.microsoft.com/fwlink/?linkid=846610
Agent: Azure Pipelines 4
Started: Today at 4:59 AM
Duration: 6h 24m 38s
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 14 08:26:42 UTC 2023,,,,,,,,,,"0|z1kbqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Oct/23 06:12;mapohl;[~Sergey Nuyanzin] That looks like a Azure CI issue. I guess we can close this one since we're not able to do anything about it on the Flink side. WDYT?;;;","05/Oct/23 21:06;Sergey Nuyanzin;yes, probably you are right, let's close it for now;;;","14/Nov/23 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54245&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2;;;","16/Nov/23 08:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54569&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2;;;","16/Nov/23 12:56;mapohl;I'm reopening this issue for documentation purposes because it's happening more often recently.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54516&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2;;;","16/Nov/23 13:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=54446&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2;;;","14/Dec/23 08:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=55496&view=logs&j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move parallelism override logic into scale method,FLINK-33081,13550470,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,13/Sep/23 08:48,14/Sep/23 12:32,04/Jun/24 20:40,14/Sep/23 12:32,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,After FLINK-32589  the parallelism overrides are applied separately from the scale call of the autoscaler implementation. We should simplify this by a small refactoring,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 14 12:32:48 UTC 2023,,,,,,,,,,"0|z1kbnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/23 12:32;gyfora;merged to main 96f07a7296696f41dadfcaf827c95ddf22f83542;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The checkpoint storage configured in the job level by config option will not take effect,FLINK-33080,13550453,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,13/Sep/23 07:02,31/Oct/23 03:38,04/Jun/24 20:40,31/Oct/23 03:38,1.17.1,1.18.0,1.19.0,,,,,,,,1.19.0,,,,Runtime / Checkpointing,Runtime / Configuration,,,,,0,pull-request-available,,,,,"When we configure the checkpoint storage at the job level, it can only be done through the following method:
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.getCheckpointConfig().setCheckpointStorage(xxx); {code}
or configure filesystem storage by config option CheckpointingOptions.CHECKPOINTS_DIRECTORY through the following method:
{code:java}
Configuration configuration = new Configuration();
configuration.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir); 
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(configuration);{code}
However, configure the other type checkpoint storage by the job-side configuration like the following will not take effect:
{code:java}
Configuration configuration = new Configuration();
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(configuration);
configuration.set(CheckpointingOptions.CHECKPOINT_STORAGE, ""aaa.bbb.ccc.CustomCheckpointStorage"");
configuration.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir); {code}
This behavior is unexpected, we should allow this way will take effect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 03:38:07 UTC 2023,,,,,,,,,,"0|z1kbjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/23 03:38;zhuzh;Fixed via 25697476095a5b9cf38dc3b61c684d0e912b1353;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The gap between the checkpoint timeout and the interval settings is too large in the example,FLINK-33079,13550450,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,liufangliang,liufangliang,13/Sep/23 06:15,13/Sep/23 06:27,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,Documentation,,,,,,0,,,,,,"The gap between the checkpoint timeout and the interval settings is too large in the following example

[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/]

Some users will think that the documentation is the optimal solution and refer to this demo setting, and the result is that the actual checkpoint interval is not as expected because of the checkpoint-timeout

!image-2023-09-13-14-17-12-718.png|width=682,height=468!

The following situation occurs when the checkpoint interval is set to 20s and the checkpoint timeout is set to 10 minutes.

!image-2023-09-13-14-19-05-493.png|width=1637,height=757!

So lets do some optimization in the checkpoint example(e.g. checkpoint interval 60s, checkpoint timeout 60s), or provide more documentation for setting up checkpoints config.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/23 06:17;liufangliang;image-2023-09-13-14-17-12-718.png;https://issues.apache.org/jira/secure/attachment/13062861/image-2023-09-13-14-17-12-718.png","13/Sep/23 06:19;liufangliang;image-2023-09-13-14-19-05-493.png;https://issues.apache.org/jira/secure/attachment/13062862/image-2023-09-13-14-19-05-493.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-13 06:15:01.0,,,,,,,,,,"0|z1kbj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enhance ParameterTool to support múltiple environments,FLINK-33078,13550414,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mostrovoi,mostrovoi,12/Sep/23 21:20,12/Sep/23 21:20,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,"I would like to see a new class e.g. EnvironmentParameterTool which can be made of composition of 2 parametertool instances and works by retrieving the value in a properties file environment specific and if not found falls back to global properties file.

This would work in the same fashion than, for instance, spring boot works with properties.yaml and properties-dev.yaml

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-12 21:20:04.0,,,,,,,,,,"0|z1kbb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minimize the risk of hard back-pressure with buffer debloating enabled,FLINK-33077,13550375,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,srichter,srichter,12/Sep/23 14:15,14/Sep/23 12:07,04/Jun/24 20:40,14/Sep/23 12:07,,,,,,,,,,,1.18.0,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,"{*}Problem{*}:
Buffer debloating sets buffer size to {{256}} bytes because of back-pressure.
Such small buffers might not be enough to emit the processing results of a single record. The task thread would request new buffers, and often block.
That results in significant checkpoint delays (up to minutes instead of seconds).

Adding more overdraft buffers helps, but depends on the job DoP
Raising {{taskmanager.memory.min-segment-size}} from {{256}} helps, but depends on the multiplication factor of the operator.

{*}Solution{*}:
 * Ignore Buffer Debloater hints and extend the buffer if possible - when this prevents emitting an output record fully AND this is the last available buffer.
 * Prevent the subsequent flush of the buffer so that more output records can be emitted (flatMap-like and join operators)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 14 12:07:56 UTC 2023,,,,,,,,,,"0|z1kb2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/23 12:07;roman;Fixed in c7f6470bb8cc314e7651b03e171af057f4edec1e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broadcastEmit of ChannelSelectorRecordWriter should reuse the serialized record,FLINK-33076,13550360,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,12/Sep/23 12:19,14/Sep/23 07:22,04/Jun/24 20:40,14/Sep/23 07:22,,,,,,,,,,,1.19.0,,,,Runtime / Network,,,,,,0,pull-request-available,,,,,ChannelSelectorRecordWriter#broadcastEmit serialize the record to ByteBuffer but didn't use it. It will re-serialize this record per-channel. We should allows all channels to reuse the serialized buffer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 14 07:22:22 UTC 2023,,,,,,,,,,"0|z1kaz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Sep/23 07:22;Weijie Guo;master(1.19) via e77dc91a59b81c9727fe2d7b3f29dbdd290266a3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notice files for Statefun are outdated,FLINK-33075,13550323,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,12/Sep/23 07:50,12/Sep/23 09:14,04/Jun/24 20:40,12/Sep/23 09:14,statefun-3.3.0,,,,,,,,,,statefun-3.3.0,,,,Stateful Functions,,,,,,0,pull-request-available,,,,,"{code:java}
- NOTICE files are present
  - Note: The copyright year is out of data (2020)
  - Concern: we bundle AnchorJS (MIT) v3.1.0 and this is not listed in the
NOTICE file
  - Concern: ""statefun-sdk-java"" bundles
""com.google.auto.service:auto-service-annotations:jar:1.0-rc6"" but does not
declare it in the NOTICE
  - Concern: ""statefun-flink-distribution""
    - bundles ""org.apache.kafka:kafka-clients:3.2.3"" but declares
""org.apache.kafka:kafka-clients:2.4.1""
    - bundles ""com.github.luben:zstd-jni:1.5.2-1"" but declares
""com.github.luben:zstd-jni:1.4.3-1""
    - bundles ""com.fasterxml.jackson.core:jackson-core:2.13.4"" but declares
""com.fasterxml.jackson.core:jackson-core:2.12.1""
    - bundles ""com.fasterxml.jackson.core:jackson-annotations:2.13.4"" but
declares ""com.fasterxml.jackson.core:jackson-annotations:2.12.1""
    - bundles ""com.fasterxml.jackson.core:jackson-databind:2.13.4.2"" but
declares ""com.fasterxml.jackson.core:jackson-databind:2.12.1""
    - bundles
""com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.13.4"" but
declares ""com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.12.1""
    - bundles ""commons-io:commons-io:jar:2.11.0"" but declares
""commons-io:commons-io:jar:2.8.0""
    - bundles ""commons-codec:commons-codec:1.15"" but declares
""commons-codec:commons-codec:1.13""
    - bundles ""com.esotericsoftware.minlog:minlog:1.2"" but does not declare
it
    - bundles ""com.ibm.icu:icu4j:jar:67.1"" but does not declare it
    - bundles ""org.objenesis:objenesis:jar:2.1"" but does not declare it
    - bundles ""com.esotericsoftware.kryo:kryo:2.24.0"" but does not declare
it
    - bundles ""commons-collections:commons-collections:3.2.2"" but does not
declare it
    - bundles ""org.apache.commons:commons-compress:1.21"" but does not
declare it
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 12 09:14:42 UTC 2023,,,,,,,,,,"0|z1kaqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 09:14;martijnvisser;Fixed in:

apache/flink-statefun@master accd75ea0109845c4b4c0ddd74021147af1439d4
apache/flink-statefun@release-3.3 5879f15f9332b2aa264d6f80ef69df80157291a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink-ci-mirror stopped running for commits after 3rd of Septemper,FLINK-33074,13550321,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,12/Sep/23 07:40,12/Sep/23 10:20,04/Jun/24 20:40,12/Sep/23 10:20,,,,,,,,,,,,,,,Build System / CI,,,,,,0,,,,,,"Blocker since it impacts all branches

it seems it completely stopped working for non master branch (latest build was on 03.09.2023)
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52961&view=results

for master it ls more tricky: it still continues running only for commit https://github.com/apache/flink/pull/23331

and it doesn't run for newer commits",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 12 10:20:13 UTC 2023,,,,,,,,,,"0|z1kaqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 10:20;jingge;disk full issue and fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement end-to-end tests for the Kinesis Streams Sink,FLINK-33073,13550286,13550285,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hong,hong,hong,11/Sep/23 23:26,31/Oct/23 15:12,04/Jun/24 20:40,31/Oct/23 15:12,,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / AWS,,,,,,0,pull-request-available,,,,,"*What*

Implement end-to-end tests for KinesisStreamsSink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-24474,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 28 12:48:38 UTC 2023,,,,,,,,,,"0|z1kaio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Sep/23 12:19;dannycranmer;Merged commit [{{9743a29}}|https://github.com/apache/flink-connector-aws/commit/9743a29ea53836ba5486548533adacc3a9b4db50] into apache:main;;;","28/Sep/23 12:48;dannycranmer;[~hong] AWS e2e tests did not run on main build, can we update to run on this branch? https://github.com/apache/flink-connector-aws/actions/runs/6338765645/job/17216531556;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement end-to-end tests for AWS Kinesis Connectors,FLINK-33072,13550285,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,hong,hong,hong,11/Sep/23 23:25,04/Jan/24 13:33,04/Jun/24 20:40,31/Oct/23 15:13,,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / AWS,Connectors / Kinesis,,,,,0,,,,,,"*What*

We want to implement end-to-end tests that target real Kinesis Data Streams.

*Why*

This solidifies our testing to ensure we pick up any integration issues with Kinesis Data Streams API.

We especially want to test happy cases and failure cases to ensure those cases are handled as expected by the KDS connector.

 

Reference: https://issues.apache.org/jira/browse/INFRA-24474

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-11 23:25:16.0,,,,,,,,,,"0|z1kaig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log checkpoint statistics ,FLINK-33071,13550246,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pnowojski,pnowojski,pnowojski,11/Sep/23 15:46,15/Sep/23 11:37,04/Jun/24 20:40,15/Sep/23 09:41,1.18.0,,,,,,,,,,1.18.1,1.19.0,,,Runtime / Checkpointing,Runtime / Metrics,,,,,0,pull-request-available,,,,,"This is a stop gap solution until we have a proper way of solving FLINK-23411.

The plan is to dump JSON serialised checkpoint statistics into Flink JM's log, with a {{DEBUG}} level. This could be used to analyse what has happened with a certain checkpoint in the past.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23411,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 09:41:11 UTC 2023,,,,,,,,,,"0|z1ka9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/23 09:41;pnowojski;merged commit ece5fad into apache:master
merged commit 6e13c60 into apache:release-1.18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add doc for 'unnest' ,FLINK-33070,13550211,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Problem,,hackergin,hackergin,11/Sep/23 12:17,11/Sep/23 13:03,04/Jun/24 20:40,11/Sep/23 13:03,,,,,,,,,,,,,,,Table SQL / Runtime,,,,,,0,,,,,,"Row and column transformation is a commonly used approach. In Flink SQL, we can use unnest for this purpose.

However, the usage and support of unnest are not explained in the documentation.

 

 I think we can at least add it to the built-in functions section ([https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/#scalar-functions]) ， or we can provide some examples. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 11 13:02:43 UTC 2023,,,,,,,,,,"0|z1ka28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/23 12:56;libenchao;FYI, {{UNNEST}} is documented under the [Joins|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/queries/joins/#array-expansion];;;","11/Sep/23 13:02;hackergin;Thanks [~libenchao] ,  Sorry, I tried searching before but didn't find it. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mysql and Postgre catalog support url extra parameters,FLINK-33069,13550172,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,waywtdcc,waywtdcc,11/Sep/23 05:38,13/Nov/23 00:47,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / JDBC,,,,,,0,pull-request-available,,,,," 

 

Mysql and Postgres catalog support url extra parameters

CREATE CATALOG mymysql WITH(
'type' = 'jdbc',
'username' = 'root',
'password' = 'xxx',
'base-url' = 'jdbc:mysql://xxx:53309',
'extra-url-param' = '?characterEncoding=utf8'
);

If used in this way, the URLs of all tables obtained from this catalog are: jdbc:mysql://xxx:53309?characterEncoding=utf8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-11 05:38:32.0,,,,,,,,,,"0|z1k9tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle DNS errors in AWS connectors as recoverable,FLINK-33068,13550078,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,elphastori,elphastori,elphastori,08/Sep/23 22:05,09/Oct/23 13:45,04/Jun/24 20:40,,1.15.4,1.16.2,1.17.1,,,,,,,,1.15.4,1.16.2,1.17.1,,Connectors / AWS,,,,,,0,,,,,,"AWS connectors are not treating DNS errors as recoverable using exponential back-off resulting in Flink job restarts on transient errors and small timeouts.

 

DNS errors typically manifest as UnknownHostException which is not handled as a recoverable exception and potentially not retried.

 

Here is an [example of an exception|https://github.com/apache/flink-connector-aws/blob/21c64ce38d3a6fd4fd9b4abda416c2de1a926780/flink-connector-aws/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/proxy/KinesisProxy.java#L459] that is recoverable with exponential-backoff. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 09 13:45:22 UTC 2023,,,,,,,,,,"0|z1k98o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/23 22:06;elphastori;[~danny.cranmer] could you please assign this to me?;;;","09/Oct/23 13:45;dannycranmer;[~elphastori] done, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose JODK rate limiter config and enable by default,FLINK-33067,13550022,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,08/Sep/23 09:42,20/Nov/23 12:31,04/Jun/24 20:40,12/Sep/23 14:21,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"There are certain cases currently when the operator can stuck in an infinite no wait retry loop under certain circumstances (see [https://github.com/operator-framework/java-operator-sdk/issues/2046] for details).

The underlying problems that cause this in some cases are not easily solved generically and it is a better practice to enable rate limiting to safeguard against these problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 12 14:21:04 UTC 2023,,,,,,,,,,"0|z1k8w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 14:21;gyfora;merged to main 4d95de3fe29de7d8465d9240d04f878358635e3d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable to inject environment variable from secret/configmap to operatorPod,FLINK-33066,13550011,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dongwoo.kim,dongwoo.kim,dongwoo.kim,08/Sep/23 08:20,14/Sep/23 05:46,04/Jun/24 20:40,14/Sep/23 05:46,,,,,,,,,,,kubernetes-operator-1.7.0,,,,Kubernetes Operator,,,,,,0,pull-request-available,,,,,"Hello, I've been working with the Flink Kubernetes operator and noticed that the {{operatorPod.env}} only allows for simple key-value pairs and doesn't support Kubernetes {{valueFrom}} syntax.

How about changing template to support more various k8s syntax? 

*Current template*
{code:java}
{{- range $k, $v := .Values.operatorPod.env }}
  - name: {{ $v.name | quote }}
    value: {{ $v.value | quote }}
{{- end }}{code}
 

*Proposed template*
1) Modify template like below 
{code:java}
{{- with .Values.operatorPod.env }} 
{{- toYaml . | nindent 12 }} 
{{- end }} 
{code}
2) create extra config, *Values.operatorPod.envFrom* and utilize this

 

I'd be happy to implement this update if it's approved.
Thanks in advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 14 05:46:59 UTC 2023,,,,,,,,,,"0|z1k8ts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/23 19:18;gyfora;I think this would be a nice improvement :) ;;;","13/Sep/23 02:13;dongwoo.kim;Thanks [~gyfora], I have opened the pr :);;;","14/Sep/23 05:46;mbalassi;1053e26 in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize the exception message when the program plan could not be fetched,FLINK-33065,13550004,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,08/Sep/23 07:27,13/Sep/23 03:44,04/Jun/24 20:40,13/Sep/23 03:44,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,"When the program plan could not be fetched, the root cause may be: the main method doesn't call the `env.execute()`.

 

We can optimize the message to help user find this root cause.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 13 03:44:11 UTC 2023,,,,,,,,,,"0|z1k8s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 03:44;fanrui;Merged via <master:1.19> d5b2cdb618cd7a50818d7eaa575c0fac6aaeac9a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message when the lookup source is used as the scan source,FLINK-33064,13549985,,Improvement,Reopened,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,337361684@qq.com,337361684@qq.com,337361684@qq.com,08/Sep/23 04:07,11/Mar/24 13:11,04/Jun/24 20:40,,1.18.0,,,,,,,,,,1.20.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"Improve the error message when the lookup source is used as the scan source. Currently, if we use a source which only implement LookupTableSource but not implement ScanTableSource, as a scan source, it cannot get a property plan and give a '

Cannot generate a valid execution plan for the given query' which can be improved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 13 05:58:51 UTC 2023,,,,,,,,,,"0|z1k8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 05:58;fsk119;Merged into master: be509e6d67471d886e58d3ddea6ddd3627a191a8

Reverted because of some bad cases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
udaf with user defined pojo object throw error while generate record equaliser,FLINK-33063,13549978,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,08/Sep/23 02:33,12/Sep/23 03:09,04/Jun/24 20:40,12/Sep/23 03:09,1.18.0,,,,,,,,,,1.18.0,1.19.0,,,Table SQL / Runtime,,,,,,0,pull-request-available,,,,,"Udaf with user define pojo object throw error while generating record equaliser: 

When user create an udaf while recore contains user define complex pojo object (like List<UserDefinedObject> or Map<UserDefinedObject>). The codegen will throw error while generating record equaliser, the error is:
{code:java}
A method named ""compareTo"" is not declared in any enclosing class nor any subtype, nor through a static import.{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 12 03:09:19 UTC 2023,,,,,,,,,,"0|z1k8mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/23 02:57;337361684@qq.com;Hi, [~lincoln.86xy] . Can you assign this issue to me? Thanks !;;;","08/Sep/23 03:53;lincoln.86xy;[~337361684@qq.com] assigned to you :);;;","09/Sep/23 09:08;jark;Fixed in 
- master: 01cb6ef008273100718cd8a8cfd921c02b8b8caa
- release-1.18: TODO;;;","12/Sep/23 03:09;fsk119;Merged into release-1.18: f2584a1df364a14ff50b5a52fe7cf5e38d4cdc9a
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deserialization creates multiple instances of case objects in Scala 2.13,FLINK-33062,13549968,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Won't Fix,,SmedbergM,SmedbergM,07/Sep/23 23:59,08/Sep/23 14:22,04/Jun/24 20:40,08/Sep/23 14:22,1.15.4,1.17.1,,,,,,,,,,,,,API / Core,,,,,,0,,,,,,"See [https://github.com/SmedbergM/mwe-flink-2-13-deserialization] for a minimal working example.

When running a Flink job with Scala 2.13, deserialized objects whose fields are case objects have those case objects re-instantiated. Thus any code that relies on reference equality (such as methods of `scala.Option`) will break.

I suspect that this is due to Kyro deserialization not being singleton-aware for case objects, but I haven't been able to drill in and catch this in the act.

Here are relevant lines of my application log:

 
{code:java}
17:37:13.224 [jobmanager-io-thread-1] INFO  o.a.f.r.c.CheckpointCoordinator -- No checkpoint found during restore.
17:37:13.531 [parse-book -> Sink: log-book (2/2)#0] WARN  smedbergm.mwe.BookSink$ -- Book.isbn: None with identityHashCode 1043314405 reports isEmpty false; true None is 2019204827
17:37:13.531 [parse-book -> Sink: log-book (1/2)#0] INFO  smedbergm.mwe.BookSink$ -- Winkler, Scott: Terraform In Action (ISBN 978-1-61729-689-5) $49.99
17:37:13.534 [parse-book -> Sink: log-book (2/2)#0] WARN  smedbergm.mwe.BookSink$ -- Book.isbn: None with identityHashCode 465138693 reports isEmpty false; true None is 2019204827
17:37:13.534 [parse-book -> Sink: log-book (1/2)#0] INFO  smedbergm.mwe.BookSink$ -- Čukić, Ivan: Functional Programming in C++ (ISBN 978-1-61729-381-8) $49.99
17:37:13.538 [flink-akka.actor.default-dispatcher-8] INFO  o.a.f.r.c.CheckpointCoordinator -- Stopping checkpoint coordinator for job eed1c049790ac5f38664ddfd6b049282. {code}
I know that https://issues.apache.org/jira/browse/FLINK-13414 (support for Scala 2.13) is still listed as in-progress, but there is no warning in the docs that using 2.13 might not be stable. (This particular error does not occur on Scala 2.12, in this case because Option.isEmpty was re-implemented in 2.13; however, I suspect that multiple deserialization may be occurring already in 2.12.)

 

 ","Scala 2.13.12

Flink 1.17.1 and 1.15.4 running inside IntelliJ

Flink 1.15.4 running in AWS Managed Flink",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 08 14:21:19 UTC 2023,,,,,,,,,,"0|z1k8k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/23 14:21;martijnvisser;It is documented in the docs that Flink isn't compatible with version of Scala higher then 2.12.7, see https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/datastream/project-configuration/#scala-versions. There's also FLINK-12461 and FLINK-20969

Given that Scala APIs are deprecated and will be removed as of Flink 2.0, this won't be fixed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Translate failure-enricher documentation to Chinese,FLINK-33061,13549940,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,wangm92,pgaref,pgaref,07/Sep/23 15:21,15/Sep/23 06:09,04/Jun/24 20:40,15/Sep/23 06:09,,,,,,,,,,,1.19.0,,,,,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 06:09:29 UTC 2023,,,,,,,,,,"0|z1k8e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/23 15:23;pgaref;[~wangm92]  feel free to take over;;;","15/Sep/23 06:09;huweihua;Resolved in master: c5241e502ef6228b9f4ca44372025e88adff8800;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the javadoc of ListState.update/addAll about not allowing null value,FLINK-33060,13549916,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zakelly,zakelly,zakelly,07/Sep/23 10:57,09/Nov/23 02:25,04/Jun/24 20:40,08/Nov/23 02:49,,,,,,,,,,,1.19.0,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,,,"After FLINK-8411, the ListState.update/add/addAll do not allow a null value passed in, while the javadoc says ""If null is passed in, the state value will remain unchanged"". This should be fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 08 02:49:53 UTC 2023,,,,,,,,,,"0|z1k88o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Nov/23 02:49;masteryhx;merged e0240c78 into master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support transparent compression for file-connector for all file input formats,FLINK-33059,13549905,,Technical Debt,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,echauchot,echauchot,echauchot,07/Sep/23 10:09,10/Nov/23 08:48,04/Jun/24 20:40,10/Nov/23 08:48,,,,,,,,,,,1.19.0,,,,Connectors / FileSystem,,,,,,0,pull-request-available,,,,,"Some FileInputFormats don't use FileInputFormat#createSplits (that would detect that the file is non-splittable and deal with reading boundaries correctly), they all create split manually from FileSourceSplit. If input files are compressed, split length is determined by the compressed file length leading to [this|https://issues.apache.org/jira/browse/FLINK-30314] bug. We should force reading the whole file split (like it is done for binary input formats) on compressed files. Parallelism is still done at the file level (as now)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 08:48:52 UTC 2023,,,,,,,,,,"0|z1k868:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/23 08:48;echauchot;master: 51252638fcb855a82da9983b3dfaa3b89754523e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for JSON-encoded Avro,FLINK-33058,13549903,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,dalelane,dalelane,dalelane,07/Sep/23 09:36,21/Nov/23 21:29,04/Jun/24 20:40,,,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,avro,flink,flink-formats,pull-request-available,,"Avro supports two serialization encoding methods: binary and JSON

cf. [https://avro.apache.org/docs/1.11.1/specification/#encodings] 

flink-avro currently has a hard-coded assumption that Avro data is binary-encoded (and cannot process Avro data that has been JSON-encoded).

I propose adding a new optional format option to flink-avro: *avro.encoding*

It will support two options: 'binary' and 'json'. 

It unset, it will default to 'binary' to maintain compatibility/consistency with current behaviour. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 17 17:44:55 UTC 2023,,,,,,,,,,"0|z1k85s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/23 14:33;rskraba;Hello!  I can review this, but in my experience, Avro-encoded JSON is good only for debugging or human-readable ""previews"" of data. I haven't run across the use of JSON encoding in production (it's typically larger, slower and the temptation to use JSON tools on it is counterproductive!)  In my opinion, this has proven to be especially true for persistent messages.

If I can ask: what are the circumstances where a user would choose Avro, but want something other than the binary encoding?;;;","17/Sep/23 17:44;dalelane;hi [~rskraba] - thanks very much, a review would be much appreciated. 

As for use cases, I should give some context. I work for IBM - we sell a Kafka distribution with a schema registry that comes with serdes clients offering both binary and JSON-encoded Avro support. As a part of this, I've worked with many customers who use and value JSON-encoding.

As you suggest, sometimes this is a temporary thing, related to the phase of a project - I've seen some customers who will use JSON-encoding during development, and when they're ready to go into test/prod phases they flip the switch to binary-encoding.

However, there have also been times where I've seen customers use JSON-encoding even in production - generally where the topic throughput is low enough that any performance issues are outweighed by the benefits of greater readability and compatibility that JSON-encoding offers. 

Don't get me wrong, I don't dispute at all that binary-encoding is the more common choice, and comes with major network and disk usage improvements - so it makes sense that Flink would've started with that. But I would love to enable my customers to use Flink with their JSON-encoded Avro topics in the same way that they're able to use other tools, which is what prompted me to offer the pull request. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add options to disable creating job-id subdirectories under the checkpoint directory,FLINK-33057,13549881,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zakelly,zakelly,zakelly,07/Sep/23 08:28,15/Jan/24 02:21,04/Jun/24 20:40,15/Jan/24 02:21,1.19.0,,,,,,,,,,1.19.0,,,,Runtime / Checkpointing,,,,,,0,pull-request-available,,,,,"By default, Flink creates subdirectories named by UUID (job id) under checkpoint directory for each job. It's a good means to avoid collision. However, it also bring in some effort to remember/find the right directory when recovering from previous checkpoint. According to previous discussion ([Yun Tang's|https://issues.apache.org/jira/browse/FLINK-11789?focusedCommentId=16782314&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16782314] and [Stephan Ewen's|https://issues.apache.org/jira/browse/FLINK-9043?focusedCommentId=16409254&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16409254] ), I think it would be useful to add an option to disable creating the UUID subdirectories under the checkpoint directory. For compatibility considerations, we create the subdirectories by default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 15 02:21:49 UTC 2024,,,,,,,,,,"0|z1k80w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/23 08:33;masteryhx;Thanks for picking this up.

+1 for introducing such an option.;;;","15/Jan/24 02:21;masteryhx;merged 73b036c3 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyClientServerSslTest#testValidSslConnection fails on AZP,FLINK-33056,13549871,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,07/Sep/23 07:29,29/Sep/23 22:35,04/Jun/24 20:40,,1.19.0,,,,,,,,,,,,,,Runtime / Configuration,Runtime / Coordination,,,,,0,auto-deprioritized-critical,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53020&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8592

fails with 
{noformat}
Test testValidSslConnection[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
01:20:31,479 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port range: 36717, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client thr
eads: 1 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
01:20:31,479 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
01:20:31,475 [Flink Netty Client (42359) Thread 0] WARN  org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline [] - An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.
org.apache.flink.shaded.netty4.io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: server certificate with unknown fingerprint: CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:499) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:800) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:499) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:397) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: javax.net.ssl.SSLHandshakeException: server certificate with unknown fingerprint: CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown
        at sun.security.ssl.Alert.createSSLException(Alert.java:131) ~[?:1.8.0_292]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:324) ~[?:1.8.0_292]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:267) ~[?:1.8.0_292]
        at sun.security.ssl.TransportContext.fatal(TransportContext.java:262) ~[?:1.8.0_292]
        at sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:654) ~[?:1.8.0_292]
        at sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473) ~[?:1.8.0_292]
        at sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369) ~[?:1.8.0_292]
        at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:377) ~[?:1.8.0_292]
        at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444) ~[?:1.8.0_292]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:968) ~[?:1.8.0_292]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:955) ~[?:1.8.0_292]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:902) ~[?:1.8.0_292]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1559) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1405) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1246) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1295) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        ... 14 more
Caused by: java.security.cert.CertificateException: server certificate with unknown fingerprint: CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=Unknown
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.util.FingerprintTrustManagerFactory$1.checkTrusted(FingerprintTrustManagerFactory.java:124) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.util.FingerprintTrustManagerFactory$1.checkServerTrusted(FingerprintTrustManagerFactory.java:108) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.util.X509TrustManagerWrapper.checkServerTrusted(X509TrustManagerWrapper.java:69) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:632) ~[?:1.8.0_292]
        at sun.security.ssl.CertificateMessage$T12CertificateConsumer.onCertificate(CertificateMessage.java:473) ~[?:1.8.0_292]
        at sun.security.ssl.CertificateMessage$T12CertificateConsumer.consume(CertificateMessage.java:369) ~[?:1.8.0_292]
        at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:377) ~[?:1.8.0_292]
        at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444) ~[?:1.8.0_292]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:968) ~[?:1.8.0_292]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:955) ~[?:1.8.0_292]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
        at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:902) ~[?:1.8.0_292]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1559) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1405) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1246) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1295) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:529) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:468) ~[flink-shaded-netty-4.1.91.Final-17.0.jar:?]
        ... 14 more

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/23 07:30;Sergey Nuyanzin;logs-cron_azure-test_cron_azure_core-1694048924.zip;https://issues.apache.org/jira/secure/attachment/13062740/logs-cron_azure-test_cron_azure_core-1694048924.zip",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 29 22:35:12 UTC 2023,,,,,,,,,,"0|z1k7yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","29/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the error value about 'state.backend.type' in the document,FLINK-33055,13549867,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lijinzhong,masteryhx,masteryhx,07/Sep/23 06:21,08/Sep/23 08:14,04/Jun/24 20:40,08/Sep/23 08:14,,,,,,,,,,,1.19.0,,,,Documentation,Runtime / State Backends,,,,,0,,,,,," 
{code:java}
state.backend.type: The state backend to use. This defines the data structure mechanism for taking snapshots. Common values are filesystem or rocksdb{code}
filesystem should be replaced with hashmap after FLINK-16444.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 08 08:14:46 UTC 2023,,,,,,,,,,"0|z1k7xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/23 04:13;lijinzhong;hi，[~masteryhx] ， I would like to take this trivial work.;;;","08/Sep/23 04:19;masteryhx;[~lijinzhong] Thanks for volunteering, already assigned to you, please go ahead.;;;","08/Sep/23 08:14;masteryhx;merged 53ccb93cfb3b6f2befdb13cdeb9d96acbef44932 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the job execution result fetching timeout in CollectResultFetcher with akka timeout,FLINK-33054,13549864,13417633,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,guoyangze,guoyangze,guoyangze,07/Sep/23 05:08,11/Sep/23 05:13,04/Jun/24 20:40,11/Sep/23 05:13,1.18.0,,,,,,,,,,1.19.0,,,,Table SQL / Gateway,,,,,,0,pull-request-available,,,,,"Currently in CollectResultFetcher, the job execution result will be fetched after job's termination. In FLINK-17735, we arbitrarily introduce a static timeout for this rpc call. However, in OLAP scenario, the Dispatcher endpoint might be too busy to reply in time. We'd like to set it according to the akka ask timeout, which is commonly used as rpc timeout in Flink.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 11 05:13:26 UTC 2023,,,,,,,,,,"0|z1k7x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Sep/23 05:13;guoyangze;master: 9c91ea3d35528ddc6a5d5be08407d5b0c71f2e76;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Watcher leak in Zookeeper HA mode,FLINK-33053,13549860,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,guoyangze,guoyangze,guoyangze,07/Sep/23 03:45,23/Nov/23 08:45,04/Jun/24 20:40,19/Sep/23 05:37,1.17.0,1.17.1,1.18.0,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"We observe a watcher leak in our OLAP stress test when enabling Zookeeper HA mode. TM's watches on the leader of JobMaster has not been stopped after job finished.

Here is how we re-produce this issue:
 - Start a session cluster and enable Zookeeper HA mode.
 - Continuously and concurrently submit short queries, e.g. WordCount to the cluster.
 - echo -n wchp | nc \{zk host} \{zk port} to get current watches.

We can see a lot of watches on /flink/\{cluster_name}/leader/\{job_id}/connection_info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ZOOKEEPER-4625,,,,,,,,"08/Sep/23 03:22;guoyangze;26.dump.zip;https://issues.apache.org/jira/secure/attachment/13062758/26.dump.zip","08/Sep/23 03:21;guoyangze;26.log;https://issues.apache.org/jira/secure/attachment/13062757/26.log","09/Sep/23 07:58;guoyangze;taskmanager_flink-native-test-117-taskmanager-1-9_thread_dump (1).json;https://issues.apache.org/jira/secure/attachment/13062778/taskmanager_flink-native-test-117-taskmanager-1-9_thread_dump+%281%29.json",,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 05:35:24 UTC 2023,,,,,,,,,,"0|z1k7w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Sep/23 03:48;guoyangze;[~mapohl] Would you like to take a look?;;;","07/Sep/23 03:52;guoyangze;Also cc [~wangyang0918];;;","07/Sep/23 05:56;mapohl;Thanks for bringing this up, [~guoyangze]. I'm going to have a look at it. Just having a brief look at the code, I suspect it to be a curator issue. Because we're properly closing all the resources in the driver. But I have to investigate further.

You didn't check, by any chance, whether this is also observable in 1.18 (because we upgraded curator to 5.4.0 in 1.18)?;;;","07/Sep/23 06:21;guoyangze;[~mapohl] Thanks for your help. I check it with curator 5.5.0 but observe the same issue. cc [~Tison];;;","07/Sep/23 06:23;guoyangze;In our test, the log shows the ZooKeeperLeaderRetrievalDriver has been close correctly. So, it's likely to be a curator issue.;;;","07/Sep/23 06:50;mapohl;Can you share the logs and the thread dump of this run?;;;","07/Sep/23 07:46;tison;The recipe in use is {{TreeCache}}, which doesn't change from 5.0.0. And it also closes watches on {{close}}.

Do you have a bisect which version introduced this regression?;;;","07/Sep/23 09:00;tison;Perhaps you can enable debug logs and check ""Removing watcher for path: "" from Curator to see if the related watchers are issued removing.;;;","07/Sep/23 09:47;guoyangze;[~tison] Could we use CuratorCache instead? Would it be more stable?;;;","07/Sep/23 09:55;mapohl;FLINK-29813 is already covering the migration to {{CuratorCache}}. I haven't had a chance to look into it, yet. We need to do an analysis whether switching would cause some side effects. But we still might want to understand where the thread leaking is coming from.;;;","07/Sep/23 10:52;guoyangze;Thanks for the pointer [~mapohl]. I'll try to get a debug log and heap dump.;;;","08/Sep/23 03:24;guoyangze;I reproduce the issue and filter out some logs. [^26.dump.zip] [^26.log] This tm still watches the leader of job 7db5c7316828f598234677e2169e7b0f. 

{{/flink/flink-native-test-117/leader/7db5c7316828f598234677e2169e7b0f/connection_info}}
{{        0x6400013b748a4420}}

 

[~mapohl] [~tison] ;;;","08/Sep/23 05:51;mapohl;Thanks for sharing this. I guess, the thread dump (which you can generate by sending {{kill -3 <taskmanager-pid>}} to the TaskManager process) would be more helpful than the heap dump. Or am I missing something. The thread dump might tell us more of what threads are still available even after the JobMaster is closed.;;;","08/Sep/23 05:57;tison;The log seems trimed. I saw:

2023-09-08 11:09:03,738 DEBUG org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalManager [] - Removing watcher for path: /flink/flink-native-test-117/leader/7db5c7316828f598234677e2169e7b0f/connection_info

So the TM has issued watcher removal request.;;;","09/Sep/23 07:58;guoyangze;[~tison] Yes, it seems teh curator framework issued the watcher removal request at least. But will there be any logs printed when this request is successful?

 

[~mapohl] Thead dump of a TM with watch leak [^taskmanager_flink-native-test-117-taskmanager-1-9_thread_dump (1).json];;;","11/Sep/23 13:21;guoyangze;I just find that this issue is very easy to reproduce. To observe the watch leak, it only requires deploying a local standalone HA cluster and submitting a single wordcount job. Thus, I'd like to promote it to blocker of 1.18. WDYT [~renqs] ?;;;","13/Sep/23 12:32;guoyangze;JFYI, I'm still investigating the root cause of this, but I found the issue will be fixed if we add a safetynet in 
ZooKeeperLeaderRetrievalDriver#close like this:
 
{code:java}
client.watchers()
   .removeAll()
   .ofType(Watcher.WatcherType.Any)
   .forPath(connectionInformationPath);{code};;;","13/Sep/23 14:12;tison;I noticed that the {{TreeCache}}'s close call {{removeWatches}} instead of {{removeAllWatches}} called by your scripts above.

{{removeWatches}} only remove the watcher in client side so remain the server side watcher as is.;;;","13/Sep/23 14:38;tison;See https://lists.apache.org/thread/3b9hn9j4c05yfztlr2zcctbg7sqwdh58.

This seems to be a ZK issue that I met one year ago..;;;","13/Sep/23 14:39;tison;But we don't have other shared watchers so we can force remove watches as above.;;;","14/Sep/23 02:26;guoyangze;Thanks for the pointer [~tison] . I'd like to add a safetynet atm as ZOOKEEPER-4625 has not been fixed and the patch might not be cherry-pick to old version. BTW, could we fix it in curator?;;;","14/Sep/23 03:00;tison;No. Both {{CuratorCache}} and {{TreeCache}} doesn't ""own"" the path so it's unclear if other recipes share the same client (connection) set up watches also. This is different from {{LeaderLatch}} which owns the path so it can ensure that no one else (should) access the related nodes.;;;","14/Sep/23 03:03;tison;But it's possible to add an option to explicitly identify the ownership. You can open an issue on the Curator JIRA project and let me with the other maintainers to figure it out.;;;","19/Sep/23 05:35;guoyangze;master: d5e151f72336abcc13082fe4bb3e05fd5a785e86;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
codespeed and benchmark server is down,FLINK-33052,13549834,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,zakelly,jingge,jingge,06/Sep/23 21:06,08/Nov/23 09:50,04/Jun/24 20:40,20/Oct/23 04:08,1.19.0,,,,,,,,,,,,,,Benchmarks,Test Infrastructure,,,,,0,,,,,,"No update in #flink-dev-benchmarks slack channel since 25th August.

It was a EC2 running in a legacy aws account. Currently on one knows which account it is. 

 

https://apache-flink.slack.com/archives/C0471S0DFJ9/p1693932155128359",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30890,FLINK-33482,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 19 08:30:01 UTC 2023,,,,,,,,,,"0|z1k7qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 12:13;jingge;The vm was in a mystery account no one knows anymore. After contacting AWS support, we could not find it. We have to start plan B to create a new one. ;;;","13/Sep/23 04:31;ym;Let me see whether i can get a new machine for this purpose and re-set up the environment.;;;","13/Sep/23 09:12;pnowojski;Thanks for investigating this issue [~jingge]. Let me know [~ym] if you will be having troubles setting this up again. Has the backup FLINK-30890 been preserved?;;;","13/Sep/23 09:38;ym;[~pnowojski] We have the historical data backed up. But just in case, we will still have the worker node there before the new setup.

 

Also, I would say this ticket is not a blocker for release 1.18, since 1.18 is already branch-cut and in the stage of manual testing?;;;","13/Sep/23 10:25;pnowojski;We have been always treating benchmarking issues the same way as regular test issues. If CI for tests was down, that would have been a clear blocker, preventing everyone from merging any code, doing any releases. The same applies for benchmarking, the only difference is that benchmarks are asynchronous and not executed per every PR due to load that they are generating.

Some of the past examples:
FLINK-23153, FLINK-23879, FLINK-29886, FLINK-30015, FLINK-15171

Also as far as I remember that has been discussed on the dev mailing list at least once or twice.;;;","13/Sep/23 13:30;ym;Hey [~pnowojski] 
 # Strictly speaking, yes it is a blocker for release.
 # But since 1.18 has been branch cut and most of the tests have already been done (No feature is allowed to be merged), maybe we should not make this a blocker issue for RC?
 # [~Zakelly] has already been working on this, but as you can see this issue takes time (applying grants for buying new machines and set up everything, e.t.c)
 # I agree we should have this set up as soon as possible, best before 1.18 release.;;;","20/Sep/23 07:27;zakelly;Hi [~pnowojski] [~jingge] ,

Is it possible we change the original domain name (codespeed.dak8s.net) resolution to the new server? Or I would rather buy a more official domain name like 'flink-speed.io' for the new speed center website. WDTY?;;;","20/Sep/23 14:56;pnowojski;I think there is no big difference one way or the other. The {{dak8s.net}} is owned by Ververica, so [~jingge] should be able to redirect the old one. If it's easier to buy new one, let's do that.;;;","22/Sep/23 08:50;jingge;HI [~Zakelly], redirection should be doable. And I think a new domain like 'flink-speed.io' is a better solution.;;;","10/Oct/23 15:04;martijnvisser;[~Zakelly] Is there any update on this ticket?;;;","11/Oct/23 10:35;zakelly;[~martijnvisser] The codespeed and benchmark server has been setup, and the benchmark has been running daily for about a week. I'm still working on the portal (domain name and public network access etc., involving code security review by colleagues), hope we could make it public soon.;;;","13/Oct/23 04:19;ym;As the pipeline has been running daily, and the reported results look OK for 1.18 (as referred to in this email discussion[1]), this ticket is no longer a blocker for the 1.18 release, so I've moved the fix version to 1.19.0

 

[1] https://www.mail-archive.com/dev@flink.apache.org/msg69920.html

 

 ;;;","19/Oct/23 08:30;zakelly;The new env is setup, Please visit [http://flink-speed.xyz.|http://flink-speed.xyz./] The old domain [http://codespeed.dak8s.net:8000|http://codespeed.dak8s.net:8000/] has also been redirected to the new machines.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify the GlobalFailureHandler and LabeledGlobalFailureHandler interface,FLINK-33051,13549816,13529017,Sub-task,In Progress,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,wangm92,pgaref,pgaref,06/Sep/23 16:07,22/Sep/23 03:41,04/Jun/24 20:40,,,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,FLIP-304 introduced `LabeledGlobalFailureHandler` interface that is an extension of `GlobalFailureHandler` interface.  The later can thus be removed in the future to avoid the existence of interfaces with duplicate functions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32804,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 22 03:40:28 UTC 2023,,,,,,,,,,"0|z1k7mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 03:44;wangm92;[~pgaref] Have you started this tricket? If you agree, I will be happy to contribute this.;;;","13/Sep/23 05:45;pgaref;Hey [~wangm92]  – I already have a POC here but feel free to pick any of the remaining tickets.;;;","14/Sep/23 04:06;pgaref;[~wangm92]  – not sure I am going to have spare cycles to wrap this is up this week, feel free to take over if you are still interested;;;","14/Sep/23 04:51;wangm92;[~pgaref] got it, i will take it;;;","14/Sep/23 12:55;wangm92;[~pgaref] Let me think about it. I don't think it's entirely reasonable to replace GlobalFailureHandler with LabeledGlobalFailureHandler. Now the implementation class of GlobalFailureHandler is mainly in the scheduler, while the implementation class of LabeledGlobalFailureHandler is {_}{color:#505f79}org.apache.flink.runtime.scheduler.adaptive.State{color}{_}. The _{color:#505f79}failureLabels{color}_ information is obtained in the scheduler ({_}{color:#505f79}DefaultScheduler/AdaptiveScheduler{color}{_}), and the API of Scheduler's _{color:#505f79}handleGlobalFailure(Throwable cause, CompletableFuture<Map<String, String>> failureLabels){color}_ is not reasonable.

However, GlobalFailureHandler and LabeledGlobalFailureHandler have overlapping functionality. Here, I tend to unify GlobalFailureHandler and LabeledGlobalFailureHandler into one interface, and State and scheduler will go through different methods. I will submit an MR today, and if you have time, help me review it.;;;","19/Sep/23 06:30;pgaref;Hey [~wangm92]  – thanks for the writeup!
I dont think merging the two interfaces in one will help, as the scheduler/State classes will still have to implement 2 methods (one with Throwble, and another with Throwable and labels), making the code less readable.
Replacing the GlobalFailureHandler with LabeledGlobalFailureHandler should in principle be doable but we might have to push the labeling logic earlier in the process and will end up changing most of the existing logic.
I believe we need to take a step back here and weight our options – WDYT?;;;","22/Sep/23 03:40;wangm92;[~pgaref] I agree with your point of view. Currently, if we want to use LabeledGlobalFailureHandler to replace GlobalFailureHandler, the implementation cost is relatively high. I prefer to unify the exception handling modules of AdaptiveScheduler and DefaultScheduler in the future. I will investigate the feasibility of this idea. If feasible, create a separate ticket or FLIP to promote it at that time. Can we close this ticket first, WDYT?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prompts user to close when atomicity implementation is not supported,FLINK-33050,13549799,13542281,Sub-task,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,tartarus,tartarus,06/Sep/23 13:07,22/Sep/23 01:48,04/Jun/24 20:40,,1.18.0,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"* When atomicity is enabled, an exception may occur when creating a DynamicTableSink, and we need to prompt the user to disable atomicity.
 * When we use InMemoryCatalog, RTAS drop table will only delete the metadata, not clean up the underlying data files, RTAS write data does not use overwrite semantics by default, so it looks like the data is duplicated, this problem needs to be clarified in the documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 19 11:06:54 UTC 2023,,,,,,,,,,"0|z1k7io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/23 11:06;tartarus;See https://issues.apache.org/jira/browse/FLINK-32787 for details ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Website: Code Samples Widget,FLINK-33049,13549767,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,dannycranmer,dannycranmer,06/Sep/23 09:09,01/Nov/23 14:15,04/Jun/24 20:40,,,,,,,,,,,,,,,,Project Website,,,,,,0,,,,,,"Implement Java/Python/SQL code samples widget as per the FLIP: 

- https://cwiki.apache.org/confluence/display/FLINK/FLIP-333%3A+Redesign+Apache+Flink+website",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33046,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-06 09:09:24.0,,,,,,,,,,"0|z1k7bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project website refresh Light Mode,FLINK-33048,13549766,13549764,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,06/Sep/23 09:09,10/Oct/23 06:11,04/Jun/24 20:40,10/Oct/23 06:11,,,,,,,,,,,,,,,Project Website,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 06:11:41 UTC 2023,,,,,,,,,,"0|z1k7bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:11;dannycranmer;Merged commit [{{4309cad}}|https://github.com/apache/flink-web/commit/4309cad66a1c866fe89ee145fe6a21641c2f32b9] into apache:asf-site;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project website refresh Dark Mode,FLINK-33047,13549765,13549764,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,06/Sep/23 09:08,10/Oct/23 06:11,04/Jun/24 20:40,10/Oct/23 06:11,,,,,,,,,,,,,,,Project Website,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 06:11:55 UTC 2023,,,,,,,,,,"0|z1k7b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:11;dannycranmer;Merged commit [{{4309cad}}|https://github.com/apache/flink-web/commit/4309cad66a1c866fe89ee145fe6a21641c2f32b9] into apache:asf-site;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FLIP-333] Redesign Apache Flink website,FLINK-33046,13549764,,New Feature,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,dannycranmer,dannycranmer,dannycranmer,06/Sep/23 09:07,01/Nov/23 14:15,04/Jun/24 20:40,01/Nov/23 14:15,,,,,,,,,,,,,,,Project Website,,,,,,0,pull-request-available,,,,,Apply styling as per https://cwiki.apache.org/confluence/display/FLINK/FLIP-333%3A+Redesign+Apache+Flink+website,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33049,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 06:12:05 UTC 2023,,,,,,,,,,"0|z1k7aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/23 06:12;dannycranmer;Merged commit [{{4309cad}}|https://github.com/apache/flink-web/commit/4309cad66a1c866fe89ee145fe6a21641c2f32b9] into apache:asf-site;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
format avro-confluent - disable auto-registering schema in registry,FLINK-33045,13549753,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,,,mlorenzi82,mlorenzi82,06/Sep/23 08:02,04/May/24 16:59,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,2,,,,,,"{code:java}
'value.format' = 'avro-confluent',
'value.avro-confluent.url' = '<url>',
'value.avro-confluent.schema' = '<schema>', -- available in 1.18.x{code}
Currently the format {{avro-confluent}} seems to always try to register the schema to the schema-registry, be it computed out of the table-schema or set by property {{schema}} (if 1.18.x is used).

In our setup we would like to prevent our flink-jobs from registering schemas. Flink should only lookup the schema-id by calling {{getId}} from confluents schema-registry-client. We control publishing of new schema-versions outside of flink.

The bevaviour is expected to be similar to the behvaiour of KafkaAvroSerializer from confluent when {{auto.register.schemas=false}} is set.

Could there be a option like
{code:java}
'value.avro-confluent.auto.register.schema' = 'false'{code}
to disable auto-registering schema to the schema-registry?

Thank you for your help!","Flink-Kubernetes-Operator

format avro-confluent",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 04 16:59:29 UTC 2024,,,,,,,,,,"0|z1k78g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/24 16:59;pavelhp;Hello,

have the same issue with Flink DataStream API (not SQL)

As I see this class just use method
{code:java}
schemaCoder.writeSchema(getSchema(), outputStream);{code}
[https://github.com/apache/flink/blob/80af4d502318348ba15a8f75a2a622ce9dbdc968/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/RegistryAvroSerializationSchema.java#L120]

without trying get registered Schema from Schema Registry

and SchemaCoder just always use
{code:java}
int registeredId = schemaRegistryClient.register(subject, schema);{code}
[https://github.com/apache/flink/blob/80af4d502318348ba15a8f75a2a622ce9dbdc968/flink-formats/flink-avro-confluent-registry/src/main/java/org/apache/flink/formats/avro/registry/confluent/ConfluentSchemaRegistryCoder.java#L85]

which calls method

[https://github.com/confluentinc/schema-registry/blob/62f09936a26058ec13805ab8e588d66ef1975f97/client/src/main/java/io/confluent/kafka/schemaregistry/client/CachedSchemaRegistryClient.java#L408]

as I see there is no additional logic which has original KafkaAvroSerializer

[https://github.com/confluentinc/schema-registry/blob/62f09936a26058ec13805ab8e588d66ef1975f97/avro-serializer/src/main/java/io/confluent/kafka/serializers/AbstractKafkaAvroSerializer.java#L115]

 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce the frequency of triggering flush for the disk tier of the tiered storage,FLINK-33044,13549751,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,06/Sep/23 07:59,21/Sep/23 02:15,04/Jun/24 20:40,21/Sep/23 02:04,1.18.0,,,,,,,,,,1.18.0,1.19.0,,,Runtime / Network,,,,,,0,pull-request-available,,,,,"The disk cache of tiered storage will flush at the end of each subpartition's segment, which is too frequent and is bad for performance. We should improve it with some better flushing methods, e.g. flushing buffers with batch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 20 11:12:39 UTC 2023,,,,,,,,,,"0|z1k780:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 11:12;Weijie Guo;master(1.19) via b076c52d8da914e81c3e004c0b0c7883463bb151.
release-1.18 via 40bead52d200f8aa9cd30336dcc4c3f6f8c68636.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Search is broken on the Flink website,FLINK-33043,13549750,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,06/Sep/23 07:52,26/Sep/23 13:19,04/Jun/24 20:40,26/Sep/23 13:19,,,,,,,,,,,,,,,Project Website,,,,,,0,,,,,,The Search box on [https://flink.apache.org/] does not work. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 26 13:19:22 UTC 2023,,,,,,,,,,"0|z1k77s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Sep/23 13:19;dannycranmer;merged commit [{{1c42165}}|https://github.com/apache/flink-web/commit/1c421652b337f4be943d0f0a07de535318cb2a1a] into apache:asf-site ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow trigger flamegraph when task is initializing ,FLINK-33042,13549749,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,06/Sep/23 07:43,09/Sep/23 13:54,04/Jun/24 20:40,09/Sep/23 13:54,,,,,,,,,,,1.19.0,,,,Runtime / REST,Runtime / Web Frontend,,,,,0,pull-request-available,,,,,"Currently, the flamegraph can be triggered when task is running.

After FLINK-17012 and FLINK-22215, flink split the running to running and initializing. We should allow trigger flamegraph when task is initializing. For example, the initialization is very slow, we need to troubleshoot.

 

Here is a stack example, task is rebuilding the rocksdb after the parallelism is changed.

 

!image-2023-09-06-15-43-37-075.png!

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 07:43;fanrui;image-2023-09-06-15-43-37-075.png;https://issues.apache.org/jira/secure/attachment/13062721/image-2023-09-06-15-43-37-075.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Sep 09 13:54:13 UTC 2023,,,,,,,,,,"0|z1k77k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/23 09:23;hackergin;+1 for this, it will help to troubleshoot the slow initialization issues.;;;","09/Sep/23 13:54;fanrui;Merged via <master:1.19> 53702736382b80fa905494e69654061a84741e84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an introduction about how to migrate DataSet API to DataStream,FLINK-33041,13549730,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Done,Wencong Liu,Wencong Liu,Wencong Liu,06/Sep/23 02:49,02/Nov/23 08:20,04/Jun/24 20:40,02/Nov/23 08:20,1.18.0,,,,,,,,,,1.18.1,1.19.0,,,Documentation,,,,,,0,pull-request-available,,,,,"The DataSet API has been formally deprecated and will no longer receive active maintenance and support. It will be removed in the Flink 2.0 version. Flink users are recommended to migrate from the DataSet API to the DataStream API, Table API and SQL for their data processing requirements.

Most of the DataSet operators can be implemented using the DataStream API. However, we believe it would be beneficial to have an introductory article on the Flink website that guides users in migrating their DataSet jobs to DataStream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 02 08:20:37 UTC 2023,,,,,,,,,,"0|z1k73c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/23 13:54;echauchot;[~Wencong Liu] there is also a blog article that I wrote, if it can help: https://flink.apache.org/2023/05/09/howto-migrate-a-real-life-batch-pipeline-from-the-dataset-api-to-the-datastream-api/;;;","07/Sep/23 02:17;Wencong Liu;Thanks very much for your reminder [~echauchot] . I have carefully read through this blog and it's really good. However, I noticed that the blog only covers a limited number of DataSet operators. It does not include other operators like MapPartition or GroupReduce on Grouped DataSet. This pull request has provided a more comprehensive article on how to migrate all DataSet operators to DataStream. I will add some of the content from your blog to this pull request such as the difference about ExecutionEnvironment/Source/Sink between DataSet and DataStream API. If you're interested, you can review the pull request and give your feedback. 😄;;;","07/Sep/23 08:18;echauchot;[~Wencong Liu] yes, your article is way more comprehensive as targeted to completeness. I pointed the article just in case it was useful to add some content (what you are proposing here). I could take a look at the PR but, as I'm quite busy at the moment, I don't want to incur delays. So, don't wait for me, if I have time in a reasonable delay, I'll comment, if not, another reviewer could merge.;;;","02/Nov/23 08:20;xtsong;- master (1.19): 9fcea6e61a99c673205baf21a1159647099fdf67
- release-1.18: c0866243ce7283e26544472368b860991463a9f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-hive builds might be blocked (but not fail) because Maven tries to access conjars.org repository (which times out),FLINK-33040,13549697,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,mapohl,mapohl,05/Sep/23 16:03,05/Sep/23 16:15,04/Jun/24 20:40,,1.16.2,1.17.1,1.18.0,1.19.0,,,,,,,,,,,Build System / CI,,,,,,0,,,,,,"We (thanks to [~mdedetrich]) experienced timeouts when building {{flink-connectors/flink-connector-hive}} because Maven tries to access {{http://conjars.org}} to retrieve meta information for  {{net.minidev:json-smart}} which fails because the repository is gone.

[~gunnar.morling] already described this in his blog post https://www.morling.dev/blog/maven-what-are-you-waiting-for/. The result is a 75s delay. This doesn't appear to happen in CI.

We investigated where this {{conjar}} repository is coming from. It turns out that the [org.apache.hive:hive-exec:2.3.9|https://repo1.maven.org/maven2/org/apache/hive/hive-exec/2.3.9/hive-exec-2.3.9.pom] dependency derives from its parent [org.apache.hive:hive:2.3.9|https://repo1.maven.org/maven2/org/apache/hive/hive/2.3.9/hive-2.3.9.pom] which pulls in the conjar.org repository:
{code}
<repository>
  <id>conjars</id>
  <name>Conjars</name>
  <url>http://conjars.org/repo</url>
  <layout>default</layout>
  <releases>
    <enabled>true</enabled>
    <updatePolicy>always</updatePolicy>
    <checksumPolicy>warn</checksumPolicy>
  </releases>
</repository>
{code}

The subsequent hive dependency [org.apache.hive:hive:3.0.0|https://repo1.maven.org/maven2/org/apache/hive/hive/3.0.0/hive-3.0.0.pom] doesn't have this reference anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31658,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 05 16:09:50 UTC 2023,,,,,,,,,,"0|z1k6w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 16:09;mapohl;I guess, the right way to handle it to upgrade to a {{2.x}} version that doesn't include the conjar repository anymore. {{2.3.9}} is the most-recent version of the {{2.x}} version branch.

[~gunnar.morling] suggested adding the {{net.minidev:json-smart:2.3}} dependency to the dependency management to make the dependency being pulled from Maven Central. That would change the dependency tree, though.

The issue doesn't appear in CI. I guess that is due to Maven caching? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro Specific Record Logical timestamp is not serialized in Parquet,FLINK-33039,13549677,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,ahassany,ahassany,05/Sep/23 14:12,05/Sep/23 14:12,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,,,0,,,,,,"I'm trying to save a SpecificRecord to S3 Parquet, which contains a field with a logical timestmap. It's defined as
{code:java}
{
  ""name"": ""ts"",
  ""type"": {
    ""type"": ""long"",
    ""logicalType"": ""timestamp-millis""
  }
}
  {code}
And I'm using the following method to save it
{code:java}
final FileSink<MyObj> sinkFlowAggregationAvro =
    FileSink.forBulkFormat(path, AvroParquetWriters.forSpecificRecord(MyObj.class))
        .withOutputFileConfig(OutputFileConfig
            .builder()
            .withPartSuffix("".parquet"")
            .build())
        .build(); {code}
 

However, I'm getting the following casting errors:

 
{noformat}
flink-taskmanager-b467cbff9-n28zp taskmanager 2023-09-05T16:10:02.124425478+02:00 Caused by: java.lang.ClassCastException: class java.time.Instant cannot be cast to class java.lang.Number (java.time.Instant and java.lang.Number are in module java.base of loader 'bootstrap')
flink-taskmanager-b467cbff9-n28zp taskmanager 2023-09-05T16:10:02.124425478+02:00     at org.apache.parquet.avro.AvroWriteSupport.writeValueWithoutConversion(AvroWriteSupport.java:340) ~[blob_p-22acff48719adf70603f57842bd158d7f5538a47-e40c3e350efab078d53261fe2bc38640:?]
flink-taskmanager-b467cbff9-wt8p9 taskmanager 2023-09-05T16:10:01.868385407+02:00     at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.17.1.jar:1.17.1]
flink-taskmanager-b467cbff9-n28zp taskmanager 2023-09-05T16:10:02.124425478+02:00     at org.apache.parquet.avro.AvroWriteSupport.writeValue(AvroWriteSupport.java:288) ~[blob_p-22acff48719adf70603f57842bd158d7f5538a47-e40c3e350efab078d53261fe2bc38640:?]
flink-taskmanager-b467cbff9-wt8p9 taskmanager 2023-09-05T16:10:01.868385407+02:00     at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39) ~[flink-dist-1.17.1.jar:1.17.1]
flink-taskmanager-b467cbff9-wt8p9 taskmanager 2023-09-05T16:10:01.868385407+02:00     at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75) ~[flink-dist-1.17.1.jar:1.17.1]
flink-taskmanager-b467cbff9-wt8p9 taskmanager 2023-09-05T16:10:01.868385407+02:00     ... 21 more
flink-taskmanager-b467cbff9-m5gdt taskmanager 2023-09-05T16:10:01.979428558+02:00     at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29) ~[flink-dist-1.17.1.jar:1.17.1]
flink-taskmanager-b467cbff9-wt8p9 taskmanager 2023-09-05T16:10:01.868385407+02:00 Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
flink-taskmanager-b467cbff9-2xn5w taskmanager 2023-09-05T16:10:01.871644827+02:00     at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:144) ~[flink-connector-base-1.17.1.jar:1.17.1]
flink-taskmanager-b467cbff9-m5gdt taskmanager 2023-09-05T16:10:01.979428558+02:00     at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39) ~[flink-dist-1.17.1.jar:1.17.1]
flink-taskmanager-b467cbff9-pqjqr taskmanager 2023-09-05T16:10:02.276107852+02:00     ... 21 more
flink-taskmanager-b467cbff9-m5gdt taskmanager 2023-09-05T16:10:01.979428558+02:00     at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput^Cflink-taskmanager-b467cbff9-m5gdt taskmanager 2023-09-05T16:10:01.979428558+02:00     ... 21 more{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-05 14:12:12.0,,,,,,,,,,"0|z1k6rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove getMinRetentionTime in StreamExecDeduplicate,FLINK-33038,13549649,,Improvement,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,zhoujira86,zhoujira86,zhoujira86,05/Sep/23 10:32,08/Sep/23 15:24,04/Jun/24 20:40,08/Sep/23 15:24,1.18.0,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,"I suggest to remove the getMinRetentionTime method in StreamExecDeduplicate as it is not called by anyone and the ttl is controlled by the state meta data.

 

Please let me take the issue if possible",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 08 15:24:01 UTC 2023,,,,,,,,,,"0|z1k6lc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 10:42;zhoujira86;[~snuyanzin] would you please help assign to me?

also cc the modifier [~qingyue] ;;;","06/Sep/23 02:25;zhoujira86;[~Sergey Nuyanzin]  fixed, would you please have a quick review?;;;","07/Sep/23 02:48;qingyue;[~zhoujira86] Thanks for the contribution, looks good to me;;;","08/Sep/23 15:24;Sergey Nuyanzin;Merged to main as [e4deaf4356b316dbc07cf41406b9a1793eb432ab|https://github.com/apache/flink/commit/e4deaf4356b316dbc07cf41406b9a1793eb432ab];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump flink-shaded-guava to 31.1-jre,FLINK-33037,13549631,,Improvement,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,jingge,jingge,jingge,05/Sep/23 07:25,06/Sep/23 21:01,04/Jun/24 20:40,,1.17.1,,,,,,,,,,,,,,,,,,,,0,,,,,,"[32.0.0-jre|https://mvnrepository.com/artifact/com.google.guava/guava/32.0.0-jre] has 2 known cves",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 05 07:29:31 UTC 2023,,,,,,,,,,"0|z1k6hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 07:29;mapohl;Hi Jing, could you add in the description why the upgrade is necessary? ...just to get more context.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for Als,FLINK-33036,13549630,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,hotsuns,hotsuns,05/Sep/23 07:13,05/Sep/23 07:19,04/Jun/24 20:40,05/Sep/23 07:19,,,,,,,,,,,,,,,Library / Machine Learning,,,,,,0,,,,,,Add Transformer and Estimator for Als,,,,,,,,,,,,,,,,,,,,,,FLINK-33035,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-05 07:13:34.0,,,,,,,,,,"0|z1k6h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Transformer and Estimator for Als,FLINK-33035,13549629,,New Feature,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,hotsuns,hotsuns,05/Sep/23 07:12,05/Sep/23 07:19,04/Jun/24 20:40,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33036,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2023-09-05 07:12:47.0,,,,,,,,,,"0|z1k6gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect StateBackendTestBase#testGetKeysAndNamespaces,FLINK-33034,13549623,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,lda-dima,lda-dima,lda-dima,05/Sep/23 05:58,13/Sep/23 04:22,04/Jun/24 20:40,13/Sep/23 04:22,1.12.2,1.15.0,1.17.1,,,,,,,,1.19.0,,,,Runtime / State Backends,,,,,,0,pull-request-available,,,,,"In this test first namespace 'ns1' doesn't exist in state, because creating ValueState is incorrect for test (When creating the 2nd value state namespace 'ns1' is overwritten by namespace 'ns2'). Need to fix it, to change creating ValueState or to change process of updating this state.

 

If to add following code for checking count of adding namespaces to state [here|https://github.com/apache/flink/blob/3e6a1aab0712acec3e9fcc955a28f2598f019377/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateBackendTestBase.java#L501C28-L501C28]
{code:java}
assertThat(keysByNamespace.size(), is(2)); {code}
then

!image-2023-09-05-12-51-28-203.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 05:50;lda-dima;image-2023-09-05-12-51-28-203.png;https://issues.apache.org/jira/secure/attachment/13062704/image-2023-09-05-12-51-28-203.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 13 04:22:27 UTC 2023,,,,,,,,,,"0|z1k6fk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 06:02;lda-dima;[~MartijnVisser] [~trohrmann] 

Hi,

Please, assign me to this task;;;","13/Sep/23 04:22;masteryhx;merged 36b9da50da9405b5b79f0d4da9393921982ab040 into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add haservice micro benchmark for olap,FLINK-33033,13549604,13418129,Sub-task,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,05/Sep/23 00:30,30/Oct/23 08:42,04/Jun/24 20:40,27/Oct/23 04:26,1.19.0,,,,,,,,,,1.19.0,,,,Benchmarks,,,,,,0,pull-request-available,,,,,Add micro benchmarks of haservice for olap to improve the performance for short-lived jobs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32667,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 27 04:26:42 UTC 2023,,,,,,,,,,"0|z1k6bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/23 04:26;guoyangze;master: 6e61678dc7fffa4529d27ec674a6b57a1e79b097;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (ExpressionTestBase),FLINK-33032,13549585,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,04/Sep/23 16:16,10/Nov/23 07:11,04/Jun/24 20:40,20/Sep/23 03:45,1.18.0,,,,,,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,[JUnit5 Migration] Module: flink-table-planner (ExpressionTestBase),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 20 03:45:38 UTC 2023,,,,,,,,,,"0|z1k674:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Sep/23 03:45;luoyuxia;master:

50939cdce510b12c1e55d050c25c822a204332da;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (AggFunctionTestBase),FLINK-33031,13549583,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,04/Sep/23 15:52,10/Nov/23 07:11,04/Jun/24 20:40,08/Sep/23 12:24,1.18.0,,,,,,,,,,,,,,Tests,,,,,,0,pull-request-available,,,,,[JUnit5 Migration] Module: flink-table-planner (AggFunctionTestBase),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 08 12:24:37 UTC 2023,,,,,,,,,,"0|z1k66o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 16:02;jiabao.sun;PR is Ready on https://github.com/apache/flink/pull/23354;;;","08/Sep/23 12:24;luoyuxia;master:da69cc7ca3b25fd133e22f2e6b6b0c6471753a20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add python 3.11 support,FLINK-33030,13549569,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Implemented,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,04/Sep/23 13:36,25/Oct/23 17:07,04/Jun/24 20:40,15/Oct/23 07:32,1.19.0,,,,,,,,,,1.19.0,,,,API / Python,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,FLINK-33029,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 15 07:32:01 UTC 2023,,,,,,,,,,"0|z1k63k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 09:03;gaborgsomogyi;Pemja is not supporting python 3.11:
{code:java}
pip install -r dev/dev-requirements.txt
...
Collecting pemja==0.3.1 (from -r dev/dev-requirements.txt (line 31))
  Downloading pemja-0.3.1.tar.gz (48 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.4/48.4 kB 1.2 MB/s eta 0:00:00
  Installing build dependencies ... error
  error: subprocess-exited-with-error
  
  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [9 lines of output]
      Collecting packaging==20.5
        Using cached packaging-20.5-py2.py3-none-any.whl (35 kB)
      Collecting setuptools==59.2.0
        Using cached setuptools-59.2.0-py3-none-any.whl (952 kB)
      Collecting wheel==0.37.0
        Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)
      ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11
{code}

Asked [~hxbks2ks] for help: https://github.com/alibaba/pemja/issues/36;;;","12/Sep/23 12:26;gaborgsomogyi;Pemja version 0.4.0 is going to be released soon which will unblock this. Thanks!;;;","15/Oct/23 07:32;mbalassi;[{{2da9a96}}|https://github.com/apache/flink/commit/2da9a9639216b8c48850ee714065f090a80dcd65] in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop python 3.7 support,FLINK-33029,13549568,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,04/Sep/23 13:35,15/Sep/23 05:34,04/Jun/24 20:40,14/Sep/23 21:51,1.19.0,,,,,,,,,,1.19.0,,,,API / Python,,,,,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,FLINK-33030,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 05:34:33 UTC 2023,,,,,,,,,,"0|z1k63c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Sep/23 15:56;gaborgsomogyi;50cb4ee on master;;;","14/Sep/23 14:34;gaborgsomogyi;Nightly failed: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53196&view=logs&j=f73b5736-8355-5390-ec71-4dfdec0ce6c5&t=90f7230e-bf5a-531b-8566-ad48d3e03bbb

Creating a PR to fix it.;;;","14/Sep/23 21:51;morhidi;fixed via merging 6ef837b11cc12d50b9f0e9306acfb786cc8804d1 on main;;;","15/Sep/23 05:34;gaborgsomogyi;I've just double checked and wheel building has been fixed in the next nightly: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=53251&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FLIP-348: Make expanding behavior of virtual metadata columns configurable,FLINK-33028,13549560,,New Feature,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,04/Sep/23 12:22,22/Feb/24 11:30,04/Jun/24 20:40,22/Feb/24 11:30,,,,,,,,,,,1.19.0,,,,Table SQL / API,Table SQL / Planner,,,,,0,,,,,,"Many SQL vendors expose additional metadata via so-called ""pseudo columns"" or ""system columns"" next to the physical columns.

However, those columns should not be selected by default when expanding SELECT *.  Also for the sake of backward compatibility. Flink SQL already offers pseudo columns next to the physical columns exposed as metadata columns.

This proposal suggests to evolve the existing column design slightly to be more useful for platform providers.

https://cwiki.apache.org/confluence/x/_o6zDw
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 06 12:42:18 UTC 2023,,,,,,,,,,"0|z1k61k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Sep/23 12:42;twalthr;Fixed in master: 1a63d6872bfc05cffeb9877b6159ec9a5cc65d31;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Users should be able to change parallelism of excluded vertices,FLINK-33027,13549553,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,04/Sep/23 10:22,05/Sep/23 14:27,04/Jun/24 20:40,05/Sep/23 14:27,kubernetes-operator-1.6.0,,,,,,,,,,kubernetes-operator-1.7.0,,,,Autoscaler,Kubernetes Operator,,,,,0,,,,,,Currently it's not possible to manually override any parallelism even for excluded vertices. We should allow this for manually excluded ones.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 05 14:27:55 UTC 2023,,,,,,,,,,"0|z1k600:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/23 14:27;gyfora;merged to main 305cccf6d4cb9d4261a3f55ef71779aeaef26e14;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The chinese doc of sql 'Performance Tuning' has a wrong title in the index page,FLINK-33026,13549550,,Improvement,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,04/Sep/23 10:00,16/Oct/23 10:06,04/Jun/24 20:40,16/Oct/23 10:06,,,,,,,,,,,1.18.0,,,,Documentation,,,,,,0,,,,,,"The chinese doc of sql 'Performance Tuning' has a wrong title in the index page
 !image-2023-09-04-13-36-02-139.png! 

 !image-2023-09-04-13-35-20-832.png! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/23 05:35;lincoln.86xy;image-2023-09-04-13-35-20-832.png;https://issues.apache.org/jira/secure/attachment/13062693/image-2023-09-04-13-35-20-832.png","04/Sep/23 05:36;lincoln.86xy;image-2023-09-04-13-36-02-139.png;https://issues.apache.org/jira/secure/attachment/13062692/image-2023-09-04-13-36-02-139.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 16 10:06:54 UTC 2023,,,,,,,,,,"0|z1k5zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Oct/23 10:06;lincoln.86xy;fixed in master: 7d8f9821d2b3ed9876eae4ffe2e3c8b86af2d88a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchArrowPythonOverWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount fails on AZP,FLINK-33025,13549545,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,,,Sergey Nuyanzin,Sergey Nuyanzin,04/Sep/23 09:14,07/Feb/24 09:07,04/Jun/24 20:40,,1.16.3,1.19.0,1.20.0,,,,,,,,,,,,API / Python,,,,,,0,auto-deprioritized-critical,test-stability,,,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52958&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=22618
 fails on AZP as
{noformat}
Sep 03 05:05:38 05:05:38.220 [ERROR] Failures: 
Sep 03 05:05:38 05:05:38.220 [ERROR]   BatchArrowPythonOverWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByCount:122->ArrowPythonAggregateFunctionOperatorTestBase.assertOutputEquals:62 
Sep 03 05:05:38 Expected size: 4 but was: 3 in:
Sep 03 05:05:38 [Record @ (undef) : +I(c1,c2,0,0,0),
Sep 03 05:05:38     Record @ (undef) : +I(c1,c4,1,0,0),
Sep 03 05:05:38     Record @ (undef) : +I(c1,c6,2,10,2)]

{noformat}

probably related to FLINK-26990",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 07 09:04:42 UTC 2024,,,,,,,,,,"0|z1k5y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 09:15;Sergey Nuyanzin;1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=52953&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26033;;;","18/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as Critical but is unassigned and neither itself nor its Sub-Tasks have been updated for 14 days. I have gone ahead and marked it ""stale-critical"". If this ticket is critical, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","26/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-critical"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually Critical, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","07/Feb/24 09:04;mapohl;master (1.19): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57357&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=21800;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (JsonPlanTestBase),FLINK-33024,13549544,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,04/Sep/23 08:53,10/Nov/23 07:11,04/Jun/24 20:40,20/Sep/23 03:46,1.18.0,,,,,,,,,,,,,,Table SQL / Planner,,,,,,0,,,,,,[JUnit5 Migration] Module: flink-table-planner (JsonPlanTestBase),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 20 03:46:37 UTC 2023,,,,,,,,,,"0|z1k5y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 13:14;jiabao.sun;PR is ready on https://github.com/apache/flink/pull/23353;;;","20/Sep/23 03:46;luoyuxia;master:

5e3abe28f62ff9c1b3de2a48f4474b616bda112a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[JUnit5 Migration] Module: flink-table-planner (TableTestBase),FLINK-33023,13549542,13485220,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,04/Sep/23 08:51,15/Nov/23 17:05,04/Jun/24 20:40,10/Nov/23 07:10,1.18.0,,,,,,,,,,1.19.0,,,,Table SQL / Planner,,,,,,0,pull-request-available,,,,,[JUnit5 Migration] Module: flink-table-planner (TableTestBase),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-33551,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 10 07:10:53 UTC 2023,,,,,,,,,,"0|z1k5xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 08:58;jiabao.sun;PR is ready on https://github.com/apache/flink/pull/23349;;;","10/Nov/23 07:10;leonard;Resolved in master: cdb759b0ecda97bb04912553c7453710a07d499d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log an error when enrichers defined as part of the configuration can not be found/loaded,FLINK-33022,13549541,13529017,Sub-task,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pgaref,wangm92,wangm92,04/Sep/23 08:49,15/Sep/23 02:21,04/Jun/24 20:40,15/Sep/23 02:21,1.18.0,,,,,,,,,,1.19.0,,,,Runtime / Coordination,,,,,,0,pull-request-available,,,,,"if we configurate the `jobmanager.failure-enrichers`, but could not load the class in 
FailureEnricherUtils, no exceptions can be seen in the log currently, and it is very inconvenient to check the problem. Here I suggest that some ERROR-level logs should be added, or an exception should be thrown directly (because the load cannot be uploaded is not an expected result)
{code:java}
// code placeholder
@VisibleForTesting
static Collection<FailureEnricher> getFailureEnrichers(
        final Configuration configuration, final PluginManager pluginManager) {
    Set<String> includedEnrichers = getIncludedFailureEnrichers(configuration);
    LOG.info(""includedEnrichers: {}"", includedEnrichers);
    //  When empty, NO enrichers will be started.
    if (includedEnrichers.isEmpty()) {
        return Collections.emptySet();
    }
    // TODO: here maybe load nothing
    final Iterator<FailureEnricherFactory> factoryIterator =
            pluginManager.load(FailureEnricherFactory.class);
    ....
} {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32804,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 02:21:44 UTC 2023,,,,,,,,,,"0|z1k5xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 08:50;wangm92;[~pg1712@imperial.ac.uk] how do you think it?;;;","06/Sep/23 11:02;pgaref;Thanks [~wangm92] for reporting this!
I see the source of confusion here –  the ideas was to use `jobmanager.failure-enrichers` as a filter to only allow loading enrichers that are defined, and not the other way around. However, we should also notify the users if there are enrichers defined that can not be found/loaded.

Please assign the ticket to me and I will take care of it;;;","15/Sep/23 02:21;huweihua;Resolved in master: b51ee30c3d1a212947398d880a676f07f46f36be;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWS nightly builds fails on architecture tests,FLINK-33021,13549536,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,hong,martijnvisser,martijnvisser,04/Sep/23 08:29,05/Sep/23 11:04,04/Jun/24 20:40,05/Sep/23 11:03,aws-connector-4.2.0,,,,,,,,,,aws-connector-4.2.0,,,,Connectors / AWS,,,,,,0,,,,,,"https://github.com/apache/flink-connector-aws/actions/runs/6067488560/job/16459208589#step:9:879

{code:java}
Error:  Failures: 
Error:    Architecture Violation [Priority: MEDIUM] - Rule 'ITCASE tests should use a MiniCluster resource or extension' was violated (1 times):
org.apache.flink.connector.firehose.sink.KinesisFirehoseSinkITCase does not satisfy: only one of the following predicates match:
* reside in a package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type InternalMiniClusterExtension and annotated with @RegisterExtension
* reside outside of package 'org.apache.flink.runtime.*' and contain any fields that are static, final, and of type MiniClusterExtension and annotated with @RegisterExtension or are , and of type MiniClusterTestEnvironment and annotated with @TestEnv
* reside in a package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class InternalMiniClusterExtension
* reside outside of package 'org.apache.flink.runtime.*' and is annotated with @ExtendWith with class MiniClusterExtension
 or contain any fields that are public, static, and of type MiniClusterWithClientResource and final and annotated with @ClassRule or contain any fields that is of type MiniClusterWithClientResource and public and final and not static and annotated with @Rule
[INFO] 
Error:  Tests run: 21, Failures: 1, Errors: 0, Skipped: 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 05 11:04:18 UTC 2023,,,,,,,,,,"0|z1k5w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 08:30;martijnvisser;[~liangtl] [~dannycranmer] FYI;;;","04/Sep/23 09:01;liangtl;This has been fixed here! 

https://github.com/apache/flink-connector-aws/pull/92;;;","04/Sep/23 09:39;martijnvisser;[~liangtl] Ah, I think that shouldn't be fixed with a hotfix but with a ticket since you basically now have a broken implementation for 1.18. ;;;","04/Sep/23 15:39;hong;Ah. I only saw this now :( But I'll link the PR for reference!;;;","05/Sep/23 11:03;hong;Latest nightly build has passed. Will resolve this JIRA

https://github.com/apache/flink-connector-aws/actions/runs/6078564261;;;","05/Sep/23 11:04;hong;merged commit [{{8b0ae0f}}|https://github.com/apache/flink-connector-aws/commit/8b0ae0f45fea40beb52e12e6a25ede6003bef1be] into apache:main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpensearchSinkTest.testAtLeastOnceSink timed out,FLINK-33020,13549535,,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,reta,martijnvisser,martijnvisser,04/Sep/23 08:27,10/Oct/23 20:45,04/Jun/24 20:40,10/Oct/23 20:45,opensearch-1.0.2,,,,,,,,,,opensearch-1.1.0,,,,Connectors / Opensearch,,,,,,0,pull-request-available,stale-blocker,,,,"https://github.com/apache/flink-connector-opensearch/actions/runs/6061205003/job/16446139552#step:13:1029

{code:java}
Error:  Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 9.837 s <<< FAILURE! - in org.apache.flink.streaming.connectors.opensearch.OpensearchSinkTest
Error:  org.apache.flink.streaming.connectors.opensearch.OpensearchSinkTest.testAtLeastOnceSink  Time elapsed: 5.022 s  <<< ERROR!
java.util.concurrent.TimeoutException: testAtLeastOnceSink() timed out after 5 seconds
	at org.junit.jupiter.engine.extension.TimeoutInvocation.createTimeoutException(TimeoutInvocation.java:70)
	at org.junit.jupiter.engine.extension.TimeoutInvocation.proceed(TimeoutInvocation.java:59)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
	Suppressed: java.lang.InterruptedException
		at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
		at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
		at org.apache.flink.streaming.connectors.opensearch.OpensearchSinkTest.awaitForFlushToFinish(OpensearchSinkTest.java:581)
		at org.apache.flink.streaming.connectors.opensearch.OpensearchSinkTest.testAtLeastOnceSink(OpensearchSinkTest.java:441)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
		at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
		at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
		at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
		at org.junit.jupiter.engine.extension.TimeoutInvocation.proceed(TimeoutInvocation.java:46)
		... 65 more

[INFO] 
[INFO] Results:
[INFO] 
Error:  Errors: 
Error:    OpensearchSinkTest.testAtLeastOnceSink » Timeout testAtLeastOnceSink() timed o...
[INFO] 
Error:  Tests run: 60, Failures: 0, Errors: 1, Skipped: 0
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 20:44:52 UTC 2023,,,,,,,,,,"0|z1k5w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 08:27;martijnvisser;[~reta] Any thoughts on this issue?;;;","04/Sep/23 14:12;reta;[~martijnvisser] yes, I think it happens from time to time because of the async flow this test exercise, it might be flaky from sometimes, will take a look shortly.;;;","05/Oct/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","10/Oct/23 20:44;Sergey Nuyanzin;Time out was increased at [87b23c46f6f3a26bdd645d623a34dee3d19dac9d|https://github.com/apache/flink-connector-opensearch/commit/87b23c46f6f3a26bdd645d623a34dee3d19dac9d];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar tests hangs during nightly builds,FLINK-33019,13549534,,Bug,Open,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,,,martijnvisser,martijnvisser,04/Sep/23 08:25,11/Sep/23 07:57,04/Jun/24 20:40,,,,,,,,,,,,,,,,Connectors / Pulsar,,,,,,0,,,,,,"https://github.com/apache/flink-connector-pulsar/actions/runs/6067569890/job/16459404675#step:13:25195

The thread dump shows multiple parked/sleeping threads. No clear indicator of what's wrong",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 11 07:56:56 UTC 2023,,,,,,,,,,"0|z1k5vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 08:25;martijnvisser;[~Tison] Can you take a look?;;;","06/Sep/23 14:27;tison;[~martijnvisser] It seems all for the SNAPSHOT version and with JDK 11. Is there anything that can be a (internal) breaking change with this property?;;;","11/Sep/23 07:56;tison;It seems we can sometimes pass the test https://github.com/apache/flink-connector-pulsar/actions/runs/6133935359

So perhaps it's because we add the new SQL connector whose tests takes more time to complete and then cause a trivial timeout?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCP Pubsub PubSubConsumingTest.testStoppingConnectorWhenDeserializationSchemaIndicatesEndOfStream failed,FLINK-33018,13549529,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,rskraba,martijnvisser,martijnvisser,04/Sep/23 07:14,17/Apr/24 13:30,04/Jun/24 20:40,28/Sep/23 15:32,gcp-pubsub-3.0.2,,,,,,,,,,gcp-pubsub-3.0.2,gcp-pubsub-3.1.0,,,Connectors / Google Cloud PubSub,,,,,,0,auto-deprioritized-blocker,pull-request-available,,,,"https://github.com/apache/flink-connector-gcp-pubsub/actions/runs/6061318336/job/16446392844#step:13:507

{code:java}
[INFO] 
[INFO] Results:
[INFO] 
Error:  Failures: 
Error:    PubSubConsumingTest.testStoppingConnectorWhenDeserializationSchemaIndicatesEndOfStream:119 
expected: [""1"", ""2"", ""3""]
 but was: [""1"", ""2""]
[INFO] 
Error:  Tests run: 30, Failures: 1, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 28 15:32:02 UTC 2023,,,,,,,,,,"0|z1k5uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 07:15;martijnvisser;[~jjayadeep] [~rskraba] Any thoughts on what might cause this? The only update that was merged recently was FLINK-32031;;;","04/Sep/23 09:33;jjayadeep;[~martijnvisser] - Shouldn't the assert just check for (""A"",""B"") as mentioned - [https://github.com/apache/flink-connector-gcp-pubsub/blame/4fdaca7b42969d19bd939c0823afb5372c51461b/flink-connector-gcp-pubsub/src/test/java/org/apache/flink/streaming/connectors/gcp/pubsub/PubSubConsumingTest.java#L109-L119C62] or change the assert to include Array List to (""A"",""B"",""C"");;;","04/Sep/23 09:40;martijnvisser;[~jjayadeep] I don't know, if you think that's the fix then more then happy to get it merged :);;;","04/Sep/23 10:02;jjayadeep;Let me look into it and raise a PR;;;","04/Sep/23 10:20;jjayadeep;I checked and looks like the test was flaky, I re-ran it in my environment and it passed successfully. Can you please re-run the test;;;","04/Sep/23 13:32;rskraba;_If I understand correctly_ -- the *""C""* element is meant to be acked but not emitted in this test.  This (third) end-of-stream message indicates that processing is finished, so it _should_ have two emitted elements (A and B) but three acknowledged ids (1, 2, and 3).

I'm reading through the code, and I don't believe there's any relevant changes due to the bom bump.  Is this always reproducible in the build environment or is it flaky?  If flaky -- how flaky?  Often or rarely?

I don't have any good hypothesis for the moment. Maybe there's a synchronization issue on the private test class [collecting the acks|https://github.com/apache/flink-connector-gcp-pubsub/blob/c06e25a97c5f74873db9d272a4f2cf4787185c4a/flink-connector-gcp-pubsub/src/test/java/org/apache/flink/streaming/connectors/gcp/pubsub/PubSubConsumingTest.java#L214]?  That doesn't seem quite right, since the {{notifyCheckpointComplete}} should have [already occurred|https://github.com/apache/flink-connector-gcp-pubsub/blob/c06e25a97c5f74873db9d272a4f2cf4787185c4a/flink-connector-gcp-pubsub/src/main/java/org/apache/flink/streaming/connectors/gcp/pubsub/common/AcknowledgeOnCheckpoint.java#L84] just before the assert.

Any other clues that might help narrow this down?;;;","07/Sep/23 13:10;rskraba;Hey, I can reproduce this -- all you need to do is run this test about 12,000 times ;)  (using IntelliJ repeat until fail).

{code}
Connected to the target VM, address: '127.0.0.1:44879', transport: 'socket'

org.opentest4j.AssertionFailedError: 
expected: [""1"", ""2"", ""3""]
 but was: [""1"", ""2""]
Expected :[""1"", ""2"", ""3""]
Actual   :[""1"", ""2""]
{code}

I'm taking a look.;;;","07/Sep/23 14:29;jjayadeep;I ran it 23K times yesterday and didnt face any issue hence stopped the test :). Good that you were able to reproduce the error!;;;","07/Sep/23 15:48;rskraba;I think I have a fix -- it looks like occasionally (every 10K runs or so) we hit the explicit cancel() written in this specific test, before we hit the implicit cancel() due to the endofStream message being discovered.  I'm letting this run for a while before submitting a PR.  -This test also slows down in consecutive runs, which might indicate a leak but I'm hoping this is not the case.- (Edit: it's not, it was definitely an IntelliJ issue with conditional breakpoints).

I can't seem to _prove_ this is a fix yet, because both IntelliJ and standalone maven will eventually segfault when I try to run it 100K times with the changes.  I'm looking into it.;;;","12/Sep/23 22:35;flink-jira-bot;I am the [Flink Jira Bot|https://github.com/apache/flink-jira-bot/] and I help the community manage its development. I see this issues has been marked as a Blocker but is unassigned and neither itself nor its Sub-Tasks have been updated for 1 days. I have gone ahead and marked it ""stale-blocker"". If this ticket is a Blocker, please either assign yourself or give an update. Afterwards, please remove the label or in 7 days the issue will be deprioritized.
;;;","13/Sep/23 15:34;rskraba;I don't think this JIRA is necessarily a blocker -- it looks like an error in the test that shows up as flakiness (as opposed to an error in the connector).  The above PR fixes the test, if anybody wants to take a look!

The ""eventual segfault"" that I noticed early happens before and after the fix, and is *very likely* a different error due to the {{MockStreamingRuntimeContext}} creating and not closing a {{MockEnvironment}}.  We could probably fix this, but it should never occur unless you try and run one of these tests 100K times!  What do you think?;;;","20/Sep/23 22:35;flink-jira-bot;This issue was labeled ""stale-blocker"" 7 days ago and has not received any updates so it is being deprioritized. If this ticket is actually a Blocker, please raise the priority and ask a committer to assign you the issue or revive the public discussion.
;;;","22/Sep/23 12:10;rskraba;Hey [~jjayadeep], would you like to take a look at the [PR|https://github.com/apache/flink-connector-gcp-pubsub/pull/19]?  It's not likely that it's related to the version bumps, but there's a lack of eyes in the Pub/Sub area :D  Any review would be appreciated!;;;","28/Sep/23 14:23;jjayadeep;Hi [~rskraba] - I looked at the PR and tested it as well and it looks good to me. ;;;","28/Sep/23 15:32;martijnvisser;Fixed in apache/flink-connector-gcp-pubsub:

main f5372f25cfc1954d00a4b2fc9342e8ed5a3ef3ab
v3.0 557acd8d6d121d27eb6b06f29f552089ea0adad3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nightly run for Flink Kafka connector fails,FLINK-33017,13549528,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Done,,martijnvisser,martijnvisser,04/Sep/23 07:11,16/Oct/23 06:43,04/Jun/24 20:40,16/Oct/23 06:43,kafka-3.1.0,,,,,,,,,,,,,,Connectors / Kafka,,,,,,0,pull-request-available,,,,,"https://github.com/apache/flink-connector-kafka/actions/runs/6061283403/job/16446313350#step:13:54462

{code:java}
2023-09-03T00:29:28.8942615Z [ERROR] Errors: 
2023-09-03T00:29:28.8942799Z [ERROR] FlinkKafkaConsumerBaseMigrationTest.testRestore
2023-09-03T00:29:28.8943079Z [ERROR]   Run 1: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8943342Z [ERROR]   Run 2: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8943604Z [ERROR]   Run 3: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8943903Z [ERROR]   Run 4: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8944164Z [ERROR]   Run 5: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8944419Z [ERROR]   Run 6: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8944714Z [ERROR]   Run 7: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8944970Z [ERROR]   Run 8: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8945221Z [ERROR]   Run 9: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8945294Z [INFO] 
2023-09-03T00:29:28.8945577Z [ERROR] FlinkKafkaConsumerBaseMigrationTest.testRestoreFromEmptyStateNoPartitions
2023-09-03T00:29:28.8945769Z [ERROR]   Run 1: org/apache/flink/shaded/guava31/com/google/common/collect/ImmutableList
2023-09-03T00:29:28.8946019Z [ERROR]   Run 2: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8946266Z [ERROR]   Run 3: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8946525Z [ERROR]   Run 4: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8946778Z [ERROR]   Run 5: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8947027Z [ERROR]   Run 6: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8947269Z [ERROR]   Run 7: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8947516Z [ERROR]   Run 8: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8947765Z [ERROR]   Run 9: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8947834Z [INFO] 
2023-09-03T00:29:28.8948117Z [ERROR] FlinkKafkaConsumerBaseMigrationTest.testRestoreFromEmptyStateWithPartitions
2023-09-03T00:29:28.8948407Z [ERROR]   Run 1: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8948660Z [ERROR]   Run 2: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8948949Z [ERROR]   Run 3: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8949192Z [ERROR]   Run 4: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8949433Z [ERROR]   Run 5: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8949673Z [ERROR]   Run 6: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8949913Z [ERROR]   Run 7: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8950155Z [ERROR]   Run 8: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8950518Z [ERROR]   Run 9: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8950598Z [INFO] 
2023-09-03T00:29:28.8950819Z [ERROR] FlinkKafkaProducerMigrationOperatorTest.testRestoreProducer
2023-09-03T00:29:28.8951072Z [ERROR]   Run 1: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8951318Z [ERROR]   Run 2: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8951563Z [ERROR]   Run 3: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8951859Z [ERROR]   Run 4: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8951934Z [INFO] 
2023-09-03T00:29:28.8952129Z [ERROR] FlinkKafkaProducerMigrationTest.testRestoreProducer
2023-09-03T00:29:28.8952377Z [ERROR]   Run 1: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8952675Z [ERROR]   Run 2: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8952906Z [ERROR]   Run 3: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8953152Z [ERROR]   Run 4: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8953395Z [ERROR]   Run 5: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8953637Z [ERROR]   Run 6: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8953884Z [ERROR]   Run 7: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8954131Z [ERROR]   Run 8: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8954382Z [ERROR]   Run 9: Could not initialize class org.apache.flink.runtime.util.config.memory.ManagedMemoryUtils
2023-09-03T00:29:28.8954457Z [INFO] 
2023-09-03T00:29:28.8954822Z [ERROR]   FlinkKafkaConsumerBaseTest.testOpen:896 » NoClassDefFound Could not initialize...
2023-09-03T00:29:28.8955129Z [ERROR]   FlinkKafkaConsumerBaseTest.testScaleDown:742->testRescaling:793 » NoClassDefFound
2023-09-03T00:29:28.8955424Z [ERROR]   FlinkKafkaConsumerBaseTest.testScaleUp:737->testRescaling:793 » NoClassDefFound
2023-09-03T00:29:28.8955772Z [ERROR]   FlinkKafkaProducerBaseTest.testAsyncErrorRethrownOnCheckpoint:191 » NoClassDefFound
2023-09-03T00:29:28.8956183Z [ERROR]   FlinkKafkaProducerBaseTest.testAsyncErrorRethrownOnCheckpointAfterFlush:236 » NoClassDefFound
2023-09-03T00:29:28.8956503Z [ERROR]   FlinkKafkaProducerBaseTest.testAsyncErrorRethrownOnInvoke:154 » NoClassDefFound
2023-09-03T00:29:28.8956817Z [ERROR]   FlinkKafkaProducerBaseTest.testAtLeastOnceProducer:297 » NoClassDefFound Could...
2023-09-03T00:29:28.8957301Z [ERROR]   FlinkKafkaProducerBaseTest.testDoesNotWaitForPendingRecordsIfFlushingDisabled:370 » NoClassDefFound
2023-09-03T00:29:28.8957644Z [ERROR]   FlinkKafkaProducerTest.testOpenKafkaCustomPartitioner:107 » NoClassDefFound Co...
2023-09-03T00:29:28.8958068Z [ERROR]   FlinkKafkaProducerTest.testOpenKafkaSerializationSchemaProducer:81 » NoClassDefFound
2023-09-03T00:29:28.8958391Z [ERROR]   FlinkKafkaProducerTest.testOpenSerializationSchemaProducer:55 » NoClassDefFound
2023-09-03T00:29:28.8958742Z [ERROR]   KafkaDynamicTableFactoryTest.testBoundedGroupOffsets:494->testBoundedOffsets:569 » Validation
2023-09-03T00:29:28.8959087Z [ERROR]   KafkaDynamicTableFactoryTest.testBoundedLatestOffset:474->testBoundedOffsets:569 » Validation
2023-09-03T00:29:28.8959445Z [ERROR]   KafkaDynamicTableFactoryTest.testBoundedSpecificOffsets:452->testBoundedOffsets:569 » Validation
2023-09-03T00:29:28.8959774Z [ERROR]   KafkaDynamicTableFactoryTest.testBoundedTimestamp:514->testBoundedOffsets:569 » Validation
2023-09-03T00:29:28.8960109Z [ERROR]   KafkaDynamicTableFactoryTest.testDisableDiscoverPartition:1143 » Validation Un...
2023-09-03T00:29:28.8960450Z [ERROR]   KafkaDynamicTableFactoryTest.testDiscoverPartitionByDefault:1105 » Validation ...
2023-09-03T00:29:28.8960770Z [ERROR]   KafkaDynamicTableFactoryTest.testPrimaryKeyValidation:1058 » Validation Unable...
2023-09-03T00:29:28.8961073Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSink:588 » Validation Unable to create a...
2023-09-03T00:29:28.8961519Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSinkAutoCompleteSchemaRegistrySubject:731->verifyEncoderSubject:825 » Validation
2023-09-03T00:29:28.8961907Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSinkSemanticTranslation:632 » Validation
2023-09-03T00:29:28.8962233Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSinkWithKeyValue:660 » Validation Unable...
2023-09-03T00:29:28.8962559Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSinkWithParallelism:699 » Validation Una...
2023-09-03T00:29:28.8962863Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSource:193 » Validation Unable to create...
2023-09-03T00:29:28.8963287Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceCommitOnCheckpointDisabled:363 » Validation
2023-09-03T00:29:28.8963734Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceSetOffsetReset:388->testSetOffsetResetForStartFromGroupOffsets:415 » Validation
2023-09-03T00:29:28.8964163Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceSetOffsetReset:388->testSetOffsetResetForStartFromGroupOffsets:415 » Validation
2023-09-03T00:29:28.8964595Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceSetOffsetReset:388->testSetOffsetResetForStartFromGroupOffsets:415 » Validation
2023-09-03T00:29:28.8965009Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceSetOffsetReset:388->testSetOffsetResetForStartFromGroupOffsets:415 » Validation
2023-09-03T00:29:28.8965323Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceWithKeyValue:271 » Validation Unab...
2023-09-03T00:29:28.8965681Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceWithKeyValueAndMetadata:310 » Validation
2023-09-03T00:29:28.8965995Z [ERROR]   KafkaDynamicTableFactoryTest.testTableSourceWithPattern:238 » Validation Unabl...
2023-09-03T00:29:28.8966373Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testBoundedGroupOffsets:480->testBoundedOffsets:850 » Validation
2023-09-03T00:29:28.8966741Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testBoundedLatestOffset:460->testBoundedOffsets:850 » Validation
2023-09-03T00:29:28.8967129Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testBoundedSpecificOffsets:438->testBoundedOffsets:850 » Validation
2023-09-03T00:29:28.8967481Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testBoundedTimestamp:502->testBoundedOffsets:850 » Validation
2023-09-03T00:29:28.8967812Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testBufferedTableSink:223 » Validation Unab...
2023-09-03T00:29:28.8968115Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testTableSink:188 » Validation Unable to cr...
2023-09-03T00:29:28.8968641Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testTableSinkAutoCompleteSchemaRegistrySubject:315->verifyEncoderSubject:389 » Validation
2023-09-03T00:29:28.8968987Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testTableSinkWithParallelism:286 » Validation
2023-09-03T00:29:28.8969348Z [ERROR]   UpsertKafkaDynamicTableFactoryTest.testTableSource:158 » Validation Unable to ...
2023-09-03T00:29:28.8969421Z [INFO] 
2023-09-03T00:29:28.8969573Z [ERROR] Tests run: 351, Failures: 17, Errors: 46, Skipped: 31
2023-09-03T00:29:28.8969642Z [INFO] 
2023-09-03T00:29:28.8969905Z [INFO] ------------------------------------------------------------------------
{code}",,,,,,,,,,,,,,,,,,FLINK-33219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 18 06:49:45 UTC 2023,,,,,,,,,,"0|z1k5ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/23 17:53;tzulitai;Fixed via:
apache/flink-connector-kafka:main - 818d1fdedaad63631eab5d44ec90c748cfcf299f
apache/flink-connector-kafka:v3.0 - a81cbeb62b1f12a3f80ff6f2380047a2d7400194;;;","12/Sep/23 17:54;tzulitai;Lets only resolve this ticket once we have a green light on the nightly build:

Manually triggered one here -
[https://github.com/apache/flink-connector-kafka/actions/runs/6163109229];;;","18/Sep/23 06:49;martijnvisser;[~tzulitai][~Gerrrr] Nightly builds are still failing, see https://github.com/apache/flink-connector-kafka/actions/runs/6210384981/job/16858550802 but with a different error. I'll create a new ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid download url for Flink JDBC Driver page,FLINK-33016,13549513,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,xiangyu0xf,xiangyu0xf,04/Sep/23 03:49,04/Sep/23 06:36,04/Jun/24 20:40,04/Sep/23 06:36,,,,,,,,,,,,,,,Documentation,Table SQL / JDBC,,,,,0,,,,,,"The download url in flink sql driver page is invalid.
!image-2023-09-04-11-47-26-874.png|width=447,height=414!

 

!image-2023-09-04-11-48-40-461.png|width=483,height=108!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32794,,,,,,,,,,"04/Sep/23 03:47;xiangyu0xf;image-2023-09-04-11-47-26-874.png;https://issues.apache.org/jira/secure/attachment/13062681/image-2023-09-04-11-47-26-874.png","04/Sep/23 03:48;xiangyu0xf;image-2023-09-04-11-48-40-461.png;https://issues.apache.org/jira/secure/attachment/13062680/image-2023-09-04-11-48-40-461.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 04 06:35:53 UTC 2023,,,,,,,,,,"0|z1k5r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Sep/23 04:05;libenchao;The artifacts are only pushed to nexus after Flink is released. Since Flink JDBC is introduced in 1.18.0, which is not released yet, it's normal the download url is not available for the time being.;;;","04/Sep/23 06:35;xiangyu0xf;[~libenchao] Thx for ur reply, I'll close this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableAggregateFunction codegen does not eval emitUpdateWithRetract,FLINK-33015,13549357,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Duplicate,,qingyue,qingyue,01/Sep/23 06:07,11/Jan/24 10:09,04/Jun/24 20:40,11/Jan/24 10:09,1.15.0,1.15.1,1.15.2,1.15.3,1.15.4,1.16.0,1.16.1,1.16.2,1.17.1,1.18.0,,,,,,,,,,,0,,,,,,"https://www.mail-archive.com/user-zh@flink.apache.org/msg15251.html
This issue arose after the retirement of Old Planner",,,,,,,,,,,,,,,,,,,,,,,FLINK-31788,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 01 06:40:47 UTC 2023,,,,,,,,,,"0|z1k4sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/23 06:38;jark;Maybe duplicated with https://issues.apache.org/jira/browse/FLINK-31788 ?;;;","01/Sep/23 06:40;qingyue;Good catch [~jark], will close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink jobmanager raise  java.io.IOException: Connection reset by peer,FLINK-33014,13549353,,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Not A Bug,,zhuyinjun,zhuyinjun,01/Sep/23 03:44,01/Nov/23 03:35,04/Jun/24 20:40,01/Nov/23 03:35,1.17.1,,,,,,,,,,,,,,Runtime / Coordination,,,,,,0,,,,,," 
The Flink cluster was deployed using the Docker image of Flink 1.17.1 java8. After deployment, on k8s, in standalone form, jobmanager printed this error at intervals, and taskmanager did not print any errors,

There are currently no jobs running
{code:java}
2023-09-01 11:34:14,293 WARN  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Unhandled exception
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_372]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_372]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[?:1.8.0_372]
        at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.17.1.jar:1.17.1]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]
{code}","|*blob.server.port*|6124|
|*classloader.resolve-order*|parent-first|
|*jobmanager.execution.failover-strategy*|region|
|*jobmanager.memory.heap.size*|2228014280b|
|*jobmanager.memory.jvm-metaspace.size*|536870912b|
|*jobmanager.memory.jvm-overhead.max*|322122552b|
|*jobmanager.memory.jvm-overhead.min*|322122552b|
|*jobmanager.memory.off-heap.size*|134217728b|
|*jobmanager.memory.process.size*|3gb|
|*jobmanager.rpc.address*|naf-flink-ms-flink-manager-1-59m7w|
|*jobmanager.rpc.port*|6123|
|*parallelism.default*|1|
|*query.server.port*|6125|
|*rest.address*|0.0.0.0|
|*rest.bind-address*|0.0.0.0|
|*rest.connection-timeout*|60000|
|*rest.server.numThreads*|8|
|*slot.request.timeout*|3000000|
|*state.backend.rocksdb.localdir*|/home/nafplat/data/flinkStateStore|
|*state.backend.type*|rocksdb|
|*taskmanager.bind-host*|0.0.0.0|
|*taskmanager.host*|0.0.0.0|
|*taskmanager.memory.framework.off-heap.batch-shuffle.size*|256mb|
|*taskmanager.memory.framework.off-heap.size*|512mb|
|*taskmanager.memory.managed.fraction*|0.4|
|*taskmanager.memory.network.fraction*|0.2|
|*taskmanager.memory.process.size*|5gb|
|*taskmanager.memory.task.off-heap.size*|268435456bytes|
|*taskmanager.numberOfTaskSlots*|2|
|*taskmanager.runtime.large-record-handler*|true|
|*web.submit.enable*|true|
|*web.tmpdir*|/tmp/flink-web-c1b57e2b-5426-4fb8-a9ce-5acd1cceefc9|
|*web.upload.dir*|/opt/flink/nafJar|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 01 03:35:49 UTC 2023,,,,,,,,,,"0|z1k4rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/23 07:50;mapohl;Hi [~zhuyinjun] do you use a LoadBalancer in your k8s setup? I'm asking because there is a similar issue FLINK-18129. There, the stacktrace is caused most-likely by some liveness checks. Apparently, the LoadBalancer sents RST packages to the port Flink listens.;;;","01/Sep/23 08:20;zhuyinjun;Hi [~mapohl]  There is no LoadBalancer configured on my k8s setup,I used the same configuration file to replace the 1.15.1 flink docker with the 1.17.1 flink docker. They have the same docker-entrypoint. sh. 
Usually, the cluster should not have any changes,But I found that /opt/cnrm/container_resource.rc file was added in docker 1.17.1, which contains the export EXCLUSIVE_ CPUS_ LIST = 4, 
I have set 2cpu for each task manager on my side.;;;","01/Sep/23 09:04;mapohl;[~wangyang0918] Did you experience something like that?;;;","05/Sep/23 02:56;zhuyinjun;[~mapohl]  [~wangyang0918]

Is there any progress now? No operators can be executed now. Due to java.io.IOException: Connection reset by peer, I found that my task will prompt a file not found error, and there will be the following error.

 
{code:java}
2023-09-05 10:41:34,349 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_372]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_372]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[?:1.8.0_372]
        at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.17.1.jar:1.17.1]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]
2023-09-05 10:41:34,350 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_372]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_372]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[?:1.8.0_372]
        at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.17.1.jar:1.17.1]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]
2023-09-05 10:41:34,354 ERROR org.apache.flink.runtime.rest.handler.legacy.files.StaticFileServerHandler [] - Caught exception
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_372]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_372]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[?:1.8.0_372]
        at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.17.1.jar:1.17.1]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]
2023-09-05 10:44:19,883 WARN  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Unhandled exception
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_372]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_372]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_372]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[?:1.8.0_372]
        at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357) ~[flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.17.1.jar:1.17.1]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.17.1.jar:1.17.1]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_372]
 {code}
 

 ;;;","05/Sep/23 09:16;mapohl;Do you have any other components that send messages to the netty server? Maybe creating a network dump as suggested in https://github.com/netty/netty/issues/6029 could bring more insights.;;;","01/Nov/23 03:35;zhuyinjun;Insufficient resource allocation for Tm;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
